{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import datetime\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.optimize import curve_fit\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.dates import DateFormatter\n",
    "import matplotlib.patheffects as pe\n",
    "from itertools import combinations_with_replacement\n",
    "from dataclasses import dataclass\n",
    "from collections.abc import Callable\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import chi2\n",
    "\n",
    "from cost import *\n",
    "from data import *\n",
    "from plotting import *\n",
    "from regression import *\n",
    "from utils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown Select the subset of the database to analyze. Either notable models, frontier models, OpenAI or Google Deepmind\n",
    "\n",
    "filter_alphago_outliers = False #@param {type:\"boolean\"}\n",
    "frontier_selection = 'external' #@param ['disabled', 'internal', 'external']\n",
    "top_n = 10 #@param\n",
    "model_selection = 'Notable models' #@param ['Google DeepMind models', 'OpenAI models', 'Meta AI models', 'Notable models', 'Language models']\n",
    "cutoff_date = '2010-01-01' # '2017-06-01' # #@param\n",
    "cost_cutoff_date = '2012-01-01'  # '2018-01-01'\n",
    "estimation_method = 'hardware-capex-energy'\n",
    "dep_var = 'log_cost'  # log_flop, log_cost\n",
    "exclude_models_containing = [] # ['GNMT', 'AlphaZero', 'AlphaGo Master', 'AlphaGo Zero']\n",
    "estimation_method_lookup = {\n",
    "    'hardware-capex-energy': estimate_hardware_capex_energy,\n",
    "    'hardware-acquisition': estimate_hardware_acquisition_cost,\n",
    "    'cloud': estimate_cloud_costs,\n",
    "}\n",
    "cost_estimation_function = estimation_method_lookup[estimation_method]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>System</th>\n",
       "      <th>Domain</th>\n",
       "      <th>Task</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Notability criteria</th>\n",
       "      <th>Notability criteria notes</th>\n",
       "      <th>Model accessibility</th>\n",
       "      <th>Link</th>\n",
       "      <th>Citations</th>\n",
       "      <th>Reference</th>\n",
       "      <th>...</th>\n",
       "      <th>Training compute lower bound</th>\n",
       "      <th>Training compute upper bound</th>\n",
       "      <th>Training chip-hours</th>\n",
       "      <th>Code accessibility</th>\n",
       "      <th>Dataset accessibility</th>\n",
       "      <th>Accessibility notes</th>\n",
       "      <th>Organization categorization (from Organization)</th>\n",
       "      <th>Possibly over 1e23 FLOP</th>\n",
       "      <th>Training cost trends 2</th>\n",
       "      <th>Training cost trends 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chameleon-34B</td>\n",
       "      <td>Multimodal,Image generation,Language,Vision</td>\n",
       "      <td>Language modelling/generation,Vision-language ...</td>\n",
       "      <td>Srinivasan Iyer, Bernie Huang, Lili Yu, Arun B...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Unreleased</td>\n",
       "      <td>https://arxiv.org/abs/2405.09818v1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Chameleon: Mixed-Modal Early-Fusion Foundation...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Industry</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chameleon-7B</td>\n",
       "      <td>Multimodal,Image generation,Vision,Language</td>\n",
       "      <td>Language modelling/generation,Vision-language ...</td>\n",
       "      <td>Srinivasan Iyer, Bernie Huang, Lili Yu, Arun B...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Unreleased</td>\n",
       "      <td>https://arxiv.org/abs/2405.09818v1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Chameleon: Mixed-Modal Early-Fusion Foundation...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Industry</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yi-Large</td>\n",
       "      <td>Language</td>\n",
       "      <td>Chat,Language modelling/generation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>API access</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Industry</td>\n",
       "      <td>checked</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fugaku-LLM</td>\n",
       "      <td>Language</td>\n",
       "      <td>Language modelling,Translation,Japanese langua...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Open source</td>\n",
       "      <td>https://www.fujitsu.com/global/about/resources...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Release of “Fugaku-LLM” – a large language mod...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Academia,Industry</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gemini 1.5 Flash</td>\n",
       "      <td>Multimodal,Language,Vision,Audio</td>\n",
       "      <td>Chat,Audio speech recognition,Image captioning...</td>\n",
       "      <td>Gemini Team</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>API access</td>\n",
       "      <td>https://storage.googleapis.com/deepmind-media/...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Gemini 1.5: Unlocking multimodal understanding...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Industry</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             System                                       Domain  \\\n",
       "0     Chameleon-34B  Multimodal,Image generation,Language,Vision   \n",
       "1      Chameleon-7B  Multimodal,Image generation,Vision,Language   \n",
       "2          Yi-Large                                     Language   \n",
       "3        Fugaku-LLM                                     Language   \n",
       "4  Gemini 1.5 Flash             Multimodal,Language,Vision,Audio   \n",
       "\n",
       "                                                Task  \\\n",
       "0  Language modelling/generation,Vision-language ...   \n",
       "1  Language modelling/generation,Vision-language ...   \n",
       "2                 Chat,Language modelling/generation   \n",
       "3  Language modelling,Translation,Japanese langua...   \n",
       "4  Chat,Audio speech recognition,Image captioning...   \n",
       "\n",
       "                                             Authors Notability criteria  \\\n",
       "0  Srinivasan Iyer, Bernie Huang, Lili Yu, Arun B...                 NaN   \n",
       "1  Srinivasan Iyer, Bernie Huang, Lili Yu, Arun B...                 NaN   \n",
       "2                                                NaN                 NaN   \n",
       "3                                                NaN                 NaN   \n",
       "4                                        Gemini Team                 NaN   \n",
       "\n",
       "  Notability criteria notes Model accessibility  \\\n",
       "0                       NaN          Unreleased   \n",
       "1                       NaN          Unreleased   \n",
       "2                       NaN          API access   \n",
       "3                       NaN         Open source   \n",
       "4                       NaN          API access   \n",
       "\n",
       "                                                Link  Citations  \\\n",
       "0                 https://arxiv.org/abs/2405.09818v1        NaN   \n",
       "1                 https://arxiv.org/abs/2405.09818v1        NaN   \n",
       "2                                                NaN        NaN   \n",
       "3  https://www.fujitsu.com/global/about/resources...        NaN   \n",
       "4  https://storage.googleapis.com/deepmind-media/...        NaN   \n",
       "\n",
       "                                           Reference  ...  \\\n",
       "0  Chameleon: Mixed-Modal Early-Fusion Foundation...  ...   \n",
       "1  Chameleon: Mixed-Modal Early-Fusion Foundation...  ...   \n",
       "2                                                NaN  ...   \n",
       "3  Release of “Fugaku-LLM” – a large language mod...  ...   \n",
       "4  Gemini 1.5: Unlocking multimodal understanding...  ...   \n",
       "\n",
       "  Training compute lower bound Training compute upper bound  \\\n",
       "0                          NaN                          NaN   \n",
       "1                          NaN                          NaN   \n",
       "2                          NaN                          NaN   \n",
       "3                          NaN                          NaN   \n",
       "4                          NaN                          NaN   \n",
       "\n",
       "   Training chip-hours Code accessibility  Dataset accessibility  \\\n",
       "0                  NaN                NaN                    NaN   \n",
       "1                  NaN                NaN                    NaN   \n",
       "2                  NaN                NaN                    NaN   \n",
       "3                  NaN                NaN                    NaN   \n",
       "4                  NaN                NaN                    NaN   \n",
       "\n",
       "  Accessibility notes Organization categorization (from Organization)  \\\n",
       "0                 NaN                                        Industry   \n",
       "1                 NaN                                        Industry   \n",
       "2                 NaN                                        Industry   \n",
       "3                 NaN                               Academia,Industry   \n",
       "4                 NaN                                        Industry   \n",
       "\n",
       "  Possibly over 1e23 FLOP  Training cost trends 2 Training cost trends 3  \n",
       "0                     NaN                     NaN                    NaN  \n",
       "1                     NaN                     NaN                    NaN  \n",
       "2                 checked                     NaN                    NaN  \n",
       "3                     NaN                     NaN                    NaN  \n",
       "4                     NaN                     NaN                    NaN  \n",
       "\n",
       "[5 rows x 60 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load PCD data\n",
    "df = load_pcd_df()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97 top Notable models found\n",
      "They span March 2010 to April 2024\n"
     ]
    }
   ],
   "source": [
    "#@title Analysis of the compute growth\n",
    "\n",
    "def find_top_models_up_to_release(df, top_n):\n",
    "    \"\"\"Find the models which were in the top n by compute when they were released.\"\"\"\n",
    "\n",
    "    # This set will keep track of models that were ever in the top 10 at their release\n",
    "    ever_in_top_n = set()\n",
    "\n",
    "    # Iterate over each date in the DataFrame\n",
    "    for current_date in df['date'].unique():\n",
    "        # Get all entries up to the current date\n",
    "        historical_data = df[df['date'] <= current_date]\n",
    "        # Find top 10 models by flop count in this subset\n",
    "        top_n_models = historical_data.nlargest(top_n, 'flop')['System']\n",
    "        # Update the set of models that were ever in top n\n",
    "        ever_in_top_n.update(top_n_models)\n",
    "\n",
    "    # Return DataFrame filtered to only include models that were ever in the top 10\n",
    "    return df[df['System'].isin(ever_in_top_n)]\n",
    "\n",
    "# Filter out NaNs and sort by date\n",
    "df_filtered = (df\n",
    "    .rename(columns={'Training compute (FLOP)': 'flop', 'Publication date': 'date'})\n",
    "    .dropna(subset=['date', 'flop'])\n",
    "    .assign(date=lambda x: pd.to_datetime(x['date']), log_flop=lambda x: np.log10(x['flop']))\n",
    "    .sort_values('date'))\n",
    "\n",
    "df_notable = df_filtered.copy() # Store a copy for Top N sensitivity analysis\n",
    "\n",
    "# Drop Alpha Go Master / Zero\n",
    "if filter_alphago_outliers:\n",
    "    mask = (df_filtered[\"System\"] == 'AlphaGo Master') | (df_filtered[\"System\"] == 'AlphaGo Zero')\n",
    "    df_filtered = df_filtered[~mask]\n",
    "\n",
    "# Filter top models\n",
    "if frontier_selection == 'external':\n",
    "    df_filtered = find_top_models_up_to_release(df_filtered, top_n)\n",
    "\n",
    "# Filter models depending on analysis\n",
    "if model_selection == 'Google DeepMind models':\n",
    "    re = 'DeepMind|Google'\n",
    "    mask = df_filtered['Organization'].str.contains(re, na=False)\n",
    "    df_filtered = df_filtered[mask]\n",
    "elif model_selection == 'OpenAI models':\n",
    "    re = 'OpenAI'\n",
    "    mask = df_filtered['Organization'].str.contains(re, na=False)\n",
    "    df_filtered = df_filtered[mask]\n",
    "    # Exclude ADAM (retroactively attributed to OpenAI but published before its foundation)\n",
    "    df_filtered = df_filtered[df_filtered['System'] != 'ADAM (CIFAR-10)']\n",
    "elif model_selection == 'Meta AI models':\n",
    "    re = 'Meta AI|Facebook|FAIR'\n",
    "    mask = df_filtered['Organization'].str.contains(re, na=False)\n",
    "    df_filtered = df_filtered[mask]\n",
    "elif model_selection == 'Language models':\n",
    "    re = 'Language|Multimodal'\n",
    "    mask = df_filtered['Domain'].str.contains(re, na=False)\n",
    "    df_filtered = df_filtered[mask]\n",
    "\n",
    "# Filter top models\n",
    "if frontier_selection == 'internal':\n",
    "    df_filtered = find_top_models_up_to_release(df_filtered, top_n)\n",
    "\n",
    "# Filter for models after the cutoff date\n",
    "df_filtered = df_filtered[df_filtered['date'] > cutoff_date]\n",
    "\n",
    "print(f\"{len(df_filtered)}{' top' if frontier_selection != 'disabled' else ''} {model_selection} found\")\n",
    "print(f\"They span {df_filtered['date'].min().strftime('%B %Y')} to {df_filtered['date'].max().strftime('%B %Y')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== System: 6-layer MLP (MNIST) ====\n",
      "Could not find hardware model for 6-layer MLP (MNIST)\n",
      "\n",
      "==== System: Feedforward NN ====\n",
      "Could not find hardware model for Feedforward NN\n",
      "\n",
      "==== System: RNN 500/10 + RT09 LM (NIST RT05) ====\n",
      "Could not find hardware model for RNN 500/10 + RT09 LM (NIST RT05)\n",
      "\n",
      "==== System: KN5 LM + RNN 400/10 (WSJ) ====\n",
      "Could not find hardware model for KN5 LM + RNN 400/10 (WSJ)\n",
      "\n",
      "==== System: MCDNN (MNIST) ====\n",
      "Could not find hardware model for MCDNN (MNIST)\n",
      "\n",
      "==== System: Dropout (ImageNet) ====\n",
      "Estimated the value of NVIDIA GeForce GTX 580 server, available from 2011-02-07 00:00:00 and used from 2012-03-31 00:00:00: 565.8813300261663 per chip\n",
      "\n",
      "==== System: Dropout (CIFAR) ====\n",
      "Estimated the value of NVIDIA GeForce GTX 580 server, available from 2011-02-07 00:00:00 and used from 2012-04-03 23:00:00: 564.3850072096027 per chip\n",
      "\n",
      "==== System: Dropout (MNIST) ====\n",
      "No training time found, assuming 33.0625\n",
      "\n",
      "Estimated the value of NVIDIA GeForce GTX 580 server, available from 2011-02-07 00:00:00 and used from 2012-03-01 22:30:00: 581.0644943602935 per chip\n",
      "\n",
      "==== System: LBL ====\n",
      "Could not find hardware model for LBL\n",
      "\n",
      "==== System: Unsupervised High-level Feature Learner ====\n",
      "Could not find hardware model for Unsupervised High-level Feature Learner\n",
      "\n",
      "==== System: AlexNet ====\n",
      "No training time found, assuming 33.0625\n",
      "\n",
      "Estimated the value of NVIDIA GeForce GTX 580 server, available from 2011-02-07 00:00:00 and used from 2012-06-28 22:30:00: 523.1325518725249 per chip\n",
      "\n",
      "==== System: RNN+weight noise+dynamic eval ====\n",
      "Could not find hardware model for RNN+weight noise+dynamic eval\n",
      "\n",
      "==== System: Mitosis ====\n",
      "Could not find hardware model for Mitosis\n",
      "\n",
      "==== System: RNTN ====\n",
      "Could not find hardware model for RNTN\n",
      "\n",
      "==== System: Word2Vec (large) ====\n",
      "Could not find hardware model for Word2Vec (large)\n",
      "\n",
      "==== System: Visualizing CNNs ====\n",
      "No training time found, assuming 33.0625\n",
      "\n",
      "Estimated the value of NVIDIA GeForce GTX 580 server, available from 2011-02-07 00:00:00 and used from 2013-08-10 22:30:00: 364.9435630174399 per chip\n",
      "\n",
      "==== System: TransE ====\n",
      "Could not find hardware model for TransE\n",
      "\n",
      "==== System: SPN-4+KN5 ====\n",
      "Could not find hardware model for SPN-4+KN5\n",
      "\n",
      "==== System: GANs ====\n",
      "Could not find hardware model for GANs\n",
      "\n",
      "==== System: SPPNet ====\n",
      "Estimated the value of NVIDIA GeForce GTX TITAN server, available from 2013-05-20 00:00:00 and used from 2014-03-22 00:00:00: 1250.6039859904286 per chip\n",
      "\n",
      "==== System: SmooCT ====\n",
      "Could not find hardware model for SmooCT\n",
      "\n",
      "==== System: RNNsearch-50* ====\n",
      "Could not find hardware model for RNNsearch-50*\n",
      "\n",
      "==== System: VGG16 ====\n",
      "No training time found, assuming 33.0625\n",
      "\n",
      "Estimated the value of NVIDIA GTX Titan Black server, available from 2014-05-02 00:00:00 and used from 2014-06-02 22:30:00: 1594.1422195144478 per chip\n",
      "\n",
      "==== System: Seq2Seq LSTM ====\n",
      "Could not find hardware model for Seq2Seq LSTM\n",
      "\n",
      "==== System: MSRA (C, PReLU) ====\n",
      "Could not find hardware model for MSRA (C, PReLU)\n",
      "\n",
      "==== System: GoogLeNet / InceptionV1 ====\n",
      "Could not find hardware model for GoogLeNet / InceptionV1\n",
      "\n",
      "==== System: AlphaGo Fan ====\n",
      "Could not find hardware model for AlphaGo Fan\n",
      "\n",
      "==== System: DeepSpeech2 (English) ====\n",
      "Estimated the value of NVIDIA GTX Titan X server, available from 2015-06-15 00:00:00 and used from 2015-10-04 00:00:00: 1485.4674335348793 per chip\n",
      "\n",
      "==== System: ResNet-152 (ImageNet) ====\n",
      "Could not find hardware model for ResNet-152 (ImageNet)\n",
      "\n",
      "==== System: AlphaGo Lee ====\n",
      "Could not find hardware model for AlphaGo Lee\n",
      "\n",
      "==== System: GNMT ====\n",
      "No training time found, assuming 33.0625\n",
      "\n",
      "Estimated the value of NVIDIA Tesla K80 server, available from 2015-02-15 00:00:00 and used from 2016-06-24 22:30:00: 5297.619876078356 per chip\n",
      "\n",
      "==== System: Xception ====\n",
      "Estimated the value of NVIDIA Tesla K80 server, available from 2015-02-15 00:00:00 and used from 2016-07-09 00:00:00: 5227.948565540916 per chip\n",
      "\n",
      "==== System: NASv3 (CIFAR-10) ====\n",
      "Could not find hardware model for NASv3 (CIFAR-10)\n",
      "\n",
      "==== System: PolyNet ====\n",
      "No training time found, assuming 33.0625\n",
      "\n",
      "Estimated the value of NVIDIA GTX Titan X server, available from 2015-06-15 00:00:00 and used from 2016-08-15 22:30:00: 1123.93350055616 per chip\n",
      "\n",
      "==== System: AlphaGo Master ====\n",
      "No training time found, assuming 33.0625\n",
      "\n",
      "Estimated the value of Google TPU v1 server, available from 2015-05-15 00:00:00 and used from 2016-09-29 22:30:00: 5747.455218482905 per chip\n",
      "\n",
      "==== System: Libratus ====\n",
      "Could not find hardware model for Libratus\n",
      "\n",
      "==== System: MoE ====\n",
      "Estimated the value of NVIDIA Tesla K40t server, available from 2014-02-20 00:00:00 and used from 2016-11-12 00:00:00: 3676.8142833011802 per chip\n",
      "\n",
      "==== System: JFT ====\n",
      "Estimated the value of NVIDIA Tesla K80 server, available from 2015-02-15 00:00:00 and used from 2017-03-12 00:00:00: 4207.651141617478 per chip\n",
      "\n",
      "==== System: OpenAI TI7 DOTA 1v1 ====\n",
      "Could not find hardware model for OpenAI TI7 DOTA 1v1\n",
      "\n",
      "==== System: AlphaGo Zero ====\n",
      "Estimated the value of Google TPU v1 server, available from 2015-05-15 00:00:00 and used from 2017-07-30 00:00:00: 4394.937128000002 per chip\n",
      "\n",
      "==== System: AlphaZero ====\n",
      "No training time found, assuming 33.0625\n",
      "\n",
      "Estimated the value of Google TPU v2 server, available from 2017-05-01 00:00:00 and used from 2017-09-02 22:30:00: 7429.335926820838 per chip\n",
      "\n",
      "==== System: ResNeXt-101 32x48d ====\n",
      "Could not find hardware model for ResNeXt-101 32x48d\n",
      "\n",
      "==== System: FTW ====\n",
      "Could not find hardware model for FTW\n",
      "\n",
      "==== System: BigGAN-deep 512x512 ====\n",
      "Estimated the value of Google TPU v3 server, available from 2018-05-18 00:00:00 and used from 2018-07-28 00:00:00: 8127.077441672149 per chip\n",
      "\n",
      "==== System: GPT-2 (1.5B) ====\n",
      "Could not find hardware model for GPT-2 (1.5B)\n",
      "\n",
      "==== System: XLNet ====\n",
      "Could not find hardware model for XLNet\n",
      "\n",
      "==== System: RoBERTa Large ====\n",
      "Estimated the value of NVIDIA Tesla V100 DGXS 32 GB server, available from 2018-06-25 00:00:00 and used from 2019-04-27 00:00:00: 14216.960398857207 per chip\n",
      "\n",
      "==== System: Megatron-LM (8.3B) ====\n",
      "Estimated the value of NVIDIA Tesla V100 DGXS 32 GB server, available from 2018-06-25 00:00:00 and used from 2019-07-05 09:00:00: 13377.011951877601 per chip\n",
      "\n",
      "==== System: Megatron-BERT ====\n",
      "Estimated the value of NVIDIA Tesla V100S PCIe 32 GB server, available from 2020-02-24 00:00:00 and used from 2020-02-24 00:00:00: 18624.999999999993 per chip\n",
      "\n",
      "==== System: T5-11B ====\n",
      "Estimated the value of Google TPU v3 server, available from 2018-05-18 00:00:00 and used from 2019-08-03 23:00:00: 5857.744544274361 per chip\n",
      "\n",
      "==== System: AlphaStar ====\n",
      "Estimated the value of Google TPU v3 server, available from 2018-05-18 00:00:00 and used from 2019-07-18 00:00:00: 5941.0500896577005 per chip\n",
      "\n",
      "==== System: OpenAI Five Rerun ====\n",
      "Could not find hardware model for OpenAI Five Rerun\n",
      "\n",
      "==== System: OpenAI Five ====\n",
      "Could not find hardware model for OpenAI Five\n",
      "\n",
      "==== System: Meena ====\n",
      "Estimated the value of Google TPU v3 server, available from 2018-05-18 00:00:00 and used from 2019-10-30 00:00:00: 5420.01023334974 per chip\n",
      "\n",
      "==== System: Turing-NLG ====\n",
      "No training time found, assuming 33.0625\n",
      "\n",
      "Estimated the value of NVIDIA Tesla V100 DGXS 32 GB server, available from 2018-06-25 00:00:00 and used from 2019-11-11 22:30:00: 11937.503993561211 per chip\n",
      "\n",
      "==== System: GPT-3 175B (davinci) ====\n",
      "Estimated the value of NVIDIA Tesla V100 DGXS 32 GB server, available from 2018-06-25 00:00:00 and used from 2019-10-01 00:00:00: 12377.381963794704 per chip\n",
      "\n",
      "==== System: iGPT-XL ====\n",
      "No training time found, assuming 33.0625\n",
      "\n",
      "Estimated the value of NVIDIA Tesla V100 DGXS 32 GB server, available from 2018-06-25 00:00:00 and used from 2020-03-15 22:30:00: 10690.576881599849 per chip\n",
      "\n",
      "==== System: mT5-XXL ====\n",
      "Could not find hardware model for mT5-XXL\n",
      "\n",
      "==== System: DALL-E ====\n",
      "No training time found, assuming 33.0625\n",
      "\n",
      "Estimated the value of NVIDIA Tesla V100 DGXS 16 GB server, available from 2018-06-25 00:00:00 and used from 2020-10-03 22:30:00: 8944.88416414377 per chip\n",
      "\n",
      "==== System: Switch ====\n",
      "Estimated the value of Google TPU v3 server, available from 2018-05-18 00:00:00 and used from 2020-10-16 00:00:00: 3972.6364195933243 per chip\n",
      "\n",
      "==== System: Meta Pseudo Labels ====\n",
      "Estimated the value of Google TPU v3 server, available from 2018-05-18 00:00:00 and used from 2020-12-20 00:00:00: 3751.149538874217 per chip\n",
      "\n",
      "==== System: PanGu-α ====\n",
      "Could not find hardware model for PanGu-α\n",
      "\n",
      "==== System: ProtT5-XXL ====\n",
      "No training time found, assuming 33.0625\n",
      "\n",
      "Estimated the value of Google TPU v3 server, available from 2018-05-18 00:00:00 and used from 2021-01-30 22:30:00: 3617.8379831648695 per chip\n",
      "\n",
      "==== System: ByT5-XXL ====\n",
      "No training time found, assuming 33.0625\n",
      "\n",
      "Estimated the value of Google TPU v3 server, available from 2018-05-18 00:00:00 and used from 2021-02-23 22:30:00: 3542.0112431121242 per chip\n",
      "\n",
      "==== System: GOAT ====\n",
      "No training time found, assuming 33.0625\n",
      "\n",
      "Estimated the value of Google TPU v3 server, available from 2018-05-18 00:00:00 and used from 2021-04-24 22:30:00: 3359.324743375384 per chip\n",
      "\n",
      "==== System: Jurassic-1-Jumbo ====\n",
      "Could not find hardware model for Jurassic-1-Jumbo\n",
      "\n",
      "==== System: HyperCLOVA 82B ====\n",
      "Estimated the value of NVIDIA A100 server, available from 2020-05-30 00:00:00 and used from 2021-06-15 05:00:00: 17771.583581670657 per chip\n",
      "\n",
      "==== System: Megatron-Turing NLG 530B ====\n",
      "Estimated the value of NVIDIA A100 SXM4 80 GB server, available from 2021-02-14 00:00:00 and used from 2021-07-10 22:00:00: 21867.615010381713 per chip\n",
      "\n",
      "==== System: Yuan 1.0 ====\n",
      "Could not find hardware model for Yuan 1.0\n",
      "\n",
      "==== System: Gopher (280B) ====\n",
      "Estimated the value of Google TPU v3 server, available from 2018-05-18 00:00:00 and used from 2021-08-31 16:00:00: 2997.825873519083 per chip\n",
      "\n",
      "==== System: GLaM ====\n",
      "Estimated the value of Google TPU v4 server, available from 2021-05-20 00:00:00 and used from 2021-08-18 02:00:00: 7826.820785365009 per chip\n",
      "\n",
      "==== System: EXAONE 1.0 ====\n",
      "Could not find hardware model for EXAONE 1.0\n",
      "\n",
      "==== System: ERNIE 3.0 Titan ====\n",
      "Skipping Huawei Ascend 910,NVIDIA Tesla V100 DGXS 32 GB\n",
      "\n",
      "==== System: LaMDA ====\n",
      "Estimated the value of Google TPU v3 server, available from 2018-05-18 00:00:00 and used from 2021-10-15 07:00:00: 2881.0974204489626 per chip\n",
      "\n",
      "==== System: Chinchilla ====\n",
      "Skipping Google TPU v4,Google TPU v3\n",
      "\n",
      "==== System: PaLM (540B) ====\n",
      "Estimated the value of Google TPU v4 server, available from 2021-05-20 00:00:00 and used from 2021-12-08 00:00:00: 7090.157608923292 per chip\n",
      "\n",
      "==== System: OPT-175B ====\n",
      "Estimated the value of NVIDIA A100 SXM4 80 GB server, available from 2021-02-14 00:00:00 and used from 2022-01-28 23:00:00: 18296.79402527096 per chip\n",
      "\n",
      "==== System: BIG-G 137B ====\n",
      "Could not find hardware model for BIG-G 137B\n",
      "\n",
      "==== System: Parti ====\n",
      "No training time found, assuming 33.0625\n",
      "\n",
      "Estimated the value of Google TPU v4 server, available from 2021-05-20 00:00:00 and used from 2022-03-20 22:30:00: 6479.766855851569 per chip\n",
      "\n",
      "==== System: Minerva (540B) ====\n",
      "Estimated the value of Google TPU v4 server, available from 2021-05-20 00:00:00 and used from 2022-04-01 00:00:00: 6411.502128342827 per chip\n",
      "\n",
      "==== System: BlenderBot 3 ====\n",
      "No training time found, assuming 33.0625\n",
      "\n",
      "Estimated the value of NVIDIA A100 SXM4 40 GB server, available from 2020-08-12 00:00:00 and used from 2022-05-08 22:30:00: 14215.15022168555 per chip\n",
      "\n",
      "==== System: U-PaLM (540B) ====\n",
      "Estimated the value of Google TPU v4 server, available from 2021-05-20 00:00:00 and used from 2022-08-16 00:00:00: 5681.301176704267 per chip\n",
      "\n",
      "==== System: Flan-PaLM 540B ====\n",
      "Estimated the value of Google TPU v4 server, available from 2021-05-20 00:00:00 and used from 2022-08-19 11:00:00: 5666.278485324646 per chip\n",
      "\n",
      "==== System: BLOOM-176B ====\n",
      "Estimated the value of NVIDIA A100 SXM4 80 GB server, available from 2021-02-14 00:00:00 and used from 2022-05-15 00:00:00: 16647.997204491367 per chip\n",
      "\n",
      "==== System: GPT-3.5 (text-davinci-003) ====\n",
      "Estimated the value of NVIDIA A100 SXM4 40 GB server, available from 2020-08-12 00:00:00 and used from 2022-03-14 00:00:00: 14922.201653959262 per chip\n",
      "\n",
      "==== System: GPT-4 ====\n",
      "Estimated the value of NVIDIA A100 SXM4 40 GB server, available from 2020-08-12 00:00:00 and used from 2022-05-12 00:00:00: 14165.054738257799 per chip\n",
      "\n",
      "==== System: PaLM 2 ====\n",
      "No training time found, assuming 33.0625\n",
      "\n",
      "Estimated the value of Google TPU v4 server, available from 2021-05-20 00:00:00 and used from 2023-02-05 22:30:00: 4876.823820498262 per chip\n",
      "\n",
      "==== System: Claude 2 ====\n",
      "Could not find hardware model for Claude 2\n",
      "\n",
      "==== System: Falcon-180B ====\n",
      "Estimated the value of NVIDIA A100 SXM4 40 GB server, available from 2020-08-12 00:00:00 and used from 2023-01-09 00:00:00: 11440.891643957973 per chip\n",
      "\n",
      "==== System: Amazon Titan ====\n",
      "Estimated the value of NVIDIA A100 server, available from 2020-05-30 00:00:00 and used from 2023-06-12 00:00:00: 9355.514480669957 per chip\n",
      "\n",
      "==== System: Grok-1 ====\n",
      "Could not find hardware model for Grok-1\n",
      "\n",
      "==== System: Inflection-2 ====\n",
      "No training time found, assuming 33.0625\n",
      "\n",
      "Estimated the value of NVIDIA H100 SXM5 server, available from 2022-12-19 00:00:00 and used from 2023-08-20 22:30:00: 35870.40262914087 per chip\n",
      "\n",
      "==== System: Gemini 1.0 Ultra ====\n",
      "Estimated the value of Google TPU v4 server, available from 2021-05-20 00:00:00 and used from 2023-05-10 00:00:00: 4488.559125020449 per chip\n",
      "\n",
      "==== System: MegaScale (Production) ====\n",
      "Estimated the value of NVIDIA A100 server, available from 2020-05-30 00:00:00 and used from 2023-12-04 00:00:00: 8016.602153313497 per chip\n",
      "\n",
      "==== System: Mistral Large ====\n",
      "Estimated the value of NVIDIA H100 PCIe server, available from 2022-12-19 00:00:00 and used from 2023-09-14 20:00:00: 35087.60891306817 per chip\n",
      "\n",
      "==== System: Inflection-2.5 ====\n",
      "No training time found, assuming 33.0625\n",
      "\n",
      "Estimated the value of NVIDIA H100 SXM5 server, available from 2022-12-19 00:00:00 and used from 2023-12-04 22:30:00: 32666.796403283115 per chip\n",
      "\n",
      "==== System: Llama 3-70B ====\n",
      "No training time found, assuming 33.0625\n",
      "\n",
      "Estimated the value of NVIDIA H100 SXM5 server, available from 2022-12-19 00:00:00 and used from 2024-01-15 22:30:00: 31478.061762024605 per chip\n",
      "\n",
      "==== System: 6-layer MLP (MNIST) ====\n",
      "Unable to estimate cost\n",
      "==== System: Feedforward NN ====\n",
      "Unable to estimate cost\n",
      "==== System: RNN 500/10 + RT09 LM (NIST RT05) ====\n",
      "Unable to estimate cost\n",
      "==== System: KN5 LM + RNN 400/10 (WSJ) ====\n",
      "Unable to estimate cost\n",
      "==== System: MCDNN (MNIST) ====\n",
      "Unable to estimate cost\n",
      "==== System: Dropout (ImageNet) ====\n",
      "Imputing training time from compute and hardware\n",
      "Found NVIDIA GeForce GTX 580 at 1581000000000.0 FLOP/s\n",
      "Overall cost per chip-hour for NVIDIA GeForce GTX 580: 0.05315481939234433\n",
      "Estimated cost: 6.803816882220074\n",
      "==== System: Dropout (CIFAR) ====\n",
      "Imputing training time from compute and hardware\n",
      "Found NVIDIA GeForce GTX 580 at 1581000000000.0 FLOP/s\n",
      "Overall cost per chip-hour for NVIDIA GeForce GTX 580: 0.05308688611962092\n",
      "Estimated cost: 0.10617377223924183\n",
      "==== System: Dropout (MNIST) ====\n",
      "Imputing training time from compute and hardware\n",
      "Found NVIDIA GeForce GTX 580 at 1581000000000.0 FLOP/s\n",
      "Overall cost per chip-hour for NVIDIA GeForce GTX 580: 0.05384413725350173\n",
      "Estimated cost: 0.1523577249654417\n",
      "==== System: LBL ====\n",
      "Unable to estimate cost\n",
      "==== System: Unsupervised High-level Feature Learner ====\n",
      "Unable to estimate cost\n",
      "==== System: AlexNet ====\n",
      "Imputing training time from compute and hardware\n",
      "Found NVIDIA GeForce GTX 580 at 1581000000000.0 FLOP/s\n",
      "Overall cost per chip-hour for NVIDIA GeForce GTX 580: 0.05121401866889263\n",
      "Estimated cost: 11.27771395243495\n",
      "==== System: RNN+weight noise+dynamic eval ====\n",
      "Unable to estimate cost\n",
      "==== System: Mitosis ====\n",
      "Unable to estimate cost\n",
      "==== System: RNTN ====\n",
      "Unable to estimate cost\n",
      "==== System: Word2Vec (large) ====\n",
      "Unable to estimate cost\n",
      "==== System: Visualizing CNNs ====\n",
      "Imputing training time from compute and hardware\n",
      "Found NVIDIA GeForce GTX 580 at 1581000000000.0 FLOP/s\n",
      "Overall cost per chip-hour for NVIDIA GeForce GTX 580: 0.04493806564966673\n",
      "Estimated cost: 11.201092100931291\n",
      "==== System: TransE ====\n",
      "Unable to estimate cost\n",
      "==== System: SPN-4+KN5 ====\n",
      "Unable to estimate cost\n",
      "==== System: GANs ====\n",
      "Unable to estimate cost\n",
      "==== System: SPPNet ====\n",
      "Imputing training time from compute and hardware\n",
      "Found NVIDIA GeForce GTX TITAN at 4709000000000.0 FLOP/s\n",
      "Overall cost per chip-hour for NVIDIA GeForce GTX TITAN: 0.08313635220510635\n",
      "Estimated cost: 44.60868206491533\n",
      "==== System: SmooCT ====\n",
      "Unable to estimate cost\n",
      "==== System: RNNsearch-50* ====\n",
      "Unable to estimate cost\n",
      "==== System: VGG16 ====\n",
      "Imputing training time from compute and hardware\n",
      "Found NVIDIA GTX Titan Black at 5650000000000.0 FLOP/s\n",
      "Overall cost per chip-hour for NVIDIA GTX Titan Black: 0.1023274127936496\n",
      "Estimated cost: 124.14035721288352\n",
      "==== System: Seq2Seq LSTM ====\n",
      "Unable to estimate cost\n",
      "==== System: MSRA (C, PReLU) ====\n",
      "Unable to estimate cost\n",
      "==== System: GoogLeNet / InceptionV1 ====\n",
      "Unable to estimate cost\n",
      "==== System: AlphaGo Fan ====\n",
      "Unable to estimate cost\n",
      "==== System: DeepSpeech2 (English) ====\n",
      "Overall cost per chip-hour for NVIDIA GTX Titan X: 0.09659199926071341\n",
      "Estimated cost: 185.45663858056975\n",
      "==== System: ResNet-152 (ImageNet) ====\n",
      "Unable to estimate cost\n",
      "==== System: AlphaGo Lee ====\n",
      "Unable to estimate cost\n",
      "==== System: GNMT ====\n",
      "Overall cost per chip-hour for NVIDIA Tesla K80: 0.2706285101203272\n",
      "Estimated cost: 177459.23294120215\n",
      "==== System: Xception ====\n",
      "Overall cost per chip-hour for NVIDIA Tesla K80: 0.2674654225249599\n",
      "Estimated cost: 11554.506253078269\n",
      "==== System: NASv3 (CIFAR-10) ====\n",
      "Unable to estimate cost\n",
      "==== System: PolyNet ====\n",
      "Imputing training time from compute and hardware\n",
      "Found NVIDIA GTX Titan X at 6690000000000.0 FLOP/s\n",
      "Overall cost per chip-hour for NVIDIA GTX Titan X: 0.07954549380893802\n",
      "Estimated cost: 563.683951034937\n",
      "==== System: AlphaGo Master ====\n",
      "Imputing training time from compute and hardware\n",
      "Overall cost per chip-hour for Google TPU v1: 0.26808649225693054\n",
      "Estimated cost: 431722.8608398466\n",
      "==== System: Libratus ====\n",
      "Unable to estimate cost\n",
      "==== System: MoE ====\n",
      "Overall cost per chip-hour for NVIDIA Tesla K40t: 0.19195906131641663\n",
      "Estimated cost: 3538.1894181841913\n",
      "==== System: JFT ====\n",
      "Overall cost per chip-hour for NVIDIA Tesla K80: 0.2216783716143869\n",
      "Estimated cost: 15960.842756235856\n",
      "==== System: OpenAI TI7 DOTA 1v1 ====\n",
      "Unable to estimate cost\n",
      "==== System: AlphaGo Zero ====\n",
      "Imputing training time from compute and hardware\n",
      "Overall cost per chip-hour for Google TPU v1: 0.20668197493817955\n",
      "Estimated cost: 567460.1727368698\n",
      "==== System: AlphaZero ====\n",
      "Imputing training time from compute and hardware\n",
      "Found Google TPU v2 at 45000000000000.0 FLOP/s\n",
      "Overall cost per chip-hour for Google TPU v2: 0.3522363978384814\n",
      "Estimated cost: 212671.1933910081\n",
      "==== System: ResNeXt-101 32x48d ====\n",
      "Unable to estimate cost\n",
      "==== System: FTW ====\n",
      "Unable to estimate cost\n",
      "==== System: BigGAN-deep 512x512 ====\n",
      "Overall cost per chip-hour for Google TPU v3: 0.39062244705144605\n",
      "Estimated cost: 4799.968629368169\n",
      "==== System: GPT-2 (1.5B) ====\n",
      "Unable to estimate cost\n",
      "==== System: XLNet ====\n",
      "Unable to estimate cost\n",
      "==== System: RoBERTa Large ====\n",
      "Overall cost per chip-hour for NVIDIA Tesla V100 DGXS 32 GB: 0.6735433124710711\n",
      "Estimated cost: 82765.00223644522\n",
      "==== System: Megatron-LM (8.3B) ====\n",
      "Overall cost per chip-hour for NVIDIA Tesla V100 DGXS 32 GB: 0.6392401562504425\n",
      "Estimated cost: 107024.14392007409\n",
      "==== System: Megatron-BERT ====\n",
      "Overall cost per chip-hour for NVIDIA Tesla V100S PCIe 32 GB: 0.8721792616887812\n",
      "Estimated cost: 621605.6485226412\n",
      "==== System: T5-11B ====\n",
      "Overall cost per chip-hour for Google TPU v3: 0.28725022618131013\n",
      "Estimated cost: 70874.1100563932\n",
      "==== System: AlphaStar ====\n",
      "Overall cost per chip-hour for Google TPU v3: 0.29103231001459395\n",
      "Estimated cost: 118014.76584015791\n",
      "==== System: OpenAI Five Rerun ====\n",
      "Unable to estimate cost\n",
      "==== System: OpenAI Five ====\n",
      "Unable to estimate cost\n",
      "==== System: Meena ====\n",
      "Overall cost per chip-hour for Google TPU v3: 0.266938979727612\n",
      "Estimated cost: 196808.7709735738\n",
      "==== System: Turing-NLG ====\n",
      "Imputing training time from compute and hardware\n",
      "Found NVIDIA Tesla V100 DGXS 32 GB at 125000000000000.0 FLOP/s\n",
      "Overall cost per chip-hour for NVIDIA Tesla V100 DGXS 32 GB: 0.5694781613252284\n",
      "Estimated cost: 52982.56078699903\n",
      "==== System: GPT-3 175B (davinci) ====\n",
      "Overall cost per chip-hour for NVIDIA Tesla V100 DGXS 32 GB: 0.5932005599222147\n",
      "Estimated cost: 2107048.3888437063\n",
      "==== System: iGPT-XL ====\n",
      "Imputing training time from compute and hardware\n",
      "Found NVIDIA Tesla V100 DGXS 32 GB at 125000000000000.0 FLOP/s\n",
      "Overall cost per chip-hour for NVIDIA Tesla V100 DGXS 32 GB: 0.5166193647339049\n",
      "Estimated cost: 101027.78688129697\n",
      "==== System: mT5-XXL ====\n",
      "Unable to estimate cost\n",
      "==== System: DALL-E ====\n",
      "Imputing training time from compute and hardware\n",
      "Found NVIDIA Tesla V100 DGXS 16 GB at 125000000000000.0 FLOP/s\n",
      "Overall cost per chip-hour for NVIDIA Tesla V100 DGXS 16 GB: 0.43975528737056946\n",
      "Estimated cost: 122479.99114913639\n",
      "==== System: Switch ====\n",
      "Overall cost per chip-hour for Google TPU v3: 0.2028237352754782\n",
      "Estimated cost: 134584.0951895141\n",
      "==== System: Meta Pseudo Labels ====\n",
      "Overall cost per chip-hour for Google TPU v3: 0.1927681987923124\n",
      "Estimated cost: 52112.18378871857\n",
      "==== System: PanGu-α ====\n",
      "Unable to estimate cost\n",
      "==== System: ProtT5-XXL ====\n",
      "Imputing training time from compute and hardware\n",
      "Found Google TPU v3 at 123000000000000.0 FLOP/s\n",
      "Overall cost per chip-hour for Google TPU v3: 0.18671583484321247\n",
      "Estimated cost: 82872.36993643337\n",
      "==== System: ByT5-XXL ====\n",
      "Imputing training time from compute and hardware\n",
      "Found Google TPU v3 at 123000000000000.0 FLOP/s\n",
      "Overall cost per chip-hour for Google TPU v3: 0.1832732898557877\n",
      "Estimated cost: 89401.60480770131\n",
      "==== System: GOAT ====\n",
      "Imputing training time from compute and hardware\n",
      "Found Google TPU v3 at 123000000000000.0 FLOP/s\n",
      "Overall cost per chip-hour for Google TPU v3: 0.17497929629228456\n",
      "Estimated cost: 82194.43005599637\n",
      "==== System: Jurassic-1-Jumbo ====\n",
      "Unable to estimate cost\n",
      "==== System: HyperCLOVA 82B ====\n",
      "Overall cost per chip-hour for NVIDIA A100: 0.8615238763670576\n",
      "Estimated cost: 567431.5013587697\n",
      "==== System: Megatron-Turing NLG 530B ====\n",
      "Overall cost per chip-hour for NVIDIA A100 SXM4 80 GB: 1.0409213280892418\n",
      "Estimated cost: 3590762.2133766487\n",
      "==== System: Yuan 1.0 ====\n",
      "Unable to estimate cost\n",
      "==== System: Gopher (280B) ====\n",
      "Overall cost per chip-hour for Google TPU v3: 0.15856719521134846\n",
      "Estimated cost: 597531.9330588286\n",
      "==== System: GLaM ====\n",
      "Overall cost per chip-hour for Google TPU v4: 0.3751002573730403\n",
      "Estimated cost: 524684.2384092908\n",
      "==== System: EXAONE 1.0 ====\n",
      "Unable to estimate cost\n",
      "==== System: ERNIE 3.0 Titan ====\n",
      "Unable to estimate cost\n",
      "==== System: LaMDA ====\n",
      "Overall cost per chip-hour for Google TPU v3: 0.1568346468253405\n",
      "Estimated cost: 222429.16951357093\n",
      "==== System: Chinchilla ====\n",
      "Unable to estimate cost\n",
      "==== System: PaLM (540B) ====\n",
      "Overall cost per chip-hour for Google TPU v4: 0.34479325573923164\n",
      "Estimated cost: 2897984.556142196\n",
      "==== System: OPT-175B ====\n",
      "Overall cost per chip-hour for NVIDIA A100 SXM4 80 GB: 0.8864471003715254\n",
      "Estimated cost: 720277.2727242807\n",
      "==== System: BIG-G 137B ====\n",
      "Unable to estimate cost\n",
      "==== System: Parti ====\n",
      "Imputing training time from compute and hardware\n",
      "Found Google TPU v4 at 275000000000000.0 FLOP/s\n",
      "Overall cost per chip-hour for Google TPU v4: 0.3170814270901849\n",
      "Estimated cost: 338467.4804827625\n",
      "==== System: Minerva (540B) ====\n",
      "Overall cost per chip-hour for Google TPU v4: 0.3139821985681666\n",
      "Skipping Minerva (540B) because it is a fine-tuned version of a base model\n",
      "Unable to estimate cost\n",
      "==== System: BlenderBot 3 ====\n",
      "Imputing training time from compute and hardware\n",
      "Found NVIDIA A100 SXM4 40 GB at 312000000000000.0 FLOP/s\n",
      "Overall cost per chip-hour for NVIDIA A100 SXM4 40 GB: 0.7011398801651415\n",
      "Skipping BlenderBot 3 because it is a fine-tuned version of a base model\n",
      "Unable to estimate cost\n",
      "==== System: U-PaLM (540B) ====\n",
      "Overall cost per chip-hour for Google TPU v4: 0.28083096954094555\n",
      "Skipping U-PaLM (540B) because it is a fine-tuned version of a base model\n",
      "Unable to estimate cost\n",
      "==== System: Flan-PaLM 540B ====\n",
      "Overall cost per chip-hour for Google TPU v4: 0.28014893717517897\n",
      "Skipping Flan-PaLM 540B because it is a fine-tuned version of a base model\n",
      "Unable to estimate cost\n",
      "==== System: BLOOM-176B ====\n",
      "Overall cost per chip-hour for NVIDIA A100 SXM4 80 GB: 0.8191964857597356\n",
      "Estimated cost: 883316.6330931216\n",
      "==== System: GPT-3.5 (text-davinci-003) ====\n",
      "Imputing training time from compute and hardware\n",
      "Found NVIDIA A100 SXM4 40 GB at 312000000000000.0 FLOP/s\n",
      "Overall cost per chip-hour for NVIDIA A100 SXM4 40 GB: 0.7408451176583034\n",
      "Estimated cost: 4534422.3963036705\n",
      "==== System: GPT-4 ====\n",
      "Overall cost per chip-hour for NVIDIA A100 SXM4 40 GB: 0.7044900692075396\n",
      "Estimated cost: 40155933.94482976\n",
      "==== System: PaLM 2 ====\n",
      "Imputing training time from compute and hardware\n",
      "Found Google TPU v4 at 275000000000000.0 FLOP/s\n",
      "Overall cost per chip-hour for Google TPU v4: 0.24359198495801143\n",
      "Estimated cost: 4816067.79688028\n",
      "==== System: Claude 2 ====\n",
      "Unable to estimate cost\n",
      "==== System: Falcon-180B ====\n",
      "Overall cost per chip-hour for NVIDIA A100 SXM4 40 GB: 0.5808126699327456\n",
      "Estimated cost: 10277317.566912353\n",
      "==== System: Amazon Titan ====\n",
      "Overall cost per chip-hour for NVIDIA A100: 0.4787689007505894\n",
      "Estimated cost: 7589214.8056259835\n",
      "==== System: Grok-1 ====\n",
      "Unable to estimate cost\n",
      "==== System: Inflection-2 ====\n",
      "Imputing training time from compute and hardware\n",
      "Found NVIDIA H100 SXM5 at 989000000000000.0 FLOP/s\n",
      "Overall cost per chip-hour for NVIDIA H100 SXM5: 1.7248636653053238\n",
      "Estimated cost: 12931794.39741324\n",
      "==== System: Gemini 1.0 Ultra ====\n",
      "Overall cost per chip-hour for Google TPU v4: 0.22596471151487793\n",
      "Estimated cost: 29827341.919963885\n",
      "==== System: MegaScale (Production) ====\n",
      "Overall cost per chip-hour for NVIDIA A100: 0.42534943079956544\n",
      "Estimated cost: 2634253.6780551905\n",
      "==== System: Mistral Large ====\n",
      "Imputing training time from compute and hardware\n",
      "Found NVIDIA H100 PCIe at 756000000000000.0 FLOP/s\n",
      "Overall cost per chip-hour for NVIDIA H100 PCIe: 1.6411536234008974\n",
      "Estimated cost: 32160564.834077112\n",
      "==== System: Inflection-2.5 ====\n",
      "Imputing training time from compute and hardware\n",
      "Found NVIDIA H100 SXM5 at 989000000000000.0 FLOP/s\n",
      "Overall cost per chip-hour for NVIDIA H100 SXM5: 1.5794194783755164\n",
      "Estimated cost: 11830711.30826764\n",
      "==== System: Llama 3-70B ====\n",
      "Overall cost per chip-hour for NVIDIA H100 SXM5: 1.5138896908875232\n",
      "Estimated cost: 9688894.021680148\n",
      "All costs:\n",
      "{'Dropout (ImageNet)': 6.803816882220074, 'Dropout (CIFAR)': 0.10617377223924183, 'Dropout (MNIST)': 0.1523577249654417, 'AlexNet': 11.27771395243495, 'Visualizing CNNs': 11.201092100931291, 'SPPNet': 44.60868206491533, 'VGG16': 124.14035721288352, 'DeepSpeech2 (English)': 185.45663858056975, 'GNMT': 177459.23294120215, 'Xception': 11554.506253078269, 'PolyNet': 563.683951034937, 'AlphaGo Master': 431722.8608398466, 'MoE': 3538.1894181841913, 'JFT': 15960.842756235856, 'AlphaGo Zero': 567460.1727368698, 'AlphaZero': 212671.1933910081, 'BigGAN-deep 512x512': 4799.968629368169, 'RoBERTa Large': 82765.00223644522, 'Megatron-LM (8.3B)': 107024.14392007409, 'Megatron-BERT': 621605.6485226412, 'T5-11B': 70874.1100563932, 'AlphaStar': 118014.76584015791, 'Meena': 196808.7709735738, 'Turing-NLG': 52982.56078699903, 'GPT-3 175B (davinci)': 2107048.3888437063, 'iGPT-XL': 101027.78688129697, 'DALL-E': 122479.99114913639, 'Switch': 134584.0951895141, 'Meta Pseudo Labels': 52112.18378871857, 'ProtT5-XXL': 82872.36993643337, 'ByT5-XXL': 89401.60480770131, 'GOAT': 82194.43005599637, 'HyperCLOVA 82B': 567431.5013587697, 'Megatron-Turing NLG 530B': 3590762.2133766487, 'Gopher (280B)': 597531.9330588286, 'GLaM': 524684.2384092908, 'LaMDA': 222429.16951357093, 'PaLM (540B)': 2897984.556142196, 'OPT-175B': 720277.2727242807, 'Parti': 338467.4804827625, 'BLOOM-176B': 883316.6330931216, 'GPT-3.5 (text-davinci-003)': 4534422.3963036705, 'GPT-4': 40155933.94482976, 'PaLM 2': 4816067.79688028, 'Falcon-180B': 10277317.566912353, 'Amazon Titan': 7589214.8056259835, 'Inflection-2': 12931794.39741324, 'Gemini 1.0 Ultra': 29827341.919963885, 'MegaScale (Production)': 2634253.6780551905, 'Mistral Large': 32160564.834077112, 'Inflection-2.5': 11830711.30826764, 'Llama 3-70B': 9688894.021680148}\n"
     ]
    }
   ],
   "source": [
    "# Run the cost estimation pipeline\n",
    "\n",
    "# Hack: undo the column renaming for the cost pipeline\n",
    "df_filtered = df_filtered.rename(columns={'flop': 'Training compute (FLOP)', 'date': 'Publication date'})\n",
    "\n",
    "price_df = load_price_df()\n",
    "# Price date in datetime format\n",
    "price_df.dropna(subset=['Price date'], inplace=True)\n",
    "price_df['Price date'] = pd.to_datetime(price_df['Price date'])\n",
    "pcd_hardware_model_colname = 'Name of the hardware (from Training hardware)'\n",
    "\n",
    "hardware_df = load_hardware_df()\n",
    "\n",
    "cost_df = cost_estimation_function(df_filtered, hardware_df, price_df)\n",
    "cost_df = adjust_column_for_inflation(cost_df, 'Cost', 'data/PCU518210518210.csv', '2023-12-01')\n",
    "\n",
    "for kw in exclude_models_containing:\n",
    "    cost_df = cost_df[cost_df['System'].str.contains(kw) == False]\n",
    "\n",
    "cost_df = cost_df[cost_df['Publication date'] > cost_cutoff_date]\n",
    "\n",
    "df_filtered = cost_df\n",
    "# Redo the column renaming\n",
    "df_filtered = df_filtered.rename(columns={\n",
    "    'Training compute (FLOP)': 'flop',\n",
    "    'Publication date': 'date',\n",
    "    'Cost (inflation-adjusted)': 'cost',\n",
    "})\n",
    "df_filtered = df_filtered.assign(log_cost=lambda x: np.log10(x['cost']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>System</th>\n",
       "      <th>date</th>\n",
       "      <th>log_cost</th>\n",
       "      <th>log_flop</th>\n",
       "      <th>cost</th>\n",
       "      <th>flop</th>\n",
       "      <th>Organization</th>\n",
       "      <th>Notability criteria</th>\n",
       "      <th>Domain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1329</th>\n",
       "      <td>Dropout (ImageNet)</td>\n",
       "      <td>2012-06-03</td>\n",
       "      <td>0.883920</td>\n",
       "      <td>17.436476</td>\n",
       "      <td>7.654551e+00</td>\n",
       "      <td>2.731968e+17</td>\n",
       "      <td>University of Toronto</td>\n",
       "      <td>Highly cited</td>\n",
       "      <td>Vision</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1330</th>\n",
       "      <td>Dropout (CIFAR)</td>\n",
       "      <td>2012-06-03</td>\n",
       "      <td>-0.922816</td>\n",
       "      <td>15.630296</td>\n",
       "      <td>1.194495e-01</td>\n",
       "      <td>4.268700e+15</td>\n",
       "      <td>University of Toronto</td>\n",
       "      <td>Highly cited</td>\n",
       "      <td>Vision</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1331</th>\n",
       "      <td>Dropout (MNIST)</td>\n",
       "      <td>2012-06-03</td>\n",
       "      <td>-0.765968</td>\n",
       "      <td>15.780992</td>\n",
       "      <td>1.714082e-01</td>\n",
       "      <td>6.039371e+15</td>\n",
       "      <td>University of Toronto</td>\n",
       "      <td>Highly cited</td>\n",
       "      <td>Vision</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1322</th>\n",
       "      <td>AlexNet</td>\n",
       "      <td>2012-09-30</td>\n",
       "      <td>1.102568</td>\n",
       "      <td>17.672098</td>\n",
       "      <td>1.266392e+01</td>\n",
       "      <td>4.700000e+17</td>\n",
       "      <td>University of Toronto</td>\n",
       "      <td>Highly cited,Historical significance</td>\n",
       "      <td>Vision</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1302</th>\n",
       "      <td>Visualizing CNNs</td>\n",
       "      <td>2013-11-12</td>\n",
       "      <td>1.097156</td>\n",
       "      <td>17.725912</td>\n",
       "      <td>1.250708e+01</td>\n",
       "      <td>5.320000e+17</td>\n",
       "      <td>New York University (NYU)</td>\n",
       "      <td>Highly cited</td>\n",
       "      <td>Vision</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1280</th>\n",
       "      <td>SPPNet</td>\n",
       "      <td>2014-06-18</td>\n",
       "      <td>1.696501</td>\n",
       "      <td>18.532891</td>\n",
       "      <td>4.971654e+01</td>\n",
       "      <td>3.411072e+18</td>\n",
       "      <td>Microsoft,Xi’an Jiaotong University,University...</td>\n",
       "      <td>Highly cited</td>\n",
       "      <td>Vision</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1272</th>\n",
       "      <td>VGG16</td>\n",
       "      <td>2014-09-04</td>\n",
       "      <td>2.140588</td>\n",
       "      <td>18.966303</td>\n",
       "      <td>1.382255e+02</td>\n",
       "      <td>9.253440e+18</td>\n",
       "      <td>University of Oxford</td>\n",
       "      <td>Highly cited</td>\n",
       "      <td>Vision</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1228</th>\n",
       "      <td>DeepSpeech2 (English)</td>\n",
       "      <td>2015-12-08</td>\n",
       "      <td>2.314511</td>\n",
       "      <td>19.414973</td>\n",
       "      <td>2.063058e+02</td>\n",
       "      <td>2.600000e+19</td>\n",
       "      <td>Baidu Research - Silicon Valley AI Lab</td>\n",
       "      <td>Highly cited</td>\n",
       "      <td>Speech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1180</th>\n",
       "      <td>GNMT</td>\n",
       "      <td>2016-09-26</td>\n",
       "      <td>5.287325</td>\n",
       "      <td>21.820858</td>\n",
       "      <td>1.937871e+05</td>\n",
       "      <td>6.620000e+21</td>\n",
       "      <td>Google</td>\n",
       "      <td>Highly cited</td>\n",
       "      <td>Language</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1179</th>\n",
       "      <td>Xception</td>\n",
       "      <td>2016-10-07</td>\n",
       "      <td>4.100978</td>\n",
       "      <td>20.639486</td>\n",
       "      <td>1.261763e+04</td>\n",
       "      <td>4.360000e+20</td>\n",
       "      <td>Google</td>\n",
       "      <td>Highly cited</td>\n",
       "      <td>Vision</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1168</th>\n",
       "      <td>PolyNet</td>\n",
       "      <td>2016-11-17</td>\n",
       "      <td>2.789262</td>\n",
       "      <td>19.806180</td>\n",
       "      <td>6.155480e+02</td>\n",
       "      <td>6.400000e+19</td>\n",
       "      <td>Chinese University of Hong Kong (CUHK)</td>\n",
       "      <td>SOTA improvement</td>\n",
       "      <td>Vision</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152</th>\n",
       "      <td>AlphaGo Master</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>5.673431</td>\n",
       "      <td>23.301052</td>\n",
       "      <td>4.714453e+05</td>\n",
       "      <td>2.000100e+23</td>\n",
       "      <td>DeepMind</td>\n",
       "      <td>Highly cited</td>\n",
       "      <td>Games</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1148</th>\n",
       "      <td>MoE</td>\n",
       "      <td>2017-01-23</td>\n",
       "      <td>3.587007</td>\n",
       "      <td>19.972846</td>\n",
       "      <td>3.863735e+03</td>\n",
       "      <td>9.393906e+19</td>\n",
       "      <td>Jagiellonian University,Google Brain</td>\n",
       "      <td>Highly cited,SOTA improvement</td>\n",
       "      <td>Language</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1126</th>\n",
       "      <td>JFT</td>\n",
       "      <td>2017-07-10</td>\n",
       "      <td>4.236527</td>\n",
       "      <td>20.925828</td>\n",
       "      <td>1.723959e+04</td>\n",
       "      <td>8.430000e+20</td>\n",
       "      <td>Google Research,Carnegie Mellon University (CMU)</td>\n",
       "      <td>Highly cited</td>\n",
       "      <td>Vision</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1092</th>\n",
       "      <td>AlphaGo Zero</td>\n",
       "      <td>2017-10-18</td>\n",
       "      <td>5.787801</td>\n",
       "      <td>23.532754</td>\n",
       "      <td>6.134806e+05</td>\n",
       "      <td>3.410000e+23</td>\n",
       "      <td>DeepMind</td>\n",
       "      <td>Highly cited</td>\n",
       "      <td>Games</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1076</th>\n",
       "      <td>AlphaZero</td>\n",
       "      <td>2017-12-05</td>\n",
       "      <td>5.361574</td>\n",
       "      <td>22.564421</td>\n",
       "      <td>2.299186e+05</td>\n",
       "      <td>3.667927e+22</td>\n",
       "      <td>DeepMind</td>\n",
       "      <td>Highly cited</td>\n",
       "      <td>Games</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>BigGAN-deep 512x512</td>\n",
       "      <td>2018-09-28</td>\n",
       "      <td>3.713529</td>\n",
       "      <td>21.255273</td>\n",
       "      <td>5.170457e+03</td>\n",
       "      <td>1.800000e+21</td>\n",
       "      <td>Heriot-Watt University,DeepMind</td>\n",
       "      <td>Highly cited</td>\n",
       "      <td>Image generation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>917</th>\n",
       "      <td>RoBERTa Large</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>4.949744</td>\n",
       "      <td>21.618449</td>\n",
       "      <td>8.907265e+04</td>\n",
       "      <td>4.153836e+21</td>\n",
       "      <td>Facebook,University of Washington</td>\n",
       "      <td>Highly cited</td>\n",
       "      <td>Language</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>892</th>\n",
       "      <td>Megatron-LM (8.3B)</td>\n",
       "      <td>2019-09-17</td>\n",
       "      <td>5.057470</td>\n",
       "      <td>21.959041</td>\n",
       "      <td>1.141485e+05</td>\n",
       "      <td>9.100000e+21</td>\n",
       "      <td>NVIDIA</td>\n",
       "      <td>Highly cited,SOTA improvement</td>\n",
       "      <td>Language</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>891</th>\n",
       "      <td>Megatron-BERT</td>\n",
       "      <td>2019-09-17</td>\n",
       "      <td>5.821504</td>\n",
       "      <td>22.780101</td>\n",
       "      <td>6.629848e+05</td>\n",
       "      <td>6.027000e+22</td>\n",
       "      <td>NVIDIA</td>\n",
       "      <td>Highly cited,SOTA improvement</td>\n",
       "      <td>Language</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>T5-11B</td>\n",
       "      <td>2019-10-23</td>\n",
       "      <td>4.878087</td>\n",
       "      <td>22.518514</td>\n",
       "      <td>7.552439e+04</td>\n",
       "      <td>3.300000e+22</td>\n",
       "      <td>Google</td>\n",
       "      <td>Highly cited</td>\n",
       "      <td>Language</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>873</th>\n",
       "      <td>AlphaStar</td>\n",
       "      <td>2019-10-30</td>\n",
       "      <td>5.099536</td>\n",
       "      <td>22.772688</td>\n",
       "      <td>1.257581e+05</td>\n",
       "      <td>5.925000e+22</td>\n",
       "      <td>DeepMind</td>\n",
       "      <td>Highly cited</td>\n",
       "      <td>Games</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>846</th>\n",
       "      <td>Meena</td>\n",
       "      <td>2020-01-28</td>\n",
       "      <td>5.315467</td>\n",
       "      <td>23.049218</td>\n",
       "      <td>2.067604e+05</td>\n",
       "      <td>1.120000e+23</td>\n",
       "      <td>Google Brain</td>\n",
       "      <td>SOTA improvement</td>\n",
       "      <td>Language</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>841</th>\n",
       "      <td>Turing-NLG</td>\n",
       "      <td>2020-02-13</td>\n",
       "      <td>4.744790</td>\n",
       "      <td>22.195900</td>\n",
       "      <td>5.556354e+04</td>\n",
       "      <td>1.570000e+22</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>SOTA improvement</td>\n",
       "      <td>Language</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>806</th>\n",
       "      <td>GPT-3 175B (davinci)</td>\n",
       "      <td>2020-05-28</td>\n",
       "      <td>6.344714</td>\n",
       "      <td>23.496930</td>\n",
       "      <td>2.211639e+06</td>\n",
       "      <td>3.140000e+23</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>Highly cited</td>\n",
       "      <td>Language</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>iGPT-XL</td>\n",
       "      <td>2020-06-17</td>\n",
       "      <td>5.022808</td>\n",
       "      <td>22.518514</td>\n",
       "      <td>1.053921e+05</td>\n",
       "      <td>3.300000e+22</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>Highly cited</td>\n",
       "      <td>Vision,Image generation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737</th>\n",
       "      <td>DALL-E</td>\n",
       "      <td>2021-01-05</td>\n",
       "      <td>5.104155</td>\n",
       "      <td>22.672098</td>\n",
       "      <td>1.271026e+05</td>\n",
       "      <td>4.700000e+22</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>Significant use,Highly cited</td>\n",
       "      <td>Image generation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>734</th>\n",
       "      <td>Switch</td>\n",
       "      <td>2021-01-11</td>\n",
       "      <td>5.145083</td>\n",
       "      <td>22.914872</td>\n",
       "      <td>1.396636e+05</td>\n",
       "      <td>8.220000e+22</td>\n",
       "      <td>Google</td>\n",
       "      <td>Highly cited,SOTA improvement</td>\n",
       "      <td>Language</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>Meta Pseudo Labels</td>\n",
       "      <td>2021-03-01</td>\n",
       "      <td>4.731140</td>\n",
       "      <td>22.680336</td>\n",
       "      <td>5.384428e+04</td>\n",
       "      <td>4.790000e+22</td>\n",
       "      <td>Google Brain,Google AI</td>\n",
       "      <td>SOTA improvement</td>\n",
       "      <td>Vision</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>680</th>\n",
       "      <td>ProtT5-XXL</td>\n",
       "      <td>2021-05-04</td>\n",
       "      <td>4.932987</td>\n",
       "      <td>22.867467</td>\n",
       "      <td>8.570126e+04</td>\n",
       "      <td>7.370000e+22</td>\n",
       "      <td>Technical University of Munich,Med AI Technolo...</td>\n",
       "      <td>SOTA improvement</td>\n",
       "      <td>Biology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672</th>\n",
       "      <td>ByT5-XXL</td>\n",
       "      <td>2021-05-28</td>\n",
       "      <td>4.965923</td>\n",
       "      <td>22.908485</td>\n",
       "      <td>9.245338e+04</td>\n",
       "      <td>8.100000e+22</td>\n",
       "      <td>Google,Google Research</td>\n",
       "      <td>SOTA improvement</td>\n",
       "      <td>Language</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>650</th>\n",
       "      <td>GOAT</td>\n",
       "      <td>2021-07-27</td>\n",
       "      <td>4.928395</td>\n",
       "      <td>22.892095</td>\n",
       "      <td>8.479979e+04</td>\n",
       "      <td>7.800000e+22</td>\n",
       "      <td>DeepMind</td>\n",
       "      <td>SOTA improvement</td>\n",
       "      <td>Games</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>HyperCLOVA 82B</td>\n",
       "      <td>2021-09-10</td>\n",
       "      <td>5.768200</td>\n",
       "      <td>23.169086</td>\n",
       "      <td>5.864088e+05</td>\n",
       "      <td>1.476000e+23</td>\n",
       "      <td>NAVER,Search Solutions</td>\n",
       "      <td>SOTA improvement</td>\n",
       "      <td>Language</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>622</th>\n",
       "      <td>Megatron-Turing NLG 530B</td>\n",
       "      <td>2021-10-11</td>\n",
       "      <td>6.568705</td>\n",
       "      <td>24.068186</td>\n",
       "      <td>3.704291e+06</td>\n",
       "      <td>1.170000e+24</td>\n",
       "      <td>Microsoft,NVIDIA</td>\n",
       "      <td>SOTA improvement</td>\n",
       "      <td>Language</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>Gopher (280B)</td>\n",
       "      <td>2021-12-08</td>\n",
       "      <td>5.790011</td>\n",
       "      <td>23.800029</td>\n",
       "      <td>6.166111e+05</td>\n",
       "      <td>6.310000e+23</td>\n",
       "      <td>DeepMind</td>\n",
       "      <td>SOTA improvement</td>\n",
       "      <td>Language</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>GLaM</td>\n",
       "      <td>2021-12-13</td>\n",
       "      <td>5.733548</td>\n",
       "      <td>23.572872</td>\n",
       "      <td>5.414374e+05</td>\n",
       "      <td>3.740000e+23</td>\n",
       "      <td>Google</td>\n",
       "      <td>SOTA improvement</td>\n",
       "      <td>Language</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>LaMDA</td>\n",
       "      <td>2022-02-10</td>\n",
       "      <td>5.361633</td>\n",
       "      <td>23.550228</td>\n",
       "      <td>2.299500e+05</td>\n",
       "      <td>3.550000e+23</td>\n",
       "      <td>Google</td>\n",
       "      <td>Historical significance</td>\n",
       "      <td>Language</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>PaLM (540B)</td>\n",
       "      <td>2022-04-04</td>\n",
       "      <td>6.469225</td>\n",
       "      <td>24.402640</td>\n",
       "      <td>2.945950e+06</td>\n",
       "      <td>2.527200e+24</td>\n",
       "      <td>Google Research</td>\n",
       "      <td>Highly cited,SOTA improvement</td>\n",
       "      <td>Language</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>OPT-175B</td>\n",
       "      <td>2022-05-02</td>\n",
       "      <td>5.864314</td>\n",
       "      <td>23.633468</td>\n",
       "      <td>7.316676e+05</td>\n",
       "      <td>4.300000e+23</td>\n",
       "      <td>Meta AI</td>\n",
       "      <td>Significant use,Highly cited</td>\n",
       "      <td>Language</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>Parti</td>\n",
       "      <td>2022-06-22</td>\n",
       "      <td>5.537634</td>\n",
       "      <td>23.598013</td>\n",
       "      <td>3.448529e+05</td>\n",
       "      <td>3.962895e+23</td>\n",
       "      <td>Google Research</td>\n",
       "      <td>SOTA improvement</td>\n",
       "      <td>Image generation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>BLOOM-176B</td>\n",
       "      <td>2022-11-08</td>\n",
       "      <td>5.954758</td>\n",
       "      <td>23.761176</td>\n",
       "      <td>9.010687e+05</td>\n",
       "      <td>5.770000e+23</td>\n",
       "      <td>Hugging Face,BigScience</td>\n",
       "      <td>Historical significance,Highly cited</td>\n",
       "      <td>Language</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>GPT-3.5 (text-davinci-003)</td>\n",
       "      <td>2022-11-28</td>\n",
       "      <td>6.665163</td>\n",
       "      <td>24.411283</td>\n",
       "      <td>4.625551e+06</td>\n",
       "      <td>2.578000e+24</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>Historical significance,Significant use,SOTA i...</td>\n",
       "      <td>Language</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>GPT-4</td>\n",
       "      <td>2023-03-15</td>\n",
       "      <td>7.608383</td>\n",
       "      <td>25.322219</td>\n",
       "      <td>4.058659e+07</td>\n",
       "      <td>2.100000e+25</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>Highly cited,SOTA improvement</td>\n",
       "      <td>Multimodal,Language,Vision,Image generation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>PaLM 2</td>\n",
       "      <td>2023-05-10</td>\n",
       "      <td>6.687134</td>\n",
       "      <td>24.865696</td>\n",
       "      <td>4.865570e+06</td>\n",
       "      <td>7.340000e+24</td>\n",
       "      <td>Google</td>\n",
       "      <td>SOTA improvement</td>\n",
       "      <td>Language</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>Falcon-180B</td>\n",
       "      <td>2023-09-06</td>\n",
       "      <td>7.014559</td>\n",
       "      <td>24.575188</td>\n",
       "      <td>1.034091e+07</td>\n",
       "      <td>3.760000e+24</td>\n",
       "      <td>Technology Innovation Institute</td>\n",
       "      <td>SOTA improvement</td>\n",
       "      <td>Language</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>Amazon Titan</td>\n",
       "      <td>2023-09-28</td>\n",
       "      <td>6.882876</td>\n",
       "      <td>24.681241</td>\n",
       "      <td>7.636175e+06</td>\n",
       "      <td>4.800000e+24</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Language,Image generation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>Inflection-2</td>\n",
       "      <td>2023-11-22</td>\n",
       "      <td>7.112671</td>\n",
       "      <td>25.000434</td>\n",
       "      <td>1.296196e+07</td>\n",
       "      <td>1.001000e+25</td>\n",
       "      <td>Inflection AI</td>\n",
       "      <td>Significant use</td>\n",
       "      <td>Language</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>Gemini 1.0 Ultra</td>\n",
       "      <td>2023-12-06</td>\n",
       "      <td>7.474615</td>\n",
       "      <td>25.698970</td>\n",
       "      <td>2.982734e+07</td>\n",
       "      <td>5.000000e+25</td>\n",
       "      <td>Google DeepMind</td>\n",
       "      <td>SOTA improvement</td>\n",
       "      <td>Multimodal,Language,Vision</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>MegaScale (Production)</td>\n",
       "      <td>2024-02-23</td>\n",
       "      <td>6.418873</td>\n",
       "      <td>25.079181</td>\n",
       "      <td>2.623454e+06</td>\n",
       "      <td>1.200000e+25</td>\n",
       "      <td>ByteDance,Peking University</td>\n",
       "      <td>SOTA improvement</td>\n",
       "      <td>Language</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Mistral Large</td>\n",
       "      <td>2024-02-26</td>\n",
       "      <td>7.505540</td>\n",
       "      <td>25.301030</td>\n",
       "      <td>3.202871e+07</td>\n",
       "      <td>2.000000e+25</td>\n",
       "      <td>Mistral AI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Language</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Inflection-2.5</td>\n",
       "      <td>2024-03-07</td>\n",
       "      <td>7.072213</td>\n",
       "      <td>25.000043</td>\n",
       "      <td>1.180898e+07</td>\n",
       "      <td>1.000100e+25</td>\n",
       "      <td>Inflection AI</td>\n",
       "      <td>Significant use</td>\n",
       "      <td>Language</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Llama 3-70B</td>\n",
       "      <td>2024-04-18</td>\n",
       "      <td>6.983971</td>\n",
       "      <td>24.799341</td>\n",
       "      <td>9.637641e+06</td>\n",
       "      <td>6.300000e+24</td>\n",
       "      <td>Meta AI</td>\n",
       "      <td>Significant use</td>\n",
       "      <td>Language</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          System       date  log_cost   log_flop  \\\n",
       "1329          Dropout (ImageNet) 2012-06-03  0.883920  17.436476   \n",
       "1330             Dropout (CIFAR) 2012-06-03 -0.922816  15.630296   \n",
       "1331             Dropout (MNIST) 2012-06-03 -0.765968  15.780992   \n",
       "1322                     AlexNet 2012-09-30  1.102568  17.672098   \n",
       "1302            Visualizing CNNs 2013-11-12  1.097156  17.725912   \n",
       "1280                      SPPNet 2014-06-18  1.696501  18.532891   \n",
       "1272                       VGG16 2014-09-04  2.140588  18.966303   \n",
       "1228       DeepSpeech2 (English) 2015-12-08  2.314511  19.414973   \n",
       "1180                        GNMT 2016-09-26  5.287325  21.820858   \n",
       "1179                    Xception 2016-10-07  4.100978  20.639486   \n",
       "1168                     PolyNet 2016-11-17  2.789262  19.806180   \n",
       "1152              AlphaGo Master 2017-01-01  5.673431  23.301052   \n",
       "1148                         MoE 2017-01-23  3.587007  19.972846   \n",
       "1126                         JFT 2017-07-10  4.236527  20.925828   \n",
       "1092                AlphaGo Zero 2017-10-18  5.787801  23.532754   \n",
       "1076                   AlphaZero 2017-12-05  5.361574  22.564421   \n",
       "1003         BigGAN-deep 512x512 2018-09-28  3.713529  21.255273   \n",
       "917                RoBERTa Large 2019-07-01  4.949744  21.618449   \n",
       "892           Megatron-LM (8.3B) 2019-09-17  5.057470  21.959041   \n",
       "891                Megatron-BERT 2019-09-17  5.821504  22.780101   \n",
       "877                       T5-11B 2019-10-23  4.878087  22.518514   \n",
       "873                    AlphaStar 2019-10-30  5.099536  22.772688   \n",
       "846                        Meena 2020-01-28  5.315467  23.049218   \n",
       "841                   Turing-NLG 2020-02-13  4.744790  22.195900   \n",
       "806         GPT-3 175B (davinci) 2020-05-28  6.344714  23.496930   \n",
       "799                      iGPT-XL 2020-06-17  5.022808  22.518514   \n",
       "737                       DALL-E 2021-01-05  5.104155  22.672098   \n",
       "734                       Switch 2021-01-11  5.145083  22.914872   \n",
       "718           Meta Pseudo Labels 2021-03-01  4.731140  22.680336   \n",
       "680                   ProtT5-XXL 2021-05-04  4.932987  22.867467   \n",
       "672                     ByT5-XXL 2021-05-28  4.965923  22.908485   \n",
       "650                         GOAT 2021-07-27  4.928395  22.892095   \n",
       "629               HyperCLOVA 82B 2021-09-10  5.768200  23.169086   \n",
       "622     Megatron-Turing NLG 530B 2021-10-11  6.568705  24.068186   \n",
       "580                Gopher (280B) 2021-12-08  5.790011  23.800029   \n",
       "576                         GLaM 2021-12-13  5.733548  23.572872   \n",
       "546                        LaMDA 2022-02-10  5.361633  23.550228   \n",
       "524                  PaLM (540B) 2022-04-04  6.469225  24.402640   \n",
       "514                     OPT-175B 2022-05-02  5.864314  23.633468   \n",
       "477                        Parti 2022-06-22  5.537634  23.598013   \n",
       "408                   BLOOM-176B 2022-11-08  5.954758  23.761176   \n",
       "396   GPT-3.5 (text-davinci-003) 2022-11-28  6.665163  24.411283   \n",
       "336                        GPT-4 2023-03-15  7.608383  25.322219   \n",
       "294                       PaLM 2 2023-05-10  6.687134  24.865696   \n",
       "193                  Falcon-180B 2023-09-06  7.014559  24.575188   \n",
       "174                 Amazon Titan 2023-09-28  6.882876  24.681241   \n",
       "104                 Inflection-2 2023-11-22  7.112671  25.000434   \n",
       "88              Gemini 1.0 Ultra 2023-12-06  7.474615  25.698970   \n",
       "42        MegaScale (Production) 2024-02-23  6.418873  25.079181   \n",
       "39                 Mistral Large 2024-02-26  7.505540  25.301030   \n",
       "27                Inflection-2.5 2024-03-07  7.072213  25.000043   \n",
       "9                    Llama 3-70B 2024-04-18  6.983971  24.799341   \n",
       "\n",
       "              cost          flop  \\\n",
       "1329  7.654551e+00  2.731968e+17   \n",
       "1330  1.194495e-01  4.268700e+15   \n",
       "1331  1.714082e-01  6.039371e+15   \n",
       "1322  1.266392e+01  4.700000e+17   \n",
       "1302  1.250708e+01  5.320000e+17   \n",
       "1280  4.971654e+01  3.411072e+18   \n",
       "1272  1.382255e+02  9.253440e+18   \n",
       "1228  2.063058e+02  2.600000e+19   \n",
       "1180  1.937871e+05  6.620000e+21   \n",
       "1179  1.261763e+04  4.360000e+20   \n",
       "1168  6.155480e+02  6.400000e+19   \n",
       "1152  4.714453e+05  2.000100e+23   \n",
       "1148  3.863735e+03  9.393906e+19   \n",
       "1126  1.723959e+04  8.430000e+20   \n",
       "1092  6.134806e+05  3.410000e+23   \n",
       "1076  2.299186e+05  3.667927e+22   \n",
       "1003  5.170457e+03  1.800000e+21   \n",
       "917   8.907265e+04  4.153836e+21   \n",
       "892   1.141485e+05  9.100000e+21   \n",
       "891   6.629848e+05  6.027000e+22   \n",
       "877   7.552439e+04  3.300000e+22   \n",
       "873   1.257581e+05  5.925000e+22   \n",
       "846   2.067604e+05  1.120000e+23   \n",
       "841   5.556354e+04  1.570000e+22   \n",
       "806   2.211639e+06  3.140000e+23   \n",
       "799   1.053921e+05  3.300000e+22   \n",
       "737   1.271026e+05  4.700000e+22   \n",
       "734   1.396636e+05  8.220000e+22   \n",
       "718   5.384428e+04  4.790000e+22   \n",
       "680   8.570126e+04  7.370000e+22   \n",
       "672   9.245338e+04  8.100000e+22   \n",
       "650   8.479979e+04  7.800000e+22   \n",
       "629   5.864088e+05  1.476000e+23   \n",
       "622   3.704291e+06  1.170000e+24   \n",
       "580   6.166111e+05  6.310000e+23   \n",
       "576   5.414374e+05  3.740000e+23   \n",
       "546   2.299500e+05  3.550000e+23   \n",
       "524   2.945950e+06  2.527200e+24   \n",
       "514   7.316676e+05  4.300000e+23   \n",
       "477   3.448529e+05  3.962895e+23   \n",
       "408   9.010687e+05  5.770000e+23   \n",
       "396   4.625551e+06  2.578000e+24   \n",
       "336   4.058659e+07  2.100000e+25   \n",
       "294   4.865570e+06  7.340000e+24   \n",
       "193   1.034091e+07  3.760000e+24   \n",
       "174   7.636175e+06  4.800000e+24   \n",
       "104   1.296196e+07  1.001000e+25   \n",
       "88    2.982734e+07  5.000000e+25   \n",
       "42    2.623454e+06  1.200000e+25   \n",
       "39    3.202871e+07  2.000000e+25   \n",
       "27    1.180898e+07  1.000100e+25   \n",
       "9     9.637641e+06  6.300000e+24   \n",
       "\n",
       "                                           Organization  \\\n",
       "1329                              University of Toronto   \n",
       "1330                              University of Toronto   \n",
       "1331                              University of Toronto   \n",
       "1322                              University of Toronto   \n",
       "1302                          New York University (NYU)   \n",
       "1280  Microsoft,Xi’an Jiaotong University,University...   \n",
       "1272                               University of Oxford   \n",
       "1228             Baidu Research - Silicon Valley AI Lab   \n",
       "1180                                             Google   \n",
       "1179                                             Google   \n",
       "1168             Chinese University of Hong Kong (CUHK)   \n",
       "1152                                           DeepMind   \n",
       "1148               Jagiellonian University,Google Brain   \n",
       "1126   Google Research,Carnegie Mellon University (CMU)   \n",
       "1092                                           DeepMind   \n",
       "1076                                           DeepMind   \n",
       "1003                    Heriot-Watt University,DeepMind   \n",
       "917                   Facebook,University of Washington   \n",
       "892                                              NVIDIA   \n",
       "891                                              NVIDIA   \n",
       "877                                              Google   \n",
       "873                                            DeepMind   \n",
       "846                                        Google Brain   \n",
       "841                                           Microsoft   \n",
       "806                                              OpenAI   \n",
       "799                                              OpenAI   \n",
       "737                                              OpenAI   \n",
       "734                                              Google   \n",
       "718                              Google Brain,Google AI   \n",
       "680   Technical University of Munich,Med AI Technolo...   \n",
       "672                              Google,Google Research   \n",
       "650                                            DeepMind   \n",
       "629                              NAVER,Search Solutions   \n",
       "622                                    Microsoft,NVIDIA   \n",
       "580                                            DeepMind   \n",
       "576                                              Google   \n",
       "546                                              Google   \n",
       "524                                     Google Research   \n",
       "514                                             Meta AI   \n",
       "477                                     Google Research   \n",
       "408                             Hugging Face,BigScience   \n",
       "396                                              OpenAI   \n",
       "336                                              OpenAI   \n",
       "294                                              Google   \n",
       "193                     Technology Innovation Institute   \n",
       "174                                              Amazon   \n",
       "104                                       Inflection AI   \n",
       "88                                      Google DeepMind   \n",
       "42                          ByteDance,Peking University   \n",
       "39                                           Mistral AI   \n",
       "27                                        Inflection AI   \n",
       "9                                               Meta AI   \n",
       "\n",
       "                                    Notability criteria  \\\n",
       "1329                                       Highly cited   \n",
       "1330                                       Highly cited   \n",
       "1331                                       Highly cited   \n",
       "1322               Highly cited,Historical significance   \n",
       "1302                                       Highly cited   \n",
       "1280                                       Highly cited   \n",
       "1272                                       Highly cited   \n",
       "1228                                       Highly cited   \n",
       "1180                                       Highly cited   \n",
       "1179                                       Highly cited   \n",
       "1168                                   SOTA improvement   \n",
       "1152                                       Highly cited   \n",
       "1148                      Highly cited,SOTA improvement   \n",
       "1126                                       Highly cited   \n",
       "1092                                       Highly cited   \n",
       "1076                                       Highly cited   \n",
       "1003                                       Highly cited   \n",
       "917                                        Highly cited   \n",
       "892                       Highly cited,SOTA improvement   \n",
       "891                       Highly cited,SOTA improvement   \n",
       "877                                        Highly cited   \n",
       "873                                        Highly cited   \n",
       "846                                    SOTA improvement   \n",
       "841                                    SOTA improvement   \n",
       "806                                        Highly cited   \n",
       "799                                        Highly cited   \n",
       "737                        Significant use,Highly cited   \n",
       "734                       Highly cited,SOTA improvement   \n",
       "718                                    SOTA improvement   \n",
       "680                                    SOTA improvement   \n",
       "672                                    SOTA improvement   \n",
       "650                                    SOTA improvement   \n",
       "629                                    SOTA improvement   \n",
       "622                                    SOTA improvement   \n",
       "580                                    SOTA improvement   \n",
       "576                                    SOTA improvement   \n",
       "546                             Historical significance   \n",
       "524                       Highly cited,SOTA improvement   \n",
       "514                        Significant use,Highly cited   \n",
       "477                                    SOTA improvement   \n",
       "408                Historical significance,Highly cited   \n",
       "396   Historical significance,Significant use,SOTA i...   \n",
       "336                       Highly cited,SOTA improvement   \n",
       "294                                    SOTA improvement   \n",
       "193                                    SOTA improvement   \n",
       "174                                                 NaN   \n",
       "104                                     Significant use   \n",
       "88                                     SOTA improvement   \n",
       "42                                     SOTA improvement   \n",
       "39                                                  NaN   \n",
       "27                                      Significant use   \n",
       "9                                       Significant use   \n",
       "\n",
       "                                           Domain  \n",
       "1329                                       Vision  \n",
       "1330                                       Vision  \n",
       "1331                                       Vision  \n",
       "1322                                       Vision  \n",
       "1302                                       Vision  \n",
       "1280                                       Vision  \n",
       "1272                                       Vision  \n",
       "1228                                       Speech  \n",
       "1180                                     Language  \n",
       "1179                                       Vision  \n",
       "1168                                       Vision  \n",
       "1152                                        Games  \n",
       "1148                                     Language  \n",
       "1126                                       Vision  \n",
       "1092                                        Games  \n",
       "1076                                        Games  \n",
       "1003                             Image generation  \n",
       "917                                      Language  \n",
       "892                                      Language  \n",
       "891                                      Language  \n",
       "877                                      Language  \n",
       "873                                         Games  \n",
       "846                                      Language  \n",
       "841                                      Language  \n",
       "806                                      Language  \n",
       "799                       Vision,Image generation  \n",
       "737                              Image generation  \n",
       "734                                      Language  \n",
       "718                                        Vision  \n",
       "680                                       Biology  \n",
       "672                                      Language  \n",
       "650                                         Games  \n",
       "629                                      Language  \n",
       "622                                      Language  \n",
       "580                                      Language  \n",
       "576                                      Language  \n",
       "546                                      Language  \n",
       "524                                      Language  \n",
       "514                                      Language  \n",
       "477                              Image generation  \n",
       "408                                      Language  \n",
       "396                                      Language  \n",
       "336   Multimodal,Language,Vision,Image generation  \n",
       "294                                      Language  \n",
       "193                                      Language  \n",
       "174                     Language,Image generation  \n",
       "104                                      Language  \n",
       "88                     Multimodal,Language,Vision  \n",
       "42                                       Language  \n",
       "39                                       Language  \n",
       "27                                       Language  \n",
       "9                                        Language  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if dep_var == 'log_cost' or dep_var == 'log_flop':\n",
    "    df_filtered = df_filtered.dropna(subset=['date', 'flop', 'cost'])\n",
    "    df_filtered = df_filtered[['System', 'date', 'log_cost', 'log_flop', 'cost', 'flop', 'Organization', 'Notability criteria', 'Domain']]\n",
    "elif dep_var == 'log_training_time':\n",
    "    df_filtered = df_filtered.dropna(subset=['date', 'Training time (hours)'])\n",
    "    df_filtered['log_training_time'] = np.log10(df_filtered['Training time (hours)'])\n",
    "    df_filtered = df_filtered[['System', 'date', 'log_training_time', 'log_cost', 'log_flop', 'cost', 'flop', 'Organization', 'Notability criteria', 'Domain']]\n",
    "\n",
    "df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [04:16<00:00,  3.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>BIC</th>\n",
       "      <th>BIC 90% CI</th>\n",
       "      <th>BIC score diff</th>\n",
       "      <th>BIC score diff 90% CI</th>\n",
       "      <th>Xi²</th>\n",
       "      <th>% times preferred over simple</th>\n",
       "      <th>K-fold mean MSE</th>\n",
       "      <th>K-fold mean MSE 90% CI</th>\n",
       "      <th>Recent slope (Nx/year)</th>\n",
       "      <th>Recent slope 90% CI</th>\n",
       "      <th>Break point</th>\n",
       "      <th>Break point 90% CI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Simple</td>\n",
       "      <td>137.55</td>\n",
       "      <td>[109.31, 153.93]</td>\n",
       "      <td>0.00</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0%</td>\n",
       "      <td>0.73</td>\n",
       "      <td>[0.41, 0.98]</td>\n",
       "      <td>3.44</td>\n",
       "      <td>[2.88, 3.91]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Discrete acceleration</td>\n",
       "      <td>128.39</td>\n",
       "      <td>[94.68, 135.96]</td>\n",
       "      <td>-9.17</td>\n",
       "      <td>[-38.93, 0.9]</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>94%</td>\n",
       "      <td>0.56</td>\n",
       "      <td>[0.37, 0.77]</td>\n",
       "      <td>2.35</td>\n",
       "      <td>[1.85, 4.23]</td>\n",
       "      <td>2017-02</td>\n",
       "      <td>[2012-12, 2023-05]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Discontinuity</td>\n",
       "      <td>121.54</td>\n",
       "      <td>[-69.56, 126.71]</td>\n",
       "      <td>-16.02</td>\n",
       "      <td>[-205.17, -5.57]</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>99%</td>\n",
       "      <td>0.58</td>\n",
       "      <td>[0.33, 0.76]</td>\n",
       "      <td>3.23</td>\n",
       "      <td>[1.98, 6.03]</td>\n",
       "      <td>2018-02</td>\n",
       "      <td>[2014-01, 2023-01]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Model     BIC        BIC 90% CI  BIC score diff  \\\n",
       "0                 Simple  137.55  [109.31, 153.93]            0.00   \n",
       "1  Discrete acceleration  128.39   [94.68, 135.96]           -9.17   \n",
       "2          Discontinuity  121.54  [-69.56, 126.71]          -16.02   \n",
       "\n",
       "  BIC score diff 90% CI       Xi² % times preferred over simple  \\\n",
       "0            [0.0, 0.0]       NaN                            0%   \n",
       "1         [-38.93, 0.9]  0.000104                           94%   \n",
       "2      [-205.17, -5.57]  0.000002                           99%   \n",
       "\n",
       "   K-fold mean MSE K-fold mean MSE 90% CI  Recent slope (Nx/year)  \\\n",
       "0             0.73           [0.41, 0.98]                    3.44   \n",
       "1             0.56           [0.37, 0.77]                    2.35   \n",
       "2             0.58           [0.33, 0.76]                    3.23   \n",
       "\n",
       "  Recent slope 90% CI Break point  Break point 90% CI  \n",
       "0        [2.88, 3.91]         NaN                 NaN  \n",
       "1        [1.85, 4.23]     2017-02  [2012-12, 2023-05]  \n",
       "2        [1.98, 6.03]     2018-02  [2014-01, 2023-01]  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@markdown Analysis of best fit to the data\n",
    "\n",
    "@dataclass\n",
    "class FitResult:\n",
    "    df: pd.DataFrame\n",
    "    p: int = None\n",
    "    bic: float = None\n",
    "    rss: float = None\n",
    "    mse: float = None\n",
    "    predict: Callable = None\n",
    "\n",
    "@dataclass\n",
    "class HyperbolicFitResult(FitResult):\n",
    "    params: tuple[float] = None\n",
    "\n",
    "@dataclass\n",
    "class KinkedFitResult(FitResult):\n",
    "    break_points: tuple[float] = None\n",
    "    break_points_dt: float = None\n",
    "    oom_year_slopes: tuple[float] = None\n",
    "\n",
    "    # Model properties for each breakpoint combination\n",
    "    # (for debugging)\n",
    "    bics: tuple[float] = None\n",
    "    rsss: tuple[float] = None\n",
    "    mses: tuple[float] = None\n",
    "    break_points_list: tuple[tuple[float]] = None\n",
    "    break_points_dt_list: tuple[tuple[float]] = None\n",
    "\n",
    "def fit_hyperbolic(df):\n",
    "    def hyperbolic_model(t, A, B, k):\n",
    "        return A / (1 + B * np.exp(-k * t))\n",
    "\n",
    "    # Prepare data for curve fitting\n",
    "    timestamp = pd.to_datetime(df['date']).apply(lambda date: date.toordinal()).values\n",
    "\n",
    "    # Initial guess for the parameters\n",
    "    # initial_guess = [0, 0, 0]\n",
    "    initial_guess = [1.72373207e-02, -9.45447534e-01, -7.50101861e-08]  # Updated initial guess\n",
    "\n",
    "    # Fit the model to the data\n",
    "    try:\n",
    "      params, covariance = curve_fit(hyperbolic_model, timestamp, df[dep_var], p0=initial_guess, maxfev=100000, ftol=1e-10)\n",
    "    except RuntimeError as e:\n",
    "      print(\"FATAL ERROR WHEN FITTING HYPERBOLIC\")\n",
    "      return None\n",
    "\n",
    "    # Extracting parameters\n",
    "    A, B, k = params\n",
    "\n",
    "    # Compute predictions to calculate residuals\n",
    "    predicted_log_y = hyperbolic_model(timestamp, *params)\n",
    "\n",
    "    # Compute the Residual Sum of Squares (RSS)\n",
    "    rss = np.sum((df[dep_var] - predicted_log_y) ** 2)\n",
    "\n",
    "    # Number of observations (n)\n",
    "    n = len(df[dep_var])\n",
    "\n",
    "    # Number of parameters (p)\n",
    "    p = len(params) + 1\n",
    "\n",
    "    # Calculate log-likelihood under the assumption of normally distributed errors\n",
    "    # log_likelihood = -0.5 * rss\n",
    "    log_likelihood = -0.5 * n * (np.log(2 * np.pi * rss/n) + 1)\n",
    "\n",
    "    # Compute bic_hyperbolic using the provided formula\n",
    "    bic = p * np.log(n) - 2 * log_likelihood\n",
    "\n",
    "    # Compute MSE\n",
    "    mse = rss / n\n",
    "\n",
    "    fit_result = HyperbolicFitResult(\n",
    "        df=df,\n",
    "        p=p,\n",
    "        bic=bic,\n",
    "        rss=rss,\n",
    "        mse=mse,\n",
    "        params=params,\n",
    "        predict=lambda date: hyperbolic_model(date.apply(lambda d: d.toordinal()), *params)\n",
    "    )\n",
    "\n",
    "    return fit_result\n",
    "\n",
    "def fit_n_phase_exponential(df, kink_count, allow_discontinuities=False, min_n_segment = 5):\n",
    "    # Generate monthly breakpoints between 2010 and 2024\n",
    "    one_month = pd.DateOffset(months=1)\n",
    "    break_point_grid = pd.date_range(start=df['date'].min() - one_month, end=df['date'].max() - 4*one_month, freq='MS')\n",
    "    break_point_grid = [x.toordinal() for x in break_point_grid]\n",
    "\n",
    "    x = pd.to_datetime(df['date']).apply(lambda date: date.toordinal()).values\n",
    "    y = df[dep_var].values\n",
    "\n",
    "    break_points_list = []\n",
    "    bics = []\n",
    "    rsss = []\n",
    "    mses = []\n",
    "    models = []\n",
    "\n",
    "    for break_points in combinations_with_replacement(break_point_grid, kink_count):\n",
    "        # Model predictors\n",
    "\n",
    "        intercept_change_points = (0,)\n",
    "        if allow_discontinuities:\n",
    "            intercept_change_points += break_points\n",
    "        slope_change_points = (0,) + break_points\n",
    "\n",
    "        predictors = np.zeros((len(x), len(intercept_change_points) + len(slope_change_points)))\n",
    "\n",
    "        for i, intercept_point in enumerate(intercept_change_points):\n",
    "            predictors[:, i] = (x >= intercept_point).astype(int)\n",
    "\n",
    "        for i, break_point in enumerate(slope_change_points):\n",
    "            predictors[:, len(intercept_change_points) + i] = np.maximum(x - break_point, 0)\n",
    "\n",
    "        # Fit the model\n",
    "        model = sm.OLS(y, predictors).fit()\n",
    "\n",
    "        # Calculate BIC manually based on log-likelihood\n",
    "        n = len(x) # Number of observations\n",
    "        p = len(model.params) + 2*kink_count + 1 # Number of parameters\n",
    "\n",
    "        # Calculate log-likelihood under the assumption of normally distributed errors\n",
    "        # We have to iterate over all points to get their individual log-likelihoods\n",
    "        log_likelihood = 0\n",
    "        rss = 0\n",
    "        invalid_model = False # Discard models with segments with less than 2 points\n",
    "        for i, break_point in enumerate(slope_change_points):\n",
    "            left_x = break_point\n",
    "            right_x = slope_change_points[i + 1] if i + 1 < len(slope_change_points) else np.inf\n",
    "\n",
    "            segment_predictors = predictors[(left_x <= x) & (x < right_x), :]\n",
    "            segment_y = y[(left_x <= x) & (x < right_x)]\n",
    "            segment_n = len(segment_y)\n",
    "\n",
    "            assert min_n_segment > 2\n",
    "\n",
    "            if segment_n < min_n_segment:\n",
    "                invalid_model = True\n",
    "                break\n",
    "\n",
    "            y_pred = model.predict(segment_predictors)\n",
    "\n",
    "            segment_rss = np.sum((y_pred - segment_y)**2)\n",
    "            assert segment_rss > 0\n",
    "            segment_mse = segment_rss / segment_n\n",
    "\n",
    "            segment_log_likelihood = -segment_n/2 * (np.log(2*np.pi) + np.log(segment_rss/segment_n) + 1)\n",
    "            log_likelihood += segment_log_likelihood\n",
    "            rss += segment_rss\n",
    "\n",
    "        if invalid_model:\n",
    "            continue\n",
    "\n",
    "        # Compute BIC using the manual method based on the log-likelihood\n",
    "        bic = p * np.log(n) - 2 * log_likelihood\n",
    "        # bic = n*np.log(rss/n) + p*np.log(n)\n",
    "\n",
    "        bics.append(bic)\n",
    "        rsss.append(rss)\n",
    "        mses.append(rss/len(df))\n",
    "        models.append(model)\n",
    "        break_points_list.append(break_points)\n",
    "\n",
    "    # Prepare the result object\n",
    "    best_bic = min(bics)\n",
    "    best_idx = bics.index(best_bic)\n",
    "    best_rss = rsss[best_idx]\n",
    "    best_mse = mses[best_idx]\n",
    "    best_model = models[best_idx]\n",
    "    best_break_points = break_points_list[best_idx]\n",
    "\n",
    "    p = len(best_model.params) + 2*kink_count + 1 # Number of parameters\n",
    "\n",
    "    intercept_change_points = (0,)\n",
    "    if allow_discontinuities:\n",
    "        intercept_change_points += best_break_points\n",
    "    slope_change_points = (0,) + best_break_points\n",
    "\n",
    "    intercepts = best_model.params[:len(intercept_change_points)]\n",
    "    oom_year_slopes = 365 * np.cumsum(best_model.params[len(intercepts):])\n",
    "\n",
    "    def predict(date):\n",
    "        if not isinstance(date, pd.Series):\n",
    "            date = pd.Series(date)\n",
    "        x = pd.to_datetime(date).apply(lambda date: date.toordinal()).values\n",
    "\n",
    "        predictors = np.zeros((len(x), len(intercept_change_points) + len(slope_change_points)))\n",
    "\n",
    "        for i, intercept_point in enumerate(intercept_change_points):\n",
    "            predictors[:, i] = (x >= intercept_point).astype(int)\n",
    "\n",
    "        for i, break_point in enumerate(slope_change_points):\n",
    "            predictors[:, len(intercept_change_points) + i] = np.maximum(x - break_point, 0)\n",
    "\n",
    "        return best_model.predict(predictors)\n",
    "\n",
    "    fit_result = KinkedFitResult(\n",
    "        df=df,\n",
    "        p=p,\n",
    "        bic=best_bic,\n",
    "        rss=best_rss,\n",
    "        mse=best_mse,\n",
    "        break_points=best_break_points,\n",
    "        predict=predict,\n",
    "        break_points_dt=[pd.Timestamp.fromordinal(bp) for bp in best_break_points],\n",
    "        bics=bics,\n",
    "        rsss=rsss,\n",
    "        mses=mses,\n",
    "        oom_year_slopes=oom_year_slopes,\n",
    "        break_points_list=break_points_list,\n",
    "        break_points_dt_list=[[pd.Timestamp.fromordinal(bp) for bp in break_points] for break_points in break_points_list],\n",
    "    )\n",
    "\n",
    "    return fit_result\n",
    "\n",
    "fit_em_all = lambda df_fit : {\n",
    "    \"Simple\" : fit_n_phase_exponential(df_fit, kink_count=0),\n",
    "    \"Discrete acceleration\" : fit_n_phase_exponential(df_fit, kink_count=1),\n",
    "    \"Discontinuity\" : fit_n_phase_exponential(df_fit, kink_count=1, allow_discontinuities=True),\n",
    "    # \"Hyperbolic\": fit_hyperbolic(df_fit),\n",
    "}\n",
    "\n",
    "# Best model fits\n",
    "models = fit_em_all(df_filtered)\n",
    "\n",
    "# K-Fold Cross Validation\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from collections import defaultdict\n",
    "\n",
    "def perform_cross_validation(df, k=10, random_state=42):\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=random_state)\n",
    "    folds_mses = defaultdict(lambda : [])\n",
    "    for train_index, test_index in kf.split(df):\n",
    "        train_df, test_df = df.iloc[train_index], df.iloc[test_index]\n",
    "\n",
    "        # Fit the models on the training set\n",
    "        fold_models = fit_em_all(train_df)\n",
    "\n",
    "        # Predict on the test set\n",
    "        for name,model in fold_models.items():\n",
    "            try:\n",
    "                predicted_log_y = model.predict(test_df[\"date\"])\n",
    "            except AttributeError:\n",
    "                continue\n",
    "            test_rss = np.sum((predicted_log_y - test_df[dep_var])**2)\n",
    "            test_mse = test_rss / len(test_df)\n",
    "            folds_mses[name].append(test_mse)\n",
    "\n",
    "    # Compute mean MSE\n",
    "    folds_mses = {name: np.mean(folds_mses[name]) for name in folds_mses}\n",
    "\n",
    "    return folds_mses\n",
    "\n",
    "folds_mses = perform_cross_validation(df_filtered)\n",
    "\n",
    "# Bootstrap\n",
    "bootstrap_sample_size = 1000 #@param\n",
    "\n",
    "rng = np.random.default_rng(43)\n",
    "bootstrap_bics = defaultdict(lambda : [])\n",
    "bootstrap_mses = defaultdict(lambda : [])\n",
    "bootstrap_bic_score_diff = defaultdict(lambda : [])\n",
    "bootstrap_slopes = defaultdict(lambda : [])\n",
    "bootstrap_breaks = defaultdict(lambda : [])\n",
    "for bootstrap_index in tqdm(range(bootstrap_sample_size)):\n",
    "    sample = df_filtered.sample(len(df_filtered), replace=True, random_state=rng)\n",
    "    sample = sample.sort_values('date')\n",
    "\n",
    "    # Compute BICs\n",
    "    boot_models = fit_em_all(sample)\n",
    "\n",
    "    # Compute K fold validation\n",
    "    boot_folds_mses = perform_cross_validation(sample)\n",
    "\n",
    "    # Store results\n",
    "    for name, model in boot_models.items():\n",
    "        # It might be None if the hyperbolic fails to fit\n",
    "        if model is None: continue\n",
    "\n",
    "        bootstrap_bics[name].append(model.bic)\n",
    "        bootstrap_mses[name].append(boot_folds_mses[name])\n",
    "        bootstrap_bic_score_diff[name].append(model.bic - boot_models[\"Simple\"].bic)\n",
    "\n",
    "        if isinstance(model, KinkedFitResult):\n",
    "            if (len(model.oom_year_slopes) > 0): bootstrap_slopes[name].append(10**model.oom_year_slopes[-1])\n",
    "            if (len(model.break_points_dt) > 0): bootstrap_breaks[name].append(model.break_points_dt[-1])\n",
    "\n",
    "ci_width = 0.90\n",
    "qs = [(1 - ci_width)/2, (1 + ci_width)/2]\n",
    "bootstrap_preferred_percent = {}\n",
    "for name in models:\n",
    "    bootstrap_preferred_percent[name] = np.mean(np.array(bootstrap_bic_score_diff[name])<0)\n",
    "    bootstrap_bics[name] = np.quantile(np.array(bootstrap_bics[name]), qs)\n",
    "    bootstrap_mses[name] = np.quantile(np.array(bootstrap_mses[name]), qs)\n",
    "    bootstrap_bic_score_diff[name] = np.quantile(np.array(bootstrap_bic_score_diff[name]), qs)\n",
    "    try:\n",
    "        bootstrap_slopes[name] = np.quantile(np.array(bootstrap_slopes[name]), qs)\n",
    "        bootstrap_breaks[name] = np.quantile(np.array(bootstrap_breaks[name]), qs)\n",
    "    except IndexError:\n",
    "        pass\n",
    "#@markdown Models with lower BIC score / MSE are preferred.\n",
    "\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    param_count = model.p\n",
    "    log_likelihood = (np.log(len(df_filtered))*param_count - model.bic)/2\n",
    "\n",
    "    param_count_simple = models['Simple'].p\n",
    "    log_likelihood_simple = (np.log(len(df_filtered))*param_count_simple - models['Simple'].bic)/2\n",
    "\n",
    "    c2 = chi2.sf(2*(log_likelihood - log_likelihood_simple), df=(param_count - param_count_simple))\n",
    "\n",
    "    result = {\n",
    "        \"Model\": name,\n",
    "        \"BIC\" : np.round(model.bic, 2),\n",
    "        \"BIC 90% CI\" : np.round(bootstrap_bics[name], 2),\n",
    "        #\"Parameter count\": param_count,\n",
    "        #\"Log likelihood\": np.round((np.log(len(df_filtered))*param_count - model.bic)/2),\n",
    "        # \"MSE\" : model.mse,\n",
    "        \"BIC score diff\": np.round(model.bic - models[\"Simple\"].bic, 2),\n",
    "        \"BIC score diff 90% CI\": np.round(bootstrap_bic_score_diff[name], 2),\n",
    "        \"Xi²\": c2,\n",
    "        \"% times preferred over simple\": f\"{bootstrap_preferred_percent[name]:.0%}\",\n",
    "        # \"bayes factor over simple\" : np.exp(-0.5 * (model.bic - models[\"simple\"].bic)),\n",
    "        \"K-fold mean MSE\" : np.round(folds_mses[name], 2),\n",
    "        \"K-fold mean MSE 90% CI\" : np.round(bootstrap_mses[name], 2),\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        result[\"Recent slope (Nx/year)\"] = np.round(10**model.oom_year_slopes[-1], 2)\n",
    "        result[\"Recent slope 90% CI\"] = np.round(bootstrap_slopes[name], 2)\n",
    "        result[\"Break point\"] = model.break_points_dt[-1].strftime('%Y-%m')\n",
    "        result[\"Break point 90% CI\"] = [date.strftime('%Y-%m') for date in bootstrap_breaks[name]]\n",
    "    except (AttributeError, IndexError):\n",
    "        pass\n",
    "    results.append(result)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# bayes_factor = np.exp(-0.5 * (kinked_fit.bic - simple_fit.bic))\n",
    "\n",
    "print(\"Results\")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple [2.8840928  3.90834779]\n",
      "Discrete acceleration [1.85473357 4.22893316]\n",
      "Discontinuity [1.97555508 6.0340793 ]\n"
     ]
    }
   ],
   "source": [
    "for name, model in models.items():\n",
    "    print(name, bootstrap_slopes[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The breakpoints are: [2017-02-01]\n",
      "The slopes are: [0.91, 0.37] OOM/year ([8.08, 2.35] x/year)\n",
      "BIC score: 128.38626165550818\n",
      "This fit is preferred over a simple exponential by a BIC score difference of -265.94021293442916\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@markdown Graph of the different model fits\n",
    "\n",
    "model = 'kinked' #@param ['simple', 'kinked', 'hyperbolic']\n",
    "\n",
    "#@markdown Parameters for the kinked model\n",
    "kink_count = 1 #@param {type:'integer'}\n",
    "allow_discontinuities = False #@param {type:'boolean'}\n",
    "\n",
    "def plot_model(df, model_type, kink_count=1, verbose=True):\n",
    "    if model_type == 'hyperbolic':\n",
    "        fit_result = fit_hyperbolic(df)\n",
    "    elif model_type == 'simple':\n",
    "        fit_result = fit_n_phase_exponential(df, 0)\n",
    "    else:\n",
    "        fit_result = fit_n_phase_exponential(df, kink_count, allow_discontinuities)\n",
    "\n",
    "    log_y = fit_result.predict(df['date'])\n",
    "\n",
    "    # Plot the original data points\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(df['date'], df[dep_var], color='blue', label='Data points')\n",
    "\n",
    "    date_grid = pd.date_range(start=df['date'].min(), end=df['date'].max(), freq='D')\n",
    "    log_y = fit_result.predict(pd.Series(date_grid)) # we shouldn't need to convert to a Series\n",
    "    plt.plot(date_grid, log_y, color='red', label='Best Fit Line')\n",
    "\n",
    "\n",
    "    # Formatting the plot\n",
    "    plt.legend()\n",
    "    if model == 'kinked':\n",
    "      plt.title(f'Fit of {model_type} model with {kink_count} kinks')\n",
    "    else:\n",
    "      plt.title(f'Fit of {model_type} model')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel(dep_var)\n",
    "\n",
    "    # Format the date on the x-axis\n",
    "    plt.gca().xaxis.set_major_formatter(DateFormatter('%Y-%m-%d'))\n",
    "    plt.gcf().autofmt_xdate()  # Rotation\n",
    "\n",
    "    if model == 'kinked' or model == 'simple':\n",
    "        if kink_count == 0 or model == 'simple':\n",
    "            print(f\"The slope is: {fit_result.oom_year_slopes[0]:0.2f} OOM/year ({10**fit_result.oom_year_slopes[0]:0.2f}x/year)\")\n",
    "        else:\n",
    "            oom_year_slopes_str = ', '.join([f'{x:0.2f}' for x in fit_result.oom_year_slopes])\n",
    "            nx_year_slopes_str = ', '.join([f'{10**x:0.2f}' for x in fit_result.oom_year_slopes])\n",
    "            break_points_str = ', '.join([bp.strftime('%Y-%m-%d') for bp in fit_result.break_points_dt])\n",
    "\n",
    "            print(f\"The breakpoints are: [{break_points_str}]\")\n",
    "            print(f\"The slopes are: [{oom_year_slopes_str}] OOM/year ([{nx_year_slopes_str}] x/year)\")\n",
    "\n",
    "        # Add slope labels\n",
    "        points = [df['date'].min()] + fit_result.break_points_dt + [df['date'].max()]\n",
    "\n",
    "        for i in range(len(points) - 1):\n",
    "            mid = points[i] + (points[i+1] - points[i]) / 2\n",
    "            y = fit_result.predict(pd.Series([mid]))[0]\n",
    "            plt.text(mid, y - 2, f'{10**fit_result.oom_year_slopes[i]:0.2f}x/year', fontsize=12, color='blue', path_effects=[pe.withStroke(linewidth=4, foreground=\"white\")])\n",
    "\n",
    "    if verbose:\n",
    "      simple_fit = fit_n_phase_exponential(df, 0)\n",
    "      bayes_factor = np.exp(-0.5 * (fit_result.bic - simple_fit.bic))\n",
    "      unadjusted_bayes_factor = np.exp(-0.5 * (fit_result.bic - (simple_fit.bic + 2*np.log(len(df_filtered)))))\n",
    "\n",
    "      print(f\"BIC score: {fit_result.bic}\")\n",
    "      bic_score_difference = fit_result.bic - simple_fit.bic\n",
    "      if bic_score_difference > 0:\n",
    "        print(f\"The simple exponential is preferred over this fit by a BIC score difference of {fit_result.bic - simple_fit.bic}\")\n",
    "      if bic_score_difference < 0:\n",
    "        print(f\"This fit is preferred over a simple exponential by a BIC score difference of {-fit_result.bic - simple_fit.bic}\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plot_model(df_filtered, model, kink_count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epoch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

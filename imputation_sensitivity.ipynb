{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from plotting import *\n",
    "from prices import *\n",
    "from imputation import *\n",
    "from inflation import *\n",
    "from cost import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = 'results/sensitivity_analysis/'\n",
    "os.makedirs(results_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "frontier_pcd_df, hardware_df, price_df = load_data_for_cost_estimation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== System: AlphaGo Fan ====\n",
      "Training time is required but no value found\n",
      "\n",
      "==== System: AlphaGo Lee ====\n",
      "Training time is required but no value found\n",
      "\n",
      "==== System: GNMT ====\n",
      "Trying NVIDIA Tesla K80 at 2016-01-30 00:00:00\n",
      "Trying Google Cloud, Price per chip-hour (3-year CUD)\n",
      "Could not find price\n",
      "Trying Google Cloud, Price per chip-hour (1-year CUD)\n",
      "Could not find price\n",
      "Trying Google Cloud, Price per chip-hour (on-demand)\n",
      "Found price: 1.4\n",
      "\n",
      "==== System: NASv3 (CIFAR-10) ====\n",
      "Training time is required but no value found\n",
      "\n",
      "==== System: AlphaGo Master ====\n",
      "Training time is required but no value found\n",
      "\n",
      "==== System: JFT ====\n",
      "Trying NVIDIA Tesla K80 at 2017-03-12 00:00:00\n",
      "Trying Google Cloud, Price per chip-hour (3-year CUD)\n",
      "Could not find price\n",
      "Trying Google Cloud, Price per chip-hour (1-year CUD)\n",
      "Could not find price\n",
      "Trying Google Cloud, Price per chip-hour (on-demand)\n",
      "Found price: 1.4\n",
      "\n",
      "==== System: OpenAI TI7 DOTA 1v1 ====\n",
      "Training time is required but no value found\n",
      "\n",
      "==== System: AlphaGo Zero ====\n",
      "Trying Google TPU v1 at 2017-07-30 00:00:00\n",
      "Trying Google Cloud, Price per chip-hour (3-year CUD)\n",
      "Could not find price\n",
      "Trying Google Cloud, Price per chip-hour (1-year CUD)\n",
      "Could not find price\n",
      "Trying Google Cloud, Price per chip-hour (on-demand)\n",
      "Could not find price\n",
      "Estimating price from FLOP/s and FLOP/$ trend\n",
      "Could not find FLOP/s for Google TPU v1\n",
      "==== System: AlphaZero ====\n",
      "Training time is required but no value found\n",
      "\n",
      "==== System: AmoebaNet-A (F=448) ====\n",
      "Trying NVIDIA Tesla K40s at 2017-11-30 00:00:00\n",
      "Trying Google Cloud, Price per chip-hour (3-year CUD)\n",
      "Could not find price\n",
      "Trying Google Cloud, Price per chip-hour (1-year CUD)\n",
      "Could not find price\n",
      "Trying Google Cloud, Price per chip-hour (on-demand)\n",
      "Could not find price\n",
      "Trying Amazon Web Services, Price per chip-hour (3-year CUD)\n",
      "Could not find price\n",
      "Trying Amazon Web Services, Price per chip-hour (1-year CUD)\n",
      "Could not find price\n",
      "Trying Amazon Web Services, Price per chip-hour (on-demand)\n",
      "Could not find price\n",
      "Trying Microsoft Azure, Price per chip-hour (3-year CUD)\n",
      "Could not find price\n",
      "Trying Microsoft Azure, Price per chip-hour (1-year CUD)\n",
      "Could not find price\n",
      "Trying Microsoft Azure, Price per chip-hour (on-demand)\n",
      "Could not find price\n",
      "Estimating price from FLOP/s and FLOP/$ trend\n",
      "Estimated price: 0.05271257202460601\n",
      "\n",
      "==== System: IMPALA ====\n",
      "Trying NVIDIA P100 at 2017-12-02 20:00:00\n",
      "Trying Google Cloud, Price per chip-hour (3-year CUD)\n",
      "Could not find price\n",
      "Trying Google Cloud, Price per chip-hour (1-year CUD)\n",
      "Could not find price\n",
      "Trying Google Cloud, Price per chip-hour (on-demand)\n",
      "Found price: 1.46\n",
      "\n",
      "==== System: ResNeXt-101 32x48d ====\n",
      "Training time is required but no value found\n",
      "\n",
      "==== System: FTW ====\n",
      "Training time is required but no value found\n",
      "\n",
      "==== System: BigGAN-deep 512x512 ====\n",
      "Trying Google TPU v3 at 2018-07-28 00:00:00\n",
      "Trying Google Cloud, Price per chip-hour (3-year CUD)\n",
      "Could not find price\n",
      "Trying Google Cloud, Price per chip-hour (1-year CUD)\n",
      "Could not find price\n",
      "Trying Google Cloud, Price per chip-hour (on-demand)\n",
      "Found price: 2.0\n",
      "\n",
      "==== System: RoBERTa Large ====\n",
      "Trying NVIDIA Tesla V100 DGXS 32 GB at 2019-04-27 00:00:00\n",
      "Trying Amazon Web Services, Price per chip-hour (3-year CUD)\n",
      "Found price: 1.21\n",
      "\n",
      "==== System: Megatron-BERT ====\n",
      "Trying NVIDIA Tesla V100S PCIe 32 GB at 2019-05-22 00:00:00\n",
      "Trying Amazon Web Services, Price per chip-hour (3-year CUD)\n",
      "Could not find price\n",
      "Trying Amazon Web Services, Price per chip-hour (1-year CUD)\n",
      "Could not find price\n",
      "Trying Amazon Web Services, Price per chip-hour (on-demand)\n",
      "Could not find price\n",
      "Trying Microsoft Azure, Price per chip-hour (3-year CUD)\n",
      "Could not find price\n",
      "Trying Microsoft Azure, Price per chip-hour (1-year CUD)\n",
      "Could not find price\n",
      "Trying Microsoft Azure, Price per chip-hour (on-demand)\n",
      "Could not find price\n",
      "Trying Google Cloud, Price per chip-hour (3-year CUD)\n",
      "Could not find price\n",
      "Trying Google Cloud, Price per chip-hour (1-year CUD)\n",
      "Could not find price\n",
      "Trying Google Cloud, Price per chip-hour (on-demand)\n",
      "Could not find price\n",
      "Estimating price from FLOP/s and FLOP/$ trend\n",
      "Estimated price: 0.1051277082983274\n",
      "\n",
      "==== System: Megatron-LM (8.3B) ====\n",
      "Trying NVIDIA Tesla V100 DGXS 32 GB at 2019-07-05 09:00:00\n",
      "Trying Amazon Web Services, Price per chip-hour (3-year CUD)\n",
      "Found price: 1.21\n",
      "\n",
      "==== System: T5-3B ====\n",
      "Training time is required but no value found\n",
      "\n",
      "==== System: T5-11B ====\n",
      "Trying Google TPU v3 at 2019-08-03 23:00:00\n",
      "Trying Google Cloud, Price per chip-hour (3-year CUD)\n",
      "Found price: 0.9\n",
      "\n",
      "==== System: AlphaStar ====\n",
      "Trying Google TPU v3 at 2019-07-18 00:00:00\n",
      "Trying Google Cloud, Price per chip-hour (3-year CUD)\n",
      "Found price: 0.9\n",
      "\n",
      "==== System: OpenAI Five ====\n",
      "Could not find hardware model for OpenAI Five\n",
      "\n",
      "==== System: OpenAI Five Rerun ====\n",
      "Training time is required but no value found\n",
      "\n",
      "==== System: Meena ====\n",
      "Trying Google TPU v3 at 2019-10-30 00:00:00\n",
      "Trying Google Cloud, Price per chip-hour (3-year CUD)\n",
      "Found price: 0.9\n",
      "\n",
      "==== System: Turing-NLG ====\n",
      "Training time is required but no value found\n",
      "\n",
      "==== System: GPT-3 175B (davinci) ====\n",
      "Trying NVIDIA Tesla V100 DGXS 32 GB at 2020-03-14 05:00:00\n",
      "Trying Microsoft Azure, Price per chip-hour (3-year CUD)\n",
      "Could not find price\n",
      "Trying Microsoft Azure, Price per chip-hour (1-year CUD)\n",
      "Could not find price\n",
      "Trying Microsoft Azure, Price per chip-hour (on-demand)\n",
      "Could not find price\n",
      "Trying Amazon Web Services, Price per chip-hour (3-year CUD)\n",
      "Found price: 1.21\n",
      "\n",
      "==== System: GShard (dense) ====\n",
      "Trying Google TPU v3 at 2020-03-20 00:00:00\n",
      "Trying Google Cloud, Price per chip-hour (3-year CUD)\n",
      "Found price: 0.9\n",
      "\n",
      "==== System: ALIGN ====\n",
      "Trying Google TPU v3 at 2021-03-28 13:00:00\n",
      "Trying Google Cloud, Price per chip-hour (3-year CUD)\n",
      "Found price: 0.9\n",
      "\n",
      "==== System: Megatron-Turing NLG 530B ====\n",
      "Trying NVIDIA A100 SXM4 80 GB at 2021-07-10 22:00:00\n",
      "Trying Microsoft Azure, Price per chip-hour (3-year CUD)\n",
      "Found price: 1.8\n",
      "\n",
      "==== System: Yuan 1.0 ====\n",
      "Training time is required but no value found\n",
      "\n",
      "==== System: Gopher (280B) ====\n",
      "Trying Google TPU v3 at 2021-08-31 16:00:00\n",
      "Trying Google Cloud, Price per chip-hour (3-year CUD)\n",
      "Found price: 0.9\n",
      "\n",
      "==== System: GLaM ====\n",
      "Trying Google TPU v4 at 2021-08-18 02:00:00\n",
      "Trying Google Cloud, Price per chip-hour (3-year CUD)\n",
      "Found price: 1.45\n",
      "\n",
      "==== System: LaMDA ====\n",
      "Trying Google TPU v3 at 2021-10-15 07:00:00\n",
      "Trying Google Cloud, Price per chip-hour (3-year CUD)\n",
      "Found price: 0.9\n",
      "\n",
      "==== System: Chinchilla ====\n",
      "Training time is required but no value found\n",
      "\n",
      "==== System: PaLM (540B) ====\n",
      "Trying Google TPU v4 at 2021-12-08 00:00:00\n",
      "Trying Google Cloud, Price per chip-hour (3-year CUD)\n",
      "Found price: 1.45\n",
      "\n",
      "==== System: OPT-175B ====\n",
      "Trying NVIDIA A100 SXM4 80 GB at 2022-01-28 23:00:00\n",
      "Trying Amazon Web Services, Price per chip-hour (3-year CUD)\n",
      "Found price: 1.81\n",
      "\n",
      "==== System: Minerva (540B) ====\n",
      "Trying Google TPU v4 at 2022-04-01 00:00:00\n",
      "Trying Google Cloud, Price per chip-hour (3-year CUD)\n",
      "Found price: 1.45\n",
      "\n",
      "==== System: GPT-3.5 (text-davinci-003) ====\n",
      "Training time is required but no value found\n",
      "\n",
      "==== System: LLaMA-65B ====\n",
      "Trying NVIDIA A100 at 2022-12-05 04:00:00\n",
      "Trying Amazon Web Services, Price per chip-hour (3-year CUD)\n",
      "Could not find price\n",
      "Trying Amazon Web Services, Price per chip-hour (1-year CUD)\n",
      "Could not find price\n",
      "Trying Amazon Web Services, Price per chip-hour (on-demand)\n",
      "Could not find price\n",
      "Trying Microsoft Azure, Price per chip-hour (3-year CUD)\n",
      "Found price: 1.36\n",
      "\n",
      "==== System: GPT-4 ====\n",
      "Trying NVIDIA A100 SXM4 40 GB at 2022-10-11 00:00:00\n",
      "Trying Microsoft Azure, Price per chip-hour (3-year CUD)\n",
      "Could not find price\n",
      "Trying Microsoft Azure, Price per chip-hour (1-year CUD)\n",
      "Could not find price\n",
      "Trying Microsoft Azure, Price per chip-hour (on-demand)\n",
      "Could not find price\n",
      "Trying Amazon Web Services, Price per chip-hour (3-year CUD)\n",
      "Found price: 1.45\n",
      "\n",
      "==== System: PaLM 2 ====\n",
      "Training time is required but no value found\n",
      "\n",
      "==== System: Claude 2 ====\n",
      "Training time is required but no value found\n",
      "\n",
      "==== System: Llama 2-70B ====\n",
      "Trying NVIDIA A100 SXM4 80 GB at 2022-11-20 00:00:00\n",
      "Trying Amazon Web Services, Price per chip-hour (3-year CUD)\n",
      "Found price: 1.81\n",
      "\n",
      "==== System: Falcon 180B ====\n",
      "Trying NVIDIA A100 SXM4 40 GB at 2023-01-09 00:00:00\n",
      "Trying Amazon Web Services, Price per chip-hour (3-year CUD)\n",
      "Found price: 1.45\n",
      "\n",
      "{'GNMT': 580608.0, 'JFT': 100800.0, 'AmoebaNet-A (F=448)': 3985.0704450602143, 'IMPALA': 146.0, 'BigGAN-deep 512x512': 24576.0, 'RoBERTa Large': 148684.8, 'Megatron-BERT': 74924.93821505114, 'Megatron-LM (8.3B)': 202583.04, 'T5-11B': 222059.52, 'AlphaStar': 364953.60000000003, 'Meena': 663552.0, 'GPT-3 175B (davinci)': 4297920.0, 'GShard (dense)': 928972.8, 'ALIGN': 160035.84, 'Megatron-Turing NLG 530B': 6209280.0, 'Gopher (280B)': 3391488.0, 'GLaM': 2028236.8, 'LaMDA': 1276416.0, 'PaLM (540B)': 12187238.4, 'OPT-175B': 1470704.6400000001, 'Minerva (540B)': 13220659.200000001, 'LLaMA-65B': 1392640.0, 'GPT-4': 82650000.0, 'Falcon 180B': 25657344.0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Domain</th>\n",
       "      <th>Task</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Notability criteria</th>\n",
       "      <th>Notability criteria notes</th>\n",
       "      <th>Open-source</th>\n",
       "      <th>Link</th>\n",
       "      <th>Citations</th>\n",
       "      <th>Reference</th>\n",
       "      <th>Publication date</th>\n",
       "      <th>...</th>\n",
       "      <th>Finetune compute notes</th>\n",
       "      <th>Authors by country</th>\n",
       "      <th>Hardware quantity</th>\n",
       "      <th>Hardware utilization</th>\n",
       "      <th>Training cost trends</th>\n",
       "      <th>Training cloud compute vendor</th>\n",
       "      <th>Training data center</th>\n",
       "      <th>System</th>\n",
       "      <th>Training time (chip hours)</th>\n",
       "      <th>Cost</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>System</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AlphaGo Fan</th>\n",
       "      <td>Games</td>\n",
       "      <td>Go</td>\n",
       "      <td>David Silver, Aja Huang, Chris J. Maddison, Ar...</td>\n",
       "      <td>SOTA improvement</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.nature.com/articles/nature24270.ep...</td>\n",
       "      <td>14389.0</td>\n",
       "      <td>Mastering the game of Go with deep neural netw...</td>\n",
       "      <td>2015-10-01</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AlphaGo Fan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AlphaGo Fan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AlphaGo Fan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AlphaGo Lee</th>\n",
       "      <td>Games</td>\n",
       "      <td>Go</td>\n",
       "      <td>David Silver, Aja Huang, Chris J. Maddison, Ar...</td>\n",
       "      <td>Highly cited</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.nature.com/articles/nature16961</td>\n",
       "      <td>14389.0</td>\n",
       "      <td>Mastering the game of Go with deep neural netw...</td>\n",
       "      <td>2016-01-27</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AlphaGo Lee</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AlphaGo Lee</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AlphaGo Lee</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GNMT</th>\n",
       "      <td>Language</td>\n",
       "      <td>Translation</td>\n",
       "      <td>Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc ...</td>\n",
       "      <td>Highly cited</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/abs/1609.08144</td>\n",
       "      <td>5948.0</td>\n",
       "      <td>Google's Neural Machine Translation System: Br...</td>\n",
       "      <td>2016-09-26</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GNMT</td>\n",
       "      <td>96.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GNMT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GNMT</td>\n",
       "      <td>414720.0</td>\n",
       "      <td>5.806080e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NASv3 (CIFAR-10)</th>\n",
       "      <td>Vision</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Barret Zoph, Quoc V. Le</td>\n",
       "      <td>Highly cited</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/abs/1611.01578</td>\n",
       "      <td>4569.0</td>\n",
       "      <td>Neural Architecture Search with Reinforcement ...</td>\n",
       "      <td>2016-11-05</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NASv3 (CIFAR-10)</td>\n",
       "      <td>800.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NASv3 (CIFAR-10)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NASv3 (CIFAR-10)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AlphaGo Master</th>\n",
       "      <td>Games</td>\n",
       "      <td>Go</td>\n",
       "      <td>D Silver, J Schrittwieser, K Simonyan, I Anton...</td>\n",
       "      <td>Highly cited</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.researchgate.net/publication/32047...</td>\n",
       "      <td>7831.0</td>\n",
       "      <td>Mastering the game of Go without human knowledge</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AlphaGo Master</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AlphaGo Master</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AlphaGo Master</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JFT</th>\n",
       "      <td>Vision</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Chen Sun, Abhinav Shrivastava, Saurabh Singh, ...</td>\n",
       "      <td>Highly cited</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/abs/1707.02968</td>\n",
       "      <td>1923.0</td>\n",
       "      <td>Revisiting Unreasonable Effectiveness of Data ...</td>\n",
       "      <td>2017-07-10</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>JFT</td>\n",
       "      <td>50.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>JFT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>JFT</td>\n",
       "      <td>72000.0</td>\n",
       "      <td>1.008000e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OpenAI TI7 DOTA 1v1</th>\n",
       "      <td>Games</td>\n",
       "      <td>DOTA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SOTA Improvement,Historical significance</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://openai.com/research/dota-2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Dota 2</td>\n",
       "      <td>2017-08-11</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OpenAI TI7 DOTA 1v1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OpenAI TI7 DOTA 1v1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OpenAI TI7 DOTA 1v1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AlphaGo Zero</th>\n",
       "      <td>Games</td>\n",
       "      <td>Go</td>\n",
       "      <td>D Silver, J Schrittwieser, K Simonyan, I Anton...</td>\n",
       "      <td>Highly cited</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.researchgate.net/publication/32047...</td>\n",
       "      <td>7831.0</td>\n",
       "      <td>Mastering the game of Go without human knowledge</td>\n",
       "      <td>2017-10-18</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AlphaGo Zero</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AlphaGo Zero</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AlphaGo Zero</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AlphaZero</th>\n",
       "      <td>Games</td>\n",
       "      <td>NaN</td>\n",
       "      <td>D Silver, T Hubert, J Schrittwieser, I Antonoglou</td>\n",
       "      <td>Highly cited</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/abs/1712.01815</td>\n",
       "      <td>1343.0</td>\n",
       "      <td>Mastering Chess and Shogi by Self-Play with a ...</td>\n",
       "      <td>2017-12-05</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AlphaZero</td>\n",
       "      <td>64.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AlphaZero</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AlphaZero</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AmoebaNet-A (F=448)</th>\n",
       "      <td>Vision</td>\n",
       "      <td>Image classification</td>\n",
       "      <td>Esteban Real, Alok Aggarwal, Yanping Huang, Qu...</td>\n",
       "      <td>Highly cited</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/abs/1802.01548</td>\n",
       "      <td>2505.0</td>\n",
       "      <td>Regularized Evolution for Image Classifier Arc...</td>\n",
       "      <td>2018-02-05</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AmoebaNet-A (F=448)</td>\n",
       "      <td>450.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AmoebaNet-A (F=448)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AmoebaNet-A (F=448)</td>\n",
       "      <td>75600.0</td>\n",
       "      <td>3.985070e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IMPALA</th>\n",
       "      <td>Games</td>\n",
       "      <td>Atari</td>\n",
       "      <td>Lasse Espeholt, Hubert Soyer, Remi Munos, Kare...</td>\n",
       "      <td>SOTA Improvement</td>\n",
       "      <td>\"IMPALA is able to achieve better performance ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/abs/1802.01561</td>\n",
       "      <td>1288.0</td>\n",
       "      <td>IMPALA: Scalable Distributed Deep-RL with Impo...</td>\n",
       "      <td>2018-02-05</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IMPALA</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IMPALA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IMPALA</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.460000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ResNeXt-101 32x48d</th>\n",
       "      <td>Vision</td>\n",
       "      <td>Image classification</td>\n",
       "      <td>Dhruv Mahajan, Ross Girshick, Vignesh Ramanath...</td>\n",
       "      <td>SOTA Improvement</td>\n",
       "      <td>\"We show improvements on several image classif...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/abs/1805.00932</td>\n",
       "      <td>1176.0</td>\n",
       "      <td>Exploring the Limits of Weakly Supervised Pret...</td>\n",
       "      <td>2018-05-02</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ResNeXt-101 32x48d</td>\n",
       "      <td>336.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ResNeXt-101 32x48d</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ResNeXt-101 32x48d</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FTW</th>\n",
       "      <td>Games</td>\n",
       "      <td>Capture the flag</td>\n",
       "      <td>Max Jaderberg, Wojciech M. Czarnecki, Iain Dun...</td>\n",
       "      <td>SOTA Improvement</td>\n",
       "      <td>\"In this work, we demonstrate for the first ti...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/abs/1807.01281</td>\n",
       "      <td>571.0</td>\n",
       "      <td>Human-level performance in first-person multip...</td>\n",
       "      <td>2018-07-03</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FTW</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FTW</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FTW</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BigGAN-deep 512x512</th>\n",
       "      <td>Drawing</td>\n",
       "      <td>Image generation</td>\n",
       "      <td>A Brock, J Donahue, K Simonyan</td>\n",
       "      <td>Highly cited</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Permissive license</td>\n",
       "      <td>https://arxiv.org/abs/1809.11096</td>\n",
       "      <td>4163.0</td>\n",
       "      <td>Large Scale GAN Training for High Fidelity Nat...</td>\n",
       "      <td>2018-09-28</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BigGAN-deep 512x512</td>\n",
       "      <td>256.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BigGAN-deep 512x512</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BigGAN-deep 512x512</td>\n",
       "      <td>12288.0</td>\n",
       "      <td>2.457600e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RoBERTa Large</th>\n",
       "      <td>Language</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du,...</td>\n",
       "      <td>Highly cited</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/abs/1907.11692</td>\n",
       "      <td>15901.0</td>\n",
       "      <td>RoBERTa: A Robustly Optimized BERT Pretraining...</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RoBERTa Large</td>\n",
       "      <td>1024.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RoBERTa Large</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RoBERTa Large</td>\n",
       "      <td>122880.0</td>\n",
       "      <td>1.486848e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Megatron-BERT</th>\n",
       "      <td>Language</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mohammad Shoeybi, Mostofa Patwary, Raul Puri, ...</td>\n",
       "      <td>SOTA Improvement</td>\n",
       "      <td>\"Our BERT model achieves SOTA results on the R...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/abs/1909.08053</td>\n",
       "      <td>1003.0</td>\n",
       "      <td>Megatron-LM: Training Multi-Billion Parameter ...</td>\n",
       "      <td>2019-09-17</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Megatron-BERT</td>\n",
       "      <td>512.0</td>\n",
       "      <td>0.2269</td>\n",
       "      <td>Megatron-BERT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Megatron-BERT</td>\n",
       "      <td>712704.0</td>\n",
       "      <td>7.492494e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Megatron-LM (8.3B)</th>\n",
       "      <td>Language</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mohammad Shoeybi, Mostofa Patwary, Raul Puri, ...</td>\n",
       "      <td>SOTA Improvement</td>\n",
       "      <td>\"Using the GPT-2 model we achieve SOTA results...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/abs/1909.08053</td>\n",
       "      <td>1003.0</td>\n",
       "      <td>Megatron-LM: Training Multi-Billion Parameter ...</td>\n",
       "      <td>2019-09-17</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Megatron-LM (8.3B)</td>\n",
       "      <td>512.0</td>\n",
       "      <td>0.1162</td>\n",
       "      <td>Megatron-LM (8.3B)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Megatron-LM (8.3B)</td>\n",
       "      <td>167424.0</td>\n",
       "      <td>2.025830e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T5-3B</th>\n",
       "      <td>Language</td>\n",
       "      <td>Text autocompletion</td>\n",
       "      <td>Colin Raffel, Noam Shazeer, Adam Roberts, Kath...</td>\n",
       "      <td>Highly cited</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/abs/1910.10683</td>\n",
       "      <td>10800.0</td>\n",
       "      <td>Exploring the Limits of Transfer Learning with...</td>\n",
       "      <td>2019-10-23</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>T5-3B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>T5-3B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>T5-3B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T5-11B</th>\n",
       "      <td>Language</td>\n",
       "      <td>Text autocompletion</td>\n",
       "      <td>Colin Raffel, Noam Shazeer, Adam Roberts, Kath...</td>\n",
       "      <td>Highly cited</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/abs/1910.10683</td>\n",
       "      <td>10800.0</td>\n",
       "      <td>Exploring the Limits of Transfer Learning with...</td>\n",
       "      <td>2019-10-23</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>T5-11B</td>\n",
       "      <td>512.0</td>\n",
       "      <td>0.3707</td>\n",
       "      <td>T5-11B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>T5-11B</td>\n",
       "      <td>246732.8</td>\n",
       "      <td>2.220595e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AlphaStar</th>\n",
       "      <td>Games</td>\n",
       "      <td>StarCraft</td>\n",
       "      <td>Oriol Vinyals,Igor Babuschkin,Wojciech M. Czar...</td>\n",
       "      <td>Highly cited</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.deepmind.com/blog/alphastar-grandm...</td>\n",
       "      <td>2681.0</td>\n",
       "      <td>Grandmaster level in StarCraft II using multi-...</td>\n",
       "      <td>2019-10-30</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AlphaStar</td>\n",
       "      <td>384.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AlphaStar</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AlphaStar</td>\n",
       "      <td>405504.0</td>\n",
       "      <td>3.649536e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OpenAI Five</th>\n",
       "      <td>Games</td>\n",
       "      <td>Dota 2</td>\n",
       "      <td>Christopher Berner, Greg Brockman, Brooke Chan...</td>\n",
       "      <td>SOTA improvement</td>\n",
       "      <td>\"On April 13th, 2019, OpenAI Five became the f...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/abs/1912.06680</td>\n",
       "      <td>1303.0</td>\n",
       "      <td>Dota 2 with Large Scale Deep Reinforcement Lea...</td>\n",
       "      <td>2019-12-13</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OpenAI Five</td>\n",
       "      <td>1536.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OpenAI Five</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OpenAI Five</td>\n",
       "      <td>10911744.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OpenAI Five Rerun</th>\n",
       "      <td>Games</td>\n",
       "      <td>Dota 2</td>\n",
       "      <td>Christopher Berner, Greg Brockman, Brooke Chan...</td>\n",
       "      <td>SOTA improvement</td>\n",
       "      <td>\"On April 13th, 2019, OpenAI Five became the f...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://cdn.openai.com/dota-2.pdf</td>\n",
       "      <td>1303.0</td>\n",
       "      <td>Dota 2 with Large Scale Deep Reinforcement Lea...</td>\n",
       "      <td>2019-12-13</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OpenAI Five Rerun</td>\n",
       "      <td>512.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OpenAI Five Rerun</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OpenAI Five Rerun</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Meena</th>\n",
       "      <td>Language</td>\n",
       "      <td>Text autocompletion</td>\n",
       "      <td>Dongling Xiao, Han Zhang, Yukun Li, Yu Sun, Ha...</td>\n",
       "      <td>SOTA Improvement</td>\n",
       "      <td>\"We also propose a human evaluation metric cal...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/abs/2001.09977</td>\n",
       "      <td>747.0</td>\n",
       "      <td>Towards a Human-like Open-Domain Chatbot</td>\n",
       "      <td>2020-01-28</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Meena</td>\n",
       "      <td>1024.0</td>\n",
       "      <td>0.3439</td>\n",
       "      <td>Meena</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Meena</td>\n",
       "      <td>737280.0</td>\n",
       "      <td>6.635520e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Turing-NLG</th>\n",
       "      <td>Language</td>\n",
       "      <td>Text autocompletion</td>\n",
       "      <td>Corby Rosset</td>\n",
       "      <td>SOTA Improvement</td>\n",
       "      <td>from paper: \"Turing Natural Language Generatio...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.microsoft.com/en-us/research/blog/...</td>\n",
       "      <td>114.0</td>\n",
       "      <td>Turing-NLG: A 17-billion-parameter language mo...</td>\n",
       "      <td>2020-02-13</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Turing-NLG</td>\n",
       "      <td>256.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Turing-NLG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Turing-NLG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPT-3 175B (davinci)</th>\n",
       "      <td>Language</td>\n",
       "      <td>Text autocompletion</td>\n",
       "      <td>Tom B. Brown, Benjamin Mann, Nick Ryder, Melan...</td>\n",
       "      <td>Highly cited</td>\n",
       "      <td>NaN</td>\n",
       "      <td>API accessible</td>\n",
       "      <td>https://arxiv.org/abs/2005.14165</td>\n",
       "      <td>18144.0</td>\n",
       "      <td>Language models are Few-Shot Learners</td>\n",
       "      <td>2020-05-28</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GPT-3 175B (davinci)</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.2196</td>\n",
       "      <td>GPT-3 175B (davinci)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GPT-3 175B (davinci)</td>\n",
       "      <td>3552000.0</td>\n",
       "      <td>4.297920e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GShard (dense)</th>\n",
       "      <td>Language</td>\n",
       "      <td>Translation</td>\n",
       "      <td>Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu,...</td>\n",
       "      <td>SOTA Improvement</td>\n",
       "      <td>\"such a giant model can efficiently be trained...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/abs/2006.16668</td>\n",
       "      <td>523.0</td>\n",
       "      <td>GShard: Scaling Giant Models with Conditional ...</td>\n",
       "      <td>2020-06-30</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GShard (dense)</td>\n",
       "      <td>1024.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GShard (dense)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GShard (dense)</td>\n",
       "      <td>1032192.0</td>\n",
       "      <td>9.289728e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ALIGN</th>\n",
       "      <td>Multimodal</td>\n",
       "      <td>Representation Learning</td>\n",
       "      <td>Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Z...</td>\n",
       "      <td>SOTA Improvement</td>\n",
       "      <td>\"The aligned visual and language representatio...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/abs/2102.05918</td>\n",
       "      <td>1766.0</td>\n",
       "      <td>Scaling up visual and vision-language represen...</td>\n",
       "      <td>2021-06-11</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ALIGN</td>\n",
       "      <td>512.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ALIGN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ALIGN</td>\n",
       "      <td>177817.6</td>\n",
       "      <td>1.600358e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Megatron-Turing NLG 530B</th>\n",
       "      <td>Language</td>\n",
       "      <td>Language modelling</td>\n",
       "      <td>Shaden Smith, Mostofa Patwary, Brandon Norick,...</td>\n",
       "      <td>SOTA improvement</td>\n",
       "      <td>The 105-layer, transformer-based MT-NLG improv...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/abs/2201.11990</td>\n",
       "      <td>460.0</td>\n",
       "      <td>Using DeepSpeed and Megatron to Train Megatron...</td>\n",
       "      <td>2021-10-11</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Megatron-Turing NLG 530B</td>\n",
       "      <td>4480.0</td>\n",
       "      <td>0.3020</td>\n",
       "      <td>Megatron-Turing NLG 530B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Megatron-Turing NLG 530B</td>\n",
       "      <td>3449600.0</td>\n",
       "      <td>6.209280e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Yuan 1.0</th>\n",
       "      <td>Language</td>\n",
       "      <td>Language modelling</td>\n",
       "      <td>Shaohua Wu, Xudong Zhao, Tong Yu, Rongguo Zhan...</td>\n",
       "      <td>SOTA Improvement</td>\n",
       "      <td>\"The zero-shot average scores of both LM and P...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/abs/2110.04725</td>\n",
       "      <td>34.0</td>\n",
       "      <td>Yuan 1.0: Large-Scale Pre-trained Language Mod...</td>\n",
       "      <td>2021-10-12</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yuan 1.0</td>\n",
       "      <td>2128.0</td>\n",
       "      <td>0.4500</td>\n",
       "      <td>Yuan 1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yuan 1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gopher (280B)</th>\n",
       "      <td>Language</td>\n",
       "      <td>Language modelling</td>\n",
       "      <td>Jack W. Rae, Sebastian Borgeaud, Trevor Cai, K...</td>\n",
       "      <td>SOTA Improvement</td>\n",
       "      <td>\"These models are evaluated on 152 diverse tas...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/abs/2112.11446</td>\n",
       "      <td>736.0</td>\n",
       "      <td>Scaling Language Models: Methods, Analysis &amp; I...</td>\n",
       "      <td>2021-12-08</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Gopher (280B)</td>\n",
       "      <td>4096.0</td>\n",
       "      <td>0.3780</td>\n",
       "      <td>Gopher (280B)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Gopher (280B)</td>\n",
       "      <td>3768320.0</td>\n",
       "      <td>3.391488e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GLaM</th>\n",
       "      <td>Language</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nan Du, Yanping Huang, Andrew M. Dai, Simon To...</td>\n",
       "      <td>SOTA Improvement</td>\n",
       "      <td>\"As shown in Table 5, GLaM (64B/64E) is better...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/abs/2112.06905</td>\n",
       "      <td>314.0</td>\n",
       "      <td>GLaM: Efficient Scaling of Language Models wit...</td>\n",
       "      <td>2021-12-13</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GLaM</td>\n",
       "      <td>1024.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GLaM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GLaM</td>\n",
       "      <td>1398784.0</td>\n",
       "      <td>2.028237e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LaMDA</th>\n",
       "      <td>Language</td>\n",
       "      <td>Language modelling</td>\n",
       "      <td>Romal Thoppilan, Daniel De Freitas, Jamie Hall...</td>\n",
       "      <td>Historical significance</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/abs/2201.08239</td>\n",
       "      <td>891.0</td>\n",
       "      <td>LaMDA: Language Models for Dialog Applications</td>\n",
       "      <td>2022-02-10</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LaMDA</td>\n",
       "      <td>1024.0</td>\n",
       "      <td>0.5650</td>\n",
       "      <td>LaMDA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LaMDA</td>\n",
       "      <td>1418240.0</td>\n",
       "      <td>1.276416e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chinchilla</th>\n",
       "      <td>Language</td>\n",
       "      <td>Language modelling</td>\n",
       "      <td>Jordan Hoffmann, Sebastian Borgeaud, Arthur Me...</td>\n",
       "      <td>SOTA Improvement</td>\n",
       "      <td>Proposes new scaling law, with good empirical ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/abs/2203.15556</td>\n",
       "      <td>793.0</td>\n",
       "      <td>Training Compute-Optimal Large Language Models</td>\n",
       "      <td>2022-03-29</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Chinchilla</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Chinchilla</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Chinchilla</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PaLM (540B)</th>\n",
       "      <td>Language</td>\n",
       "      <td>Language modelling</td>\n",
       "      <td>Aakanksha Chowdhery, Sharan Narang, Jacob Devl...</td>\n",
       "      <td>SOTA Improvement</td>\n",
       "      <td>Demonstrates continued benefits of scaling, as...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/abs/2204.02311</td>\n",
       "      <td>2612.0</td>\n",
       "      <td>PaLM: Scaling Language Modeling with Pathways</td>\n",
       "      <td>2022-04-04</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PaLM (540B)</td>\n",
       "      <td>6144.0</td>\n",
       "      <td>0.4620</td>\n",
       "      <td>PaLM (540B)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PaLM (540B)</td>\n",
       "      <td>8404992.0</td>\n",
       "      <td>1.218724e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OPT-175B</th>\n",
       "      <td>Language</td>\n",
       "      <td>Language modelling</td>\n",
       "      <td>Susan Zhang, Stephen Roller, Naman Goyal, Mike...</td>\n",
       "      <td>Significant use</td>\n",
       "      <td>https://ai.meta.com/blog/opt-175b-large-langua...</td>\n",
       "      <td>Fully open-source</td>\n",
       "      <td>https://ai.facebook.com/blog/democratizing-acc...</td>\n",
       "      <td>1443.0</td>\n",
       "      <td>OPT: Open Pre-trained Transformer Language Models</td>\n",
       "      <td>2022-05-02</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OPT-175B</td>\n",
       "      <td>1024.0</td>\n",
       "      <td>0.4712</td>\n",
       "      <td>OPT-175B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OPT-175B</td>\n",
       "      <td>812544.0</td>\n",
       "      <td>1.470705e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Minerva (540B)</th>\n",
       "      <td>Language</td>\n",
       "      <td>Quantitative Reasoning Problems</td>\n",
       "      <td>Aitor Lewkowycz, Anders Andreassen, David Doha...</td>\n",
       "      <td>SOTA Improvement</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/abs/2206.14858</td>\n",
       "      <td>307.0</td>\n",
       "      <td>Solving Quantitative Reasoning Problems with L...</td>\n",
       "      <td>2022-06-29</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Minerva (540B)</td>\n",
       "      <td>1024.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Minerva (540B)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Minerva (540B)</td>\n",
       "      <td>712704.0</td>\n",
       "      <td>1.322066e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPT-3.5 (text-davinci-003)</th>\n",
       "      <td>Language</td>\n",
       "      <td>Language modelling</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SOTA Improvement,Historical significance,Signi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>API accessible</td>\n",
       "      <td>https://platform.openai.com/docs/models/gpt-3-5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-11-28</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GPT-3.5 (text-davinci-003)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GPT-3.5 (text-davinci-003)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GPT-3.5 (text-davinci-003)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LLaMA-65B</th>\n",
       "      <td>Language</td>\n",
       "      <td>Language modelling</td>\n",
       "      <td>Hugo Touvron, Thibaut Lavril, Gautier Izacard,...</td>\n",
       "      <td>Historical significance</td>\n",
       "      <td>Widely-used foundation model that has been ada...</td>\n",
       "      <td>Fully open-source</td>\n",
       "      <td>https://arxiv.org/abs/2302.13971</td>\n",
       "      <td>2695.0</td>\n",
       "      <td>LLaMA: Open and Efficient Foundation Language ...</td>\n",
       "      <td>2023-02-24</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LLaMA-65B</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>0.4746</td>\n",
       "      <td>LLaMA-65B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LLaMA-65B</td>\n",
       "      <td>1024000.0</td>\n",
       "      <td>1.392640e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPT-4</th>\n",
       "      <td>Multimodal</td>\n",
       "      <td>Language modelling</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>SOTA Improvement</td>\n",
       "      <td>See the paper, p.1: \"On a suite of traditional...</td>\n",
       "      <td>API accessible</td>\n",
       "      <td>https://arxiv.org/abs/2303.08774</td>\n",
       "      <td>1871.0</td>\n",
       "      <td>GPT-4 Technical Report</td>\n",
       "      <td>2023-03-15</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GPT-4</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>0.3400</td>\n",
       "      <td>GPT-4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GPT-4</td>\n",
       "      <td>57000000.0</td>\n",
       "      <td>8.265000e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PaLM 2</th>\n",
       "      <td>Language</td>\n",
       "      <td>Language modelling</td>\n",
       "      <td>Andrew M. Dai, David R. So, Dmitry Lepikhin, J...</td>\n",
       "      <td>SOTA Improvement</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Unreleased</td>\n",
       "      <td>https://ai.google/static/documents/palm2techre...</td>\n",
       "      <td>367.0</td>\n",
       "      <td>PaLM 2 Technical Report</td>\n",
       "      <td>2023-05-10</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PaLM 2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PaLM 2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PaLM 2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Claude 2</th>\n",
       "      <td>Language</td>\n",
       "      <td>Language modelling</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Historical significance</td>\n",
       "      <td>NaN</td>\n",
       "      <td>API accessible</td>\n",
       "      <td>https://www.anthropic.com/index/claude-2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-07-11</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Claude 2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Claude 2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Claude 2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Llama 2-70B</th>\n",
       "      <td>Language</td>\n",
       "      <td>Language modelling</td>\n",
       "      <td>Hugo Touvron, Louis Martin, Kevin Stone, Peter...</td>\n",
       "      <td>Historical significance,Significant use</td>\n",
       "      <td>Model has been open-sourced and frequently dow...</td>\n",
       "      <td>Fully open-source</td>\n",
       "      <td>https://ai.meta.com/research/publications/llam...</td>\n",
       "      <td>1122.0</td>\n",
       "      <td>Llama 2: Open Foundation and Fine-Tuned Chat M...</td>\n",
       "      <td>2023-07-18</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Llama 2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Llama 2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Meta’s Research Super Cluster</td>\n",
       "      <td>Llama 2-70B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Falcon 180B</th>\n",
       "      <td>Language</td>\n",
       "      <td>Language modelling</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SOTA Improvement</td>\n",
       "      <td>\"It's currently at the top of the Hugging Face...</td>\n",
       "      <td>Permissive license</td>\n",
       "      <td>https://falconllm.tii.ae/falcon-180b.html</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Falcon LLM - Falcon 180B</td>\n",
       "      <td>2023-09-06</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Falcon 180B</td>\n",
       "      <td>4096.0</td>\n",
       "      <td>0.1876</td>\n",
       "      <td>Falcon 180B</td>\n",
       "      <td>Amazon Web Services</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Falcon 180B</td>\n",
       "      <td>17694720.0</td>\n",
       "      <td>2.565734e+07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Domain                             Task  \\\n",
       "System                                                                    \n",
       "AlphaGo Fan                      Games                               Go   \n",
       "AlphaGo Lee                      Games                               Go   \n",
       "GNMT                          Language                      Translation   \n",
       "NASv3 (CIFAR-10)                Vision                              NaN   \n",
       "AlphaGo Master                   Games                               Go   \n",
       "JFT                             Vision                              NaN   \n",
       "OpenAI TI7 DOTA 1v1              Games                             DOTA   \n",
       "AlphaGo Zero                     Games                               Go   \n",
       "AlphaZero                        Games                              NaN   \n",
       "AmoebaNet-A (F=448)             Vision             Image classification   \n",
       "IMPALA                           Games                            Atari   \n",
       "ResNeXt-101 32x48d              Vision             Image classification   \n",
       "FTW                              Games                 Capture the flag   \n",
       "BigGAN-deep 512x512            Drawing                 Image generation   \n",
       "RoBERTa Large                 Language                              NaN   \n",
       "Megatron-BERT                 Language                              NaN   \n",
       "Megatron-LM (8.3B)            Language                              NaN   \n",
       "T5-3B                         Language              Text autocompletion   \n",
       "T5-11B                        Language              Text autocompletion   \n",
       "AlphaStar                        Games                        StarCraft   \n",
       "OpenAI Five                      Games                           Dota 2   \n",
       "OpenAI Five Rerun                Games                           Dota 2   \n",
       "Meena                         Language              Text autocompletion   \n",
       "Turing-NLG                    Language              Text autocompletion   \n",
       "GPT-3 175B (davinci)          Language              Text autocompletion   \n",
       "GShard (dense)                Language                      Translation   \n",
       "ALIGN                       Multimodal          Representation Learning   \n",
       "Megatron-Turing NLG 530B      Language               Language modelling   \n",
       "Yuan 1.0                      Language               Language modelling   \n",
       "Gopher (280B)                 Language               Language modelling   \n",
       "GLaM                          Language                              NaN   \n",
       "LaMDA                         Language               Language modelling   \n",
       "Chinchilla                    Language               Language modelling   \n",
       "PaLM (540B)                   Language               Language modelling   \n",
       "OPT-175B                      Language               Language modelling   \n",
       "Minerva (540B)                Language  Quantitative Reasoning Problems   \n",
       "GPT-3.5 (text-davinci-003)    Language               Language modelling   \n",
       "LLaMA-65B                     Language               Language modelling   \n",
       "GPT-4                       Multimodal               Language modelling   \n",
       "PaLM 2                        Language               Language modelling   \n",
       "Claude 2                      Language               Language modelling   \n",
       "Llama 2-70B                   Language               Language modelling   \n",
       "Falcon 180B                   Language               Language modelling   \n",
       "\n",
       "                                                                      Authors  \\\n",
       "System                                                                          \n",
       "AlphaGo Fan                 David Silver, Aja Huang, Chris J. Maddison, Ar...   \n",
       "AlphaGo Lee                 David Silver, Aja Huang, Chris J. Maddison, Ar...   \n",
       "GNMT                        Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc ...   \n",
       "NASv3 (CIFAR-10)                                      Barret Zoph, Quoc V. Le   \n",
       "AlphaGo Master              D Silver, J Schrittwieser, K Simonyan, I Anton...   \n",
       "JFT                         Chen Sun, Abhinav Shrivastava, Saurabh Singh, ...   \n",
       "OpenAI TI7 DOTA 1v1                                                       NaN   \n",
       "AlphaGo Zero                D Silver, J Schrittwieser, K Simonyan, I Anton...   \n",
       "AlphaZero                   D Silver, T Hubert, J Schrittwieser, I Antonoglou   \n",
       "AmoebaNet-A (F=448)         Esteban Real, Alok Aggarwal, Yanping Huang, Qu...   \n",
       "IMPALA                      Lasse Espeholt, Hubert Soyer, Remi Munos, Kare...   \n",
       "ResNeXt-101 32x48d          Dhruv Mahajan, Ross Girshick, Vignesh Ramanath...   \n",
       "FTW                         Max Jaderberg, Wojciech M. Czarnecki, Iain Dun...   \n",
       "BigGAN-deep 512x512                            A Brock, J Donahue, K Simonyan   \n",
       "RoBERTa Large               Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du,...   \n",
       "Megatron-BERT               Mohammad Shoeybi, Mostofa Patwary, Raul Puri, ...   \n",
       "Megatron-LM (8.3B)          Mohammad Shoeybi, Mostofa Patwary, Raul Puri, ...   \n",
       "T5-3B                       Colin Raffel, Noam Shazeer, Adam Roberts, Kath...   \n",
       "T5-11B                      Colin Raffel, Noam Shazeer, Adam Roberts, Kath...   \n",
       "AlphaStar                   Oriol Vinyals,Igor Babuschkin,Wojciech M. Czar...   \n",
       "OpenAI Five                 Christopher Berner, Greg Brockman, Brooke Chan...   \n",
       "OpenAI Five Rerun           Christopher Berner, Greg Brockman, Brooke Chan...   \n",
       "Meena                       Dongling Xiao, Han Zhang, Yukun Li, Yu Sun, Ha...   \n",
       "Turing-NLG                                                       Corby Rosset   \n",
       "GPT-3 175B (davinci)        Tom B. Brown, Benjamin Mann, Nick Ryder, Melan...   \n",
       "GShard (dense)              Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu,...   \n",
       "ALIGN                       Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Z...   \n",
       "Megatron-Turing NLG 530B    Shaden Smith, Mostofa Patwary, Brandon Norick,...   \n",
       "Yuan 1.0                    Shaohua Wu, Xudong Zhao, Tong Yu, Rongguo Zhan...   \n",
       "Gopher (280B)               Jack W. Rae, Sebastian Borgeaud, Trevor Cai, K...   \n",
       "GLaM                        Nan Du, Yanping Huang, Andrew M. Dai, Simon To...   \n",
       "LaMDA                       Romal Thoppilan, Daniel De Freitas, Jamie Hall...   \n",
       "Chinchilla                  Jordan Hoffmann, Sebastian Borgeaud, Arthur Me...   \n",
       "PaLM (540B)                 Aakanksha Chowdhery, Sharan Narang, Jacob Devl...   \n",
       "OPT-175B                    Susan Zhang, Stephen Roller, Naman Goyal, Mike...   \n",
       "Minerva (540B)              Aitor Lewkowycz, Anders Andreassen, David Doha...   \n",
       "GPT-3.5 (text-davinci-003)                                                NaN   \n",
       "LLaMA-65B                   Hugo Touvron, Thibaut Lavril, Gautier Izacard,...   \n",
       "GPT-4                                                                  OpenAI   \n",
       "PaLM 2                      Andrew M. Dai, David R. So, Dmitry Lepikhin, J...   \n",
       "Claude 2                                                                  NaN   \n",
       "Llama 2-70B                 Hugo Touvron, Louis Martin, Kevin Stone, Peter...   \n",
       "Falcon 180B                                                               NaN   \n",
       "\n",
       "                                                          Notability criteria  \\\n",
       "System                                                                          \n",
       "AlphaGo Fan                                                  SOTA improvement   \n",
       "AlphaGo Lee                                                      Highly cited   \n",
       "GNMT                                                             Highly cited   \n",
       "NASv3 (CIFAR-10)                                                 Highly cited   \n",
       "AlphaGo Master                                                   Highly cited   \n",
       "JFT                                                              Highly cited   \n",
       "OpenAI TI7 DOTA 1v1                  SOTA Improvement,Historical significance   \n",
       "AlphaGo Zero                                                     Highly cited   \n",
       "AlphaZero                                                        Highly cited   \n",
       "AmoebaNet-A (F=448)                                              Highly cited   \n",
       "IMPALA                                                       SOTA Improvement   \n",
       "ResNeXt-101 32x48d                                           SOTA Improvement   \n",
       "FTW                                                          SOTA Improvement   \n",
       "BigGAN-deep 512x512                                              Highly cited   \n",
       "RoBERTa Large                                                    Highly cited   \n",
       "Megatron-BERT                                                SOTA Improvement   \n",
       "Megatron-LM (8.3B)                                           SOTA Improvement   \n",
       "T5-3B                                                            Highly cited   \n",
       "T5-11B                                                           Highly cited   \n",
       "AlphaStar                                                        Highly cited   \n",
       "OpenAI Five                                                  SOTA improvement   \n",
       "OpenAI Five Rerun                                            SOTA improvement   \n",
       "Meena                                                        SOTA Improvement   \n",
       "Turing-NLG                                                   SOTA Improvement   \n",
       "GPT-3 175B (davinci)                                             Highly cited   \n",
       "GShard (dense)                                               SOTA Improvement   \n",
       "ALIGN                                                        SOTA Improvement   \n",
       "Megatron-Turing NLG 530B                                     SOTA improvement   \n",
       "Yuan 1.0                                                     SOTA Improvement   \n",
       "Gopher (280B)                                                SOTA Improvement   \n",
       "GLaM                                                         SOTA Improvement   \n",
       "LaMDA                                                 Historical significance   \n",
       "Chinchilla                                                   SOTA Improvement   \n",
       "PaLM (540B)                                                  SOTA Improvement   \n",
       "OPT-175B                                                      Significant use   \n",
       "Minerva (540B)                                               SOTA Improvement   \n",
       "GPT-3.5 (text-davinci-003)  SOTA Improvement,Historical significance,Signi...   \n",
       "LLaMA-65B                                             Historical significance   \n",
       "GPT-4                                                        SOTA Improvement   \n",
       "PaLM 2                                                       SOTA Improvement   \n",
       "Claude 2                                              Historical significance   \n",
       "Llama 2-70B                           Historical significance,Significant use   \n",
       "Falcon 180B                                                  SOTA Improvement   \n",
       "\n",
       "                                                    Notability criteria notes  \\\n",
       "System                                                                          \n",
       "AlphaGo Fan                                                               NaN   \n",
       "AlphaGo Lee                                                               NaN   \n",
       "GNMT                                                                      NaN   \n",
       "NASv3 (CIFAR-10)                                                          NaN   \n",
       "AlphaGo Master                                                            NaN   \n",
       "JFT                                                                       NaN   \n",
       "OpenAI TI7 DOTA 1v1                                                       NaN   \n",
       "AlphaGo Zero                                                              NaN   \n",
       "AlphaZero                                                                 NaN   \n",
       "AmoebaNet-A (F=448)                                                       NaN   \n",
       "IMPALA                      \"IMPALA is able to achieve better performance ...   \n",
       "ResNeXt-101 32x48d          \"We show improvements on several image classif...   \n",
       "FTW                         \"In this work, we demonstrate for the first ti...   \n",
       "BigGAN-deep 512x512                                                       NaN   \n",
       "RoBERTa Large                                                             NaN   \n",
       "Megatron-BERT               \"Our BERT model achieves SOTA results on the R...   \n",
       "Megatron-LM (8.3B)          \"Using the GPT-2 model we achieve SOTA results...   \n",
       "T5-3B                                                                     NaN   \n",
       "T5-11B                                                                    NaN   \n",
       "AlphaStar                                                                 NaN   \n",
       "OpenAI Five                 \"On April 13th, 2019, OpenAI Five became the f...   \n",
       "OpenAI Five Rerun           \"On April 13th, 2019, OpenAI Five became the f...   \n",
       "Meena                       \"We also propose a human evaluation metric cal...   \n",
       "Turing-NLG                  from paper: \"Turing Natural Language Generatio...   \n",
       "GPT-3 175B (davinci)                                                      NaN   \n",
       "GShard (dense)              \"such a giant model can efficiently be trained...   \n",
       "ALIGN                       \"The aligned visual and language representatio...   \n",
       "Megatron-Turing NLG 530B    The 105-layer, transformer-based MT-NLG improv...   \n",
       "Yuan 1.0                    \"The zero-shot average scores of both LM and P...   \n",
       "Gopher (280B)               \"These models are evaluated on 152 diverse tas...   \n",
       "GLaM                        \"As shown in Table 5, GLaM (64B/64E) is better...   \n",
       "LaMDA                                                                     NaN   \n",
       "Chinchilla                  Proposes new scaling law, with good empirical ...   \n",
       "PaLM (540B)                 Demonstrates continued benefits of scaling, as...   \n",
       "OPT-175B                    https://ai.meta.com/blog/opt-175b-large-langua...   \n",
       "Minerva (540B)                                                            NaN   \n",
       "GPT-3.5 (text-davinci-003)                                                NaN   \n",
       "LLaMA-65B                   Widely-used foundation model that has been ada...   \n",
       "GPT-4                       See the paper, p.1: \"On a suite of traditional...   \n",
       "PaLM 2                                                                    NaN   \n",
       "Claude 2                                                                  NaN   \n",
       "Llama 2-70B                 Model has been open-sourced and frequently dow...   \n",
       "Falcon 180B                 \"It's currently at the top of the Hugging Face...   \n",
       "\n",
       "                                   Open-source  \\\n",
       "System                                           \n",
       "AlphaGo Fan                                NaN   \n",
       "AlphaGo Lee                                NaN   \n",
       "GNMT                                       NaN   \n",
       "NASv3 (CIFAR-10)                           NaN   \n",
       "AlphaGo Master                             NaN   \n",
       "JFT                                        NaN   \n",
       "OpenAI TI7 DOTA 1v1                        NaN   \n",
       "AlphaGo Zero                               NaN   \n",
       "AlphaZero                                  NaN   \n",
       "AmoebaNet-A (F=448)                        NaN   \n",
       "IMPALA                                     NaN   \n",
       "ResNeXt-101 32x48d                         NaN   \n",
       "FTW                                        NaN   \n",
       "BigGAN-deep 512x512         Permissive license   \n",
       "RoBERTa Large                              NaN   \n",
       "Megatron-BERT                              NaN   \n",
       "Megatron-LM (8.3B)                         NaN   \n",
       "T5-3B                                      NaN   \n",
       "T5-11B                                     NaN   \n",
       "AlphaStar                                  NaN   \n",
       "OpenAI Five                                NaN   \n",
       "OpenAI Five Rerun                          NaN   \n",
       "Meena                                      NaN   \n",
       "Turing-NLG                                 NaN   \n",
       "GPT-3 175B (davinci)            API accessible   \n",
       "GShard (dense)                             NaN   \n",
       "ALIGN                                      NaN   \n",
       "Megatron-Turing NLG 530B                   NaN   \n",
       "Yuan 1.0                                   NaN   \n",
       "Gopher (280B)                              NaN   \n",
       "GLaM                                       NaN   \n",
       "LaMDA                                      NaN   \n",
       "Chinchilla                                 NaN   \n",
       "PaLM (540B)                                NaN   \n",
       "OPT-175B                     Fully open-source   \n",
       "Minerva (540B)                             NaN   \n",
       "GPT-3.5 (text-davinci-003)      API accessible   \n",
       "LLaMA-65B                    Fully open-source   \n",
       "GPT-4                           API accessible   \n",
       "PaLM 2                              Unreleased   \n",
       "Claude 2                        API accessible   \n",
       "Llama 2-70B                  Fully open-source   \n",
       "Falcon 180B                 Permissive license   \n",
       "\n",
       "                                                                         Link  \\\n",
       "System                                                                          \n",
       "AlphaGo Fan                 https://www.nature.com/articles/nature24270.ep...   \n",
       "AlphaGo Lee                       https://www.nature.com/articles/nature16961   \n",
       "GNMT                                         https://arxiv.org/abs/1609.08144   \n",
       "NASv3 (CIFAR-10)                             https://arxiv.org/abs/1611.01578   \n",
       "AlphaGo Master              https://www.researchgate.net/publication/32047...   \n",
       "JFT                                          https://arxiv.org/abs/1707.02968   \n",
       "OpenAI TI7 DOTA 1v1                        https://openai.com/research/dota-2   \n",
       "AlphaGo Zero                https://www.researchgate.net/publication/32047...   \n",
       "AlphaZero                                    https://arxiv.org/abs/1712.01815   \n",
       "AmoebaNet-A (F=448)                          https://arxiv.org/abs/1802.01548   \n",
       "IMPALA                                       https://arxiv.org/abs/1802.01561   \n",
       "ResNeXt-101 32x48d                           https://arxiv.org/abs/1805.00932   \n",
       "FTW                                          https://arxiv.org/abs/1807.01281   \n",
       "BigGAN-deep 512x512                          https://arxiv.org/abs/1809.11096   \n",
       "RoBERTa Large                                https://arxiv.org/abs/1907.11692   \n",
       "Megatron-BERT                                https://arxiv.org/abs/1909.08053   \n",
       "Megatron-LM (8.3B)                           https://arxiv.org/abs/1909.08053   \n",
       "T5-3B                                        https://arxiv.org/abs/1910.10683   \n",
       "T5-11B                                       https://arxiv.org/abs/1910.10683   \n",
       "AlphaStar                   https://www.deepmind.com/blog/alphastar-grandm...   \n",
       "OpenAI Five                                  https://arxiv.org/abs/1912.06680   \n",
       "OpenAI Five Rerun                           https://cdn.openai.com/dota-2.pdf   \n",
       "Meena                                        https://arxiv.org/abs/2001.09977   \n",
       "Turing-NLG                  https://www.microsoft.com/en-us/research/blog/...   \n",
       "GPT-3 175B (davinci)                         https://arxiv.org/abs/2005.14165   \n",
       "GShard (dense)                               https://arxiv.org/abs/2006.16668   \n",
       "ALIGN                                        https://arxiv.org/abs/2102.05918   \n",
       "Megatron-Turing NLG 530B                     https://arxiv.org/abs/2201.11990   \n",
       "Yuan 1.0                                     https://arxiv.org/abs/2110.04725   \n",
       "Gopher (280B)                                https://arxiv.org/abs/2112.11446   \n",
       "GLaM                                         https://arxiv.org/abs/2112.06905   \n",
       "LaMDA                                        https://arxiv.org/abs/2201.08239   \n",
       "Chinchilla                                   https://arxiv.org/abs/2203.15556   \n",
       "PaLM (540B)                                  https://arxiv.org/abs/2204.02311   \n",
       "OPT-175B                    https://ai.facebook.com/blog/democratizing-acc...   \n",
       "Minerva (540B)                               https://arxiv.org/abs/2206.14858   \n",
       "GPT-3.5 (text-davinci-003)    https://platform.openai.com/docs/models/gpt-3-5   \n",
       "LLaMA-65B                                    https://arxiv.org/abs/2302.13971   \n",
       "GPT-4                                        https://arxiv.org/abs/2303.08774   \n",
       "PaLM 2                      https://ai.google/static/documents/palm2techre...   \n",
       "Claude 2                             https://www.anthropic.com/index/claude-2   \n",
       "Llama 2-70B                 https://ai.meta.com/research/publications/llam...   \n",
       "Falcon 180B                         https://falconllm.tii.ae/falcon-180b.html   \n",
       "\n",
       "                            Citations  \\\n",
       "System                                  \n",
       "AlphaGo Fan                   14389.0   \n",
       "AlphaGo Lee                   14389.0   \n",
       "GNMT                           5948.0   \n",
       "NASv3 (CIFAR-10)               4569.0   \n",
       "AlphaGo Master                 7831.0   \n",
       "JFT                            1923.0   \n",
       "OpenAI TI7 DOTA 1v1               0.0   \n",
       "AlphaGo Zero                   7831.0   \n",
       "AlphaZero                      1343.0   \n",
       "AmoebaNet-A (F=448)            2505.0   \n",
       "IMPALA                         1288.0   \n",
       "ResNeXt-101 32x48d             1176.0   \n",
       "FTW                             571.0   \n",
       "BigGAN-deep 512x512            4163.0   \n",
       "RoBERTa Large                 15901.0   \n",
       "Megatron-BERT                  1003.0   \n",
       "Megatron-LM (8.3B)             1003.0   \n",
       "T5-3B                         10800.0   \n",
       "T5-11B                        10800.0   \n",
       "AlphaStar                      2681.0   \n",
       "OpenAI Five                    1303.0   \n",
       "OpenAI Five Rerun              1303.0   \n",
       "Meena                           747.0   \n",
       "Turing-NLG                      114.0   \n",
       "GPT-3 175B (davinci)          18144.0   \n",
       "GShard (dense)                  523.0   \n",
       "ALIGN                          1766.0   \n",
       "Megatron-Turing NLG 530B        460.0   \n",
       "Yuan 1.0                         34.0   \n",
       "Gopher (280B)                   736.0   \n",
       "GLaM                            314.0   \n",
       "LaMDA                           891.0   \n",
       "Chinchilla                      793.0   \n",
       "PaLM (540B)                    2612.0   \n",
       "OPT-175B                       1443.0   \n",
       "Minerva (540B)                  307.0   \n",
       "GPT-3.5 (text-davinci-003)        NaN   \n",
       "LLaMA-65B                      2695.0   \n",
       "GPT-4                          1871.0   \n",
       "PaLM 2                          367.0   \n",
       "Claude 2                          0.0   \n",
       "Llama 2-70B                    1122.0   \n",
       "Falcon 180B                       0.0   \n",
       "\n",
       "                                                                    Reference  \\\n",
       "System                                                                          \n",
       "AlphaGo Fan                 Mastering the game of Go with deep neural netw...   \n",
       "AlphaGo Lee                 Mastering the game of Go with deep neural netw...   \n",
       "GNMT                        Google's Neural Machine Translation System: Br...   \n",
       "NASv3 (CIFAR-10)            Neural Architecture Search with Reinforcement ...   \n",
       "AlphaGo Master               Mastering the game of Go without human knowledge   \n",
       "JFT                         Revisiting Unreasonable Effectiveness of Data ...   \n",
       "OpenAI TI7 DOTA 1v1                                                    Dota 2   \n",
       "AlphaGo Zero                 Mastering the game of Go without human knowledge   \n",
       "AlphaZero                   Mastering Chess and Shogi by Self-Play with a ...   \n",
       "AmoebaNet-A (F=448)         Regularized Evolution for Image Classifier Arc...   \n",
       "IMPALA                      IMPALA: Scalable Distributed Deep-RL with Impo...   \n",
       "ResNeXt-101 32x48d          Exploring the Limits of Weakly Supervised Pret...   \n",
       "FTW                         Human-level performance in first-person multip...   \n",
       "BigGAN-deep 512x512         Large Scale GAN Training for High Fidelity Nat...   \n",
       "RoBERTa Large               RoBERTa: A Robustly Optimized BERT Pretraining...   \n",
       "Megatron-BERT               Megatron-LM: Training Multi-Billion Parameter ...   \n",
       "Megatron-LM (8.3B)          Megatron-LM: Training Multi-Billion Parameter ...   \n",
       "T5-3B                       Exploring the Limits of Transfer Learning with...   \n",
       "T5-11B                      Exploring the Limits of Transfer Learning with...   \n",
       "AlphaStar                   Grandmaster level in StarCraft II using multi-...   \n",
       "OpenAI Five                 Dota 2 with Large Scale Deep Reinforcement Lea...   \n",
       "OpenAI Five Rerun           Dota 2 with Large Scale Deep Reinforcement Lea...   \n",
       "Meena                                Towards a Human-like Open-Domain Chatbot   \n",
       "Turing-NLG                  Turing-NLG: A 17-billion-parameter language mo...   \n",
       "GPT-3 175B (davinci)                    Language models are Few-Shot Learners   \n",
       "GShard (dense)              GShard: Scaling Giant Models with Conditional ...   \n",
       "ALIGN                       Scaling up visual and vision-language represen...   \n",
       "Megatron-Turing NLG 530B    Using DeepSpeed and Megatron to Train Megatron...   \n",
       "Yuan 1.0                    Yuan 1.0: Large-Scale Pre-trained Language Mod...   \n",
       "Gopher (280B)               Scaling Language Models: Methods, Analysis & I...   \n",
       "GLaM                        GLaM: Efficient Scaling of Language Models wit...   \n",
       "LaMDA                          LaMDA: Language Models for Dialog Applications   \n",
       "Chinchilla                     Training Compute-Optimal Large Language Models   \n",
       "PaLM (540B)                     PaLM: Scaling Language Modeling with Pathways   \n",
       "OPT-175B                    OPT: Open Pre-trained Transformer Language Models   \n",
       "Minerva (540B)              Solving Quantitative Reasoning Problems with L...   \n",
       "GPT-3.5 (text-davinci-003)                                                NaN   \n",
       "LLaMA-65B                   LLaMA: Open and Efficient Foundation Language ...   \n",
       "GPT-4                                                  GPT-4 Technical Report   \n",
       "PaLM 2                                                PaLM 2 Technical Report   \n",
       "Claude 2                                                                  NaN   \n",
       "Llama 2-70B                 Llama 2: Open Foundation and Fine-Tuned Chat M...   \n",
       "Falcon 180B                                          Falcon LLM - Falcon 180B   \n",
       "\n",
       "                           Publication date  ... Finetune compute notes  \\\n",
       "System                                       ...                          \n",
       "AlphaGo Fan                      2015-10-01  ...                    NaN   \n",
       "AlphaGo Lee                      2016-01-27  ...                    NaN   \n",
       "GNMT                             2016-09-26  ...                    NaN   \n",
       "NASv3 (CIFAR-10)                 2016-11-05  ...                    NaN   \n",
       "AlphaGo Master                   2017-01-01  ...                    NaN   \n",
       "JFT                              2017-07-10  ...                    NaN   \n",
       "OpenAI TI7 DOTA 1v1              2017-08-11  ...                    NaN   \n",
       "AlphaGo Zero                     2017-10-18  ...                    NaN   \n",
       "AlphaZero                        2017-12-05  ...                    NaN   \n",
       "AmoebaNet-A (F=448)              2018-02-05  ...                    NaN   \n",
       "IMPALA                           2018-02-05  ...                    NaN   \n",
       "ResNeXt-101 32x48d               2018-05-02  ...                    NaN   \n",
       "FTW                              2018-07-03  ...                    NaN   \n",
       "BigGAN-deep 512x512              2018-09-28  ...                    NaN   \n",
       "RoBERTa Large                    2019-07-01  ...                    NaN   \n",
       "Megatron-BERT                    2019-09-17  ...                    NaN   \n",
       "Megatron-LM (8.3B)               2019-09-17  ...                    NaN   \n",
       "T5-3B                            2019-10-23  ...                    NaN   \n",
       "T5-11B                           2019-10-23  ...                    NaN   \n",
       "AlphaStar                        2019-10-30  ...                    NaN   \n",
       "OpenAI Five                      2019-12-13  ...                    NaN   \n",
       "OpenAI Five Rerun                2019-12-13  ...                    NaN   \n",
       "Meena                            2020-01-28  ...                    NaN   \n",
       "Turing-NLG                       2020-02-13  ...                    NaN   \n",
       "GPT-3 175B (davinci)             2020-05-28  ...                    NaN   \n",
       "GShard (dense)                   2020-06-30  ...                    NaN   \n",
       "ALIGN                            2021-06-11  ...                    NaN   \n",
       "Megatron-Turing NLG 530B         2021-10-11  ...                    NaN   \n",
       "Yuan 1.0                         2021-10-12  ...                    NaN   \n",
       "Gopher (280B)                    2021-12-08  ...                    NaN   \n",
       "GLaM                             2021-12-13  ...                    NaN   \n",
       "LaMDA                            2022-02-10  ...                    NaN   \n",
       "Chinchilla                       2022-03-29  ...                    NaN   \n",
       "PaLM (540B)                      2022-04-04  ...                    NaN   \n",
       "OPT-175B                         2022-05-02  ...                    NaN   \n",
       "Minerva (540B)                   2022-06-29  ...                    NaN   \n",
       "GPT-3.5 (text-davinci-003)       2022-11-28  ...                    NaN   \n",
       "LLaMA-65B                        2023-02-24  ...                    NaN   \n",
       "GPT-4                            2023-03-15  ...                    NaN   \n",
       "PaLM 2                           2023-05-10  ...                    NaN   \n",
       "Claude 2                         2023-07-11  ...                    NaN   \n",
       "Llama 2-70B                      2023-07-18  ...                    NaN   \n",
       "Falcon 180B                      2023-09-06  ...                    NaN   \n",
       "\n",
       "                                    Authors by country  Hardware quantity  \\\n",
       "System                                                                      \n",
       "AlphaGo Fan                                AlphaGo Fan                NaN   \n",
       "AlphaGo Lee                                AlphaGo Lee                NaN   \n",
       "GNMT                                              GNMT               96.0   \n",
       "NASv3 (CIFAR-10)                      NASv3 (CIFAR-10)              800.0   \n",
       "AlphaGo Master                          AlphaGo Master                NaN   \n",
       "JFT                                                JFT               50.0   \n",
       "OpenAI TI7 DOTA 1v1                OpenAI TI7 DOTA 1v1                NaN   \n",
       "AlphaGo Zero                              AlphaGo Zero                NaN   \n",
       "AlphaZero                                    AlphaZero               64.0   \n",
       "AmoebaNet-A (F=448)                AmoebaNet-A (F=448)              450.0   \n",
       "IMPALA                                          IMPALA                1.0   \n",
       "ResNeXt-101 32x48d                  ResNeXt-101 32x48d              336.0   \n",
       "FTW                                                FTW                NaN   \n",
       "BigGAN-deep 512x512                BigGAN-deep 512x512              256.0   \n",
       "RoBERTa Large                            RoBERTa Large             1024.0   \n",
       "Megatron-BERT                            Megatron-BERT              512.0   \n",
       "Megatron-LM (8.3B)                  Megatron-LM (8.3B)              512.0   \n",
       "T5-3B                                            T5-3B                NaN   \n",
       "T5-11B                                          T5-11B              512.0   \n",
       "AlphaStar                                    AlphaStar              384.0   \n",
       "OpenAI Five                                OpenAI Five             1536.0   \n",
       "OpenAI Five Rerun                    OpenAI Five Rerun              512.0   \n",
       "Meena                                            Meena             1024.0   \n",
       "Turing-NLG                                  Turing-NLG              256.0   \n",
       "GPT-3 175B (davinci)              GPT-3 175B (davinci)            10000.0   \n",
       "GShard (dense)                          GShard (dense)             1024.0   \n",
       "ALIGN                                            ALIGN              512.0   \n",
       "Megatron-Turing NLG 530B      Megatron-Turing NLG 530B             4480.0   \n",
       "Yuan 1.0                                      Yuan 1.0             2128.0   \n",
       "Gopher (280B)                            Gopher (280B)             4096.0   \n",
       "GLaM                                              GLaM             1024.0   \n",
       "LaMDA                                            LaMDA             1024.0   \n",
       "Chinchilla                                  Chinchilla                NaN   \n",
       "PaLM (540B)                                PaLM (540B)             6144.0   \n",
       "OPT-175B                                      OPT-175B             1024.0   \n",
       "Minerva (540B)                          Minerva (540B)             1024.0   \n",
       "GPT-3.5 (text-davinci-003)  GPT-3.5 (text-davinci-003)                NaN   \n",
       "LLaMA-65B                                    LLaMA-65B             2048.0   \n",
       "GPT-4                                            GPT-4            25000.0   \n",
       "PaLM 2                                          PaLM 2                NaN   \n",
       "Claude 2                                      Claude 2                NaN   \n",
       "Llama 2-70B                                    Llama 2                NaN   \n",
       "Falcon 180B                                Falcon 180B             4096.0   \n",
       "\n",
       "                           Hardware utilization        Training cost trends  \\\n",
       "System                                                                        \n",
       "AlphaGo Fan                                 NaN                 AlphaGo Fan   \n",
       "AlphaGo Lee                                 NaN                 AlphaGo Lee   \n",
       "GNMT                                        NaN                        GNMT   \n",
       "NASv3 (CIFAR-10)                            NaN            NASv3 (CIFAR-10)   \n",
       "AlphaGo Master                              NaN              AlphaGo Master   \n",
       "JFT                                         NaN                         JFT   \n",
       "OpenAI TI7 DOTA 1v1                         NaN         OpenAI TI7 DOTA 1v1   \n",
       "AlphaGo Zero                                NaN                AlphaGo Zero   \n",
       "AlphaZero                                   NaN                   AlphaZero   \n",
       "AmoebaNet-A (F=448)                         NaN         AmoebaNet-A (F=448)   \n",
       "IMPALA                                      NaN                      IMPALA   \n",
       "ResNeXt-101 32x48d                          NaN          ResNeXt-101 32x48d   \n",
       "FTW                                         NaN                         FTW   \n",
       "BigGAN-deep 512x512                         NaN         BigGAN-deep 512x512   \n",
       "RoBERTa Large                               NaN               RoBERTa Large   \n",
       "Megatron-BERT                            0.2269               Megatron-BERT   \n",
       "Megatron-LM (8.3B)                       0.1162          Megatron-LM (8.3B)   \n",
       "T5-3B                                       NaN                       T5-3B   \n",
       "T5-11B                                   0.3707                      T5-11B   \n",
       "AlphaStar                                   NaN                   AlphaStar   \n",
       "OpenAI Five                                 NaN                 OpenAI Five   \n",
       "OpenAI Five Rerun                           NaN           OpenAI Five Rerun   \n",
       "Meena                                    0.3439                       Meena   \n",
       "Turing-NLG                                  NaN                  Turing-NLG   \n",
       "GPT-3 175B (davinci)                     0.2196        GPT-3 175B (davinci)   \n",
       "GShard (dense)                              NaN              GShard (dense)   \n",
       "ALIGN                                       NaN                       ALIGN   \n",
       "Megatron-Turing NLG 530B                 0.3020    Megatron-Turing NLG 530B   \n",
       "Yuan 1.0                                 0.4500                    Yuan 1.0   \n",
       "Gopher (280B)                            0.3780               Gopher (280B)   \n",
       "GLaM                                        NaN                        GLaM   \n",
       "LaMDA                                    0.5650                       LaMDA   \n",
       "Chinchilla                                  NaN                  Chinchilla   \n",
       "PaLM (540B)                              0.4620                 PaLM (540B)   \n",
       "OPT-175B                                 0.4712                    OPT-175B   \n",
       "Minerva (540B)                              NaN              Minerva (540B)   \n",
       "GPT-3.5 (text-davinci-003)                  NaN  GPT-3.5 (text-davinci-003)   \n",
       "LLaMA-65B                                0.4746                   LLaMA-65B   \n",
       "GPT-4                                    0.3400                       GPT-4   \n",
       "PaLM 2                                      NaN                      PaLM 2   \n",
       "Claude 2                                    NaN                    Claude 2   \n",
       "Llama 2-70B                                 NaN                     Llama 2   \n",
       "Falcon 180B                              0.1876                 Falcon 180B   \n",
       "\n",
       "                           Training cloud compute vendor  \\\n",
       "System                                                     \n",
       "AlphaGo Fan                                          NaN   \n",
       "AlphaGo Lee                                          NaN   \n",
       "GNMT                                                 NaN   \n",
       "NASv3 (CIFAR-10)                                     NaN   \n",
       "AlphaGo Master                                       NaN   \n",
       "JFT                                                  NaN   \n",
       "OpenAI TI7 DOTA 1v1                                  NaN   \n",
       "AlphaGo Zero                                         NaN   \n",
       "AlphaZero                                            NaN   \n",
       "AmoebaNet-A (F=448)                                  NaN   \n",
       "IMPALA                                               NaN   \n",
       "ResNeXt-101 32x48d                                   NaN   \n",
       "FTW                                                  NaN   \n",
       "BigGAN-deep 512x512                                  NaN   \n",
       "RoBERTa Large                                        NaN   \n",
       "Megatron-BERT                                        NaN   \n",
       "Megatron-LM (8.3B)                                   NaN   \n",
       "T5-3B                                                NaN   \n",
       "T5-11B                                               NaN   \n",
       "AlphaStar                                            NaN   \n",
       "OpenAI Five                                          NaN   \n",
       "OpenAI Five Rerun                                    NaN   \n",
       "Meena                                                NaN   \n",
       "Turing-NLG                                           NaN   \n",
       "GPT-3 175B (davinci)                                 NaN   \n",
       "GShard (dense)                                       NaN   \n",
       "ALIGN                                                NaN   \n",
       "Megatron-Turing NLG 530B                             NaN   \n",
       "Yuan 1.0                                             NaN   \n",
       "Gopher (280B)                                        NaN   \n",
       "GLaM                                                 NaN   \n",
       "LaMDA                                                NaN   \n",
       "Chinchilla                                           NaN   \n",
       "PaLM (540B)                                          NaN   \n",
       "OPT-175B                                             NaN   \n",
       "Minerva (540B)                                       NaN   \n",
       "GPT-3.5 (text-davinci-003)                           NaN   \n",
       "LLaMA-65B                                            NaN   \n",
       "GPT-4                                                NaN   \n",
       "PaLM 2                                               NaN   \n",
       "Claude 2                                             NaN   \n",
       "Llama 2-70B                                          NaN   \n",
       "Falcon 180B                          Amazon Web Services   \n",
       "\n",
       "                                     Training data center  \\\n",
       "System                                                      \n",
       "AlphaGo Fan                                           NaN   \n",
       "AlphaGo Lee                                           NaN   \n",
       "GNMT                                                  NaN   \n",
       "NASv3 (CIFAR-10)                                      NaN   \n",
       "AlphaGo Master                                        NaN   \n",
       "JFT                                                   NaN   \n",
       "OpenAI TI7 DOTA 1v1                                   NaN   \n",
       "AlphaGo Zero                                          NaN   \n",
       "AlphaZero                                             NaN   \n",
       "AmoebaNet-A (F=448)                                   NaN   \n",
       "IMPALA                                                NaN   \n",
       "ResNeXt-101 32x48d                                    NaN   \n",
       "FTW                                                   NaN   \n",
       "BigGAN-deep 512x512                                   NaN   \n",
       "RoBERTa Large                                         NaN   \n",
       "Megatron-BERT                                         NaN   \n",
       "Megatron-LM (8.3B)                                    NaN   \n",
       "T5-3B                                                 NaN   \n",
       "T5-11B                                                NaN   \n",
       "AlphaStar                                             NaN   \n",
       "OpenAI Five                                           NaN   \n",
       "OpenAI Five Rerun                                     NaN   \n",
       "Meena                                                 NaN   \n",
       "Turing-NLG                                            NaN   \n",
       "GPT-3 175B (davinci)                                  NaN   \n",
       "GShard (dense)                                        NaN   \n",
       "ALIGN                                                 NaN   \n",
       "Megatron-Turing NLG 530B                              NaN   \n",
       "Yuan 1.0                                              NaN   \n",
       "Gopher (280B)                                         NaN   \n",
       "GLaM                                                  NaN   \n",
       "LaMDA                                                 NaN   \n",
       "Chinchilla                                            NaN   \n",
       "PaLM (540B)                                           NaN   \n",
       "OPT-175B                                              NaN   \n",
       "Minerva (540B)                                        NaN   \n",
       "GPT-3.5 (text-davinci-003)                            NaN   \n",
       "LLaMA-65B                                             NaN   \n",
       "GPT-4                                                 NaN   \n",
       "PaLM 2                                                NaN   \n",
       "Claude 2                                              NaN   \n",
       "Llama 2-70B                 Meta’s Research Super Cluster   \n",
       "Falcon 180B                                           NaN   \n",
       "\n",
       "                                                System  \\\n",
       "System                                                   \n",
       "AlphaGo Fan                                AlphaGo Fan   \n",
       "AlphaGo Lee                                AlphaGo Lee   \n",
       "GNMT                                              GNMT   \n",
       "NASv3 (CIFAR-10)                      NASv3 (CIFAR-10)   \n",
       "AlphaGo Master                          AlphaGo Master   \n",
       "JFT                                                JFT   \n",
       "OpenAI TI7 DOTA 1v1                OpenAI TI7 DOTA 1v1   \n",
       "AlphaGo Zero                              AlphaGo Zero   \n",
       "AlphaZero                                    AlphaZero   \n",
       "AmoebaNet-A (F=448)                AmoebaNet-A (F=448)   \n",
       "IMPALA                                          IMPALA   \n",
       "ResNeXt-101 32x48d                  ResNeXt-101 32x48d   \n",
       "FTW                                                FTW   \n",
       "BigGAN-deep 512x512                BigGAN-deep 512x512   \n",
       "RoBERTa Large                            RoBERTa Large   \n",
       "Megatron-BERT                            Megatron-BERT   \n",
       "Megatron-LM (8.3B)                  Megatron-LM (8.3B)   \n",
       "T5-3B                                            T5-3B   \n",
       "T5-11B                                          T5-11B   \n",
       "AlphaStar                                    AlphaStar   \n",
       "OpenAI Five                                OpenAI Five   \n",
       "OpenAI Five Rerun                    OpenAI Five Rerun   \n",
       "Meena                                            Meena   \n",
       "Turing-NLG                                  Turing-NLG   \n",
       "GPT-3 175B (davinci)              GPT-3 175B (davinci)   \n",
       "GShard (dense)                          GShard (dense)   \n",
       "ALIGN                                            ALIGN   \n",
       "Megatron-Turing NLG 530B      Megatron-Turing NLG 530B   \n",
       "Yuan 1.0                                      Yuan 1.0   \n",
       "Gopher (280B)                            Gopher (280B)   \n",
       "GLaM                                              GLaM   \n",
       "LaMDA                                            LaMDA   \n",
       "Chinchilla                                  Chinchilla   \n",
       "PaLM (540B)                                PaLM (540B)   \n",
       "OPT-175B                                      OPT-175B   \n",
       "Minerva (540B)                          Minerva (540B)   \n",
       "GPT-3.5 (text-davinci-003)  GPT-3.5 (text-davinci-003)   \n",
       "LLaMA-65B                                    LLaMA-65B   \n",
       "GPT-4                                            GPT-4   \n",
       "PaLM 2                                          PaLM 2   \n",
       "Claude 2                                      Claude 2   \n",
       "Llama 2-70B                                Llama 2-70B   \n",
       "Falcon 180B                                Falcon 180B   \n",
       "\n",
       "                            Training time (chip hours)          Cost  \n",
       "System                                                                \n",
       "AlphaGo Fan                                        NaN           NaN  \n",
       "AlphaGo Lee                                        NaN           NaN  \n",
       "GNMT                                          414720.0  5.806080e+05  \n",
       "NASv3 (CIFAR-10)                                   NaN           NaN  \n",
       "AlphaGo Master                                     NaN           NaN  \n",
       "JFT                                            72000.0  1.008000e+05  \n",
       "OpenAI TI7 DOTA 1v1                                NaN           NaN  \n",
       "AlphaGo Zero                                       NaN           NaN  \n",
       "AlphaZero                                          NaN           NaN  \n",
       "AmoebaNet-A (F=448)                            75600.0  3.985070e+03  \n",
       "IMPALA                                           100.0  1.460000e+02  \n",
       "ResNeXt-101 32x48d                                 NaN           NaN  \n",
       "FTW                                                NaN           NaN  \n",
       "BigGAN-deep 512x512                            12288.0  2.457600e+04  \n",
       "RoBERTa Large                                 122880.0  1.486848e+05  \n",
       "Megatron-BERT                                 712704.0  7.492494e+04  \n",
       "Megatron-LM (8.3B)                            167424.0  2.025830e+05  \n",
       "T5-3B                                              NaN           NaN  \n",
       "T5-11B                                        246732.8  2.220595e+05  \n",
       "AlphaStar                                     405504.0  3.649536e+05  \n",
       "OpenAI Five                                 10911744.0           NaN  \n",
       "OpenAI Five Rerun                                  NaN           NaN  \n",
       "Meena                                         737280.0  6.635520e+05  \n",
       "Turing-NLG                                         NaN           NaN  \n",
       "GPT-3 175B (davinci)                         3552000.0  4.297920e+06  \n",
       "GShard (dense)                               1032192.0  9.289728e+05  \n",
       "ALIGN                                         177817.6  1.600358e+05  \n",
       "Megatron-Turing NLG 530B                     3449600.0  6.209280e+06  \n",
       "Yuan 1.0                                           NaN           NaN  \n",
       "Gopher (280B)                                3768320.0  3.391488e+06  \n",
       "GLaM                                         1398784.0  2.028237e+06  \n",
       "LaMDA                                        1418240.0  1.276416e+06  \n",
       "Chinchilla                                         NaN           NaN  \n",
       "PaLM (540B)                                  8404992.0  1.218724e+07  \n",
       "OPT-175B                                      812544.0  1.470705e+06  \n",
       "Minerva (540B)                                712704.0  1.322066e+07  \n",
       "GPT-3.5 (text-davinci-003)                         NaN           NaN  \n",
       "LLaMA-65B                                    1024000.0  1.392640e+06  \n",
       "GPT-4                                       57000000.0  8.265000e+07  \n",
       "PaLM 2                                             NaN           NaN  \n",
       "Claude 2                                           NaN           NaN  \n",
       "Llama 2-70B                                        NaN           NaN  \n",
       "Falcon 180B                                 17694720.0  2.565734e+07  \n",
       "\n",
       "[43 rows x 51 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost_df = estimate_costs(frontier_pcd_df, hardware_df, price_df)\n",
    "cost_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 of 43 systems have a cost estimate\n"
     ]
    }
   ],
   "source": [
    "print(cost_df['Cost'].notna().sum(), 'of', len(cost_df), 'systems have a cost estimate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sensitivity of cost estimates to imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "frontier_pcd_df, hardware_df, price_df = load_data_for_cost_estimation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "frontier_pcd_df = frontier_pcd_df.set_index('System')\n",
    "frontier_pcd_df['System'] = frontier_pcd_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "known_costs_mask = cost_df['Cost'].notna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Domain</th>\n",
       "      <th>Task</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Notability criteria</th>\n",
       "      <th>Notability criteria notes</th>\n",
       "      <th>Open-source</th>\n",
       "      <th>Link</th>\n",
       "      <th>Citations</th>\n",
       "      <th>Reference</th>\n",
       "      <th>Publication date</th>\n",
       "      <th>...</th>\n",
       "      <th>Base model</th>\n",
       "      <th>Finetune compute (FLOP)</th>\n",
       "      <th>Finetune compute notes</th>\n",
       "      <th>Authors by country</th>\n",
       "      <th>Hardware quantity</th>\n",
       "      <th>Hardware utilization</th>\n",
       "      <th>Training cost trends</th>\n",
       "      <th>Training cloud compute vendor</th>\n",
       "      <th>Training data center</th>\n",
       "      <th>System</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>System</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GNMT</th>\n",
       "      <td>Language</td>\n",
       "      <td>Translation</td>\n",
       "      <td>Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc ...</td>\n",
       "      <td>Highly cited</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/abs/1609.08144</td>\n",
       "      <td>5948.0</td>\n",
       "      <td>Google's Neural Machine Translation System: Br...</td>\n",
       "      <td>2016-09-26</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GNMT</td>\n",
       "      <td>96.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GNMT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GNMT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JFT</th>\n",
       "      <td>Vision</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Chen Sun, Abhinav Shrivastava, Saurabh Singh, ...</td>\n",
       "      <td>Highly cited</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/abs/1707.02968</td>\n",
       "      <td>1923.0</td>\n",
       "      <td>Revisiting Unreasonable Effectiveness of Data ...</td>\n",
       "      <td>2017-07-10</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>JFT</td>\n",
       "      <td>50.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>JFT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>JFT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AmoebaNet-A (F=448)</th>\n",
       "      <td>Vision</td>\n",
       "      <td>Image classification</td>\n",
       "      <td>Esteban Real, Alok Aggarwal, Yanping Huang, Qu...</td>\n",
       "      <td>Highly cited</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/abs/1802.01548</td>\n",
       "      <td>2505.0</td>\n",
       "      <td>Regularized Evolution for Image Classifier Arc...</td>\n",
       "      <td>2018-02-05</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AmoebaNet-A (F=448)</td>\n",
       "      <td>450.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AmoebaNet-A (F=448)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AmoebaNet-A (F=448)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IMPALA</th>\n",
       "      <td>Games</td>\n",
       "      <td>Atari</td>\n",
       "      <td>Lasse Espeholt, Hubert Soyer, Remi Munos, Kare...</td>\n",
       "      <td>SOTA Improvement</td>\n",
       "      <td>\"IMPALA is able to achieve better performance ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/abs/1802.01561</td>\n",
       "      <td>1288.0</td>\n",
       "      <td>IMPALA: Scalable Distributed Deep-RL with Impo...</td>\n",
       "      <td>2018-02-05</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IMPALA</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IMPALA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IMPALA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BigGAN-deep 512x512</th>\n",
       "      <td>Drawing</td>\n",
       "      <td>Image generation</td>\n",
       "      <td>A Brock, J Donahue, K Simonyan</td>\n",
       "      <td>Highly cited</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Permissive license</td>\n",
       "      <td>https://arxiv.org/abs/1809.11096</td>\n",
       "      <td>4163.0</td>\n",
       "      <td>Large Scale GAN Training for High Fidelity Nat...</td>\n",
       "      <td>2018-09-28</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BigGAN-deep 512x512</td>\n",
       "      <td>256.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BigGAN-deep 512x512</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BigGAN-deep 512x512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RoBERTa Large</th>\n",
       "      <td>Language</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du,...</td>\n",
       "      <td>Highly cited</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/abs/1907.11692</td>\n",
       "      <td>15901.0</td>\n",
       "      <td>RoBERTa: A Robustly Optimized BERT Pretraining...</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RoBERTa Large</td>\n",
       "      <td>1024.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RoBERTa Large</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RoBERTa Large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Megatron-BERT</th>\n",
       "      <td>Language</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mohammad Shoeybi, Mostofa Patwary, Raul Puri, ...</td>\n",
       "      <td>SOTA Improvement</td>\n",
       "      <td>\"Our BERT model achieves SOTA results on the R...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/abs/1909.08053</td>\n",
       "      <td>1003.0</td>\n",
       "      <td>Megatron-LM: Training Multi-Billion Parameter ...</td>\n",
       "      <td>2019-09-17</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Megatron-BERT</td>\n",
       "      <td>512.0</td>\n",
       "      <td>0.2269</td>\n",
       "      <td>Megatron-BERT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Megatron-BERT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Megatron-LM (8.3B)</th>\n",
       "      <td>Language</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mohammad Shoeybi, Mostofa Patwary, Raul Puri, ...</td>\n",
       "      <td>SOTA Improvement</td>\n",
       "      <td>\"Using the GPT-2 model we achieve SOTA results...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/abs/1909.08053</td>\n",
       "      <td>1003.0</td>\n",
       "      <td>Megatron-LM: Training Multi-Billion Parameter ...</td>\n",
       "      <td>2019-09-17</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Megatron-LM (8.3B)</td>\n",
       "      <td>512.0</td>\n",
       "      <td>0.1162</td>\n",
       "      <td>Megatron-LM (8.3B)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Megatron-LM (8.3B)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T5-11B</th>\n",
       "      <td>Language</td>\n",
       "      <td>Text autocompletion</td>\n",
       "      <td>Colin Raffel, Noam Shazeer, Adam Roberts, Kath...</td>\n",
       "      <td>Highly cited</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/abs/1910.10683</td>\n",
       "      <td>10800.0</td>\n",
       "      <td>Exploring the Limits of Transfer Learning with...</td>\n",
       "      <td>2019-10-23</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>T5-11B</td>\n",
       "      <td>512.0</td>\n",
       "      <td>0.3707</td>\n",
       "      <td>T5-11B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>T5-11B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AlphaStar</th>\n",
       "      <td>Games</td>\n",
       "      <td>StarCraft</td>\n",
       "      <td>Oriol Vinyals,Igor Babuschkin,Wojciech M. Czar...</td>\n",
       "      <td>Highly cited</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.deepmind.com/blog/alphastar-grandm...</td>\n",
       "      <td>2681.0</td>\n",
       "      <td>Grandmaster level in StarCraft II using multi-...</td>\n",
       "      <td>2019-10-30</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AlphaStar</td>\n",
       "      <td>384.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AlphaStar</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AlphaStar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Meena</th>\n",
       "      <td>Language</td>\n",
       "      <td>Text autocompletion</td>\n",
       "      <td>Dongling Xiao, Han Zhang, Yukun Li, Yu Sun, Ha...</td>\n",
       "      <td>SOTA Improvement</td>\n",
       "      <td>\"We also propose a human evaluation metric cal...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/abs/2001.09977</td>\n",
       "      <td>747.0</td>\n",
       "      <td>Towards a Human-like Open-Domain Chatbot</td>\n",
       "      <td>2020-01-28</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Meena</td>\n",
       "      <td>1024.0</td>\n",
       "      <td>0.3439</td>\n",
       "      <td>Meena</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Meena</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPT-3 175B (davinci)</th>\n",
       "      <td>Language</td>\n",
       "      <td>Text autocompletion</td>\n",
       "      <td>Tom B. Brown, Benjamin Mann, Nick Ryder, Melan...</td>\n",
       "      <td>Highly cited</td>\n",
       "      <td>NaN</td>\n",
       "      <td>API accessible</td>\n",
       "      <td>https://arxiv.org/abs/2005.14165</td>\n",
       "      <td>18144.0</td>\n",
       "      <td>Language models are Few-Shot Learners</td>\n",
       "      <td>2020-05-28</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GPT-3 175B (davinci)</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.2196</td>\n",
       "      <td>GPT-3 175B (davinci)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GPT-3 175B (davinci)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GShard (dense)</th>\n",
       "      <td>Language</td>\n",
       "      <td>Translation</td>\n",
       "      <td>Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu,...</td>\n",
       "      <td>SOTA Improvement</td>\n",
       "      <td>\"such a giant model can efficiently be trained...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/abs/2006.16668</td>\n",
       "      <td>523.0</td>\n",
       "      <td>GShard: Scaling Giant Models with Conditional ...</td>\n",
       "      <td>2020-06-30</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GShard (dense)</td>\n",
       "      <td>1024.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GShard (dense)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GShard (dense)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ALIGN</th>\n",
       "      <td>Multimodal</td>\n",
       "      <td>Representation Learning</td>\n",
       "      <td>Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Z...</td>\n",
       "      <td>SOTA Improvement</td>\n",
       "      <td>\"The aligned visual and language representatio...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/abs/2102.05918</td>\n",
       "      <td>1766.0</td>\n",
       "      <td>Scaling up visual and vision-language represen...</td>\n",
       "      <td>2021-06-11</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ALIGN</td>\n",
       "      <td>512.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ALIGN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ALIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Megatron-Turing NLG 530B</th>\n",
       "      <td>Language</td>\n",
       "      <td>Language modelling</td>\n",
       "      <td>Shaden Smith, Mostofa Patwary, Brandon Norick,...</td>\n",
       "      <td>SOTA improvement</td>\n",
       "      <td>The 105-layer, transformer-based MT-NLG improv...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/abs/2201.11990</td>\n",
       "      <td>460.0</td>\n",
       "      <td>Using DeepSpeed and Megatron to Train Megatron...</td>\n",
       "      <td>2021-10-11</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Megatron-Turing NLG 530B</td>\n",
       "      <td>4480.0</td>\n",
       "      <td>0.3020</td>\n",
       "      <td>Megatron-Turing NLG 530B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Megatron-Turing NLG 530B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gopher (280B)</th>\n",
       "      <td>Language</td>\n",
       "      <td>Language modelling</td>\n",
       "      <td>Jack W. Rae, Sebastian Borgeaud, Trevor Cai, K...</td>\n",
       "      <td>SOTA Improvement</td>\n",
       "      <td>\"These models are evaluated on 152 diverse tas...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/abs/2112.11446</td>\n",
       "      <td>736.0</td>\n",
       "      <td>Scaling Language Models: Methods, Analysis &amp; I...</td>\n",
       "      <td>2021-12-08</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Gopher (280B)</td>\n",
       "      <td>4096.0</td>\n",
       "      <td>0.3780</td>\n",
       "      <td>Gopher (280B)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Gopher (280B)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GLaM</th>\n",
       "      <td>Language</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nan Du, Yanping Huang, Andrew M. Dai, Simon To...</td>\n",
       "      <td>SOTA Improvement</td>\n",
       "      <td>\"As shown in Table 5, GLaM (64B/64E) is better...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/abs/2112.06905</td>\n",
       "      <td>314.0</td>\n",
       "      <td>GLaM: Efficient Scaling of Language Models wit...</td>\n",
       "      <td>2021-12-13</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GLaM</td>\n",
       "      <td>1024.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GLaM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GLaM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LaMDA</th>\n",
       "      <td>Language</td>\n",
       "      <td>Language modelling</td>\n",
       "      <td>Romal Thoppilan, Daniel De Freitas, Jamie Hall...</td>\n",
       "      <td>Historical significance</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/abs/2201.08239</td>\n",
       "      <td>891.0</td>\n",
       "      <td>LaMDA: Language Models for Dialog Applications</td>\n",
       "      <td>2022-02-10</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LaMDA</td>\n",
       "      <td>1024.0</td>\n",
       "      <td>0.5650</td>\n",
       "      <td>LaMDA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LaMDA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PaLM (540B)</th>\n",
       "      <td>Language</td>\n",
       "      <td>Language modelling</td>\n",
       "      <td>Aakanksha Chowdhery, Sharan Narang, Jacob Devl...</td>\n",
       "      <td>SOTA Improvement</td>\n",
       "      <td>Demonstrates continued benefits of scaling, as...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/abs/2204.02311</td>\n",
       "      <td>2612.0</td>\n",
       "      <td>PaLM: Scaling Language Modeling with Pathways</td>\n",
       "      <td>2022-04-04</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PaLM (540B)</td>\n",
       "      <td>6144.0</td>\n",
       "      <td>0.4620</td>\n",
       "      <td>PaLM (540B)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PaLM (540B)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OPT-175B</th>\n",
       "      <td>Language</td>\n",
       "      <td>Language modelling</td>\n",
       "      <td>Susan Zhang, Stephen Roller, Naman Goyal, Mike...</td>\n",
       "      <td>Significant use</td>\n",
       "      <td>https://ai.meta.com/blog/opt-175b-large-langua...</td>\n",
       "      <td>Fully open-source</td>\n",
       "      <td>https://ai.facebook.com/blog/democratizing-acc...</td>\n",
       "      <td>1443.0</td>\n",
       "      <td>OPT: Open Pre-trained Transformer Language Models</td>\n",
       "      <td>2022-05-02</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OPT-175B</td>\n",
       "      <td>1024.0</td>\n",
       "      <td>0.4712</td>\n",
       "      <td>OPT-175B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OPT-175B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Minerva (540B)</th>\n",
       "      <td>Language</td>\n",
       "      <td>Quantitative Reasoning Problems</td>\n",
       "      <td>Aitor Lewkowycz, Anders Andreassen, David Doha...</td>\n",
       "      <td>SOTA Improvement</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/abs/2206.14858</td>\n",
       "      <td>307.0</td>\n",
       "      <td>Solving Quantitative Reasoning Problems with L...</td>\n",
       "      <td>2022-06-29</td>\n",
       "      <td>...</td>\n",
       "      <td>PaLM (540B)</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Minerva (540B)</td>\n",
       "      <td>1024.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Minerva (540B)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Minerva (540B)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LLaMA-65B</th>\n",
       "      <td>Language</td>\n",
       "      <td>Language modelling</td>\n",
       "      <td>Hugo Touvron, Thibaut Lavril, Gautier Izacard,...</td>\n",
       "      <td>Historical significance</td>\n",
       "      <td>Widely-used foundation model that has been ada...</td>\n",
       "      <td>Fully open-source</td>\n",
       "      <td>https://arxiv.org/abs/2302.13971</td>\n",
       "      <td>2695.0</td>\n",
       "      <td>LLaMA: Open and Efficient Foundation Language ...</td>\n",
       "      <td>2023-02-24</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LLaMA-65B</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>0.4746</td>\n",
       "      <td>LLaMA-65B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LLaMA-65B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPT-4</th>\n",
       "      <td>Multimodal</td>\n",
       "      <td>Language modelling</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>SOTA Improvement</td>\n",
       "      <td>See the paper, p.1: \"On a suite of traditional...</td>\n",
       "      <td>API accessible</td>\n",
       "      <td>https://arxiv.org/abs/2303.08774</td>\n",
       "      <td>1871.0</td>\n",
       "      <td>GPT-4 Technical Report</td>\n",
       "      <td>2023-03-15</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GPT-4</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>0.3400</td>\n",
       "      <td>GPT-4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GPT-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Falcon 180B</th>\n",
       "      <td>Language</td>\n",
       "      <td>Language modelling</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SOTA Improvement</td>\n",
       "      <td>\"It's currently at the top of the Hugging Face...</td>\n",
       "      <td>Permissive license</td>\n",
       "      <td>https://falconllm.tii.ae/falcon-180b.html</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Falcon LLM - Falcon 180B</td>\n",
       "      <td>2023-09-06</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Falcon 180B</td>\n",
       "      <td>4096.0</td>\n",
       "      <td>0.1876</td>\n",
       "      <td>Falcon 180B</td>\n",
       "      <td>Amazon Web Services</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Falcon 180B</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24 rows × 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Domain                             Task  \\\n",
       "System                                                                  \n",
       "GNMT                        Language                      Translation   \n",
       "JFT                           Vision                              NaN   \n",
       "AmoebaNet-A (F=448)           Vision             Image classification   \n",
       "IMPALA                         Games                            Atari   \n",
       "BigGAN-deep 512x512          Drawing                 Image generation   \n",
       "RoBERTa Large               Language                              NaN   \n",
       "Megatron-BERT               Language                              NaN   \n",
       "Megatron-LM (8.3B)          Language                              NaN   \n",
       "T5-11B                      Language              Text autocompletion   \n",
       "AlphaStar                      Games                        StarCraft   \n",
       "Meena                       Language              Text autocompletion   \n",
       "GPT-3 175B (davinci)        Language              Text autocompletion   \n",
       "GShard (dense)              Language                      Translation   \n",
       "ALIGN                     Multimodal          Representation Learning   \n",
       "Megatron-Turing NLG 530B    Language               Language modelling   \n",
       "Gopher (280B)               Language               Language modelling   \n",
       "GLaM                        Language                              NaN   \n",
       "LaMDA                       Language               Language modelling   \n",
       "PaLM (540B)                 Language               Language modelling   \n",
       "OPT-175B                    Language               Language modelling   \n",
       "Minerva (540B)              Language  Quantitative Reasoning Problems   \n",
       "LLaMA-65B                   Language               Language modelling   \n",
       "GPT-4                     Multimodal               Language modelling   \n",
       "Falcon 180B                 Language               Language modelling   \n",
       "\n",
       "                                                                    Authors  \\\n",
       "System                                                                        \n",
       "GNMT                      Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc ...   \n",
       "JFT                       Chen Sun, Abhinav Shrivastava, Saurabh Singh, ...   \n",
       "AmoebaNet-A (F=448)       Esteban Real, Alok Aggarwal, Yanping Huang, Qu...   \n",
       "IMPALA                    Lasse Espeholt, Hubert Soyer, Remi Munos, Kare...   \n",
       "BigGAN-deep 512x512                          A Brock, J Donahue, K Simonyan   \n",
       "RoBERTa Large             Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du,...   \n",
       "Megatron-BERT             Mohammad Shoeybi, Mostofa Patwary, Raul Puri, ...   \n",
       "Megatron-LM (8.3B)        Mohammad Shoeybi, Mostofa Patwary, Raul Puri, ...   \n",
       "T5-11B                    Colin Raffel, Noam Shazeer, Adam Roberts, Kath...   \n",
       "AlphaStar                 Oriol Vinyals,Igor Babuschkin,Wojciech M. Czar...   \n",
       "Meena                     Dongling Xiao, Han Zhang, Yukun Li, Yu Sun, Ha...   \n",
       "GPT-3 175B (davinci)      Tom B. Brown, Benjamin Mann, Nick Ryder, Melan...   \n",
       "GShard (dense)            Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu,...   \n",
       "ALIGN                     Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Z...   \n",
       "Megatron-Turing NLG 530B  Shaden Smith, Mostofa Patwary, Brandon Norick,...   \n",
       "Gopher (280B)             Jack W. Rae, Sebastian Borgeaud, Trevor Cai, K...   \n",
       "GLaM                      Nan Du, Yanping Huang, Andrew M. Dai, Simon To...   \n",
       "LaMDA                     Romal Thoppilan, Daniel De Freitas, Jamie Hall...   \n",
       "PaLM (540B)               Aakanksha Chowdhery, Sharan Narang, Jacob Devl...   \n",
       "OPT-175B                  Susan Zhang, Stephen Roller, Naman Goyal, Mike...   \n",
       "Minerva (540B)            Aitor Lewkowycz, Anders Andreassen, David Doha...   \n",
       "LLaMA-65B                 Hugo Touvron, Thibaut Lavril, Gautier Izacard,...   \n",
       "GPT-4                                                                OpenAI   \n",
       "Falcon 180B                                                             NaN   \n",
       "\n",
       "                              Notability criteria  \\\n",
       "System                                              \n",
       "GNMT                                 Highly cited   \n",
       "JFT                                  Highly cited   \n",
       "AmoebaNet-A (F=448)                  Highly cited   \n",
       "IMPALA                           SOTA Improvement   \n",
       "BigGAN-deep 512x512                  Highly cited   \n",
       "RoBERTa Large                        Highly cited   \n",
       "Megatron-BERT                    SOTA Improvement   \n",
       "Megatron-LM (8.3B)               SOTA Improvement   \n",
       "T5-11B                               Highly cited   \n",
       "AlphaStar                            Highly cited   \n",
       "Meena                            SOTA Improvement   \n",
       "GPT-3 175B (davinci)                 Highly cited   \n",
       "GShard (dense)                   SOTA Improvement   \n",
       "ALIGN                            SOTA Improvement   \n",
       "Megatron-Turing NLG 530B         SOTA improvement   \n",
       "Gopher (280B)                    SOTA Improvement   \n",
       "GLaM                             SOTA Improvement   \n",
       "LaMDA                     Historical significance   \n",
       "PaLM (540B)                      SOTA Improvement   \n",
       "OPT-175B                          Significant use   \n",
       "Minerva (540B)                   SOTA Improvement   \n",
       "LLaMA-65B                 Historical significance   \n",
       "GPT-4                            SOTA Improvement   \n",
       "Falcon 180B                      SOTA Improvement   \n",
       "\n",
       "                                                  Notability criteria notes  \\\n",
       "System                                                                        \n",
       "GNMT                                                                    NaN   \n",
       "JFT                                                                     NaN   \n",
       "AmoebaNet-A (F=448)                                                     NaN   \n",
       "IMPALA                    \"IMPALA is able to achieve better performance ...   \n",
       "BigGAN-deep 512x512                                                     NaN   \n",
       "RoBERTa Large                                                           NaN   \n",
       "Megatron-BERT             \"Our BERT model achieves SOTA results on the R...   \n",
       "Megatron-LM (8.3B)        \"Using the GPT-2 model we achieve SOTA results...   \n",
       "T5-11B                                                                  NaN   \n",
       "AlphaStar                                                               NaN   \n",
       "Meena                     \"We also propose a human evaluation metric cal...   \n",
       "GPT-3 175B (davinci)                                                    NaN   \n",
       "GShard (dense)            \"such a giant model can efficiently be trained...   \n",
       "ALIGN                     \"The aligned visual and language representatio...   \n",
       "Megatron-Turing NLG 530B  The 105-layer, transformer-based MT-NLG improv...   \n",
       "Gopher (280B)             \"These models are evaluated on 152 diverse tas...   \n",
       "GLaM                      \"As shown in Table 5, GLaM (64B/64E) is better...   \n",
       "LaMDA                                                                   NaN   \n",
       "PaLM (540B)               Demonstrates continued benefits of scaling, as...   \n",
       "OPT-175B                  https://ai.meta.com/blog/opt-175b-large-langua...   \n",
       "Minerva (540B)                                                          NaN   \n",
       "LLaMA-65B                 Widely-used foundation model that has been ada...   \n",
       "GPT-4                     See the paper, p.1: \"On a suite of traditional...   \n",
       "Falcon 180B               \"It's currently at the top of the Hugging Face...   \n",
       "\n",
       "                                 Open-source  \\\n",
       "System                                         \n",
       "GNMT                                     NaN   \n",
       "JFT                                      NaN   \n",
       "AmoebaNet-A (F=448)                      NaN   \n",
       "IMPALA                                   NaN   \n",
       "BigGAN-deep 512x512       Permissive license   \n",
       "RoBERTa Large                            NaN   \n",
       "Megatron-BERT                            NaN   \n",
       "Megatron-LM (8.3B)                       NaN   \n",
       "T5-11B                                   NaN   \n",
       "AlphaStar                                NaN   \n",
       "Meena                                    NaN   \n",
       "GPT-3 175B (davinci)          API accessible   \n",
       "GShard (dense)                           NaN   \n",
       "ALIGN                                    NaN   \n",
       "Megatron-Turing NLG 530B                 NaN   \n",
       "Gopher (280B)                            NaN   \n",
       "GLaM                                     NaN   \n",
       "LaMDA                                    NaN   \n",
       "PaLM (540B)                              NaN   \n",
       "OPT-175B                   Fully open-source   \n",
       "Minerva (540B)                           NaN   \n",
       "LLaMA-65B                  Fully open-source   \n",
       "GPT-4                         API accessible   \n",
       "Falcon 180B               Permissive license   \n",
       "\n",
       "                                                                       Link  \\\n",
       "System                                                                        \n",
       "GNMT                                       https://arxiv.org/abs/1609.08144   \n",
       "JFT                                        https://arxiv.org/abs/1707.02968   \n",
       "AmoebaNet-A (F=448)                        https://arxiv.org/abs/1802.01548   \n",
       "IMPALA                                     https://arxiv.org/abs/1802.01561   \n",
       "BigGAN-deep 512x512                        https://arxiv.org/abs/1809.11096   \n",
       "RoBERTa Large                              https://arxiv.org/abs/1907.11692   \n",
       "Megatron-BERT                              https://arxiv.org/abs/1909.08053   \n",
       "Megatron-LM (8.3B)                         https://arxiv.org/abs/1909.08053   \n",
       "T5-11B                                     https://arxiv.org/abs/1910.10683   \n",
       "AlphaStar                 https://www.deepmind.com/blog/alphastar-grandm...   \n",
       "Meena                                      https://arxiv.org/abs/2001.09977   \n",
       "GPT-3 175B (davinci)                       https://arxiv.org/abs/2005.14165   \n",
       "GShard (dense)                             https://arxiv.org/abs/2006.16668   \n",
       "ALIGN                                      https://arxiv.org/abs/2102.05918   \n",
       "Megatron-Turing NLG 530B                   https://arxiv.org/abs/2201.11990   \n",
       "Gopher (280B)                              https://arxiv.org/abs/2112.11446   \n",
       "GLaM                                       https://arxiv.org/abs/2112.06905   \n",
       "LaMDA                                      https://arxiv.org/abs/2201.08239   \n",
       "PaLM (540B)                                https://arxiv.org/abs/2204.02311   \n",
       "OPT-175B                  https://ai.facebook.com/blog/democratizing-acc...   \n",
       "Minerva (540B)                             https://arxiv.org/abs/2206.14858   \n",
       "LLaMA-65B                                  https://arxiv.org/abs/2302.13971   \n",
       "GPT-4                                      https://arxiv.org/abs/2303.08774   \n",
       "Falcon 180B                       https://falconllm.tii.ae/falcon-180b.html   \n",
       "\n",
       "                          Citations  \\\n",
       "System                                \n",
       "GNMT                         5948.0   \n",
       "JFT                          1923.0   \n",
       "AmoebaNet-A (F=448)          2505.0   \n",
       "IMPALA                       1288.0   \n",
       "BigGAN-deep 512x512          4163.0   \n",
       "RoBERTa Large               15901.0   \n",
       "Megatron-BERT                1003.0   \n",
       "Megatron-LM (8.3B)           1003.0   \n",
       "T5-11B                      10800.0   \n",
       "AlphaStar                    2681.0   \n",
       "Meena                         747.0   \n",
       "GPT-3 175B (davinci)        18144.0   \n",
       "GShard (dense)                523.0   \n",
       "ALIGN                        1766.0   \n",
       "Megatron-Turing NLG 530B      460.0   \n",
       "Gopher (280B)                 736.0   \n",
       "GLaM                          314.0   \n",
       "LaMDA                         891.0   \n",
       "PaLM (540B)                  2612.0   \n",
       "OPT-175B                     1443.0   \n",
       "Minerva (540B)                307.0   \n",
       "LLaMA-65B                    2695.0   \n",
       "GPT-4                        1871.0   \n",
       "Falcon 180B                     0.0   \n",
       "\n",
       "                                                                  Reference  \\\n",
       "System                                                                        \n",
       "GNMT                      Google's Neural Machine Translation System: Br...   \n",
       "JFT                       Revisiting Unreasonable Effectiveness of Data ...   \n",
       "AmoebaNet-A (F=448)       Regularized Evolution for Image Classifier Arc...   \n",
       "IMPALA                    IMPALA: Scalable Distributed Deep-RL with Impo...   \n",
       "BigGAN-deep 512x512       Large Scale GAN Training for High Fidelity Nat...   \n",
       "RoBERTa Large             RoBERTa: A Robustly Optimized BERT Pretraining...   \n",
       "Megatron-BERT             Megatron-LM: Training Multi-Billion Parameter ...   \n",
       "Megatron-LM (8.3B)        Megatron-LM: Training Multi-Billion Parameter ...   \n",
       "T5-11B                    Exploring the Limits of Transfer Learning with...   \n",
       "AlphaStar                 Grandmaster level in StarCraft II using multi-...   \n",
       "Meena                              Towards a Human-like Open-Domain Chatbot   \n",
       "GPT-3 175B (davinci)                  Language models are Few-Shot Learners   \n",
       "GShard (dense)            GShard: Scaling Giant Models with Conditional ...   \n",
       "ALIGN                     Scaling up visual and vision-language represen...   \n",
       "Megatron-Turing NLG 530B  Using DeepSpeed and Megatron to Train Megatron...   \n",
       "Gopher (280B)             Scaling Language Models: Methods, Analysis & I...   \n",
       "GLaM                      GLaM: Efficient Scaling of Language Models wit...   \n",
       "LaMDA                        LaMDA: Language Models for Dialog Applications   \n",
       "PaLM (540B)                   PaLM: Scaling Language Modeling with Pathways   \n",
       "OPT-175B                  OPT: Open Pre-trained Transformer Language Models   \n",
       "Minerva (540B)            Solving Quantitative Reasoning Problems with L...   \n",
       "LLaMA-65B                 LLaMA: Open and Efficient Foundation Language ...   \n",
       "GPT-4                                                GPT-4 Technical Report   \n",
       "Falcon 180B                                        Falcon LLM - Falcon 180B   \n",
       "\n",
       "                         Publication date  ...   Base model  \\\n",
       "System                                     ...                \n",
       "GNMT                           2016-09-26  ...          NaN   \n",
       "JFT                            2017-07-10  ...          NaN   \n",
       "AmoebaNet-A (F=448)            2018-02-05  ...          NaN   \n",
       "IMPALA                         2018-02-05  ...          NaN   \n",
       "BigGAN-deep 512x512            2018-09-28  ...          NaN   \n",
       "RoBERTa Large                  2019-07-01  ...          NaN   \n",
       "Megatron-BERT                  2019-09-17  ...          NaN   \n",
       "Megatron-LM (8.3B)             2019-09-17  ...          NaN   \n",
       "T5-11B                         2019-10-23  ...          NaN   \n",
       "AlphaStar                      2019-10-30  ...          NaN   \n",
       "Meena                          2020-01-28  ...          NaN   \n",
       "GPT-3 175B (davinci)           2020-05-28  ...          NaN   \n",
       "GShard (dense)                 2020-06-30  ...          NaN   \n",
       "ALIGN                          2021-06-11  ...          NaN   \n",
       "Megatron-Turing NLG 530B       2021-10-11  ...          NaN   \n",
       "Gopher (280B)                  2021-12-08  ...          NaN   \n",
       "GLaM                           2021-12-13  ...          NaN   \n",
       "LaMDA                          2022-02-10  ...          NaN   \n",
       "PaLM (540B)                    2022-04-04  ...          NaN   \n",
       "OPT-175B                       2022-05-02  ...          NaN   \n",
       "Minerva (540B)                 2022-06-29  ...  PaLM (540B)   \n",
       "LLaMA-65B                      2023-02-24  ...          NaN   \n",
       "GPT-4                          2023-03-15  ...          NaN   \n",
       "Falcon 180B                    2023-09-06  ...          NaN   \n",
       "\n",
       "                         Finetune compute (FLOP)  Finetune compute notes  \\\n",
       "System                                                                     \n",
       "GNMT                                         NaN                     NaN   \n",
       "JFT                                          NaN                     NaN   \n",
       "AmoebaNet-A (F=448)                          NaN                     NaN   \n",
       "IMPALA                                       NaN                     NaN   \n",
       "BigGAN-deep 512x512                          NaN                     NaN   \n",
       "RoBERTa Large                                NaN                     NaN   \n",
       "Megatron-BERT                                NaN                     NaN   \n",
       "Megatron-LM (8.3B)                           NaN                     NaN   \n",
       "T5-11B                                       NaN                     NaN   \n",
       "AlphaStar                                    NaN                     NaN   \n",
       "Meena                                        NaN                     NaN   \n",
       "GPT-3 175B (davinci)                         NaN                     NaN   \n",
       "GShard (dense)                               NaN                     NaN   \n",
       "ALIGN                                        NaN                     NaN   \n",
       "Megatron-Turing NLG 530B                     NaN                     NaN   \n",
       "Gopher (280B)                                NaN                     NaN   \n",
       "GLaM                                         NaN                     NaN   \n",
       "LaMDA                                        NaN                     NaN   \n",
       "PaLM (540B)                                  NaN                     NaN   \n",
       "OPT-175B                                     NaN                     NaN   \n",
       "Minerva (540B)                                 2                     NaN   \n",
       "LLaMA-65B                                    NaN                     NaN   \n",
       "GPT-4                                        NaN                     NaN   \n",
       "Falcon 180B                                  NaN                     NaN   \n",
       "\n",
       "                                Authors by country  Hardware quantity  \\\n",
       "System                                                                  \n",
       "GNMT                                          GNMT               96.0   \n",
       "JFT                                            JFT               50.0   \n",
       "AmoebaNet-A (F=448)            AmoebaNet-A (F=448)              450.0   \n",
       "IMPALA                                      IMPALA                1.0   \n",
       "BigGAN-deep 512x512            BigGAN-deep 512x512              256.0   \n",
       "RoBERTa Large                        RoBERTa Large             1024.0   \n",
       "Megatron-BERT                        Megatron-BERT              512.0   \n",
       "Megatron-LM (8.3B)              Megatron-LM (8.3B)              512.0   \n",
       "T5-11B                                      T5-11B              512.0   \n",
       "AlphaStar                                AlphaStar              384.0   \n",
       "Meena                                        Meena             1024.0   \n",
       "GPT-3 175B (davinci)          GPT-3 175B (davinci)            10000.0   \n",
       "GShard (dense)                      GShard (dense)             1024.0   \n",
       "ALIGN                                        ALIGN              512.0   \n",
       "Megatron-Turing NLG 530B  Megatron-Turing NLG 530B             4480.0   \n",
       "Gopher (280B)                        Gopher (280B)             4096.0   \n",
       "GLaM                                          GLaM             1024.0   \n",
       "LaMDA                                        LaMDA             1024.0   \n",
       "PaLM (540B)                            PaLM (540B)             6144.0   \n",
       "OPT-175B                                  OPT-175B             1024.0   \n",
       "Minerva (540B)                      Minerva (540B)             1024.0   \n",
       "LLaMA-65B                                LLaMA-65B             2048.0   \n",
       "GPT-4                                        GPT-4            25000.0   \n",
       "Falcon 180B                            Falcon 180B             4096.0   \n",
       "\n",
       "                         Hardware utilization      Training cost trends  \\\n",
       "System                                                                    \n",
       "GNMT                                      NaN                      GNMT   \n",
       "JFT                                       NaN                       JFT   \n",
       "AmoebaNet-A (F=448)                       NaN       AmoebaNet-A (F=448)   \n",
       "IMPALA                                    NaN                    IMPALA   \n",
       "BigGAN-deep 512x512                       NaN       BigGAN-deep 512x512   \n",
       "RoBERTa Large                             NaN             RoBERTa Large   \n",
       "Megatron-BERT                          0.2269             Megatron-BERT   \n",
       "Megatron-LM (8.3B)                     0.1162        Megatron-LM (8.3B)   \n",
       "T5-11B                                 0.3707                    T5-11B   \n",
       "AlphaStar                                 NaN                 AlphaStar   \n",
       "Meena                                  0.3439                     Meena   \n",
       "GPT-3 175B (davinci)                   0.2196      GPT-3 175B (davinci)   \n",
       "GShard (dense)                            NaN            GShard (dense)   \n",
       "ALIGN                                     NaN                     ALIGN   \n",
       "Megatron-Turing NLG 530B               0.3020  Megatron-Turing NLG 530B   \n",
       "Gopher (280B)                          0.3780             Gopher (280B)   \n",
       "GLaM                                      NaN                      GLaM   \n",
       "LaMDA                                  0.5650                     LaMDA   \n",
       "PaLM (540B)                            0.4620               PaLM (540B)   \n",
       "OPT-175B                               0.4712                  OPT-175B   \n",
       "Minerva (540B)                            NaN            Minerva (540B)   \n",
       "LLaMA-65B                              0.4746                 LLaMA-65B   \n",
       "GPT-4                                  0.3400                     GPT-4   \n",
       "Falcon 180B                            0.1876               Falcon 180B   \n",
       "\n",
       "                         Training cloud compute vendor  Training data center  \\\n",
       "System                                                                         \n",
       "GNMT                                               NaN                   NaN   \n",
       "JFT                                                NaN                   NaN   \n",
       "AmoebaNet-A (F=448)                                NaN                   NaN   \n",
       "IMPALA                                             NaN                   NaN   \n",
       "BigGAN-deep 512x512                                NaN                   NaN   \n",
       "RoBERTa Large                                      NaN                   NaN   \n",
       "Megatron-BERT                                      NaN                   NaN   \n",
       "Megatron-LM (8.3B)                                 NaN                   NaN   \n",
       "T5-11B                                             NaN                   NaN   \n",
       "AlphaStar                                          NaN                   NaN   \n",
       "Meena                                              NaN                   NaN   \n",
       "GPT-3 175B (davinci)                               NaN                   NaN   \n",
       "GShard (dense)                                     NaN                   NaN   \n",
       "ALIGN                                              NaN                   NaN   \n",
       "Megatron-Turing NLG 530B                           NaN                   NaN   \n",
       "Gopher (280B)                                      NaN                   NaN   \n",
       "GLaM                                               NaN                   NaN   \n",
       "LaMDA                                              NaN                   NaN   \n",
       "PaLM (540B)                                        NaN                   NaN   \n",
       "OPT-175B                                           NaN                   NaN   \n",
       "Minerva (540B)                                     NaN                   NaN   \n",
       "LLaMA-65B                                          NaN                   NaN   \n",
       "GPT-4                                              NaN                   NaN   \n",
       "Falcon 180B                        Amazon Web Services                   NaN   \n",
       "\n",
       "                                            System  \n",
       "System                                              \n",
       "GNMT                                          GNMT  \n",
       "JFT                                            JFT  \n",
       "AmoebaNet-A (F=448)            AmoebaNet-A (F=448)  \n",
       "IMPALA                                      IMPALA  \n",
       "BigGAN-deep 512x512            BigGAN-deep 512x512  \n",
       "RoBERTa Large                        RoBERTa Large  \n",
       "Megatron-BERT                        Megatron-BERT  \n",
       "Megatron-LM (8.3B)              Megatron-LM (8.3B)  \n",
       "T5-11B                                      T5-11B  \n",
       "AlphaStar                                AlphaStar  \n",
       "Meena                                        Meena  \n",
       "GPT-3 175B (davinci)          GPT-3 175B (davinci)  \n",
       "GShard (dense)                      GShard (dense)  \n",
       "ALIGN                                        ALIGN  \n",
       "Megatron-Turing NLG 530B  Megatron-Turing NLG 530B  \n",
       "Gopher (280B)                        Gopher (280B)  \n",
       "GLaM                                          GLaM  \n",
       "LaMDA                                        LaMDA  \n",
       "PaLM (540B)                            PaLM (540B)  \n",
       "OPT-175B                                  OPT-175B  \n",
       "Minerva (540B)                      Minerva (540B)  \n",
       "LLaMA-65B                                LLaMA-65B  \n",
       "GPT-4                                        GPT-4  \n",
       "Falcon 180B                            Falcon 180B  \n",
       "\n",
       "[24 rows x 49 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter for rows where all of the required information was known\n",
    "all_known_df = frontier_pcd_df[known_costs_mask]\n",
    "all_known_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = 'Training hardware'\n",
    "imputer_fn = knn_impute_categorical_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "System\n",
       "GNMT                                     NVIDIA Tesla K80\n",
       "JFT                                      NVIDIA Tesla K80\n",
       "AmoebaNet-A (F=448)                     NVIDIA Tesla K40s\n",
       "IMPALA                                        NVIDIA P100\n",
       "BigGAN-deep 512x512                         Google TPU v3\n",
       "RoBERTa Large                NVIDIA Tesla V100 DGXS 32 GB\n",
       "Megatron-BERT               NVIDIA Tesla V100S PCIe 32 GB\n",
       "Megatron-LM (8.3B)           NVIDIA Tesla V100 DGXS 32 GB\n",
       "T5-11B                                      Google TPU v3\n",
       "AlphaStar                                   Google TPU v3\n",
       "Meena                                       Google TPU v3\n",
       "GPT-3 175B (davinci)         NVIDIA Tesla V100 DGXS 32 GB\n",
       "GShard (dense)                              Google TPU v3\n",
       "ALIGN                                       Google TPU v3\n",
       "Megatron-Turing NLG 530B           NVIDIA A100 SXM4 80 GB\n",
       "Gopher (280B)                               Google TPU v3\n",
       "GLaM                                        Google TPU v4\n",
       "LaMDA                                       Google TPU v3\n",
       "PaLM (540B)                                 Google TPU v4\n",
       "OPT-175B                           NVIDIA A100 SXM4 80 GB\n",
       "Minerva (540B)                              Google TPU v4\n",
       "LLaMA-65B                                     NVIDIA A100\n",
       "GPT-4                              NVIDIA A100 SXM4 40 GB\n",
       "Falcon 180B                        NVIDIA A100 SXM4 40 GB\n",
       "Name: Training hardware, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_known_df[target_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "System\n",
       "GNMT                                     NVIDIA Tesla K80\n",
       "JFT                                      NVIDIA Tesla K80\n",
       "AmoebaNet-A (F=448)                                   NaN\n",
       "IMPALA                                        NVIDIA P100\n",
       "BigGAN-deep 512x512                         Google TPU v3\n",
       "RoBERTa Large                                         NaN\n",
       "Megatron-BERT               NVIDIA Tesla V100S PCIe 32 GB\n",
       "Megatron-LM (8.3B)           NVIDIA Tesla V100 DGXS 32 GB\n",
       "T5-11B                                      Google TPU v3\n",
       "AlphaStar                                   Google TPU v3\n",
       "Meena                                       Google TPU v3\n",
       "GPT-3 175B (davinci)         NVIDIA Tesla V100 DGXS 32 GB\n",
       "GShard (dense)                                        NaN\n",
       "ALIGN                                       Google TPU v3\n",
       "Megatron-Turing NLG 530B           NVIDIA A100 SXM4 80 GB\n",
       "Gopher (280B)                               Google TPU v3\n",
       "GLaM                                        Google TPU v4\n",
       "LaMDA                                       Google TPU v3\n",
       "PaLM (540B)                                 Google TPU v4\n",
       "OPT-175B                                              NaN\n",
       "Minerva (540B)                              Google TPU v4\n",
       "LLaMA-65B                                     NVIDIA A100\n",
       "GPT-4                                                 NaN\n",
       "Falcon 180B                        NVIDIA A100 SXM4 40 GB\n",
       "Name: Training hardware, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop random values\n",
    "dropped_df, holdout_df = drop_random_values(all_known_df, target_col, num_drop=5)\n",
    "dropped_df[target_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== System: GNMT ====\n",
      "Trying NVIDIA Tesla K80 at 2016-01-30 00:00:00\n",
      "Trying Google Cloud, Price per chip-hour (3-year CUD)\n",
      "Could not find price\n",
      "Trying Google Cloud, Price per chip-hour (1-year CUD)\n",
      "Could not find price\n",
      "Trying Google Cloud, Price per chip-hour (on-demand)\n",
      "Found price: 1.4\n",
      "\n",
      "==== System: JFT ====\n",
      "Trying NVIDIA Tesla K80 at 2017-03-13 00:00:00\n",
      "Trying Google Cloud, Price per chip-hour (3-year CUD)\n",
      "Could not find price\n",
      "Trying Google Cloud, Price per chip-hour (1-year CUD)\n",
      "Could not find price\n",
      "Trying Google Cloud, Price per chip-hour (on-demand)\n",
      "Found price: 1.4\n",
      "\n",
      "==== System: AmoebaNet-A (F=448) ====\n",
      "Trying NVIDIA Tesla K80 at 2017-11-29 00:00:00\n",
      "Trying Google Cloud, Price per chip-hour (3-year CUD)\n",
      "Could not find price\n",
      "Trying Google Cloud, Price per chip-hour (1-year CUD)\n",
      "Could not find price\n",
      "Trying Google Cloud, Price per chip-hour (on-demand)\n",
      "Found price: 1.4\n",
      "\n",
      "==== System: IMPALA ====\n",
      "Trying NVIDIA P100 at 2017-12-01 20:00:00\n",
      "Trying Google Cloud, Price per chip-hour (3-year CUD)\n",
      "Could not find price\n",
      "Trying Google Cloud, Price per chip-hour (1-year CUD)\n",
      "Could not find price\n",
      "Trying Google Cloud, Price per chip-hour (on-demand)\n",
      "Found price: 1.46\n",
      "\n",
      "==== System: BigGAN-deep 512x512 ====\n",
      "Trying Google TPU v3 at 2018-07-28 00:00:00\n",
      "Trying Google Cloud, Price per chip-hour (3-year CUD)\n",
      "Could not find price\n",
      "Trying Google Cloud, Price per chip-hour (1-year CUD)\n",
      "Could not find price\n",
      "Trying Google Cloud, Price per chip-hour (on-demand)\n",
      "Found price: 2.0\n",
      "\n",
      "==== System: RoBERTa Large ====\n",
      "Trying NVIDIA Tesla K80 at 2019-04-28 00:00:00\n",
      "Trying Amazon Web Services, Price per chip-hour (3-year CUD)\n",
      "Found price: 0.43\n",
      "\n",
      "==== System: Megatron-BERT ====\n",
      "Trying NVIDIA Tesla V100S PCIe 32 GB at 2019-05-22 00:00:00\n",
      "Trying Amazon Web Services, Price per chip-hour (3-year CUD)\n",
      "Could not find price\n",
      "Trying Amazon Web Services, Price per chip-hour (1-year CUD)\n",
      "Could not find price\n",
      "Trying Amazon Web Services, Price per chip-hour (on-demand)\n",
      "Could not find price\n",
      "Trying Microsoft Azure, Price per chip-hour (3-year CUD)\n",
      "Could not find price\n",
      "Trying Microsoft Azure, Price per chip-hour (1-year CUD)\n",
      "Could not find price\n",
      "Trying Microsoft Azure, Price per chip-hour (on-demand)\n",
      "Could not find price\n",
      "Trying Google Cloud, Price per chip-hour (3-year CUD)\n",
      "Could not find price\n",
      "Trying Google Cloud, Price per chip-hour (1-year CUD)\n",
      "Could not find price\n",
      "Trying Google Cloud, Price per chip-hour (on-demand)\n",
      "Could not find price\n",
      "Estimating price from FLOP/s and FLOP/$ trend\n",
      "Estimated price: 0.1051277082983274\n",
      "\n",
      "==== System: Megatron-LM (8.3B) ====\n",
      "Trying NVIDIA Tesla V100 DGXS 32 GB at 2019-07-05 09:00:00\n",
      "Trying Amazon Web Services, Price per chip-hour (3-year CUD)\n",
      "Found price: 1.21\n",
      "\n",
      "==== System: T5-11B ====\n",
      "Trying Google TPU v3 at 2019-08-03 23:00:00\n",
      "Trying Google Cloud, Price per chip-hour (3-year CUD)\n",
      "Found price: 0.9\n",
      "\n",
      "==== System: AlphaStar ====\n",
      "Trying Google TPU v3 at 2019-07-18 00:00:00\n",
      "Trying Google Cloud, Price per chip-hour (3-year CUD)\n",
      "Found price: 0.9\n",
      "\n",
      "==== System: Meena ====\n",
      "Trying Google TPU v3 at 2019-10-30 00:00:00\n",
      "Trying Google Cloud, Price per chip-hour (3-year CUD)\n",
      "Found price: 0.9\n",
      "\n",
      "==== System: GPT-3 175B (davinci) ====\n",
      "Trying NVIDIA Tesla V100 DGXS 32 GB at 2020-03-15 05:00:00\n",
      "Trying Microsoft Azure, Price per chip-hour (3-year CUD)\n",
      "Could not find price\n",
      "Trying Microsoft Azure, Price per chip-hour (1-year CUD)\n",
      "Could not find price\n",
      "Trying Microsoft Azure, Price per chip-hour (on-demand)\n",
      "Could not find price\n",
      "Trying Amazon Web Services, Price per chip-hour (3-year CUD)\n",
      "Found price: 1.21\n",
      "\n",
      "==== System: GShard (dense) ====\n",
      "Trying Google TPU v3 at 2020-03-20 00:00:00\n",
      "Trying Google Cloud, Price per chip-hour (3-year CUD)\n",
      "Found price: 0.9\n",
      "\n",
      "==== System: ALIGN ====\n",
      "Trying Google TPU v3 at 2021-03-29 13:00:00\n",
      "Trying Google Cloud, Price per chip-hour (3-year CUD)\n",
      "Found price: 0.9\n",
      "\n",
      "==== System: Megatron-Turing NLG 530B ====\n",
      "Trying NVIDIA A100 SXM4 80 GB at 2021-07-10 22:00:00\n",
      "Trying Microsoft Azure, Price per chip-hour (3-year CUD)\n",
      "Found price: 1.8\n",
      "\n",
      "==== System: Gopher (280B) ====\n",
      "Trying Google TPU v3 at 2021-08-31 16:00:00\n",
      "Trying Google Cloud, Price per chip-hour (3-year CUD)\n",
      "Found price: 0.9\n",
      "\n",
      "==== System: GLaM ====\n",
      "Trying Google TPU v4 at 2021-08-18 02:00:00\n",
      "Trying Google Cloud, Price per chip-hour (3-year CUD)\n",
      "Found price: 1.45\n",
      "\n",
      "==== System: LaMDA ====\n",
      "Trying Google TPU v3 at 2021-10-14 07:00:00\n",
      "Trying Google Cloud, Price per chip-hour (3-year CUD)\n",
      "Found price: 0.9\n",
      "\n",
      "==== System: PaLM (540B) ====\n",
      "Trying Google TPU v4 at 2021-12-09 00:00:00\n",
      "Trying Google Cloud, Price per chip-hour (3-year CUD)\n",
      "Found price: 1.45\n",
      "\n",
      "==== System: OPT-175B ====\n",
      "Trying Google TPU v3 at 2022-01-29 23:00:00\n",
      "Trying Google Cloud, Price per chip-hour (3-year CUD)\n",
      "Found price: 0.9\n",
      "\n",
      "==== System: Minerva (540B) ====\n",
      "Trying Google TPU v4 at 2022-04-02 00:00:00\n",
      "Trying Google Cloud, Price per chip-hour (3-year CUD)\n",
      "Found price: 1.45\n",
      "\n",
      "==== System: LLaMA-65B ====\n",
      "Trying NVIDIA A100 at 2022-12-04 04:00:00\n",
      "Trying Amazon Web Services, Price per chip-hour (3-year CUD)\n",
      "Could not find price\n",
      "Trying Amazon Web Services, Price per chip-hour (1-year CUD)\n",
      "Could not find price\n",
      "Trying Amazon Web Services, Price per chip-hour (on-demand)\n",
      "Could not find price\n",
      "Trying Microsoft Azure, Price per chip-hour (3-year CUD)\n",
      "Found price: 1.36\n",
      "\n",
      "==== System: GPT-4 ====\n",
      "Trying Google TPU v4 at 2022-10-12 00:00:00\n",
      "Trying Google Cloud, Price per chip-hour (3-year CUD)\n",
      "Found price: 1.45\n",
      "\n",
      "==== System: Falcon 180B ====\n",
      "Trying NVIDIA A100 SXM4 40 GB at 2023-01-09 00:00:00\n",
      "Trying Amazon Web Services, Price per chip-hour (3-year CUD)\n",
      "Found price: 1.45\n",
      "\n",
      "{'GNMT': 580608.0, 'JFT': 100800.0, 'AmoebaNet-A (F=448)': 105840.0, 'IMPALA': 146.0, 'BigGAN-deep 512x512': 24576.0, 'RoBERTa Large': 52838.4, 'Megatron-BERT': 74924.93821505114, 'Megatron-LM (8.3B)': 202583.04, 'T5-11B': 222059.52, 'AlphaStar': 364953.60000000003, 'Meena': 663552.0, 'GPT-3 175B (davinci)': 4297920.0, 'GShard (dense)': 928972.8, 'ALIGN': 160035.84, 'Megatron-Turing NLG 530B': 6209280.0, 'Gopher (280B)': 3391488.0, 'GLaM': 2028236.8, 'LaMDA': 1276416.0, 'PaLM (540B)': 12187238.4, 'OPT-175B': 731289.6, 'Minerva (540B)': 13220659.200000001, 'LLaMA-65B': 1392640.0, 'GPT-4': 82650000.0, 'Falcon 180B': 25657344.0}\n"
     ]
    }
   ],
   "source": [
    "cost_df_with_imputation = estimate_costs(dropped_df, hardware_df, price_df, impute_pcd_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "System\n",
       "GNMT                                     NVIDIA Tesla K80\n",
       "JFT                                      NVIDIA Tesla K80\n",
       "AmoebaNet-A (F=448)                      NVIDIA Tesla K80\n",
       "IMPALA                                        NVIDIA P100\n",
       "BigGAN-deep 512x512                         Google TPU v3\n",
       "RoBERTa Large                            NVIDIA Tesla K80\n",
       "Megatron-BERT               NVIDIA Tesla V100S PCIe 32 GB\n",
       "Megatron-LM (8.3B)           NVIDIA Tesla V100 DGXS 32 GB\n",
       "T5-11B                                      Google TPU v3\n",
       "AlphaStar                                   Google TPU v3\n",
       "Meena                                       Google TPU v3\n",
       "GPT-3 175B (davinci)         NVIDIA Tesla V100 DGXS 32 GB\n",
       "GShard (dense)                              Google TPU v3\n",
       "ALIGN                                       Google TPU v3\n",
       "Megatron-Turing NLG 530B           NVIDIA A100 SXM4 80 GB\n",
       "Gopher (280B)                               Google TPU v3\n",
       "GLaM                                        Google TPU v4\n",
       "LaMDA                                       Google TPU v3\n",
       "PaLM (540B)                                 Google TPU v4\n",
       "OPT-175B                                    Google TPU v3\n",
       "Minerva (540B)                              Google TPU v4\n",
       "LLaMA-65B                                     NVIDIA A100\n",
       "GPT-4                                       Google TPU v4\n",
       "Falcon 180B                        NVIDIA A100 SXM4 40 GB\n",
       "Name: Training hardware, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost_df_with_imputation['Training hardware']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "System\n",
       "GNMT                        5.806080e+05\n",
       "JFT                         1.008000e+05\n",
       "AmoebaNet-A (F=448)         1.058400e+05\n",
       "IMPALA                      1.460000e+02\n",
       "BigGAN-deep 512x512         2.457600e+04\n",
       "RoBERTa Large               5.283840e+04\n",
       "Megatron-BERT               7.492494e+04\n",
       "Megatron-LM (8.3B)          2.025830e+05\n",
       "T5-11B                      2.220595e+05\n",
       "AlphaStar                   3.649536e+05\n",
       "Meena                       6.635520e+05\n",
       "GPT-3 175B (davinci)        4.297920e+06\n",
       "GShard (dense)              9.289728e+05\n",
       "ALIGN                       1.600358e+05\n",
       "Megatron-Turing NLG 530B    6.209280e+06\n",
       "Gopher (280B)               3.391488e+06\n",
       "GLaM                        2.028237e+06\n",
       "LaMDA                       1.276416e+06\n",
       "PaLM (540B)                 1.218724e+07\n",
       "OPT-175B                    7.312896e+05\n",
       "Minerva (540B)              1.322066e+07\n",
       "LLaMA-65B                   1.392640e+06\n",
       "GPT-4                       8.265000e+07\n",
       "Falcon 180B                 2.565734e+07\n",
       "Name: Cost, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost_df_with_imputation['Cost']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "System\n",
       "AlphaGo Fan                            NaN\n",
       "AlphaGo Lee                            NaN\n",
       "GNMT                          5.806080e+05\n",
       "NASv3 (CIFAR-10)                       NaN\n",
       "AlphaGo Master                         NaN\n",
       "JFT                           1.008000e+05\n",
       "OpenAI TI7 DOTA 1v1                    NaN\n",
       "AlphaGo Zero                           NaN\n",
       "AlphaZero                              NaN\n",
       "AmoebaNet-A (F=448)           3.985070e+03\n",
       "IMPALA                        1.460000e+02\n",
       "ResNeXt-101 32x48d                     NaN\n",
       "FTW                                    NaN\n",
       "BigGAN-deep 512x512           2.457600e+04\n",
       "RoBERTa Large                 1.486848e+05\n",
       "Megatron-BERT                 7.492494e+04\n",
       "Megatron-LM (8.3B)            2.025830e+05\n",
       "T5-3B                                  NaN\n",
       "T5-11B                        2.220595e+05\n",
       "AlphaStar                     3.649536e+05\n",
       "OpenAI Five                            NaN\n",
       "OpenAI Five Rerun                      NaN\n",
       "Meena                         6.635520e+05\n",
       "Turing-NLG                             NaN\n",
       "GPT-3 175B (davinci)          4.297920e+06\n",
       "GShard (dense)                9.289728e+05\n",
       "ALIGN                         1.600358e+05\n",
       "Megatron-Turing NLG 530B      6.209280e+06\n",
       "Yuan 1.0                               NaN\n",
       "Gopher (280B)                 3.391488e+06\n",
       "GLaM                          2.028237e+06\n",
       "LaMDA                         1.276416e+06\n",
       "Chinchilla                             NaN\n",
       "PaLM (540B)                   1.218724e+07\n",
       "OPT-175B                      1.470705e+06\n",
       "Minerva (540B)                1.322066e+07\n",
       "GPT-3.5 (text-davinci-003)             NaN\n",
       "LLaMA-65B                     1.392640e+06\n",
       "GPT-4                         8.265000e+07\n",
       "PaLM 2                                 NaN\n",
       "Claude 2                               NaN\n",
       "Llama 2-70B                            NaN\n",
       "Falcon 180B                   2.565734e+07\n",
       "Name: Cost, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost_df['Cost']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "System\n",
       "AmoebaNet-A (F=448)      105840.0\n",
       "OPT-175B                 731289.6\n",
       "RoBERTa Large             52838.4\n",
       "GShard (dense)           928972.8\n",
       "GPT-4                  82650000.0\n",
       "Name: Cost, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Measure the difference in cost for rows where data was dropped and imputed\n",
    "holdout_costs_with_imputation = cost_df_with_imputation.loc[holdout_df.index, 'Cost']\n",
    "holdout_costs = cost_df.loc[holdout_df.index, 'Cost']\n",
    "\n",
    "holdout_costs_with_imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "System\n",
       "AmoebaNet-A (F=448)    3.985070e+03\n",
       "OPT-175B               1.470705e+06\n",
       "RoBERTa Large          1.486848e+05\n",
       "GShard (dense)         9.289728e+05\n",
       "GPT-4                  8.265000e+07\n",
       "Name: Cost, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "holdout_costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "System\n",
       "AmoebaNet-A (F=448)    2555.912899\n",
       "GPT-4                     0.000000\n",
       "GShard (dense)            0.000000\n",
       "OPT-175B                 50.276243\n",
       "RoBERTa Large            64.462810\n",
       "Name: Cost, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the difference\n",
    "difference = holdout_costs_with_imputation.sub(holdout_costs)\n",
    "\n",
    "# Calculate percentage difference\n",
    "percentage_difference = np.abs((difference / cost_df['Cost']) * 100)\n",
    "\n",
    "# Drop NaN values to keep only Systems with meaningful percentage differences\n",
    "percentage_difference = percentage_difference.dropna()\n",
    "percentage_difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "534.1303904795897"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percentage_difference.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epoch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

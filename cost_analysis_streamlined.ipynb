{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-08T01:54:49.691298800Z",
     "start_time": "2024-03-08T01:54:49.634206900Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-08T01:54:56.468786200Z",
     "start_time": "2024-03-08T01:54:49.695864500Z"
    },
    "id": "qltoZ7TbdkHZ"
   },
   "outputs": [],
   "source": [
    "from contextlib import redirect_stdout\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from cost import *\n",
    "from prices import *\n",
    "from inflation import *\n",
    "from regression import *\n",
    "from utils import *\n",
    "import imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-08T01:54:56.578403700Z",
     "start_time": "2024-03-08T01:54:56.468786200Z"
    }
   },
   "outputs": [],
   "source": [
    "compute_threshold_method = 'top_n'  # top_n, window_percentile\n",
    "compute_threshold = 10  # e.g. 10 to select top 10; 75 to select top 25%\n",
    "variant = '2025-03-17_exclude_finetunes_at_threshold_stage'  # whatever else distinguishes this run, e.g. 'excluding-AlphaGo'\n",
    "exclude_models_containing = []  # ['GNMT', 'AlphaZero', 'AlphaGo Master', 'AlphaGo Zero']\n",
    "\n",
    "# Imputation configuration (matching john_og_version)\n",
    "enable_imputation = True  # Set to False to disable imputation\n",
    "imputation_method = 'most_common'  # 'knn', 'most_common', 'none'\n",
    "knn_neighbors = 5  # Number of neighbors for KNN imputation (if using KNN)\n",
    "\n",
    "# Run all three cost estimation methods\n",
    "estimation_methods = ['hardware-capex-energy', 'hardware-acquisition', 'cloud']\n",
    "estimation_method_lookup = {\n",
    "    'hardware-capex-energy': estimate_hardware_capex_energy,\n",
    "    'hardware-acquisition': estimate_hardware_acquisition_cost,\n",
    "    'cloud': estimate_cloud_costs,\n",
    "}\n",
    "\n",
    "results_dir = f'results/all-methods-{compute_threshold_method}={compute_threshold}-{variant}/'\n",
    "os.makedirs(results_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-08T01:54:57.129865300Z",
     "start_time": "2024-03-08T01:54:56.872275900Z"
    }
   },
   "outputs": [],
   "source": [
    "frontier_pcd_df, hardware_df, price_df = load_data_for_cost_estimation(\n",
    "    compute_threshold_method=compute_threshold_method, compute_threshold=compute_threshold,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-08T01:54:57.278230500Z",
     "start_time": "2024-03-08T01:54:57.129865300Z"
    }
   },
   "outputs": [],
   "source": [
    "len(frontier_pcd_df), len(hardware_df), len(price_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality report before imputation\n",
    "print(\"Data Quality Report (Before Imputation):\")\n",
    "print(f\"Models with known Training hardware: {frontier_pcd_df['Training hardware'].notna().sum()}/{len(frontier_pcd_df)}\")\n",
    "print(f\"Models with known Hardware quantity: {frontier_pcd_df['Hardware quantity'].notna().sum()}/{len(frontier_pcd_df)}\")\n",
    "print(f\"Models with known Hardware utilization: {frontier_pcd_df['Hardware utilization'].notna().sum()}/{len(frontier_pcd_df)}\")\n",
    "print(f\"Models with known Training time (hours): {frontier_pcd_df['Training time (hours)'].notna().sum()}/{len(frontier_pcd_df)}\")\n",
    "\n",
    "# Apply imputation if enabled\n",
    "if enable_imputation and imputation_method != 'none':\n",
    "    print(f\"\\nApplying {imputation_method} imputation...\")\n",
    "    if imputation_method == 'knn':\n",
    "        # Apply KNN imputation - uses the established pipeline from imputation.py\n",
    "        frontier_pcd_df = imputation.knn_impute_pcd(\n",
    "            frontier_pcd_df.copy(), num_neighbors=knn_neighbors\n",
    "        )\n",
    "        print(f\"Applied KNN imputation with {knn_neighbors} neighbors\")\n",
    "    elif imputation_method == 'most_common':\n",
    "        # Apply most common value imputation for training hardware\n",
    "        frontier_pcd_df = imputation.most_common_impute_training_hardware(frontier_pcd_df.copy())\n",
    "        print(\"Applied most common value imputation for training hardware\")\n",
    "    \n",
    "    # Data quality report after imputation\n",
    "    print(\"\\nData Quality Report (After Imputation):\")\n",
    "    print(f\"Models with known Training hardware: {frontier_pcd_df['Training hardware'].notna().sum()}/{len(frontier_pcd_df)}\")\n",
    "    print(f\"Models with known Hardware quantity: {frontier_pcd_df['Hardware quantity'].notna().sum()}/{len(frontier_pcd_df)}\")\n",
    "    print(f\"Models with known Hardware utilization: {frontier_pcd_df['Hardware utilization'].notna().sum()}/{len(frontier_pcd_df)}\")\n",
    "    print(f\"Models with known Training time (hours): {frontier_pcd_df['Training time (hours)'].notna().sum()}/{len(frontier_pcd_df)}\")\n",
    "else:\n",
    "    print(\"\\nSkipping imputation (disabled in configuration)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine imputation function based on configuration\n",
    "if enable_imputation and imputation_method != 'none':\n",
    "    if imputation_method == 'knn':\n",
    "        impute_pcd_fn = imputation.knn_impute_pcd\n",
    "        impute_kwargs = {'num_neighbors': knn_neighbors}\n",
    "    elif imputation_method == 'most_common':\n",
    "        impute_pcd_fn = imputation.most_common_impute_training_hardware\n",
    "        impute_kwargs = {}\n",
    "    else:\n",
    "        impute_pcd_fn = None\n",
    "        impute_kwargs = {}\n",
    "else:\n",
    "    impute_pcd_fn = None\n",
    "    impute_kwargs = {}\n",
    "\n",
    "print(f\"Imputation enabled: {enable_imputation}\")\n",
    "if enable_imputation:\n",
    "    print(f\"Imputation method: {imputation_method}\")\n",
    "    if imputation_method == 'knn':\n",
    "        print(f\"KNN neighbors: {knn_neighbors}\")\n",
    "    print(f\"Imputation function: {impute_pcd_fn}\")\n",
    "else:\n",
    "    print(\"No imputation will be applied\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all three cost estimation methods with imputation support\n",
    "cost_dfs = {}\n",
    "component_cost_df = None\n",
    "\n",
    "for estimation_method in estimation_methods:\n",
    "    print(f\"\\n=== Running {estimation_method} estimation ===\")\n",
    "    cost_estimation_function = estimation_method_lookup[estimation_method]\n",
    "    \n",
    "    with open(f'{results_dir}/cost_estimation_{estimation_method}.out', 'w') as f:\n",
    "        with redirect_stdout(f):\n",
    "            if impute_pcd_fn is not None:\n",
    "                # Call with imputation parameters (matching john_og_version)\n",
    "                cost_df = cost_estimation_function(\n",
    "                    frontier_pcd_df.copy(), hardware_df, price_df,\n",
    "                    impute_pcd_fn=impute_pcd_fn, **impute_kwargs\n",
    "                )\n",
    "            else:\n",
    "                # Call without imputation (original behavior)\n",
    "                cost_df = cost_estimation_function(frontier_pcd_df.copy(), hardware_df, price_df)\n",
    "    \n",
    "    cost_dfs[estimation_method] = cost_df\n",
    "    \n",
    "    # Create component cost breakdown only for hardware-capex-energy method\n",
    "    if estimation_method == 'hardware-capex-energy':\n",
    "        frontier_pcd_df_copy = frontier_pcd_df.copy()\n",
    "        with open(f'{results_dir}/component_cost_estimation.out', 'w') as f:\n",
    "            with redirect_stdout(f):\n",
    "                if impute_pcd_fn is not None:\n",
    "                    component_cost_df = cost_estimation_function(\n",
    "                        frontier_pcd_df_copy, hardware_df, price_df,\n",
    "                        separate_components=True, impute_pcd_fn=impute_pcd_fn, **impute_kwargs\n",
    "                    )\n",
    "                else:\n",
    "                    component_cost_df = cost_estimation_function(\n",
    "                        frontier_pcd_df_copy, hardware_df, price_df, separate_components=True\n",
    "                    )\n",
    "\n",
    "print(f\"\\nCost estimation completed for all methods\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation: Report on imputation impact\n",
    "if enable_imputation and imputation_method != 'none':\n",
    "    print(f\"Imputation Impact Assessment:\")\n",
    "    print(f\"Imputation method used: {imputation_method}\")\n",
    "    if imputation_method == 'knn':\n",
    "        print(f\"KNN neighbors: {knn_neighbors}\")\n",
    "    \n",
    "    # For comparison, calculate how many models would succeed without imputation\n",
    "    # Load original data without imputation\n",
    "    original_frontier_df, _, _ = load_data_for_cost_estimation(\n",
    "        compute_threshold_method=compute_threshold_method, \n",
    "        compute_threshold=compute_threshold\n",
    "    )\n",
    "    \n",
    "    # Run cost estimation WITHOUT imputation to get baseline (using hardware-capex-energy as reference)\n",
    "    print(\"Running baseline cost estimation without imputation for comparison...\")\n",
    "    original_results = estimate_hardware_capex_energy(original_frontier_df, hardware_df, price_df)\n",
    "    original_success_count = original_results['Cost'].notna().sum()\n",
    "    \n",
    "    models_with_missing_data = (\n",
    "        original_frontier_df['Training hardware'].isna() |\n",
    "        original_frontier_df['Hardware quantity'].isna() |\n",
    "        original_frontier_df['Training time (hours)'].isna()\n",
    "    ).sum()\n",
    "    \n",
    "    hardware_capex_success_count = cost_dfs['hardware-capex-energy']['Cost'].notna().sum()\n",
    "    \n",
    "    print(f\"Models with missing critical data (pre-imputation): {models_with_missing_data}\")\n",
    "    print(f\"Models with successful cost estimates WITHOUT imputation: {original_success_count}\")\n",
    "    print(f\"Models with successful cost estimates WITH imputation (hardware-capex-energy): {hardware_capex_success_count}\")\n",
    "    \n",
    "    # Calculate the actual imputation impact\n",
    "    imputation_enabled_count = hardware_capex_success_count - original_success_count\n",
    "    if imputation_enabled_count > 0:\n",
    "        print(f\"âœ“ Imputation enabled cost estimation for {imputation_enabled_count} additional models\")\n",
    "    elif imputation_enabled_count == 0:\n",
    "        print(\"= Imputation did not enable cost estimation for any additional models\")\n",
    "    else:\n",
    "        print(f\"! Imputation resulted in {-imputation_enabled_count} fewer successful cost estimates\")\n",
    "        \n",
    "    # Show specific models that benefited from imputation\n",
    "    if imputation_enabled_count > 0:\n",
    "        imputed_models = cost_dfs['hardware-capex-energy'][\n",
    "            (cost_dfs['hardware-capex-energy']['Cost'].notna()) & \n",
    "            (~cost_dfs['hardware-capex-energy']['Model'].isin(original_results[original_results['Cost'].notna()]['Model']))\n",
    "        ]['Model'].tolist()\n",
    "        \n",
    "        if len(imputed_models) > 0:\n",
    "            print(f\"\\nModels that benefited from imputation:\")\n",
    "            for model in imputed_models:\n",
    "                print(f\"  - {model}\")\n",
    "else:\n",
    "    print(\"Imputation was disabled, so no impact assessment available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputation Impact Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-08T01:54:57.807954200Z",
     "start_time": "2024-03-08T01:54:57.278230500Z"
    }
   },
   "outputs": [],
   "source": [
    "# Run all three cost estimation methods\n",
    "cost_dfs = {}\n",
    "component_cost_df = None\n",
    "\n",
    "for estimation_method in estimation_methods:\n",
    "    print(f\"\\n=== Running {estimation_method} estimation ===\")\n",
    "    cost_estimation_function = estimation_method_lookup[estimation_method]\n",
    "    \n",
    "    with open(f'{results_dir}/cost_estimation_{estimation_method}.out', 'w') as f:\n",
    "        with redirect_stdout(f):\n",
    "            cost_df = cost_estimation_function(frontier_pcd_df.copy(), hardware_df, price_df)\n",
    "    \n",
    "    cost_dfs[estimation_method] = cost_df\n",
    "    \n",
    "    # Create component cost breakdown only for hardware-capex-energy method\n",
    "    if estimation_method == 'hardware-capex-energy':\n",
    "        frontier_pcd_df_copy = frontier_pcd_df.copy()\n",
    "        with open(f'{results_dir}/component_cost_estimation.out', 'w') as f:\n",
    "            with redirect_stdout(f):\n",
    "                component_cost_df = cost_estimation_function(frontier_pcd_df_copy, hardware_df, price_df, separate_components=True)\n",
    "\n",
    "print(f\"\\nCost estimation completed for all methods\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the old conditional component cost creation since it's now handled in the loop above\n",
    "# Display results for each method\n",
    "for method, df in cost_dfs.items():\n",
    "    print(f\"\\n=== {method} results ===\")\n",
    "    print(f\"Total models: {len(df)}\")\n",
    "    print(f\"Models with cost estimates: {df['Cost'].notna().sum()}\")\n",
    "    print(f\"Models with training time: {df.dropna(subset=['Cost'])['Training time (hours)'].notna().sum()}\")\n",
    "    print(f\"Models with hardware utilization: {df.dropna(subset=['Cost'])['Hardware utilization'].notna().sum()}\")\n",
    "    print(f\"Cost range: ${df['Cost'].min():.0f} - ${df['Cost'].max():.0f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-08T01:54:57.967995500Z",
     "start_time": "2024-03-08T01:54:57.807954200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use hardware-capex-energy results as the base for further analysis\n",
    "cost_df = cost_dfs['hardware-capex-energy']\n",
    "cost_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-08T01:54:58.104807Z",
     "start_time": "2024-03-08T01:54:57.967995500Z"
    }
   },
   "outputs": [],
   "source": [
    "cost_df['Cost'].notna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_df.dropna(subset=['Cost'])['Training time (hours)'].notna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_df.dropna(subset=['Cost'])['Hardware utilization'].notna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_df[['Model', 'Publication date']].tail(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply exclusions to all cost dataframes\n",
    "for method in estimation_methods:\n",
    "    for kw in exclude_models_containing:\n",
    "        cost_dfs[method] = cost_dfs[method][cost_dfs[method]['Model'].str.contains(kw) == False]\n",
    "\n",
    "# Show the models after exclusion (using hardware-capex-energy as reference)\n",
    "cost_dfs['hardware-capex-energy'][['Model', 'Publication date']].tail(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the below to check data availability for specific systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-08T01:54:58.418907200Z",
     "start_time": "2024-03-08T01:54:58.109807800Z"
    }
   },
   "outputs": [],
   "source": [
    "# system = 'WizardLM-7B'\n",
    "# row = cost_df.loc[cost_df['Model'] == system]\n",
    "# print('Cost:', row['Cost'].values[0])\n",
    "# print('Training hardware:', row['Training hardware'].values[0])\n",
    "# print('Training time (hours):', row['Training time (hours)'].values[0])\n",
    "# print('Hardware quantity:', row['Hardware quantity'].values[0])\n",
    "# print('Hardware utilization:', row['Hardware utilization'].values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply inflation adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show costs before inflation adjustment (using hardware-capex-energy)\n",
    "cost_dfs['hardware-capex-energy']['Cost'].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-08T01:54:59.049781600Z",
     "start_time": "2024-03-08T01:54:58.881981400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Apply inflation adjustment to all cost dataframes\n",
    "for method in estimation_methods:\n",
    "    cost_dfs[method] = adjust_column_for_inflation(cost_dfs[method], 'Cost', 'data/PCU518210518210.csv', '2024-12-01')\n",
    "\n",
    "# Update the main cost_df reference\n",
    "cost_df = cost_dfs['hardware-capex-energy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-08T01:54:59.177190500Z",
     "start_time": "2024-03-08T01:54:59.049781600Z"
    }
   },
   "outputs": [],
   "source": [
    "cost_df['Cost (inflation-adjusted)'].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-08T01:54:59.344452300Z",
     "start_time": "2024-03-08T01:54:59.182457200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Equal number of non-null values\n",
    "assert cost_df['Cost (inflation-adjusted)'].notna().sum() == cost_df['Cost'].notna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-08T01:54:59.490722200Z",
     "start_time": "2024-03-08T01:54:59.337705700Z"
    }
   },
   "outputs": [],
   "source": [
    "cost_df['Publication date (float)'] = datetime_to_float_year(pd.to_datetime(cost_df['Publication date']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-08T01:55:00.548785500Z",
     "start_time": "2024-03-08T01:54:59.809703300Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_start_year = 2015\n",
    "pred_end_year = 2025\n",
    "pred_start_date = f'{pred_start_year}-01-01'\n",
    "pred_end_date = f'{pred_end_year}-01-01'\n",
    "\n",
    "pred_years = pd.DataFrame({'Publication date (float)': np.linspace(pred_start_year, pred_end_year, 100)})\n",
    "pred_years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-08T01:55:00.564420400Z",
     "start_time": "2024-03-08T01:54:59.920051200Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "predicted_cost_df = get_predictions(reg_results, pred_years, ['Publication date (float)'])\n",
    "predicted_cost_df['Publication date'] = predicted_cost_df['Publication date (float)'].apply(float_year_to_datetime)\n",
    "predicted_cost_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save prediction dataset - this uses hardware-capex-energy method for regression\n",
    "predicted_cost_df.to_csv(results_dir + 'predicted_cost_dataset_hardware_capex_energy.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cost_dataset_3_estimates.csv with Model + 3 cost columns\n",
    "cost_comparison_df = pd.DataFrame()\n",
    "cost_comparison_df['Model'] = cost_dfs['hardware-capex-energy']['Model']\n",
    "\n",
    "# Add inflation-adjusted costs from each method\n",
    "for method in estimation_methods:\n",
    "    method_df = cost_dfs[method]\n",
    "    # Apply inflation adjustment to each method's costs\n",
    "    method_df = adjust_column_for_inflation(method_df, 'Cost', 'data/PCU518210518210.csv', '2024-12-01')\n",
    "    cost_comparison_df[f'{method.replace(\"-\", \"_\")}_cost'] = method_df['Cost (inflation-adjusted)']\n",
    "\n",
    "# Display the comparison\n",
    "print(\"Cost comparison across methods:\")\n",
    "print(cost_comparison_df.dropna().head(10))\n",
    "\n",
    "# Save the 3-method comparison dataset\n",
    "cost_comparison_df.to_csv(results_dir + 'cost_dataset_3_estimates.csv', index=False)\n",
    "print(f\"\\nSaved cost_dataset_3_estimates.csv with {len(cost_comparison_df)} models\")\n",
    "\n",
    "# Also keep the original detailed export for the hardware-capex-energy method\n",
    "keep_cols = [\n",
    "    'Model',\n",
    "    'Domain',\n",
    "    'Task',\n",
    "    'Model accessibility',\n",
    "    'Reference',\n",
    "    'Publication date',\n",
    "    'Organization',\n",
    "    'Parameters',\n",
    "    'Training compute (FLOP)',\n",
    "    'Training dataset size (datapoints)',\n",
    "    'Epochs',\n",
    "    'Training time (hours)',\n",
    "    'Training hardware',\n",
    "    'Base model',\n",
    "    'Finetune compute (FLOP)',\n",
    "    'Hardware quantity',\n",
    "    'Hardware utilization',\n",
    "    'Training cloud compute vendor',\n",
    "    'Training data center',\n",
    "    'Cost',\n",
    "    'Cost (inflation-adjusted)',\n",
    "]\n",
    "cost_df[keep_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep the detailed export for the hardware-capex-energy method\n",
    "cost_df[keep_cols].to_csv(results_dir + 'cost_dataset_detailed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_component_names = [\n",
    "    'AI accelerator chip cost',\n",
    "    'Other server components cost',\n",
    "    'Cluster-level interconnect cost',\n",
    "    'Energy cost',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in cost_component_names:\n",
    "    component_cost_df[f\"{key} (%)\"] = component_cost_df[key] / component_cost_df['Cost'] * 100\n",
    "component_cost_df['AI accelerator chip cost (%)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_component_pc_names = [name + ' (%)' for name in cost_component_names]\n",
    "filtered_component_cost_df = component_cost_df.dropna(subset=cost_component_pc_names).sort_values(by='Publication date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_component_cost_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_component_cost_df.to_csv(results_dir + 'cost_components.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average percentage for each component\n",
    "filtered_component_cost_df[cost_component_pc_names].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_component_cost_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_component_cost_df = filtered_component_cost_df.dropna(subset=['Training hardware'])\n",
    "power_col = 'Power capacity for final training run (kW)'\n",
    "filtered_component_cost_df.loc[:, power_col] = [\n",
    "    cluster_power_capacity(row['Training hardware'], row['Hardware quantity'], hardware_df, row['Organization'])\n",
    "    for _, row in filtered_component_cost_df.iterrows()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_component_cost_df['Publication date (float)'] = datetime_to_float_year(\n",
    "    pd.to_datetime(filtered_component_cost_df['Publication date'])\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

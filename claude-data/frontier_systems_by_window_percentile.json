{
    "95": [
        "AlphaGo Master",
        "AlphaGo Zero",
        "AlphaZero",
        "ERNIE 3.0 Titan",
        "GNMT",
        "GPT-3 175B (davinci)",
        "GPT-4",
        "Gemini 1.0 Ultra",
        "Gopher (280B)",
        "Meena",
        "Megatron-Turing NLG 530B",
        "OpenAI Five",
        "PaLM (540B)"
    ],
    "90": [
        "AlphaGo Lee",
        "AlphaStar",
        "Chinchilla",
        "GLaM",
        "GPT-3.5",
        "GPT-4 Turbo",
        "Jurassic-1-Jumbo",
        "NASv3 (CIFAR-10)",
        "OPT-175B",
        "Parti",
        "ResNeXt-101 32x48d",
        "T5-11B",
        "mT5-XXL"
    ],
    "85": [
        "AlphaGo Fan",
        "GShard (dense)",
        "JFT",
        "LaMDA",
        "Megatron-BERT",
        "Noisy Student (L2)",
        "OpenAI TI7 DOTA 1v1",
        "PaLM 2",
        "Switch",
        "XLM-RoBERTa",
        "Yuan 1.0"
    ],
    "80": [
        "AlphaCode",
        "AmoebaNet-A (F=448)",
        "BLOOM-176B",
        "Big Transformer for Back-Translation",
        "BigGAN-deep 512x512",
        "DALL\u00b7E 2",
        "HyperCLOVA 204B",
        "Inflection-2",
        "Libratus",
        "Megatron-LM (8.3B)",
        "OpenAI Five Rerun",
        "ST-MoE",
        "T5-3B",
        "Turing-NLG",
        "XLNet",
        "Xception"
    ],
    "75": [
        "Amazon Titan",
        "BERT-Large",
        "ByT5-XXL",
        "Claude 2",
        "DALL-E",
        "GLM-130B",
        "GPT-2 (1.5B)",
        "GPT-NeoX-20B",
        "Inception v3",
        "Meta Pseudo Labels",
        "ProtT5-XXL",
        "RoBERTa Large",
        "ViT-G/14",
        "iGPT-XL"
    ],
    "70": [
        "AlexaTM 20B",
        "CoAtNet",
        "ContextNet + Noisy Student",
        "ConvS2S (ensemble of 8 models)",
        "Falcon-180B",
        "Florence",
        "Galactica",
        "LLaMA-65B",
        "Mesh-TensorFlow Transformer 4.9B (language)",
        "MnasNet-A1 + SSDLite",
        "MnasNet-A3",
        "MoE-Multi",
        "PNASNet-5",
        "PolyNet",
        "Transformer-XL (257M)",
        "UL2"
    ],
    "65": [
        "BASIC-L",
        "DeepSpeech2 (English)",
        "GLIDE",
        "Grok-1",
        "InternLM",
        "Mesh-TensorFlow Transformer 2.9B (translation)",
        "PaLI",
        "PanGu-\u03a3",
        "ProtBERT-BFD",
        "ProtT5-XXL-BFD",
        "Stable Diffusion (LDM-KL-8-G)"
    ],
    "60": [
        "ALBERT-xxlarge",
        "ALIGN",
        "BERT-Large-CAS (PTB+WT2+WT103)",
        "CoCa",
        "CogView",
        "DeBERTa",
        "ELECTRA",
        "ESM2-15B",
        "LSTM (Hebbian, Cache, MbPA)",
        "Llama 2-70B",
        "QT-Opt",
        "ResNet-200",
        "Student of Games",
        "Transformer (Adaptive Input Embeddings) WT103",
        "ViT-22B",
        "XLMR-XXL",
        "iGPT-L",
        "xTrimoPGLM -100B"
    ],
    "55": [
        "BloombergGPT",
        "CLIP (ViT L/14@336px)",
        "CamemBERT",
        "Conformer + Wav2vec 2.0 + Noisy Student",
        "DeepStack",
        "ERNIE 3.0",
        "Falcon-40B",
        "GOAT",
        "GPT-1",
        "Gemini 1.0 Pro",
        "Generative BST",
        "ProtT5-XL-U50",
        "ResNet-152 (ImageNet)",
        "SEER",
        "SciBERT",
        "Transformer",
        "Vega v2",
        "YOLOv3"
    ],
    "50": [
        "DD-PPO",
        "FAIRSEQ Adaptive Inputs",
        "NLLB",
        "Qwen-72B",
        "RETRO-7B",
        "StarCoder",
        "Transformer + Simple Recurrent Unit",
        "XGLM-7.5B"
    ],
    "45": [
        "AWD-LSTM-MoS + dynamic evaluation (WT2, 2017)",
        "AlphaFold",
        "AlphaFold 2",
        "BIDAF",
        "ESM1b",
        "Imagen",
        "KataGo",
        "M6-T",
        "MSA Transformer",
        "Mixtral 8x7B",
        "ProGen2-xlarge",
        "RetinaNet-R101",
        "ViT-Huge/14",
        "Yi-34B",
        "wave2vec 2.0 LARGE"
    ],
    "40": [
        "(ensemble): AWD-LSTM-DOC (fin) \u00d7 5 (WT2)",
        "ADM",
        "DETR",
        "EVA-01",
        "GBERT-Large",
        "GL-LWGC-AWD-MoS-LSTM + dynamic evaluation (WT2)",
        "German ELECTRA Large",
        "HuBERT",
        "Llama 2-7B",
        "MuZero",
        "N\u00dcWA",
        "OmegaPLM",
        "Once for All",
        "PLATO-XL",
        "Part-of-sentence tagging model",
        "QRNN",
        "R-FCN",
        "Skywork-13B",
        "Tranception",
        "TrellisNet"
    ],
    "35": [
        "4 layer QRNN (h=2500)",
        "AudioGen",
        "Base LM + kNN LM + Continuous Cache",
        "DistilBERT",
        "Hanabi 4 player",
        "Named Entity Recognition model",
        "Nucleotide Transformer",
        "Pangu-Weather",
        "Sandwich Transformer"
    ],
    "30": [
        "ATLAS",
        "AWD-LSTM - 3-layer LSTM (tied) + continuous cache pointer (WT2)",
        "Ankh_large",
        "CPM-Large",
        "ChatGLM3-6B",
        "CodeT5-large",
        "DINOv2",
        "DLRM-2020",
        "EMDR",
        "ERNIE-GEN (large)",
        "Fraternal dropout + AWD-LSTM 3-layer (WT2)",
        "Gato",
        "Hybrid H3-2.7B",
        "Jais",
        "Nemotron-3-8B",
        "ProxylessNAS",
        "TaLK Convolution",
        "Tensorized Transformer (257M)",
        "TransformerXL + spectrum control",
        "ViT-G (model soup)",
        "Whisper"
    ],
    "25": [
        "Big-Little Net (speech)",
        "CodeT5-base",
        "Feedback Transformer",
        "GraphCast",
        "Incoder-6.7B",
        "InternImage",
        "PolyCoder",
        "SPIDER2",
        "SigLIP 400M",
        "Swin Transformer V2 (SwinV2-G)",
        "VD-LSTM+REAL Large"
    ],
    "20": [
        "BLIP-2 (Q-Former)",
        "DARTS",
        "Decoupled weight decay regularization",
        "DeiT-B",
        "GenSLM",
        "HyenaDNA",
        "Sparse all-MLP",
        "ViT + DINO",
        "WeNet (Penn Treebank)",
        "YOLOX-X",
        "Zidong Taichu",
        "Zoneout + Variational LSTM (WT2)"
    ],
    "15": [
        "Adaptive Input Transformer + RD",
        "AlphaX-1",
        "Big-Little Net",
        "DDPM-IP (CelebA)",
        "DNABERT",
        "Denoising Diffusion Probabilistic Models (LSUN Bedroom)",
        "Discriminator Guidance",
        "DreamerV3",
        "Dropout-LSTM+Noise(Bernoulli) (WT2)",
        "ENAS",
        "ERNIE-Doc (247M)",
        "EfficientNetV2-XL",
        "MMLSTM",
        "NAS with base 8 and shared embeddings",
        "Refined Part Pooling",
        "Transformer-XL DeFINE (141M)",
        "aLSTM(depth-2)+RecurrentPolicy (WT2)"
    ],
    "10": [
        "BEIT-3",
        "DiffDock",
        "EI-REHN-1000D",
        "ELMo",
        "Fusion in Encoder",
        "MEGNet (crystal band gap model)",
        "MEGNet (crystal elasticity model)",
        "MEGNet (crystal formation energy model)",
        "MEGNet (molecule model)",
        "NAS+ESS (156M)",
        "ONE-PEACE",
        "Pointer Sentinel-LSTM (medium)",
        "ProBERTa",
        "ProteinBERT",
        "S4",
        "SRU++ Large",
        "Tensor-Transformer(1core)+PN (WT103)",
        "UDSMProt",
        "Variational (untied weights, MC) LSTM (Large)"
    ],
    "5": [
        "AWD-LSTM + MoS + Partial Shuffled",
        "AWD-LSTM+WT+Cache+IOG (WT2)",
        "AWD-LSTM-DRILL + dynamic evaluation\u2020 (WT2)",
        "CaLM",
        "DeLighT",
        "Detic",
        "ISS",
        "Projected GAN",
        "Segatron-XL large, M=384 + HCP",
        "Transformer local-attention (NesT-B)",
        "VALL-E",
        "eDiff-I"
    ],
    "0": [
        "2-layer-LSTM+Deep-Gradient-Compression",
        "AudioLM",
        "CODEFUSION (Python)",
        "DITTO",
        "DensePhrases",
        "MedBERT",
        "Mogrifier RLSTM (WT2)",
        "Multi-cell LSTM",
        "MultiBand Diffusion",
        "PermuteFormer",
        "Pluribus",
        "PyramidNet",
        "VD-RHN"
    ]
}
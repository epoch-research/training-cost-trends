{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Production cost of a DGX H100 server\n",
    "\n",
    "Raymond James (a global financial services company) estimated that it costs Nvidia \\$3,320 to make a H100, which is then sold to customers for \\$25,000 to \\$30,000. This was [reported](https://twitter.com/firstadopter/status/1691877797487165443) second-hand via Tae Kim (a senior writer for financial and investment news site Barrons), who added \"[High bandwidth memory] was included in their BOM estimates\".\n",
    "\n",
    "However, this doesn't seem to account for off-board components in a DGX server, such as CPUs, tranceivers and switches. For that, we use this SemiAnalysis breakdown of DGX H100 server cost (includes networking hardware for the server): https://www.semianalysis.com/p/ai-server-cost-analysis-memory-is. Note that the HGX model is more applicable to large-scale clusters, but we don't have information about HGX so we assume it is similar to DGX in costs.\n",
    "  - Sale price of DGX H100 estimated at ~\\$270,000\n",
    "  - “Nvidia's gross profit per DGX H100 is almost \\$190,000. Of course, R&D and other operating expenses bring this much lower.”\n",
    "  - So SemiAnalysis implies that DGX H100 costs \\$80,000 to make."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consistency check:\n",
    "\n",
    "If the Raymond James estimate of \\$3,320 only includes the H100 chip and peripherals, it's actually not way off the cost implied by SemiAnalysis, at \\$5,875 (and that includes an additional cost of a 4 NVSwitch Baseboard, which may not be in the Raymond James estimate). See calculation below. Note that the SemiAnalysis estimate was published in May 2023 while Tae Kim's post was in August 2023.\n",
    "\n",
    "A colleague who independently estimated the cost of the H100 GPU also informed us that they got roughly $3,000 (based on TSMC wafer prices, among other things)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32010"
      ]
     },
     "execution_count": 491,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using SemiAnalysis BOM https://www.semianalysis.com/p/ai-server-cost-analysis-memory-is\n",
    "non_gpu_dgx_cost = 5200 + 7860 + 3456 + 10908 + 563 + 875 + 463 + 1200 + 1485\n",
    "non_gpu_dgx_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5875.0"
      ]
     },
     "execution_count": 492,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dgx_h100_cost = 269010 - 190000\n",
    "gpu_and_baseboard_cost = dgx_h100_cost - non_gpu_dgx_cost\n",
    "gpu_per_dgx = 8\n",
    "gpu_and_baseboard_cost_per_gpu = gpu_and_baseboard_cost / gpu_per_dgx\n",
    "gpu_and_baseboard_cost_per_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting it all together to get the average overall expense per H100 sold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median: 4.3e+03 [90% CI: 2.5e+03, 7.9e+03]\n"
     ]
    }
   ],
   "source": [
    "# We have two precise cost estimates for an H100 GPU: $3,320 for the GPU and $5,875 for the GPU plus NVSwitch baseboard.\n",
    "# Based on that we'll centre on $4,500 and range from $2,500 to $8000 to be conservative.\n",
    "gpu_and_baseboard_cost_per_gpu = lognorm_from_90_ci(2500, 8000, NUM_SAMPLES)\n",
    "print_median_and_ci(gpu_and_baseboard_cost_per_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median: 3.2e+04 [90% CI: 2.1e+04, 4.8e+04]\n"
     ]
    }
   ],
   "source": [
    "# Also using conservative bounds on additional server components, centred on the SemiAnalysis estimate\n",
    "non_gpu_dgx_cost_dist = lognorm_from_90_ci(non_gpu_dgx_cost / 1.5, non_gpu_dgx_cost * 1.5, NUM_SAMPLES)\n",
    "print_median_and_ci(non_gpu_dgx_cost_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median: 6.7e+04 [90% CI: 5.2e+04, 9.6e+04]\n"
     ]
    }
   ],
   "source": [
    "dgx_h100_cost = gpu_and_baseboard_cost_per_gpu * gpu_per_dgx + non_gpu_dgx_cost\n",
    "print_median_and_ci(dgx_h100_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're making an analogy between large clusters of H100 servers and large clusters of TPUv4 servers, so we base the average production cost on the DGX server cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median: 8.3e+03 [90% CI: 6.5e+03, 1.2e+04]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8665.028437749053"
      ]
     },
     "execution_count": 496,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dgx_h100_cost_per_gpu = dgx_h100_cost / gpu_per_dgx\n",
    "print_median_and_ci(dgx_h100_cost_per_gpu)\n",
    "dgx_h100_cost_per_gpu.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epoch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

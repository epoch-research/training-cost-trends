{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60910114",
   "metadata": {},
   "source": [
    "This script takes a price dataset and a CSV of foundation models as input, and outputs the price dataset with an indicator column showing which models are foundation models.\n",
    "\n",
    "The gold standard list of foundation models comes from this table provided by Stanford's Center for Research on Foundation Models:\n",
    "\n",
    "https://github.com/stanford-crfm/ecosystem-graphs/blob/main/resources/all_assets.csv (last updated on 2024-02-20)\n",
    "\n",
    "Place it in the folder `CRFM data` so that it can be loaded below.\n",
    "\n",
    "Then, place the price dataset in this folder with the filename `price dataset.csv` and run this script. It will produce a copy with one more boolean column, saved as `price dataset with FM indicator.csv`.\n",
    "\n",
    "Note that some models are named differently in the Epoch database and the CRFM dataset, so you should manually check the models that are not matched as foundation models to see if there is a match with a slightly different name (often with/without a parameter count)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36642596",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02550475",
   "metadata": {},
   "source": [
    "Import the CRFM foundation model ecosystem table of models, datasets, and applications, then filter to models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "272fefcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "crfm_table = pd.read_csv('CRFM data/all_assets.csv')\n",
    "price_dataset = pd.read_csv('../results/ai-index-final/price dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "44eda2d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>System</th>\n",
       "      <th>Cost (inflation-adjusted)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gemini Ultra</td>\n",
       "      <td>1.914000e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Falcon 180B</td>\n",
       "      <td>2.581632e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Llama 2-70B</td>\n",
       "      <td>3.931897e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>StarCoder</td>\n",
       "      <td>5.856264e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>BloombergGPT</td>\n",
       "      <td>9.529677e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>GPT-4</td>\n",
       "      <td>7.835203e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Falcon-40B</td>\n",
       "      <td>8.103978e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LLaMA-65B</td>\n",
       "      <td>1.503763e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>BLOOM-176B</td>\n",
       "      <td>1.990912e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>U-PaLM (540B)</td>\n",
       "      <td>1.252379e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>PaLI</td>\n",
       "      <td>2.547193e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>GLM-130B</td>\n",
       "      <td>1.635573e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Minerva (540B)</td>\n",
       "      <td>1.347019e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Imagen</td>\n",
       "      <td>3.619903e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>UL2</td>\n",
       "      <td>5.610850e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>OPT-175B</td>\n",
       "      <td>1.493975e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Flamingo</td>\n",
       "      <td>8.150695e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>PaLM (540B)</td>\n",
       "      <td>1.238906e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>LaMDA</td>\n",
       "      <td>1.319586e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>GPT-NeoX-20B</td>\n",
       "      <td>3.108410e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>XGLM-7.5B</td>\n",
       "      <td>1.930600e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>GLaM</td>\n",
       "      <td>2.093016e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Gopher (280B)</td>\n",
       "      <td>3.499808e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Florence</td>\n",
       "      <td>1.724109e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>T0-XXL</td>\n",
       "      <td>6.417537e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Megatron-Turing NLG 530B</td>\n",
       "      <td>6.405653e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>HyperCLOVA</td>\n",
       "      <td>9.869716e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>ALIGN</td>\n",
       "      <td>1.655001e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Meta Pseudo Labels</td>\n",
       "      <td>2.513914e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Switch</td>\n",
       "      <td>6.197414e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>GShard (dense)</td>\n",
       "      <td>9.691116e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>GPT-3 175B (davinci)</td>\n",
       "      <td>4.324883e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>Meena</td>\n",
       "      <td>6.971103e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>AlphaStar</td>\n",
       "      <td>3.889027e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>T5-11B</td>\n",
       "      <td>2.366316e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Megatron-BERT</td>\n",
       "      <td>8.817783e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Megatron-LM (8.3B)</td>\n",
       "      <td>2.160704e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>RoBERTa Large</td>\n",
       "      <td>1.600176e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>SciBERT</td>\n",
       "      <td>6.387439e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>BERT-Large</td>\n",
       "      <td>3.287635e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>BigGAN-deep 512x512</td>\n",
       "      <td>1.164818e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>Transformer (Adaptive Input Embeddings)</td>\n",
       "      <td>5.358052e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>Big Transformer for Back-Translation</td>\n",
       "      <td>4.434402e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>IMPALA</td>\n",
       "      <td>1.381309e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>JFT</td>\n",
       "      <td>3.344073e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>Transformer</td>\n",
       "      <td>9.299259e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>Xception</td>\n",
       "      <td>2.028533e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>GNMT</td>\n",
       "      <td>1.947392e+05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      System  Cost (inflation-adjusted)\n",
       "0                               Gemini Ultra               1.914000e+08\n",
       "9                                Falcon 180B               2.581632e+07\n",
       "11                               Llama 2-70B               3.931897e+06\n",
       "16                                 StarCoder               5.856264e+05\n",
       "18                              BloombergGPT               9.529677e+05\n",
       "20                                     GPT-4               7.835203e+07\n",
       "21                                Falcon-40B               8.103978e+05\n",
       "22                                 LLaMA-65B               1.503763e+06\n",
       "26                                BLOOM-176B               1.990912e+06\n",
       "28                             U-PaLM (540B)               1.252379e+07\n",
       "30                                      PaLI               2.547193e+05\n",
       "32                                  GLM-130B               1.635573e+06\n",
       "36                            Minerva (540B)               1.347019e+07\n",
       "39                                    Imagen               3.619903e+04\n",
       "40                                       UL2               5.610850e+05\n",
       "41                                  OPT-175B               1.493975e+06\n",
       "42                                  Flamingo               8.150695e+05\n",
       "44                               PaLM (540B)               1.238906e+07\n",
       "46                                     LaMDA               1.319586e+06\n",
       "47                              GPT-NeoX-20B               3.108410e+05\n",
       "50                                 XGLM-7.5B               1.930600e+05\n",
       "51                                      GLaM               2.093016e+06\n",
       "52                             Gopher (280B)               3.499808e+06\n",
       "53                                  Florence               1.724109e+05\n",
       "55                                    T0-XXL               6.417537e+04\n",
       "57                  Megatron-Turing NLG 530B               6.405653e+06\n",
       "58                                HyperCLOVA               9.869716e+05\n",
       "62                                     ALIGN               1.655001e+05\n",
       "72                        Meta Pseudo Labels               2.513914e+05\n",
       "74                                    Switch               6.197414e+05\n",
       "80                            GShard (dense)               9.691116e+05\n",
       "83                      GPT-3 175B (davinci)               4.324883e+06\n",
       "88                                     Meena               6.971103e+05\n",
       "94                                 AlphaStar               3.889027e+05\n",
       "96                                    T5-11B               2.366316e+05\n",
       "97                             Megatron-BERT               8.817783e+05\n",
       "98                        Megatron-LM (8.3B)               2.160704e+05\n",
       "99                             RoBERTa Large               1.600176e+05\n",
       "103                                  SciBERT               6.387439e+02\n",
       "106                               BERT-Large               3.287635e+03\n",
       "107                      BigGAN-deep 512x512               1.164818e+04\n",
       "108  Transformer (Adaptive Input Embeddings)               5.358052e+03\n",
       "110     Big Transformer for Back-Translation               4.434402e+03\n",
       "118                                   IMPALA               1.381309e+02\n",
       "123                                      JFT               3.344073e+04\n",
       "124                              Transformer               9.299259e+02\n",
       "132                                 Xception               2.028533e+04\n",
       "133                                     GNMT               1.947392e+05"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price_dataset[['System', 'Cost (inflation-adjusted)']].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90ca3909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>organization</th>\n",
       "      <th>description</th>\n",
       "      <th>created_date</th>\n",
       "      <th>url</th>\n",
       "      <th>datasheet</th>\n",
       "      <th>modality</th>\n",
       "      <th>size</th>\n",
       "      <th>sample</th>\n",
       "      <th>...</th>\n",
       "      <th>adaptation</th>\n",
       "      <th>output_space</th>\n",
       "      <th>terms_of_service</th>\n",
       "      <th>monthly_active_users</th>\n",
       "      <th>user_distribution</th>\n",
       "      <th>failures</th>\n",
       "      <th>model_card</th>\n",
       "      <th>training_emissions</th>\n",
       "      <th>training_time</th>\n",
       "      <th>training_hardware</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>model</td>\n",
       "      <td>FalconLite2</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>FalconLite2 is a fine-tuned and quantized Falc...</td>\n",
       "      <td>2023-08-08</td>\n",
       "      <td>https://huggingface.co/amazon/FalconLite2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>text; text</td>\n",
       "      <td>40B parameters (dense)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://huggingface.co/amazon/FalconLite2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>model</td>\n",
       "      <td>StarCoder</td>\n",
       "      <td>BigCode</td>\n",
       "      <td>StarCoder is a Large Language Model for Code (...</td>\n",
       "      <td>2023-05-09</td>\n",
       "      <td>https://arxiv.org/pdf/2305.06161.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>code; code</td>\n",
       "      <td>15.5B parameters (dense)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://huggingface.co/bigcode/starcoder</td>\n",
       "      <td>16.68 tons of CO2eq</td>\n",
       "      <td>2 days</td>\n",
       "      <td>64 NVIDIA A100 GPUs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>model</td>\n",
       "      <td>SantaCoder</td>\n",
       "      <td>BigCode</td>\n",
       "      <td>Multilingual code model derived from findings ...</td>\n",
       "      <td>2023-02-24</td>\n",
       "      <td>https://arxiv.org/pdf/2301.03988.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>code; code</td>\n",
       "      <td>1.1B parameters (dense)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.1 days</td>\n",
       "      <td>96 NVIDIA Tesla V100 GPUs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>model</td>\n",
       "      <td>YaLM</td>\n",
       "      <td>Yandex</td>\n",
       "      <td>YaLM is a 100B parameter autoregressive model ...</td>\n",
       "      <td>2022-06-22</td>\n",
       "      <td>https://medium.com/yandex/yandex-publishes-yal...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>text; text</td>\n",
       "      <td>100B parameters (dense)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yandex 800 A100 Cluster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>model</td>\n",
       "      <td>Koala</td>\n",
       "      <td>Berkeley</td>\n",
       "      <td>A relatively small chatbot trained by fine-tun...</td>\n",
       "      <td>2023-04-03</td>\n",
       "      <td>https://bair.berkeley.edu/blog/2023/04/03/koala/</td>\n",
       "      <td>NaN</td>\n",
       "      <td>text; text</td>\n",
       "      <td>13B parameters (dense)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://huggingface.co/TheBloke/koala-7B-GPTQ-...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6 hours</td>\n",
       "      <td>8 A100 GPUs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     type         name organization  \\\n",
       "2   model  FalconLite2       Amazon   \n",
       "3   model    StarCoder      BigCode   \n",
       "4   model   SantaCoder      BigCode   \n",
       "6   model         YaLM       Yandex   \n",
       "10  model        Koala     Berkeley   \n",
       "\n",
       "                                          description created_date  \\\n",
       "2   FalconLite2 is a fine-tuned and quantized Falc...   2023-08-08   \n",
       "3   StarCoder is a Large Language Model for Code (...   2023-05-09   \n",
       "4   Multilingual code model derived from findings ...   2023-02-24   \n",
       "6   YaLM is a 100B parameter autoregressive model ...   2022-06-22   \n",
       "10  A relatively small chatbot trained by fine-tun...   2023-04-03   \n",
       "\n",
       "                                                  url datasheet    modality  \\\n",
       "2           https://huggingface.co/amazon/FalconLite2       NaN  text; text   \n",
       "3                https://arxiv.org/pdf/2305.06161.pdf       NaN  code; code   \n",
       "4                https://arxiv.org/pdf/2301.03988.pdf       NaN  code; code   \n",
       "6   https://medium.com/yandex/yandex-publishes-yal...       NaN  text; text   \n",
       "10   https://bair.berkeley.edu/blog/2023/04/03/koala/       NaN  text; text   \n",
       "\n",
       "                        size sample  ... adaptation output_space  \\\n",
       "2     40B parameters (dense)    NaN  ...        NaN          NaN   \n",
       "3   15.5B parameters (dense)    NaN  ...        NaN          NaN   \n",
       "4    1.1B parameters (dense)    NaN  ...        NaN          NaN   \n",
       "6    100B parameters (dense)    NaN  ...        NaN          NaN   \n",
       "10    13B parameters (dense)    NaN  ...        NaN          NaN   \n",
       "\n",
       "   terms_of_service monthly_active_users user_distribution failures  \\\n",
       "2               NaN                  NaN               NaN      NaN   \n",
       "3               NaN                  NaN               NaN      NaN   \n",
       "4               NaN                  NaN               NaN      NaN   \n",
       "6               NaN                  NaN               NaN      NaN   \n",
       "10              NaN                  NaN               NaN      NaN   \n",
       "\n",
       "                                           model_card   training_emissions  \\\n",
       "2           https://huggingface.co/amazon/FalconLite2                  NaN   \n",
       "3            https://huggingface.co/bigcode/starcoder  16.68 tons of CO2eq   \n",
       "4                                                 NaN                  NaN   \n",
       "6                                                 NaN                  NaN   \n",
       "10  https://huggingface.co/TheBloke/koala-7B-GPTQ-...                  NaN   \n",
       "\n",
       "   training_time          training_hardware  \n",
       "2            NaN                        NaN  \n",
       "3         2 days        64 NVIDIA A100 GPUs  \n",
       "4       3.1 days  96 NVIDIA Tesla V100 GPUs  \n",
       "6            NaN    Yandex 800 A100 Cluster  \n",
       "10       6 hours                8 A100 GPUs  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crfm_models = crfm_table[crfm_table.type == 'model']\n",
    "crfm_models.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37945dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "foundation_models = crfm_models['name'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0669249c",
   "metadata": {},
   "source": [
    "The list of foundation models in the CRFM dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "054124c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FalconLite2',\n",
       " 'StarCoder',\n",
       " 'SantaCoder',\n",
       " 'YaLM',\n",
       " 'Koala',\n",
       " 'Gorilla',\n",
       " 'OpenLLaMA',\n",
       " 'PolyCoder',\n",
       " 'CodeGeeX',\n",
       " 'CogView',\n",
       " 'CogView 2',\n",
       " 'CogVideo',\n",
       " 'GLM-130B',\n",
       " 'CogVLM',\n",
       " 'Jurassic-1',\n",
       " 'Jurassic-1 Instruct',\n",
       " 'Jurassic-2',\n",
       " 'Inflection-1',\n",
       " 'Inflection-2',\n",
       " 'GenSLM',\n",
       " 'Falcon-40B',\n",
       " 'Falcon-180B',\n",
       " 'Mistral',\n",
       " 'T5',\n",
       " 'Internal Google BERT',\n",
       " 'LaMDA',\n",
       " 'Flan-T5',\n",
       " 'UL2',\n",
       " 'Parti',\n",
       " 'Imagen',\n",
       " 'VATT',\n",
       " 'PaLM',\n",
       " 'Med-PaLM',\n",
       " 'Med-PaLM Multimodal',\n",
       " 'MultiMedQA',\n",
       " 'Flan-PaLM',\n",
       " 'Flan-U-PaLM',\n",
       " 'U-PaLM',\n",
       " 'PaLM-SayCan',\n",
       " 'GLaM',\n",
       " 'MUM',\n",
       " 'Phenaki',\n",
       " 'Flan-UL2',\n",
       " 'MusicLM',\n",
       " 'SoundStream',\n",
       " 'w2v-BERT',\n",
       " 'MuLan',\n",
       " 'MusicLM semantic model',\n",
       " 'MusicLM acoustic model',\n",
       " 'Noise2Music',\n",
       " 'Noise2Music pseudolabeler',\n",
       " 'Minerva',\n",
       " 'USM',\n",
       " 'PaLM-E',\n",
       " 'ViT-22B',\n",
       " 'AudioLM',\n",
       " 'PaLI',\n",
       " 'ViT-e',\n",
       " 'Vid2Seq',\n",
       " 'Google Joint SLM',\n",
       " 'PaLM 2',\n",
       " 'MedLM',\n",
       " 'Gemini',\n",
       " 'Skywork',\n",
       " 'GPT-J',\n",
       " 'GPT-Neo',\n",
       " 'GPT-NeoX',\n",
       " 'VQGAN-CLIP',\n",
       " 'Pythia',\n",
       " 'Llemma',\n",
       " 'GPT-2',\n",
       " 'GPT-3',\n",
       " 'Codex',\n",
       " 'InstructGPT',\n",
       " 'Whisper',\n",
       " 'CLIP',\n",
       " 'DALL·E',\n",
       " 'Jukebox',\n",
       " 'DALL·E 2',\n",
       " 'VPT',\n",
       " 'gpt-3.5-turbo',\n",
       " 'GPT-4 Turbo',\n",
       " 'code-davinci-002',\n",
       " 'text-davinci-002',\n",
       " 'text-davinci-003',\n",
       " 'OpenAI toxicity classifier',\n",
       " 'Sage',\n",
       " 'Dragonfly',\n",
       " 'GPT-4',\n",
       " 'DALL·E 3',\n",
       " 'h2oGPT',\n",
       " 'H2O Danube',\n",
       " 'OpenFlamingo',\n",
       " 'Anthropic RLHF models',\n",
       " 'Claude',\n",
       " 'Claude Instant',\n",
       " 'Claude 2',\n",
       " 'Claude 2.1',\n",
       " 'Guanaco',\n",
       " 'Llark',\n",
       " 'MPT',\n",
       " 'CommonCanvas',\n",
       " 'OpenBA',\n",
       " 'Neeva model',\n",
       " 'Nous Hermes 2',\n",
       " 'Grok-1',\n",
       " 'MAmmoTH',\n",
       " 'T0++',\n",
       " 'BLOOM',\n",
       " 'mT0',\n",
       " 'BLOOMZ',\n",
       " 'Wu Dao 2.0',\n",
       " 'JudgeLM',\n",
       " 'SegMamba',\n",
       " 'BGE M3 Embedding',\n",
       " 'Palmyra',\n",
       " 'Camel',\n",
       " 'Cerebras-GPT',\n",
       " 'Jais',\n",
       " 'Jais Chat',\n",
       " 'MediTron',\n",
       " 'BioMedLM',\n",
       " 'RoentGen',\n",
       " 'CORGI',\n",
       " 'Alpaca',\n",
       " 'ChatGLM',\n",
       " 'ERNIE 3.0 Titan',\n",
       " 'ERNIE-ViLG',\n",
       " 'ERNIE-ViLG 2.0',\n",
       " 'ERNIE 4.0',\n",
       " 'Conformer-1',\n",
       " 'DeepFloyd IF',\n",
       " 'StableLM',\n",
       " 'Stable Video Diffusion',\n",
       " 'Firefly Image 2',\n",
       " 'Firefly Vector',\n",
       " 'Firefly Design',\n",
       " 'GOAT',\n",
       " 'OpenMoE',\n",
       " 'Pegasus-1',\n",
       " 'Vicuna',\n",
       " 'Starling',\n",
       " 'ACT-1',\n",
       " 'Persimmon',\n",
       " 'Fuyu',\n",
       " 'Fuyu Heavy',\n",
       " 'Notus',\n",
       " 'BiomedGPT',\n",
       " 'BigTrans',\n",
       " 'YAYI 2',\n",
       " 'Ocean-1',\n",
       " 'InternVideo',\n",
       " 'Lego-MT',\n",
       " 'MathCoder',\n",
       " 'InternLM',\n",
       " 'Megatron-LM',\n",
       " 'VIMA',\n",
       " 'JARVIS-1',\n",
       " 'CodeGen',\n",
       " 'BLIP',\n",
       " 'BLIP-2',\n",
       " 'CodeParrot',\n",
       " 'Zephyr',\n",
       " 'IDEFICS',\n",
       " 'FinGPT',\n",
       " 'BLUUMI',\n",
       " 'Cohere Base',\n",
       " 'Cohere Command',\n",
       " 'Cohere Embed (English)',\n",
       " 'Cohere Embed (Multilingual)',\n",
       " 'Cohere Embedv3 (English)',\n",
       " 'Yi',\n",
       " 'Yi-VL',\n",
       " 'Platypus',\n",
       " 'UFOGen',\n",
       " 'HyperCLOVA',\n",
       " 'GPT-JT',\n",
       " 'GPT-NeoXT-Chat-Base',\n",
       " 'OpenChatKit moderation model',\n",
       " 'Llama-2-7B-32K-Instruct',\n",
       " 'StripedHyena',\n",
       " 'StripedHyena Nous',\n",
       " 'You model',\n",
       " 'SALMONN',\n",
       " 'Bark',\n",
       " 'Luminous',\n",
       " 'MAGMA',\n",
       " 'Orion',\n",
       " 'Prithvi',\n",
       " 'RT-1-X',\n",
       " 'RT-2-X',\n",
       " 'ESM-2',\n",
       " 'FLAVA',\n",
       " 'Galactica',\n",
       " 'InCoder',\n",
       " 'OPT',\n",
       " 'Make-A-Video',\n",
       " 'LLaMA',\n",
       " 'LLaMA 2',\n",
       " 'OPT-IML',\n",
       " 'SAM',\n",
       " 'Voicebox',\n",
       " 'PEER',\n",
       " 'MusicGen',\n",
       " 'AudioGen',\n",
       " 'Emu',\n",
       " 'Code LLaMA',\n",
       " 'Emu Video',\n",
       " 'Emu Edit',\n",
       " 'MetaCLIP',\n",
       " 'Otter',\n",
       " 'GAIA-1',\n",
       " 'Baichuan 2',\n",
       " 'AlphaFold2',\n",
       " 'Flamingo',\n",
       " 'AlphaCode',\n",
       " 'Gopher',\n",
       " 'Chinchilla',\n",
       " 'Gato',\n",
       " 'Sparrow',\n",
       " 'RETRO',\n",
       " 'Sparrow Rule reward model',\n",
       " 'Sparrow Preference reward model',\n",
       " 'GopherCite',\n",
       " 'GopherCite reward model',\n",
       " 'Dramatron',\n",
       " 'RT-2',\n",
       " 'Lyria',\n",
       " 'Dolly',\n",
       " 'MoMo',\n",
       " 'Amber',\n",
       " 'CrystalCoder',\n",
       " 'Taiyi Diffusion XL',\n",
       " 'Xwin-LM',\n",
       " 'COSMO',\n",
       " 'Tulu 2',\n",
       " 'Tulu 2 DPO',\n",
       " 'Code Tulu 2',\n",
       " 'OLMo',\n",
       " 'VLMo',\n",
       " 'T-ULRv5',\n",
       " 'Turing NLR-v5',\n",
       " 'Megatron-Turing NLG',\n",
       " 'VALL-E',\n",
       " 'BioGPT',\n",
       " 'KOSMOS-1',\n",
       " 'Prometheus',\n",
       " 'Florence',\n",
       " 'VisualChatGPT',\n",
       " 'UniLM',\n",
       " 'Docugami',\n",
       " 'BEiT-3',\n",
       " 'WizardLM',\n",
       " 'WizardCoder',\n",
       " 'Florence-2',\n",
       " 'LlongOrca',\n",
       " 'Phi-1.5',\n",
       " 'Deepseek',\n",
       " 'Deepseek Chat',\n",
       " 'OpenFold',\n",
       " 'Ferret',\n",
       " 'Lemur',\n",
       " 'Lemur-Chat',\n",
       " 'BloombergGPT',\n",
       " 'Composer',\n",
       " 'Qwen',\n",
       " 'Qwen 1.5',\n",
       " 'DeciLM']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foundation_models.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5d6707",
   "metadata": {},
   "source": [
    "Let's check Epoch's list of models to see if anything is named differently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3adc3b9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Gemini Ultra',\n",
       " 'Qwen-72B',\n",
       " 'Inflection-2',\n",
       " 'Nemotron-3-8B',\n",
       " 'CogVLM',\n",
       " 'Yi-34B',\n",
       " 'Skywork-13B',\n",
       " 'ChatGLM3',\n",
       " 'XGen-7B',\n",
       " 'Falcon 180B',\n",
       " 'Jais',\n",
       " 'Llama 2-70B',\n",
       " 'Llama 2-7B',\n",
       " 'Claude 2',\n",
       " 'Pangu-Weather',\n",
       " 'PaLM 2',\n",
       " 'StarCoder',\n",
       " 'WizardLM-7B',\n",
       " 'BloombergGPT',\n",
       " 'PanGu-Σ',\n",
       " 'GPT-4',\n",
       " 'Falcon-40B',\n",
       " 'LLaMA-65B',\n",
       " 'LLaMA-7B',\n",
       " 'GPT-3.5 (text-davinci-003)',\n",
       " 'Galactica',\n",
       " 'BLOOM-176B',\n",
       " 'Taiyi-Stable Diffusion',\n",
       " 'U-PaLM (540B)',\n",
       " 'Whisper',\n",
       " 'PaLI',\n",
       " 'BlenderBot 3',\n",
       " 'GLM-130B',\n",
       " 'AlexaTM 20B',\n",
       " 'ESM2-15B',\n",
       " 'NLLB',\n",
       " 'Minerva (540B)',\n",
       " 'Parti',\n",
       " 'CoCa',\n",
       " 'Imagen',\n",
       " 'UL2',\n",
       " 'OPT-175B',\n",
       " 'Flamingo',\n",
       " 'Stable Diffusion (LDM-KL-8-G)',\n",
       " 'PaLM (540B)',\n",
       " 'Chinchilla',\n",
       " 'LaMDA',\n",
       " 'GPT-NeoX-20B',\n",
       " 'AlphaCode',\n",
       " 'ERNIE 3.0 Titan',\n",
       " 'XGLM-7.5B',\n",
       " 'GLaM',\n",
       " 'Gopher (280B)',\n",
       " 'Florence',\n",
       " 'BASIC-L',\n",
       " 'T0-XXL',\n",
       " 'Yuan 1.0',\n",
       " 'Megatron-Turing NLG 530B',\n",
       " 'HyperCLOVA',\n",
       " 'FLAN 137B',\n",
       " 'GOAT',\n",
       " 'ERNIE 3.0',\n",
       " 'ALIGN',\n",
       " 'DeBERTa',\n",
       " 'CoAtNet',\n",
       " 'ByT5-XXL',\n",
       " 'CogView',\n",
       " 'ProtT5-XXL',\n",
       " 'ProtT5-XXL-BFD',\n",
       " 'ProtBERT-BFD',\n",
       " 'PLUG',\n",
       " 'M6-T',\n",
       " 'Meta Pseudo Labels',\n",
       " 'MSA Transformer',\n",
       " 'Switch',\n",
       " 'DALL-E',\n",
       " 'CLIP (ViT L/14@336px)',\n",
       " 'ViT-Huge/14',\n",
       " 'Conformer + Wav2vec 2.0 + Noisy Student',\n",
       " 'mT5-XXL',\n",
       " 'GShard (dense)',\n",
       " 'iGPT-XL',\n",
       " 'iGPT-L',\n",
       " 'GPT-3 175B (davinci)',\n",
       " 'Once for All',\n",
       " 'ELECTRA',\n",
       " 'Turing-NLG',\n",
       " 'ALBERT-xxlarge',\n",
       " 'Meena',\n",
       " 'ContextNet + Noisy Student',\n",
       " 'DD-PPO',\n",
       " 'OpenAI Five',\n",
       " 'OpenAI Five Rerun',\n",
       " 'Noisy Student (L2)',\n",
       " 'AlphaStar',\n",
       " 'T5-3B',\n",
       " 'T5-11B',\n",
       " 'Megatron-BERT',\n",
       " 'Megatron-LM (8.3B)',\n",
       " 'RoBERTa Large',\n",
       " 'MnasNet-A3',\n",
       " 'MnasNet-A1 + SSDLite',\n",
       " 'BERT-Large-CAS (PTB+WT2+WT103)',\n",
       " 'SciBERT',\n",
       " 'ProxylessNAS',\n",
       " 'GPT-2 (1.5B)',\n",
       " 'BERT-Large',\n",
       " 'BigGAN-deep 512x512',\n",
       " 'Transformer (Adaptive Input Embeddings)',\n",
       " 'Transformer + Simple Recurrent Unit',\n",
       " 'Big Transformer for Back-Translation',\n",
       " 'FTW',\n",
       " 'Population-based DRL',\n",
       " 'GPT',\n",
       " 'ResNeXt-101 32x48d',\n",
       " 'YOLOv3',\n",
       " 'LSTM (Hebbian, Cache, MbPA)',\n",
       " 'AmoebaNet-A (F=448)',\n",
       " 'IMPALA',\n",
       " 'AlphaZero',\n",
       " 'PNASNet-5',\n",
       " 'AlphaGo Zero',\n",
       " 'OpenAI TI7 DOTA 1v1',\n",
       " 'JFT',\n",
       " 'Transformer',\n",
       " 'MoE',\n",
       " 'DeepStack',\n",
       " 'Libratus',\n",
       " 'AlphaGo Master',\n",
       " 'PolyNet',\n",
       " 'NASv3 (CIFAR-10)',\n",
       " 'BIDAF',\n",
       " 'Xception',\n",
       " 'GNMT',\n",
       " 'AlphaGo Lee',\n",
       " 'ResNet-152 (ImageNet)',\n",
       " 'DeepSpeech2 (English)',\n",
       " 'AlphaGo Fan']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price_dataset['System'].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b083fb",
   "metadata": {},
   "source": [
    "Add models with not-exactly-matching names to the foundation model list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f94d288",
   "metadata": {},
   "outputs": [],
   "source": [
    "foundation_models = np.append(\n",
    "    foundation_models,\n",
    "    ['GPT-3 175B (davinci)', 'ChatGLM3', 'HyperClova', 'Megatron-Turing NLG 530B', 'Minerva (540B)', 'PaLM (540B)']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d9786a",
   "metadata": {},
   "source": [
    "Filter the price dataset to only include frontier models listed by CRFM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcc2ebbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_foundation_model = [_ in foundation_models for _ in price_dataset['System']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "626060be",
   "metadata": {},
   "outputs": [],
   "source": [
    "foundation_model_prices = price_dataset[is_foundation_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7dcb0f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "foundation_model_prices.to_csv('foundation model prices.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6bfb95ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_dataset['Foundation model?'] = is_foundation_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e2eecbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_dataset.to_csv('price dataset with FM indicator.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "834323dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(foundation_model_prices['Cost'].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f9c6508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>System</th>\n",
       "      <th>Cost (inflation-adjusted)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>StarCoder</td>\n",
       "      <td>5.856264e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>BloombergGPT</td>\n",
       "      <td>9.529677e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>GPT-4</td>\n",
       "      <td>7.835203e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Falcon-40B</td>\n",
       "      <td>8.103978e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>PaLI</td>\n",
       "      <td>2.547193e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>GLM-130B</td>\n",
       "      <td>1.635573e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Minerva (540B)</td>\n",
       "      <td>1.347019e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Imagen</td>\n",
       "      <td>3.619903e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>UL2</td>\n",
       "      <td>5.610850e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Flamingo</td>\n",
       "      <td>8.150695e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>PaLM (540B)</td>\n",
       "      <td>1.238906e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>LaMDA</td>\n",
       "      <td>1.319586e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>GLaM</td>\n",
       "      <td>2.093016e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Florence</td>\n",
       "      <td>1.724109e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Megatron-Turing NLG 530B</td>\n",
       "      <td>6.405653e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>HyperCLOVA</td>\n",
       "      <td>9.869716e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>GPT-3 175B (davinci)</td>\n",
       "      <td>4.324883e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      System  Cost (inflation-adjusted)\n",
       "16                 StarCoder               5.856264e+05\n",
       "18              BloombergGPT               9.529677e+05\n",
       "20                     GPT-4               7.835203e+07\n",
       "21                Falcon-40B               8.103978e+05\n",
       "30                      PaLI               2.547193e+05\n",
       "32                  GLM-130B               1.635573e+06\n",
       "36            Minerva (540B)               1.347019e+07\n",
       "39                    Imagen               3.619903e+04\n",
       "40                       UL2               5.610850e+05\n",
       "42                  Flamingo               8.150695e+05\n",
       "44               PaLM (540B)               1.238906e+07\n",
       "46                     LaMDA               1.319586e+06\n",
       "51                      GLaM               2.093016e+06\n",
       "53                  Florence               1.724109e+05\n",
       "57  Megatron-Turing NLG 530B               6.405653e+06\n",
       "58                HyperCLOVA               9.869716e+05\n",
       "83      GPT-3 175B (davinci)               4.324883e+06"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foundation_model_prices[['System', 'Cost (inflation-adjusted)']].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758772ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

,System,Domain,Task,Open-source,Reference,Publication date,Organization,Parameters,Training compute (FLOP),Training dataset size (datapoints),Epochs,Training time (hours),Training hardware,Country (from Organization),Base model,Finetune compute (FLOP),Hardware quantity,Hardware utilization,Training cloud compute vendor,Training data center,Training time (chip hours),Cost,Cost (inflation-adjusted),Foundation model?
0,Yuan 1.0,Language,Language modelling,,Yuan 1.0: Large-Scale Pre-trained Language Model in Zero-Shot and Few-Shot Learning,2021-10-12 00:00:00,Inspur,245730000000.0,3.5380000000001e+23,1000000000000.0,0.22,,,China,,,2128.0,0.45,,,,,,False
1,OpenAI TI7 DOTA 1v1,Games,DOTA,,Dota 2,2017-08-11 00:00:00,OpenAI,,6.046095222592002e+20,,,,,United States of America,,,,,,,,,,False
2,Claude 2,Language,Language modelling,API accessible,,2023-07-11 00:00:00,Anthropic,,3.866e+24,,,,,United States of America,,,,,,,,,,True
3,Turing-NLG,Language,Text autocompletion,,Turing-NLG: A 17-billion-parameter language model by Microsoft,2020-02-13 00:00:00,Microsoft,17000000000.0,1.57e+22,34800000000.0,3.39,,NVIDIA Tesla V100 DGXS 32 GB,Multinational,,,256.0,,,,,,,False
4,GPT-3.5 (text-davinci-003),Language,Language modelling,API accessible,,2022-11-28 00:00:00,OpenAI,,2.578e+24,,,,NVIDIA A100 SXM4 40 GB,United States of America,,,,,,,,,,False
5,AlphaGo Master,Games,Go,,Mastering the game of Go without human knowledge,2017-01-01 00:00:00,DeepMind,,1.5e+23,,,,Google TPU v1,United Kingdom of Great Britain and Northern Ireland,,,,,,,,,,False
6,Minerva (540B),Language,Quantitative reasoning,,Solving Quantitative Reasoning Problems with Language Models,2022-06-29 00:00:00,Google,540350000000.0,2.7415e+24,613875000000.0,,696.0,Google TPU v4,Multinational,PaLM (540B),2.0,1024.0,,,,712704.0,13220659.2,13470191.00856837,False
7,FTW,Games,Capture the flag,,Human-level performance in first-person multiplayer games with population-based deep reinforcement learning,2018-07-03 00:00:00,DeepMind,126001330.0,7.26e+21,,,,,United Kingdom of Great Britain and Northern Ireland,,,,,,,,,,False
8,AlphaGo Zero,Games,Go,,Mastering the game of Go without human knowledge,2017-10-18 00:00:00,DeepMind,46400244.0,3.41e+23,5800000000.0,,480.0,Google TPU v1,United Kingdom of Great Britain and Northern Ireland,,,,,,,,,,False
9,NASv3 (CIFAR-10),Vision,,,Neural Architecture Search with Reinforcement Learning,2016-11-05 00:00:00,Google Brain,37400000.0,2.2e+21,,,,,Multinational,,,800.0,,,,,,,False
10,AlphaZero,Games,,,Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm,2017-12-05 00:00:00,DeepMind,,3.6679273004682862e+22,700000.0,,,Google TPU v2,United Kingdom of Great Britain and Northern Ireland,,,64.0,,,,,,,False
11,PaLM (540B),Language,Language modelling,,PaLM: Scaling Language Modeling with Pathways,2022-04-04 00:00:00,Google Research,540350000000.0,2.5272e+24,585000000000.0,,1368.0,Google TPU v4,Multinational,,,6144.0,0.462,,,8404992.0,12187238.4,12389056.261813464,False
12,OPT-175B,Language,Language modelling,Fully open-source,OPT: Open Pre-trained Transformer Language Models,2022-05-02 00:00:00,Meta AI,175000000000.0,4.3e+23,135000000000.0,1.67,793.5,NVIDIA A100 SXM4 80 GB,Multinational,,,1024.0,0.4712,,,812544.0,1470704.64,1493974.647531918,False
13,ALIGN,Multimodal,Representation learning,,Scaling up visual and vision-language representation learning with noisy text supervision,2021-06-11 00:00:00,Google Research,820000000.0,2.598670000000999e+22,1600000000.0,,347.3,Google TPU v3,Multinational,,,512.0,,,,177817.6,160035.84,165500.13931537792,False
14,AlphaGo Fan,Games,Go,,Mastering the game of Go with deep neural networks and tree search,2015-10-01 00:00:00,Google DeepMind,8209984.0,3.8000000000000007e+20,,,,,Multinational,,,,,,,,,,False
15,GPT-4,Multimodal,Language modelling,API accessible,GPT-4 Technical Report,2023-03-15 00:00:00,OpenAI,,2.1e+25,4900000000000.0,2.0,2280.0,NVIDIA A100 SXM4 40 GB,United States of America,,,25000.0,0.34,,,57000000.0,82650000.0,83537094.74721056,True
16,GLaM,Language,,,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,2021-12-13 00:00:00,Google,1200000000000.0,3.74e+23,800000000000.0,,1366.0,Google TPU v4,Multinational,,,1024.0,,,,1398784.0,2028236.8,2093016.0239973643,True
17,AlphaGo Lee,Games,Go,,Mastering the game of Go with deep neural networks and tree search,2016-01-27 00:00:00,DeepMind,,1.9e+21,29400000.0,,,,United Kingdom of Great Britain and Northern Ireland,,,,,,,,,,False
18,AlphaStar,Games,StarCraft,,Grandmaster level in StarCraft II using multi-agent reinforcement learning,2019-10-30 00:00:00,DeepMind,139000000.0,5.9250000000001e+22,,,1056.0,Google TPU v3,United Kingdom of Great Britain and Northern Ireland,,,384.0,,,,405504.0,364953.6,388902.65897940914,False
19,OpenAI Five,Games,Dota 2,,Dota 2 with Large Scale Deep Reinforcement Learning,2019-12-13 00:00:00,OpenAI,159000000.0,6.7e+22,454321373184.0,,7104.0,,United States of America,,,1536.0,,,,10911744.0,,,False
20,RoBERTa Large,Language,,,RoBERTa: A Robustly Optimized BERT Pretraining Approach,2019-07-01 00:00:00,"Facebook,University of Washington",355000000.0,4.15383552e+21,32000000000.0,,120.0,NVIDIA Tesla V100 DGXS 32 GB,"Multinational,United States of America",,,1024.0,,,,122880.0,148684.8,160017.6468716094,False
21,OpenAI Five Rerun,Games,Dota 2,,Dota 2 with Large Scale Deep Reinforcement Learning,2019-12-13 00:00:00,OpenAI,159000000.0,1.3e+22,53084160000.0,,,,United States of America,,,512.0,,,,,,,False
22,GNMT,Language,Translation,,Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation,2016-09-26 00:00:00,Google,278000000.0,6.899999999999999e+21,360000000.0,,4320.0,NVIDIA Tesla K80,Multinational,,,96.0,,,,414720.0,178329.6,194739.1953027523,False
23,GShard (dense),Language,Translation,,GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding,2020-06-30 00:00:00,Google,600000000000.0,1.28e+22,260000000000.0,,1008.0,Google TPU v3,Multinational,,,1024.0,,,,1032192.0,928972.8,969111.5896932516,False
24,AlphaCode,Language,Code generation,,Competition-Level Code Generation with AlphaCode,2022-02-02 00:00:00,DeepMind,41100000000.0,1.568160000001e+23,,,,"Google TPU v4,Google TPU v4i",United Kingdom of Great Britain and Northern Ireland,,,3750.0,,,,,,,True
25,Megatron-LM (8.3B),Language,,,Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism,2019-09-17 00:00:00,NVIDIA,8300000000.0,9.1e+21,34800000000.0,4.4,327.0,NVIDIA Tesla V100 DGXS 32 GB,United States of America,,,512.0,0.1162,,,167424.0,202583.04,216070.42339784943,False
26,Gopher (280B),Language,Language modelling,,"Scaling Language Models: Methods, Analysis & Insights from Training Gopher",2021-12-08 00:00:00,DeepMind,280000000000.0,6.31e+23,225000000000.0,1.0,920.0,Google TPU v3,United Kingdom of Great Britain and Northern Ireland,,,4096.0,0.378,,,3768320.0,3391488.0,3499807.67985019,False
27,Meena,Language,Text autocompletion,,Towards a Human-like Open-Domain Chatbot,2020-01-28 00:00:00,Google Brain,2600000000.0,1.12e+23,40000000000.0,,720.0,Google TPU v3,Multinational,,,1024.0,0.3439,,,737280.0,663552.0,697110.2785525154,False
28,IMPALA,Games,Atari,,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,2018-02-05 00:00:00,DeepMind,1600000.0,1.68e+20,240000000000.0,,100.0,NVIDIA P100,United Kingdom of Great Britain and Northern Ireland,,,1.0,,,,100.0,128.0,138.13091568449684,False
29,PaLM 2,Language,Language modelling,API accessible,PaLM 2 Technical Report,2023-05-10 00:00:00,Google,340000000000.0,7.34e+24,2700000000000.0,,,Google TPU v4,Multinational,,,,,,,,,,True
30,Megatron-BERT,Language,,,Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism,2019-09-17 00:00:00,NVIDIA,3900000000.0,6.027e+22,34800000000.0,,1392.0,NVIDIA Tesla V100S PCIe 32 GB,United States of America,,,512.0,0.2269,,,712704.0,,,False
31,Falcon 180B,Language,Language modelling,Permissive license,Falcon LLM - Falcon 180B,2023-09-06 00:00:00,Technology Innovation Institute,180000000000.0,3.76e+24,2625000000000.0,,4320.0,NVIDIA A100 SXM4 40 GB,United Arab Emirates,,,4096.0,0.1876,Amazon Web Services,,17694720.0,25657344.0,25816323.797898512,False
32,JFT,Vision,"Image classification,Object detection,Semantic segmentation,Pose estimation",,Revisiting Unreasonable Effectiveness of Data in Deep Learning Era.,2017-07-10 00:00:00,"Google Research,Carnegie Mellon University (CMU)",,8.43e+20,300000000.0,,1440.0,NVIDIA Tesla K80,"Multinational,United States of America",,,50.0,,,,72000.0,30960.0,33440.7332123412,False
33,AmoebaNet-A (F=448),Vision,Image classification,,Regularized Evolution for Image Classifier Architecture Search,2018-02-05 00:00:00,Google Brain,469000000.0,3.85296912e+20,1280000.0,,168.0,NVIDIA Tesla K40s,Multinational,,,450.0,,,,75600.0,,,False
34,Megatron-Turing NLG 530B,Language,Language modelling,,"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model",2021-10-11 00:00:00,"Microsoft,NVIDIA",530000000000.0,1.17e+24,202500000000.0,,770.0,NVIDIA A100 SXM4 80 GB,"Multinational,United States of America",,,4480.0,0.302,,,3449600.0,6209280.0,6405652.563246981,True
35,T5-3B,Language,Text autocompletion,,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,2019-10-23 00:00:00,Google,2800000000.0,8.658654068736e+20,25500000000.0,0.17,,Google TPU v3,Multinational,,,,,,,,,,False
36,LaMDA,Language,Language modelling,,LaMDA: Language Models for Dialog Applications,2022-02-10 00:00:00,Google,137000000000.0,3.55e+23,1560000000000.0,,1385.0,Google TPU v3,Multinational,,,1024.0,0.565,,,1418240.0,1276416.0,1319585.503057254,True
37,T5-11B,Language,Text autocompletion,,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,2019-10-23 00:00:00,Google,11000000000.0,3.3e+22,150000000000.0,,481.9,Google TPU v3,Multinational,,,512.0,0.3707,,,246732.8,222059.52,236631.5547502238,False
38,BigGAN-deep 512x512,Image generation,Image generation,Permissive license,Large Scale GAN Training for High Fidelity Natural Image Synthesis,2018-09-28 00:00:00,"Heriot-Watt University,DeepMind",112694781.0,3.00000000001e+21,292000000.0,,48.0,Google TPU v3,"United Kingdom of Great Britain and Northern Ireland,United Kingdom of Great Britain and Northern Ireland",,,256.0,,,,12288.0,24576.0,26473.13375565611,False
39,ResNeXt-101 32x48d,Vision,Image classification,,Exploring the Limits of Weakly Supervised Pretraining,2018-05-02 00:00:00,Facebook,829000000.0,8.74395e+21,9525000000.0,,,,Multinational,,,336.0,,,,,,,False
40,Chinchilla,Language,Language modelling,,Training Compute-Optimal Large Language Models,2022-03-29 00:00:00,DeepMind,70000000000.0,5.76e+23,1050000000000.0,1.0,,"Google TPU v4,Google TPU v3",United Kingdom of Great Britain and Northern Ireland,,,,,,,,,,True
41,ERNIE 3.0 Titan,Language,,,ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training for Language Understanding and Generation,2021-12-23 00:00:00,"Baidu,Peng Cheng Laboratory",260000000000.0,1.0421e+24,668000000000.0,,,"Huawei Ascend 910,NVIDIA Tesla V100 DGXS 32 GB","China,China",,,1920.0,,,,,,,True
42,LLaMA-65B,Language,Language modelling,Fully open-source,LLaMA: Open and Efficient Foundation Language Models,2023-02-24 00:00:00,Meta AI,65200000000.0,5.5e+23,1050000000000.0,1.09,500.0,NVIDIA A100,Multinational,,,2048.0,0.4746,,,1024000.0,1392640.0,1410425.8455359954,False
43,DALL-E,Image generation,Text-to-image,,Zero-Shot Text-to-Image Generation,2021-01-05 00:00:00,OpenAI,12000000000.0,4.7e+22,250000000.0,,,NVIDIA Tesla V100 DGXS 16 GB,United States of America,,,1024.0,,,,,,,False
44,GPT-2 (1.5B),Language,,,Language Models are Unsupervised Multitask Learners,2019-02-14 00:00:00,OpenAI,1500000000.0,4.3e+21,3000000000.0,20.0,,,United States of America,,,,,,,,,,False
45,GOAT,Games,Open ended play,,Open-Ended Learning Leads to Generally Capable Agents,2021-07-27 00:00:00,DeepMind,3500000.0,7.8e+22,390000000000.0,,,Google TPU v3,United Kingdom of Great Britain and Northern Ireland,,,,,,,,,,True
46,Switch,Language,Text autocompletion,,Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity,2021-01-11 00:00:00,Google,1600000000000.0,8.22e+22,432000000000.0,,648.0,Google TPU v3,Multinational,,,1024.0,0.28,,,663552.0,597196.8,619741.3696948562,False
47,iGPT-XL,"Vision,Image generation",Image completion,,Generative Pretraining from Pixels,2020-06-17 00:00:00,OpenAI,6801000000.0,3.3e+22,9600000.0,,,NVIDIA Tesla V100 DGXS 32 GB,United States of America,,,,,,,,,,False
48,Parti,Image generation,Text-to-image,,Scaling Autoregressive Models for Content-Rich Text-to-Image Generation,2022-06-22 00:00:00,Google Research,20000000000.0,3.962895376192635e+23,4800000000.0,,,Google TPU v4,Multinational,,,,,,,,,,True
49,mT5-XXL,Language,Language modelling,,mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer,2020-10-20 00:00:00,Google,13000000000.0,7.8e+22,750000000000.0,1.0,,,Multinational,,,,,,,,,,False
50,ByT5-XXL,Language,Language modelling,Permissive license,ByT5: Towards a token-free future with pre-trained byte-to-byte models,2021-05-28 00:00:00,Google,12900000000.0,8.1e+22,,,,Google TPU v3,Multinational,,,64.0,,,,,,,False
51,BlenderBot 3,Language,Chat,Permissive license,BlenderBot 3: a deployed conversational agent that continually learns to responsibly engage,2022-08-10 00:00:00,"McGill University,Meta AI,Mila- Quebec AI",175000000000.0,4.3e+23,,,,NVIDIA A100 SXM4 40 GB,"Canada,Multinational,Canada",OPT-175B,1.0,128.0,,,,,,,False
52,ChatGLM3,Multimodal,"Chat,Visual question answering",,Zhipu AI launches third-generation base model,2023-10-27 00:00:00,Zhipu AI,130000000000.0,1.09200000000001e+24,1050000000000.0,,,,China,,,,,,,,,,True
53,Yi-34B,Language,Chat,Permissive license,,2023-11-02 00:00:00,01.AI,34000000000.0,6.1e+23,,,,,China,,,,,,,,,,False
54,Qwen-72B,Language,"Chat,Code generation",Permissive license,,2023-11-30 00:00:00,Alibaba,72000000000.0,1.3e+24,,,,,China,,,,,,,,,,False
55,Gemini Ultra,Multimodal,"Language modelling,Visual question answering,Chat,Translation",Unreleased,Gemini: A Family of Highly Capable Multimodal Models,2023-12-06 00:00:00,Google DeepMind,,8.00000000001e+25,,,2400.0,Google TPU v4,Multinational,,,55000.0,,,,132000000.0,191400000.0,191400000.0,False
56,ProtT5-XXL,Biology,"Proteins,general-purpose protein or nucleotide language model (pLM/nLM)",,ProtTrans: Towards Cracking the Language of Life’s Code Through Self-Supervised Learning,2021-05-04 00:00:00,"Technical University of Munich,Med AI Technology,NVIDIA,Oak Ridge National Laboratory,Google",11000000000.0,7.370000000000001e+22,393000000000.0,,,Google TPU v3,"Germany,China,United States of America,United States of America,Multinational",,,512.0,,,,,,,False
57,GPT-3 175B (davinci),Language,Text autocompletion,API accessible,Language models are Few-Shot Learners,2020-05-28 00:00:00,OpenAI,175000000000.0,3.14e+23,374000000000.0,0.6,355.2,NVIDIA Tesla V100 DGXS 32 GB,United States of America,,,10000.0,0.2196,,,3552000.0,4297920.0,4511299.978835979,True
58,Inflection-2,Language,Language modelling,API accessible,Inflection-2: The Next Step Up,2023-11-22 00:00:00,Inflection AI,,1.001e+25,,,,NVIDIA H100 SXM5,United States of America,,,5000.0,,,,,,,True
59,BERT-Large,Language,Next sentence prediction,Fully open-source,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11 00:00:00,Google,340000000.0,2.85e+20,3300000000.0,,96.0,Google TPU v2,Multinational,,,64.0,0.29,,,6144.0,6942.719999999999,7471.898386980109,False
60,Meta Pseudo Labels,Vision,Image classification,,Meta pseudo labels,2021-03-01 00:00:00,Google Brain,480000000.0,4.79e+22,130000000.0,,264.0,Google TPU v3,Multinational,,,1024.0,,,,270336.0,243302.4,251391.36,False
61,Llama 2-70B,Language,Language modelling,Fully open-source,Llama 2: Open Foundation and Fine-Tuned Chat Models,2023-07-18 00:00:00,Meta AI,70000000000.0,8.1e+23,1500000000000.0,1.0,4320.0,NVIDIA A100 SXM4 80 GB,Multinational,,,400.0,0.435,,Meta’s Research Super Cluster,1728000.0,3127680.0,3145517.7254868825,False
62,U-PaLM (540B),Language,Language Generation,Unreleased,Transcending Scaling Laws with 0.1% Extra Compute,2022-10-20 00:00:00,Google,540000000000.0,2.53e+24,,,120.0,Google TPU v4,Multinational,PaLM (540B),4.0,512.0,,,,61440.0,12276326.4,12523793.100601656,False
63,HyperClova,Language,,,What Changes Can Large-scale Language Models Bring? Intensive Study on HyperCLOVA: Billions-scale Korean Generative Pretrained Transformers,2021-09-10 00:00:00,"NAVER,Search Solutions",82000000000.0,1.476e+23,190000000000.0,,643.2,NVIDIA A100,"Korea (Republic of),Korea (Republic of)",,,1024.0,0.2,,,658636.8,948436.992,980164.9214492476,True
64,PanGu-Σ,Language,"Code generation,Language modelling",,PanGu-Σ: Towards Trillion Parameter Language Model with Sparse Heterogeneous Computing,2023-03-20 00:00:00,Huawei Noah's Ark Lab,1085000000000.0,4.669999999999999e+23,246750000000.0,1.84,2400.0,Huawei Ascend 910,China,,,512.0,,,,1228800.0,,,False
65,GLM-130B,Language,,,GLM-130B: An open bilingual pre-trained model,2022-08-04 00:00:00,Tsinghua University,130000000000.0,3.778e+23,,1.0,1440.0,NVIDIA A100 SXM4 40 GB,China,,,768.0,0.433,,,1105920.0,1603584.0,1635572.685301023,True
66,Flamingo,Multimodal,"Visual question answering,Image captioning",,Flamingo: a Visual Language Model for Few-Shot Learning,2022-04-29 00:00:00,DeepMind,80000000000.0,2.7e+23,,,360.0,Google TPU v4,United Kingdom of Great Britain and Northern Ireland,,,1536.0,,,,552960.0,801792.0,815069.4909087805,True
67,GPT-NeoX-20B,Language,,Fully open-source,GPT-NeoX-20B: An Open-Source Autoregressive Language Model,2022-02-09 00:00:00,EleutherAI,20000000000.0,9.31627008e+22,177167400000.0,1.0,2160.0,NVIDIA A100 SXM4 40 GB,Multinational,,,96.0,0.375,,,207360.0,300672.0,310840.989438577,False

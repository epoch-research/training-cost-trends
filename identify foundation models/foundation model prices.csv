,System,Domain,Task,Open-source,Reference,Publication date,Organization,Parameters,Training compute (FLOP),Training dataset size (datapoints),Epochs,Training time (hours),Training hardware,Country (from Organization),Base model,Finetune compute (FLOP),Hardware quantity,Hardware utilization,Training cloud compute vendor,Training data center,Training time (chip hours),Cost,Cost (inflation-adjusted)
2,Claude 2,Language,Language modelling,API accessible,,2023-07-11 00:00:00,Anthropic,,3.866e+24,,,,,United States of America,,,,,,,,,
15,GPT-4,Multimodal,Language modelling,API accessible,GPT-4 Technical Report,2023-03-15 00:00:00,OpenAI,,2.1e+25,4900000000000.0,2.0,2280.0,NVIDIA A100 SXM4 40 GB,United States of America,,,25000.0,0.34,,,57000000.0,82650000.0,83537094.74721056
16,GLaM,Language,,,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,2021-12-13 00:00:00,Google,1200000000000.0,3.74e+23,800000000000.0,,1366.0,Google TPU v4,Multinational,,,1024.0,,,,1398784.0,2028236.8,2093016.0239973643
24,AlphaCode,Language,Code generation,,Competition-Level Code Generation with AlphaCode,2022-02-02 00:00:00,DeepMind,41100000000.0,1.568160000001e+23,,,,"Google TPU v4,Google TPU v4i",United Kingdom of Great Britain and Northern Ireland,,,3750.0,,,,,,
29,PaLM 2,Language,Language modelling,API accessible,PaLM 2 Technical Report,2023-05-10 00:00:00,Google,340000000000.0,7.34e+24,2700000000000.0,,,Google TPU v4,Multinational,,,,,,,,,
34,Megatron-Turing NLG 530B,Language,Language modelling,,"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model",2021-10-11 00:00:00,"Microsoft,NVIDIA",530000000000.0,1.17e+24,202500000000.0,,770.0,NVIDIA A100 SXM4 80 GB,"Multinational,United States of America",,,4480.0,0.302,,,3449600.0,6209280.0,6405652.563246981
36,LaMDA,Language,Language modelling,,LaMDA: Language Models for Dialog Applications,2022-02-10 00:00:00,Google,137000000000.0,3.55e+23,1560000000000.0,,1385.0,Google TPU v3,Multinational,,,1024.0,0.565,,,1418240.0,1276416.0,1319585.503057254
40,Chinchilla,Language,Language modelling,,Training Compute-Optimal Large Language Models,2022-03-29 00:00:00,DeepMind,70000000000.0,5.76e+23,1050000000000.0,1.0,,"Google TPU v4,Google TPU v3",United Kingdom of Great Britain and Northern Ireland,,,,,,,,,
41,ERNIE 3.0 Titan,Language,,,ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training for Language Understanding and Generation,2021-12-23 00:00:00,"Baidu,Peng Cheng Laboratory",260000000000.0,1.0421e+24,668000000000.0,,,"Huawei Ascend 910,NVIDIA Tesla V100 DGXS 32 GB","China,China",,,1920.0,,,,,,
45,GOAT,Games,Open ended play,,Open-Ended Learning Leads to Generally Capable Agents,2021-07-27 00:00:00,DeepMind,3500000.0,7.8e+22,390000000000.0,,,Google TPU v3,United Kingdom of Great Britain and Northern Ireland,,,,,,,,,
48,Parti,Image generation,Text-to-image,,Scaling Autoregressive Models for Content-Rich Text-to-Image Generation,2022-06-22 00:00:00,Google Research,20000000000.0,3.962895376192635e+23,4800000000.0,,,Google TPU v4,Multinational,,,,,,,,,
52,ChatGLM3,Multimodal,"Chat,Visual question answering",,Zhipu AI launches third-generation base model,2023-10-27 00:00:00,Zhipu AI,130000000000.0,1.09200000000001e+24,1050000000000.0,,,,China,,,,,,,,,
57,GPT-3 175B (davinci),Language,Text autocompletion,API accessible,Language models are Few-Shot Learners,2020-05-28 00:00:00,OpenAI,175000000000.0,3.14e+23,374000000000.0,0.6,355.2,NVIDIA Tesla V100 DGXS 32 GB,United States of America,,,10000.0,0.2196,,,3552000.0,4297920.0,4511299.978835979
58,Inflection-2,Language,Language modelling,API accessible,Inflection-2: The Next Step Up,2023-11-22 00:00:00,Inflection AI,,1.001e+25,,,,NVIDIA H100 SXM5,United States of America,,,5000.0,,,,,,
63,HyperClova,Language,,,What Changes Can Large-scale Language Models Bring? Intensive Study on HyperCLOVA: Billions-scale Korean Generative Pretrained Transformers,2021-09-10 00:00:00,"NAVER,Search Solutions",82000000000.0,1.476e+23,190000000000.0,,643.2,NVIDIA A100,"Korea (Republic of),Korea (Republic of)",,,1024.0,0.2,,,658636.8,948436.992,980164.9214492476
65,GLM-130B,Language,,,GLM-130B: An open bilingual pre-trained model,2022-08-04 00:00:00,Tsinghua University,130000000000.0,3.778e+23,,1.0,1440.0,NVIDIA A100 SXM4 40 GB,China,,,768.0,0.433,,,1105920.0,1603584.0,1635572.685301023
66,Flamingo,Multimodal,"Visual question answering,Image captioning",,Flamingo: a Visual Language Model for Few-Shot Learning,2022-04-29 00:00:00,DeepMind,80000000000.0,2.7e+23,,,360.0,Google TPU v4,United Kingdom of Great Britain and Northern Ireland,,,1536.0,,,,552960.0,801792.0,815069.4909087805

,System,Domain,Task,Open-source,Reference,Publication date,Organization,Parameters,Training compute (FLOP),Training dataset size (datapoints),Epochs,Training time (hours),Training hardware,Country (from Organization),Base model,Finetune compute (FLOP),Hardware quantity,Hardware utilization,Training cloud compute vendor,Training data center,Training time (chip hours),Cost,Cost (inflation-adjusted)
2,Inflection-2,Language,Language modelling,API accessible,Inflection-2: The Next Step Up,2023-11-22 00:00:00,Inflection AI,,1.001e+25,,,,NVIDIA H100 SXM5,United States of America,,,5000.0,,,,,,
4,CogVLM,"Multimodal,Vision,Language","Image captioning,Visual question answering,Chat",Permissive license,CogVLM: Visual Expert for Pretrained Language Models,2023-11-06 00:00:00,"Tsinghua University,Zhipu AI,Beihang University",17000000000.0,1.988064e+22,,,,,"China,China,China",,,,,,,,,
7,ChatGLM3,Multimodal,"Chat,Visual question answering",,Zhipu AI launches third-generation base model,2023-10-27 00:00:00,Zhipu AI,130000000000.0,1.09200000000001e+24,1050000000000.0,,,,China,,,,,,,,,
10,Jais,Language,Language modelling,,Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models,2023-08-29 00:00:00,"Cerebras Systems,Mohamed bin Zayed University of Artificial Intelligence,Inception",13000000000.0,3.08e+22,300000000000.0,,600.0,,"Multinational,United Arab Emirates,United States of America",,,,,,,,,
13,Claude 2,Language,Language modelling,API accessible,,2023-07-11 00:00:00,Anthropic,,3.866e+24,,,,,United States of America,,,,,,,,,
15,PaLM 2,Language,Language modelling,API accessible,PaLM 2 Technical Report,2023-05-10 00:00:00,Google,340000000000.0,7.34e+24,2700000000000.0,,,Google TPU v4,Multinational,,,,,,,,,
16,StarCoder,Language,Code generation,Fully open-source,StarCoder: may the source be with you!,2023-05-09 00:00:00,"Hugging Face,ServiceNow,Northeastern University,Mila- Quebec AI,Carnegie Mellon University (CMU),Johns Hopkins University,Leipzig University,ScaDS.AI,Queen Mary University of London,Roblox,Sea AI Lab,Technion - Israel Institute of Technology,Monash University,CSIRO,Data61,McGill University,Saama,University of British Columbia (UBC),Massachusetts Institute of Technology (MIT),Technical University of Munich,IBM,University of Vermont,UnfoldML,SAP,University of Notre Dame,Columbia University,New York University (NYU),University of Allahabad,Discover Dollar,Toloka,Telefonica,Stanford University,Weizmann Institute of Science,Alan Turing Institute,Wellesley College,EleutherAI,Forschungszentrum Julich",15500000000.0,1.12e+23,,1.0,,NVIDIA A100 SXM4 80 GB,"Multinational,United States of America,United States of America,Canada,United States of America,United States of America,Germany,Germany,United Kingdom of Great Britain and Northern Ireland,United States of America,Singapore,Israel,Australia,Australia,Australia,Canada,United States of America,Canada,United States of America,Germany,Multinational,United States of America,Sweden,Multinational,United States of America,United States of America,United States of America,India,India,Multinational,Spain,United States of America,Israel,United Kingdom of Great Britain and Northern Ireland,United States of America,Multinational,Germany",,,,,,,,,
18,BloombergGPT,Language,Language modelling,,BloombergGPT: A Large Language Model for Finance,2023-03-30 00:00:00,"Bloomberg,Johns Hopkins University",50558868480.0,2.3599999999999997e+23,532000000000.0,0.8,1270.0,NVIDIA A100,"United States of America,United States of America",,,,,,,,,
20,GPT-4,Multimodal,Language modelling,API accessible,GPT-4 Technical Report,2023-03-15 00:00:00,OpenAI,,2.1e+25,4900000000000.0,2.0,2280.0,NVIDIA A100 SXM4 40 GB,United States of America,,,25000.0,0.34,,,57000000.0,77520000.0,78352033.69393542
21,Falcon-40B,Language,Language modelling,Fully open-source,Abu Dhabi-based Technology Innovation Institute Introduces Falcon LLM: Foundational Large Language Model (LLM) outperforms GPT-3 with 40 Billion Parameters,2023-03-15 00:00:00,Technology Innovation Institute,40000000000.0,2.4e+23,750000000000.0,,1440.0,NVIDIA A100,United Arab Emirates,,,,,,,,,
25,Galactica,"Language,Biology",Language modelling,,Galactica: A Large Language Model for Science,2022-11-16 00:00:00,Meta AI,120000000000.0,3.24e+23,,4.0,,NVIDIA A100 SXM4 80 GB,Multinational,,,,,,,,,
29,Whisper,Speech,Audio speech recognition,,Robust Speech Recognition via Large-Scale Weak Supervision,2022-09-21 00:00:00,OpenAI,1550000000.0,4.65e+22,9302400000.0,3.0,,,United States of America,,,,,,,,,
30,PaLI,"Language,Vision",,,PaLI: A Jointly-Scaled Multilingual Language-Image Model,2022-09-14 00:00:00,Google,16900000000.0,5.1e+22,,1.0,168.0,Google TPU v4,Multinational,,,,,,,,,
32,GLM-130B,Language,,,GLM-130B: An open bilingual pre-trained model,2022-08-04 00:00:00,Tsinghua University,130000000000.0,3.778e+23,,1.0,1440.0,NVIDIA A100 SXM4 40 GB,China,,,768.0,0.433,,,1105920.0,1603584.0,1635572.685301023
36,Minerva (540B),Language,Quantitative reasoning,,Solving Quantitative Reasoning Problems with Language Models,2022-06-29 00:00:00,Google,540350000000.0,2.7415e+24,613875000000.0,,696.0,Google TPU v4,Multinational,PaLM (540B),2.0,1024.0,,,,712704.0,13220659.2,13470191.00856837
37,Parti,Image generation,Text-to-image,,Scaling Autoregressive Models for Content-Rich Text-to-Image Generation,2022-06-22 00:00:00,Google Research,20000000000.0,3.962895376192635e+23,4800000000.0,,,Google TPU v4,Multinational,,,,,,,,,
39,Imagen,Image generation,Text-to-image,,Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding,2022-05-23 00:00:00,Google Brain,3000000000.0,1.4600000000000002e+22,860000000.0,,,,Multinational,,,,,,,,,
40,UL2,Language,,,Unifying Language Learning Paradigms,2022-05-10 00:00:00,"Google Research,Google Brain",20000000000.0,1.2e+23,,,,,"Multinational,Multinational",,,,,,,,,
42,Flamingo,Multimodal,"Visual question answering,Image captioning",,Flamingo: a Visual Language Model for Few-Shot Learning,2022-04-29 00:00:00,DeepMind,80000000000.0,2.7e+23,,,360.0,Google TPU v4,United Kingdom of Great Britain and Northern Ireland,,,1536.0,,,,552960.0,801792.0,815069.4909087805
44,PaLM (540B),Language,Language modelling,,PaLM: Scaling Language Modeling with Pathways,2022-04-04 00:00:00,Google Research,540350000000.0,2.5272e+24,585000000000.0,,1368.0,Google TPU v4,Multinational,,,6144.0,0.462,,,8404992.0,12187238.4,12389056.261813464
45,Chinchilla,Language,Language modelling,,Training Compute-Optimal Large Language Models,2022-03-29 00:00:00,DeepMind,70000000000.0,5.76e+23,1050000000000.0,1.0,,"Google TPU v4,Google TPU v3",United Kingdom of Great Britain and Northern Ireland,,,,,,,,,
46,LaMDA,Language,Language modelling,,LaMDA: Language Models for Dialog Applications,2022-02-10 00:00:00,Google,137000000000.0,3.55e+23,1560000000000.0,,1385.0,Google TPU v3,Multinational,,,1024.0,0.565,,,1418240.0,1276416.0,1319585.503057254
48,AlphaCode,Language,Code generation,,Competition-Level Code Generation with AlphaCode,2022-02-02 00:00:00,DeepMind,41100000000.0,1.568160000001e+23,,,,"Google TPU v4,Google TPU v4i",United Kingdom of Great Britain and Northern Ireland,,,3750.0,,,,,,
49,ERNIE 3.0 Titan,Language,,,ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training for Language Understanding and Generation,2021-12-23 00:00:00,"Baidu,Peng Cheng Laboratory",260000000000.0,1.0421e+24,668000000000.0,,,"Huawei Ascend 910,NVIDIA Tesla V100 DGXS 32 GB","China,China",,,1920.0,,,,,,
51,GLaM,Language,,,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,2021-12-13 00:00:00,Google,1200000000000.0,3.74e+23,800000000000.0,,1366.0,Google TPU v4,Multinational,,,1024.0,,,,1398784.0,2028236.8,2093016.0239973643
53,Florence,Vision,,,Florence: A New Foundation Model for Computer Vision,2021-11-22 00:00:00,Microsoft,893000000.0,4.831e+22,900000000.0,,240.0,NVIDIA A100 SXM4 40 GB,Multinational,,,,,,,,,
57,Megatron-Turing NLG 530B,Language,Language modelling,,"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model",2021-10-11 00:00:00,"Microsoft,NVIDIA",530000000000.0,1.17e+24,202500000000.0,,770.0,NVIDIA A100 SXM4 80 GB,"Multinational,United States of America",,,4480.0,0.302,,,3449600.0,6209280.0,6405652.563246981
58,HyperCLOVA,Language,,,What Changes Can Large-scale Language Models Bring? Intensive Study on HyperCLOVA: Billions-scale Korean Generative Pretrained Transformers,2021-09-10 00:00:00,"NAVER,Search Solutions",82000000000.0,1.476e+23,190000000000.0,,643.2,NVIDIA A100,"Korea (Republic of),Korea (Republic of)",,,1024.0,0.2,,,658636.8,948436.992,980164.9214492476
60,GOAT,Games,Open ended play,,Open-Ended Learning Leads to Generally Capable Agents,2021-07-27 00:00:00,DeepMind,3500000.0,7.8e+22,390000000000.0,,,Google TPU v3,United Kingdom of Great Britain and Northern Ireland,,,,,,,,,
66,CogView,Image generation,Text-to-image,,CogView: Mastering Text-to-Image Generation via Transformers,2021-05-26 00:00:00,"Tsinghua University,DAMO Academy",4000000000.0,2.68e+22,50000000000.0,,,NVIDIA Tesla V100 DGXS 16 GB,"China,China",,,,,,,,,
83,GPT-3 175B (davinci),Language,Text autocompletion,API accessible,Language models are Few-Shot Learners,2020-05-28 00:00:00,OpenAI,175000000000.0,3.14e+23,374000000000.0,0.6,355.2,NVIDIA Tesla V100 DGXS 32 GB,United States of America,,,10000.0,0.2196,,,3552000.0,4120319.999999999,4324882.624338623

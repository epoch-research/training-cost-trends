{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from parameters import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hardware acquisition cost\n",
    "\n",
    "ai_accelerator_cost = hardware_quantity * hardware_price\n",
    "\n",
    "server_cost = ai_accelerator_cost * server_cost_overhead\n",
    "\n",
    "cluster_cost = server_cost * cluster_interconnect_overhead\n",
    "\n",
    "90% credible intervals:\n",
    "- hardware_quantity: 0.8x to 1.25x of the central value. Generally confident, as it was usually reported directly by the developers. Could be slightly inaccurate if it doesn't account for hardware failures during training, or it describes the number of chips available in a cluster rather than the number of chips actually employed for training.\n",
    "- hardware_price: 0.5x to 2x of the central value. We know of two reports of NVIDIA's profit margin for Hopper GPUs: 1000% and 340%. So the minimum sale price could be 10 to 30% of the value we used. But this excludes R&D cost, and there should always be some profit, so we set the minimum higher than this. Big customers could be getting much bigger discounts compared to the prices we use. On the other hand, demand could have led to price spikes that weren't captured in our sparse data.\n",
    "- server_cost_overhead: 1.3 to 2.1. The three actual values we calculated ranged from 1.54 (P100) to 1.69 (V100). A cost breakdown of a DGX H100 by Semianalysis (2023) https://www.semianalysis.com/p/ai-server-cost-analysis-memory-is implies a ratio of approximately 1.4 (total cost divided by \"8 GPU + 4 NVSwitch Baseboard\" cost).\n",
    "- cluster_interconnect_overhead: 1.07 to 1.32. Prices of interconnect components are plausibly off by a factor of 2x due to variation in the brand, the merchant, and supply/demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median: 1 [90% CI: 0.46, 2.2]\n"
     ]
    }
   ],
   "source": [
    "NUM_SAMPLES = 1000\n",
    "\n",
    "hardware_quantity = lognorm_from_90_ci(0.8, 1.25, NUM_SAMPLES)\n",
    "hardware_price = lognorm_from_90_ci(0.5, 2, NUM_SAMPLES)\n",
    "server_cost_overhead = lognorm_from_90_ci(1.3, 2.1, NUM_SAMPLES)\n",
    "cluster_interconnect_overhead = lognorm_from_90_ci(1.07, 1.32, NUM_SAMPLES)\n",
    "hardware_acquisition_cost = hardware_quantity * hardware_price * server_cost_overhead * cluster_interconnect_overhead\n",
    "print_median_and_ci(hardware_acquisition_cost / np.median(hardware_acquisition_cost))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amortized hardware CapEx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "depreciation = 10 ** (ML_GPU_PRICE_PERFORMANCE_OOMS_PER_YEAR * years_since)\n",
    "\n",
    "training_chip_hours = hardware_quantity * training_time\n",
    "\n",
    "OR\n",
    "\n",
    "training_chip_hours = training_compute / (peak_flop_per_second * hardware_utilization)\n",
    "\n",
    "In the amortized hardware CapEx + energy approach, 32 values use the first method, 15 values use the second method.\n",
    "\n",
    "amortized_hardware_capex = server_cost_overhead * cluster_interconnect_overhead * (hardware_price / depreciation) * ML_GPU_PRICE_PERFORMANCE_OOMS_PER_YEAR * np.log(10) / HOURS_PER_YEAR * training_chip_hours\n",
    "\n",
    "90% credible intervals:\n",
    "\n",
    "- ML_GPU_PRICE_PERFORMANCE_OOMS_PER_YEAR: 0.10 to 0.18, based on Hobbhahn et al. (2023)\n",
    "- years_since: -120 days or +120 days from the central value.\n",
    "  - years_since measures the difference between the time the hardware could first be acquired at large scale, and the model training start date. The default is at least 90 days between hardware release date and hardware acquisition date, and 60 days + training time between training start date and publication date. However, shipping could be almost immediate relative to the release date, especially if customers can pre-order. Preparing results after training may only take about 30 days, especially if the evaluations are ready to run and the model is announced before a detailed report is released. In total that's (90 - 0) + (60 - 30) = 120 fewer days than the default. We keep the bounds symmetric to fit a normal distribution, since years_since is used as an exponent in the formula.\n",
    "- training_time: 0.5x to 2x of the central value. It was usually reported directly by the developers, but sometimes it's reported imprecisely - e.g. \"2 days\" is plausibly anywhere between 24 and 72 hours; \"weeks\" could mean 2 weeks or 6 weeks.\n",
    "- training_compute: 0.33x to 3x of the central value - a rule of thumb based on the \"Confident\" Confidence level in the database.\n",
    "- peak_flop_per_second: 0.5 to 2x of the central value. In case we got the number format wrong.\n",
    "- hardware_utilization: normal, 0.1 to 0.65. The range of values in our dataset is 0.19 to 0.56; we go a bit wider to be conservative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_with_imputed_chip_hours = 15\n",
    "num_with_direct_chip_hours = num_ml_models - num_with_imputed_chip_hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def amortized_hardware_capex_sample():    \n",
    "    price_performance = norm_from_ci(0.10, 0.18, 90, num_ml_models)\n",
    "    years_since = norm_from_ci(-120, 120, 90, num_ml_models) / DAYS_PER_YEAR\n",
    "    depreciation = 10 ** (price_performance * years_since)\n",
    "\n",
    "    hardware_price = lognorm_from_90_ci(0.5, 2, num_ml_models)\n",
    "    server_cost_overhead = lognorm_from_90_ci(1.3, 2.1, num_ml_models)\n",
    "    cluster_interconnect_overhead = lognorm_from_90_ci(1.07, 1.32, num_ml_models)\n",
    "\n",
    "    training_compute = lognorm_from_90_ci(1/3, 3, num_with_imputed_chip_hours)\n",
    "    peak_flop_per_second = lognorm_from_90_ci(0.5, 2, num_with_imputed_chip_hours)\n",
    "    hardware_utilization_raw = norm_from_ci(0.1, 0.65, 90, num_with_imputed_chip_hours, clip=[0.01, 0.99])\n",
    "    hardware_utilization = hardware_utilization_raw / 0.37  # relative to average value\n",
    "    realised_flop_per_second = peak_flop_per_second * hardware_utilization\n",
    "    imputed_chip_hours = training_compute / realised_flop_per_second\n",
    "\n",
    "    hardware_quantity = lognorm_from_90_ci(0.8, 1.25, num_with_direct_chip_hours)\n",
    "    training_time = lognorm_from_90_ci(0.5, 2, num_with_direct_chip_hours)\n",
    "    direct_chip_hours = training_time * hardware_quantity\n",
    "\n",
    "    training_chip_hours = np.concatenate([imputed_chip_hours, direct_chip_hours])\n",
    "    amortized_hardware_capex = server_cost_overhead * cluster_interconnect_overhead * (hardware_price / depreciation) * price_performance * training_chip_hours\n",
    "\n",
    "    return amortized_hardware_capex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "amortized_hardware_capex_medians = np.zeros(NUM_SAMPLES)\n",
    "amortized_hardware_capex_90_cis = np.zeros((NUM_SAMPLES, 2))\n",
    "for i in range(NUM_SAMPLES):\n",
    "    amortized_hardware_capex = amortized_hardware_capex_sample()\n",
    "    amortized_hardware_capex_medians[i] = np.median(amortized_hardware_capex)\n",
    "    amortized_hardware_capex_90_cis[i] = np.percentile(amortized_hardware_capex, [5, 95])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.30220936, 3.66144434])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(amortized_hardware_capex_90_cis, axis=0) / np.median(amortized_hardware_capex_medians)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Energy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Energy cost rate ($/kWh) ∗ Hardware peak TDP (kW) ∗ Average power to TDP ratio (%) ∗ Data center PUE ∗ Number of chip-hours (h)\n",
    "\n",
    "- Energy cost rate: 0.6x to 1.7x of central value (based on variation among likely data center states; see code below)\n",
    "- Hardware peak TDP (kW): assumed accurate based on hardware specifications.\n",
    "- Average power to TDP ratio: \n",
    "  - TPUs: 0.3 to 0.62\n",
    "  - GPUs: 0.56 to 1\n",
    "- Data center PUE\n",
    "  - 1.05 to 1.16 for hyperscalers (50 of 64 selected models)\n",
    "  - 1.12 to 1.4 for non-hyperscalers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncertainty in energy prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.727914785226352, 0.6247539654972791)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from energy import *\n",
    "likely_datacenter_states = ['California', 'Nevada', 'Oregon', 'Washington']\n",
    "energy_prices = [US_STATE_ENERGY_PRICES_PER_KWH[state] for state in likely_datacenter_states]\n",
    "mean = np.mean(energy_prices)\n",
    "np.max(energy_prices) / mean, np.min(energy_prices) / mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_with_imputed_chip_hours = 15\n",
    "num_with_direct_chip_hours = num_ml_models - num_with_imputed_chip_hours\n",
    "num_with_TPUs = 22\n",
    "num_with_GPUs = 23\n",
    "num_hyperscalers = 35  # 50/64 initially, scaled down to 45 models\n",
    "num_non_hyperscalers = num_ml_models - num_hyperscalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def energy_sample():\n",
    "    training_compute = lognorm_from_90_ci(1/3, 3, num_with_imputed_chip_hours)\n",
    "    peak_flop_per_second = lognorm_from_90_ci(0.5, 2, num_with_imputed_chip_hours)\n",
    "    hardware_utilization_raw = norm_from_ci(0.1, 0.65, 90, num_with_imputed_chip_hours, clip=[0.01, 0.99])\n",
    "    hardware_utilization = hardware_utilization_raw / 0.37  # relative to average value\n",
    "    realised_flop_per_second = peak_flop_per_second * hardware_utilization\n",
    "    imputed_chip_hours = training_compute / realised_flop_per_second\n",
    "\n",
    "    hardware_quantity = lognorm_from_90_ci(0.8, 1.25, num_with_direct_chip_hours)\n",
    "    training_time = lognorm_from_90_ci(0.5, 2, num_with_direct_chip_hours)\n",
    "    direct_chip_hours = training_time * hardware_quantity\n",
    "\n",
    "    training_chip_hours = np.concatenate([imputed_chip_hours, direct_chip_hours])\n",
    "\n",
    "    avg_power_ratio_tpu_raw = lognorm_from_90_ci(0.3, 0.62, num_with_TPUs)\n",
    "    avg_power_ratio_tpu = avg_power_ratio_tpu_raw / 0.43  # relative to central estimate\n",
    "    avg_power_ratio_gpu_raw = lognorm_from_90_ci(0.56, 1.00, num_with_GPUs)\n",
    "    avg_power_ratio_gpu = avg_power_ratio_gpu_raw / 0.75  # relative to central estimate\n",
    "    avg_power_ratio = np.concatenate([avg_power_ratio_tpu, avg_power_ratio_gpu])\n",
    "\n",
    "    pue_hyperscalers = lognorm_from_90_ci(1.05, 1.16, num_hyperscalers)\n",
    "    pue_non_hyperscalers = lognorm_from_90_ci(1.12, 1.4, num_non_hyperscalers)\n",
    "    pue = np.concatenate([pue_hyperscalers, pue_non_hyperscalers])\n",
    "\n",
    "    tdp = 1\n",
    "\n",
    "    energy_price = lognorm_from_90_ci(0.6, 1.7, num_ml_models)\n",
    "\n",
    "    energy = energy_price * tdp * avg_power_ratio * pue * training_chip_hours\n",
    "\n",
    "    return energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_medians = np.zeros(NUM_SAMPLES)\n",
    "energy_90_cis = np.zeros((NUM_SAMPLES, 2))\n",
    "for i in range(NUM_SAMPLES):\n",
    "    energy = energy_sample()\n",
    "    energy_medians[i] = np.median(energy)\n",
    "    energy_90_cis[i] = np.percentile(energy, [5, 95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.33353193, 3.27406934])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(energy_90_cis, axis=0) / np.median(energy_medians)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amortized hardware CapEx + energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.42441896, 2.93423517])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amortized_hardware_capex_energy_medians = np.zeros(NUM_SAMPLES)\n",
    "amortized_hardware_capex_energy_90_cis = np.zeros((NUM_SAMPLES, 2))\n",
    "for i in range(NUM_SAMPLES):\n",
    "    amortized_hardware_capex = amortized_hardware_capex_sample()\n",
    "    energy = energy_sample()\n",
    "    amortized_hardware_capex_energy_medians[i] = np.median(amortized_hardware_capex + energy)\n",
    "    amortized_hardware_capex_energy_90_cis[i] = np.percentile(amortized_hardware_capex + energy, [5, 95])\n",
    "np.median(amortized_hardware_capex_energy_90_cis, axis=0) / np.median(amortized_hardware_capex_energy_medians)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall uncertainty is a factor of about 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epoch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

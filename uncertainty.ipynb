{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from parameters import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hardware acquisition cost\n",
    "\n",
    "ai_accelerator_cost = hardware_quantity * hardware_price\n",
    "\n",
    "server_cost = ai_accelerator_cost * server_cost_overhead\n",
    "\n",
    "cluster_cost = server_cost * cluster_interconnect_overhead\n",
    "\n",
    "90% credible intervals:\n",
    "- hardware_quantity: 0.8x to 1.25x of the central value. Generally confident, as it was usually reported directly by the developers. Could be slightly inaccurate if it doesn't account for hardware failures during training, or it describes the number of chips available in a cluster rather than the number of chips actually employed for training.\n",
    "- hardware_price: 0.5x to 2x of the central value. We know of two reports of NVIDIA's profit margin for Hopper GPUs: 1000% and 340%. So the minimum sale price could be 10 to 30% of the value we used. But this excludes R&D cost, and there should always be some profit, so we set the minimum higher than this. Big customers could be getting much bigger discounts compared to the prices we use. On the other hand, demand could have led to price spikes that weren't captured in our sparse data.\n",
    "- server_cost_overhead: 1.3 to 2.1. The three actual values we calculated ranged from 1.54 (P100) to 1.69 (V100). A cost breakdown of a DGX H100 by Semianalysis (2023) https://www.semianalysis.com/p/ai-server-cost-analysis-memory-is implies a ratio of approximately 1.4 (total cost divided by \"8 GPU + 4 NVSwitch Baseboard\" cost).\n",
    "- cluster_interconnect_overhead: 1.07 to 1.32. Prices of interconnect components are plausibly off by a factor of 2x due to variation in the brand, the merchant, and supply/demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 10000\n",
    "num_ml_models = 47\n",
    "\n",
    "hardware_acquisition_cost_median = np.zeros(NUM_SAMPLES)\n",
    "hardware_acquisition_cost_90_ci = np.zeros((NUM_SAMPLES, 2))\n",
    "for i in range(NUM_SAMPLES):\n",
    "    # TODO: special cases where hardware quantity is less confident e.g. GPT-4\n",
    "    hardware_quantity = lognorm_from_90_ci(0.8, 1.25, num_ml_models)\n",
    "    hardware_price = lognorm_from_90_ci(0.5, 2, num_ml_models)\n",
    "    server_cost_overhead = lognorm_from_90_ci(1.3, 2.1, num_ml_models)\n",
    "    cluster_interconnect_overhead = lognorm_from_90_ci(1.07, 1.32, num_ml_models)\n",
    "    hardware_acquisition_cost = hardware_quantity * hardware_price * server_cost_overhead * cluster_interconnect_overhead\n",
    "    hardware_acquisition_cost_median[i] = np.median(hardware_acquisition_cost)\n",
    "    hardware_acquisition_cost_90_ci[i] = np.percentile(hardware_acquisition_cost, [5, 95])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.48646084, 2.06352952])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(hardware_acquisition_cost_90_ci, axis=0) / np.median(hardware_acquisition_cost_median)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amortized hardware CapEx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "depreciation = 10 ** (ML_GPU_PRICE_PERFORMANCE_OOMS_PER_YEAR * years_since)\n",
    "\n",
    "training_chip_hours = hardware_quantity * training_time\n",
    "\n",
    "OR\n",
    "\n",
    "training_chip_hours = training_compute / (peak_flop_per_second * hardware_utilization)\n",
    "\n",
    "In the amortized hardware CapEx + energy approach, 32 values use the first method, 15 values use the second method.\n",
    "\n",
    "amortized_hardware_capex = server_cost_overhead * cluster_interconnect_overhead * (hardware_price / depreciation) * ML_GPU_PRICE_PERFORMANCE_OOMS_PER_YEAR * np.log(10) / HOURS_PER_YEAR * training_chip_hours\n",
    "\n",
    "90% credible intervals:\n",
    "\n",
    "- ML_GPU_PRICE_PERFORMANCE_OOMS_PER_YEAR: 0.10 to 0.18, based on Hobbhahn et al. (2023)\n",
    "- years_since: -120 days or +120 days from the central value.\n",
    "  - years_since measures the difference between the time the hardware could first be acquired at large scale, and the model training start date. The default is at least 90 days between hardware release date and hardware acquisition date, and 60 days + training time between training start date and publication date. However, shipping could be almost immediate relative to the release date, especially if customers can pre-order. Preparing results after training may only take about 30 days, especially if the evaluations are ready to run and the model is announced before a detailed report is released. In total that's (90 - 0) + (60 - 30) = 120 fewer days than the default. We keep the bounds symmetric to fit a normal distribution, since years_since is used as an exponent in the formula.\n",
    "- training_time: 0.5x to 2x of the central value. It was usually reported directly by the developers, but sometimes it's reported imprecisely - e.g. \"2 days\" is plausibly anywhere between 24 and 72 hours; \"weeks\" could mean 2 weeks or 6 weeks.\n",
    "- training_compute: 0.33x to 3x of the central value - a rule of thumb based on the \"Confident\" Confidence level in the database.\n",
    "- peak_flop_per_second: 0.5 to 2x of the central value. In case we got the number format wrong.\n",
    "- hardware_utilization: normal, 0.1 to 0.65. The range of values in our dataset is 0.19 to 0.56; we go a bit wider to be conservative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median: 3.1 [90% CI: -18, 7.8]\n",
      "Median: 0.96 [90% CI: 0.45, 1.9]\n",
      "Median: 0.4 [90% CI: 0.094, 0.56]\n",
      "Median: 3.1 [90% CI: -18, 7.8]\n",
      "Median: 0.87 [90% CI: 0.4, 2.6]\n",
      "Median: 0.33 [90% CI: 0.065, 0.58]\n",
      "Median: 3.1 [90% CI: -18, 7.8]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m hardware_utilization \u001b[38;5;241m=\u001b[39m norm_from_ci(\u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.65\u001b[39m, \u001b[38;5;241m90\u001b[39m, num_with_imputed_chip_hours)\n\u001b[1;32m     18\u001b[0m imputed_chip_hours \u001b[38;5;241m=\u001b[39m training_compute \u001b[38;5;241m/\u001b[39m (peak_flop_per_second \u001b[38;5;241m*\u001b[39m hardware_utilization)\n\u001b[0;32m---> 20\u001b[0m hardware_quantity \u001b[38;5;241m=\u001b[39m lognorm_from_90_ci(\u001b[38;5;241m0.8\u001b[39m, \u001b[38;5;241m1.25\u001b[39m, num_with_direct_chip_hours)\n\u001b[1;32m     21\u001b[0m training_time \u001b[38;5;241m=\u001b[39m lognorm_from_90_ci(\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m2\u001b[39m, num_with_direct_chip_hours)\n\u001b[1;32m     22\u001b[0m direct_chip_hours \u001b[38;5;241m=\u001b[39m training_time \u001b[38;5;241m*\u001b[39m hardware_quantity\n",
      "File \u001b[0;32m~/Projects/training_cost_2/training-cost-trends/utils.py:72\u001b[0m, in \u001b[0;36mlognorm_from_90_ci\u001b[0;34m(p_5th, p_95th, num_samples)\u001b[0m\n\u001b[1;32m     70\u001b[0m p_95th_log \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(p_95th)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Solve for mu and sigma\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m sigma \u001b[38;5;241m=\u001b[39m (p_95th_log \u001b[38;5;241m-\u001b[39m p_5th_log) \u001b[38;5;241m/\u001b[39m (stats\u001b[38;5;241m.\u001b[39mnorm\u001b[38;5;241m.\u001b[39mppf(\u001b[38;5;241m0.95\u001b[39m) \u001b[38;5;241m-\u001b[39m stats\u001b[38;5;241m.\u001b[39mnorm\u001b[38;5;241m.\u001b[39mppf(\u001b[38;5;241m0.05\u001b[39m))\n\u001b[1;32m     73\u001b[0m mu \u001b[38;5;241m=\u001b[39m p_5th_log \u001b[38;5;241m-\u001b[39m stats\u001b[38;5;241m.\u001b[39mnorm\u001b[38;5;241m.\u001b[39mppf(\u001b[38;5;241m0.05\u001b[39m) \u001b[38;5;241m*\u001b[39m sigma\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Generate lognormal samples\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/epoch/lib/python3.11/site-packages/scipy/stats/_distn_infrastructure.py:2243\u001b[0m, in \u001b[0;36mrv_continuous.ppf\u001b[0;34m(self, q, *args, **kwds)\u001b[0m\n\u001b[1;32m   2241\u001b[0m lower_bound \u001b[38;5;241m=\u001b[39m _a \u001b[38;5;241m*\u001b[39m scale \u001b[38;5;241m+\u001b[39m loc\n\u001b[1;32m   2242\u001b[0m upper_bound \u001b[38;5;241m=\u001b[39m _b \u001b[38;5;241m*\u001b[39m scale \u001b[38;5;241m+\u001b[39m loc\n\u001b[0;32m-> 2243\u001b[0m place(output, cond2, argsreduce(cond2, lower_bound)[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m   2244\u001b[0m place(output, cond3, argsreduce(cond3, upper_bound)[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m   2246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(cond):  \u001b[38;5;66;03m# call only if at least 1 entry\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/epoch/lib/python3.11/site-packages/scipy/stats/_distn_infrastructure.py:594\u001b[0m, in \u001b[0;36margsreduce\u001b[0;34m(cond, *args)\u001b[0m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Clean arguments to:\u001b[39;00m\n\u001b[1;32m    561\u001b[0m \n\u001b[1;32m    562\u001b[0m \u001b[38;5;124;03m1. Ensure all arguments are iterable (arrays of dimension at least one\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    591\u001b[0m \n\u001b[1;32m    592\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;66;03m# some distributions assume arguments are iterable.\u001b[39;00m\n\u001b[0;32m--> 594\u001b[0m newargs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39matleast_1d(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    596\u001b[0m \u001b[38;5;66;03m# np.atleast_1d returns an array if only one argument, or a list of arrays\u001b[39;00m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;66;03m# if more than one argument.\u001b[39;00m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(newargs, \u001b[38;5;28mlist\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/epoch/lib/python3.11/site-packages/numpy/core/shape_base.py:19\u001b[0m, in \u001b[0;36m_atleast_1d_dispatcher\u001b[0;34m(*arys)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fromnumeric \u001b[38;5;28;01mas\u001b[39;00m _from_nx\n\u001b[1;32m     15\u001b[0m array_function_dispatch \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(\n\u001b[1;32m     16\u001b[0m     overrides\u001b[38;5;241m.\u001b[39marray_function_dispatch, module\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_atleast_1d_dispatcher\u001b[39m(\u001b[38;5;241m*\u001b[39marys):\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arys\n\u001b[1;32m     23\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_atleast_1d_dispatcher)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21matleast_1d\u001b[39m(\u001b[38;5;241m*\u001b[39marys):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_with_imputed_chip_hours = 15\n",
    "num_with_direct_chip_hours = num_ml_models - num_with_imputed_chip_hours\n",
    "\n",
    "amortized_hardware_capex_medians = np.zeros(NUM_SAMPLES)\n",
    "amortized_hardware_capex_90_cis = np.zeros((NUM_SAMPLES, 2))\n",
    "for i in range(NUM_SAMPLES):\n",
    "    price_performance = norm_from_ci(0.10, 0.18, 90, num_ml_models)\n",
    "    years_since = norm_from_ci(-120, 120, 90, num_ml_models) / DAYS_PER_YEAR\n",
    "    depreciation = 10 ** (price_performance * years_since)\n",
    "\n",
    "    hardware_price = lognorm_from_90_ci(0.5, 2, num_ml_models)\n",
    "    server_cost_overhead = lognorm_from_90_ci(1.3, 2.1, num_ml_models)\n",
    "    cluster_interconnect_overhead = lognorm_from_90_ci(1.07, 1.32, num_ml_models)\n",
    "\n",
    "    training_compute = lognorm_from_90_ci(1/3, 3, num_with_imputed_chip_hours)\n",
    "    peak_flop_per_second = lognorm_from_90_ci(0.5, 2, num_with_imputed_chip_hours)\n",
    "    hardware_utilization = norm_from_ci(0.1, 0.65, 90, num_with_imputed_chip_hours)\n",
    "    realised_flop_per_second = peak_flop_per_second * hardware_utilization\n",
    "    imputed_chip_hours = training_compute / realised_flop_per_second\n",
    "\n",
    "    hardware_quantity = lognorm_from_90_ci(0.8, 1.25, num_with_direct_chip_hours)\n",
    "    training_time = lognorm_from_90_ci(0.5, 2, num_with_direct_chip_hours)\n",
    "    direct_chip_hours = training_time * hardware_quantity\n",
    "\n",
    "    training_chip_hours = np.concatenate([imputed_chip_hours, direct_chip_hours])\n",
    "    amortized_hardware_capex = server_cost_overhead * cluster_interconnect_overhead * (hardware_price / depreciation) * price_performance * training_chip_hours\n",
    "\n",
    "    amortized_hardware_capex_medians[i] = np.median(amortized_hardware_capex)\n",
    "    amortized_hardware_capex_90_cis[i] = np.percentile(amortized_hardware_capex, [5, 95])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.30794631, 6.01740447])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(amortized_hardware_capex_90_cis, axis=0) / np.median(amortized_hardware_capex_medians)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epoch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

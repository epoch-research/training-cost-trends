{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aPSm37gghSrl"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "8Edmo7vA2Eg1",
    "ExecuteTime": {
     "end_time": "2024-05-15T01:06:35.252782400Z",
     "start_time": "2024-05-15T01:06:33.918845100Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "fXBbtVKi2LmG",
    "ExecuteTime": {
     "end_time": "2024-05-14T22:01:35.430939600Z",
     "start_time": "2024-05-14T22:01:34.726683700Z"
    }
   },
   "outputs": [],
   "source": [
    "data_url = 'https://epochai.org/data/epochdb/all_systems.csv'\n",
    "dtypes = {\n",
    "    'Training compute (FLOP)': np.float64,\n",
    "}\n",
    "pcd_df = pd.read_csv(data_url, dtype=dtypes)\n",
    "pcd_df['Decimal year'] = pd.to_datetime(pcd_df['Publication date']).dt.year + (pd.to_datetime(pcd_df['Publication date']).dt.month - 1) / 12 + (pd.to_datetime(pcd_df['Publication date']).dt.day - 1) / 365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "yyQJeGOA2m7e",
    "outputId": "0fe58fa3-79c0-46d1-f5ff-00802616c0b0",
    "ExecuteTime": {
     "end_time": "2024-05-14T22:01:35.479932900Z",
     "start_time": "2024-05-14T22:01:35.438743300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                  System    Domain  \\\n0                             Fugaku-LLM  Language   \n1                          Qwen 1.5 110B  Language   \n2                       phi-3-medium 14B  Language   \n3                          SenseNova 5.0  Language   \n4                            Llama 3-70B  Language   \n...                                  ...       ...   \n1288  Sequence-based pattern recognition    Vision   \n1289              Self Organizing System     Other   \n1290                   Genetic algorithm     Other   \n1291                               SNARC  Robotics   \n1292                             Theseus  Robotics   \n\n                                           Organization Publication date  \\\n0     Tohoku University,CyberAgent,Tokyo Institute o...       2024-05-10   \n1                                               Alibaba       2024-04-25   \n2                                             Microsoft       2024-04-23   \n3                                             SenseTime       2024-04-23   \n4                                               Meta AI       2024-04-18   \n...                                                 ...              ...   \n1288        Massachusetts Institute of Technology (MIT)       1955-03-01   \n1289        Massachusetts Institute of Technology (MIT)       1955-03-01   \n1290                       Institute for Advanced Study       1954-07-02   \n1291                                 Harvard University       1952-01-08   \n1292                                  Bell Laboratories       1950-07-02   \n\n                                              Reference  \\\n0     Release of “Fugaku-LLM” – a large language mod...   \n1     Qwen1.5-110B: The First 100B+ Model of the Qwe...   \n2     Phi-3 Technical Report: A Highly Capable Langu...   \n3                                                   NaN   \n4     Introducing Meta Llama 3: The most capable ope...   \n...                                                 ...   \n1288           Pattern recognition and modern computers   \n1289  Generalization of pattern recognition in a sel...   \n1290            Numerical testing of evolution theories   \n1291  A Neural-Analogue Calculator Based upon a Prob...   \n1292                                       Mighty Mouse   \n\n                                                   Link    Parameters  \\\n0     https://www.fujitsu.com/global/about/resources...  1.300000e+10   \n1     https://qwenlm.github.io/blog/qwen1.5-110b/?re...  1.100000e+11   \n2                      https://arxiv.org/abs/2404.14219  1.400000e+10   \n3                       https://zhidx.com/p/421866.html           NaN   \n4     https://ai.meta.com/blog/meta-llama-3/\\n\\nhttp...  7.000000e+10   \n...                                                 ...           ...   \n1288     https://dl.acm.org/doi/10.1145/1455292.1455310           NaN   \n1289     https://dl.acm.org/doi/10.1145/1455292.1455309  2.250000e+02   \n1290  https://link.springer.com/article/10.1007/BF01...           NaN   \n1291  https://en.wikipedia.org/wiki/Stochastic_neura...  4.000000e+01   \n1292  https://www.technologyreview.com/2018/12/19/13...  4.000000e+01   \n\n                                       Parameters notes  \\\n0            \"Fugaku-LLM has 13 billion parameters (2)\"   \n1                                                   NaN   \n2                                                   14B   \n3                                                   NaN   \n4                                                   NaN   \n...                                                 ...   \n1288                                                NaN   \n1289         Figure 4 contains the learnt weight matrix   \n1290                                                NaN   \n1291  The link below seems to suggest the SNARC had ...   \n1292  The learned part is the maze configuration. Th...   \n\n      Training compute (FLOP)  \\\n0                2.964000e+22   \n1                         NaN   \n2                4.032000e+23   \n3                         NaN   \n4                6.300000e+24   \n...                       ...   \n1288                      NaN   \n1289                      NaN   \n1290                      NaN   \n1291                      NaN   \n1292             4.000000e+01   \n\n                                 Training compute notes  ... Citations  \\\n0     https://www.wolframalpha.com/input?i=6+FLOP+*+...  ...       NaN   \n1                                                   NaN  ...       NaN   \n2     counting operations: 6×4.8×10^12×14×10^9 ≈ 4.0...  ...       NaN   \n3                                                   NaN  ...       NaN   \n4     direct calculation\\n15000000000000 tokens*7000...  ...       NaN   \n...                                                 ...  ...       ...   \n1288                                                NaN  ...     290.0   \n1289                                                NaN  ...      93.0   \n1290                                                NaN  ...     266.0   \n1291                                                NaN  ...      33.0   \n1292  The \"training\" consists on the mouse running a...  ...       0.0   \n\n      Base model Finetune compute notes Training cloud compute vendor  \\\n0            NaN                    NaN                           NaN   \n1            NaN                    NaN                           NaN   \n2            NaN                    NaN                           NaN   \n3            NaN                    NaN                           NaN   \n4            NaN                    NaN                           NaN   \n...          ...                    ...                           ...   \n1288         NaN                    NaN                           NaN   \n1289         NaN                    NaN                           NaN   \n1290         NaN                    NaN                           NaN   \n1291         NaN                    NaN                           NaN   \n1292         NaN                    NaN                           NaN   \n\n     Batch size notes Finetune compute (FLOP) Training compute upper bound  \\\n0                 NaN                     NaN                          NaN   \n1                 NaN                     NaN                          NaN   \n2                 NaN                     NaN                          NaN   \n3                 NaN                     NaN                          NaN   \n4                 NaN                     NaN                          NaN   \n...               ...                     ...                          ...   \n1288              NaN                     NaN                          NaN   \n1289              NaN                     NaN                          NaN   \n1290              NaN                     NaN                          NaN   \n1291              NaN                     NaN                          NaN   \n1292              NaN                     NaN                          NaN   \n\n     Archived links Benchmark data Decimal year  \n0               NaN            NaN  2024.357991  \n1               NaN            NaN  2024.315753  \n2               NaN            NaN  2024.310274  \n3               NaN            NaN  2024.310274  \n4               NaN            NaN  2024.296575  \n...             ...            ...          ...  \n1288            NaN            NaN  1955.166667  \n1289            NaN            NaN  1955.166667  \n1290            NaN            NaN  1954.502740  \n1291            NaN            NaN  1952.019178  \n1292            NaN            NaN  1950.502740  \n\n[1293 rows x 46 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>System</th>\n      <th>Domain</th>\n      <th>Organization</th>\n      <th>Publication date</th>\n      <th>Reference</th>\n      <th>Link</th>\n      <th>Parameters</th>\n      <th>Parameters notes</th>\n      <th>Training compute (FLOP)</th>\n      <th>Training compute notes</th>\n      <th>...</th>\n      <th>Citations</th>\n      <th>Base model</th>\n      <th>Finetune compute notes</th>\n      <th>Training cloud compute vendor</th>\n      <th>Batch size notes</th>\n      <th>Finetune compute (FLOP)</th>\n      <th>Training compute upper bound</th>\n      <th>Archived links</th>\n      <th>Benchmark data</th>\n      <th>Decimal year</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Fugaku-LLM</td>\n      <td>Language</td>\n      <td>Tohoku University,CyberAgent,Tokyo Institute o...</td>\n      <td>2024-05-10</td>\n      <td>Release of “Fugaku-LLM” – a large language mod...</td>\n      <td>https://www.fujitsu.com/global/about/resources...</td>\n      <td>1.300000e+10</td>\n      <td>\"Fugaku-LLM has 13 billion parameters (2)\"</td>\n      <td>2.964000e+22</td>\n      <td>https://www.wolframalpha.com/input?i=6+FLOP+*+...</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2024.357991</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Qwen 1.5 110B</td>\n      <td>Language</td>\n      <td>Alibaba</td>\n      <td>2024-04-25</td>\n      <td>Qwen1.5-110B: The First 100B+ Model of the Qwe...</td>\n      <td>https://qwenlm.github.io/blog/qwen1.5-110b/?re...</td>\n      <td>1.100000e+11</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2024.315753</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>phi-3-medium 14B</td>\n      <td>Language</td>\n      <td>Microsoft</td>\n      <td>2024-04-23</td>\n      <td>Phi-3 Technical Report: A Highly Capable Langu...</td>\n      <td>https://arxiv.org/abs/2404.14219</td>\n      <td>1.400000e+10</td>\n      <td>14B</td>\n      <td>4.032000e+23</td>\n      <td>counting operations: 6×4.8×10^12×14×10^9 ≈ 4.0...</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2024.310274</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>SenseNova 5.0</td>\n      <td>Language</td>\n      <td>SenseTime</td>\n      <td>2024-04-23</td>\n      <td>NaN</td>\n      <td>https://zhidx.com/p/421866.html</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2024.310274</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Llama 3-70B</td>\n      <td>Language</td>\n      <td>Meta AI</td>\n      <td>2024-04-18</td>\n      <td>Introducing Meta Llama 3: The most capable ope...</td>\n      <td>https://ai.meta.com/blog/meta-llama-3/\\n\\nhttp...</td>\n      <td>7.000000e+10</td>\n      <td>NaN</td>\n      <td>6.300000e+24</td>\n      <td>direct calculation\\n15000000000000 tokens*7000...</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2024.296575</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1288</th>\n      <td>Sequence-based pattern recognition</td>\n      <td>Vision</td>\n      <td>Massachusetts Institute of Technology (MIT)</td>\n      <td>1955-03-01</td>\n      <td>Pattern recognition and modern computers</td>\n      <td>https://dl.acm.org/doi/10.1145/1455292.1455310</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>290.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1955.166667</td>\n    </tr>\n    <tr>\n      <th>1289</th>\n      <td>Self Organizing System</td>\n      <td>Other</td>\n      <td>Massachusetts Institute of Technology (MIT)</td>\n      <td>1955-03-01</td>\n      <td>Generalization of pattern recognition in a sel...</td>\n      <td>https://dl.acm.org/doi/10.1145/1455292.1455309</td>\n      <td>2.250000e+02</td>\n      <td>Figure 4 contains the learnt weight matrix</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>93.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1955.166667</td>\n    </tr>\n    <tr>\n      <th>1290</th>\n      <td>Genetic algorithm</td>\n      <td>Other</td>\n      <td>Institute for Advanced Study</td>\n      <td>1954-07-02</td>\n      <td>Numerical testing of evolution theories</td>\n      <td>https://link.springer.com/article/10.1007/BF01...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>266.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1954.502740</td>\n    </tr>\n    <tr>\n      <th>1291</th>\n      <td>SNARC</td>\n      <td>Robotics</td>\n      <td>Harvard University</td>\n      <td>1952-01-08</td>\n      <td>A Neural-Analogue Calculator Based upon a Prob...</td>\n      <td>https://en.wikipedia.org/wiki/Stochastic_neura...</td>\n      <td>4.000000e+01</td>\n      <td>The link below seems to suggest the SNARC had ...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>33.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1952.019178</td>\n    </tr>\n    <tr>\n      <th>1292</th>\n      <td>Theseus</td>\n      <td>Robotics</td>\n      <td>Bell Laboratories</td>\n      <td>1950-07-02</td>\n      <td>Mighty Mouse</td>\n      <td>https://www.technologyreview.com/2018/12/19/13...</td>\n      <td>4.000000e+01</td>\n      <td>The learned part is the maze configuration. Th...</td>\n      <td>4.000000e+01</td>\n      <td>The \"training\" consists on the mouse running a...</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1950.502740</td>\n    </tr>\n  </tbody>\n</table>\n<p>1293 rows × 46 columns</p>\n</div>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pcd_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "XZ0FKyaZ3h_W",
    "ExecuteTime": {
     "end_time": "2024-05-14T22:01:35.496147Z",
     "start_time": "2024-05-14T22:01:35.479932900Z"
    }
   },
   "outputs": [],
   "source": [
    "pcd_df['Publication date'] = pd.to_datetime(pcd_df['Publication date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "rcOYiwwg3oVY",
    "ExecuteTime": {
     "end_time": "2024-05-14T22:01:35.537518Z",
     "start_time": "2024-05-14T22:01:35.496147Z"
    }
   },
   "outputs": [],
   "source": [
    "pcd_df.sort_values('Publication date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "QLUi4aysAXWr",
    "ExecuteTime": {
     "end_time": "2024-05-14T22:01:35.620091600Z",
     "start_time": "2024-05-14T22:01:35.512620400Z"
    }
   },
   "outputs": [],
   "source": [
    "pcd_df.dropna(subset=['Publication date', 'Notability criteria', 'Training compute (FLOP)'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "wGGuurmk4uZd",
    "outputId": "5d822c9c-67e5-4f2c-ff8b-9418f82d05e2",
    "ExecuteTime": {
     "end_time": "2024-05-14T22:01:35.727245900Z",
     "start_time": "2024-05-14T22:01:35.554324500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                      System                      Domain  \\\n1292                 Theseus                    Robotics   \n1286       Perceptron Mark I                       Other   \n1285     Pandemonium (morse)                    Language   \n1284  Samuel Neural Checkers                       Games   \n1282       Perceptron (1960)                      Vision   \n...                      ...                         ...   \n29    MegaScale (Production)                    Language   \n20            Inflection-2.5                    Language   \n19                   MM1-30B  Multimodal,Language,Vision   \n12         Mixture-of-Depths                    Language   \n4                Llama 3-70B                    Language   \n\n                                           Organization Publication date  \\\n1292                                  Bell Laboratories       1950-07-02   \n1286  Cornell Aeronautical Laboratory,Cornell Univer...       1957-01-01   \n1285        Massachusetts Institute of Technology (MIT)       1959-02-01   \n1284                                                IBM       1959-07-01   \n1282                    Cornell Aeronautical Laboratory       1960-03-30   \n...                                                 ...              ...   \n29                          ByteDance,Peking University       2024-02-23   \n20                                        Inflection AI       2024-03-07   \n19                                                Apple       2024-03-14   \n12               Google DeepMind,McGill University,Mila       2024-04-02   \n4                                               Meta AI       2024-04-18   \n\n                                              Reference  \\\n1292                                       Mighty Mouse   \n1286  The Perceptron—a perceiving and recognizing au...   \n1285               Pandemonium: A Paradigm for Learning   \n1284  Some studies in machine learning using the gam...   \n1282                  Perceptron Simulation Experiments   \n...                                                 ...   \n29    MegaScale: Scaling Large Language Model Traini...   \n20    Inflection-2.5: meet the world's best personal AI   \n19    MM1: Methods, Analysis & Insights from Multimo...   \n12    Mixture-of-Depths: Dynamically allocating comp...   \n4     Introducing Meta Llama 3: The most capable ope...   \n\n                                                   Link    Parameters  \\\n1292  https://www.technologyreview.com/2018/12/19/13...  4.000000e+01   \n1286  https://blogs.umass.edu/brain-wars/files/2016/...  1.000000e+03   \n1285        https://aitopics.org/doc/classics:504E1BAC/           NaN   \n1284  https://ieeexplore.ieee.org/abstract/document/...  1.600000e+01   \n1282  https://www.semanticscholar.org/paper/Perceptr...  1.000000e+03   \n...                                                 ...           ...   \n29                     https://arxiv.org/abs/2402.15627  5.300000e+11   \n20                 https://inflection.ai/inflection-2-5           NaN   \n19                     https://arxiv.org/abs/2403.09611  3.000000e+10   \n12                     https://arxiv.org/abs/2404.02258  3.000000e+09   \n4     https://ai.meta.com/blog/meta-llama-3/\\n\\nhttp...  7.000000e+10   \n\n                                       Parameters notes  \\\n1292  The learned part is the maze configuration. Th...   \n1286  \"Figure 4.8 Illustration of the Mark 1 percept...   \n1285  The paper mentions 11 function types. Unclear ...   \n1284  \"with 16 terms for generalization learning\"\\n\\...   \n1282  \" The first program was designed to handle\\nup...   \n...                                                 ...   \n29    Production run is stated to have \"hundreds of ...   \n20                                                  NaN   \n19                                                  30B   \n12    Figure 4: \"We used the 12.5% capacity MoD vari...   \n4                                                   NaN   \n\n      Training compute (FLOP)  \\\n1292             4.000000e+01   \n1286             6.948949e+05   \n1285             6.000000e+08   \n1284             4.284000e+08   \n1282             7.200000e+08   \n...                       ...   \n29               1.200000e+25   \n20               1.000100e+25   \n19               4.300000e+23   \n12               1.000000e+20   \n4                6.300000e+24   \n\n                                 Training compute notes  ... Citations  \\\n1292  The \"training\" consists on the mouse running a...  ...       0.0   \n1286  Extracted from AI and Compute (https://openai....  ...    1610.0   \n1285  The paper mentions using an IBM 704, which can...  ...    1453.0   \n1284  \"it can learn to do this in a remarkably short...  ...    4509.0   \n1282  4000 * 12000 * 15\\nfrom the text \"This program...  ...     394.0   \n...                                                 ...  ...       ...   \n29    Speculative. The model is stated to have train...  ...       1.0   \n20    \"Inflection-1 used approximately 4% the traini...  ...       NaN   \n19    Pre-trained on ~2B image-text pairs and 2T tok...  ...      11.0   \n12    Figure 4: \"We used the 12.5% capacity MoD vari...  ...       1.0   \n4     direct calculation\\n15000000000000 tokens*7000...  ...       NaN   \n\n      Base model Finetune compute notes Training cloud compute vendor  \\\n1292         NaN                    NaN                           NaN   \n1286         NaN                    NaN                           NaN   \n1285         NaN                    NaN                           NaN   \n1284         NaN                    NaN                           NaN   \n1282         NaN                    NaN                           NaN   \n...          ...                    ...                           ...   \n29           NaN                    NaN                           NaN   \n20           NaN                    NaN                           NaN   \n19           NaN                    NaN                           NaN   \n12           NaN                    NaN                           NaN   \n4            NaN                    NaN                           NaN   \n\n     Batch size notes Finetune compute (FLOP) Training compute upper bound  \\\n1292              NaN                     NaN                          NaN   \n1286              NaN                     NaN                          NaN   \n1285              NaN                     NaN                          NaN   \n1284              NaN                     NaN                          NaN   \n1282              NaN                     NaN                          NaN   \n...               ...                     ...                          ...   \n29                NaN                     NaN                          NaN   \n20                NaN                     NaN                          NaN   \n19                NaN                     NaN                          NaN   \n12                NaN                     NaN                          NaN   \n4                 NaN                     NaN                          NaN   \n\n     Archived links Benchmark data Decimal year  \n1292            NaN            NaN  1950.502740  \n1286            NaN            NaN  1957.000000  \n1285            NaN            NaN  1959.083333  \n1284            NaN            NaN  1959.500000  \n1282            NaN            NaN  1960.246119  \n...             ...            ...          ...  \n29              NaN            NaN  2024.143607  \n20              NaN            NaN  2024.183105  \n19              NaN            NaN  2024.202283  \n12              NaN            NaN  2024.252740  \n4               NaN            NaN  2024.296575  \n\n[365 rows x 46 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>System</th>\n      <th>Domain</th>\n      <th>Organization</th>\n      <th>Publication date</th>\n      <th>Reference</th>\n      <th>Link</th>\n      <th>Parameters</th>\n      <th>Parameters notes</th>\n      <th>Training compute (FLOP)</th>\n      <th>Training compute notes</th>\n      <th>...</th>\n      <th>Citations</th>\n      <th>Base model</th>\n      <th>Finetune compute notes</th>\n      <th>Training cloud compute vendor</th>\n      <th>Batch size notes</th>\n      <th>Finetune compute (FLOP)</th>\n      <th>Training compute upper bound</th>\n      <th>Archived links</th>\n      <th>Benchmark data</th>\n      <th>Decimal year</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1292</th>\n      <td>Theseus</td>\n      <td>Robotics</td>\n      <td>Bell Laboratories</td>\n      <td>1950-07-02</td>\n      <td>Mighty Mouse</td>\n      <td>https://www.technologyreview.com/2018/12/19/13...</td>\n      <td>4.000000e+01</td>\n      <td>The learned part is the maze configuration. Th...</td>\n      <td>4.000000e+01</td>\n      <td>The \"training\" consists on the mouse running a...</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1950.502740</td>\n    </tr>\n    <tr>\n      <th>1286</th>\n      <td>Perceptron Mark I</td>\n      <td>Other</td>\n      <td>Cornell Aeronautical Laboratory,Cornell Univer...</td>\n      <td>1957-01-01</td>\n      <td>The Perceptron—a perceiving and recognizing au...</td>\n      <td>https://blogs.umass.edu/brain-wars/files/2016/...</td>\n      <td>1.000000e+03</td>\n      <td>\"Figure 4.8 Illustration of the Mark 1 percept...</td>\n      <td>6.948949e+05</td>\n      <td>Extracted from AI and Compute (https://openai....</td>\n      <td>...</td>\n      <td>1610.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1957.000000</td>\n    </tr>\n    <tr>\n      <th>1285</th>\n      <td>Pandemonium (morse)</td>\n      <td>Language</td>\n      <td>Massachusetts Institute of Technology (MIT)</td>\n      <td>1959-02-01</td>\n      <td>Pandemonium: A Paradigm for Learning</td>\n      <td>https://aitopics.org/doc/classics:504E1BAC/</td>\n      <td>NaN</td>\n      <td>The paper mentions 11 function types. Unclear ...</td>\n      <td>6.000000e+08</td>\n      <td>The paper mentions using an IBM 704, which can...</td>\n      <td>...</td>\n      <td>1453.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1959.083333</td>\n    </tr>\n    <tr>\n      <th>1284</th>\n      <td>Samuel Neural Checkers</td>\n      <td>Games</td>\n      <td>IBM</td>\n      <td>1959-07-01</td>\n      <td>Some studies in machine learning using the gam...</td>\n      <td>https://ieeexplore.ieee.org/abstract/document/...</td>\n      <td>1.600000e+01</td>\n      <td>\"with 16 terms for generalization learning\"\\n\\...</td>\n      <td>4.284000e+08</td>\n      <td>\"it can learn to do this in a remarkably short...</td>\n      <td>...</td>\n      <td>4509.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1959.500000</td>\n    </tr>\n    <tr>\n      <th>1282</th>\n      <td>Perceptron (1960)</td>\n      <td>Vision</td>\n      <td>Cornell Aeronautical Laboratory</td>\n      <td>1960-03-30</td>\n      <td>Perceptron Simulation Experiments</td>\n      <td>https://www.semanticscholar.org/paper/Perceptr...</td>\n      <td>1.000000e+03</td>\n      <td>\" The first program was designed to handle\\nup...</td>\n      <td>7.200000e+08</td>\n      <td>4000 * 12000 * 15\\nfrom the text \"This program...</td>\n      <td>...</td>\n      <td>394.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1960.246119</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>MegaScale (Production)</td>\n      <td>Language</td>\n      <td>ByteDance,Peking University</td>\n      <td>2024-02-23</td>\n      <td>MegaScale: Scaling Large Language Model Traini...</td>\n      <td>https://arxiv.org/abs/2402.15627</td>\n      <td>5.300000e+11</td>\n      <td>Production run is stated to have \"hundreds of ...</td>\n      <td>1.200000e+25</td>\n      <td>Speculative. The model is stated to have train...</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2024.143607</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>Inflection-2.5</td>\n      <td>Language</td>\n      <td>Inflection AI</td>\n      <td>2024-03-07</td>\n      <td>Inflection-2.5: meet the world's best personal AI</td>\n      <td>https://inflection.ai/inflection-2-5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.000100e+25</td>\n      <td>\"Inflection-1 used approximately 4% the traini...</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2024.183105</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>MM1-30B</td>\n      <td>Multimodal,Language,Vision</td>\n      <td>Apple</td>\n      <td>2024-03-14</td>\n      <td>MM1: Methods, Analysis &amp; Insights from Multimo...</td>\n      <td>https://arxiv.org/abs/2403.09611</td>\n      <td>3.000000e+10</td>\n      <td>30B</td>\n      <td>4.300000e+23</td>\n      <td>Pre-trained on ~2B image-text pairs and 2T tok...</td>\n      <td>...</td>\n      <td>11.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2024.202283</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>Mixture-of-Depths</td>\n      <td>Language</td>\n      <td>Google DeepMind,McGill University,Mila</td>\n      <td>2024-04-02</td>\n      <td>Mixture-of-Depths: Dynamically allocating comp...</td>\n      <td>https://arxiv.org/abs/2404.02258</td>\n      <td>3.000000e+09</td>\n      <td>Figure 4: \"We used the 12.5% capacity MoD vari...</td>\n      <td>1.000000e+20</td>\n      <td>Figure 4: \"We used the 12.5% capacity MoD vari...</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2024.252740</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Llama 3-70B</td>\n      <td>Language</td>\n      <td>Meta AI</td>\n      <td>2024-04-18</td>\n      <td>Introducing Meta Llama 3: The most capable ope...</td>\n      <td>https://ai.meta.com/blog/meta-llama-3/\\n\\nhttp...</td>\n      <td>7.000000e+10</td>\n      <td>NaN</td>\n      <td>6.300000e+24</td>\n      <td>direct calculation\\n15000000000000 tokens*7000...</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2024.296575</td>\n    </tr>\n  </tbody>\n</table>\n<p>365 rows × 46 columns</p>\n</div>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pcd_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "zFUQoG_01L8m",
    "ExecuteTime": {
     "end_time": "2024-05-14T22:01:35.727798700Z",
     "start_time": "2024-05-14T22:01:35.620091600Z"
    }
   },
   "outputs": [],
   "source": [
    "outlier_window_size = 2  # years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T22:01:35.727798700Z",
     "start_time": "2024-05-14T22:01:35.636551200Z"
    }
   },
   "outputs": [],
   "source": [
    "start_large_scale_era = '2015-10-01'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OrayDGa4hK6X"
   },
   "source": [
    "# Top n all-time most compute-intensive (FIRST CHOICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T22:01:48.336113600Z",
     "start_time": "2024-05-14T22:01:35.661296500Z"
    }
   },
   "outputs": [],
   "source": [
    "for n in range(1, 21):\n",
    "    # Add a column to mark the top n models\n",
    "    pcd_df[f'top_{n}_at_release'] = False\n",
    "    \n",
    "    for row, model in pcd_df.iterrows():\n",
    "        # Filter for models released through the model's release date\n",
    "        yearly_df = pcd_df[pcd_df['Decimal year'] <= model['Decimal year']]\n",
    "        # get the top n models by compute\n",
    "        top_n_models = yearly_df.nlargest(n, 'Training compute (FLOP)')\n",
    "        # mark these models in the original dataframe\n",
    "        pcd_df.loc[top_n_models.index, f'top_{n}_at_release'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T22:01:48.344166Z",
     "start_time": "2024-05-14T22:01:48.336113600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['System', 'Domain', 'Organization', 'Publication date', 'Reference',\n       'Link', 'Parameters', 'Parameters notes', 'Training compute (FLOP)',\n       'Training compute notes', 'Training dataset notes',\n       'Training dataset size (datapoints)', 'Dataset size notes', 'Abstract',\n       'Confidence', 'Model accessibility', 'Last modified', 'Created By',\n       'Country (from Organization)', 'Organization categorization',\n       'Training dataset', 'Authors', 'Notability criteria',\n       'Notability criteria notes', 'Training hardware', 'Exclude',\n       'Hardware quantity', 'Hardware utilization', 'Training time (hours)',\n       'Training time notes', 'Batch size', 'Approach',\n       'Training compute lower bound', 'Epochs', 'Foundation model',\n       'Training data center', 'Citations', 'Base model',\n       'Finetune compute notes', 'Training cloud compute vendor',\n       'Batch size notes', 'Finetune compute (FLOP)',\n       'Training compute upper bound', 'Archived links', 'Benchmark data',\n       'Decimal year', 'top_1_at_release', 'top_2_at_release',\n       'top_3_at_release', 'top_4_at_release', 'top_5_at_release',\n       'top_6_at_release', 'top_7_at_release', 'top_8_at_release',\n       'top_9_at_release', 'top_10_at_release', 'top_11_at_release',\n       'top_12_at_release', 'top_13_at_release', 'top_14_at_release',\n       'top_15_at_release', 'top_16_at_release', 'top_17_at_release',\n       'top_18_at_release', 'top_19_at_release', 'top_20_at_release'],\n      dtype='object')"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pcd_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T22:01:48.410647300Z",
     "start_time": "2024-05-14T22:01:48.344166Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "74"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(pcd_df['top_4_at_release'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T22:01:48.410914400Z",
     "start_time": "2024-05-14T22:01:48.361080100Z"
    }
   },
   "outputs": [],
   "source": [
    "pcd_df_n = pcd_df[(pcd_df['Decimal year'] > 2015.75) & (pcd_df['Decimal year'] < 2024)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T22:01:48.860072900Z",
     "start_time": "2024-05-14T22:01:48.393328900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "top_n = [np.sum(pcd_df_n[f'top_{n}_at_release']) for n in range(1, 21)]\n",
    "plt.scatter(range(1, 21), top_n)\n",
    "plt.xticks(ticks=range(1, 21))\n",
    "plt.xlabel('n-value for top n')\n",
    "plt.ylabel('Models which have ever been in the top n')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T22:01:48.901504100Z",
     "start_time": "2024-05-14T22:01:48.860072900Z"
    }
   },
   "outputs": [],
   "source": [
    "top_n_models = {}\n",
    "for n in range(1, 21):\n",
    "    models = pcd_df_n[pcd_df_n[f'top_{n}_at_release']]['System'].values.tolist()\n",
    "    top_n_models[n] = set(models)\n",
    "\n",
    "for n in range(20, 1, -1):\n",
    "    top_n_models[n] = list(top_n_models[n].difference(top_n_models[n-1]))\n",
    "top_n_models[1] = list(top_n_models[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T22:01:48.901504100Z",
     "start_time": "2024-05-14T22:01:48.884061400Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('data/frontier_systems_by_top_n.json', 'w') as f:\n",
    "    json.dump(top_n_models, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OrayDGa4hK6X"
   },
   "source": [
    "# Default large scale systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ort2OjXauNI-"
   },
   "source": [
    "https://colab.research.google.com/drive/1PLGY5ErysqQMfy7Z08uIR2cTnnDgSaVR?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "Xo0BSLmQ4RpG",
    "ExecuteTime": {
     "end_time": "2024-05-14T22:01:48.918675700Z",
     "start_time": "2024-05-14T22:01:48.892760100Z"
    }
   },
   "outputs": [],
   "source": [
    "high_outliers_z_value_threshold = 0.76"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "sTTmucLNtU8l",
    "ExecuteTime": {
     "end_time": "2024-05-14T22:01:49.570926600Z",
     "start_time": "2024-05-14T22:01:48.918675700Z"
    }
   },
   "outputs": [],
   "source": [
    "large_scale_idx = set()\n",
    "\n",
    "for index, row in pcd_df.iterrows():\n",
    "  # Filter entries in a 2-year window around the paper\n",
    "  window_size = pd.Timedelta(f'{outlier_window_size*52*7} days')\n",
    "  half_window_size = window_size / 2\n",
    "  mask = ( row['Publication date'] - half_window_size <= pcd_df['Publication date'] ) &\\\n",
    "        ( pcd_df['Publication date'] <= row['Publication date'] + half_window_size )\n",
    "  window_df = pcd_df[mask].copy()\n",
    "\n",
    "  if len(window_df) < 2: continue\n",
    "\n",
    "  window_df['Training compute (FLOP) z scores'] = stats.zscore(np.log10(window_df['Training compute (FLOP)'].values))\n",
    "  if window_df.loc[index, 'Training compute (FLOP) z scores'] > high_outliers_z_value_threshold:\n",
    "    large_scale_idx.add(index)\n",
    "\n",
    "large_scale_mask = pcd_df.index.isin(large_scale_idx) & (pcd_df['Publication date'] > start_large_scale_era)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "w2HakL5g4iUq",
    "ExecuteTime": {
     "end_time": "2024-05-14T22:01:49.589103400Z",
     "start_time": "2024-05-14T22:01:49.570926600Z"
    }
   },
   "outputs": [],
   "source": [
    "large_scale_df = pcd_df[large_scale_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "y4-TmYMQ7Iex",
    "outputId": "485901a3-4ccd-4956-bba8-e81f64bf5c0f",
    "ExecuteTime": {
     "end_time": "2024-05-14T22:01:49.633974600Z",
     "start_time": "2024-05-14T22:01:49.589103400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                      System                      Domain  \\\n1010             AlphaGo Lee                       Games   \n971                     GNMT                    Language   \n970                 Xception                      Vision   \n964         NASv3 (CIFAR-10)                      Vision   \n949           AlphaGo Master                       Games   \n...                      ...                         ...   \n80                  Qwen-72B                    Language   \n72              Gemini Ultra  Multimodal,Language,Vision   \n29    MegaScale (Production)                    Language   \n20            Inflection-2.5                    Language   \n4                Llama 3-70B                    Language   \n\n                     Organization Publication date  \\\n1010                     DeepMind       2016-01-27   \n971                        Google       2016-09-26   \n970                        Google       2016-10-07   \n964                  Google Brain       2016-11-05   \n949                      DeepMind       2017-01-01   \n...                           ...              ...   \n80                        Alibaba       2023-11-30   \n72                Google DeepMind       2023-12-06   \n29    ByteDance,Peking University       2024-02-23   \n20                  Inflection AI       2024-03-07   \n4                         Meta AI       2024-04-18   \n\n                                              Reference  \\\n1010  Mastering the game of Go with deep neural netw...   \n971   Google's Neural Machine Translation System: Br...   \n970   Xception: Deep Learning with Depthwise Separab...   \n964   Neural Architecture Search with Reinforcement ...   \n949    Mastering the game of Go without human knowledge   \n...                                                 ...   \n80                                                  NaN   \n72    Gemini: A Family of Highly Capable Multimodal ...   \n29    MegaScale: Scaling Large Language Model Traini...   \n20    Inflection-2.5: meet the world's best personal AI   \n4     Introducing Meta Llama 3: The most capable ope...   \n\n                                                   Link    Parameters  \\\n1010        https://www.nature.com/articles/nature16961           NaN   \n971                    https://arxiv.org/abs/1609.08144  2.780000e+08   \n970                    https://arxiv.org/abs/1610.02357  2.285595e+07   \n964                    https://arxiv.org/abs/1611.01578  3.740000e+07   \n949         https://www.nature.com/articles/nature24270           NaN   \n...                                                 ...           ...   \n80                 https://huggingface.co/Qwen/Qwen-72B  7.200000e+10   \n72    https://storage.googleapis.com/deepmind-media/...           NaN   \n29                     https://arxiv.org/abs/2402.15627  5.300000e+11   \n20                 https://inflection.ai/inflection-2-5           NaN   \n4     https://ai.meta.com/blog/meta-llama-3/\\n\\nhttp...  7.000000e+10   \n\n                                       Parameters notes  \\\n1010                                                NaN   \n971   Table 5 in 'Outrageously Large Neural Networks...   \n970                                             Table 3   \n964                                             Table 1   \n949                                                 NaN   \n...                                                 ...   \n80                                                  72B   \n72                                                  NaN   \n29    Production run is stated to have \"hundreds of ...   \n20                                                  NaN   \n4                                                   NaN   \n\n      Training compute (FLOP)  \\\n1010             1.900000e+21   \n971              6.900000e+21   \n970              4.360000e+20   \n964              2.200000e+21   \n949              2.000100e+23   \n...                       ...   \n80               1.300000e+24   \n72               5.000000e+25   \n29               1.200000e+25   \n20               1.000100e+25   \n4                6.300000e+24   \n\n                                 Training compute notes  ...  \\\n1010  This number is pretty uncertain. I expect it t...  ...   \n971   sqrt(10 * 100) factor added because production...  ...   \n970   60 K80 GPUs * 30 days * 8.5 TFLOPS/GPU * 0.33 ...  ...   \n964   50 epochs * 50,000 images * 10.0 GFLOPSs * 128...  ...   \n949   This is a guess. There was no single journal p...  ...   \n...                                                 ...  ...   \n80    72 billion params, 3 trillion tokens\\n72b * 3T...  ...   \n72    This number is an estimate based on limited ev...  ...   \n29    Speculative. The model is stated to have train...  ...   \n20    \"Inflection-1 used approximately 4% the traini...  ...   \n4     direct calculation\\n15000000000000 tokens*7000...  ...   \n\n     top_11_at_release  top_12_at_release top_13_at_release top_14_at_release  \\\n1010              True               True              True              True   \n971               True               True              True              True   \n970               True               True              True              True   \n964               True               True              True              True   \n949               True               True              True              True   \n...                ...                ...               ...               ...   \n80               False               True              True              True   \n72                True               True              True              True   \n29                True               True              True              True   \n20                True               True              True              True   \n4                 True               True              True              True   \n\n     top_15_at_release top_16_at_release top_17_at_release top_18_at_release  \\\n1010              True              True              True              True   \n971               True              True              True              True   \n970               True              True              True              True   \n964               True              True              True              True   \n949               True              True              True              True   \n...                ...               ...               ...               ...   \n80                True              True              True              True   \n72                True              True              True              True   \n29                True              True              True              True   \n20                True              True              True              True   \n4                 True              True              True              True   \n\n     top_19_at_release top_20_at_release  \n1010              True              True  \n971               True              True  \n970               True              True  \n964               True              True  \n949               True              True  \n...                ...               ...  \n80                True              True  \n72                True              True  \n29                True              True  \n20                True              True  \n4                 True              True  \n\n[75 rows x 66 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>System</th>\n      <th>Domain</th>\n      <th>Organization</th>\n      <th>Publication date</th>\n      <th>Reference</th>\n      <th>Link</th>\n      <th>Parameters</th>\n      <th>Parameters notes</th>\n      <th>Training compute (FLOP)</th>\n      <th>Training compute notes</th>\n      <th>...</th>\n      <th>top_11_at_release</th>\n      <th>top_12_at_release</th>\n      <th>top_13_at_release</th>\n      <th>top_14_at_release</th>\n      <th>top_15_at_release</th>\n      <th>top_16_at_release</th>\n      <th>top_17_at_release</th>\n      <th>top_18_at_release</th>\n      <th>top_19_at_release</th>\n      <th>top_20_at_release</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1010</th>\n      <td>AlphaGo Lee</td>\n      <td>Games</td>\n      <td>DeepMind</td>\n      <td>2016-01-27</td>\n      <td>Mastering the game of Go with deep neural netw...</td>\n      <td>https://www.nature.com/articles/nature16961</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.900000e+21</td>\n      <td>This number is pretty uncertain. I expect it t...</td>\n      <td>...</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>971</th>\n      <td>GNMT</td>\n      <td>Language</td>\n      <td>Google</td>\n      <td>2016-09-26</td>\n      <td>Google's Neural Machine Translation System: Br...</td>\n      <td>https://arxiv.org/abs/1609.08144</td>\n      <td>2.780000e+08</td>\n      <td>Table 5 in 'Outrageously Large Neural Networks...</td>\n      <td>6.900000e+21</td>\n      <td>sqrt(10 * 100) factor added because production...</td>\n      <td>...</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>970</th>\n      <td>Xception</td>\n      <td>Vision</td>\n      <td>Google</td>\n      <td>2016-10-07</td>\n      <td>Xception: Deep Learning with Depthwise Separab...</td>\n      <td>https://arxiv.org/abs/1610.02357</td>\n      <td>2.285595e+07</td>\n      <td>Table 3</td>\n      <td>4.360000e+20</td>\n      <td>60 K80 GPUs * 30 days * 8.5 TFLOPS/GPU * 0.33 ...</td>\n      <td>...</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>964</th>\n      <td>NASv3 (CIFAR-10)</td>\n      <td>Vision</td>\n      <td>Google Brain</td>\n      <td>2016-11-05</td>\n      <td>Neural Architecture Search with Reinforcement ...</td>\n      <td>https://arxiv.org/abs/1611.01578</td>\n      <td>3.740000e+07</td>\n      <td>Table 1</td>\n      <td>2.200000e+21</td>\n      <td>50 epochs * 50,000 images * 10.0 GFLOPSs * 128...</td>\n      <td>...</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>949</th>\n      <td>AlphaGo Master</td>\n      <td>Games</td>\n      <td>DeepMind</td>\n      <td>2017-01-01</td>\n      <td>Mastering the game of Go without human knowledge</td>\n      <td>https://www.nature.com/articles/nature24270</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2.000100e+23</td>\n      <td>This is a guess. There was no single journal p...</td>\n      <td>...</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>80</th>\n      <td>Qwen-72B</td>\n      <td>Language</td>\n      <td>Alibaba</td>\n      <td>2023-11-30</td>\n      <td>NaN</td>\n      <td>https://huggingface.co/Qwen/Qwen-72B</td>\n      <td>7.200000e+10</td>\n      <td>72B</td>\n      <td>1.300000e+24</td>\n      <td>72 billion params, 3 trillion tokens\\n72b * 3T...</td>\n      <td>...</td>\n      <td>False</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>72</th>\n      <td>Gemini Ultra</td>\n      <td>Multimodal,Language,Vision</td>\n      <td>Google DeepMind</td>\n      <td>2023-12-06</td>\n      <td>Gemini: A Family of Highly Capable Multimodal ...</td>\n      <td>https://storage.googleapis.com/deepmind-media/...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>5.000000e+25</td>\n      <td>This number is an estimate based on limited ev...</td>\n      <td>...</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>MegaScale (Production)</td>\n      <td>Language</td>\n      <td>ByteDance,Peking University</td>\n      <td>2024-02-23</td>\n      <td>MegaScale: Scaling Large Language Model Traini...</td>\n      <td>https://arxiv.org/abs/2402.15627</td>\n      <td>5.300000e+11</td>\n      <td>Production run is stated to have \"hundreds of ...</td>\n      <td>1.200000e+25</td>\n      <td>Speculative. The model is stated to have train...</td>\n      <td>...</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>Inflection-2.5</td>\n      <td>Language</td>\n      <td>Inflection AI</td>\n      <td>2024-03-07</td>\n      <td>Inflection-2.5: meet the world's best personal AI</td>\n      <td>https://inflection.ai/inflection-2-5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.000100e+25</td>\n      <td>\"Inflection-1 used approximately 4% the traini...</td>\n      <td>...</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Llama 3-70B</td>\n      <td>Language</td>\n      <td>Meta AI</td>\n      <td>2024-04-18</td>\n      <td>Introducing Meta Llama 3: The most capable ope...</td>\n      <td>https://ai.meta.com/blog/meta-llama-3/\\n\\nhttp...</td>\n      <td>7.000000e+10</td>\n      <td>NaN</td>\n      <td>6.300000e+24</td>\n      <td>direct calculation\\n15000000000000 tokens*7000...</td>\n      <td>...</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n<p>75 rows × 66 columns</p>\n</div>"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large_scale_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ngeVX9J-1yx",
    "outputId": "949a42f5-e8c9-4c65-fcc9-c5ae79651d10",
    "ExecuteTime": {
     "end_time": "2024-05-14T22:01:49.643681400Z",
     "start_time": "2024-05-14T22:01:49.633974600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama 3-70B\n",
      "Inflection-2.5\n",
      "MegaScale (Production)\n",
      "Gemini Ultra\n",
      "Qwen-72B\n",
      "Inflection-2\n",
      "Grok-1\n",
      "ChatGLM3\n",
      "Falcon-180B\n",
      "Llama 2-70B\n",
      "Claude 2\n",
      "PaLM 2\n",
      "GPT-4\n",
      "LLaMA-65B\n",
      "GPT-3.5 (text-davinci-003)\n",
      "Galactica\n",
      "BLOOM-176B\n",
      "U-PaLM (540B)\n",
      "Flan-PaLM 540B\n",
      "BlenderBot 3\n",
      "GLM-130B\n",
      "Minerva (540B)\n",
      "Parti\n",
      "OPT-175B\n",
      "Flamingo\n",
      "PaLM (540B)\n",
      "Chinchilla\n",
      "ST-MoE\n",
      "LaMDA\n",
      "AlphaCode\n",
      "ERNIE 3.0 Titan\n",
      "GLaM\n",
      "Gopher (280B)\n",
      "Yuan 1.0\n",
      "Megatron-Turing NLG 530B\n",
      "HyperCLOVA\n",
      "GOAT\n",
      "ByT5-XXL\n",
      "ProtT5-XXL\n",
      "Meta Pseudo Labels\n",
      "Switch\n",
      "DALL-E\n",
      "mT5-XXL\n",
      "GShard (dense)\n",
      "iGPT-XL\n",
      "GPT-3 175B (davinci)\n",
      "Turing-NLG\n",
      "Meena\n",
      "ContextNet + Noisy Student\n",
      "OpenAI Five\n",
      "OpenAI Five Rerun\n",
      "AlphaStar\n",
      "T5-11B\n",
      "Megatron-LM (8.3B)\n",
      "Megatron-BERT\n",
      "RoBERTa Large\n",
      "XLNet\n",
      "MnasNet-A3\n",
      "MnasNet-A1 + SSDLite\n",
      "GPT-2 (1.5B)\n",
      "BigGAN-deep 512x512\n",
      "FTW\n",
      "ResNeXt-101 32x48d\n",
      "AmoebaNet-A (F=448)\n",
      "IMPALA\n",
      "AlphaZero\n",
      "AlphaGo Zero\n",
      "OpenAI TI7 DOTA 1v1\n",
      "JFT\n",
      "Libratus\n",
      "AlphaGo Master\n",
      "NASv3 (CIFAR-10)\n",
      "Xception\n",
      "GNMT\n",
      "AlphaGo Lee\n"
     ]
    }
   ],
   "source": [
    "for system in large_scale_df['System'][::-1]:\n",
    "  print(system)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f1TTIzTQhPan"
   },
   "source": [
    "# Percentiles (SECOND CHOICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c-cxrNxPbW7r",
    "outputId": "1ab54428-dc65-4d79-e6a0-feff77ad6e1d",
    "ExecuteTime": {
     "end_time": "2024-05-14T22:01:56.732908200Z",
     "start_time": "2024-05-14T22:01:49.654457200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95\n",
      "90\n",
      "85\n",
      "80\n",
      "75\n",
      "70\n",
      "65\n",
      "60\n",
      "55\n",
      "50\n",
      "45\n",
      "40\n",
      "35\n",
      "30\n",
      "25\n",
      "20\n",
      "15\n",
      "10\n",
      "5\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "frontier_systems_by_percentile = {}\n",
    "percentile_interval = 5\n",
    "for percentile in range(95, -5, -percentile_interval):\n",
    "  print(percentile)\n",
    "  percentile_compute_low = np.zeros(len(pcd_df))\n",
    "  percentile_compute_high = np.zeros(len(pcd_df))\n",
    "  # Iterate through each row and calculate the 2-year moving average for each date\n",
    "  for i, (index, row) in enumerate(pcd_df.iterrows()):\n",
    "    # Define the 2-year window\n",
    "    start_date = row['Publication date'] - pd.DateOffset(years=outlier_window_size/2)\n",
    "    end_date = row['Publication date'] + pd.DateOffset(years=outlier_window_size/2)\n",
    "\n",
    "    # Filter the DataFrame for this window\n",
    "    window_df = pcd_df[(pcd_df['Publication date'] >= start_date) & (pcd_df['Publication date'] <= end_date)]\n",
    "\n",
    "    percentile_compute_low[i] = np.percentile(window_df['Training compute (FLOP)'], percentile)\n",
    "    percentile_compute_high[i] = np.percentile(window_df['Training compute (FLOP)'], percentile + percentile_interval)\n",
    "\n",
    "  frontier_systems_flag = pcd_df['Training compute (FLOP)'] > np.array(percentile_compute_low)\n",
    "  extra_frontier_systems_flag = pcd_df['Training compute (FLOP)'] <= np.array(percentile_compute_high)\n",
    "\n",
    "  # raise Exception(\"Edit the following line if you want to consider models released after 2023-12-31.\")\n",
    "  extra_frontier_systems = pcd_df['System'][frontier_systems_flag & extra_frontier_systems_flag & (pcd_df['Publication date'] > pd.to_datetime('2015-09-30')) & (pcd_df['Publication date'] < pd.to_datetime('2024-01-01'))].values\n",
    "\n",
    "  frontier_systems_by_percentile[percentile] = list(extra_frontier_systems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lGXo_vGde5mz",
    "outputId": "6684d0f4-3ec9-40ca-ff4a-94cd73c830d7",
    "ExecuteTime": {
     "end_time": "2024-05-14T22:01:56.749305300Z",
     "start_time": "2024-05-14T22:01:56.732908200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{95: ['GNMT',\n  'AlphaGo Master',\n  'AlphaGo Zero',\n  'AlphaZero',\n  'ResNeXt-101 32x48d',\n  'FTW',\n  'Megatron-BERT',\n  'OpenAI Five',\n  'Meena',\n  'GPT-3 175B (davinci)',\n  'Megatron-Turing NLG 530B',\n  'PaLM (540B)',\n  'Minerva (540B)',\n  'GPT-4',\n  'Gemini Ultra'],\n 90: ['NASv3 (CIFAR-10)',\n  'T5-11B',\n  'AlphaStar',\n  'mT5-XXL',\n  'Switch',\n  'Gopher (280B)',\n  'ERNIE 3.0 Titan',\n  'Chinchilla',\n  'Flan-PaLM 540B',\n  'U-PaLM (540B)',\n  'GPT-3.5 (text-davinci-003)',\n  'PaLM 2',\n  'Claude 2',\n  'Inflection-2'],\n 85: ['AlphaGo Fan',\n  'AlphaGo Lee',\n  'JFT',\n  'AmoebaNet-A (F=448)',\n  'Megatron-LM (8.3B)',\n  'OpenAI Five Rerun',\n  'Turing-NLG',\n  'Yuan 1.0',\n  'GLaM',\n  'LaMDA',\n  'OPT-175B',\n  'BLOOM-176B',\n  'Falcon-180B',\n  'Grok-1'],\n 80: ['OpenAI TI7 DOTA 1v1',\n  'IMPALA',\n  'BigGAN-deep 512x512',\n  'GPT-2 (1.5B)',\n  'XLNet',\n  'iGPT-XL',\n  'DALL-E',\n  'ByT5-XXL',\n  'HyperCLOVA',\n  'AlphaCode',\n  'ST-MoE',\n  'Flamingo',\n  'Parti',\n  'BlenderBot 3',\n  'Llama 2-70B',\n  'ChatGLM3',\n  'Qwen-72B'],\n 75: ['DeepSpeech2 (English)',\n  'Xception',\n  'Libratus',\n  'BERT-Large',\n  'RoBERTa Large',\n  'ContextNet + Noisy Student',\n  'Meta Pseudo Labels',\n  'ProtT5-XXL',\n  'GOAT',\n  'GPT-NeoX-20B',\n  'GLM-130B',\n  'ViT-22B',\n  'LLaMA-65B',\n  'PanGu-Σ',\n  'xTrimoPGLM -100B',\n  'Yi-34B'],\n 70: ['PolyNet',\n  'MoE',\n  'Big Transformer for Back-Translation',\n  'Mesh-TensorFlow Transformer 4.9B (language modelling)',\n  'MnasNet-A1 + SSDLite',\n  'MnasNet-A3',\n  'iGPT-L',\n  'GShard (dense)',\n  'CoAtNet',\n  'FLAN 137B',\n  'UL2',\n  'AlexaTM 20B',\n  'Galactica'],\n 65: ['ResNet-152 (ImageNet)',\n  'ConvS2S (ensemble of 8 models)',\n  'YOLOv3',\n  'Transformer (Adaptive Input Embeddings)',\n  'BERT-Large-CAS (PTB+WT2+WT103)',\n  'ALBERT-xxlarge',\n  'ELECTRA',\n  'Conformer + Wav2vec 2.0 + Noisy Student',\n  'PLUG',\n  'ProtBERT-BFD',\n  'ProtT5-XXL-BFD',\n  'Florence',\n  'Stable Diffusion (LDM-KL-8-G)',\n  'CoCa',\n  'Falcon-40B',\n  'BloombergGPT',\n  'Skywork-13B',\n  'FunSearch'],\n 60: ['PNASNet-5',\n  'Mesh-TensorFlow Transformer 2.9B (translation)',\n  'T5-3B',\n  'CLIP (ViT L/14@336px)',\n  'CogView',\n  'ALIGN',\n  'ERNIE 3.0',\n  'BASIC-L',\n  'XGLM-7.5B',\n  'ESM2-15B',\n  'PaLI',\n  'Nemotron-3-8B'],\n 55: ['DeepStack',\n  'LSTM (Hebbian, Cache, MbPA)',\n  'QT-Opt',\n  'Population-based DRL',\n  'SciBERT',\n  'CamemBERT',\n  'Noisy Student (L2)',\n  'Once for All',\n  'ViT-Huge/14',\n  'T0-XXL',\n  'Student of Games',\n  'Whisper',\n  'Taiyi-Stable Diffusion',\n  'StarCoder',\n  'WizardCoder-15.5B',\n  'Llama 2-7B',\n  'FinGPT-13B'],\n 50: ['BIDAF',\n  'Transformer',\n  'GPT',\n  'ProxylessNAS',\n  'DD-PPO',\n  'GBERT-Large',\n  'AlphaFold 2',\n  'MSA Transformer',\n  'M6-T',\n  'DeBERTa',\n  'XGLM',\n  'RETRO-7B',\n  'Flan-T5 11B',\n  'LLaMA-7B'],\n 45: ['RetinaNet-R101',\n  'Transformer + Simple Recurrent Unit',\n  'Sandwich Transformer',\n  'German ELECTRA Large',\n  'wave2vec 2.0 LARGE',\n  'CPM-Large',\n  'HuBERT',\n  'Imagen',\n  'ProGen2-xlarge',\n  'NLLB',\n  'OmegaPLM',\n  'WizardLM-7B',\n  'Pangu-Weather',\n  'Jais'],\n 40: ['Part-of-sentence tagging model',\n  'GL-LWGC-AWD-MoS-LSTM + dynamic evaluation (WT2)',\n  'Transformer-XL (257M)',\n  'KataGo',\n  'MuZero',\n  'AlphaFold',\n  'DETR',\n  'ESM1-670M (UR50/D)',\n  'ViT-G/14',\n  'SEER',\n  'Tranception',\n  'CogVLM',\n  'GraphCast',\n  'CogAgent'],\n 35: ['Named Entity Recognition model',\n  'R-FCN',\n  'ResNet-152 + ObjectNet',\n  'Feedback Transformer',\n  'AlphaFold-Multimer',\n  'NÜWA',\n  'ViT-G (model soup)',\n  'Gato',\n  'Nucleotide Transformer',\n  'VideoMAE V2',\n  'Segment Anything Model',\n  'PeptideBERT'],\n 30: ['AWD-LSTM - 3-layer LSTM (tied) + continuous cache pointer (WT2)',\n  'AWD-LSTM-MoS + dynamic evaluation (WT2, 2017)',\n  'QRNN',\n  '(ensemble): AWD-LSTM-DOC (fin) × 5 (WT2)',\n  'TrellisNet',\n  'FAIRSEQ Adaptive Inputs',\n  'DistilBERT',\n  'TaLK Convolution',\n  'ATLAS',\n  'ERNIE-GEN (large)',\n  'LUKE',\n  'ADM',\n  'EMDR',\n  'CodeT5-base',\n  'CodeT5-large',\n  'EVA-01',\n  'Ankh_large',\n  'AudioGen',\n  'DINOv2'],\n 25: ['VD-LSTM+REAL Large',\n  'ULM-FiT',\n  'Big-Little Net (speech)',\n  'Decoupled weight decay regularization',\n  'Hanabi 4 player',\n  'Transformer-XL Large + Phrase Induction',\n  'Tensorized Transformer (257M)',\n  'AlphaX-1',\n  'KEPLER',\n  'ViT + DINO',\n  'Denoising Diffusion Probabilistic Models (LSUN Bedroom)',\n  'S4',\n  'Swin Transformer V2',\n  'PolyCoder',\n  'GenSLM',\n  'Ankh_base',\n  'Incoder-6.7B'],\n 20: ['Zoneout + Variational LSTM (WT2)',\n  'Fraternal dropout + AWD-LSTM 3-layer (WT2)',\n  '4 layer QRNN (h=2500)',\n  'Dropout-LSTM+Noise(Bernoulli) (WT2)',\n  'Cross-lingual alignment',\n  'DLRM-2020',\n  'Base LM + kNN LM + Continuous Cache',\n  'ConSERT',\n  'Masked Autoencoders',\n  'Hybrid H3-2.7B',\n  'BLIP-2 (Q-Former)',\n  'Flan T5-XXL + BLIP-2',\n  'DiT-XL/2'],\n 15: ['Variational (untied weights, MC) LSTM (Large)',\n  'Pointer Sentinel-LSTM (medium)',\n  'Neural Architecture Search with base 8 and shared embeddings',\n  'ENAS',\n  'aLSTM(depth-2)+RecurrentPolicy (WT2)',\n  'Transformer-XL DeFINE (141M)',\n  'DeLight',\n  'ERNIE-Doc (247M)',\n  'EfficientNetV2',\n  'Adaptive Input Transformer + RD',\n  'DNABERT',\n  'AR-LDM',\n  'Discriminator Guidance',\n  'DDPM-IP (CelebA)',\n  'ONE-PEACE'],\n 10: ['EI-REHN-1000D',\n  'DARTS',\n  'NAS+ESS (156M)',\n  'ProBERTa',\n  'SRU++ Large',\n  'Transformer local-attention (NesT-B)',\n  'ProteinBERT',\n  'BERT-RBP',\n  'BEIT-3',\n  'DiffDock',\n  'Fusion in Encoder',\n  'LLaVA 1.5'],\n 5: ['VD-RHN',\n  'ISS',\n  'AWD-LSTM+WT+Cache+IOG (WT2)',\n  'AWD-LSTM-DRILL + dynamic evaluation† (WT2)',\n  'AWD-LSTM + MoS + Partial Shuffled',\n  'UDSMProt',\n  'MMLSTM',\n  'TransformerXL + spectrum control',\n  'Tensor-Transformer(1core)+PN (WT103)',\n  'MedBERT',\n  'Detic',\n  'Segatron-XL large, M=384 + HCP',\n  'Sparse all-MLP',\n  'CaLM',\n  'LLaVA',\n  'MultiBand Diffusion'],\n 0: ['2-layer-LSTM+Deep-Gradient-Compression',\n  'Fine-tuned-AWD-LSTM-DOC(fin)',\n  'Multi-cell LSTM',\n  'Pluribus',\n  'DensePhrases',\n  'CT-MoS (WT2)',\n  'PermuteFormer',\n  'base LM+GNN+kNN',\n  'DITTO',\n  'Mogrifier RLSTM (WT2)',\n  'VALL-E',\n  'HyenaDNA',\n  'CODEFUSION (Python)']}"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frontier_systems_by_percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T22:01:56.807640800Z",
     "start_time": "2024-05-14T22:01:56.749305300Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save to JSON\n",
    "with open('data/frontier_systems_by_window_percentile.json', 'w') as f:\n",
    "    json.dump(frontier_systems_by_percentile, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p8pzY7skfxkV",
    "outputId": "195c3ec6-c885-4008-ba36-abb60e5dd645",
    "ExecuteTime": {
     "end_time": "2024-05-14T22:01:56.807884900Z",
     "start_time": "2024-05-14T22:01:56.766409300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95 to 100\n",
      "15 systems\n",
      "Total systems above 95th percentile: 15\n",
      "Gemini Ultra\n",
      "GPT-4\n",
      "Minerva (540B)\n",
      "PaLM (540B)\n",
      "Megatron-Turing NLG 530B\n",
      "GPT-3 175B (davinci)\n",
      "Meena\n",
      "OpenAI Five\n",
      "Megatron-BERT\n",
      "FTW\n",
      "ResNeXt-101 32x48d\n",
      "AlphaZero\n",
      "AlphaGo Zero\n",
      "AlphaGo Master\n",
      "GNMT\n",
      "\n",
      "90 to 95\n",
      "14 systems\n",
      "Total systems above 90th percentile: 29\n",
      "Inflection-2\n",
      "Claude 2\n",
      "PaLM 2\n",
      "GPT-3.5 (text-davinci-003)\n",
      "U-PaLM (540B)\n",
      "Flan-PaLM 540B\n",
      "Chinchilla\n",
      "ERNIE 3.0 Titan\n",
      "Gopher (280B)\n",
      "Switch\n",
      "mT5-XXL\n",
      "AlphaStar\n",
      "T5-11B\n",
      "NASv3 (CIFAR-10)\n",
      "\n",
      "85 to 90\n",
      "14 systems\n",
      "Total systems above 85th percentile: 43\n",
      "Grok-1\n",
      "Falcon-180B\n",
      "BLOOM-176B\n",
      "OPT-175B\n",
      "LaMDA\n",
      "GLaM\n",
      "Yuan 1.0\n",
      "Turing-NLG\n",
      "OpenAI Five Rerun\n",
      "Megatron-LM (8.3B)\n",
      "AmoebaNet-A (F=448)\n",
      "JFT\n",
      "AlphaGo Lee\n",
      "AlphaGo Fan\n",
      "\n",
      "80 to 85\n",
      "17 systems\n",
      "Total systems above 80th percentile: 60\n",
      "Qwen-72B\n",
      "ChatGLM3\n",
      "Llama 2-70B\n",
      "BlenderBot 3\n",
      "Parti\n",
      "Flamingo\n",
      "ST-MoE\n",
      "AlphaCode\n",
      "HyperCLOVA\n",
      "ByT5-XXL\n",
      "DALL-E\n",
      "iGPT-XL\n",
      "XLNet\n",
      "GPT-2 (1.5B)\n",
      "BigGAN-deep 512x512\n",
      "IMPALA\n",
      "OpenAI TI7 DOTA 1v1\n",
      "\n",
      "75 to 80\n",
      "16 systems\n",
      "Total systems above 75th percentile: 76\n",
      "Yi-34B\n",
      "xTrimoPGLM -100B\n",
      "PanGu-Σ\n",
      "LLaMA-65B\n",
      "ViT-22B\n",
      "GLM-130B\n",
      "GPT-NeoX-20B\n",
      "GOAT\n",
      "ProtT5-XXL\n",
      "Meta Pseudo Labels\n",
      "ContextNet + Noisy Student\n",
      "RoBERTa Large\n",
      "BERT-Large\n",
      "Libratus\n",
      "Xception\n",
      "DeepSpeech2 (English)\n",
      "\n",
      "70 to 75\n",
      "13 systems\n",
      "Total systems above 70th percentile: 89\n",
      "Galactica\n",
      "AlexaTM 20B\n",
      "UL2\n",
      "FLAN 137B\n",
      "CoAtNet\n",
      "GShard (dense)\n",
      "iGPT-L\n",
      "MnasNet-A3\n",
      "MnasNet-A1 + SSDLite\n",
      "Mesh-TensorFlow Transformer 4.9B (language modelling)\n",
      "Big Transformer for Back-Translation\n",
      "MoE\n",
      "PolyNet\n",
      "\n",
      "65 to 70\n",
      "18 systems\n",
      "Total systems above 65th percentile: 107\n",
      "FunSearch\n",
      "Skywork-13B\n",
      "BloombergGPT\n",
      "Falcon-40B\n",
      "CoCa\n",
      "Stable Diffusion (LDM-KL-8-G)\n",
      "Florence\n",
      "ProtT5-XXL-BFD\n",
      "ProtBERT-BFD\n",
      "PLUG\n",
      "Conformer + Wav2vec 2.0 + Noisy Student\n",
      "ELECTRA\n",
      "ALBERT-xxlarge\n",
      "BERT-Large-CAS (PTB+WT2+WT103)\n",
      "Transformer (Adaptive Input Embeddings)\n",
      "YOLOv3\n",
      "ConvS2S (ensemble of 8 models)\n",
      "ResNet-152 (ImageNet)\n",
      "\n",
      "60 to 65\n",
      "12 systems\n",
      "Total systems above 60th percentile: 119\n",
      "Nemotron-3-8B\n",
      "PaLI\n",
      "ESM2-15B\n",
      "XGLM-7.5B\n",
      "BASIC-L\n",
      "ERNIE 3.0\n",
      "ALIGN\n",
      "CogView\n",
      "CLIP (ViT L/14@336px)\n",
      "T5-3B\n",
      "Mesh-TensorFlow Transformer 2.9B (translation)\n",
      "PNASNet-5\n",
      "\n",
      "55 to 60\n",
      "17 systems\n",
      "Total systems above 55th percentile: 136\n",
      "FinGPT-13B\n",
      "Llama 2-7B\n",
      "WizardCoder-15.5B\n",
      "StarCoder\n",
      "Taiyi-Stable Diffusion\n",
      "Whisper\n",
      "Student of Games\n",
      "T0-XXL\n",
      "ViT-Huge/14\n",
      "Once for All\n",
      "Noisy Student (L2)\n",
      "CamemBERT\n",
      "SciBERT\n",
      "Population-based DRL\n",
      "QT-Opt\n",
      "LSTM (Hebbian, Cache, MbPA)\n",
      "DeepStack\n",
      "\n",
      "50 to 55\n",
      "14 systems\n",
      "Total systems above 50th percentile: 150\n",
      "LLaMA-7B\n",
      "Flan-T5 11B\n",
      "RETRO-7B\n",
      "XGLM\n",
      "DeBERTa\n",
      "M6-T\n",
      "MSA Transformer\n",
      "AlphaFold 2\n",
      "GBERT-Large\n",
      "DD-PPO\n",
      "ProxylessNAS\n",
      "GPT\n",
      "Transformer\n",
      "BIDAF\n",
      "\n",
      "45 to 50\n",
      "14 systems\n",
      "Total systems above 45th percentile: 164\n",
      "Jais\n",
      "Pangu-Weather\n",
      "WizardLM-7B\n",
      "OmegaPLM\n",
      "NLLB\n",
      "ProGen2-xlarge\n",
      "Imagen\n",
      "HuBERT\n",
      "CPM-Large\n",
      "wave2vec 2.0 LARGE\n",
      "German ELECTRA Large\n",
      "Sandwich Transformer\n",
      "Transformer + Simple Recurrent Unit\n",
      "RetinaNet-R101\n",
      "\n",
      "40 to 45\n",
      "14 systems\n",
      "Total systems above 40th percentile: 178\n",
      "CogAgent\n",
      "GraphCast\n",
      "CogVLM\n",
      "Tranception\n",
      "SEER\n",
      "ViT-G/14\n",
      "ESM1-670M (UR50/D)\n",
      "DETR\n",
      "AlphaFold\n",
      "MuZero\n",
      "KataGo\n",
      "Transformer-XL (257M)\n",
      "GL-LWGC-AWD-MoS-LSTM + dynamic evaluation (WT2)\n",
      "Part-of-sentence tagging model\n",
      "\n",
      "35 to 40\n",
      "12 systems\n",
      "Total systems above 35th percentile: 190\n",
      "PeptideBERT\n",
      "Segment Anything Model\n",
      "VideoMAE V2\n",
      "Nucleotide Transformer\n",
      "Gato\n",
      "ViT-G (model soup)\n",
      "NÜWA\n",
      "AlphaFold-Multimer\n",
      "Feedback Transformer\n",
      "ResNet-152 + ObjectNet\n",
      "R-FCN\n",
      "Named Entity Recognition model\n",
      "\n",
      "30 to 35\n",
      "19 systems\n",
      "Total systems above 30th percentile: 209\n",
      "DINOv2\n",
      "AudioGen\n",
      "Ankh_large\n",
      "EVA-01\n",
      "CodeT5-large\n",
      "CodeT5-base\n",
      "EMDR\n",
      "ADM\n",
      "LUKE\n",
      "ERNIE-GEN (large)\n",
      "ATLAS\n",
      "TaLK Convolution\n",
      "DistilBERT\n",
      "FAIRSEQ Adaptive Inputs\n",
      "TrellisNet\n",
      "(ensemble): AWD-LSTM-DOC (fin) × 5 (WT2)\n",
      "QRNN\n",
      "AWD-LSTM-MoS + dynamic evaluation (WT2, 2017)\n",
      "AWD-LSTM - 3-layer LSTM (tied) + continuous cache pointer (WT2)\n",
      "\n",
      "25 to 30\n",
      "17 systems\n",
      "Total systems above 25th percentile: 226\n",
      "Incoder-6.7B\n",
      "Ankh_base\n",
      "GenSLM\n",
      "PolyCoder\n",
      "Swin Transformer V2\n",
      "S4\n",
      "Denoising Diffusion Probabilistic Models (LSUN Bedroom)\n",
      "ViT + DINO\n",
      "KEPLER\n",
      "AlphaX-1\n",
      "Tensorized Transformer (257M)\n",
      "Transformer-XL Large + Phrase Induction\n",
      "Hanabi 4 player\n",
      "Decoupled weight decay regularization\n",
      "Big-Little Net (speech)\n",
      "ULM-FiT\n",
      "VD-LSTM+REAL Large\n",
      "\n",
      "20 to 25\n",
      "13 systems\n",
      "Total systems above 20th percentile: 239\n",
      "DiT-XL/2\n",
      "Flan T5-XXL + BLIP-2\n",
      "BLIP-2 (Q-Former)\n",
      "Hybrid H3-2.7B\n",
      "Masked Autoencoders\n",
      "ConSERT\n",
      "Base LM + kNN LM + Continuous Cache\n",
      "DLRM-2020\n",
      "Cross-lingual alignment\n",
      "Dropout-LSTM+Noise(Bernoulli) (WT2)\n",
      "4 layer QRNN (h=2500)\n",
      "Fraternal dropout + AWD-LSTM 3-layer (WT2)\n",
      "Zoneout + Variational LSTM (WT2)\n",
      "\n",
      "15 to 20\n",
      "15 systems\n",
      "Total systems above 15th percentile: 254\n",
      "ONE-PEACE\n",
      "DDPM-IP (CelebA)\n",
      "Discriminator Guidance\n",
      "AR-LDM\n",
      "DNABERT\n",
      "Adaptive Input Transformer + RD\n",
      "EfficientNetV2\n",
      "ERNIE-Doc (247M)\n",
      "DeLight\n",
      "Transformer-XL DeFINE (141M)\n",
      "aLSTM(depth-2)+RecurrentPolicy (WT2)\n",
      "ENAS\n",
      "Neural Architecture Search with base 8 and shared embeddings\n",
      "Pointer Sentinel-LSTM (medium)\n",
      "Variational (untied weights, MC) LSTM (Large)\n",
      "\n",
      "10 to 15\n",
      "12 systems\n",
      "Total systems above 10th percentile: 266\n",
      "LLaVA 1.5\n",
      "Fusion in Encoder\n",
      "DiffDock\n",
      "BEIT-3\n",
      "BERT-RBP\n",
      "ProteinBERT\n",
      "Transformer local-attention (NesT-B)\n",
      "SRU++ Large\n",
      "ProBERTa\n",
      "NAS+ESS (156M)\n",
      "DARTS\n",
      "EI-REHN-1000D\n",
      "\n",
      "5 to 10\n",
      "16 systems\n",
      "Total systems above 5th percentile: 282\n",
      "MultiBand Diffusion\n",
      "LLaVA\n",
      "CaLM\n",
      "Sparse all-MLP\n",
      "Segatron-XL large, M=384 + HCP\n",
      "Detic\n",
      "MedBERT\n",
      "Tensor-Transformer(1core)+PN (WT103)\n",
      "TransformerXL + spectrum control\n",
      "MMLSTM\n",
      "UDSMProt\n",
      "AWD-LSTM + MoS + Partial Shuffled\n",
      "AWD-LSTM-DRILL + dynamic evaluation† (WT2)\n",
      "AWD-LSTM+WT+Cache+IOG (WT2)\n",
      "ISS\n",
      "VD-RHN\n",
      "\n",
      "0 to 5\n",
      "13 systems\n",
      "Total systems above 0th percentile: 295\n",
      "CODEFUSION (Python)\n",
      "HyenaDNA\n",
      "VALL-E\n",
      "Mogrifier RLSTM (WT2)\n",
      "DITTO\n",
      "base LM+GNN+kNN\n",
      "PermuteFormer\n",
      "CT-MoS (WT2)\n",
      "DensePhrases\n",
      "Pluribus\n",
      "Multi-cell LSTM\n",
      "Fine-tuned-AWD-LSTM-DOC(fin)\n",
      "2-layer-LSTM+Deep-Gradient-Compression\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_num_systems = 0\n",
    "for percentile, systems in frontier_systems_by_percentile.items():\n",
    "  total_num_systems += len(systems)\n",
    "  print(percentile, 'to', percentile + percentile_interval)\n",
    "  print(len(systems), \"systems\")\n",
    "  print(f'Total systems above {percentile}th percentile: {total_num_systems}')\n",
    "  for system in systems[::-1]:\n",
    "    print(system)\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Backward window percentile"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95\n",
      "90\n",
      "85\n",
      "80\n",
      "75\n",
      "70\n",
      "65\n",
      "60\n",
      "55\n",
      "50\n",
      "45\n",
      "40\n",
      "35\n",
      "30\n",
      "25\n",
      "20\n",
      "15\n",
      "10\n",
      "5\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "frontier_systems_by_backward_percentile = {}\n",
    "percentile_interval = 5\n",
    "for percentile in range(95, -5, -percentile_interval):\n",
    "    print(percentile)\n",
    "    percentile_compute_low = np.zeros(len(pcd_df))\n",
    "    percentile_compute_high = np.zeros(len(pcd_df))\n",
    "    # Iterate through each row and calculate the 2-year moving average for each date\n",
    "    for i, (index, row) in enumerate(pcd_df.iterrows()):\n",
    "        # Define the 1-year window\n",
    "        start_date = row['Publication date'] - pd.DateOffset(years=outlier_window_size/2)\n",
    "        end_date = row['Publication date']\n",
    "\n",
    "        # Filter the DataFrame for this window\n",
    "        window_df = pcd_df[(pcd_df['Publication date'] >= start_date) & (pcd_df['Publication date'] <= end_date)]\n",
    "\n",
    "        percentile_compute_low[i] = np.percentile(window_df['Training compute (FLOP)'], percentile)\n",
    "        percentile_compute_high[i] = np.percentile(window_df['Training compute (FLOP)'], percentile + percentile_interval)\n",
    "\n",
    "    frontier_systems_flag = pcd_df['Training compute (FLOP)'] > np.array(percentile_compute_low)\n",
    "    extra_frontier_systems_flag = pcd_df['Training compute (FLOP)'] <= np.array(percentile_compute_high)\n",
    "\n",
    "    # raise Exception(\"Edit the following line if you want to consider models released after 2023-12-31.\")\n",
    "    extra_frontier_systems = pcd_df['System'][frontier_systems_flag & extra_frontier_systems_flag & (pcd_df['Publication date'] > pd.to_datetime('2015-09-30')) & (pcd_df['Publication date'] < pd.to_datetime('2024-01-01'))].values\n",
    "\n",
    "    frontier_systems_by_backward_percentile[percentile] = list(extra_frontier_systems)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-14T22:02:03.518026900Z",
     "start_time": "2024-05-14T22:01:56.800396700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "{95: ['AlphaGo Fan',\n  'AlphaGo Lee',\n  'GNMT',\n  'AlphaGo Master',\n  'AlphaGo Zero',\n  'Megatron-BERT',\n  'Megatron-LM (8.3B)',\n  'T5-11B',\n  'AlphaStar',\n  'OpenAI Five',\n  'Meena',\n  'GPT-3 175B (davinci)',\n  'HyperCLOVA',\n  'Megatron-Turing NLG 530B',\n  'Yuan 1.0',\n  'Gopher (280B)',\n  'GLaM',\n  'ERNIE 3.0 Titan',\n  'PaLM (540B)',\n  'Minerva (540B)',\n  'U-PaLM (540B)',\n  'GPT-3.5 (text-davinci-003)',\n  'GPT-4',\n  'PaLM 2',\n  'Claude 2',\n  'Inflection-2',\n  'Gemini Ultra'],\n 90: ['NASv3 (CIFAR-10)',\n  'ResNeXt-101 32x48d',\n  'GPT-2 (1.5B)',\n  'XLNet',\n  'RoBERTa Large',\n  'mT5-XXL',\n  'DALL-E',\n  'Switch',\n  'Meta Pseudo Labels',\n  'ProtT5-XXL',\n  'ByT5-XXL',\n  'GOAT',\n  'LaMDA',\n  'Chinchilla',\n  'OPT-175B',\n  'Flan-PaLM 540B',\n  'xTrimoPGLM -100B',\n  'Falcon-180B',\n  'Grok-1'],\n 85: ['DeepSpeech2 (English)',\n  'AlphaZero',\n  'FTW',\n  'BigGAN-deep 512x512',\n  'BERT-Large-CAS (PTB+WT2+WT103)',\n  'MnasNet-A1 + SSDLite',\n  'MnasNet-A3',\n  'OpenAI Five Rerun',\n  'Turing-NLG',\n  'CoAtNet',\n  'FLAN 137B',\n  'AlphaCode',\n  'GPT-NeoX-20B',\n  'ST-MoE',\n  'Parti',\n  'BlenderBot 3',\n  'BLOOM-176B',\n  'LLaMA-65B',\n  'Llama 2-70B',\n  'ChatGLM3',\n  'Yi-34B',\n  'Qwen-72B'],\n 80: ['Xception',\n  'JFT',\n  'ContextNet + Noisy Student',\n  'iGPT-XL',\n  'CLIP (ViT L/14@336px)',\n  'PLUG',\n  'ProtBERT-BFD',\n  'Florence',\n  'Flamingo',\n  'GLM-130B',\n  'ViT-22B',\n  'PanGu-Σ'],\n 75: ['PolyNet',\n  'OpenAI TI7 DOTA 1v1',\n  'AmoebaNet-A (F=448)',\n  'Big Transformer for Back-Translation',\n  'BERT-Large',\n  'Mesh-TensorFlow Transformer 4.9B (language modelling)',\n  'SciBERT',\n  'GShard (dense)',\n  'ProtT5-XXL-BFD',\n  'BASIC-L',\n  'XGLM-7.5B',\n  'UL2'],\n 70: ['Libratus',\n  'IMPALA',\n  'Transformer (Adaptive Input Embeddings)',\n  'T5-3B',\n  'Noisy Student (L2)',\n  'ALBERT-xxlarge',\n  'ELECTRA',\n  'iGPT-L',\n  'Conformer + Wav2vec 2.0 + Noisy Student',\n  'MSA Transformer',\n  'M6-T',\n  'CogView',\n  'ALIGN',\n  'ERNIE 3.0',\n  'Stable Diffusion (LDM-KL-8-G)',\n  'AlexaTM 20B',\n  'Galactica',\n  'Falcon-40B',\n  'BloombergGPT',\n  'StarCoder',\n  'Skywork-13B',\n  'FunSearch'],\n 65: ['ResNet-152 (ImageNet)',\n  'MoE',\n  'YOLOv3',\n  'Mesh-TensorFlow Transformer 2.9B (translation)',\n  'CamemBERT',\n  'Once for All',\n  'ViT-Huge/14',\n  'Student of Games',\n  'CoCa',\n  'ESM2-15B',\n  'WizardCoder-15.5B',\n  'FinGPT-13B',\n  'Nemotron-3-8B'],\n 60: ['BIDAF',\n  'PNASNet-5',\n  'LSTM (Hebbian, Cache, MbPA)',\n  'QT-Opt',\n  'Population-based DRL',\n  'ProxylessNAS',\n  'Sandwich Transformer',\n  'DD-PPO',\n  'GBERT-Large',\n  'AlphaFold 2',\n  'DeBERTa',\n  'HuBERT',\n  'T0-XXL',\n  'Llama 2-7B'],\n 55: ['DeepStack',\n  'GPT',\n  'German ELECTRA Large',\n  'wave2vec 2.0 LARGE',\n  'SEER',\n  'XGLM',\n  'PaLI',\n  'Taiyi-Stable Diffusion',\n  'LLaMA-7B',\n  'WizardLM-7B',\n  'Pangu-Weather'],\n 50: ['KataGo',\n  'ResNet-152 + ObjectNet',\n  'DistilBERT',\n  'MuZero',\n  'AlphaFold',\n  'DETR',\n  'CPM-Large',\n  'ViT-G/14',\n  'AlphaFold-Multimer',\n  'NÜWA',\n  'RETRO-7B',\n  'Whisper',\n  'Jais',\n  'GraphCast'],\n 45: ['Transformer',\n  'ConvS2S (ensemble of 8 models)',\n  '(ensemble): AWD-LSTM-DOC (fin) × 5 (WT2)',\n  'Transformer + Simple Recurrent Unit',\n  'Transformer-XL (257M)',\n  'AlphaX-1',\n  'ESM1-670M (UR50/D)',\n  'Imagen',\n  'Tranception',\n  'NLLB',\n  'Flan-T5 11B',\n  'CogVLM',\n  'CogAgent'],\n 40: ['Part-of-sentence tagging model',\n  'QRNN',\n  'Big-Little Net (speech)',\n  'TrellisNet',\n  'Decoupled weight decay regularization',\n  'Hanabi 4 player',\n  'TaLK Convolution',\n  'Feedback Transformer',\n  'ERNIE-GEN (large)',\n  'LUKE',\n  'CodeT5-base',\n  'Gato',\n  'ProGen2-xlarge',\n  'OmegaPLM',\n  'Nucleotide Transformer',\n  'AudioGen',\n  'VideoMAE V2',\n  'Segment Anything Model',\n  'DINOv2',\n  'PeptideBERT'],\n 35: ['VD-LSTM+REAL Large',\n  'FAIRSEQ Adaptive Inputs',\n  'Transformer-XL Large + Phrase Induction',\n  'Tensorized Transformer (257M)',\n  'ATLAS',\n  'ViT + DINO',\n  'ADM',\n  'EMDR',\n  'S4',\n  'Swin Transformer V2',\n  'ViT-G (model soup)',\n  'Ankh_large'],\n 30: ['Named Entity Recognition model',\n  'R-FCN',\n  'RetinaNet-R101',\n  'GL-LWGC-AWD-MoS-LSTM + dynamic evaluation (WT2)',\n  'DLRM-2020',\n  'Base LM + kNN LM + Continuous Cache',\n  'KEPLER',\n  'ConSERT',\n  'Denoising Diffusion Probabilistic Models (LSUN Bedroom)',\n  'Masked Autoencoders',\n  'Incoder-6.7B'],\n 25: ['Zoneout + Variational LSTM (WT2)',\n  'AWD-LSTM - 3-layer LSTM (tied) + continuous cache pointer (WT2)',\n  'AWD-LSTM-MoS + dynamic evaluation (WT2, 2017)',\n  'ULM-FiT',\n  '4 layer QRNN (h=2500)',\n  'Cross-lingual alignment',\n  'Transformer-XL DeFINE (141M)',\n  'DeLight',\n  'PolyCoder',\n  'CodeT5-large',\n  'EVA-01',\n  'Hybrid H3-2.7B',\n  'Ankh_base',\n  'BLIP-2 (Q-Former)',\n  'Flan T5-XXL + BLIP-2'],\n 20: ['Neural Architecture Search with base 8 and shared embeddings',\n  'Fraternal dropout + AWD-LSTM 3-layer (WT2)',\n  'Dropout-LSTM+Noise(Bernoulli) (WT2)',\n  'ERNIE-Doc (247M)',\n  'SRU++ Large',\n  'EfficientNetV2',\n  'Adaptive Input Transformer + RD',\n  'DNABERT',\n  'GenSLM',\n  'DDPM-IP (CelebA)',\n  'DiT-XL/2'],\n 15: ['Variational (untied weights, MC) LSTM (Large)',\n  'Pointer Sentinel-LSTM (medium)',\n  'ENAS',\n  'aLSTM(depth-2)+RecurrentPolicy (WT2)',\n  'AWD-LSTM-DRILL + dynamic evaluation† (WT2)',\n  'UDSMProt',\n  'NAS+ESS (156M)',\n  'ProBERTa',\n  'Transformer local-attention (NesT-B)',\n  'BERT-RBP',\n  'AR-LDM',\n  'Discriminator Guidance'],\n 10: ['VD-RHN',\n  'EI-REHN-1000D',\n  'DARTS',\n  'AWD-LSTM + MoS + Partial Shuffled',\n  'MMLSTM',\n  'Tensor-Transformer(1core)+PN (WT103)',\n  'ProteinBERT',\n  'Sparse all-MLP',\n  'BEIT-3',\n  'DiffDock',\n  'Fusion in Encoder',\n  'ONE-PEACE',\n  'LLaVA 1.5'],\n 5: ['Fine-tuned-AWD-LSTM-DOC(fin)',\n  'Multi-cell LSTM',\n  'Pluribus',\n  'TransformerXL + spectrum control',\n  'DensePhrases',\n  'MedBERT',\n  'PermuteFormer',\n  'base LM+GNN+kNN',\n  'Detic',\n  'Segatron-XL large, M=384 + HCP',\n  'CaLM',\n  'LLaVA',\n  'CODEFUSION (Python)',\n  'MultiBand Diffusion'],\n 0: ['ISS',\n  'AWD-LSTM+WT+Cache+IOG (WT2)',\n  'CT-MoS (WT2)',\n  'DITTO',\n  'VALL-E',\n  'HyenaDNA']}"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frontier_systems_by_backward_percentile"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-14T22:02:03.535693600Z",
     "start_time": "2024-05-14T22:02:03.518026900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "# Save to JSON\n",
    "with open('data/frontier_systems_by_backward_window_percentile.json', 'w') as f:\n",
    "    json.dump(frontier_systems_by_backward_percentile, f, indent=4)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-14T22:02:03.593769600Z",
     "start_time": "2024-05-14T22:02:03.543707200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95 to 100\n",
      "27 systems\n",
      "Total systems above 95th percentile: 27\n",
      "Gemini Ultra\n",
      "Inflection-2\n",
      "Claude 2\n",
      "PaLM 2\n",
      "GPT-4\n",
      "GPT-3.5 (text-davinci-003)\n",
      "U-PaLM (540B)\n",
      "Minerva (540B)\n",
      "PaLM (540B)\n",
      "ERNIE 3.0 Titan\n",
      "GLaM\n",
      "Gopher (280B)\n",
      "Yuan 1.0\n",
      "Megatron-Turing NLG 530B\n",
      "HyperCLOVA\n",
      "GPT-3 175B (davinci)\n",
      "Meena\n",
      "OpenAI Five\n",
      "AlphaStar\n",
      "T5-11B\n",
      "Megatron-LM (8.3B)\n",
      "Megatron-BERT\n",
      "AlphaGo Zero\n",
      "AlphaGo Master\n",
      "GNMT\n",
      "AlphaGo Lee\n",
      "AlphaGo Fan\n",
      "\n",
      "90 to 95\n",
      "19 systems\n",
      "Total systems above 90th percentile: 46\n",
      "Grok-1\n",
      "Falcon-180B\n",
      "xTrimoPGLM -100B\n",
      "Flan-PaLM 540B\n",
      "OPT-175B\n",
      "Chinchilla\n",
      "LaMDA\n",
      "GOAT\n",
      "ByT5-XXL\n",
      "ProtT5-XXL\n",
      "Meta Pseudo Labels\n",
      "Switch\n",
      "DALL-E\n",
      "mT5-XXL\n",
      "RoBERTa Large\n",
      "XLNet\n",
      "GPT-2 (1.5B)\n",
      "ResNeXt-101 32x48d\n",
      "NASv3 (CIFAR-10)\n",
      "\n",
      "85 to 90\n",
      "22 systems\n",
      "Total systems above 85th percentile: 68\n",
      "Qwen-72B\n",
      "Yi-34B\n",
      "ChatGLM3\n",
      "Llama 2-70B\n",
      "LLaMA-65B\n",
      "BLOOM-176B\n",
      "BlenderBot 3\n",
      "Parti\n",
      "ST-MoE\n",
      "GPT-NeoX-20B\n",
      "AlphaCode\n",
      "FLAN 137B\n",
      "CoAtNet\n",
      "Turing-NLG\n",
      "OpenAI Five Rerun\n",
      "MnasNet-A3\n",
      "MnasNet-A1 + SSDLite\n",
      "BERT-Large-CAS (PTB+WT2+WT103)\n",
      "BigGAN-deep 512x512\n",
      "FTW\n",
      "AlphaZero\n",
      "DeepSpeech2 (English)\n",
      "\n",
      "80 to 85\n",
      "12 systems\n",
      "Total systems above 80th percentile: 80\n",
      "PanGu-Σ\n",
      "ViT-22B\n",
      "GLM-130B\n",
      "Flamingo\n",
      "Florence\n",
      "ProtBERT-BFD\n",
      "PLUG\n",
      "CLIP (ViT L/14@336px)\n",
      "iGPT-XL\n",
      "ContextNet + Noisy Student\n",
      "JFT\n",
      "Xception\n",
      "\n",
      "75 to 80\n",
      "12 systems\n",
      "Total systems above 75th percentile: 92\n",
      "UL2\n",
      "XGLM-7.5B\n",
      "BASIC-L\n",
      "ProtT5-XXL-BFD\n",
      "GShard (dense)\n",
      "SciBERT\n",
      "Mesh-TensorFlow Transformer 4.9B (language modelling)\n",
      "BERT-Large\n",
      "Big Transformer for Back-Translation\n",
      "AmoebaNet-A (F=448)\n",
      "OpenAI TI7 DOTA 1v1\n",
      "PolyNet\n",
      "\n",
      "70 to 75\n",
      "22 systems\n",
      "Total systems above 70th percentile: 114\n",
      "FunSearch\n",
      "Skywork-13B\n",
      "StarCoder\n",
      "BloombergGPT\n",
      "Falcon-40B\n",
      "Galactica\n",
      "AlexaTM 20B\n",
      "Stable Diffusion (LDM-KL-8-G)\n",
      "ERNIE 3.0\n",
      "ALIGN\n",
      "CogView\n",
      "M6-T\n",
      "MSA Transformer\n",
      "Conformer + Wav2vec 2.0 + Noisy Student\n",
      "iGPT-L\n",
      "ELECTRA\n",
      "ALBERT-xxlarge\n",
      "Noisy Student (L2)\n",
      "T5-3B\n",
      "Transformer (Adaptive Input Embeddings)\n",
      "IMPALA\n",
      "Libratus\n",
      "\n",
      "65 to 70\n",
      "13 systems\n",
      "Total systems above 65th percentile: 127\n",
      "Nemotron-3-8B\n",
      "FinGPT-13B\n",
      "WizardCoder-15.5B\n",
      "ESM2-15B\n",
      "CoCa\n",
      "Student of Games\n",
      "ViT-Huge/14\n",
      "Once for All\n",
      "CamemBERT\n",
      "Mesh-TensorFlow Transformer 2.9B (translation)\n",
      "YOLOv3\n",
      "MoE\n",
      "ResNet-152 (ImageNet)\n",
      "\n",
      "60 to 65\n",
      "14 systems\n",
      "Total systems above 60th percentile: 141\n",
      "Llama 2-7B\n",
      "T0-XXL\n",
      "HuBERT\n",
      "DeBERTa\n",
      "AlphaFold 2\n",
      "GBERT-Large\n",
      "DD-PPO\n",
      "Sandwich Transformer\n",
      "ProxylessNAS\n",
      "Population-based DRL\n",
      "QT-Opt\n",
      "LSTM (Hebbian, Cache, MbPA)\n",
      "PNASNet-5\n",
      "BIDAF\n",
      "\n",
      "55 to 60\n",
      "11 systems\n",
      "Total systems above 55th percentile: 152\n",
      "Pangu-Weather\n",
      "WizardLM-7B\n",
      "LLaMA-7B\n",
      "Taiyi-Stable Diffusion\n",
      "PaLI\n",
      "XGLM\n",
      "SEER\n",
      "wave2vec 2.0 LARGE\n",
      "German ELECTRA Large\n",
      "GPT\n",
      "DeepStack\n",
      "\n",
      "50 to 55\n",
      "14 systems\n",
      "Total systems above 50th percentile: 166\n",
      "GraphCast\n",
      "Jais\n",
      "Whisper\n",
      "RETRO-7B\n",
      "NÜWA\n",
      "AlphaFold-Multimer\n",
      "ViT-G/14\n",
      "CPM-Large\n",
      "DETR\n",
      "AlphaFold\n",
      "MuZero\n",
      "DistilBERT\n",
      "ResNet-152 + ObjectNet\n",
      "KataGo\n",
      "\n",
      "45 to 50\n",
      "13 systems\n",
      "Total systems above 45th percentile: 179\n",
      "CogAgent\n",
      "CogVLM\n",
      "Flan-T5 11B\n",
      "NLLB\n",
      "Tranception\n",
      "Imagen\n",
      "ESM1-670M (UR50/D)\n",
      "AlphaX-1\n",
      "Transformer-XL (257M)\n",
      "Transformer + Simple Recurrent Unit\n",
      "(ensemble): AWD-LSTM-DOC (fin) × 5 (WT2)\n",
      "ConvS2S (ensemble of 8 models)\n",
      "Transformer\n",
      "\n",
      "40 to 45\n",
      "20 systems\n",
      "Total systems above 40th percentile: 199\n",
      "PeptideBERT\n",
      "DINOv2\n",
      "Segment Anything Model\n",
      "VideoMAE V2\n",
      "AudioGen\n",
      "Nucleotide Transformer\n",
      "OmegaPLM\n",
      "ProGen2-xlarge\n",
      "Gato\n",
      "CodeT5-base\n",
      "LUKE\n",
      "ERNIE-GEN (large)\n",
      "Feedback Transformer\n",
      "TaLK Convolution\n",
      "Hanabi 4 player\n",
      "Decoupled weight decay regularization\n",
      "TrellisNet\n",
      "Big-Little Net (speech)\n",
      "QRNN\n",
      "Part-of-sentence tagging model\n",
      "\n",
      "35 to 40\n",
      "12 systems\n",
      "Total systems above 35th percentile: 211\n",
      "Ankh_large\n",
      "ViT-G (model soup)\n",
      "Swin Transformer V2\n",
      "S4\n",
      "EMDR\n",
      "ADM\n",
      "ViT + DINO\n",
      "ATLAS\n",
      "Tensorized Transformer (257M)\n",
      "Transformer-XL Large + Phrase Induction\n",
      "FAIRSEQ Adaptive Inputs\n",
      "VD-LSTM+REAL Large\n",
      "\n",
      "30 to 35\n",
      "11 systems\n",
      "Total systems above 30th percentile: 222\n",
      "Incoder-6.7B\n",
      "Masked Autoencoders\n",
      "Denoising Diffusion Probabilistic Models (LSUN Bedroom)\n",
      "ConSERT\n",
      "KEPLER\n",
      "Base LM + kNN LM + Continuous Cache\n",
      "DLRM-2020\n",
      "GL-LWGC-AWD-MoS-LSTM + dynamic evaluation (WT2)\n",
      "RetinaNet-R101\n",
      "R-FCN\n",
      "Named Entity Recognition model\n",
      "\n",
      "25 to 30\n",
      "15 systems\n",
      "Total systems above 25th percentile: 237\n",
      "Flan T5-XXL + BLIP-2\n",
      "BLIP-2 (Q-Former)\n",
      "Ankh_base\n",
      "Hybrid H3-2.7B\n",
      "EVA-01\n",
      "CodeT5-large\n",
      "PolyCoder\n",
      "DeLight\n",
      "Transformer-XL DeFINE (141M)\n",
      "Cross-lingual alignment\n",
      "4 layer QRNN (h=2500)\n",
      "ULM-FiT\n",
      "AWD-LSTM-MoS + dynamic evaluation (WT2, 2017)\n",
      "AWD-LSTM - 3-layer LSTM (tied) + continuous cache pointer (WT2)\n",
      "Zoneout + Variational LSTM (WT2)\n",
      "\n",
      "20 to 25\n",
      "11 systems\n",
      "Total systems above 20th percentile: 248\n",
      "DiT-XL/2\n",
      "DDPM-IP (CelebA)\n",
      "GenSLM\n",
      "DNABERT\n",
      "Adaptive Input Transformer + RD\n",
      "EfficientNetV2\n",
      "SRU++ Large\n",
      "ERNIE-Doc (247M)\n",
      "Dropout-LSTM+Noise(Bernoulli) (WT2)\n",
      "Fraternal dropout + AWD-LSTM 3-layer (WT2)\n",
      "Neural Architecture Search with base 8 and shared embeddings\n",
      "\n",
      "15 to 20\n",
      "12 systems\n",
      "Total systems above 15th percentile: 260\n",
      "Discriminator Guidance\n",
      "AR-LDM\n",
      "BERT-RBP\n",
      "Transformer local-attention (NesT-B)\n",
      "ProBERTa\n",
      "NAS+ESS (156M)\n",
      "UDSMProt\n",
      "AWD-LSTM-DRILL + dynamic evaluation† (WT2)\n",
      "aLSTM(depth-2)+RecurrentPolicy (WT2)\n",
      "ENAS\n",
      "Pointer Sentinel-LSTM (medium)\n",
      "Variational (untied weights, MC) LSTM (Large)\n",
      "\n",
      "10 to 15\n",
      "13 systems\n",
      "Total systems above 10th percentile: 273\n",
      "LLaVA 1.5\n",
      "ONE-PEACE\n",
      "Fusion in Encoder\n",
      "DiffDock\n",
      "BEIT-3\n",
      "Sparse all-MLP\n",
      "ProteinBERT\n",
      "Tensor-Transformer(1core)+PN (WT103)\n",
      "MMLSTM\n",
      "AWD-LSTM + MoS + Partial Shuffled\n",
      "DARTS\n",
      "EI-REHN-1000D\n",
      "VD-RHN\n",
      "\n",
      "5 to 10\n",
      "14 systems\n",
      "Total systems above 5th percentile: 287\n",
      "MultiBand Diffusion\n",
      "CODEFUSION (Python)\n",
      "LLaVA\n",
      "CaLM\n",
      "Segatron-XL large, M=384 + HCP\n",
      "Detic\n",
      "base LM+GNN+kNN\n",
      "PermuteFormer\n",
      "MedBERT\n",
      "DensePhrases\n",
      "TransformerXL + spectrum control\n",
      "Pluribus\n",
      "Multi-cell LSTM\n",
      "Fine-tuned-AWD-LSTM-DOC(fin)\n",
      "\n",
      "0 to 5\n",
      "6 systems\n",
      "Total systems above 0th percentile: 293\n",
      "HyenaDNA\n",
      "VALL-E\n",
      "DITTO\n",
      "CT-MoS (WT2)\n",
      "AWD-LSTM+WT+Cache+IOG (WT2)\n",
      "ISS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_num_systems = 0\n",
    "for percentile, systems in frontier_systems_by_backward_percentile.items():\n",
    "    total_num_systems += len(systems)\n",
    "    print(percentile, 'to', percentile + percentile_interval)\n",
    "    print(len(systems), \"systems\")\n",
    "    print(f'Total systems above {percentile}th percentile: {total_num_systems}')\n",
    "    for system in systems[::-1]:\n",
    "        print(system)\n",
    "    print()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-14T22:02:03.601803600Z",
     "start_time": "2024-05-14T22:02:03.568884300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Residuals from compute trend (last choice)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "frontier_systems_by_residual = {}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-15T01:19:18.478235400Z",
     "start_time": "2024-05-15T01:19:18.453467300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "# compute in log scale\n",
    "pcd_df['log compute'] = np.log10(pcd_df['Training compute (FLOP)'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-15T01:07:18.745559100Z",
     "start_time": "2024-05-15T01:07:18.662994200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "data": {
      "text/plain": "LinearRegression()"
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = pcd_df['Decimal year'].values.reshape(-1, 1)\n",
    "y = pcd_df['log compute']\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(x, y)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-15T01:07:20.510410100Z",
     "start_time": "2024-05-15T01:07:20.318749700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "# Model coefficients from regression on training compute in the Epoch database\n",
    "a, b = regressor.coef_[0], regressor.intercept_"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-15T01:07:23.407355500Z",
     "start_time": "2024-05-15T01:07:23.396637700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "# find residuals of all models\n",
    "pcd_df['residual'] = pcd_df['log compute'] - (a * pcd_df['Decimal year'] + b)\n",
    "\n",
    "# sort models by residuals\n",
    "pcd_df.sort_values(by='residual', axis=0, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-15T01:08:42.998356900Z",
     "start_time": "2024-05-15T01:08:42.948723300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "chunk_size = pcd_df.shape[0] / 20\n",
    "indices = [round(chunk_size*i) for i in range(21)]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-15T01:13:52.901847400Z",
     "start_time": "2024-05-15T01:13:52.877068300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    frontier_systems_by_residual[5*i] = pcd_df['System'].values.tolist()[indices[i]:indices[i+1]]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-15T01:21:02.961383800Z",
     "start_time": "2024-05-15T01:21:02.936519600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [
    {
     "data": {
      "text/plain": "{0: ['SPIDER2',\n  'LSTM+NeuralCache',\n  'Fine-tuned-AWD-LSTM-DOC(fin)',\n  'Multi-cell LSTM',\n  'Swift',\n  '2-layer-LSTM+Deep-Gradient-Compression',\n  'Image Classification with the Fisher Vector: Theory and Practice',\n  'AWD-LSTM+WT+Cache+IOG (WT2)',\n  'ISS',\n  'Mogrifier RLSTM (WT2)',\n  'Image generation',\n  'DARTS',\n  'VD-RHN',\n  'LSTM-Char-Large',\n  'EI-REHN-1000D',\n  'Pointer Sentinel-LSTM (medium)',\n  'Search-Proven Best LSTM',\n  'ENAS'],\n 5: ['System 11',\n  'Variational (untied weights, MC) LSTM (Large)',\n  'Neural Architecture Search with base 8 and shared embeddings',\n  '6-layer MLP (MNIST)',\n  'Pluribus',\n  'Innervator',\n  'DeiT',\n  'Fuzzy NN',\n  'DQN',\n  'Zoneout + Variational LSTM (WT2)',\n  'VD-LSTM+REAL Large',\n  'Feedforward NN',\n  'aLSTM(depth-2)+RecurrentPolicy (WT2)',\n  'RNN+weight noise+dynamic eval',\n  'CT-MoS (WT2)',\n  'BiLSTM for Speech',\n  'Dropout-LSTM+Noise(Bernoulli) (WT2)',\n  'TransformerXL + spectrum control'],\n 10: ['Fraternal dropout + AWD-LSTM 3-layer (WT2)',\n  'Back-propagation',\n  'HyenaDNA',\n  'AWD-LSTM + MoS + Partial Shuffled',\n  'CODEFUSION (Python)',\n  'R-FCN',\n  'Dropout (CIFAR)',\n  'AWD-LSTM-DRILL + dynamic evaluation† (WT2)',\n  'MCDNN (MNIST)',\n  '4 layer QRNN (h=2500)',\n  'Dropout (MNIST)',\n  'ULM-FiT',\n  'PermuteFormer',\n  'Named Entity Recognition model',\n  'GPU DBNs',\n  'DensePhrases',\n  'UDSMProt',\n  'Big-Little Net (speech)',\n  'QRNN'],\n 15: ['Tensor-Transformer(1core)+PN (WT103)',\n  'VALL-E',\n  'AWD-LSTM - 3-layer LSTM (tied) + continuous cache pointer (WT2)',\n  'Part-of-sentence tagging model',\n  'RNN 500/10 + RT09 LM (NIST RT05)',\n  '(ensemble): AWD-LSTM-DOC (fin) × 5 (WT2)',\n  'ADAM (CIFAR-10)',\n  'genCNN + dyn eval',\n  'AWD-LSTM-MoS + dynamic evaluation (WT2, 2017)',\n  'MultiBand Diffusion',\n  'DITTO',\n  'base LM+GNN+kNN',\n  'GL-LWGC-AWD-MoS-LSTM + dynamic evaluation (WT2)',\n  'NAS+ESS (156M)',\n  'SPN-4+KN5',\n  'Word2Vec (large)',\n  'MMLSTM',\n  'SmooCT'],\n 20: ['Fractional Max-Pooling',\n  'Large regularized LSTM',\n  'LeNet-5',\n  'MedBERT',\n  'CaLM',\n  'Cross-lingual alignment',\n  'Decoupled weight decay regularization',\n  'LLaVA 1.5',\n  'SRU++ Large',\n  'LLaVA',\n  'DLRM-2020',\n  'Mixture-of-Depths',\n  'Segatron-XL large, M=384 + HCP',\n  'Detic',\n  'TrellisNet',\n  'ProBERTa',\n  'Transformer-XL DeFINE (141M)',\n  'Tensorized Transformer (257M)'],\n 25: ['Hanabi 4 player',\n  'Base LM + kNN LM + Continuous Cache',\n  'Mitosis',\n  'ASE+ACE',\n  'AlphaX-1',\n  'Transformer local-attention (NesT-B)',\n  'Decision tree (classification)',\n  'RetinaNet-R101',\n  'Transformer-XL Large + Phrase Induction',\n  'DiffDock',\n  'FAIRSEQ Adaptive Inputs',\n  'BEIT-3',\n  'Sparse all-MLP',\n  'DistilBERT',\n  'ERNIE-Doc (247M)',\n  'Invariant image recognition',\n  'ProteinBERT',\n  'DeLight',\n  'Zip CNN'],\n 30: ['ONE-PEACE',\n  'Fusion in Encoder',\n  'GANs',\n  'Transformer-XL (257M)',\n  'ResNet-152 + ObjectNet',\n  'Transformer + Simple Recurrent Unit',\n  'TaLK Convolution',\n  'BIDAF',\n  'KN5 LM + RNN 400/10 (WSJ)',\n  'Adaptive Input Transformer + RD',\n  'ATLAS',\n  'Visualizing CNNs',\n  'BERT-RBP',\n  'Cognitron',\n  'Discriminator Guidance',\n  'GoogLeNet / InceptionV1',\n  'ALVINN',\n  'EfficientNetV2'],\n 35: ['Transformer',\n  'Feedback Transformer',\n  'KataGo',\n  'Dropout (ImageNet)',\n  'DDPM-IP (CelebA)',\n  'DNABERT',\n  'LSTM',\n  'GPT',\n  'MuZero',\n  'RNNsearch-50*',\n  'AlexNet',\n  'Neocognitron',\n  'NetTalk (dictionary)',\n  'NetTalk (transcription)',\n  'ProxylessNAS',\n  'KEPLER',\n  'DiT-XL/2',\n  'TransE'],\n 40: ['AR-LDM',\n  'LSTM (Hebbian, Cache, MbPA)',\n  'Unsupervised High-level Feature Learner',\n  'NPLM',\n  'ViT + DINO',\n  'Population-based DRL',\n  'QT-Opt',\n  'AlphaFold',\n  'DeepStack',\n  'LUKE',\n  'ConSERT',\n  'Hybrid H3-2.7B',\n  'SPPNet',\n  'Masked Autoencoders',\n  'ERNIE-GEN (large)',\n  'Denoising Diffusion Probabilistic Models (LSUN Bedroom)',\n  'Mesh-TensorFlow Transformer 2.9B (translation)',\n  'SciBERT'],\n 45: ['Flan T5-XXL + BLIP-2',\n  'BLIP-2 (Q-Former)',\n  'YOLOv3',\n  'Transformer (Adaptive Input Embeddings)',\n  'Sandwich Transformer',\n  'ResNet-152 (ImageNet)',\n  'S4',\n  'ADM',\n  'GenSLM',\n  'PNASNet-5',\n  'Big Transformer for Back-Translation',\n  'PolyCoder',\n  'ConvS2S (ensemble of 8 models)',\n  'DETR',\n  'ESM1-670M (UR50/D)',\n  'VGG16',\n  'Swin Transformer V2',\n  'Incoder-6.7B'],\n 50: ['Mesh-TensorFlow Transformer 4.9B (language modelling)',\n  'Ankh_base',\n  'DeepSpeech2 (English)',\n  'CodeT5-base',\n  'EMDR',\n  'PolyNet',\n  'CodeT5-large',\n  'EVA-01',\n  'IMPALA',\n  'MSRA (C, PReLU)',\n  'BERT-Large',\n  'MoE',\n  'PeptideBERT',\n  'DD-PPO',\n  'ViT-G (model soup)',\n  'BERT-Large-CAS (PTB+WT2+WT103)',\n  'German ELECTRA Large',\n  'DINOv2',\n  'Ankh_large'],\n 55: ['AudioGen',\n  'Segment Anything Model',\n  'CamemBERT',\n  'Noisy Student (L2)',\n  'CPM-Large',\n  'T5-3B',\n  'wave2vec 2.0 LARGE',\n  'VideoMAE V2',\n  'Gato',\n  'ADALINE',\n  'ViT-G/14',\n  'GBERT-Large',\n  'AlphaFold-Multimer',\n  'AmoebaNet-A (F=448)',\n  'NÜWA',\n  'Once for All',\n  'Tranception',\n  'SEER'],\n 60: ['Nucleotide Transformer',\n  'TD-Gammon',\n  'GraphCast',\n  'AlphaFold 2',\n  'CogVLM',\n  'Seq2Seq LSTM',\n  'HuBERT',\n  'CogAgent',\n  'MnasNet-A1 + SSDLite',\n  'MnasNet-A3',\n  'ALBERT-xxlarge',\n  'DeBERTa',\n  'ViT-Huge/14',\n  'M6-T',\n  'OmegaPLM',\n  'ProGen2-xlarge',\n  'MSA Transformer',\n  'ELECTRA'],\n 65: ['Jais',\n  'OpenAI TI7 DOTA 1v1',\n  'Imagen',\n  'NLLB',\n  'Xception',\n  'Theseus',\n  'Pangu-Weather',\n  'Libratus',\n  'BigGAN-deep 512x512',\n  'Heuristic Reinforcement Learning',\n  'RETRO-7B',\n  'JFT',\n  'Conformer + Wav2vec 2.0 + Noisy Student',\n  'WizardLM-7B',\n  'LLaMA-7B',\n  'Flan-T5 11B',\n  'ContextNet + Noisy Student',\n  'CLIP (ViT L/14@336px)',\n  'T0-XXL'],\n 70: ['RoBERTa Large',\n  'XGLM',\n  'AlphaGo Fan',\n  'XLNet',\n  'iGPT-L',\n  'Whisper',\n  'GPT-2 (1.5B)',\n  'Taiyi-Stable Diffusion',\n  'Llama 2-7B',\n  'ERNIE 3.0',\n  'PaLI',\n  'StarCoder',\n  'GShard (dense)',\n  'ALIGN',\n  'Student of Games',\n  'CogView',\n  'Megatron-LM (8.3B)',\n  'Stable Diffusion (LDM-KL-8-G)'],\n 75: ['WizardCoder-15.5B',\n  'XGLM-7.5B',\n  'BASIC-L',\n  'FinGPT-13B',\n  'OpenAI Five Rerun',\n  'ESM2-15B',\n  'Florence',\n  'Nemotron-3-8B',\n  'CoCa',\n  'Turing-NLG',\n  'ProtT5-XXL-BFD',\n  'PLUG',\n  'ProtBERT-BFD',\n  'FLAN 137B',\n  'CoAtNet',\n  'NASv3 (CIFAR-10)',\n  'FTW',\n  'Skywork-13B'],\n 80: ['Meta Pseudo Labels',\n  'GPT-NeoX-20B',\n  'DALL-E',\n  'iGPT-XL',\n  'UL2',\n  'AlphaGo Lee',\n  'ResNeXt-101 32x48d',\n  'MM1-30B',\n  'BloombergGPT',\n  'GOAT',\n  'FunSearch',\n  'Falcon-40B',\n  'ProtT5-XXL',\n  'ByT5-XXL',\n  'AlexaTM 20B',\n  'T5-11B',\n  'AlphaCode',\n  'Switch'],\n 85: ['mT5-XXL',\n  'HyperCLOVA',\n  'Yi-34B',\n  'Galactica',\n  'ViT-22B',\n  'Linear Decision Functions',\n  'PanGu-Σ',\n  'Flamingo',\n  'xTrimoPGLM -100B',\n  'GNMT',\n  'AlphaStar',\n  'GLM-130B',\n  'LLaMA-65B',\n  'OpenAI Five',\n  'ST-MoE',\n  'Megatron-BERT',\n  'BlenderBot 3',\n  'Parti'],\n 90: ['Llama 2-70B',\n  'ChatGLM3',\n  'BLOOM-176B',\n  'LaMDA',\n  'OPT-175B',\n  'Qwen-72B',\n  'GLaM',\n  'Meena',\n  'Yuan 1.0',\n  'Chinchilla',\n  'AlphaZero',\n  'Gopher (280B)',\n  'Grok-1',\n  'GPT-3 175B (davinci)',\n  'ERNIE 3.0 Titan',\n  'Falcon-180B',\n  'Megatron-Turing NLG 530B',\n  'Llama 3-70B',\n  'Claude 2'],\n 95: ['GPT-3.5 (text-davinci-003)',\n  'Flan-PaLM 540B',\n  'U-PaLM (540B)',\n  'Minerva (540B)',\n  'Print Recognition Logic',\n  'PaLM (540B)',\n  'Inflection-2.5',\n  'Inflection-2',\n  'MegaScale (Production)',\n  'PaLM 2',\n  'Perceptron Mark I',\n  'AlphaGo Master',\n  'AlphaGo Zero',\n  'GPT-4',\n  'Gemini Ultra',\n  'Samuel Neural Checkers',\n  'Perceptron (1960)',\n  'Pandemonium (morse)']}"
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frontier_systems_by_residual"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-15T01:21:04.635838100Z",
     "start_time": "2024-05-15T01:21:04.612285400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [],
   "source": [
    "# Save to JSON\n",
    "with open('data/frontier_systems_by_residual.json', 'w') as f:\n",
    "    json.dump(frontier_systems_by_residual, f, indent=4)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-15T01:21:25.939219500Z",
     "start_time": "2024-05-15T01:21:25.360031Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TllyX8IqSiG2"
   },
   "source": [
    "# Distance from compute record at the time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "CC8_j5AASl0y",
    "ExecuteTime": {
     "end_time": "2024-05-14T22:02:03.601803600Z",
     "start_time": "2024-05-14T22:02:03.579595400Z"
    }
   },
   "outputs": [],
   "source": [
    "ooms_from_frontier = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N395lGkYSoNU",
    "outputId": "ee6d6df7-3662-4dd8-ce41-b9490a33389a",
    "ExecuteTime": {
     "end_time": "2024-05-14T22:02:03.668286100Z",
     "start_time": "2024-05-14T22:02:03.601803600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([4.00000000e+01, 6.94894938e+05, 6.00000000e+08, 6.00000000e+08,\n       7.20000000e+08, 7.20000000e+08, 7.20000000e+08, 7.20000000e+08,\n       7.20000000e+08, 7.20000000e+08, 7.20000000e+08, 7.20000000e+08,\n       7.20000000e+08, 2.76640650e+10, 2.83280026e+10, 2.83280026e+10,\n       2.83280026e+10, 8.11870414e+10, 8.11870414e+10, 1.82321576e+13,\n       1.82321576e+13, 1.82321576e+13, 2.10080000e+13, 2.10080000e+13,\n       6.30000000e+13, 1.30389876e+15, 1.30389876e+15, 1.30389876e+15,\n       1.30389876e+15, 1.30389876e+15, 3.41463600e+15, 6.14400000e+16,\n       6.14400000e+16, 6.14400000e+16, 6.14400000e+16, 2.73196800e+17,\n       6.00000000e+17, 6.00000000e+17, 6.00000000e+17, 6.00000000e+17,\n       6.00000000e+17, 6.00000000e+17, 6.00000000e+17, 1.34092800e+18,\n       1.34092800e+18, 1.34092800e+18, 1.34092800e+18, 1.34092800e+18,\n       3.41107200e+18, 3.41107200e+18, 3.41107200e+18, 9.25344000e+18,\n       9.25344000e+18, 5.60000000e+19, 5.60000000e+19, 5.60000000e+19,\n       5.60000000e+19, 5.60000000e+19, 5.60000000e+19, 5.60000000e+19,\n       5.60000000e+19, 3.80000000e+20, 3.80000000e+20, 3.80000000e+20,\n       3.80000000e+20, 1.90000000e+21, 1.90000000e+21, 1.90000000e+21,\n       1.90000000e+21, 1.90000000e+21, 1.90000000e+21, 1.90000000e+21,\n       6.90000000e+21, 6.90000000e+21, 6.90000000e+21, 6.90000000e+21,\n       6.90000000e+21, 6.90000000e+21, 6.90000000e+21, 6.90000000e+21,\n       2.00010000e+23, 2.00010000e+23, 2.00010000e+23, 2.00010000e+23,\n       2.00010000e+23, 2.00010000e+23, 2.00010000e+23, 2.00010000e+23,\n       2.00010000e+23, 2.00010000e+23, 2.00010000e+23, 2.00010000e+23,\n       2.00010000e+23, 2.00010000e+23, 3.41000000e+23, 3.41000000e+23,\n       3.41000000e+23, 3.41000000e+23, 3.41000000e+23, 3.41000000e+23,\n       3.41000000e+23, 3.41000000e+23, 3.41000000e+23, 3.41000000e+23,\n       3.41000000e+23, 3.41000000e+23, 3.41000000e+23, 3.41000000e+23,\n       3.41000000e+23, 3.41000000e+23, 3.41000000e+23, 3.41000000e+23,\n       3.41000000e+23, 3.41000000e+23, 3.41000000e+23, 3.41000000e+23,\n       3.41000000e+23, 3.41000000e+23, 3.41000000e+23, 3.41000000e+23,\n       3.41000000e+23, 3.41000000e+23, 3.41000000e+23, 3.41000000e+23,\n       3.41000000e+23, 3.41000000e+23, 3.41000000e+23, 3.41000000e+23,\n       3.41000000e+23, 3.41000000e+23, 3.41000000e+23, 3.41000000e+23,\n       3.41000000e+23, 3.41000000e+23, 3.41000000e+23, 3.41000000e+23,\n       3.41000000e+23, 3.41000000e+23, 3.41000000e+23, 3.41000000e+23,\n       3.41000000e+23, 3.41000000e+23, 3.41000000e+23, 3.41000000e+23,\n       3.41000000e+23, 3.41000000e+23, 3.41000000e+23, 3.41000000e+23,\n       3.41000000e+23, 3.41000000e+23, 3.41000000e+23, 3.41000000e+23,\n       3.41000000e+23, 3.41000000e+23, 3.41000000e+23, 3.41000000e+23,\n       3.41000000e+23, 3.41000000e+23, 3.41000000e+23, 3.41000000e+23,\n       3.41000000e+23, 3.41000000e+23, 3.41000000e+23, 3.41000000e+23,\n       3.41000000e+23, 3.41000000e+23, 3.41000000e+23, 3.41000000e+23,\n       3.41000000e+23, 3.41000000e+23, 3.41000000e+23, 3.41000000e+23,\n       3.41000000e+23, 3.41000000e+23, 3.41000000e+23, 3.41000000e+23,\n       3.41000000e+23, 3.41000000e+23, 3.41000000e+23, 3.41000000e+23,\n       3.41000000e+23, 3.41000000e+23, 3.41000000e+23, 3.41000000e+23,\n       3.41000000e+23, 3.41000000e+23, 3.41000000e+23, 3.41000000e+23,\n       3.41000000e+23, 3.41000000e+23, 3.41000000e+23, 3.41000000e+23,\n       3.41000000e+23, 3.41000000e+23, 3.41000000e+23, 3.41000000e+23,\n       3.41000000e+23, 3.41000000e+23, 3.41000000e+23, 3.41000000e+23,\n       3.41000000e+23, 3.41000000e+23, 3.41000000e+23, 3.41000000e+23,\n       3.41000000e+23, 3.41000000e+23, 3.41000000e+23, 3.41000000e+23,\n       3.41000000e+23, 3.41000000e+23, 3.41000000e+23, 3.41000000e+23,\n       3.41000000e+23, 3.41000000e+23, 3.41000000e+23, 3.41000000e+23,\n       3.41000000e+23, 3.41000000e+23, 3.41000000e+23, 3.41000000e+23,\n       3.41000000e+23, 3.41000000e+23, 3.41000000e+23, 3.41000000e+23,\n       3.41000000e+23, 3.41000000e+23, 3.41000000e+23, 3.41000000e+23,\n       3.41000000e+23, 3.41000000e+23, 3.41000000e+23, 3.41000000e+23,\n       3.41000000e+23, 3.41000000e+23, 3.41000000e+23, 3.41000000e+23,\n       3.41000000e+23, 3.41000000e+23, 3.41000000e+23, 1.17000000e+24,\n       1.17000000e+24, 1.17000000e+24, 1.17000000e+24, 1.17000000e+24,\n       1.17000000e+24, 1.17000000e+24, 1.17000000e+24, 1.17000000e+24,\n       1.17000000e+24, 1.17000000e+24, 1.17000000e+24, 1.17000000e+24,\n       1.17000000e+24, 1.17000000e+24, 1.17000000e+24, 1.17000000e+24,\n       1.17000000e+24, 1.17000000e+24, 1.17000000e+24, 1.17000000e+24,\n       1.17000000e+24, 1.17000000e+24, 1.17000000e+24, 1.17000000e+24,\n       1.17000000e+24, 1.17000000e+24, 1.17000000e+24, 2.52720000e+24,\n       2.52720000e+24, 2.52720000e+24, 2.52720000e+24, 2.52720000e+24,\n       2.52720000e+24, 2.52720000e+24, 2.52720000e+24, 2.52720000e+24,\n       2.52720000e+24, 2.52720000e+24, 2.52720000e+24, 2.52720000e+24,\n       2.52720000e+24, 2.74150000e+24, 2.74150000e+24, 2.74150000e+24,\n       2.74150000e+24, 2.74150000e+24, 2.74150000e+24, 2.74150000e+24,\n       2.74150000e+24, 2.74150000e+24, 2.74150000e+24, 2.74150000e+24,\n       2.74150000e+24, 2.74150000e+24, 2.74150000e+24, 2.74150000e+24,\n       2.74150000e+24, 2.74150000e+24, 2.74150000e+24, 2.74150000e+24,\n       2.74150000e+24, 2.74150000e+24, 2.74150000e+24, 2.74150000e+24,\n       2.74150000e+24, 2.74150000e+24, 2.74150000e+24, 2.74150000e+24,\n       2.74150000e+24, 2.74150000e+24, 2.74150000e+24, 2.74150000e+24,\n       2.74150000e+24, 2.74150000e+24, 2.74150000e+24, 2.74150000e+24,\n       2.74150000e+24, 2.74150000e+24, 2.74150000e+24, 2.74150000e+24,\n       2.74150000e+24, 2.10000000e+25, 2.10000000e+25, 2.10000000e+25,\n       2.10000000e+25, 2.10000000e+25, 2.10000000e+25, 2.10000000e+25,\n       2.10000000e+25, 2.10000000e+25, 2.10000000e+25, 2.10000000e+25,\n       2.10000000e+25, 2.10000000e+25, 2.10000000e+25, 2.10000000e+25,\n       2.10000000e+25, 2.10000000e+25, 2.10000000e+25, 2.10000000e+25,\n       2.10000000e+25, 2.10000000e+25, 2.10000000e+25, 2.10000000e+25,\n       2.10000000e+25, 2.10000000e+25, 2.10000000e+25, 2.10000000e+25,\n       2.10000000e+25, 2.10000000e+25, 2.10000000e+25, 2.10000000e+25,\n       2.10000000e+25, 2.10000000e+25, 2.10000000e+25, 2.10000000e+25,\n       2.10000000e+25, 5.00000000e+25, 5.00000000e+25, 5.00000000e+25,\n       5.00000000e+25, 5.00000000e+25, 5.00000000e+25, 5.00000000e+25,\n       5.00000000e+25])"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_max = 0\n",
    "running_max = np.zeros(len(pcd_df))\n",
    "for i, compute in enumerate(pcd_df['Training compute (FLOP)']):\n",
    "  if compute > current_max:\n",
    "    running_max[i] = compute\n",
    "    current_max = compute\n",
    "  else:\n",
    "    running_max[i] = current_max\n",
    "running_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "SV1tpYgETmYT",
    "ExecuteTime": {
     "end_time": "2024-05-14T22:02:03.694861700Z",
     "start_time": "2024-05-14T22:02:03.618511Z"
    }
   },
   "outputs": [],
   "source": [
    "pcd_df['Frontier training compute (FLOP)'] = running_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "WdpWpDvvTqOs",
    "outputId": "331ce3ef-cc47-4393-dc7f-98148184fd4c",
    "ExecuteTime": {
     "end_time": "2024-05-14T22:02:03.694861700Z",
     "start_time": "2024-05-14T22:02:03.634551Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                      System  Frontier system\n1292                 Theseus            False\n1286       Perceptron Mark I            False\n1285     Pandemonium (morse)            False\n1284  Samuel Neural Checkers            False\n1282       Perceptron (1960)            False\n...                      ...              ...\n29    MegaScale (Production)             True\n20            Inflection-2.5             True\n19                   MM1-30B            False\n12         Mixture-of-Depths            False\n4                Llama 3-70B             True\n\n[365 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>System</th>\n      <th>Frontier system</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1292</th>\n      <td>Theseus</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1286</th>\n      <td>Perceptron Mark I</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1285</th>\n      <td>Pandemonium (morse)</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1284</th>\n      <td>Samuel Neural Checkers</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1282</th>\n      <td>Perceptron (1960)</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>MegaScale (Production)</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>Inflection-2.5</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>MM1-30B</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>Mixture-of-Depths</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Llama 3-70B</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n<p>365 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pcd_df['Frontier system'] = (pcd_df['Publication date'] > start_large_scale_era) & (np.log10(pcd_df['Frontier training compute (FLOP)']) - np.log10(pcd_df['Training compute (FLOP)']) <= ooms_from_frontier)\n",
    "pcd_df[['System', 'Frontier system']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "yrk0wu7LT6ZK",
    "outputId": "9d95a1fa-1ed8-4c4b-ae94-f9b42a8dddd8",
    "ExecuteTime": {
     "end_time": "2024-05-14T22:02:03.759028Z",
     "start_time": "2024-05-14T22:02:03.675504600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                      System                      Domain  \\\n1016   DeepSpeech2 (English)                      Speech   \n1014   ResNet-152 (ImageNet)                      Vision   \n1010             AlphaGo Lee                       Games   \n971                     GNMT                    Language   \n970                 Xception                      Vision   \n...                      ...                         ...   \n80                  Qwen-72B                    Language   \n72              Gemini Ultra  Multimodal,Language,Vision   \n29    MegaScale (Production)                    Language   \n20            Inflection-2.5                    Language   \n4                Llama 3-70B                    Language   \n\n                                Organization Publication date  \\\n1016  Baidu Research - Silicon Valley AI Lab       2015-12-08   \n1014                               Microsoft       2015-12-10   \n1010                                DeepMind       2016-01-27   \n971                                   Google       2016-09-26   \n970                                   Google       2016-10-07   \n...                                      ...              ...   \n80                                   Alibaba       2023-11-30   \n72                           Google DeepMind       2023-12-06   \n29               ByteDance,Peking University       2024-02-23   \n20                             Inflection AI       2024-03-07   \n4                                    Meta AI       2024-04-18   \n\n                                              Reference  \\\n1016  Deep Speech 2: End-to-End Speech Recognition i...   \n1014       Deep Residual Learning for Image Recognition   \n1010  Mastering the game of Go with deep neural netw...   \n971   Google's Neural Machine Translation System: Br...   \n970   Xception: Deep Learning with Depthwise Separab...   \n...                                                 ...   \n80                                                  NaN   \n72    Gemini: A Family of Highly Capable Multimodal ...   \n29    MegaScale: Scaling Large Language Model Traini...   \n20    Inflection-2.5: meet the world's best personal AI   \n4     Introducing Meta Llama 3: The most capable ope...   \n\n                                                   Link    Parameters  \\\n1016                   https://arxiv.org/abs/1512.02595  3.800000e+07   \n1014                   https://arxiv.org/abs/1512.03385  6.000000e+07   \n1010        https://www.nature.com/articles/nature16961           NaN   \n971                    https://arxiv.org/abs/1609.08144  2.780000e+08   \n970                    https://arxiv.org/abs/1610.02357  2.285595e+07   \n...                                                 ...           ...   \n80                 https://huggingface.co/Qwen/Qwen-72B  7.200000e+10   \n72    https://storage.googleapis.com/deepmind-media/...           NaN   \n29                     https://arxiv.org/abs/2402.15627  5.300000e+11   \n20                 https://inflection.ai/inflection-2-5           NaN   \n4     https://ai.meta.com/blog/meta-llama-3/\\n\\nhttp...  7.000000e+10   \n\n                                       Parameters notes  \\\n1016           All networks have 38 million parameters.   \n1014        Taken from https://arxiv.org/abs/1605.07146   \n1010                                                NaN   \n971   Table 5 in 'Outrageously Large Neural Networks...   \n970                                             Table 3   \n...                                                 ...   \n80                                                  72B   \n72                                                  NaN   \n29    Production run is stated to have \"hundreds of ...   \n20                                                  NaN   \n4                                                   NaN   \n\n      Training compute (FLOP)  \\\n1016             2.600000e+19   \n1014             1.210000e+19   \n1010             1.900000e+21   \n971              6.900000e+21   \n970              4.360000e+20   \n...                       ...   \n80               1.300000e+24   \n72               5.000000e+25   \n29               1.200000e+25   \n20               1.000100e+25   \n4                6.300000e+24   \n\n                                 Training compute notes  ...  \\\n1016  1 timestep = (1280 hidden units)^2 * (7 RNN la...  ...   \n1014  (11.4 *10^9) mult-adds per forward pass\\n2 FLO...  ...   \n1010  This number is pretty uncertain. I expect it t...  ...   \n971   sqrt(10 * 100) factor added because production...  ...   \n970   60 K80 GPUs * 30 days * 8.5 TFLOPS/GPU * 0.33 ...  ...   \n...                                                 ...  ...   \n80    72 billion params, 3 trillion tokens\\n72b * 3T...  ...   \n72    This number is an estimate based on limited ev...  ...   \n29    Speculative. The model is stated to have train...  ...   \n20    \"Inflection-1 used approximately 4% the traini...  ...   \n4     direct calculation\\n15000000000000 tokens*7000...  ...   \n\n     top_13_at_release  top_14_at_release top_15_at_release top_16_at_release  \\\n1016              True               True              True              True   \n1014              True               True              True              True   \n1010              True               True              True              True   \n971               True               True              True              True   \n970               True               True              True              True   \n...                ...                ...               ...               ...   \n80                True               True              True              True   \n72                True               True              True              True   \n29                True               True              True              True   \n20                True               True              True              True   \n4                 True               True              True              True   \n\n     top_17_at_release top_18_at_release top_19_at_release top_20_at_release  \\\n1016              True              True              True              True   \n1014              True              True              True              True   \n1010              True              True              True              True   \n971               True              True              True              True   \n970               True              True              True              True   \n...                ...               ...               ...               ...   \n80                True              True              True              True   \n72                True              True              True              True   \n29                True              True              True              True   \n20                True              True              True              True   \n4                 True              True              True              True   \n\n     Frontier training compute (FLOP) Frontier system  \n1016                     3.800000e+20            True  \n1014                     3.800000e+20            True  \n1010                     1.900000e+21            True  \n971                      6.900000e+21            True  \n970                      6.900000e+21            True  \n...                               ...             ...  \n80                       2.100000e+25            True  \n72                       5.000000e+25            True  \n29                       5.000000e+25            True  \n20                       5.000000e+25            True  \n4                        5.000000e+25            True  \n\n[112 rows x 68 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>System</th>\n      <th>Domain</th>\n      <th>Organization</th>\n      <th>Publication date</th>\n      <th>Reference</th>\n      <th>Link</th>\n      <th>Parameters</th>\n      <th>Parameters notes</th>\n      <th>Training compute (FLOP)</th>\n      <th>Training compute notes</th>\n      <th>...</th>\n      <th>top_13_at_release</th>\n      <th>top_14_at_release</th>\n      <th>top_15_at_release</th>\n      <th>top_16_at_release</th>\n      <th>top_17_at_release</th>\n      <th>top_18_at_release</th>\n      <th>top_19_at_release</th>\n      <th>top_20_at_release</th>\n      <th>Frontier training compute (FLOP)</th>\n      <th>Frontier system</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1016</th>\n      <td>DeepSpeech2 (English)</td>\n      <td>Speech</td>\n      <td>Baidu Research - Silicon Valley AI Lab</td>\n      <td>2015-12-08</td>\n      <td>Deep Speech 2: End-to-End Speech Recognition i...</td>\n      <td>https://arxiv.org/abs/1512.02595</td>\n      <td>3.800000e+07</td>\n      <td>All networks have 38 million parameters.</td>\n      <td>2.600000e+19</td>\n      <td>1 timestep = (1280 hidden units)^2 * (7 RNN la...</td>\n      <td>...</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>3.800000e+20</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1014</th>\n      <td>ResNet-152 (ImageNet)</td>\n      <td>Vision</td>\n      <td>Microsoft</td>\n      <td>2015-12-10</td>\n      <td>Deep Residual Learning for Image Recognition</td>\n      <td>https://arxiv.org/abs/1512.03385</td>\n      <td>6.000000e+07</td>\n      <td>Taken from https://arxiv.org/abs/1605.07146</td>\n      <td>1.210000e+19</td>\n      <td>(11.4 *10^9) mult-adds per forward pass\\n2 FLO...</td>\n      <td>...</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>3.800000e+20</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1010</th>\n      <td>AlphaGo Lee</td>\n      <td>Games</td>\n      <td>DeepMind</td>\n      <td>2016-01-27</td>\n      <td>Mastering the game of Go with deep neural netw...</td>\n      <td>https://www.nature.com/articles/nature16961</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.900000e+21</td>\n      <td>This number is pretty uncertain. I expect it t...</td>\n      <td>...</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>1.900000e+21</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>971</th>\n      <td>GNMT</td>\n      <td>Language</td>\n      <td>Google</td>\n      <td>2016-09-26</td>\n      <td>Google's Neural Machine Translation System: Br...</td>\n      <td>https://arxiv.org/abs/1609.08144</td>\n      <td>2.780000e+08</td>\n      <td>Table 5 in 'Outrageously Large Neural Networks...</td>\n      <td>6.900000e+21</td>\n      <td>sqrt(10 * 100) factor added because production...</td>\n      <td>...</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>6.900000e+21</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>970</th>\n      <td>Xception</td>\n      <td>Vision</td>\n      <td>Google</td>\n      <td>2016-10-07</td>\n      <td>Xception: Deep Learning with Depthwise Separab...</td>\n      <td>https://arxiv.org/abs/1610.02357</td>\n      <td>2.285595e+07</td>\n      <td>Table 3</td>\n      <td>4.360000e+20</td>\n      <td>60 K80 GPUs * 30 days * 8.5 TFLOPS/GPU * 0.33 ...</td>\n      <td>...</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>6.900000e+21</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>80</th>\n      <td>Qwen-72B</td>\n      <td>Language</td>\n      <td>Alibaba</td>\n      <td>2023-11-30</td>\n      <td>NaN</td>\n      <td>https://huggingface.co/Qwen/Qwen-72B</td>\n      <td>7.200000e+10</td>\n      <td>72B</td>\n      <td>1.300000e+24</td>\n      <td>72 billion params, 3 trillion tokens\\n72b * 3T...</td>\n      <td>...</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>2.100000e+25</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>72</th>\n      <td>Gemini Ultra</td>\n      <td>Multimodal,Language,Vision</td>\n      <td>Google DeepMind</td>\n      <td>2023-12-06</td>\n      <td>Gemini: A Family of Highly Capable Multimodal ...</td>\n      <td>https://storage.googleapis.com/deepmind-media/...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>5.000000e+25</td>\n      <td>This number is an estimate based on limited ev...</td>\n      <td>...</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>5.000000e+25</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>MegaScale (Production)</td>\n      <td>Language</td>\n      <td>ByteDance,Peking University</td>\n      <td>2024-02-23</td>\n      <td>MegaScale: Scaling Large Language Model Traini...</td>\n      <td>https://arxiv.org/abs/2402.15627</td>\n      <td>5.300000e+11</td>\n      <td>Production run is stated to have \"hundreds of ...</td>\n      <td>1.200000e+25</td>\n      <td>Speculative. The model is stated to have train...</td>\n      <td>...</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>5.000000e+25</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>Inflection-2.5</td>\n      <td>Language</td>\n      <td>Inflection AI</td>\n      <td>2024-03-07</td>\n      <td>Inflection-2.5: meet the world's best personal AI</td>\n      <td>https://inflection.ai/inflection-2-5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.000100e+25</td>\n      <td>\"Inflection-1 used approximately 4% the traini...</td>\n      <td>...</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>5.000000e+25</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Llama 3-70B</td>\n      <td>Language</td>\n      <td>Meta AI</td>\n      <td>2024-04-18</td>\n      <td>Introducing Meta Llama 3: The most capable ope...</td>\n      <td>https://ai.meta.com/blog/meta-llama-3/\\n\\nhttp...</td>\n      <td>7.000000e+10</td>\n      <td>NaN</td>\n      <td>6.300000e+24</td>\n      <td>direct calculation\\n15000000000000 tokens*7000...</td>\n      <td>...</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>5.000000e+25</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n<p>112 rows × 68 columns</p>\n</div>"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frontier_df = pcd_df[pcd_df['Frontier system']]\n",
    "frontier_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pjDaQcsVsyYz",
    "outputId": "7aa5fd43-3b36-45f5-997c-d57abd4a94d3",
    "ExecuteTime": {
     "end_time": "2024-05-14T22:02:03.808557Z",
     "start_time": "2024-05-14T22:02:03.725936500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama 3-70B\n",
      "Inflection-2.5\n",
      "MegaScale (Production)\n",
      "Gemini Ultra\n",
      "Qwen-72B\n",
      "Inflection-2\n",
      "Grok-1\n",
      "Yi-34B\n",
      "Skywork-13B\n",
      "ChatGLM3\n",
      "Falcon-180B\n",
      "Llama 2-70B\n",
      "Claude 2\n",
      "xTrimoPGLM -100B\n",
      "PaLM 2\n",
      "BloombergGPT\n",
      "PanGu-Σ\n",
      "GPT-4\n",
      "Falcon-40B\n",
      "LLaMA-65B\n",
      "LLaMA-7B\n",
      "ViT-22B\n",
      "GPT-3.5 (text-davinci-003)\n",
      "Galactica\n",
      "BLOOM-176B\n",
      "Taiyi-Stable Diffusion\n",
      "U-PaLM (540B)\n",
      "Flan-PaLM 540B\n",
      "Flan-T5 11B\n",
      "Whisper\n",
      "PaLI\n",
      "BlenderBot 3\n",
      "GLM-130B\n",
      "AlexaTM 20B\n",
      "ESM2-15B\n",
      "Minerva (540B)\n",
      "Parti\n",
      "CoCa\n",
      "UL2\n",
      "OPT-175B\n",
      "Flamingo\n",
      "Stable Diffusion (LDM-KL-8-G)\n",
      "PaLM (540B)\n",
      "Chinchilla\n",
      "ST-MoE\n",
      "LaMDA\n",
      "GPT-NeoX-20B\n",
      "RETRO-7B\n",
      "AlphaCode\n",
      "ERNIE 3.0 Titan\n",
      "XGLM-7.5B\n",
      "XGLM\n",
      "GLaM\n",
      "Gopher (280B)\n",
      "Student of Games\n",
      "Florence\n",
      "BASIC-L\n",
      "T0-XXL\n",
      "Yuan 1.0\n",
      "Megatron-Turing NLG 530B\n",
      "AlphaFold-Multimer\n",
      "HyperCLOVA\n",
      "FLAN 137B\n",
      "SEER\n",
      "GOAT\n",
      "HuBERT\n",
      "ERNIE 3.0\n",
      "ALIGN\n",
      "DeBERTa\n",
      "CoAtNet\n",
      "ByT5-XXL\n",
      "CogView\n",
      "ProtT5-XXL\n",
      "ProtT5-XXL-BFD\n",
      "ProtBERT-BFD\n",
      "PLUG\n",
      "M6-T\n",
      "Meta Pseudo Labels\n",
      "MSA Transformer\n",
      "Switch\n",
      "DALL-E\n",
      "CLIP (ViT L/14@336px)\n",
      "ViT-Huge/14\n",
      "mT5-XXL\n",
      "Conformer + Wav2vec 2.0 + Noisy Student\n",
      "GShard (dense)\n",
      "iGPT-L\n",
      "iGPT-XL\n",
      "GPT-3 175B (davinci)\n",
      "Turing-NLG\n",
      "Meena\n",
      "ContextNet + Noisy Student\n",
      "OpenAI Five\n",
      "OpenAI Five Rerun\n",
      "AlphaStar\n",
      "T5-11B\n",
      "Megatron-LM (8.3B)\n",
      "Megatron-BERT\n",
      "RoBERTa Large\n",
      "XLNet\n",
      "GPT-2 (1.5B)\n",
      "FTW\n",
      "ResNeXt-101 32x48d\n",
      "AlphaZero\n",
      "AlphaGo Zero\n",
      "AlphaGo Master\n",
      "NASv3 (CIFAR-10)\n",
      "Xception\n",
      "GNMT\n",
      "AlphaGo Lee\n",
      "ResNet-152 (ImageNet)\n",
      "DeepSpeech2 (English)\n"
     ]
    }
   ],
   "source": [
    "for system in frontier_df['System'][::-1]:\n",
    "  print(system)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OJ4rEiW4hW6_"
   },
   "source": [
    "# Constant threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "L8IgNL9shAZb",
    "ExecuteTime": {
     "end_time": "2024-05-14T22:02:03.808557Z",
     "start_time": "2024-05-14T22:02:03.734019600Z"
    }
   },
   "outputs": [],
   "source": [
    "compute_threshold = 1e23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "tGPWWQJ6hZCU",
    "ExecuteTime": {
     "end_time": "2024-05-14T22:02:03.825050600Z",
     "start_time": "2024-05-14T22:02:03.750656700Z"
    }
   },
   "outputs": [],
   "source": [
    "above_threshold = pcd_df[pcd_df['Training compute (FLOP)'] > compute_threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wB6Qu1pBhd41",
    "outputId": "456309a2-7c4a-4f19-8e18-9922508807b5",
    "ExecuteTime": {
     "end_time": "2024-05-14T22:02:03.898888400Z",
     "start_time": "2024-05-14T22:02:03.767320600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54 systems\n",
      "Llama 3-70B\n",
      "MM1-30B\n",
      "Inflection-2.5\n",
      "MegaScale (Production)\n",
      "FunSearch\n",
      "Gemini Ultra\n",
      "Qwen-72B\n",
      "Inflection-2\n",
      "Nemotron-3-8B\n",
      "Grok-1\n",
      "Yi-34B\n",
      "Skywork-13B\n",
      "ChatGLM3\n",
      "FinGPT-13B\n",
      "Falcon-180B\n",
      "Llama 2-70B\n",
      "Claude 2\n",
      "xTrimoPGLM -100B\n",
      "WizardCoder-15.5B\n",
      "PaLM 2\n",
      "BloombergGPT\n",
      "PanGu-Σ\n",
      "GPT-4\n",
      "Falcon-40B\n",
      "LLaMA-65B\n",
      "ViT-22B\n",
      "GPT-3.5 (text-davinci-003)\n",
      "Galactica\n",
      "BLOOM-176B\n",
      "U-PaLM (540B)\n",
      "Flan-PaLM 540B\n",
      "BlenderBot 3\n",
      "GLM-130B\n",
      "AlexaTM 20B\n",
      "Minerva (540B)\n",
      "Parti\n",
      "UL2\n",
      "OPT-175B\n",
      "Flamingo\n",
      "PaLM (540B)\n",
      "Chinchilla\n",
      "ST-MoE\n",
      "LaMDA\n",
      "AlphaCode\n",
      "ERNIE 3.0 Titan\n",
      "GLaM\n",
      "Gopher (280B)\n",
      "Yuan 1.0\n",
      "Megatron-Turing NLG 530B\n",
      "HyperCLOVA\n",
      "GPT-3 175B (davinci)\n",
      "Meena\n",
      "AlphaGo Zero\n",
      "AlphaGo Master\n"
     ]
    }
   ],
   "source": [
    "print(len(above_threshold), 'systems')\n",
    "for system in above_threshold['System'][::-1]:\n",
    "  print(system)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

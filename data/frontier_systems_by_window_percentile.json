{
    "95": [
        "AlphaGo Master",
        "AlphaGo Zero",
        "AlphaZero",
        "FTW",
        "GNMT",
        "GPT-3 175B (davinci)",
        "GPT-4",
        "Gemini 1.0 Ultra",
        "Meena",
        "Megatron-BERT",
        "Megatron-Turing NLG 530B",
        "Minerva (540B)",
        "OpenAI Five",
        "PaLM (540B)",
        "ResNeXt-101 32x48d"
    ],
    "90": [
        "AlphaStar",
        "Chinchilla",
        "Claude 2",
        "ERNIE 3.0 Titan",
        "Flan-PaLM 540B",
        "GPT-3.5 (text-davinci-003)",
        "Gopher (280B)",
        "Inflection-2",
        "NASv3 (CIFAR-10)",
        "PaLM 2",
        "Switch",
        "T5-11B",
        "U-PaLM (540B)",
        "mT5-XXL"
    ],
    "85": [
        "AlphaGo Fan",
        "AlphaGo Lee",
        "BLOOM-176B",
        "Falcon-180B",
        "GLaM",
        "Grok-1",
        "JFT",
        "LaMDA",
        "Megatron-LM (8.3B)",
        "OPT-175B",
        "OpenAI Five Rerun",
        "Turing-NLG",
        "Yuan 1.0"
    ],
    "80": [
        "AlphaCode",
        "AmoebaNet-A (F=448)",
        "BigGAN-deep 512x512",
        "BlenderBot 3",
        "ByT5-XXL",
        "ChatGLM3",
        "DALL-E",
        "Flamingo",
        "GOAT",
        "GPT-2 (1.5B)",
        "Llama 2-70B",
        "Meta Pseudo Labels",
        "OpenAI TI7 DOTA 1v1",
        "Parti",
        "ProtT5-XXL",
        "Qwen-72B",
        "ST-MoE",
        "XLNet",
        "iGPT-XL"
    ],
    "75": [
        "ContextNet + Noisy Student",
        "DeepSpeech2 (English)",
        "GLM-130B",
        "GPT-NeoX-20B",
        "IMPALA",
        "LLaMA-65B",
        "Libratus",
        "PanGu-\u03a3",
        "RoBERTa Large",
        "Transformer (Adaptive Input Embeddings)",
        "ViT-22B",
        "Xception",
        "Yi-34B",
        "xTrimoPGLM -100B"
    ],
    "70": [
        "AlexaTM 20B",
        "BERT-Large",
        "Big Transformer for Back-Translation",
        "CoAtNet",
        "FLAN 137B",
        "GShard (dense)",
        "Galactica",
        "MnasNet-A1 + SSDLite",
        "MnasNet-A3",
        "MoE",
        "PolyNet",
        "ProtBERT-BFD",
        "UL2",
        "iGPT-L"
    ],
    "65": [
        "ALBERT-xxlarge",
        "BERT-Large-CAS (PTB+WT2+WT103)",
        "BloombergGPT",
        "CLIP (ViT L/14@336px)",
        "CoCa",
        "Conformer + Wav2vec 2.0 + Noisy Student",
        "ConvS2S (ensemble of 8 models)",
        "ELECTRA",
        "Falcon-40B",
        "Florence",
        "FunSearch",
        "Mesh-TensorFlow Transformer 4.9B (language modelling)",
        "PLUG",
        "ProtT5-XXL-BFD",
        "ResNet-152 (ImageNet)",
        "Skywork-13B",
        "Stable Diffusion (LDM-KL-8-G)",
        "YOLOv3"
    ],
    "60": [
        "ALIGN",
        "BASIC-L",
        "CogView",
        "ERNIE 3.0",
        "ESM2-15B",
        "Mesh-TensorFlow Transformer 2.9B (translation)",
        "Nemotron-3-8B",
        "PNASNet-5",
        "PaLI",
        "T5-3B",
        "XGLM-7.5B"
    ],
    "55": [
        "CamemBERT",
        "DeepStack",
        "FinGPT-13B",
        "LSTM (Hebbian, Cache, MbPA)",
        "Llama 2-7B",
        "M6-T",
        "MSA Transformer",
        "Noisy Student (L2)",
        "Once for All",
        "Population-based DRL",
        "QT-Opt",
        "SciBERT",
        "StarCoder",
        "Student of Games",
        "Taiyi-Stable Diffusion",
        "ViT-Huge/14",
        "Whisper",
        "WizardCoder-15.5B"
    ],
    "50": [
        "AlphaFold 2",
        "BIDAF",
        "DD-PPO",
        "DeBERTa",
        "Flan-T5 11B",
        "GBERT-Large",
        "GPT",
        "HuBERT",
        "Imagen",
        "LLaMA-7B",
        "NLLB",
        "ProxylessNAS",
        "RETRO-7B",
        "Transformer",
        "XGLM",
        "wave2vec 2.0 LARGE"
    ],
    "45": [
        "CPM-Large",
        "German ELECTRA Large",
        "Jais",
        "OmegaPLM",
        "Pangu-Weather",
        "ProGen2-xlarge",
        "RetinaNet-R101",
        "SEER",
        "Sandwich Transformer",
        "Transformer + Simple Recurrent Unit",
        "WizardLM-7B"
    ],
    "40": [
        "AlphaFold",
        "AlphaFold-Multimer",
        "CogAgent",
        "CogVLM",
        "DETR",
        "ESM1-670M (UR50/D)",
        "GL-LWGC-AWD-MoS-LSTM + dynamic evaluation (WT2)",
        "Gato",
        "GraphCast",
        "KataGo",
        "MuZero",
        "N\u00dcWA",
        "Part-of-sentence tagging model",
        "Tranception",
        "Transformer-XL (257M)",
        "ViT-G/14"
    ],
    "35": [
        "EMDR",
        "Feedback Transformer",
        "Named Entity Recognition model",
        "Nucleotide Transformer",
        "PeptideBERT",
        "R-FCN",
        "ResNet-152 + ObjectNet",
        "Segment Anything Model",
        "ViT-G (model soup)",
        "VideoMAE V2"
    ],
    "30": [
        "(ensemble): AWD-LSTM-DOC (fin) \u00d7 5 (WT2)",
        "ADM",
        "ATLAS",
        "AWD-LSTM - 3-layer LSTM (tied) + continuous cache pointer (WT2)",
        "AWD-LSTM-MoS + dynamic evaluation (WT2, 2017)",
        "Ankh_large",
        "AudioGen",
        "CodeT5-base",
        "CodeT5-large",
        "DINOv2",
        "DistilBERT",
        "ERNIE-GEN (large)",
        "EVA-01",
        "FAIRSEQ Adaptive Inputs",
        "LUKE",
        "QRNN",
        "TaLK Convolution",
        "TrellisNet"
    ],
    "25": [
        "AlphaX-1",
        "Ankh_base",
        "Big-Little Net (speech)",
        "Decoupled weight decay regularization",
        "Denoising Diffusion Probabilistic Models (LSUN Bedroom)",
        "GenSLM",
        "Hanabi 4 player",
        "Incoder-6.7B",
        "KEPLER",
        "PolyCoder",
        "S4",
        "Swin Transformer V2",
        "T0-XXL",
        "Tensorized Transformer (257M)",
        "Transformer-XL Large + Phrase Induction",
        "ULM-FiT",
        "VD-LSTM+REAL Large",
        "ViT + DINO"
    ],
    "20": [
        "4 layer QRNN (h=2500)",
        "BLIP-2 (Q-Former)",
        "Base LM + kNN LM + Continuous Cache",
        "ConSERT",
        "Cross-lingual alignment",
        "DLRM-2020",
        "DiT-XL/2",
        "Dropout-LSTM+Noise(Bernoulli) (WT2)",
        "Flan T5-XXL + BLIP-2",
        "Fraternal dropout + AWD-LSTM 3-layer (WT2)",
        "Hybrid H3-2.7B",
        "Masked Autoencoders",
        "Zoneout + Variational LSTM (WT2)"
    ],
    "15": [
        "AR-LDM",
        "Adaptive Input Transformer + RD",
        "BERT-RBP",
        "DDPM-IP (CelebA)",
        "DNABERT",
        "DeLight",
        "Discriminator Guidance",
        "ENAS",
        "ERNIE-Doc (247M)",
        "EfficientNetV2",
        "Neural Architecture Search with base 8 and shared embeddings",
        "ONE-PEACE",
        "Pointer Sentinel-LSTM (medium)",
        "Transformer-XL DeFINE (141M)",
        "Variational (untied weights, MC) LSTM (Large)",
        "aLSTM(depth-2)+RecurrentPolicy (WT2)"
    ],
    "10": [
        "BEIT-3",
        "DARTS",
        "DiffDock",
        "EI-REHN-1000D",
        "Fusion in Encoder",
        "LLaVA 1.5",
        "NAS+ESS (156M)",
        "ProBERTa",
        "ProteinBERT",
        "SRU++ Large",
        "Transformer local-attention (NesT-B)"
    ],
    "5": [
        "AWD-LSTM + MoS + Partial Shuffled",
        "AWD-LSTM+WT+Cache+IOG (WT2)",
        "AWD-LSTM-DRILL + dynamic evaluation\u2020 (WT2)",
        "CaLM",
        "Detic",
        "ISS",
        "LLaVA",
        "MMLSTM",
        "MedBERT",
        "MultiBand Diffusion",
        "Segatron-XL large, M=384 + HCP",
        "Sparse all-MLP",
        "Tensor-Transformer(1core)+PN (WT103)",
        "TransformerXL + spectrum control",
        "UDSMProt",
        "VD-RHN"
    ],
    "0": [
        "2-layer-LSTM+Deep-Gradient-Compression",
        "CODEFUSION (Python)",
        "CT-MoS (WT2)",
        "DITTO",
        "DensePhrases",
        "Fine-tuned-AWD-LSTM-DOC(fin)",
        "HyenaDNA",
        "Mogrifier RLSTM (WT2)",
        "Multi-cell LSTM",
        "PermuteFormer",
        "Pluribus",
        "VALL-E",
        "base LM+GNN+kNN"
    ]
}
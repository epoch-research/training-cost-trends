{
    "1": [
        "AlphaGo Lee",
        "GNMT",
        "Megatron-Turing NLG 530B",
        "GPT-4",
        "AlphaGo Zero",
        "AlphaGo Master",
        "Minerva (540B)",
        "PaLM (540B)",
        "Gemini 1.0 Ultra"
    ],
    "2": [
        "GPT-3 175B (davinci)",
        "ERNIE 3.0 Titan",
        "Gopher (280B)",
        "GPT-3.5 (text-davinci-003)",
        "Yuan 1.0",
        "NASv3 (CIFAR-10)",
        "PaLM 2",
        "U-PaLM (540B)",
        "Inflection-2"
    ],
    "3": [
        "Xception",
        "DeepSpeech2 (English)",
        "Claude 2",
        "AlphaZero",
        "Megatron-BERT",
        "Meena",
        "OpenAI Five",
        "GLaM"
    ],
    "4": [
        "AlphaStar",
        "Flan-PaLM 540B",
        "Chinchilla",
        "Falcon-180B",
        "HyperCLOVA 82B",
        "ResNeXt-101 32x48d"
    ],
    "5": [
        "mT5-XXL",
        "Switch",
        "T5-11B",
        "Megatron-LM (8.3B)",
        "LaMDA",
        "Grok-1",
        "Libratus",
        "FTW",
        "JFT",
        "ResNet-152 (ImageNet)"
    ],
    "6": [
        "PolyNet",
        "OPT-175B",
        "OpenAI TI7 DOTA 1v1"
    ],
    "7": [
        "ByT5-XXL",
        "GPT-2 (1.5B)",
        "ProtT5-XXL",
        "Parti"
    ],
    "8": [
        "BlenderBot 3",
        "OpenAI Five Rerun",
        "GOAT",
        "MoE",
        "XLNet",
        "BLOOM-176B"
    ],
    "9": [
        "RoBERTa Large",
        "ST-MoE",
        "AlphaCode",
        "DALL-E",
        "Turing-NLG",
        "BigGAN-deep 512x512",
        "GLM-130B"
    ],
    "10": [
        "Meta Pseudo Labels",
        "Transformer (Adaptive Input Embeddings) WT103",
        "iGPT-XL"
    ],
    "11": [
        "MnasNet-A1 + SSDLite",
        "xTrimoPGLM -100B",
        "ChatGLM3",
        "Llama 2-70B",
        "LLaMA-65B",
        "AmoebaNet-A (F=448)",
        "ConvS2S (ensemble of 8 models)",
        "BIDAF"
    ],
    "12": [
        "Qwen-72B",
        "PNASNet-5",
        "Flamingo",
        "DeepStack",
        "MnasNet-A3",
        "GPT-NeoX-20B"
    ],
    "13": [
        "FLAN 137B",
        "ViT-22B",
        "GShard (dense)",
        "PanGu-\u03a3",
        "ProtBERT-BFD",
        "IMPALA",
        "ContextNet + Noisy Student",
        "PLUG"
    ],
    "14": [
        "iGPT-L",
        "ProtT5-XXL-BFD",
        "CoAtNet"
    ],
    "15": [
        "Yi-34B",
        "BERT-Large-CAS (PTB+WT2+WT103)"
    ],
    "16": [
        "AlexaTM 20B",
        "Transformer",
        "Big Transformer for Back-Translation",
        "CLIP (ViT L/14@336px)"
    ],
    "17": [
        "Florence",
        "UL2",
        "Part-of-sentence tagging model",
        "BERT-Large"
    ],
    "18": [
        "ALBERT-xxlarge",
        "Galactica",
        "Conformer + Wav2vec 2.0 + Noisy Student"
    ],
    "19": [
        "ELECTRA",
        "YOLOv3",
        "Mesh-TensorFlow Transformer 4.9B (language modelling)",
        "T5-3B",
        "CogView"
    ],
    "20": [
        "BASIC-L",
        "LSTM (Hebbian, Cache, MbPA)",
        "Named Entity Recognition model"
    ]
}
{
    "1": [
        "AlphaGo Lee",
        "AlphaGo Zero",
        "ERNIE 3.0 Titan",
        "GNMT",
        "GPT-3.5",
        "GPT-4",
        "GPT-4 Turbo",
        "Gemini 1.0 Ultra",
        "Grok-3",
        "Jurassic-1-Jumbo",
        "Megatron-Turing NLG 530B",
        "PaLM (540B)"
    ],
    "2": [
        "AlphaGo Master",
        "GPT-3 175B (davinci)",
        "GPT-4.5",
        "GPT-4o",
        "Gopher (280B)",
        "NASv3 (CIFAR-10)",
        "PaLM 2"
    ],
    "3": [
        "AlphaStar",
        "AlphaZero",
        "Amazon Titan",
        "Claude 2",
        "Claude 3.5 Sonnet",
        "Inception v3",
        "Inflection-2",
        "Llama 3.1-405B",
        "Llama 4 Behemoth (preview)",
        "Meena",
        "OpenAI Five",
        "Xception",
        "Yuan 1.0"
    ],
    "4": [
        "Chinchilla",
        "Claude 3 Opus",
        "Falcon-180B",
        "GLM-4-Plus",
        "GLaM",
        "Gemini 1.5 Pro",
        "Grok-2",
        "JFT",
        "Megatron-BERT",
        "ResNeXt-101 32x48d",
        "T5-11B"
    ],
    "5": [
        "DeepSpeech2 (English)",
        "HyperCLOVA 204B",
        "Megatron-LM (8.3B)",
        "Mistral Large",
        "Nemotron-4 340B",
        "OpenAI TI7 DOTA 1v1",
        "RoBERTa Large",
        "Switch",
        "mT5-XXL"
    ],
    "6": [
        "Claude 3.7 Sonnet",
        "Grok-1",
        "InternLM",
        "LaMDA",
        "Libratus",
        "Mistral Large 2",
        "Noisy Student (L2)",
        "OPT-175B",
        "Parti",
        "ResNet-200",
        "XLNet"
    ],
    "7": [
        "Aramco Metabrain AI",
        "ByT5-XXL",
        "Doubao-pro",
        "GPT-2 (1.5B)",
        "GShard (dense)",
        "LLaMA-65B",
        "ProtT5-XXL",
        "T5-3B",
        "XLM-RoBERTa"
    ],
    "8": [
        "BigGAN-deep 512x512",
        "MegaScale (Production)",
        "MoE-Multi",
        "PolyNet"
    ],
    "9": [
        "BLOOM-176B",
        "DALL-E",
        "Inflection-2.5",
        "Llama 2-70B",
        "Meta Pseudo Labels",
        "ResNet-152 (ImageNet)",
        "iGPT-XL",
        "xTrimoPGLM -100B"
    ],
    "10": [
        "MnasNet-A3",
        "OpenAI Five Rerun",
        "PanGu-\u03a3",
        "Reka Core",
        "ST-MoE"
    ],
    "11": [
        "AlphaCode",
        "AmoebaNet-A (F=448)",
        "Big Transformer for Back-Translation",
        "ConvS2S (ensemble of 8 models)",
        "DALL\u00b7E 2",
        "Llama 3-70B",
        "MnasNet-A1 + SSDLite",
        "Qwen-72B",
        "Turing-NLG",
        "ViT-G/14"
    ],
    "12": [
        "GLM-130B",
        "Gemini 1.0 Pro"
    ],
    "13": [
        "DeepStack",
        "GPT-NeoX-20B",
        "ProtBERT-BFD"
    ],
    "14": [
        "PNASNet-5",
        "ProtT5-XXL-BFD",
        "Qwen1.5-72B",
        "Yi-34B"
    ],
    "15": [
        "BIDAF",
        "CoAtNet",
        "ContextNet + Noisy Student",
        "GPT-4o mini"
    ],
    "16": [
        "DBRX",
        "Florence",
        "Galactica",
        "Transformer-XL (257M)"
    ],
    "17": [
        "AlexaTM 20B",
        "BERT-Large",
        "iGPT-L"
    ],
    "18": [
        "CLIP (ViT L/14@336px)",
        "CogView",
        "Mesh-TensorFlow Transformer 4.9B (language)",
        "Mixtral 8x7B",
        "Qwen2-72B",
        "R-FCN",
        "Transformer",
        "UL2"
    ],
    "19": [
        "ALBERT-xxlarge",
        "Generative BST",
        "Pangu Ultra",
        "Qwen2.5-72B"
    ],
    "20": [
        "AFM-server",
        "BASIC-L",
        "ELECTRA",
        "LSTM (Hebbian, Cache, MbPA)",
        "XLMR-XXL"
    ]
}
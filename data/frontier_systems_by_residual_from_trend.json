{
    "0": [
        "SPIDER2",
        "LSTM+NeuralCache",
        "Fine-tuned-AWD-LSTM-DOC(fin)",
        "Multi-cell LSTM",
        "Swift",
        "2-layer-LSTM+Deep-Gradient-Compression",
        "Image Classification with the Fisher Vector: Theory and Practice",
        "AWD-LSTM+WT+Cache+IOG (WT2)",
        "ISS",
        "Mogrifier RLSTM (WT2)",
        "Image generation",
        "DARTS",
        "VD-RHN",
        "LSTM-Char-Large",
        "EI-REHN-1000D",
        "Pointer Sentinel-LSTM (medium)",
        "Search-Proven Best LSTM",
        "ENAS"
    ],
    "5": [
        "System 11",
        "Variational (untied weights, MC) LSTM (Large)",
        "Neural Architecture Search with base 8 and shared embeddings",
        "6-layer MLP (MNIST)",
        "Pluribus",
        "Innervator",
        "DeiT",
        "Fuzzy NN",
        "DQN",
        "Zoneout + Variational LSTM (WT2)",
        "VD-LSTM+REAL Large",
        "Feedforward NN",
        "aLSTM(depth-2)+RecurrentPolicy (WT2)",
        "RNN+weight noise+dynamic eval",
        "CT-MoS (WT2)",
        "BiLSTM for Speech",
        "Dropout-LSTM+Noise(Bernoulli) (WT2)",
        "TransformerXL + spectrum control"
    ],
    "10": [
        "Fraternal dropout + AWD-LSTM 3-layer (WT2)",
        "Back-propagation",
        "HyenaDNA",
        "AWD-LSTM + MoS + Partial Shuffled",
        "CODEFUSION (Python)",
        "R-FCN",
        "Dropout (CIFAR)",
        "AWD-LSTM-DRILL + dynamic evaluation\u2020 (WT2)",
        "MCDNN (MNIST)",
        "4 layer QRNN (h=2500)",
        "Dropout (MNIST)",
        "ULM-FiT",
        "PermuteFormer",
        "Named Entity Recognition model",
        "GPU DBNs",
        "DensePhrases",
        "UDSMProt",
        "Big-Little Net (speech)",
        "QRNN"
    ],
    "15": [
        "Tensor-Transformer(1core)+PN (WT103)",
        "VALL-E",
        "AWD-LSTM - 3-layer LSTM (tied) + continuous cache pointer (WT2)",
        "Part-of-sentence tagging model",
        "RNN 500/10 + RT09 LM (NIST RT05)",
        "(ensemble): AWD-LSTM-DOC (fin) \u00d7 5 (WT2)",
        "ADAM (CIFAR-10)",
        "genCNN + dyn eval",
        "AWD-LSTM-MoS + dynamic evaluation (WT2, 2017)",
        "MultiBand Diffusion",
        "DITTO",
        "base LM+GNN+kNN",
        "GL-LWGC-AWD-MoS-LSTM + dynamic evaluation (WT2)",
        "NAS+ESS (156M)",
        "SPN-4+KN5",
        "Word2Vec (large)",
        "MMLSTM",
        "SmooCT"
    ],
    "20": [
        "Fractional Max-Pooling",
        "Large regularized LSTM",
        "LeNet-5",
        "MedBERT",
        "CaLM",
        "Cross-lingual alignment",
        "Decoupled weight decay regularization",
        "LLaVA 1.5",
        "SRU++ Large",
        "LLaVA",
        "DLRM-2020",
        "Mixture-of-Depths",
        "Segatron-XL large, M=384 + HCP",
        "Detic",
        "TrellisNet",
        "ProBERTa",
        "Transformer-XL DeFINE (141M)",
        "Tensorized Transformer (257M)"
    ],
    "25": [
        "Hanabi 4 player",
        "Base LM + kNN LM + Continuous Cache",
        "Mitosis",
        "ASE+ACE",
        "AlphaX-1",
        "Transformer local-attention (NesT-B)",
        "Decision tree (classification)",
        "RetinaNet-R101",
        "Transformer-XL Large + Phrase Induction",
        "DiffDock",
        "FAIRSEQ Adaptive Inputs",
        "BEIT-3",
        "Sparse all-MLP",
        "DistilBERT",
        "ERNIE-Doc (247M)",
        "Invariant image recognition",
        "ProteinBERT",
        "DeLight",
        "Zip CNN"
    ],
    "30": [
        "ONE-PEACE",
        "Fusion in Encoder",
        "GANs",
        "Transformer-XL (257M)",
        "ResNet-152 + ObjectNet",
        "Transformer + Simple Recurrent Unit",
        "TaLK Convolution",
        "BIDAF",
        "KN5 LM + RNN 400/10 (WSJ)",
        "Adaptive Input Transformer + RD",
        "ATLAS",
        "Visualizing CNNs",
        "BERT-RBP",
        "Cognitron",
        "Discriminator Guidance",
        "GoogLeNet / InceptionV1",
        "ALVINN",
        "EfficientNetV2"
    ],
    "35": [
        "Transformer",
        "Feedback Transformer",
        "KataGo",
        "Dropout (ImageNet)",
        "DDPM-IP (CelebA)",
        "DNABERT",
        "LSTM",
        "GPT",
        "MuZero",
        "RNNsearch-50*",
        "AlexNet",
        "Neocognitron",
        "NetTalk (dictionary)",
        "NetTalk (transcription)",
        "ProxylessNAS",
        "KEPLER",
        "DiT-XL/2",
        "TransE"
    ],
    "40": [
        "AR-LDM",
        "LSTM (Hebbian, Cache, MbPA)",
        "Unsupervised High-level Feature Learner",
        "NPLM",
        "ViT + DINO",
        "Population-based DRL",
        "QT-Opt",
        "AlphaFold",
        "DeepStack",
        "LUKE",
        "ConSERT",
        "Hybrid H3-2.7B",
        "SPPNet",
        "Masked Autoencoders",
        "ERNIE-GEN (large)",
        "Denoising Diffusion Probabilistic Models (LSUN Bedroom)",
        "Mesh-TensorFlow Transformer 2.9B (translation)",
        "SciBERT"
    ],
    "45": [
        "Flan T5-XXL + BLIP-2",
        "BLIP-2 (Q-Former)",
        "YOLOv3",
        "Transformer (Adaptive Input Embeddings)",
        "Sandwich Transformer",
        "ResNet-152 (ImageNet)",
        "S4",
        "ADM",
        "GenSLM",
        "PNASNet-5",
        "Big Transformer for Back-Translation",
        "PolyCoder",
        "ConvS2S (ensemble of 8 models)",
        "DETR",
        "ESM1-670M (UR50/D)",
        "VGG16",
        "Swin Transformer V2",
        "Incoder-6.7B"
    ],
    "50": [
        "Mesh-TensorFlow Transformer 4.9B (language modelling)",
        "Ankh_base",
        "DeepSpeech2 (English)",
        "CodeT5-base",
        "EMDR",
        "PolyNet",
        "CodeT5-large",
        "EVA-01",
        "IMPALA",
        "MSRA (C, PReLU)",
        "BERT-Large",
        "MoE",
        "PeptideBERT",
        "DD-PPO",
        "ViT-G (model soup)",
        "BERT-Large-CAS (PTB+WT2+WT103)",
        "German ELECTRA Large",
        "DINOv2",
        "Ankh_large"
    ],
    "55": [
        "AudioGen",
        "Segment Anything Model",
        "CamemBERT",
        "Noisy Student (L2)",
        "CPM-Large",
        "T5-3B",
        "wave2vec 2.0 LARGE",
        "VideoMAE V2",
        "Gato",
        "ADALINE",
        "ViT-G/14",
        "GBERT-Large",
        "AlphaFold-Multimer",
        "AmoebaNet-A (F=448)",
        "N\u00dcWA",
        "Once for All",
        "Tranception",
        "SEER"
    ],
    "60": [
        "Nucleotide Transformer",
        "TD-Gammon",
        "GraphCast",
        "AlphaFold 2",
        "CogVLM",
        "Seq2Seq LSTM",
        "HuBERT",
        "CogAgent",
        "MnasNet-A1 + SSDLite",
        "MnasNet-A3",
        "ALBERT-xxlarge",
        "DeBERTa",
        "ViT-Huge/14",
        "M6-T",
        "OmegaPLM",
        "ProGen2-xlarge",
        "MSA Transformer",
        "ELECTRA"
    ],
    "65": [
        "Jais",
        "OpenAI TI7 DOTA 1v1",
        "Imagen",
        "NLLB",
        "Xception",
        "Theseus",
        "Pangu-Weather",
        "Libratus",
        "BigGAN-deep 512x512",
        "Heuristic Reinforcement Learning",
        "RETRO-7B",
        "JFT",
        "Conformer + Wav2vec 2.0 + Noisy Student",
        "WizardLM-7B",
        "LLaMA-7B",
        "Flan-T5 11B",
        "ContextNet + Noisy Student",
        "CLIP (ViT L/14@336px)",
        "T0-XXL"
    ],
    "70": [
        "RoBERTa Large",
        "XGLM",
        "AlphaGo Fan",
        "XLNet",
        "iGPT-L",
        "Whisper",
        "GPT-2 (1.5B)",
        "Taiyi-Stable Diffusion",
        "Llama 2-7B",
        "ERNIE 3.0",
        "PaLI",
        "StarCoder",
        "GShard (dense)",
        "ALIGN",
        "Student of Games",
        "CogView",
        "Megatron-LM (8.3B)",
        "Stable Diffusion (LDM-KL-8-G)"
    ],
    "75": [
        "WizardCoder-15.5B",
        "XGLM-7.5B",
        "BASIC-L",
        "FinGPT-13B",
        "OpenAI Five Rerun",
        "ESM2-15B",
        "Florence",
        "Nemotron-3-8B",
        "CoCa",
        "Turing-NLG",
        "ProtT5-XXL-BFD",
        "PLUG",
        "ProtBERT-BFD",
        "FLAN 137B",
        "CoAtNet",
        "NASv3 (CIFAR-10)",
        "FTW",
        "Skywork-13B"
    ],
    "80": [
        "Meta Pseudo Labels",
        "GPT-NeoX-20B",
        "DALL-E",
        "iGPT-XL",
        "UL2",
        "AlphaGo Lee",
        "ResNeXt-101 32x48d",
        "MM1-30B",
        "BloombergGPT",
        "GOAT",
        "FunSearch",
        "Falcon-40B",
        "ProtT5-XXL",
        "ByT5-XXL",
        "AlexaTM 20B",
        "T5-11B",
        "AlphaCode",
        "Switch"
    ],
    "85": [
        "mT5-XXL",
        "HyperCLOVA",
        "Yi-34B",
        "Galactica",
        "ViT-22B",
        "Linear Decision Functions",
        "PanGu-\u03a3",
        "Flamingo",
        "xTrimoPGLM -100B",
        "GNMT",
        "AlphaStar",
        "GLM-130B",
        "LLaMA-65B",
        "OpenAI Five",
        "ST-MoE",
        "Megatron-BERT",
        "BlenderBot 3",
        "Parti"
    ],
    "90": [
        "Llama 2-70B",
        "ChatGLM3",
        "BLOOM-176B",
        "LaMDA",
        "OPT-175B",
        "Qwen-72B",
        "GLaM",
        "Meena",
        "Yuan 1.0",
        "Chinchilla",
        "AlphaZero",
        "Gopher (280B)",
        "Grok-1",
        "GPT-3 175B (davinci)",
        "ERNIE 3.0 Titan",
        "Falcon-180B",
        "Megatron-Turing NLG 530B",
        "Llama 3-70B",
        "Claude 2"
    ],
    "95": [
        "GPT-3.5 (text-davinci-003)",
        "Flan-PaLM 540B",
        "U-PaLM (540B)",
        "Minerva (540B)",
        "Print Recognition Logic",
        "PaLM (540B)",
        "Inflection-2.5",
        "Inflection-2",
        "MegaScale (Production)",
        "PaLM 2",
        "Perceptron Mark I",
        "AlphaGo Master",
        "AlphaGo Zero",
        "GPT-4",
        "Gemini Ultra",
        "Samuel Neural Checkers",
        "Perceptron (1960)",
        "Pandemonium (morse)"
    ]
}
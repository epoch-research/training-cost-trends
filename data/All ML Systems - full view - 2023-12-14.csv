System,Domain,Task,Authors,Notability criteria,Notability criteria notes,Open-source,Link,Citations,Reference,Publication date,Organization,Organization categorization,Parameters,Parameters notes,Training compute (FLOP),Training compute notes,Training dataset,Training dataset notes,Training dataset size (datapoints),Dataset size notes,Epochs,Inference compute (FLOP),Inference compute notes,Training time (hours),Training time notes,Training hardware,Name of the hardware (from Training hardware),Approach,Training compute cost (2020 USD),Compute cost notes,Compute sponsor categorization,Confidence,Abstract,Last modified,Created By,Benchmark data,Exclude,Country (from Organization),Organization (from Organization),Base model,Finetune compute (FLOP),Finetune compute notes,Authors by country,Hardware quantity,Hardware utilization,Training cost trends
Cohere Command,Language,,,,,,https://cohere.com/models/command,,"World-class AI, at your command",,Cohere,Industry,,,,,,,,,,,,,https://docs.cohere.com/docs/environmental-impact,Google TPU v4,Google TPU v4,,,,Industry,Speculative,,2023-11-21 01:39,Anonymous,,,Canada,Cohere,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Unverified,,2023-12-13 07:24,Robi Rahman,,,,,,,,,,,
Theseus,Other,Maze solving,Claude Shannon,Historical significance,,,https://www.technologyreview.com/2018/12/19/138508/mighty-mouse/,0.00,Mighty Mouse,1950-07-02,Bell Laboratories,Industry,40.00,"The learned part is the maze configuration. There are 25 squares of the maze. The 16 squares to the left top corner have each one adjacent square down and one adjacent square up, for a total of 16*2 walls. We only need to count the 8 spare walls connecting the squares in the right side and the bottom side. In total there are 16*2+8 walls.",40.00,"The ""training"" consists on the mouse running around and checking each wall.",,,40,Each wall Theseus bumps into is a datapoint,,,,,,,,,,,Industry,,,2023-11-26 03:25,Robi Rahman,,,United States of America,Bell Laboratories,,,,Theseus,,,
SNARC,Other,Maze solving,Marvin Minsky,Historical significance,,,https://en.wikipedia.org/wiki/Stochastic_neural_analog_reinforcement_calculator,33.00,A Neural-Analogue Calculator Based upon a Probability Model of Reinforcement,1952-01-08,Harvard University,Academia,40.00,"The link below seems to suggest the SNARC had 40 cells, each with a dial that acts as a configurable weight.

https://www.webofstories.com/play/marvin.minsky/137",,,,,,,,,,,,,,,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,United States of America,Harvard University,,,,SNARC,,,
Genetic algorithm,,,NA Barricelli,Historical significance,Possibly first computer simulation of a genetic evolution algorithm,,https://link.springer.com/article/10.1007/BF01556771,266.00,Numerical testing of evolution theories,1954-07-02,Institute for Advanced Study,Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,United States of America,Institute for Advanced Study,,,,Genetic algorithm,,,
Sequence-based pattern recognition,Vision,Character recognition,O. G. Selfridge,Historical significance,,,https://dl.acm.org/doi/10.1145/1455292.1455310,290.00,Pattern recognition and modern computers,1955-03-01,Massachusetts Institute of Technology (MIT),Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,United States of America,Massachusetts Institute of Technology (MIT),,,,Sequence-based pattern recognition,,,
Self Organizing System,Vision,Pattern recognition,W. A. Clark and B. G. Farley,Historical significance,,,https://dl.acm.org/doi/10.1145/1455292.1455309,93.00,Generalization of pattern recognition in a self-organizing system,1955-03-01,Massachusetts Institute of Technology (MIT),Academia,225.00,Figure 4 contains the learnt weight matrix,,,,,256,""" The modifier was then
disabled so that no further changes in the net could
occur and all 256 possible input patterns were then presented in turn.""

""For these purposes, 16-element nets (8 input and 8
output) were used because it was desired to exhaust all
possible input patterns, and we were limited to about
2^8 inputs by available time. """,,,,,,,,,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,United States of America,Massachusetts Institute of Technology (MIT),,,,Self Organizing System,,,
Conditional probability machines,Vision,,AM Uttley,Historical significance,,,https://www.moma.org/collection/works/illustratedbooks/16252?locale=es,84.00,Conditional probability machines,1956-07-01,Princeton University,Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,United States of America,Princeton University,,,,Conditional probability machines,,,
Perceptron Mark I,Vision,Binary classification,F Rosenblatt,"Historical significance,Highly cited",First modern neural network ,,https://blogs.umass.edu/brain-wars/files/2016/03/rosenblatt-1957.pdf,1610.00,The Perceptron—a perceiving and recognizing automaton,1957-01-01,"Cornell Aeronautical Laboratory,Cornell University",Industry,1000.00,"""Figure 4.8 Illustration of the Mark 1 perceptron hardware. The photograph on the left shows how the inputs were obtained using a simple camera system in which an input scene, in this case a printed character, was illuminated by powerful lights, and an image focussed onto a 20 × 20 array of cadmium sulphide photocells, giving a primitive 400 pixel image. The perceptron also had a patch board, shown in the middle photograph, which allowed different configurations of input features to be tried. Often these were wired up at random to demonstrate the ability of the perceptron to learn without the need for precise wiring, in contrast to a modern digital computer. The photograph on the right shows one of the racks of adaptive weights. Each weight was implemented using a rotary variable resistor, also called a potentiometer, driven by an electric motor thereby allowing the value of the weight to be adjusted automatically by the learning algorithm.""

source: Bishop, Christopher M. (2006). Pattern Recognition and Machine Learning

The Perceptron had a 400-pixel visual input and 1000 neurons in the hidden layer. https://twitter.com/DiegoKuonen/status/1130352233223262208",694894.94,Extracted from AI and Compute (https://openai.com/blog/ai-and-compute/) charts by using https://automeris.io/WebPlotDigitizer/.,,,6,Appendix II describes an experiment with 6 stimulus patterns,,,,,,,,,,,Industry,,,2023-11-26 03:25,Robi Rahman,,,"United States of America,United States of America","Cornell Aeronautical Laboratory, Cornell University",,,,Perceptron Mark I,,,
Pandemonium (morse),Other,Morse translation,OG Selfridge,Highly cited,,,https://aitopics.org/doc/classics:504E1BAC/,1453.00,Pandemonium: A Paradigm for Learning,1959-02-01,Massachusetts Institute of Technology (MIT),Academia,,"The paper mentions 11 function types. Unclear how many times they are called (number of ""demons"" in their Pandemonium implementation).",600000000.00,"The paper mentions using an IBM 704, which can execute up to 12,000 floating-point additions per second (https://wikiless.org/wiki/IBM_704). My best guess as to how long it ran for ranges between 1h to 2 days, which when plugged into guesstimate (https://www.getguesstimate.com/models/19625), i.e., taking the log mean, gives a mean estimate of 600M",,,,??? Might need to make a guesstimate here.,,,,,,,,,,,Academia,Speculative,,2023-11-26 03:25,Robi Rahman,,,United States of America,Massachusetts Institute of Technology (MIT),,,,Pandemonium (morse),,,
Samuel Neural Checkers,Games,Checkers,Arthur L. Samuel,Highly cited,,,https://ieeexplore.ieee.org/abstract/document/5392560,4404.00,Some studies in machine learning using the game of checkers,1959-07-01,IBM,Industry,16.00,"""with 16 terms for generalization learning""

""Mention has been made several times of the procedure
for replacing terms in the scoring polynomial. The program, as it is currently running, contains 38 different
terms (in addition to the piece-advantage term), 16 of
these being included in the scoring polynomial at anyone
time and the remaining 22 being kept in reserve.""",428400000.00,"""it can learn to do this in a remarkably short period of time 8 or 10 hours of machine-playing time)""

""The availability of a larger and faster machine (the IBM 704), coupled with many detailed changes in the programming procedure, leads to a fairly interesting game being played, even without any learning.""

""The Type 704 is the first large-scale, commercially available computer to employ fully automatic floating point arithmetic commands. [...]. Floating point addition or subtraction operations require 84 microseconds.""

source: https://www.ibm.com/ibm/history/exhibits/mainframe/mainframe_PP704.html

""An idea of the learning ability of this procedure can be gained by analyzing an initial test series of 28 games""

""Each game averaged 68 moves (34 to a side), of which approximately 20 caused changes to be made in the scoring polynomial.""",,,53000,"Based on number of board positions

At the present time the memory tape contains something over 53,000 board positions (averaging 3.8 word search) which have been selected from a much larger
number of positions by means of the culling techniques
described. While this is still far from the number which
would tax the listing and searching procedures used in
the program, rough estimates, based on the frequency
with which the saved boards are utilized during normal
play (these figures being tabulated automatically), indicate that a library tape containing at least 20 times the
present number of board positions would be needed to
improve the midgame play significantly. At the present
rate of acquisition of new positions this would require
an inordinate amount of play and, consequently, of
machine time.",,,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,Multinational,IBM,,,,Samuel Neural Checkers,,,
Pattern recognition and reading by machine,Vision,Character recognition,"W. W. Bledsoe, I. Browning",Historical significance,,,https://dl.acm.org/doi/10.1145/1460299.1460326,587.00,Pattern recognition and reading by machine,1959-12-01,Sandia Corporation,Industry,2625.00,A two bit state is recorded for each of the 75 cell pairs and each of the 25+10 characters recognized.,,,,,,,,,,,,,,,,,Industry,,,2023-11-26 03:25,Robi Rahman,,,United States of America,Sandia Corporation,,,,Pattern recognition and reading by machine,,,
LMS,,,Widrow and Hoff,Highly cited,,,https://www.scirp.org/(S(351jmbntvnsjt1aadkposzje))/reference/ReferencesPapers.aspx?ReferenceID=547230,6329.00,Adaptive switching circuits (technical report),1960-06-30,Stanford University,Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,United States of America,Stanford University,,,,LMS,,,
ADALINE,Vision,Pattern recognition,Widrow and Hoff,Highly cited,,,https://isl.stanford.edu/~widrow/papers/c1960adaptiveswitching.pdf,6329.00,Adaptive switching circuits,1960-06-30,Stanford University,Academia,17.00,"""The machine's total experience is stored in the values of the weights a0,...,a16""",9900.00,"""The method of searching that has proven most useful is the method of steepest descent""

Apparently each pattern was only shown once to the system.

So the training compute is (forward pass compute) * (3 for backprop) * dataset size",,,100,"""The best system, arrived at by slow precise adaptation on the full body of 100 noisy patterns, was able to classify these patterns as desired except for twelve errors.""

https://isl.stanford.edu/~widrow/papers/c1960adaptiveswitching.pdf",,33.00,We have 16 weights and a bias parameter. So 16 multadds and an add. The result is then thresholded to produce a binary output.,,,,,,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,United States of America,Stanford University,,,,ADALINE,,,
Heuristic problem solving for AI,,,Marvin Minsky,Highly cited,,,https://ieeexplore.ieee.org/abstract/document/4066245,2430.00,Steps Toward Artificial Intelligence,1961-01-01,Massachusetts Institute of Technology (MIT),Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,United States of America,Massachusetts Institute of Technology (MIT),,,,Heuristic problem solving for AI,,,
PAPA,,Binary classification,"A Gamba, L Gamberini, G Palmieri, R Sanna",,,,https://www.semanticscholar.org/paper/Further-experiments-with-PAPA-Gamba-Gamberini/c3a20b9aa86033cec29f08e69f4bc81e8b329ae2,24.00,Further experiments with PAPA,1961-09-01,University of Genoa,Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-11-21 01:39,Robi Rahman,,,Italy,University of Genoa,,,,,,,
MADALINE I,,,William Combs Ridgway,Historical significance,,,https://www.proquest.com/openview/7898314db50a218b58052ac91e3bde1e/1?,75.00,An adaptive logic system with generalizing properties,1962-07-01,Stanford University,Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,United States of America,Stanford University,,,,MADALINE I,,,
STeLLA,,,J.H. Andreae and Peter L. Joyce,,,,https://www.researchgate.net/publication/252919025_STELLA_A_scheme_for_a_learning_machine,34.00,STeLLA: A Scheme for a Learning Machine,1963-06-01,University of Canterbury,Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-11-21 01:39,Robi Rahman,,,New Zealand,University of Canterbury,,,,,,,
MENACE,Games,Tic Tac Toe,Donald Michie,,,,https://academic.oup.com/comjnl/article/6/3/232/360077,46.00,Experiments on the Mechanization of Game-Learning Part I. Characterization of the Model and its parameters,1963-11-01,University of Edinburgh,Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-11-21 01:39,Robi Rahman,,,United Kingdom of Great Britain and Northern Ireland,University of Edinburgh,,,,,,,
Samuel Neural Checkers II,Games,Checkers,"Palmieri, G. and R. Sanna",,,,https://www.cs.virginia.edu/~evans/greatworks/samuel.pdf,747.00,Some studies in machine learning using the game of checkers. Part II,1967-11-01,University of Geneva,Academia,40.00,"""The total number of parameters used at any one time has been varied from a very few to as many as 40""",,,,,,,,,,,,,,,,,Academia,,,2023-11-21 01:39,Robi Rahman,,,Switzerland,University of Geneva,,,,,,,
GLEE,Games,Tic Tac Toe,Michie and Chambers,Historical significance,,,https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.474.2430,590.00,Boxes: An Experiment in Adaptive Control,1968-07-01,University of Edinburgh,Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,United Kingdom of Great Britain and Northern Ireland,University of Edinburgh,,,,GLEE,,,
BOXES,Games,Pole balancing,Michie and Chambers,Historical significance,,,https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.474.2430,590.00,Boxes: An Experiment in Adaptive Control,1968-07-01,University of Edinburgh,Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,United Kingdom of Great Britain and Northern Ireland,University of Edinburgh,,,,BOXES,,,
Graph-based structural reasoning,,,Patrick Winston,Highly cited,,,https://dspace.mit.edu/handle/1721.1/6884,1805.00,Learning Structural Definitions from Examples,1970-09-01,Massachusetts Institute of Technology (MIT),Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,United States of America,Massachusetts Institute of Technology (MIT),,,,Graph-based structural reasoning,,,
Punish/Reward,Games,Blackjack,"Widrow, Gupta, and Maitra",,,,https://ieeexplore.ieee.org/document/4309272,397.00,Punish/Reward: Learning with a Critic in Adaptive Threshold Systems,1973-09-01,IEEE,Academia,21.00,"Fig. 1 shows that there is a bias term, while Fig. 5 shows that the input is a sequence of 20 bits, corresponding to 20 weights. So the total number of parameters is 21.",,,,,,??? Seemingly no info,,,,,,,,,,,Academia,,,2023-11-21 01:39,Robi Rahman,,,Multinational,IEEE,,,,,,,
Naive Bayes,Vision,,Duda and Hart,Highly cited,,,https://www.semanticscholar.org/paper/Pattern-classification-and-scene-analysis-Duda-Hart/b07ce649d6f6eb636872527104b0209d3edc8188,23127.00,Pattern Classification and Scene Analysis,1974-09-01,Stanford Research Institute,Industry,,,,,,,,,,,,,,,,,,,Industry,,,2023-11-26 03:25,Robi Rahman,,,United States of America,Stanford Research Institute,,,,Naive Bayes,,,
Cognitron,,,Kunihiko Fukushima,Historical significance,Precursor of the Neocognitron,,https://link.springer.com/article/10.1007%2FBF00342633,791.00,Cognitron: a self-organizing multilayered neural network,1975-09-01,Biological Cybernetics,Industry,,,,,,,,??? Seemingly no info,,,,,,,,,,,Industry,,,2023-11-26 03:25,Robi Rahman,,,Japan,Biological Cybernetics,,,,Cognitron,,,
TD(0),,,Ian Witten,Historical significance,,,https://www.sciencedirect.com/science/article/pii/S0019995877903540,269.00,An adaptive optimal controller for discrete-time Markov environments,1977-08-01,University of Essex,Academia,,,,,,,,??? Seemingly no info,,,,,,,,,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,United Kingdom of Great Britain and Northern Ireland,University of Essex,,,,TD(0),,,
Internal functionality of visual invariants,Vision,,Koenderink & van Doom,Historical significance,,,https://link.springer.com/article/10.1007/BF00337644,981.00,The internal representation of solid shape with respect to vision,1979-05-02,Utrecht University,Academia,,,,,,,,??? Seemingly no info,,,,,,,,,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,Netherlands,Utrecht University,,,,Internal functionality of visual invariants,,,
Neocognitron,Vision,Character recognition,"K Fukushima, S Miyake",Highly cited,,,https://link.springer.com/article/10.1007/BF00344251,5782.00,Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position,1980-04-01,NHK Broadcasting Science Research Laboratories,Industry,1140576.00,"""The synaptic connections from S-layers to C-layers
are fixed and unmodifiable. [...]
The numbers of excitatory cells in these seven layers are: 16x16 in U0, 16x16x24 in Us1, 10x10x 24 in Uc1, 8x8x24 in Us2, 6x 6x 24 in Uc2, 2x2x24 in Us3, and 24 in Uc3 
[...]
 the number of input synapses to each S-cell is 5 x 5 in layer Us1 and 5x5x24 in layers Us2 and Us3
[...]
The number of excitatory input synapses to each C-cell is 5x5 in layers Uc1 and Uc2, and is 2x2 in
layer Uc3
""

The number of synapses into each S-layer is:

S1: (16*16*24)*(5*5) 
S2: (8*8*24)*(5*5*24)
S3: (2*2*24)*(5*5*24)

We assume one parameter a per synapse into each cell in a S-layer, and one parameter b per each cell in a S-layer.",228115200.00,"""It does not necessarily mean that all of these input synapses are
always fully reinforced. In usual situations, only some of these input synapses are reinforced, and the rest of them remains in small values [...] Each of the five stimulus patterns has been presented 20 times to the network. By that time, self organization of the network has almost been completed.""

We multiply by 2 to account for multadds
",,,5,"""In order to self-organize the network, we have presented five stimulus patterns ""0"", ""1"", ""2"", ""3"", and ""4"", which are shown in Fig. 6""",,,,,,,,,,,Industry,,,2023-11-26 03:25,Robi Rahman,,,Japan,NHK Broadcasting Science Research Laboratories,,,,Neocognitron,,,
Kohonen network,Other,Dimensionality reduction,T Kohonen,Highly cited,,,https://link.springer.com/article/10.1007/BF00337288,11841.00,Self-organized formation of topologically correct feature maps,1981-07-25,Helsinki University of Technology,Academia,4096.00,"The input vectors are 3D.
I could not find the grid size, but from the images it looks 8x8.
So the network was 8x8x3 parameters.",,,,,,??? Seemingly no info,,,,,,,,,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,Finland,Helsinki University of Technology,,,,Kohonen network,,,
Hopfield network,Other,Sequence memorization,JJ Hopfield,Highly cited,,,https://www.pnas.org/doi/10.1073/pnas.79.8.2554,23315.00,Neural networks and physical systems with emergent collective computational abilities,1982-04-01,California Institute of Technology,Academia,9900.00,"My understanding is that the biggest Hopfield networks they studied had N=100 units. 

Each unit has 99 synapses Tij from each other unit, for a total of 100*99 parameters",,,,,,,,,,,,,,,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,United States of America,California Institute of Technology,,,,Hopfield network,,,
ASE+ACE,Games,Pole balancing,"Andrew G. Barto, Richard S. Sutton, and Charles W. Anderson",Highly cited,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6313077,4296.00,Neuronlike adaptive elements that can solve difficult learning control problems,1983-09-01,Stanford University,Academia,324.00,,,,,,,,,,,,,,,,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,United States of America,Stanford University,,,,ASE+ACE,,,
Learnability theory of language development,Language,,Steven Pinker,Highly cited,,,https://psycnet.apa.org/record/1985-97439-000,4730.00,Language learnability and language development,1984-07-01,Massachusetts Institute of Technology (MIT),Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,United States of America,Massachusetts Institute of Technology (MIT),,,,Learnability theory of language development,,,
Error Propagation,,,"D. E. Rumelhart, G. E. Hinton, and R. J. Williams",Highly cited,,,https://stanford.edu/~jlmcc/papers/PDP/Volume%201/Chap8_PDP86.pdf,27322.00,Learning internal representations by error propagation,1986-01-03,"UC San Diego,Carnegie Mellon University (CMU)",Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,"United States of America,United States of America","UC San Diego, Carnegie Mellon University (CMU)",,,,Error Propagation,,,
Learning past tenses,Language,Verb conjugation,"Rumelhart, D. E., & McClelland, J. L",,,,https://www.semanticscholar.org/paper/On-learning-the-past-tenses-of-English-verbs%3A-rules-Rumelhart-McClelland/4fa569625b5ab35e955a8d5be11a4aa9f59ca424,318.00,Learning the past tenses of English verbs: Implicit rules or parallel distributed processing?,1986-01-03,Stanford University,Academia,211600.00,"Source: https://files.eric.ed.gov/fulltext/ED267419.pdf
p.9: network architecture is given, with two layers of hidden units. The hidden units are  called “Wickelfeature representation”. The “modifiable connections” are only between the hidden units. p.19: “All in all then, we used only 460 of the 1,210 possible Wickelfeatures. Using this representation, a verb is represented by a pattern of activation over a set of 460 Wickelfeature units.""",,,,,,,,,,,,,,,,,Industry,,,2023-11-21 01:39,Robi Rahman,,,United States of America,Stanford University,,,,,,,
PDP model for serial order,,,"Jordan, M.I.",Highly cited,,,https://www.osti.gov/biblio/6910294,1502.00,Serial order: A parallel distributed processing approach,1986-01-05,UC San Diego,Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,United States of America,UC San Diego,,,,PDP model for serial order,,,
Back-propagation,Other,Learning to complete triples,"Rumelhart, David E.; Hinton, Geoffrey E.; Williams, Ronald J.",Highly cited,,,https://www.semanticscholar.org/paper/Learning-representations-by-back-propagating-errors-Rumelhart-Hinton/052b1d8ce63b07fec3de9dbb583772d860b7c769,25301.00,Learning representations by back-propagating errors,1986-10-01,"UC San Diego,Carnegie Mellon University (CMU)",Academia,144.00,Figure 4 includes a representation of the weights learned by the people to relationship network,124416000.00,"We assume that the number of mult-adds per pass is equal to the number of parameters.

""We trained the network for 1500 sweeps""

There are 12*12 possible pairs of people, so we assume that is the dataset size",,,144,"There are 12*12 possible pairs of people, so we assume that is the dataset size",,288.00,We assume that the number of mult-adds is equal to the number of parameters.,,,,,Unsupervised,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,"United States of America,United States of America","UC San Diego, Carnegie Mellon University (CMU)",,,,Back-propagation,,,
Optimized Multi-Scale Edge Detection,Vision,,John Canny,Highly cited,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4767851,37931.00,A Computational Approach To Edge Detection,1986-11-01,Massachusetts Institute of Technology (MIT),Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,United States of America,Massachusetts Institute of Technology (MIT),,,,Optimized Multi-Scale Edge Detection,,,
NetTalk (transcription),Speech,Speech synthesis,"TJ Sejnowski, CR Rosenberg",Highly cited,,,http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=03A3D3EDF0BAF35405ABCF083411B55E?doi=10.1.1.154.7012&rep=rep1&type=pdf,2558.00,Parallel Networks that Learn to Pronounce English Text,1987-06-06,Princeton University,Academia,18629.00,"""The connections in the network are specified by a total of 18629
weight parameters (including a variable threshold for each unit)""",28328002560.00,18629 params * 2 FLOP/param * (3 for forward + backward pass) * 55 epochs * 1024 words/epoch * 4.5 letters/word,,,1024,"We used the first two pages of transcriptions, which contained 1024 words from a child in firstgrade",55.00,,,,,,,,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,United States of America,Princeton University,,,,NetTalk (transcription),,,
NetTalk (dictionary),Speech,Speech synthesis,"TJ Sejnowski, CR Rosenberg",Highly cited,,,http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=03A3D3EDF0BAF35405ABCF083411B55E?doi=10.1.1.154.7012&rep=rep1&type=pdf,2558.00,Parallel Networks that Learn to Pronounce English Text,1987-06-06,Princeton University,Academia,18629.00,"""The connections in the network are specified by a total of 18629 weight parameters (including a variable threshold for each unit)""",27664065000.00,18629 params * 2 FLOP/param * (3 for forward + backward pass) * 55 epochs * 1000 words/epoch * 4.5 letters/word,,,1000,"""A subset of the 1000 most commonly occurring words was selected from this dictionary based on frequency counts in the Brown corpus""",55.00,,,,,,,,,,,,,2023-11-26 03:25,Anonymous,,,United States of America,Princeton University,,,,NetTalk (dictionary),,,
Motion-Driven 3D Feature Tracking,Vision,,Harris & Stephens,Highly cited,,,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.434.4816&rep=rep1&type=pdf,19068.00,A Combined Corner and Edge Detector,1988-07-01,Roke Manor Research,Industry,,"""The simulation studies reported here all involved a 16-bit input pattern. """,,,,,1500,"""The total number of possible input patterns was 65,536. Training sets of 650 and 1500 patterns picked at random from this total were used.""",,,,,,,,,,,Industry,,,2023-11-26 03:25,Robi Rahman,,,United Kingdom of Great Britain and Northern Ireland,Roke Manor Research,,,,Motion-Driven 3D Feature Tracking,,,
Adaptive Broom Balancer,Games,Pole balancing,"VV Tolat, B Widrow",,,,https://ieeexplore.ieee.org/document/23982,80.00,An Adaptive “Broom Balancer” with Visual Inputs,1988-07-24,Stanford University,Academia,110.00,Figure 3,,,,,,,,,,,,,,,,,Academia,,,2023-11-21 01:39,Robi Rahman,,,United States of America,Stanford University,,,,,,,
MADALINE II,Other,Pattern classification,"Rodney Winter, Bernard Widrow",,,,https://ieeexplore.ieee.org/document/23872,81.00,MADALINE RULE II: A Training Algorithm for Neural Networks,1988-07-24,Stanford University,Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-11-21 01:39,Robi Rahman,,,United States of America,Stanford University,,,,,,,
Innervator,Other,Pattern classification,"Geoffrey Miller, Peter Todd, and Shailesh Hegde",Highly cited,,,https://www.researchgate.net/publication/220885651_Designing_Neural_Networks_using_Genetic_Algorithms,1132.00,Designing neural networks using genetic algorithms,1989-01-01,"Stanford University,California Institute of Technology",Academia,10.00,Each net has 5 units,120000000.00,10 params * 6 FLOP/param/pass * 4 datapoints * 1000 epochs * 50 individuals * 10 generations,,,4,,,,,,,,,,,,,,,2023-11-26 03:25,Robi Rahman,,,"United States of America,United States of America","Stanford University, California Institute of Technology",,,,Innervator,,,
Q-learning,,,Christopher Watkins,Highly cited,,,http://www.cs.rhul.ac.uk/~chrisw/thesis.html,8025.00,Learning from delayed rewards,1989-01-01,University of London,Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,United Kingdom of Great Britain and Northern Ireland,University of London,,,,Q-learning,,,
Time-delay neural networks,,,"A. Waibel, T. Hanazawa, G. Hinton, K. Shikano, and K. J. Lang",Highly cited,,,https://ieeexplore.ieee.org/abstract/document/21701,3445.00,Phoneme recognition using time-delay neural networks,1989-03-03,"Advanced Telecommunications Research Institute,Carnegie Mellon University (CMU)",Industry - Academia Collaboration,,,,,,,,,,,,,,,,,,,Industry,,,2023-11-26 03:25,Robi Rahman,,,"Japan,United States of America","Advanced Telecommunications Research Institute, Carnegie Mellon University (CMU)",,,,Time-delay neural networks,,,
Universal approximation via Feedforward Networks,,,Kurt Hornik & Maxwell Stinchcombe & Halbert White,Highly cited,,,https://www.sciencedirect.com/science/article/abs/pii/0893608089900208,21663.00,Multilayer feedforward networks are universal approximators,1989-03-09,"UC San Diego,Technische Universität Wien",Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,"United States of America,Austria","UC San Diego, Technische Universität Wien",,,,Universal approximation via Feedforward Networks,,,
Zip CNN,Vision,Character recognition,Y. LeCun B. Boser J. S. Denker D. Henderson R. E. Howard W. Hubbard L. D. Jackel,Highly cited,,,https://ieeexplore.ieee.org/document/6795724,9704.00,Backpropagation applied to handwritten zip code recognition,1989-12-01,"AT&T,Bell Laboratories",Industry,9760.00,"""In summary, the network has 1256 units, 64,660 connections, and 9760 independent parameters""",43372117520.00,"Its a deep CNN so we assume a backward-forward ratio of 2:1

""The network was trained for 23
passes through the training set (167,693 pattern presentations).""",Buffalo zips,"""The data base used to train and test the network consists of 9298 segmented numerals digitized from handwritten zip codes
that appeared on U.S. mail passing through the Buffalo, NY post office.
Examples of such images are shown in Figure 1. The digits were written
by many different people, using a great variety of sizes, writing styles,
and instruments, with widely varying amounts of care; 7291 examples
are used for training the network and 2007 are used for testing the generalization performance. One important feature of this data base is that
both the training set and the testing set contain numerous examples that
are ambiguous, unclassifiable, or even misclassified. """,7291,"The digits were written
by many different people, using a great variety of sizes, writing styles,
and instruments, with widely varying amounts of care; 7291 examples
are used for training the network and 2007 are used for testing the generalization performance",,129320.00,Roughly twice the number of connections,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,"United States of America,United States of America","AT&T, Bell Laboratories",,,,Zip CNN,,,
ALVINN,Driving,,DA Pomerleau,Highly cited,,,https://proceedings.neurips.cc/paper/1988/hash/812b4ba287f5ee0bc9d43bbf5bbe87fb-Abstract.html,1863.00,ALVINN: an autonomous land vehicle in a neural network,1989-12-01,Carnegie Mellon University (CMU),Academia,3994.00,http://www.cs.cmu.edu/afs/cs.cmu.edu/academic/class/15782-f06/slides/alvinn.pdf,81187041441.21,Extracted from AI and Compute (https://openai.com/blog/ai-and-compute/) charts by using https://automeris.io/WebPlotDigitizer/.,Road snapshots,,1200,"""Training involves first creating a set of 1200 road snapshots depicting roads with a wide variety of retinal orientations and positions, under a variety of lighting conditions and with realistic noise levels""",,,,,,,,,,,Academia,,,2023-12-05 04:33,Robi Rahman,,,United States of America,Carnegie Mellon University (CMU),,,,ALVINN,,,
MADALINE III,,,"B Widrow, M. A. Lehr",Highly cited,,,https://ieeexplore.ieee.org/document/58323,3013.00,"30 years of adaptive neural networks: perceptron, madaline, and backpropagation",1990-09-01,Stanford University,Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,United States of America,Stanford University,,,,MADALINE III,,,
MLP as Bayesian Approximator,,,D.W. Ruck & S.K. Rogers & M. Kabrisky & M.E. Oxley & B.W. Suter,Highly cited,,,https://ieeexplore.ieee.org/abstract/document/80266,1046.00,The multilayer perceptron as an approximation to a Bayes optimal discriminant function,1990-12-01,Air Force Institute of Technology,Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,United States of America,Air Force Institute of Technology,,,,MLP as Bayesian Approximator,,,
DIABETES,Other,Medical diagnosis,"S. Andreassen, R. Hovorka, J. Benn, K. G. Olesen, and E. R. Carson",,,,https://link.springer.com/chapter/10.1007/978-3-642-48650-0_19,132.00,A Model-based Approach to Insulin Adjustment,1991-06-24,"Aalborg University,University of London",Academia,429409.00,From https://www.bnlearn.com/bnrepository/,,,,,,,,,,,,,,,,,Academia,,,2023-11-21 01:39,Robi Rahman,,,"Denmark,United Kingdom of Great Britain and Northern Ireland","Aalborg University, University of London",,,,,,,
SRN-Encoded Grammatical Structures,Language,,J. L. Elman,Highly cited,,,https://dl.acm.org/doi/10.1007/BF00114844,1717.00,"Distributed representations, simple recurrent networks, and grammatical structure",1991-09-01,UC San Diego,Academia,,,,,,,177805,4 training sets of 10k sentences each. Total number of words calculated by multiplying 10k and the avg. number of words per sentence in the training set.,,,,,,,,,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,United States of America,UC San Diego,,,,SRN-Encoded Grammatical Structures,,,
REINFORCE in Stochastic Connectionism,,,R. J. Williams,Highly cited,,,https://dl.acm.org/doi/10.1007/BF00992696,7559.00,Simple statistical gradient-following algorithms for connectionist reinforcement learning,1992-05-01,Northeastern University,Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-12-05 04:33,Robi Rahman,,,United States of America,Northeastern University,,,,REINFORCE in Stochastic Connectionism,,,
TD-Gammon,Games,Backgammon,G Tesauro,Highly cited,,,https://papers.nips.cc/paper/1991/file/68ce199ec2c5517597ce0a4d89620f55-Paper.pdf,1344.00,Practical Issues in Temporal Difference Learning,1992-05-01,IBM,Industry,25000.00,"""The best performance was obtained with a network containing 80 hidden units and over 25,000 weights.""",18232157622832.70,Extracted from AI and Compute (https://openai.com/blog/ai-and-compute/) charts by using https://automeris.io/WebPlotDigitizer/.,,,6300000,"""This network was trained
for over 300,000 training games""

Each backgammon game has an avg of around 21 movements
https://www.bkgm.com/rgb/rgb.cgi?view+712",,,,,,,,,,,Industry,,,2023-11-26 03:25,Robi Rahman,,,Multinational,IBM,,,,TD-Gammon,,,
Fuzzy NN,Speech,Speech recognition,"SK Pal, S Mitra",Highly cited,,,https://ieeexplore.ieee.org/document/159058,1223.00,"Multilayer perceptron, fuzzy sets, and classification",1992-09-01,Indian Statistical Institute,Academia,1166.00,"Table II: ""he neural network has three hidden layers, with m hidden nodes in each layer"", m = 20, input dim. = 9, output dim. = 6",1403117760.00,1166 params * 2 FLOP/param * (3 for forward + backward pass) * 460 epochs * 436 examples,,,436,"""The above-mentioned algorithm was tested on a set of 871 Indian Telugu vowel sounds"" and 50% of the dataset was used. 871*0.5 ~= 436",,,,,,,,,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,India,Indian Statistical Institute,,,,Fuzzy NN,,,
IBM-5,Language,Translation,"Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, Robert L. Mercer",Highly cited,,,https://dl.acm.org/doi/10.5555/972470.972474,5752.00,The Mathematics of Statistical Machine Translation: Parameter Estimation,1993-06-15,IBM,Industry,1658364.00,"The model is initiallized with 2.44E+09 translation probabilities, which are progressively culled until 1,658,364 remain. There are other parameters in the models (eg the fertility probabilities that relate each word in the input to the number of words it will align to) but the parameter count is dominated by the translation probabilities.",,,Proceedings of the Canadian parliament,,53358600,"""They used the algorithm to extract a large number of translations from several years of the proceedings of the Canadian parliament. From these translations, we have chosen as our training data those for which both the English sentence and the French sentence are 30 or fewer words in length. This is a collection of 1,778,620 translations.""",,,,,,,,,,,Industry,,,2023-11-26 03:25,Robi Rahman,,,Multinational,IBM,,,,IBM-5,,,
Markov-driven POS tagger,Language,Part-of-speech tagging,Bernard Merialdo,,,,https://dl.acm.org/doi/10.5555/972525.972526,788.00,Tagging English Text with a Probabilistic Model,1994-06-01,EURECOM,Academia,2447124.00,"""The total number of free parameters is then:
(Nw - 1).NT + (NT - 1).NT.NT.""
Where:
Nw= Vocabulary size
NT = Number of tags

""In the treebank 159 different tags are used. These tags were projected on a smaller system of 76 tags designed by Evelyne Tzoukermann and Peter Brown (see Appendix). The results quoted in this paper all refer to this smaller system""
So NT = 76

https://www.aclweb.org/anthology/J94-2001/

There is no direct reference to Nw, but the data is from ""Lexicon and grammar in probabilistic tagging of written English."" which says

""(the new CLAWS lexicón has almost 26,500 entries)""
So tentatively Nw=26500

https://dl.acm.org/doi/10.3115/982023.982049",,,,,1000000,"""We use the ""treebank"" data described in Beale (1988). It contains 42,186 sentences (about one million words) from the Associated Press.""
https://www.aclweb.org/anthology/J94-2001.pdf",,,,,,,,,,,Academia,,,2023-11-21 01:39,Robi Rahman,,,France,EURECOM,,,,,,,
GroupLens,Recommendation,,"Paul Resnick, Neophytos Iacovou, Mitesh Suchak, Peter Bergstrom, John Riedl",Highly cited,,,https://dl.acm.org/doi/10.1145/192844.192905,7733.00,GroupLens: an Open Architecture for Collaborative Filtering of Netnews,1994-10-22,Massachusetts Institute of Technology (MIT),Academia,,"For each pair of users, the system computes the correlation between their scores in the articles they have rated.

Then to make the prediction of a score for a given article and user the system computes a weighted average taking into account the correlations with each other user, the average rating of each user and the average rating of the article.

So the system in total has n+m+n*n ~= n*n parameters, where n is the number of users and m is the number of articles.

To address scaling issues, the system is partioned into clusters of users. It's very unclear what is the number of users per cluster, though the Daily ratings traffic table provided suggests that is around 10k users ",,,,,100000000,"For each pair of users, the system computes the correlation between their scores in the articles they have rated.

Then to make the prediction of a score for a given article and user the system computes a weighted average taking into account the correlations with each other user, the average rating of each user and the average rating of the article.

So the system in total has n+m+n*n ~= n*n parameters, where n is the number of users and m is the number of articles.

To address scaling issues, the system is partioned into clusters of users. It's very unclear what is the number of users per cluster, though the Daily ratings traffic table provided suggests that is around 10k users ",,,,,,,,,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,United States of America,Massachusetts Institute of Technology (MIT),,,,GroupLens,,,
Multi-cause Binary Clustering,,,Eric Saund,,,,https://ieeexplore.ieee.org/document/6795568,176.00,A Multiple Cause Mixture Model for Unsupervised Learning,1995-01-01,Xerox,Industry,,,,,,,,,,,,,,,,,,,Industry,,,2023-11-21 01:39,Robi Rahman,,,United States of America,Xerox,,,,,,,
Iterative Bootstrapping WSD,Language,,D Yarowsky,Highly cited,,,https://dl.acm.org/doi/10.3115/981658.981684,2996.00,Unsupervised Word Sense Disambiguation Rivaling Supervised Methods,1995-06-26,University of Pennsylvania,Academia,,,,,,,460000000,the data were extracted from a 460 million word corpus,,,,,,,,,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,United States of America,University of Pennsylvania,,,,Iterative Bootstrapping WSD,,,
Random Decision Forests,,,TK Ho,Highly cited,,,https://ieeexplore.ieee.org/document/598994,4678.00,Random decision forests,1995-08-14,"AT&T,Bell Laboratories",Industry,,,,,MNIST,,60000,The images are from the 1992 NIST (National Institute of Standards and Technology) Competition,,,,,,,,,,,Industry,,,2023-11-26 03:25,Robi Rahman,,,"United States of America,United States of America","AT&T, Bell Laboratories",,,,Random Decision Forests,,,
Support Vector Machines,Vision,Image Classification,"C Cortes, V Vapnik",Highly cited,,,https://link.springer.com/article/10.1007/BF00994018,48968.00,Support-Vector Networks,1995-09-01,"AT&T,Bell Laboratories",Industry,100000000.00,"Section 6.2.2: ""...polynomials
of degree 4 (that have more than 10^8 free parameters)...""
They used 4-degree polynomials for MNIST",,,MNIST,,60000,"Section 6.2: ""The large database consists of 60,000 training and 10,000 test patterns""",,,,,,,,,,,Industry,,,2023-11-26 03:25,Robi Rahman,,,"United States of America,United States of America","AT&T, Bell Laboratories",,,,Support Vector Machines,,,
System 11,Vision,Face detection,"HA Rowley, S Baluja, T Kanade",Highly cited,,,https://ieeexplore.ieee.org/document/655647,6011.00,Neural Network-Based Face Detection,1996-06-18,Carnegie Mellon University (CMU),Academia,6452.00,"System 11 is a combination of Network 1 and Network 2

Network 1 has 2095 connections and network 2 has 4357 connections (see table 1)",12930000000.00,"Since there is no parameter sharing, the forward compute is roughly twice that of the number of parameters. We use a 2:1 forward-backward ratio as this is a shallow network, with most connections in the first layer.

Number of passes (Section 2.1):
* ""Nearly 1,050 face examples were gathered from face databases [...]""
* ""Fifteen face examples are generated for the training set from each original image""

Training loop:
1. ""initial set of nonface images by generating 1,000 random images""
2. Train (presumably on whole set)
3. Run + collect false positives
4. ""Select up to 250 of these subimages [...] and add them into the training set [...] Go to step 2""

""A typical training run selects approximately 8,000 nonface images ""

Selecting 8,000 nonface images implies 8000/250 = 32 loops.

Assuming compute is 3 * N * D, we have
* Loop 1: D = 15*1050 + 1000
* Loop 2: D = 15*1050 + 1000 + 250
* So on.

Hence D overall is 32*(15*1050 + 1000) + 250*32/2*(32+1) = 668,000.

Hence compute = 3 * 6452 * 668e3 = 1.3e10.",,,9050,"""A typical training
run selects approximately 8000 non-face images from the
146,212,178 subimages that are available at all locations
and scales in the training scenery images.""

""Nearly 1050 face examples were gathered from face databases at CMU and Harvard [...] In the training set,15 face examples are generated from each
original image [...]""

""Create an initial set of non-face images by generating
1000 images with random pixel intensities""",,12904.00,The connections are linear so roughly twice the number of parameters,,,,,,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,United States of America,Carnegie Mellon University (CMU),,,,System 11,,,
HMM Word Alignment,Language,Word alignment,"Stephan Vogel, Hermann Ney, Christoph Tillmann",Highly cited,,,https://dl.acm.org/doi/10.3115/993268.993313,1099.00,HMM-Based Word Alignment in Statistical Translation,1996-08-05,University of Erlangen - Nuremburg,Academia,,,,,,,442316,"[WORDS]
Table 1.
I take the sum of all words. Maybe it would be better to use only the sum of English or German words?",,,,,,,,Supervised,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,Germany,University of Erlangen - Nuremburg,,,,HMM Word Alignment,,,
SVM for face detection,Vision,,"E. Osuna, R. Freund, F. Girosi",Highly cited,,,https://ieeexplore.ieee.org/document/609310,3851.00,Training Support Vector Machines: An Application to Face Detection,1997-06-17,Massachusetts Institute of Technology (MIT),Academia,,,,,,,50000,"Section 1: ""The problem that we have to solve involves training a classifier
to discriminate between face and non-face patterns, using a
data set of 50,000points. """,,,,,,,,,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,United States of America,Massachusetts Institute of Technology (MIT),,,,SVM for face detection,,,
n-gram LM,Language,,"P Clarkson, R Rosenfeld",,,,https://www.semanticscholar.org/paper/Statistical-language-modeling-using-the-toolkit-Clarkson-Rosenfeld/fdf4aa623e4d5b5edaeb873ed8e8b1cef0b59c87,954.00,Statistical language modeling using the CMU-Cambridge toolkit,1997-07-01,"University of Cambridge,Carnegie Mellon University (CMU)",Academia,,,,,,,,,,,,,,,,Supervised,,,Academia,,,2023-11-21 01:39,Robi Rahman,,,"United Kingdom of Great Britain and Northern Ireland,United States of America","University of Cambridge, Carnegie Mellon University (CMU)",,,,,,,
Bidirectional RNN,Speech,Speech recognition,"M. Schuster, KK Paliwal",Highly cited,,,https://ieeexplore.ieee.org/document/650093,7023.00,Bidirectional recurrent neural networks,1997-11-01,Advanced Telecommunications Research Institute,Industry,13000.00,"Page 7: ""The structures of all networks are adjusted so that
each of them has about the same number of free parameters
(approximately 13 000 here""",,,TIMIT,,73920,"""the training data set consisting of 3696 sentences
from 462 speakers""

Assuming avg sentence length of 20 words

3696 * 20 total words",,,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,Japan,Advanced Telecommunications Research Institute,,,,Bidirectional RNN,,,
LSTM,Language,Language modelling,Sepp Hochreiter ; Jurgen Schmidhuber,Highly cited,,,https://direct.mit.edu/neco/article-abstract/9/8/1735/6109/Long-Short-Term-Memory?redirectedFrom=fulltext,70674.00,Long short-term memory,1997-11-15,Technical University of Munich,Academia,10504.00,"Table 2

http://www.bioinf.jku.at/publications/older/2604.pdf",21008000000000.00,"""Due to limited computation time, training is stopped after 5 million sequence presentations""

Each sequence has p=100 elements in the long-delay setting.

COMPUTE = PRESENTATIONS * PRESENTATION LENGTH * UPDATE COMPUTE PER TOKEN",,,1273000,"Table 8. The rightmost column lists numbers of training sequences required to achieve the stopping
criterion.

This applies to experiment 5 (multiplication)

Sequences have random lengths, on the order of 100-1000 (table 7 )",,42016.00,"Appendix A.1
""LSTM's update complexity per time is [...] K + 2KH + KC + 2KSC + H I + C I + 4CSI steps [...] where K is the number of output units, C is the number of memory cell blocks, S > 0 is the size of the memory cell blocks, H is the number of hidden units, I is the (maximal) number of units forward-connected to memory cells, gate units and hidden units""

""W = KH + KCS + CSI + 2C is the number of weights""

So the update complexity is roughly twice the number of weights.

The authors take 1 FMA = 1 step, so this is roughly 4*W FLOP",,,,,,,,Academia,,,2023-12-05 04:33,Robi Rahman,,,Germany,Technical University of Munich,,,,LSTM,,,
Sparse coding model for V1 receptive fields,,,"Bruno A. Olshausen, David J. Field",Highly cited,,,https://www.sciencedirect.com/science/article/pii/S0042698997001697,4257.00,Sparse coding with an overcomplete basis set: A strategy employed by V1?,1997-12-01,"UC Davis,Cornell University",Academia,,,,,,,10,"In Simulation Methods: ""The data for training were taken from ten 512 × 512
pixel images of natural surroundings""",,,,,,,,,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,"United States of America,United States of America","UC Davis, Cornell University",,,,Sparse coding model for V1 receptive fields,,,
RNN for speech,Speech,Speech synthesis,"SH Chen, SH Hwang, YR Wang",,,,https://ieeexplore.ieee.org/abstract/document/668817,231.00,An RNN-based prosodic information synthesizer for Mandarin text-to-speech,1998-05-15,National Chiao Tung University,Academia,7512.00,"""The RNN generated a total of eigt output prosodic parameters. [...] The numbers of nodes in the first and second hidden layers were determined empirically and set to be 35 and 30, respectively""

Figure 1 contains an overview of the architecture.

Layer 1: (102 + 35 + 1)*35 parameters
Layer 2: (43 + 35 + 1)*30 parameters
Output layer: (30+8+1)*8 parameters",226690156032.02,Extracted from AI and Compute (https://openai.com/blog/ai-and-compute/) charts by using https://automeris.io/WebPlotDigitizer/.,,,14096,"The data base was divided into two parts: a training set and an open test set. These two sets consisted of 28 191 and 7051 syllables,
respectively.

Of the top 10,000 Chinese words, 15% have 1 syllable, 78% have 2 syllables, and 7% have more than two syllables. Assuming 2 syllables per word, the training set is around 14100 words.",,,,,,,,,,,Academia,,,2023-11-23 06:53,Robi Rahman,,,Taiwan,National Chiao Tung University,,,,,,,
Probabilistic modeling for object recognition,Vision,Face recognition,"H Schneiderman, T Kanade",,,,https://ieeexplore.ieee.org/document/698586,602.00,Probabilistic modeling of local appearance and spatial relationships for object recognition,1998-06-23,Carnegie Mellon University (CMU),Academia,,,,,,,120472,"Section 5.1: ""We formed training sets from 991 faces images and 1,552
non-face images.""
""For each face image we generated
120 synthetic variations""",,,,,,,,,,,Academia,,,2023-11-21 01:39,Robi Rahman,,,United States of America,Carnegie Mellon University (CMU),,,,,,,
Social and content-based classification,Other,Recommender system,"C Basu, H Hirsh, W Cohen",Highly cited,,,https://www.aaai.org/Papers/AAAI/1998/AAAI98-101.pdf,1564.00,Recommendation as Classification: Using Social and Content-based Information in Recommendation,1998-07-01,"AT&T,Bell Laboratories,Rutgers University",Industry - Academia Collaboration,,,,,,,45000,"""Our data set consists of more than 45,000 movie rat-
ings collected from approximately 260 users.""",,,,,,,,,,,Industry,,,2023-11-26 03:25,Robi Rahman,,,"United States of America,United States of America,United States of America","AT&T, Bell Laboratories, Rutgers University",,,,Social and content-based classification,,,
LeNet-5,Vision,Character recognition,"Yann LeCun, Léon Bottou, Yoshua Bengio, Patrick Haffner",Historical significance,,,http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf,44745.00,Gradient-based Learning Applied to Document Recognition,1998-11-01,AT&T,Industry - Academia Collaboration (Industry leaning),60000.00,"""[LeNet5] contains 390408 connections, but only 60000 trainable free parameters because of the weight sharing""",2810937600000.00,"""[LeNet5] contains 390408 connections"" = multiply-adds
MNIST - 60,000 data points
20 epochs",MNIST,,60000,"The MNIST database contains 60,000 training images and 10,000 testing images (Wikipedia)",,780816.00,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,United States of America,AT&T,,,,LeNet-5,,,
LSTM with forget gates,,,"F. A. Gers, J. Schmidhuber, and F. Cummins",Highly cited,,,https://ieeexplore.ieee.org/document/818041,5681.00,Learning to forget: Continual prediction with LSTM,1999-01-02,IDSIA,Academia,276.00,See Table 1,,,,,30000,"Training was stopped after at most 30000
training streams, each of which was ended
when the first prediction error or the
100000th successive input symbol occurred

NOTE this is a weird task. Not sure how to measure dataset size (#seqs? #symbols?)",,,,,,,,,,,Academia,,,2023-12-05 04:33,Robi Rahman,,,Switzerland,IDSIA,,,,LSTM with forget gates,,,
IBM Model 4,Language,Translation,"Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin Knight, John Lafferty, Dan Melamed, Franz-Josef Och, David Purdy, Noah A. Smith, and David Yarowsky",Highly cited,,,http://www-i6.informatik.rwth-aachen.de/publications/download/266/al-onaizan--1999.pdf,1921.00,Statistical machine translation,1999-07-02,"University of Southern California,IBM,University of Pennsylvania",Industry - Academia Collaboration (Academia leaning),,,,,,,800000,"[WORDS]
See FIgure 6",,,,,,,,,,,Industry,,,2023-11-26 03:25,Robi Rahman,,,"United States of America,Multinational,United States of America","University of Southern California, IBM, University of Pennsylvania",,,,IBM Model 4,,,
Perceptron for Large Margin Classification,Vision,Character recognition,Yoav Freund & Robert E. Schapire,Highly cited,,,https://link.springer.com/article/10.1023/A:1007662407062,1731.00,Large Margin Classification Using the Perceptron Algorithm,1999-12-01,"UC San Diego,Shannon Laboratory,AT&T",Industry,,,,,MNIST,,60000,"""The dataset consists of 60,000 training examples and 10,000 test examples.""",,,,,,,,,,,Industry,,,2023-11-26 03:25,Robi Rahman,,,"United States of America,United States of America,United States of America","UC San Diego, Shannon Laboratory, AT&T",,,,Perceptron for Large Margin Classification,,,
SVD in recommender systems,Recommendation,,"B Sarwar, G Karypis, J Konstan, J Riedl",Highly cited,,,http://robotics.stanford.edu/~ronnyk/WEBKDD2000/papers/sarwar.pdf,2126.00,Application of Dimensionality Reduction in Recommender System -- A Case Study,2000-07-14,University of Minnesota,Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,United States of America,University of Minnesota,,,,SVD in recommender systems,,,
Peephole LSTM,Other,Periodic function approximation,F.A. Gers; J. Schmidhuber,,,,https://ieeexplore.ieee.org/document/861302,630.00,Recurrent nets that time and count,2000-07-26,IDSIA,Academia,17.00,"""In absence of the 3 peephole connections there are 14 adjustable weights""",,,,,64970000,See Table 2,,,,,,,,,,,Academia,,,2023-11-21 01:39,Robi Rahman,,,Switzerland,IDSIA,,,,,,,
FrameNet role labeling,Language,,"Daniel Gildea, Daniel Jurafsky",Highly cited,,,https://dl.acm.org/doi/10.1162/089120102760275983,2499.00,Automatic Labeling of Semantic Roles,2000-09-01,University of Rochester,Academia,,,,,FrameNet,,50000,"Abstract: ""The system is based on statistical classifiers trained on roughly 50,000 sentences""",,,,,,,,,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,United States of America,University of Rochester,,,,FrameNet role labeling,,,
Immediate trihead,Language,,E Charniak,SOTA Improvement,"""The perplexity for both of these models significantly improve
upon the trigram model base-line as
well as the best previous grammar based language model""",,https://dl.acm.org/doi/10.3115/1073012.1073029,422.00,Immediate-Head Parsing for Language Models,2001-07-06,Brown University,Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,United States of America,Brown University,,,,Immediate trihead,,,
Gradient Boosting Machine,,,Jerome H. Friedman,Highly cited,,,https://projecteuclid.org/journals/annals-of-statistics/volume-29/issue-5/Greedy-function-approximation-A-gradient-boostingmachine/10.1214/aos/1013203451.full,17891.00,Greedy function approximation: A gradient boosting machine,2001-10-01,Stanford University,Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,United States of America,Stanford University,,,,Gradient Boosting Machine,,,
Decision tree (classification),Vision,Face recognition,"P. Viola, M. Jones",Highly cited,,,https://www.cs.cmu.edu/~efros/courses/LBMV07/Papers/viola-cvpr-01.pdf,23449.00,Rapid object detection using a boosted cascade of simple features,2001-12-08,"Mitsubishi Electric Research Labs,Compaq CRL",Industry - Academia Collaboration,120000000.00,"From table 1, it looks like the number of weights depends on the dataset size, which in this case is 2*4916 faces+9544 non-faces = 19376, and multiplies that by the number of filters T = 6061, so no. of params = 1.2e8 (Note:I think ""features"" = ""filters"" in this paper)",63000000000000.00,"
The training compute can be tediously worked out from the pseudocode. I think for dataset size D, number of filters T, the training compute is roughly 180k * D * 3 * T = 6.3e13 FLOPs",,They scraped the dataset personally for training,14460,Section 5: 4916 hand labeled faces  + 9544 non-face images = 14460,,67000000.00,"The inference compute depends on the image - because the algorithm works via pass/fail conditions of the decision tree, I think the compute varies a lot (e.g. if the first image fails then little compute is needed). They claim to take about 0.067s to classify an image using a 700MHz Pentium III processor - I'm not sure about how many FLOPs this required but an estimate is 1e9 FLOPs, which works out to 6.7e7 FLOPs for inference",,,,,,,,Industry,,,2023-11-26 03:25,Robi Rahman,,,"United States of America,United States of America","Mitsubishi Electric Research Labs, Compaq CRL",,,,Decision tree (classification),,,
Thumbs Up?,Language,Sentiment classification,"Bo Pang, Lillian Lee, Shivakumar Vaithyanathan",Highly cited,,,https://arxiv.org/abs/cs/0205070,10656.00,Thumbs up? Sentiment Classification using Machine Learning Techniques,2002-05-28,"Cornell University,IBM",Industry - Academia Collaboration,,,,,IMDb,,2053,"yielding a corpus of 752 negative and
1301 positive reviews",,,,,,,,,,,Industry,,,2023-11-26 03:25,Robi Rahman,,,"United States of America,Multinational","Cornell University, IBM",,,,Thumbs Up?,,,
Tagging via Viterbi Decoding,Language,,Michael Collins,Highly cited,,,https://dl.acm.org/doi/10.3115/1118693.1118694,2582.00,Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms,2002-06-01,AT&T,Industry,,,,,,,,,,,,,,,,,,,Industry,,,2023-11-26 03:25,Robi Rahman,,,United States of America,AT&T,,,,Tagging via Viterbi Decoding,,,
NEAT in neuroevolution,,,"Justin Bayer, Daan Wierstra, Julian Togelius, Jürgen Schmidhuber",Highly cited,,,https://direct.mit.edu/evco/article/10/2/99/1123/Evolving-Neural-Networks-through-Augmenting,3366.00,Evolving Neural Networks through Augmenting Topologies ,2002-06-01,IDSIA,Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,Switzerland,IDSIA,,,,NEAT in neuroevolution,,,
Joint Probability Machine Translation,Language,,Daniel Marcu and William Wong,,,,https://dl.acm.org/doi/10.3115/1118693.1118711,623.00,"A Phrase-Based, Joint Probability Model for Statistical Machine Translation",2002-06-01,University of Southern California,Industry - Academia Collaboration,,,,,Hansard Corpus,,1073480,"[WORDS]
""To evaluate our system, we trained [...] our joint
probability model on a French-English parallel corpus of 100,000 sentence pairs from the Hansard corpus. The sentences in the corpus were at most
20 words long. The English side had a total of 1,073,480 words (21,484 unique tokens). The French side had a total of 1,177,143 words (28,132
unique tokens)""",,,,,,,,,,,Industry,,,2023-11-21 01:39,Robi Rahman,,,United States of America,University of Southern California,,,,,,,
Maximum Entropy Models for machine translation,Language,Translation,Franz Josef Och and Hermann Ney,Highly cited,,,https://aclanthology.org/P02-1038/,1413.00,Discriminative Training and Maximum Entropy Models for Statistical Machine Translation,2002-07-06,"University of Southern California,RWTH Aachen",Academia,,,,,,,519523,"[WORDS]
Table 1",,,,,,,,,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,"United States of America,Germany","University of Southern California, RWTH Aachen",,,,Maximum Entropy Models for machine translation,,,
Web mining + Decision tree recommender,,,"YH Cho, JK Kim, SH Kim",,,,https://reader.elsevier.com/reader/sd/pii/S0957417402000520?token=155B6D1937982D7D0271AFD1CFB034DFD7F3D1DE816B66C025EBC9D0A305BA6DA685DD62989DC05246C794CAC74CDAEF&originRegion=us-east-1&originCreation=20220325235441,656.00,A personalized recommender system based on web usage mining and decision tree induction,2002-10-01,Korea Advanced Institute of Science and Technology (KAIST),Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-11-21 01:39,Robi Rahman,,,Korea (Republic of),Korea Advanced Institute of Science and Technology (KAIST),,,,,,,
Statistical Shape Constellations,Vision,Image Classification,"M. Weber, M. Welling, and P. Perona",Historical significance,,,https://link.springer.com/content/pdf/10.1007/3-540-45054-8_2.pdf,949.00,Unsupervised Learning of Models for Recognition,2003-01-01,California Institute of Technology,Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,United States of America,California Institute of Technology,,,,Statistical Shape Constellations,,,
LDA,Language,Document classification,"David M. Blei, Andrew Y. Ng, Michael I. Jordan",Highly cited,,,https://jmlr.org/papers/volume3/blei03a/blei03a.pdf,38724.00,Latent Dirichlet Allocation,2003-02-02,Stanford University,Academia,,,,,,,,Multiple experiments with different tasks and datasets,,,,,,,,,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,United States of America,Stanford University,,,,LDA,,,
NPLM,Language,Text autocompletion,"Yoshua Bengio, Réjean Ducharme, Pascal Vincent, Christian Jauvin",Highly cited,,,https://dl.acm.org/doi/10.5555/944919.944966,7627.00,A Neural Probabilistic Language Model,2003-03-15,Université de Montréal,Academia,11904264.00,"""The number of free parameters is |V|(1 + nm + h) + h(1 + (n − 1)m) [...] For example, consider the following architecture used in the experiments on the AP (Associated Press) news data: the vocabulary size is |V| = 17,964, the number of hidden units is h = 60, the order of the model is n = 6, the number of word features is m = 100""",1303898760000000.00,"""For example, consider the following architecture used in the experiments on the AP (Associated
Press) news data: the vocabulary size is |V| = 17,964, the number of hidden units is h = 60, the order
of the model is n = 6, the number of word features is m = 100. The total number of numerical operations to process a single training example is approximately |V|(1+nm+h)+h(1+nm)+nm""

The first 800,000 words were used for training... reducing the vocabulary size to |V| = 16,383

convergence of the stochastic gradient ascent procedure was obtained after around 10
to 20 epochs for the Brown corpus

NOTE: there are two corpuses. The one represented in this calculation is the Brown one, which got a better improvement over sota",Brown corpus,,1000000,"""Comparative experiments were performed on the Brown corpus which is a stream of 1,181,041 words, from a large variety of English texts and books. The first 800,000 words were used for training, the following 200,000 for validation (model selection, weight decay, early stopping) and the remaining 181,041 for testing. The number of different words is 47,578 (including punctuation, distinguishing between upper and lower case, and including the syntactical marks used to separate texts and paragraphs). Rare words with frequency ≤ 3 were merged into a single symbol, reducing the vocabulary size to |V| = 16,383.""",,21731646.00,,,,,,,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,Canada,Université de Montréal,,,,NPLM,,,
Phrase-based translation,Language,Translation,"Philipp Koehn, Franz Josef Och, Daniel Marcu",Highly cited,,,https://dl.acm.org/doi/10.3115/1073445.1073462,4270.00,Statistical Phrase-Based Translation,2003-05-01,University of Southern California,Academia,9178890.00,"There are various components to the system:

- Translation probability model phi
- The distortion probability distribution d
- A langage model p_LM
- A length factor w

Several translation probability models are considered. The most performant one is the AP word alignment model. The sentence length preferred by the authors is 3 words maximum. In the biggest corpus considered (320k phrase pairs) it produces a phrase translation probability table of 1996k entries.

The distortion probability model d is taken from  (Marcu and Wong, 2002).

The distortion probability model must have ~10 parameters at most

The language model p_LM is a back off trigram model from (Seymore and Rosenfeld,1997). AFAIK the cutoff used is not specified. Based on the example on section 4.3 of (Seymore and Rosefeld, 1997), a trigram probability model has about 3866964 + 2674322 + 641604 parameters.

""For each possible phrase translation anywhere in the sentence (we call it a translation option), we multiply its phrase translation probability with the language model probability for the generated English phrase. As language model probability we use the unigram probability for the first word, the bigram probability for the second, and the trigram probability for all following words""

The length factor w is an additional single parameter.

""In order to calibrate the output length, we introduce a
factor w for each generated English word in addition to
the trigram language model ""

In summary, the parameter count seems to be dominated by the trigram language model and the word alignment phrase translation model. ",,,,,20000000,"[WORDS]
""We used the freely available Europarl corpus to carry out experiments. This corpus contains over 20 million words in each of the eleven official languages of the European Union, covering the proceedings of the European Parliament 1996-2001. 1755 sentences of length 5-15 were reserved for testing.""

""These results are consistent
over training corpus sizes from 10,000 sentence pairs to
320,000 sentence pairs. ""

So 20 million words or 320k sentence pairs.",,,"""With our decoder, translating 1755 sentence of length 5-15 words
takes about 10 minutes on a 2 GHz Linux system.""",,,,,,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,United States of America,University of Southern California,,,,Phrase-based translation,,,
Unsupervised Scale-Invariant Learning,Vision,,"R Fergus, P Perona, A Zisserman",Highly cited,,,https://ieeexplore.ieee.org/document/1211479,2970.00,Object Class Recognition by Unsupervised Scale-Invariant Learning,2003-06-18,University of Oxford,Academia,451.00,"See Table 1
",,,,,3500,"See Table 2 and Figure 1.
There are 7 datasets, each with 200-800 of pictures. I pick 500 as the avg number of pictures",,,,,,,,,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,United Kingdom of Great Britain and Northern Ireland,University of Oxford,,,,Unsupervised Scale-Invariant Learning,,,
CNN Best Practices,Vision,Character recognition,"PY Simard, D Steinkraus, JC Platt",Highly cited,,,https://ieeexplore.ieee.org/document/1227801,3065.00,Best practices for convolutional neural networks applied to visual document analysis,2003-08-06,Microsoft Research,Industry,,,,,MNIST,,50000,,,,,,,,,,,,Industry,,,2023-11-26 03:25,Robi Rahman,,,United States of America,Microsoft Research,,,,CNN Best Practices,,,
Max-Margin Markov Networks,,,"B. Taskar, C. Guestrin, and D. Koller",Highly cited,,,https://papers.nips.cc/paper/2003/file/878d5691c824ee2aaf770f7d36c151d6-Paper.pdf,1764.00,Max-margin markov networks,2004-03-01,Stanford University,Academia,,,,,,,600,"The data set is divided into 10 folds of ∼ 600 training and ∼ 5500 testing examples.
The accuracy results, ... are averages over the 10 folds",,,,,,,,,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,United States of America,Stanford University,,,,Max-Margin Markov Networks,,,
GPU implementation of neural networks,,,"KS Oh, K Jung",,,,https://www.sciencedirect.com/science/article/pii/S0031320304000524,471.00,GPU implementation of neural networks,2004-06-01,Soongsil University,Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-11-21 01:39,Robi Rahman,,,Korea (Republic of),Soongsil University,,,,,,,
Sandstorm (DARPA Grand Challenge I),Driving,Self-driving car,William Red L. Whittaker,,,,https://ieeexplore.ieee.org/document/1336386,66.00,DARPA Grand Challenge Technical Paper,2004-06-14,Carnegie Mellon University (CMU),Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-11-21 01:39,Robi Rahman,,,United States of America,Carnegie Mellon University (CMU),,,,,,,
Automated WSD via WordNet,Language,Word sense disambiguation,"D McCarthy, R Koeling, J Weeds",,,,https://aclanthology.org/P04-1036/,475.00,Finding Predominant Word Senses in Untagged Text,2004-07-01,University of Sussex,Academia,,,,,,,5000,"They do two experiments, one on a dataset of 5.000 tagged words and
another one on two datasets containing a total of around 40 million words, of which they only select 38 unique words and manually annotate the senses?
I think the first one is more representative",,,,,,,,,,,Academia,,,2023-11-21 01:39,Robi Rahman,,,United Kingdom of Great Britain and Northern Ireland,University of Sussex,,,,,,,
LIRA,Vision,Character recognition,"E Kussul, T Baidyk",,,,https://www.sciencedirect.com/science/article/abs/pii/S0262885604000721,188.00,Improved method of handwritten digit recognition tested on MNIST database,2004-07-30,Instituto de Ciencias Aplicadas y Technologia,Academia,100000.00,"""For the first modification of the Rosenblatt perceptron 10 neurons were included into the R-layer. [...] The number of the A-layer neurons was 256,000"" The relation between the S-layer and A-layer is hardcoded",,The coding time was 20 h and the training time was 45 h.,,,10000,,,,,,,,,,,,Academia,,,2023-11-21 01:39,Robi Rahman,,,Mexico,Instituto de Ciencias Aplicadas y Technologia,,,,,,,
SACHS,Other,,"K. Sachs, O. Perez, D. Pe'er, D. A. Lauffenburger and G. P. Nolan",Highly cited,,,https://science.sciencemag.org/content/308/5721/523.long,1682.00,Causal Protein-Signaling Networks Derived from Multiparameter Single-Cell Data.,2005-04-22,"Massachusetts Institute of Technology (MIT),Stanford University",Academia,178.00,From https://www.bnlearn.com/bnrepository/,,,,,5400,"I think? 

"" The truncated singlecell data set (420 data points) shows a large
(11-arc) decline in accuracy, missing more connections and reporting more unexplained arcs than its larger (5400 data points) counterpart (fig. S4B). ""

Seems potentially wrong by maybe 20%. Might need to add 1200.",,,,,,,,,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,"United States of America,United States of America","Massachusetts Institute of Technology (MIT), Stanford University",,,,SACHS,,,
Hiero,Language,Translation,David Chiang,Highly cited,,,https://aclanthology.org/P05-1033/,1487.00,A Hierarchical Phrase-Based Model for Statistical Machine Translation,2005-06-01,University of Maryland College Park,Academia,120000000.00,"Very unsure, but the paper mentions 
""We ran the training process of Section 3 on the same data, obtaining a grammar of 24M rules"" 
and 
""For our experiments we used the following features, analogous to Pharaoh’s default feature set:
• P(γ | α) and P(α | γ), the latter of which is not
found in the noisy-channel model, but has been
previously found to be a helpful feature (Och
and Ney, 2002);
• the lexical weights Pw(γ | α) and Pw(α | γ) (Koehn et al., 2003), which estimate how well the words in α translate the words in γ;
2
• a phrase penalty exp(1), which allows the
model to learn a preference for longer or
shorter derivations, analogous to Koehn’sphrase penalty (Koehn, 2003).""

Suggesting 24M rules * 5 features per rule (?)",,,,,171400000,"[WORDS]
155M words dataset for the language model plus (7.2+9.2)M words for the translation model?",,,,,,,,,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,United States of America,University of Maryland College Park,,,,Hiero,,,
ConvNet similarity metric,Vision,,"S Chopra, R Hadsell, Y LeCun",Highly cited,,,https://ieeexplore.ieee.org/document/1467314,3771.00,"Learning a similarity metric discriminatively, with application to face verification",2005-06-20,New York University (NYU),Academia,,,,,,,140000,"The actual training set that was used contained
140,000 image pairs that were evenly split between genuine
and impostor.",,,,,,,,,,,Academia,,,2023-12-05 04:33,Robi Rahman,,,United States of America,New York University (NYU),,,,ConvNet similarity metric,,,
Histograms of Oriented Gradients,Vision,,"N Dalal, B Triggs",Highly cited,,,https://ieeexplore.ieee.org/document/1467360,36578.00,Histograms of oriented gradients for human detection,2005-06-25,INRIA,Academia,,,,,,,1805," we produced a new and significantly more
challenging data set, ‘INRIA’, containing 1805 64×128 im-
ages",,,,,,,,,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,France,INRIA,,,,Histograms of Oriented Gradients,,,
BiLSTM for Speech,Speech,Speech recognition,"A Graves, J Schmidhuber",Highly cited,,,https://www.sciencedirect.com/science/article/abs/pii/S0893608005001206,4281.00,Framewise phoneme classification with bidirectional LSTM and other neural network architectures,2005-08-01,"IDSIA,Technical University of Munich",Academia,152061.00,"""The hidden layer sizes were chosen to ensure that all networks had roughly the same number of weights W (≈100,000). However, for the MLPs the network grew with the time-window size, and W varied between 22,061 and 152,061.""",24124575958774.88,Extracted from AI and Compute (https://openai.com/blog/ai-and-compute/) charts by using https://automeris.io/WebPlotDigitizer/.,TIMIT,,36960,"https://catalog.ldc.upenn.edu/LDC93s1
One sample utterance has around 10 words

3696 utterances * 10 words = around 37k words",,,,,,,,,,,Academia,,,2023-12-05 04:33,Robi Rahman,,,"Switzerland,Germany","IDSIA, Technical University of Munich",,,,BiLSTM for Speech,,,
Stanley (DARPA Grand Challenge 2),Driving,Self-driving car,"S Thrun, M Montemerlo, H Dahlkamp",Highly cited,,,https://www.researchgate.net/publication/220648006_Stanley_The_robot_that_won_the_DARPA_Grand_Challenge,2561.00,Stanley: The Robot that Wonthe DARPA Grand Challenge,2006-01-01,Stanford University,Industry - Academia Collaboration,,"""Our  approach  and  the underlying  probabilistic  Markov  model  possess  anumber  of  unknown  parameters.  These  parameters include the height threshold, the statistical acceptance  probability  threshold,  and  various  Markov chain error parameters the noise covariances of theprocess noise and the measurement noise. Stanley uses a discriminative learning algorithm for  locally  optimizing  these  parameters.""",,,,,,,,,,,,,,,,,Industry,,,2023-11-26 03:25,Robi Rahman,,,United States of America,Stanford University,,,,Stanley (DARPA Grand Challenge 2),,,
TFE SVM,,,"Fabian Lauer, Ching Y Suen, Gerard Bloch",SOTA Improvement,best at affine-transformed digits in table 4,,https://hal.archives-ouvertes.fr/hal-00018426/en,365.00,A trainable feature extractor for handwritten digit recognition,2006-02-02,"Centre de Recherche en Automatique de Nancy (CRAN),CENPARMI",Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,"France,Canada","Centre de Recherche en Automatique de Nancy (CRAN), CENPARMI",,,,TFE SVM,,,
RL for helicopter flight,Driving,Helicopter driving,"H. Kim, Michael Jordan, Shankar Sastry, Andrew Ng",,,,https://papers.nips.cc/paper/2003/hash/b427426b8acd2c2e53827970f2c2f526-Abstract.html,361.00,Autonomous helicopter flight via reinforcement learning,2006-03-09,"UC Berkeley,Stanford University",Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-12-05 04:33,Robi Rahman,,,"United States of America,United States of America","UC Berkeley, Stanford University",,,,,,,
FAST,Vision,Corner detection,Edward Rosten and Tom Drummond,Highly cited,,,https://link.springer.com/chapter/10.1007/11744023_34,5419.00,Machine Learning for High-Speed Corner Detection,2006-05-07,University of Cambridge,Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,United Kingdom of Great Britain and Northern Ireland,University of Cambridge,,,,FAST,,,
DrLIM,Vision,Image embedding,R. Hadsell; S. Chopra; Y. LeCun,Highly cited,,,https://ieeexplore.ieee.org/document/1640964,4312.00,Dimensionality Reduction by Learning an Invariant Mapping,2006-06-17,New York University (NYU),Academia,37097.00,Architecture described in figure 3,,,,,217470,"""The dataset was split into 660 training images and a 312
test images. The result of training on all 10989 similar pairs
and 206481 dissimilar pairs is a 3-dimensional manifold in
the shape of a cylinder (see figure 8).""

206481 + 10989 = 217470",,,,,,,,,,,Academia,,,2023-12-05 04:33,Robi Rahman,,,United States of America,New York University (NYU),,,,DrLIM,,,
Spatial Pyramid Matching,Vision,,"S Lazebnik, C Schmid, J Ponce",Highly cited,,,https://inc.ucsd.edu/mplab/users/marni/Igert/Lazebnik_06.pdf,9807.00,Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories,2006-06-17,"INRIA,Ecole Normale,University of Illinois Urbana-Champaign (UIUC)",Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,"France,France,United States of America","INRIA, Ecole Normale, University of Illinois Urbana-Champaign (UIUC)",,,,Spatial Pyramid Matching,,,
CTC-Trained LSTM,Speech,Speech recognition,"Alex Graves, Santiago Fernández, Faustino Gómez, Jürgen Schmidhuber",Highly cited,,,https://www.cs.toronto.edu/~graves/icml_2006.pdf,4824.00,Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks,2006-06-25,"IDSIA,Technical University of Munich",Academia,114662.00,"""The hidden layers were fully connected to themselves
and the output layer, and fully connected from the input layer. The input layer was size 26, the softmax output layer size 62 (61 phoneme categories plus the blank label), and the total number of weights was
114, 662.""

https://www.cs.toronto.edu/~graves/icml_2006.pdf",,,TIMIT,,41620,"4162 utterances, guesstimated avg 10 words per utterance",,,,,,,,,,,Academia,,,2023-12-05 04:33,Robi Rahman,,,"Switzerland,Germany","IDSIA, Technical University of Munich",,,,CTC-Trained LSTM,,,
Semantic Taxonomy Induction,Language,,"Rion Snow, Dan Jurafsky, and Andrew Y. Ng",,,,https://www.aclweb.org/anthology/P06-1101/,571.00,Semantic Taxonomy Induction from Heterogenous Evidence,2006-07-07,Stanford University,Academia,100.00,"The main learning algorithm is a logistic classifier. The input is a matrix M, where the rows are pairs of words, and the columns (variables) are counts of occurrences of synthetic dependency paths between those two words.

Since there are on the order of 10~100 different types of syntactic relationships, this is the number of length-1 paths, and thus the number of parameters if only length-1 paths are used.

However, if the length of the paths considered is longer (say, 5), then the parameters would be on the order of (10~100)^5. It's not clear to me which is the case",,,,,850750,"[Classification task]

The labeled training set is
constructed by labeling the collected feature vectors as positive “known hypernym” or negative
“known non-hypernym” examples using WordNet
2.0; 49,922 feature vectors were labeled as positive training examples, and 800,828 noun pairs
were labeled as negative training examples.

800,828 + 49,922 = 850750",,,,,,,,,,,Academia,,,2023-11-21 01:39,Robi Rahman,,,United States of America,Stanford University,,,,,,,
Deep Belief Nets,Vision,Character recognition,"GE Hinton, S Osindero, YW Teh",Highly cited,,,https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf,16071.00,A fast learning algorithm for deep belief nets,2006-07-18,"University of Toronto,National University of Singapore",Academia,1600000.00,,,,MNIST,,60000,"""The network that performed best on the validation set was
then tested and had an error rate of 1.39%. This network was
then trained on all 60,000 training images8 until its error-rate
on the full training set was as low as its final error-rate had
been on the initial training set of 44,000 images.""",,,,,,,,,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,"Canada,Singapore","University of Toronto, National University of Singapore",,,,Deep Belief Nets,,,
DImensionality Reduction,Vision,Face recognition,"GE Hinton, RR Salakhutdinov",Highly cited,,,https://www.cs.toronto.edu/~hinton/science.pdf,15697.00,Reducing the dimensionality of data with neural networks.,2006-07-18,University of Toronto,Academia,3800000.00,,,,,,70000,"After fine-tuning on all 60,000 training images, the autoencoder was tested on 10,000 new images and produced much better reconstructions than did PCA
(Fig. 2B)",,,,,,,,,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,Canada,University of Toronto,,,,DImensionality Reduction,,,
Local Binary Patterns for facial recognition,,,"Timo Ahonen, Abdenour Hadid, and Matti Pietikainen",Highly cited,,,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.456.1094&rep=rep1&type=pdf,5808.00,Face Description with Local Binary Patterns: Application to Face Recognition,2006-12-01,"University of Oulu,IEEE",Academia,,"Shallowly investigated, couldn't find much.
",,,,,,,,,,,,,,,,,Academia,,,2023-12-05 04:33,Robi Rahman,,,"Finland,Multinational","University of Oulu, IEEE",,,,Local Binary Patterns for facial recognition,,,
Greedy layer-wise DNN training,,,"Y Bengio, P Lamblin, D Popovici",Highly cited,,,https://dl.acm.org/doi/10.5555/2976456.2976476,5605.00,Greedy layer-wise training of deep networks,2006-12-04,University of Montreal,Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,Canada,University of Montreal,,,,Greedy layer-wise DNN training,,,
Sparse Energy-Based Model,Vision,,"M Ranzato, C Poultney, S Chopra, Y Cun",Highly cited,,,https://papers.nips.cc/paper/2006/hash/87f4d79e36d68c3031ccf6c55e9bbd39-Abstract.html,1601.00,Efficient Learning of Sparse Representations with an Energy-Based Model,2006-12-04,New York University (NYU),Academia,,,,,MNIST,,60000,,,,,,,,,,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,United States of America,New York University (NYU),,,,Sparse Energy-Based Model,,,
λ-WASP,Language,,"YW Wong, R Mooney",SOTA Improvement,"""The resulting parser is shown to be the bestperforming system so far in a database query domain""",,https://www.aclweb.org/anthology/P07-1121/,383.00,Learning Synchronous Grammars for Semantic Parsing with Lambda Calculus,2007-06-01,UT Austin,Academia,,,,,,,792,"""Table 1 summarizes the results at the end of the learning curves (792 training examples for λWASP, WASP and SCISSOR, 600 for Z&C)""",,,,,,,,,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,United States of America,UT Austin,,,,λ-WASP,,,
Empirical evaluation of deep architectures,,,"Hugo Larechelle, Dumithru Erhan, Aaron C Courville, James Bergsta, Yoshua Bengio",,,,https://dl.acm.org/doi/10.1145/1273496.1273556,1124.00,An empirical evaluation of deep architectures on problems with many factors of variation,2007-06-01,University of Montreal,Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-12-05 04:33,Robi Rahman,,,Canada,University of Montreal,,,,,,,
Restricted Bolzmann machines,Recommendation,,"Russ Salukhutdinov, Andriy Mnih, GE Hinton",Highly cited,,,https://dl.acm.org/doi/abs/10.1145/1273496.1273596?casa_token=cfdkH2x12MwAAAAA:sEUzfllIGyPcOfzgUoDPHlpC1ukfCAo8ewocBXWBswIIF9eS5HdFo30nOtfmIV8gm-XpBpQJJ5zYVO8,2140.00,Restricted Boltzmann machines for collaborative filtering,2007-06-20,University of Toronto,Academia,,,,,Netflix Prize,,100480507,"The training data set consists of 100,480,507
ratings",,,,,,,,,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,Canada,University of Toronto,,,,Restricted Bolzmann machines,,,
Regularized SVD for Collaborative Filtering,Recommendation,,A Paterek,Highly cited,,,https://www.semanticscholar.org/paper/Improving-regularized-singular-value-decomposition-Paterek/f732d0f69fe4e84a95c32706b28b9e4ef1753c61,1117.00,Improving regularized singular value decomposition for collaborative filtering,2007-08-12,Warsaw University,Academia,,,,,Netflix Prize,,100480507,,,,,,,,,,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,Poland,Warsaw University,,,,Regularized SVD for Collaborative Filtering,,,
BLSTM for handwriting (1),Vision,,"M Liwicki, A Graves, S Fernàndez",SOTA Improvement,,,https://people.idsia.ch//~juergen/icdar_2007.pdf,287.00,A Novel Approach to On-Line Handwriting Recognition Based on Bidirectional Long Short-Term Memory Networks,2007-09-23,"University of Bern,IDSIA,Technical University of Munich",Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,"Switzerland,Switzerland,Germany","University of Bern, IDSIA, Technical University of Munich",,,,BLSTM for handwriting (1),,,
Enhanced Neighborhood-Based Filtering,Recommendation,,"RM Bell, Y Koren",SOTA Improvement,"""We evaluate these methods on the Netflix dataset, where they deliver significantly better results than the commercial Netflix Cinematch recommender system.""",,https://ieeexplore.ieee.org/abstract/document/4470228,687.00,Scalable Collaborative Filtering with Jointly Derived Neighborhood Interpolation Weights,2007-10-28,AT&T,Industry,,,,,,,,,,,,,,,,,,,Industry,,,2023-11-26 03:25,Robi Rahman,,,United States of America,AT&T,,,,Enhanced Neighborhood-Based Filtering,,,
BLSTM for handwriting (2),Vision,Character recognition,"Alex Graves, Marcus Liwicki, Horst Bunke, Jürgen Schmidhuber, Santiago Fernández",SOTA Improvement,"""In experiments on an unconstrained
online database, we record excellent results using either raw or preprocessed data, well outperforming a state-of-the-art HMM based system in both cases.""",,https://proceedings.neurips.cc/paper/2007/hash/4b0250793549726d5c1ea3906726ebfe-Abstract.html,341.00,Unconstrained online handwriting recognition with recurrent neural networks,2007-12-03,"University of Bern,IDSIA,Technical University of Munich",Academia,100881.00,"For the raw input representation,
there were 4 input units and a total of 100,881 weights",,,,,,,,,,,,,,,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,"Switzerland,Switzerland,Germany","University of Bern, IDSIA, Technical University of Munich",,,,BLSTM for handwriting (2),,,
Multiscale deformable part model,Vision,,"Pedro Felzenszwalb, David McAllester, Deva Ramanan",Highly cited,,,https://ieeexplore.ieee.org/abstract/document/4587597,3093.00,"A discriminatively trained, multiscale, deformable part model",2008-06-23,"UC Irvine,University of Chicago,Toyota Technological Institute at Chicago",Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,"United States of America,United States of America,United States of America","UC Irvine, University of Chicago, Toyota Technological Institute at Chicago",,,,Multiscale deformable part model,,,
Denoising Autoencoders,,,"Pascal Vincent, Hugo Larechelle, Yoshua Bengio, Pierre- Antoine Manzagol",Highly cited,,,https://dl.acm.org/doi/10.1145/1390156.1390294,6647.00,Extracting and Composing Robust Features with Denoising Autoencoders,2008-07-05,University of Montreal,Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-12-05 04:33,Robi Rahman,,,Canada,University of Montreal,,,,Denoising Autoencoders,,,
Semi-Supervised Embedding for DL,,,"Jason Weston, Frederick, Ratle, Hossein Mobahi, Ronan Collobert",Highly cited,,,https://dl.acm.org/doi/10.1145/1390156.1390303,1087.00,Deep Learning via Semi-Supervised Embedding,2008-07-05,"Google,NUANCE Communications,IDIAP,University of Illinois Urbana-Champaign (UIUC)",Industry - Academia Collaboration,,,,,,,,,,,,,,,,,,,Industry,,,2023-11-26 03:25,Robi Rahman,,,"Multinational,United States of America,Switzerland,United States of America","Google, NUANCE Communications, IDIAP, University of Illinois Urbana-Champaign (UIUC)",,,,Semi-Supervised Embedding for DL,,,
Deep Multitask NLP Network,Language,Language modelling,"Ronan Collobert, Jason Weston","Highly cited,SOTA improvement",,,http://icml2008.cs.helsinki.fi/papers/391.pdf,7095.00,"A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning
",2008-07-05,NEC Laboratories,Industry,1500000.00,"With a word vector size of 50 and a vocabulary size of 30,000, the embedding matrix has 1,500,000 parameters. There are also some small convolutional and dense layers with far fewer parameters.",,,"PropBank, Penn Treebank, Wikipedia","PropBank (1M words) for semantic role labeling task
Penn Treebank (1M words) for part-of-speech tagging and chunking tasks
Stanford NER dataset for named entity recognition task
Wikipedia text (631M words) for unsupervised pretraining",633000000,"Section 7: ""631 million words
from Wikipedia""",,,,168.0,1 week on 1 computer,,,Unsupervised,,,Industry,Speculative,"We describe a single convolutional neural network architecture that, given a sentence, outputs a host of language processing predictions: part-of-speech tags, chunks, named entity tags, semantic roles, semantically similar words and the likelihood that the sentence makes sense (grammatically and semantically) using a language model. The entire network is trained jointly on all these tasks using weight-sharing, an instance of multitask learning. All the tasks use labeled data except the language model which is learnt from unlabeled text and represents a novel form of semi-supervised learning for the shared tasks. We show how both multitask learning and semi-supervised learning improve the generalization of the shared tasks, resulting in stateof-the-art performance.",2023-11-26 03:25,Anonymous,,,United States of America,NEC Laboratories,,,,Deep Multitask NLP Network,,,
Stacked Semisuperviser Autoencoders,Language,Document representation,"MA Ranzato, M Szummer",,,,https://dl.acm.org/doi/10.1145/1390156.1390256,243.00,Semisupervised learning of compact document representations with deep networks,2008-07-15,"New York University (NYU),Microsoft",Industry - Academia Collaboration,3000000.00,,,,,,66087,"""The 20 Newsgroups dataset contains 18845
postings taken from the Usenet newsgroup collection.
Documents are partitioned into 20 topics. The dataset
is split into 11314 training documents and 7531 test
documents. Training and test articles are separated in
time. Reuters has a predefined ModApte split of the
data into 11413 training documents and 4024 test doc-
uments. Documents belong to one of 91 topics. The
Ohsumed dataset has 34389 documents with 30689
words and each document might be assigned to more
than one topic, for a total of 23 topics. The dataset is
split into training and test by randomly selecting the
67% and the 33% of the data""

total # documents = 11314 + 11413 + 34389*0.6

I'm using #documents here since the task is document representation. Using #words would increase the size by ~3 OOMs",,,,,,,,,,,Industry,,,2023-11-21 01:39,Robi Rahman,,,"United States of America,Multinational","New York University (NYU), Microsoft",,,,,,,
Boss (DARPA Urban Challenge),Driving,Self-driving car,"Chris Urmson, Joshua Anhalt, Drew Bagnell,Christopher Baker, Robert Bittner,M. N. Clark, John Dolan, Dave Duggins,Tugrul Galatali, Chris Geyer,Michele Gittleman, Sam Harbaugh,Martial Hebert, Thomas M. Howard,Sascha Kolski, Alonzo Kelly,Maxim Likhachev, Matt McNaughton,Nick Miller, Kevin Peterson, Brian Pilnick,Raj Rajkumar, Paul Rybski, Bryan Salesky,Young-Woo Seo, Sanjiv Singh, Jarrod Snider,Anthony Stentz, William “Red” Whittaker,Ziv Wolkowicki, and Jason Ziglar",Highly cited,,,https://onlinelibrary.wiley.com/doi/abs/10.1002/rob.20255,1840.00,Autonomous driving in urban environments: Boss and the Urban Challenge,2008-07-23,Carnegie Mellon University (CMU),Industry - Academia Collaboration,,,,,,,,,,,,,,,,,,,Industry,,,2023-11-26 03:25,Robi Rahman,,,United States of America,Carnegie Mellon University (CMU),,,,Boss (DARPA Urban Challenge),,,
Sparse digit recognition SVM,Vision,,"Kai Labusch, Erhadt Barth, Thomas Martinetz",SOTA Improvement,"""Finally, we train a support vector machine (SVM) on the resulting feature vectors and obtain state-of-the-art classification performance in the digit recognition task defined by the MNIST benchmark""",,https://pubmed.ncbi.nlm.nih.gov/19000969/,124.00,Simple method for high-performance digit recognition based on sparse coding,2008-11-19,University of Lubeck,Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,Germany,University of Lubeck,,,,Sparse digit recognition SVM,,,
BigChaos 2008,Recommendation,Movie ratings,"A Töscher, M Jahrer",Historical significance,Winners of the 2008 Netflix Price,,https://www.researchgate.net/publication/228419683_The_bigchaos_solution_to_the_netflix_prize_2008,35.00,The BigChaos Solution to the Netflix Prize 2008,2008-11-25,AT&T,Industry,,,,,Netflix Prize,,100480507,"""Netflix provided a training data set of 100,480,507 ratings that 480,189 users gave to 17,770 movies.""",,,,,,,,,,,Industry,,,2023-11-26 03:25,Robi Rahman,,,United States of America,AT&T,,,,BigChaos 2008,,,
Semantic Hashing,Other,,"R Salakhutdinov, G Hinton",Highly cited,,,https://www.cs.cmu.edu/~rsalakhu/papers/sdarticle.pdf,1487.00,Semantic Hashing,2008-12-10,University of Toronto,Academia,2600000.00,,,,,,310521,Section 4.1,,,,,,,,,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,Canada,University of Toronto,,,,Semantic Hashing,,,
RBM Image Classifier,Vision,Image Classification,Alex Krizhevsky,Highly cited,,,https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf,26335.00,Learning Multiple Layers of Features from Tiny Images,2009-04-08,University of Toronto,Academia,80000000.00,"Best performing model (see Figure 3.1) had 10,000 hidden units in one hidden layer and 8000 visible units",,,CIFAR-10,"This paper is the origin of CIFAR-10. However, CIFAR-10 is a labeled subset of all the training data used

""The tiny images dataset on which we based all of our experiments was collected by colleagues at MIT and NYU over the span of six months; it is described in detail in [14]. They assembled it by searching the web for images of every non-abstract English noun in the lexical database WordNet[15, 8]. They used several search engines, including Google, Flickr, and Altavista and kept roughly the rst 3000 results for each search term. After collecting all the images for a particular search term, they removed perfect duplicates and images in which an excessively large portion of the pixels were white, as they tended to be synthetic gures rather than natural images. The search term used to nd an image provides it with a rough label, although it is extremely unreliable due to the nature of online image search technology.
In total, the dataset contains 80 million colour images downscaled to 32 × 32 and spread out across 79000 search terms. Most of our experiments with unsupervised learning were performed on a subset of about 2 million images.""

""We paid students to label a subset of the tiny images dataset... We call this the CIFAR-10 dataset, after the Canadian Institute for Advanced Research, which funded the project""",2000000,,,,,,,,,,,,,Likely,"Groups at MIT and NYU have collected a dataset of millions of tiny colour images from the web. It
is, in principle, an excellent dataset for unsupervised training of deep generative models, but previous
researchers who have tried this have found it dicult to learn a good set of lters from the images.
We show how to train a multi-layer generative model that learns to extract meaningful features which
resemble those found in the human visual cortex. Using a novel parallelization algorithm to distribute
the work among multiple machines connected on a network, we show how training such a model can be
done in reasonable time.
A second problematic aspect of the tiny images dataset is that there are no reliable class labels
which makes it hard to use for object recognition experiments. We created two sets of reliable labels.
The CIFAR-10 set has 6000 examples of each of 10 classes and the CIFAR-100 set has 600 examples of
each of 100 non-overlapping classes. Using these labels, we show that object recognition is signicantly
improved by pre-training a layer of features on a large set of unlabeled tiny images.",2023-12-05 04:33,Anonymous,,,Canada,University of Toronto,,,,RBM Image Classifier,,,
Deep Boltzmann Machines,,,"Ruslan Salakhutdinov, Geoffrey Hinton",Highly cited,,,https://www.sciencedirect.com/topics/computer-science/deep-boltzmann-machine,2666.00,Deep Boltzmann Machines,2009-04-16,University of Toronto,Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,Canada,University of Toronto,,,,Deep Boltzmann Machines,,,
Conv-DBN,,,"H Lee, R Grosse, R Ranganath, AY Ng",Highly cited,,,https://dl.acm.org/doi/10.1145/1553374.1553453,2964.00,Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations,2009-06-14,Stanford University,Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,United States of America,Stanford University,,,,Conv-DBN,,,
GPU DBNs,Other,,"R Raina, A Madhavan, AY Ng",Highly cited,,,http://www.machinelearning.org/archive/icml2009/papers/218.pdf,1032.00,Large-scale Deep Unsupervised Learning using Graphics Processors,2009-06-15,Stanford University,Academia,100000000.00,"""For example, we are able to reduce the time required to learn a four-layer DBN with 100 million free parameters from several weeks to around a single day.""",1000000000000000.00,https://www.getguesstimate.com/models/19602,,,1000000,"Table 2 shows the running time for processing 1 million
examples for RBMs of varying size",,,,,,,,,$0.06,,Academia,,,2023-11-26 03:25,Robi Rahman,,,United States of America,Stanford University,,,,GPU DBNs,,,
BellKor 2009,Recommendation,Movie ratings,Y Koren,Historical significance,Introduced new algorithms; won Netflix Grand Prize,,https://www2.seas.gwu.edu/~simhaweb/champalg/cf/papers/KorenBellKor2009.pdf,508.00,The BellKor Solution to the Netflix Grand Prize,2009-08-01,AT&T,Industry,,,,,Netflix Prize,,100480507,"""Netflix provided a training data set of 100,480,507 ratings that 480,189 users gave to 17,770 movies.""",,,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,United States of America,AT&T,,,,BellKor 2009,,,
RL mapping instructions (games),Reading,Instruction interpretation,"SRK Branavan, H Chen, LS Zettlemoyer, R Barzilay",,,,https://aclanthology.org/P09-1010/,306.00,Reinforcement Learning for Mapping Instructions to Actions,2009-08-01,Massachusetts Institute of Technology (MIT),Academia,80940.00,"""We use a policy gradient
algorithm to estimate the parameters of a log-linear model for action selection [...] In total, there are 8,094 features [in the Crossblock domain]. [...]  This difficulty can be attributed in part to the large branching factor of possible actions at each step — on average, there are [...] 9.78 [actions] in the Crossblock
domain""",,,Windows Help and Support,,293,"Shown at beginning of section 7
Total number of documents is 50, average number of actions per document is 5.86

source: https://en.wikipedia.org/wiki/Netflix_Prize",,,,,,,,,,,Academia,,,2023-12-05 04:33,Robi Rahman,,,United States of America,Massachusetts Institute of Technology (MIT),,,,,,,
BellKor 2008,Recommendation,Movie ratings,"RM Bell, Y Koren, C Volinsky",SOTA Improvement,Won Netflix prize,,https://www.researchgate.net/publication/228766792_The_BellKor_2008_solution_to_the_Netflix_Prize,162.00,The BellKor 2008 Solution to the Netflix Prize,2009-08-01,AT&T,Industry,,,,,Netflix Prize,,100480507,"""Netflix provided a training data set of 100,480,507 ratings that 480,189 users gave to 17,770 movies.""",,,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,United States of America,AT&T,,,,BellKor 2008,,,
Pragmatic Theory solution (Netflix 2009),Recommendation,Movie ratings,"M Piotte, M Chabbert",SOTA Improvement,Netflix grand prize winner (along with two other teams),,https://www.asc.ohio-state.edu/statistics/statgen/joul_aut2009/PragmaticTheory.pdf,111.00,The Pragmatic Theory solution to the Netflix Grand Prize,2009-08-01,Pragmatic Theory Inc.,Industry,,"This is an ensemble of many smaller models. Ideally, the number of parameters of all the sub-models should be added up and recorded here.",,"This is an ensemble of many smaller models. Ideally, the training compute of all the sub-models should be added up and recorded here.",Netflix Prize,,100480507,"""Netflix provided a training data set of 100,480,507 ratings that 480,189 users gave to 17,770 movies.""",,,,,,,,,,,Industry,Likely,,2023-11-26 03:25,Robi Rahman,,,Canada,Pragmatic Theory Inc.,,,,Pragmatic Theory solution (Netflix 2009),,,
BigChaos OptiBlend,Recommendation,Movie ratings,"A Töscher, M Jahrer, RM Bell",SOTA Improvement,"won Netflix prize
",,https://www.asc.ohio-state.edu/statistics/statgen/joul_aut2009/BigChaos.pdf,237.00,The BigChaos Solution to the Netflix Grand Prize,2009-08-01,AT&T,Industry,,,,,Netflix Prize,,100480507,"""Netflix provided a training data set of 100,480,507 ratings that 480,189 users gave to 17,770 movies.""",,,,,,,,,,,Industry,,,2023-11-26 03:25,Robi Rahman,,,United States of America,AT&T,,,,BigChaos OptiBlend,,,
RL mapping instructions (troubleshooting),Reading,Instruction interpretation,"SRK Branavan, H Chen, LS Zettlemoyer, R Barzilay",,,,https://aclanthology.org/P09-1010/,306.00,Reinforcement Learning for Mapping Instructions to Actions,2009-08-02,Massachusetts Institute of Technology (MIT),Academia,133140.00,"""We use a policy gradient
algorithm to estimate the parameters of a log-linear model for action selection [...] In total, there are 4,438 features [in the Windows domain]. [...]  This difficulty can be attributed in part to the large branching factor of possible actions at each step — on average, there are 27.14 choices per action in the Windows domain""",,,Windows Help and Support,,1327,"Shown at beginning of section 7
Total number of documents is 128, average number of actions per document is 10.37",,,,,,,,,,,Academia,,,2023-12-05 04:33,Robi Rahman,,,United States of America,Massachusetts Institute of Technology (MIT),,,,,,,
MatrixFac for Recommenders,Recommendation,,"Yehuda Koren, Robert Bell, and Chris Volinsky",Highly cited,,,https://ieeexplore.ieee.org/document/5197422,8913.00,Matrix factorization techniques for recommender systems,2009-08-07,"Yahoo Research,AT&T",Industry,,,,,Netflix Prize,,100480507,,,,,,,,,,,,Industry,,,2023-11-26 03:25,Robi Rahman,,,"Multinational,United States of America","Yahoo Research, AT&T",,,,MatrixFac for Recommenders,,,
Polarity Classifier,Language,,"Theresa Wilson, Janyce Wiebe, Paul Hoffmann.",,,,https://aclanthology.org/J09-3003.pdf,866.00,Recognizing Contextual Polarity: An Exploration of Features for Phrase-Level Sentiment Analysis,2009-09-01,"University of Edinburgh,University of Pittsburgh",Academia,,,,,,,11112,"Section 3.3 reveals there are 11,112 sentences. Since this is phrase-level sentiment analysis sentences seem like the best unit",,,,,,,,,,,Academia,,,2023-11-21 01:39,Robi Rahman,,,"United Kingdom of Great Britain and Northern Ireland,United States of America","University of Edinburgh, University of Pittsburgh",,,,,,,
BellKor 2007,Recommendation,Movie ratings,"RM Bell, Y Koren, C Volinsky",SOTA Improvement,Won Netflix prize,,https://www.semanticscholar.org/paper/The-BellKor-solution-to-the-Netflix-Prize-Bell-Koren/f4ebb542c752a0dc423f94fd121e2edb8f6275ba,239.00,The BellKor solution to the Netflix Prize,2009-09-21,AT&T,Industry,,,,,Netflix Prize,,100480507,"The training data set consists of 100,480,507
ratings",,,,,,,,,,,Industry,,,2023-11-26 03:25,Robi Rahman,,,United States of America,AT&T,,,,BellKor 2007,,,
3D city reconstruction,3D reconstruction,,"Sameer Agarwal, Noah Snavely, Ian Simon, Steven M. Seitz and Richard Szeliski",Highly cited,,,https://grail.cs.washington.edu/rome/,2203.00,Building Rome in a Day,2009-09-29,"University of Washington,Microsoft Research,Cornell University",Industry - Academia Collaboration (Academia leaning),,,,,,,,,,,,,,,,,,,Industry,,,2023-11-26 03:25,Robi Rahman,,,"United States of America,United States of America,United States of America","University of Washington, Microsoft Research, Cornell University",,,,3D city reconstruction,,,
Learning deep architectures,,,Y Bengio,Highly cited,,,https://www.nowpublishers.com/article/Details/MAL-006,9782.00,Learning deep architectures for AI,2009-11-15,"University of Montreal,Microsoft Research",Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,"Canada,United States of America","University of Montreal, Microsoft Research",,,,Learning deep architectures,,,
Stacked Denoising Autoencoders,,,"P Vincent, H Larochelle, I Lajoie, Y Bengio",Highly cited,,,https://www.jmlr.org/papers/v11/vincent10a.html,6620.00,Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion,2010-01-03,"University of Montreal,University of Toronto",Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-12-05 04:33,Robi Rahman,,,"Canada,Canada","University of Montreal, University of Toronto",,,,Stacked Denoising Autoencoders,,,
6-layer MLP (MNIST),Vision,Character recognition,"Dan Claudiu Ciresan, Ueli Meier, Luca Maria Gambardella, Juergen Schmidhuber",Highly cited,,,https://arxiv.org/abs/1003.0358,1264.00,Deep Big Simple Neural Nets Excel on Handwritten Digit Recognition,2010-03-01,"IDSIA,University of Lugano,SUPSI",Academia,12110000.00,Table 1,130788000000000.00,"""Networks with up to 12 million weights can successfully be trained by plain gradient descent to achieve test errors below 1% after 20-30 epochs in less than 2 hours of training.""

I assume that the number of passes per epoch is 60k, the training set size.",MNIST,,60000,"""MNIST consists of two datasets, one for training (60,000 images) and one for testing (10,000 images). Many studies divide the training set into two sets consisting of 50,000 images for training and 10,000 for validation. Our network is trained on slightly deformed images, continually generated in on-line fashion; hence we may use the whole un-deformed training set for validation, without wasting training images""",,,,,,,,,$0.01,,Academia,,,2023-11-26 03:25,Robi Rahman,,,"Switzerland,Switzerland,Switzerland","IDSIA, University of Lugano, SUPSI",,,,6-layer MLP (MNIST),,,
Feedforward NN,Vision,Digit recognition,"X Glorot, Y Bengio",Highly cited,,,https://proceedings.mlr.press/v9/glorot10a.html,15599.00,Understanding the difficulty of training deep feedforward neural networks,2010-05-13,University of Montreal,Academia,7082000.00,"pg250 of the paper, section 2.3: 
""We optimized feedforward neural networks with one to
five hidden layers, with one thousand hidden units per
layer""

Input is a flattened 32x32 image, which corresponds to an input vector of length 3072

Output is a number from 0-9, so 10 neurons

No. of params: 3072*1000 + 4*1000*1000 + 1000*10 = 7,082,000
",350000000000000.00,"Roughly two times the number of parameters for ops per forward pass. 

So 2*7082000 params*3.5*140 epochs * 50k training images = 3.5e14",MNIST,,,,,14000000.00,Roughly twice the no. of params,,,,,,$0.01,,Academia,,,2023-12-05 04:33,Robi Rahman,,,Canada,University of Montreal,,,,Feedforward NN,,,
Word Representations,Language,,"Joseph Turian, Lev-Arie Ratinov, Yoshua Bengio",Highly cited,,,https://aclanthology.org/P10-1040.pdf,2510.00,Word Representations: A Simple and General Method for Semi-Supervised Learning,2010-06-01,"University of Montreal,University of Illinois Urbana-Champaign (UIUC)",Academia,,,,,,,37000000,"Section 6: ""After cleaning, there are 37 million words (58%
of the original) in 1.3 million sentences""",,,,,,,,,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,"Canada,United States of America","University of Montreal, University of Illinois Urbana-Champaign (UIUC)",,,,Word Representations,,,
Mid-level Features,Vision,,"YL Boureau, F Bach, Y LeCun, J Ponce",Highly cited,,,https://ieeexplore.ieee.org/document/5539963,1314.00,Learning mid-level features for recognition,2010-06-13,"INRIA,Ecole Normale Supèrieure,New York University (NYU)",Academia,,This is extracting low-level SIFT features then max-pooling them and using in a linear SVM. The training compute could be estimated loosely for the SVM part.,,,,,,,,,,,,,,,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,"France,France,United States of America","INRIA, Ecole Normale Supèrieure, New York University (NYU)",,,,Mid-level Features,,,
Deconvolutional Network,Vision,,"Matthew D. Zeiler, Dilip Krishnan, Graham W. Taylor and Rob Fergus",Highly cited,,,https://ieeexplore.ieee.org/document/5539957,1516.00,Deconvolutional Networks,2010-06-13,New York University (NYU),Academia,,,,,,,,,,,Inference time of the largest model was 55s on Caltech 101 images.,,,,,,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,United States of America,New York University (NYU),,,,Deconvolutional Network,,,
ReLU (LFW),Vision,Face recognition,"Nair, V., Hinton, G. E.",Highly cited,,,https://dl.acm.org/doi/10.5555/3104322.3104425,15846.00,Rectified linear units improve restricted boltzmann machines,2010-06-15,University of Toronto,Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-12-05 04:33,Robi Rahman,,,Canada,University of Toronto,,,,ReLU (LFW),,,
ReLU (NORB),Vision,Object recognition,"Nair, V., Hinton, G. E.",Highly cited,,,https://dl.acm.org/doi/10.5555/3104322.3104425,15846.00,Rectified linear units improve restricted boltzmann machines,2010-06-15,University of Toronto,Academia,16210006.00,"""The stereo-pair images are subsampled from their original resolution of 108 × 108 × 2 to 32 × 32 × 2 to speed up experiments [...]  the architecture
with the best results have 4000 units in the first layer
and 2000 in the second [...] there are 58,320 test
cases (9,720 cases per class) ""

So the architecture has (32*32*2+1)x4000 + (4000+1)*2000 + (2000+1)*58,320/9,720 parameters",,,,,291600,"""There are 291,600 training cases (48,600 cases per class) and 58,320 test cases (9,720 cases per class).""",,,,,,,,,,,Academia,,,2023-12-05 04:33,Robi Rahman,,,Canada,University of Toronto,,,,ReLU (NORB),,,
RBM-tuning,,,GE Hinton,Highly cited,,,https://link.springer.com/chapter/10.1007/978-3-642-35289-8_32,3335.00,A practical guide to training restricted boltzmann machines,2010-08-02,University of Toronto,Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,Canada,University of Toronto,,,,RBM-tuning,,,
Fisher-Boost,Vision,,Florent PerronninJorge SánchezThomas Mensink,Highly cited,,,https://link.springer.com/chapter/10.1007/978-3-642-15561-1_11,3062.00,Improving the Fisher Kernel for Large-Scale Image Classification,2010-09-05,Xerox Research Centre Europe (XRCE),Industry,,,,,,,,,,,,,,,,,,,Industry,,,2023-11-26 03:25,Robi Rahman,,,France,Xerox Research Centre Europe (XRCE),,,,Fisher-Boost,,,
YouTube Video Recommendation System,Recommendation,,"J Davidson, B Liebald, J Liu, P Nandy",Highly cited,,,https://dl.acm.org/doi/10.1145/1864708.1864770,1098.00,The YouTube Video Recommendation System,2010-09-26,Google,Industry,,,,,,,10000000000,"""We currently handle millions of users
and tens of billions of activity events with a total footprint
of several terabytes of data""

If 10M users each watch 1000 videos, that's 10B visualizations, which matches their ""activity events"" count.",,,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,Multinational,Google,,,,YouTube Video Recommendation System,,,
KN5 LM + RNN 400/10 (WSJ),Speech,Transcription,"T. Mikolov, M. Karafiat, L. Burget, J. Cernock ´ y, and S. Khudanpur",Highly cited,,,https://www.researchgate.net/publication/221489926_Recurrent_neural_network_based_language_model,5665.00,Recurrent neural network based language model,2010-09-26,"Brno University of Technology,Johns Hopkins University",Academia,80000000.00,"""- size of vector x is equal to
size of vocabulary V (this can be in practice 30 000 − 200 000)
plus size of context layer. Size of context (hidden) layer s is
usually 30 − 500 hidden units.""

""In further experiments, we denote modified Kneser-Ney
smoothed 5-gram as KN5. Configurations of neural network
LMs, such as RNN 90/2, indicate that the hidden layer size is
90 and threshold for merging words to rare token is 2.""",61440000000000000.00,"""Convergence is usually
achieved after 10-20 epochs.""

Assuming a backward-forward ratio of 2:1, since this is a shallow network",WSJ,,6400000,"The training corpus consists of 37M words from NYT section of English Gigaword. As it is very time consuming to train RNN LM on large data, we have used only up to 6.4M words for training RNN models (300K sentences) - it takes several weeks to train the most complex models",,160000000.00,Roughly twice the number of parameters,,,,,,$2.03,,Academia,,,2023-11-26 03:25,Robi Rahman,,,"Czechia,United States of America","Brno University of Technology, Johns Hopkins University",,,,KN5 LM + RNN 400/10 (WSJ),,,
RNN 500/10 + RT09 LM (NIST RT05),Speech,Transcription,"T. Mikolov, M. Karafiat, L. Burget, J. Cernocky, and S. Khudanpur",Highly cited,,,https://www.researchgate.net/publication/221489926_Recurrent_neural_network_based_language_model,5665.00,Recurrent neural network based language model,2010-09-26,"Brno University of Technology,Johns Hopkins University",Academia,5269500.00,"""Size of context (hidden) layer s is usually 30 − 500 hidden units.""

""The acoustic HMMs are based on cross-word tied-states triphones trained discriminatively using MPE criteria. Feature extraction use 13 Mel-PLP’s features with deltas, double and triple deltas reduced by HLDA to 39-dimension feature vector""

10k words vocabulary

(39+500)*500 + 500*10000
",3414636000000000.00,"""Convergence is usually achieved after 10-20 epochs.""

Assuming a backward-forward ratio of 2:1, since this is a shallow network",NIST RT05,,5400000,"""Table 4: Comparison of very large back-off LMs and RNN LMs
trained only on limited in-domain data (5.4M words).""",,10539000.00,Roughly twice the number of parameters,,,,,,$0.11,,Academia,,,2023-11-26 03:25,Robi Rahman,,,"Czechia,United States of America","Brno University of Technology, Johns Hopkins University",,,,RNN 500/10 + RT09 LM (NIST RT05),,,
Culturome,Language,,"JB Michel, YK Shen, AP Aiden, A Veres, MK Gray",Highly cited,,,https://science.sciencemag.org/content/331/6014/176,2543.00,Quantitative Analysis of Culture Using Millions of Digitized Books,2010-12-16,Harvard University,Industry - Academia Collaboration,,,,,,,,,,,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,United States of America,Harvard University,,,,Culturome,,,
Optimized Single-layer Net,,,"A Coates, A Ng, H Lee",Highly cited,,,http://proceedings.mlr.press/v15/coates11a.html,3318.00,An analysis of single-layer networks in unsupervised feature learning,2011-04-11,"University of Michigan,Stanford University",Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-12-05 04:33,Robi Rahman,,,"United States of America,United States of America","University of Michigan, Stanford University",,,,Optimized Single-layer Net,,,
Deep rectifier networks,,,"X Glorot, A Bordes, Y Bengio",Highly cited,,,http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2011_GlorotBB11.pdf,7385.00,Deep sparse rectifier neural networks,2011-04-13,University of Montreal,Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-12-05 04:33,Robi Rahman,,,Canada,University of Montreal,,,,Deep rectifier networks,,,
RNN-SpeedUp,Language,,"T. Mikolov, S. Kombrink, L. Burget, J. Cernock ˇ y, and S. Khudanpur",Highly cited,,,https://ieeexplore.ieee.org/document/5947611,1538.00,Extensions of recurrent neural network language model,2011-05-22,"Brno University of Technology,Johns Hopkins University",Academia,,,,,Penn Tree Bank,,697500,"Section 3: ""The data used in the following experiments were obtained from
Penn Tree Bank: sections 0-20 were used as training data (about
930K tokens)""

0.75 words per token for English",,,,,,,,,,,Academia,,,2023-12-05 04:33,Robi Rahman,,,"Czechia,United States of America","Brno University of Technology, Johns Hopkins University",,,,RNN-SpeedUp,,,
Cross-Lingual POS Tagger,Language,Part-of-speech tagging,"Dipanjan Das, Slav Petrov",SOTA Improvement,"""Across eight European languages, our approach results in an average absolute improvement of 10.4% over a state-of-the-art baseline, and 16.7% over
vanilla hidden Markov models induced with
the Expectation Maximization algorithm.""",,https://aclanthology.org/P11-1061/,316.00,Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections,2011-06-19,"Carnegie Mellon University (CMU),Google Research",Industry - Academia Collaboration,,,,,,,,,,,,,,,,,,,Industry,,,2023-11-26 03:25,Robi Rahman,,,"United States of America,Multinational","Carnegie Mellon University (CMU), Google Research",,,,Cross-Lingual POS Tagger,,,
Recursive sentiment autoencoder,Language,,"R. Socher, J. Pennington, E. H. Huang, A. Y. Ng, and C. D. Manning",Highly cited,,,https://aclanthology.org/D11-1014/,1477.00,Semi-supervised recursive autoencoders for predicting sentiment distributions,2011-07-01,Stanford University,Academia,,,,,,,,"They use several datasets for self-supervised and supervised learning
",,,,,,,,,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,United States of America,Stanford University,,,,Recursive sentiment autoencoder,,,
Bayesian Starcraft,Games,,"G Synnaeve, P Bessiere",,,,https://ieeexplore.ieee.org/document/6032006,86.00,A Bayesian Model for RTS Units Control applied to StarCraft,2011-08-31,Collège de France,Academia,13125.00,"It's a bayes net, parameters are probabilty tables for probability that X happens in direction i given that we go in direction i. There are 25 directions.",,,,,,,,,,,,,,,,,Academia,,,2023-11-21 01:39,Robi Rahman,,,France,Collège de France,,,,,,,
Adaptive Subgrad,,,"J Duchi, E Hazan, Y Singer",Highly cited,,,https://dl.acm.org/doi/10.5555/1953048.2021068,9494.00,Adaptive Subgradient Methods for Online Learning and Stochastic Optimization,2011-10-03,"Technion - Israel Institute of Technology,Google,UC Berkeley",Industry - Academia Collaboration,,,,,,,,,,,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,"Israel,Multinational,United States of America","Technion - Israel Institute of Technology, Google, UC Berkeley",,,,Adaptive Subgrad,,,
Domain Adaptation,Vision,Object Recognition,"Raghuraman Gopalan, Ruonan Li, Rama Chellappa",Highly cited,,,http://ftp.idiap.ch/pub/courses/EE-700/material/05-12-2012/2011_ICCV_DomainAdaptation.pdf,1061.00,Domain Adaptation for Object Recognition: An Unsupervised Approach,2011-11-06,University of Maryland College Park,Academia,15260.00,"Did not take into account initial image feature extraction, only novel stuff.

1. Perform PCA on the feature matrices from both domains. Learnable parameters are projection matrices.
= 800 (# features) x 200 (reduced dimension) x 2 (once per subdomain)

2. Perform partial least squares regression. Learnable parameters are

Matrix P with dimensions 200 (# features) x 30 (dimension of latent space)
Matrix Q with dimensions 1 (# responses) x 30 (dimension of latent space)
Projection matrix of X onto latent space:  200 (# features) x 30 (dimension of latent space)
Projection matrix of Y onto latent space:  1 (# responses) x 30 (dimension of latent space)
",,,Dataset introduced in 'Adapting Visual Category Models to New Domains',,4652,"Dataset introduced in 'Adapting Visual Category Models to New
Domains'",,,,,,,,Supervised,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,United States of America,University of Maryland College Park,,,,Domain Adaptation,,,
NLP from scratch,Language,,"Ronan Collobert, J. Weston, L. Bottou, Michael Karlen, K. Kavukcuoglu, P. Kuksa",Highly cited,,,https://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf,7640.00,Natural Language Processing (Almost) from Scratch,2011-11-08,"NEC Laboratories,Princeton University",Industry - Academia Collaboration,5000000.00,"""The capacity of our network architectures lies mainly in the word lookup table, which contains 50 × 100,000 parameters to train. [...] most of the trainable parameters are located in the lookup tables.""",,,,,852000000,"""Section 4 leverages large unlabeled data sets (∼ 852 million words)""",,,,,,,,,,,Industry,,,2023-11-26 03:25,Robi Rahman,,,"United States of America,United States of America","NEC Laboratories, Princeton University",,,,NLP from scratch,,,
HOGWILD!,,,"F Niu, B Recht, C Ré, SJ Wright",Highly cited,,,https://arxiv.org/abs/1106.5730,2143.00,HOGWILD!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent,2011-11-11,University of Wisconsin Madison,Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-12-05 04:33,Robi Rahman,,,United States of America,University of Wisconsin Madison,,,,HOGWILD!,,,
MCDNN (MNIST),Vision,Character recognition,"D Ciregan, U Meier, J Schmidhuber",Highly cited,,,https://arxiv.org/abs/1202.2745v1,4828.00,Multi-column Deep Neural Networks for Image Classification,2012-02-13,IDSIA,Academia,1994300.00,"""We train five DNN columns per normalization, resulting in a total of 35 columns for the entire MCDNN.
[Each DNN has an architecture] 1x29x29-20C4-MP2-40C5-MP3-150N-10N DNN""",3726979200000000.00,"Num of multiply-adds per forward pass
2 FLOPs/mult-add
3 (fp+bp FLOPs / fp FLOPs)
800 epochs
60.000 training size
35 networks

""Training a DNN takes almost 14 hours and after 500 training epochs little additional improvement is observed""",MNIST,,60000,"The MNIST database contains 60,000 training images and 10,000 testing images (Wikipedia)",,25881800.00,"Num mult-add per fp per network
35 networks
2 FLOPs/mult-add",,,,,,$0.08,,Academia,,,2023-11-26 03:25,Robi Rahman,,,Switzerland,IDSIA,,,,MCDNN (MNIST),,,
Dropout (ImageNet),Vision,Image classification,"GE Hinton, N Srivastava, A Krizhevsky",Highly cited,,,https://arxiv.org/abs/1207.0580,7119.00,Improving neural networks by preventing co-adaptation of feature detectors,2012-06-03,University of Toronto,Academia,,"""We achieved comparable performance of 48.6% error using a single neural network with five convolutional hidden layers interleaved with “max-pooling” layer followed by two globally
connected layers and a final 1000-way softmax layer""",273196800000000000.00,"""a single NVIDIA GTX 580 GPU... Training on ImageNet takes
roughly four days with dropout and two days without.""
1.581 TFLOP/s * 4 day * 86400 s/day * 0.5 utilization",ImageNet,,1000000,"In 2010, a subset of 1000 classes
with roughly 1000 examples per class was the basis of an object recognition competition...",,,,96.0,4 days with dropout; 2 days without dropout,NVIDIA GeForce GTX 580,NVIDIA GeForce GTX 580,,,,Academia,,,2023-12-05 04:33,Robi Rahman,,,Canada,University of Toronto,,,,Dropout (ImageNet),,,
Dropout (CIFAR),Vision,Character recognition,"GE Hinton, N Srivastava, A Krizhevsky",Highly cited,,,https://arxiv.org/abs/1207.0580,7119.00,Improving neural networks by preventing co-adaptation of feature detectors,2012-06-03,University of Toronto,Academia,,,4268700000000000.00,"""a single NVIDIA GTX 580 GPU. Training on CIFAR-10 takes roughly 90 minutes"" p17
1.581 TFLOP/s * 90 min * 60 s/min * 0.5 utilization",CIFAR-10,,,,,,,1.5,90 minutes,NVIDIA GeForce GTX 580,NVIDIA GeForce GTX 580,,,,Academia,,,2023-12-05 04:33,Robi Rahman,,,Canada,University of Toronto,,,,Dropout (CIFAR),,,
Dropout (MNIST),Vision,Character recognition,"GE Hinton, N Srivastava, A Krizhevsky",Highly cited,,,https://arxiv.org/abs/1207.0580,7119.00,Improving neural networks by preventing co-adaptation of feature detectors,2012-06-03,University of Toronto,Academia,5592010.00,,6039370800000000.00,"Num mul-add / forward pass
2 FLOPs / mult-add
3 total mult-add / fp mult-add
3000 epochs
60000 training samples",MNIST,,60000,"The MNIST database contains 60,000 training images and 10,000 testing images (Wikipedia)",,11184020.00,"mult-add / fp
* 2 FLOPs / mult-add",,,NVIDIA GeForce GTX 580,NVIDIA GeForce GTX 580,,$0.10,,Academia,,,2023-12-05 04:33,Robi Rahman,,,Canada,University of Toronto,,,,Dropout (MNIST),,,
Dropout (TIMIT),Speech,Speech recognition,"GE Hinton, N Srivastava, A Krizhevsky",Highly cited,,,https://arxiv.org/abs/1207.0580,7119.00,Improving neural networks by preventing co-adaptation of feature detectors,2012-06-03,University of Toronto,Academia,48840185.00,The input to the net is 21 adjacent frames with an advance of 10ms per frame. The neural net has 4 fully-connected hidden layers of 4000 units per layer and 185 “softmax” output units that are subsequently merged into the 39 distinct classes used for the benchmark.,,,TIMIT,,41620,"4162 utterances, guesstimated avg 10 words per utterance",,,,,,NVIDIA GeForce GTX 580,NVIDIA GeForce GTX 580,,,,Academia,,,2023-12-05 04:33,Robi Rahman,,,Canada,University of Toronto,,,,Dropout (TIMIT),,,
LBL,Language,,"Andriy Mnih, Yee Whye Teh",,,,https://arxiv.org/pdf/1206.6426,835.00,A Fast and Simple Algorithm for Training Neural Probabilistic Language Models,2012-06-27,,,2000000.00,,501999999999999.94,,,,,,45.00,,,,,,,,,,,,,2023-11-23 06:53,Robi Rahman,LBL,,,,,,,,,,
Ngram corpus,Language,,"Yuri Lin, Jean-Baptiste Michel, Erez Lieberman Aiden, Jon Orwant, Will Brockman and Slav Petrov",,,,https://aclanthology.org/P12-3029/,489.00,Syntactic Annotations for the Google Books NGram Corpus,2012-07-08,Google,Industry,,,,,,,,,,,,,,,,,,,Industry,,,2023-11-21 01:39,Robi Rahman,,,Multinational,Google,,,,,,,
MV-RNN,Language,Text classification,"R. Socher, B. Huval, C. D. Manning, and A. Y. Ng",Highly cited,,,https://www.aclweb.org/anthology/D12-1110/,1459.00,Semantic Compositionality through Recursive Matrix-Vector Spaces,2012-07-12,Stanford University,Academia,3510255.00,"""We represent a word as both a continuous vector and a matrix of parameters. We initialize all word vectors x ∈ Rn with pre-trained 50-dimensional word vectors from the unsupervised model of Collobert and Weston (2008). [...] Every word is also associated with a matrix X.  [...] If the vectors have dimensionality n, then each word’s matrix has dimensionality X ∈ Rn×n.""

""We propose the following combination function which is input dependent:
p = fA,B(a, b) = f(Ba, Ab) = g(W x (Ba Ab)) ,(2)
where A, B are matrices for single words, the global W ∈ Rn×2n is a matrix that maps both transformed words back into the same n-dimensional space.""

""For computing nonterminal phrase matrices, we define the function
P = fM(A, B) = WMA, B, (3)
where WM ∈ Rn×2n, so P ∈ Rn×n just like each input matrix.""

""If every word is represented by an n-dimensional vector and additionally by an n × n matrix, the dimensionality of the whole model may become too large with commonly used vector sizes of n = 100. In order to reduce the number of parameters, we represent word matrices by the following low-rank plus diagonal approximation: A = UV + diag(a), (5)
where U ∈ Rn×r, V ∈ Rr×n, a ∈ Rnand we set the rank for all experiments to r = 3.""

""We train these representations by adding on top of each parent node a simple softmax classifier
to predict a class distribution over, e.g., sentiment or relationship classes: d(p) = softmax(Wlabelp). If there are K labels, then d ∈ RK is a K-dimensional multinomial distribution""

In total there are V*(n+n*r + r*n) + n*2n + n*2n + (n+1)*k parameters, where n is the vector dimension, r is the low-rank decomposition dimension, V is the vocabulary size and k is the number of classes.

In the experiments we have that n=50, r=3, k=? and V=?. I'm guesstimating k=5 and V=10k.",,,,,,,,,,,,,,,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,United States of America,Stanford University,,,,MV-RNN,,,
Unsupervised High-level Feature Learner,Vision,Image classification,"Quoc V. Le, Marc'Aurelio Ranzato, Rajat Monga, Matthieu Devin, Kai Chen, Greg S. Corrado, Jeff Dean, Andrew Y. Ng","SOTA Improvement,Highly cited","""we trained our network to obtain 15.8% accuracy in recognizing 20,000 object categories from ImageNet, a leap of 70% relative improvement over the previous state-of-the-art""",,https://arxiv.org/abs/1112.6209,2909.00,Building High-level Features Using Large Scale Unsupervised Learning,2012-07-12,Google,Industry,1000000000.00,"""To answer this, we train a 9-layered locally connected sparse autoencoder with pooling and local contrast normalization on a large dataset of images (the model has 1 billion connections, the dataset has 10 million 200x200 pixel images downloaded from the Internet)""",600000000000000000.00,"Assuming 1 epoch, 10 million images and 1 billion parameters, 6*N*D = 6*10^17 FLOP",,10 million 200x200 images extracted from Youtube videos,10000000,10 million 200x200 images extracted from Youtube videos,,,,72.0,"""We train this network using model parallelism and asynchronous SGD on a cluster with 1,000 machines (16,000 cores) for three days. """,,,Unsupervised,,Hardware not reported,Industry,Likely,"We consider the problem of building high-level, class-specific feature detectors from only unlabeled data. For example, is it possible to learn a face detector using only unlabeled images using unlabeled images? To answer this, we train a 9-layered locally connected sparse autoencoder with pooling and local contrast normalization on a large dataset of images (the model has 1 billion connections, the dataset has 10 million 200×200 pixel images downloaded from the Internet). We train this network using model parallelism and asynchronous SGD on a cluster with 1,000 machines (16,000 cores) for three days. Contrary to what appears to be a widely-held intuition, our experimental results reveal that it is possible to train a face detector without having to label images as containing a face or not. Control experiments show that this feature detector is robust not only to translation but also to scaling and out-of-plane rotation. We also find that the same network is sensitive to other high-level concepts such as cat faces and human bodies. Starting with these learned features, we trained our network to obtain 15.8% accuracy in recognizing 20,000 object categories from ImageNet, a leap of 70% relative improvement over the previous state-of-the-art.",2023-11-26 03:25,Anonymous,,,Multinational,Google,,,,Unsupervised High-level Feature Learner,,,
Context-dependent RNN,Language,Language modelling,"Tomas Mikolov, Geoffrey Zweig",SOTA Improvement,New SOTA perplexity on PTB,,https://www.microsoft.com/en-us/research/wp-content/uploads/2012/07/rnn_ctxt_TR.sav_.pdf,707.00,Context Dependent Recurrent Neural Network Language Model,2012-07-27,"Microsoft Research,Brno University of Technology",Industry - Academia Collaboration (Industry leaning),3136400.00,Calculated by Claude 2. Should be manually verified.,,,,,,,,,,,,,,,,,Industry,Unverified,"Recurrent neural network language models (RNNLMs) have recently demonstrated state-of-the-art performance across a variety of tasks. In this paper, we improve their performance by providing a contextual real-valued input vector in association with each word. This vector is used to convey contextual information about the sentence being modeled. By performing Latent Dirichlet Allocation using a block of preceding text, we achieve a topic-conditioned RNNLM. This approach has the key advantage of avoiding the data fragmentation associated with building multiple topic models on different data subsets. We report perplexity results on the Penn Treebank data, where we achieve a new state-of-the-art. We further apply the model to the Wall Street Journal speech recognition task, where we observe 
improvements in word-error-rate.",2023-11-26 03:25,Anonymous,,,"United States of America,Czechia","Microsoft Research, Brno University of Technology",,,,Context-dependent RNN,,,
LSTM-300units,Language,,"Martin Sundermeyer, Ralf Schlüter, Hermann Ney",Highly cited,,,http://www.quaero.org/media/files/bibliographie/sundermeyer_lstm_neural_interspeech2012.pdf,2503.00,LSTM Neural Networks for Language Modeling,2012-09-01,RWTH Aachen,,12000000.00,,,,,,,,,,,,,,,,,,,,,2023-11-26 03:25,Robi Rahman,LSTM-300units,,Germany,RWTH Aachen,,,,LSTM-300units,,,
AlexNet,Vision,Image classification,"Alex Krizhevsky, Ilya Sutskever, Geoffrey Hinton",Highly cited,,,https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html,104026.00,ImageNet Classification with Deep Convolutional Neural Networks,2012-09-30,University of Toronto,Academia,60000000.00,"""Our neural network architecture has 60 million parameters.""",470000000000000000.00,"1.2M images * 90 epochs * 0.75 GFLOP * (2 add-multiply) * (3 backward pass) 
= 470 PF = 0.0054 pfs-days

Source: https://openai.com/blog/ai-and-compute/",ImageNet,,1200000,"""ImageNet is a dataset of over 15 million labeled high-resolution images belonging to roughly 22,000 categories. The images were collected from the web and labeled by human labelers using Amazon’s Mechanical Turk crowd-sourcing tool. Starting in 2010, as part of the Pascal Visual Object Challenge, an annual competition called the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) has been held. ILSVRC uses a subset of ImageNet with roughly 1000 images in each of 1000 categories. In all, there are roughly 1.2 million training images, 50,000 validation images, and 150,000 testing images.""",,,,,,NVIDIA GeForce GTX 580,NVIDIA GeForce GTX 580,,$8.00,,Academia,,,2023-12-05 04:33,Robi Rahman,,,Canada,University of Toronto,,,,AlexNet,,,
RNN+LDA+KN5+cache,Language,,"Tomas Mikolov, Geoffrey Zweig",SOTA Improvement,"""We report perplexity results on the Penn Treebank data, where we achieve a new state-of-the-art""",,https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/rnn_ctxt.pdf,716.00,Context dependent recurrent neural network language model,2012-12-01,"Microsoft,Brno University of Technology",,9000000.00,,,,Penn TreeBank,,,,,,,,,,,,,,,,,2023-11-26 03:25,Robi Rahman,RNN+LDA+KN5+cache,,"Multinational,Czechia","Microsoft, Brno University of Technology",,,,RNN+LDA+KN5+cache,,,
RNN+LDA,Language,,,,,,,,,2012-12-01,,,,,,,,,,,,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,RNN+LDA,1,,,,,,,,,
RNN+LSA+KN5+cache (model combination w/ linear extrapolation),Language,,,,,,,,,2012-12-01,,,,,,,,,,,,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,RNN+LSA+KN5+cache (model combination w/ linear extrapolation),1,,,,,,,,,
RNN,Language,,,,,,,,,2012-12-01,,,,,,,,,,,,,,,,,,,,,,Unverified,,2023-12-08 01:01,Robi Rahman,RNN,1,,,,,,,,,
Bayesian automated hyperparameter tuning,,,"J Snoek, H Larochelle, RP Adams",Highly cited,,,https://arxiv.org/abs/1206.2944,6506.00,Practical Bayesian optimization of machine learning algorithms,2012-12-02,"University of Toronto,University of Sherbrooke,Harvard University",Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-12-05 04:33,Robi Rahman,,,"Canada,Canada,United States of America","University of Toronto, University of Sherbrooke, Harvard University",,,,Bayesian automated hyperparameter tuning,,,
RNN (SGD+CLR) (PTB),Language,,"Yoshua Bengio, Nicolas Boulanger-Lewandowski, Razvan Pascanu",,,,https://arxiv.org/abs/1212.0901,665.00,Advances in Optimizing Recurrent Networks,2012-12-04,,,2050000.00,,,,,,,,,,,,,,,,,,,,,2023-11-21 01:39,Robi Rahman,RNN (SGD+CLR) (PTB),,,,,,,,,,
RNN (SGD+CLR),Audio,,"Yoshua Bengio, Nicolas Boulanger-Lewandowski and Razvan Pascanu",,,,https://arxiv.org/abs/1212.0901,647.00,Advances in Optimizing Recurrent Networks,2012-12-14,Universite de Montréal,Academia,195600.00,"It uses 400 hidden units (selected via hyperparameter tuning)
The input size is 88 (corresponding to the 88 piano pitches)
It uses rectified linear units, so no activation function parameters
So the number of parameters would be:
Input to hidden weights: 88 * 400 = 35,200
Hidden to hidden weights: 400 * 400 = 160,000
Biases: 400
Total: ~195,600 parameters
Above estimate is by Claude 2. Should be checked manually.",,,,"""We evaluate our models on the four polyphonic music datasets of varying complexity used in [25]: classical piano music (Pianomidi.de), folk tunes with chords instantiated from ABC notation (Nottingham), orchestral music (MuseData) and the four-part chorales by J.S. Bach (JSB chorales)""",,,,,,,,,,,,,,Speculative,"After a more than decade-long period of relatively little research activity in the area of recurrent neural networks, several new developments will be reviewed here that have allowed substantial progress both in understanding and in technical solutions towards more efficient training of recurrent networks. These advances have been motivated by and related to the optimization issues surrounding deep learning. Although recurrent networks are extremely powerful in what they can in principle represent in terms of modeling sequences, their training is plagued by two aspects of the same issue regarding the learning of long-term dependencies. Experiments reported here evaluate the use of clipping gradients, spanning longer time ranges
with leaky integration, advanced momentum techniques, using more powerful output probability models, and encouraging sparser gradients to help symmetry breaking and credit assignment. The experiments are performed on text and music data and show off the combined effects of these techniques in generally improving both training and test error",2023-11-21 01:39,Anonymous,,,Canada,Universite de Montréal,,,,,,,
Textual Imager,Vision,,"R Socher, M Ganjoo, H Sridhar, O Bastani",Highly cited,,,https://arxiv.org/abs/1301.3666,1369.00,Zero-Shot Learning Through Cross-Modal Transfer,2013-01-16,Stanford University,Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-12-05 04:33,Robi Rahman,,,United States of America,Stanford University,,,,Textual Imager,,,
Maxout Networks,Vision,Image classification," Ian J. Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, Yoshua Bengio",Highly cited,,,https://arxiv.org/abs/1302.4389,2576.00,Maxout Networks ,2013-02-18,University of Montreal,Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,Canada,University of Montreal,,,,Maxout Networks,,,
PreTrans-3L-250H,Speech,Speech recognition,"Alex Graves, Abdel-rahman Mohamed, Geoffrey Hinton",Highly cited,,,https://arxiv.org/abs/1303.5778,7941.00,Speech Recognition with Deep Recurrent Neural Networks,2013-03-22,University of Toronto,Academia,43000000.00,Table 1,,,,,,,,,,,,,,,,,Academia,,,2023-12-05 04:33,Robi Rahman,,,Canada,University of Toronto,,,,PreTrans-3L-250H,,,
SearchFusion,Vision,Object detection,"JRR Uijlings, KEA Van De Sande, T Gevers",Highly cited,,,https://link.springer.com/article/10.1007/s11263-013-0620-5,5603.00,Selective search for object recognition,2013-04-02,"University of Trento,University of Amsterdam",Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-12-05 04:33,Robi Rahman,,,"Italy,Netherlands","University of Trento, University of Amsterdam",,,,SearchFusion,,,
SemVec,Language,,"T Mikolov, W Yih, G Zweig",Highly cited,,,https://www.aclweb.org/anthology/N13-1090/,3625.00,Linguistic Regularities in Continuous Space Word Representations,2013-06-09,Microsoft Research,Industry,,,,,,,,,,,,,,,,,,,Industry,,,2023-11-26 03:25,Robi Rahman,,,United States of America,Microsoft Research,,,,SemVec,,,
Image Classification with the Fisher Vector: Theory and Practice,Vision,Image Classification,"orge Sanchez, Florent Perronnin, Thomas Mensink, Jakob Verbeek",Highly cited,,,https://hal.inria.fr/hal-00830491v2/document,1707.00,Image Classification with the Fisher Vector: Theory and Practice,2013-06-12,"Universidad Nacional de Cordoba,Inteligent Systems Lab Amsterdam,University of Amsterdam,LEAR Team,INRIA,Xerox Research Centre Europe (XRCE)",Industry - Academia Collaboration,,,90842400000000.00,"They use a Intel Xeon E5-2470 Processor for 2 hours. This can do 12,617 MOps/Sec 
https://www.cpubenchmark.net/cpu.php?cpu=Intel+Xeon+E5-2470+%40+2.30GHz&id=2003",ImageNet,,,,,,,2.0,,,,,$0.00,,,,,2023-11-26 03:25,Robi Rahman,,,"Argentina,Netherlands,Netherlands,France,France,France","Universidad Nacional de Cordoba, Inteligent Systems Lab Amsterdam, University of Amsterdam, LEAR Team, INRIA, Xerox Research Centre Europe (XRCE)",,,,Image Classification with the Fisher Vector: Theory and Practice,,,
RNN+weight noise+dynamic eval,Language,,Alex Graves,Highly cited,,,https://arxiv.org/abs/1308.0850,4734.00,Generating Sequences With Recurrent Neural Networks,2013-08-04,University of Toronto,,54000000.00,,4210000000000000.00,,,,,,14.00,,,,,,,,,,,,,2023-11-26 03:25,Robi Rahman,RNN+weight noise+dynamic eval,,Canada,University of Toronto,,,,RNN+weight noise+dynamic eval,,,
Mitosis,Vision,,"Dan C. Cireşan, Alessandro Giusti, Luca M. Gambardella, Jürgen Schmidhuber",ICPR 2012 mitosis detection competition winner,,,https://link.springer.com/chapter/10.1007/978-3-642-40763-5_51,1456.00,Mitosis Detection in Breast Cancer Histology Images with Deep Neural Networks,2013-09-22,IDSIA,Academia,37230.00,Sum numbers of weights in Table 1.b,137000000000000000.00,"""Training each network requires one day of computation with an optimized GPU
implementation""

Assuming 1.58E+12 FLOP/second on FP32 (from the table in the Estimating compute post), we get

3600*24*1.58E+12 = 1.37E+17 FLOP",,,1000000,"The dataset is built in two stages. First a classifier is trained on small sample, and used to curate a more representative larger dataset.

The final dataset has 1M instances

""We build the actual training set, composed by 1 million instances, which includes
all mitosis pixels (6.6% of the training instances). The remaining 95.4% is sampled
from non-mitosis pixels by assigning to each pixel p a weight D(p).""",,,,,,,,,$2.00,,Academia,,,2023-11-26 03:25,Robi Rahman,,,Switzerland,IDSIA,,,,Mitosis,,,
Word2Vec (small),Language,Semantic embedding,"T Mikolov, I Sutskever, K Chen, GS Corrado",Highly cited,,,https://arxiv.org/abs/1310.4546,30712.00,Distributed Representations of Words and Phrases and their Compositionality,2013-10-16,Google,Industry,207600000.00,"""We discarded from the vocabulary all words that occurred less than 5 times in the training data, which resulted in a vocabulary of size 692K [...] Starting with the same news data as in the previous experiments, we first constructed the phrase based training corpus and then we trained several Skip-gram models using different hyperparameters. As before, we used vector dimensionality 300 and context size 5.""",,,,,692000,"""For training the Skip-gram models, we have used a large dataset consisting of various news articles (an internal Google dataset with one billion words). We discarded from the vocabulary all words that occurred less than 5 times in the training data, which resulted in a vocabulary of size 692K""",,,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,Multinational,Google,,,,Word2Vec (small),,,
Word2Vec (large),Language,Semantic embedding,"T Mikolov, I Sutskever, K Chen, GS Corrado",Highly cited,,,https://arxiv.org/abs/1310.4546,30712.00,Distributed Representations of Words and Phrases and their Compositionality,2013-10-16,Google,Industry,692000000.00,"""To maximize the accuracy on the phrase analogy task, we increased the amount of the training data by using a dataset with about 33 billion words. We used the hierarchical softmax, dimensionality of 1000, and the entire sentence for the context.""",38880000000000000.00,"From https://openai.com/blog/ai-and-compute/ Appendix.

""less than 0.00045 pfs days""
(86400*10^15*0.00045)",,,33000000000,"""For training the Skip-gram models, we have used a large dataset consisting of various news articles (an internal Google dataset with one billion words). We discarded from the vocabulary all words that occurred less than 5 times in the training data, which resulted in a vocabulary of size 692K""",,,,,,,,,$0.55,,Industry,,,2023-12-05 04:33,Robi Rahman,,,Multinational,Google,,,,Word2Vec (large),,,
R-CNN (T-net),Vision,Object detection,"Ross Girshick, Jeff Donahue, Trevor Darrell, Jitendra Malik",Highly cited,,,https://arxiv.org/abs/1311.2524,22627.00,Rich feature hierarchies for accurate object detection and semantic segmentation,2013-11-11,UC Berkeley,Academia,69003872.00,"Computed from architecture description in Caffee

https://nbviewer.jupyter.org/github/BVLC/caffe/blob/master/examples/detection.ipynb",,,,,,,,,,,,,,,,,Academia,,,2023-12-05 04:33,Robi Rahman,,,United States of America,UC Berkeley,,,,R-CNN (T-net),,,
Visualizing CNNs,Vision,,"MD Zeiler, R Fergus",Highly cited,,,https://arxiv.org/abs/1311.2901,14239.00,Visualizing and Understanding Convolutional Networks,2013-11-12,New York University (NYU),Academia,,,532000000000000000.00,"1 GPU * 12 days * 1.54 TFLOPS/GTX 580 * 0.33 utilization 
= 532 PF = 0.0062 pfs-days

Source: https://openai.com/blog/ai-and-compute",,,,,,,,,,NVIDIA GeForce GTX 580,NVIDIA GeForce GTX 580,,$9.02,,Academia,,,2023-12-05 04:33,Robi Rahman,,,United States of America,New York University (NYU),,,,Visualizing CNNs,,,
TensorReasoner,,,"R Socher, D Chen, CD Manning, A Ng",Highly cited,,,https://papers.nips.cc/paper/2013/hash/b337e84de8752b27eda3a12363109e80-Abstract.html,1825.00,Reasoning With Neural Tensor Networks for Knowledge Base Completion,2013-12-01,Stanford University,Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-12-05 04:33,Robi Rahman,,,United States of America,Stanford University,,,,TensorReasoner,,,
TransE,Other,Entity embedding,"Antoine Bordes, Nicolas Usunier, Alberto Garcia- Duran, Jason Weston, and Oksana Yakhnenko",Highly cited,,,https://papers.nips.cc/paper/2013/hash/1cecc7a77928ca8133fa24680a88d2f9-Abstract.html,7039.00,Translating Embeddings for Modeling Multi- relational Data,2013-12-05,"Universite de Technologie de Compiègne – CNRS,Google",Industry - Academia Collaboration,942000000.00,"Based on the TransE architecture, the authors give a formula for how the model size scales with the dimensionality of the dataset. The model scale is proportional to: k*(n_e+n_r) where k is the embeddings dimension, n_e is the number of entities, and n_r is the number of relationships.

They studied using the TransE model for two datasets: FB15k and FB1M. The FB15k model has 810000 parameters.

FB15k has 14951 entities and 1345 relationships. FB1M has 1000000 entities and 23382 relationships. Therefore, the FB1M model will be bigger than the FB15k model by a factor of (23382e6)/(14951*1345) => N = 8.1e5 * (23382e6)/(14951*1345) = 942e6.",1340928000000000000.00,"8 GPUs (they don't specify which, so I used the average for FP32 for 2017 from the write-up table)
8 hours 
0.33 util rate",,,17000000,"""it can be successfully trained on a large scale data set with 1M
entities, 25k relationships and more than 17M training samples""",,,,,,,,,$17.58,,Industry,Speculative,,2023-11-26 03:25,Robi Rahman,,,"France,Multinational","Universite de Technologie de Compiègne – CNRS, Google",,,,TransE,,,
DBLSTM,Speech,Speech recognition,"A Graves, N Jaitly, A Mohamed",Highly cited,,,https://ieeexplore.ieee.org/document/6707742,1516.00,Hybrid speech recognition with Deep Bidirectional LSTM,2013-12-08,University of Toronto,Academia,29900000.00,"""The DBLSTM network had five bidirectional hidden levels, with 500 LSTM cells in each of the forward and backward
layers, and a size 3385 softmax output layer, giving a total of
29.9M weights.""",,,,,,,,,,,,,,,,,Academia,,,2023-12-05 04:33,Robi Rahman,,,Canada,University of Toronto,,,,DBLSTM,,,
Deep RNN,Language,,,,,,,,,2013-12-11,,,,,,,,,,,64.00,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,Deep RNN,1,,,,,,,,,
Network in Network,,,"M Lin, Q Chen, S Yan",Highly cited,,,https://arxiv.org/abs/1312.4400,5499.00,Network In Network,2013-12-16,National University of Singapore,Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,Singapore,National University of Singapore,,,,Network in Network,,,
DQN,Games,Atari,"V Mnih, K Kavukcuoglu, D Silver, A Graves",Highly cited,,,https://arxiv.org/abs/1312.5602,9696.00,Playing Atari with Deep Reinforcement Learning,2013-12-19,DeepMind,Industry,836096.00,"""The input to the neural network consists is an 84 × 84 × 4 image produced by φ. The first hidden layer convolves 16 8 × 8 filters with stride 4 with the input image and applies a rectifier nonlinearity [10, 18]. The second hidden layer convolves 32 4 × 4 filters with stride 2, again followed by a rectifier nonlinearity. The final hidden layer is fully-connected and consists of 256 rectifier units. The output layer is a fully connected linear layer with a single output for each valid action. The number of valid actions varied between 4 and 18 on the games we considered.""",2300000000000000.00,"Network is 84x84x3 input, 16, 8x8, stride 4, 32 4x4 stride 2, 256 fully connected
First layer: 20*20*3*16*8*8 = 1.23M add-multiplies
Second layer: 9*9*16*32*4*4 = 0.66M add-multiplies
Third layer: 9*9*32*256 = 0.66M add-mutliplies
Total ~ 2.55M add-multiplies
2.5 MFLOPs * 5M updates * 32 batch size * 2 multiply-add * 3 backward pass
= 2.3 PF = 2.7e-5 pfs-days

",,,,,,,,,,,,,$0.04,,Industry,,,2023-12-05 04:33,Robi Rahman,,,United Kingdom of Great Britain and Northern Ireland,DeepMind,,,,DQN,,,
Image generation,Vision,Image clustering,"DP Kingma, M Welling",Highly cited,,,https://arxiv.org/abs/1312.6114,21760.00,Auto-Encoding Variational Bayes,2013-12-20,University of Amsterdam,Academia,,,475200000000000.00,"From https://openai.com/blog/ai-and-compute/ Appendix

""less than 0.0000055 pfs-days""
(86400*10^15*0.0000055)",MNIST,,60000,"""We trained generative models of images from the MNIST and Frey Face datasets""

MNIST has 60k images
https://en.wikipedia.org/wiki/MNIST_database

Frey Face has 2k images
https://cs.nyu.edu/~roweis/data.html",,,,,,,,,$0.01,,Academia,,,2023-12-05 04:33,Robi Rahman,,,Netherlands,University of Amsterdam,,,,Image generation,,,
DOT(S)-RNN,Language,,"Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Yoshua Bengio",Highly cited,,,https://arxiv.org/pdf/1312.6026.pdf,1255.00,How to Construct Deep Recurrent Neural Networks,2013-12-20,"Université de Montréal,Aalto University",,6160000.00,,,,,,,,,,,,,,,,,,,,,2023-11-26 03:25,Robi Rahman,DOT(S)-RNN,,"Canada,Finland","Université de Montréal, Aalto University",,,,DOT(S)-RNN,,,
OverFeat,Vision,Image classification,"Pierre Sermanet, David Eigen, Xiang Zhang, Michael Mathieu, Rob Fergus, Yann LeCun",Highly cited,,,https://arxiv.org/abs/1312.6229,5148.00,"OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks",2013-12-21,New York University (NYU),Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,United States of America,New York University (NYU),,,,OverFeat,,,
GloVe (6B),Language,Semantic embedding,"J Pennington, R Socher, CD Manning",Highly cited,,,https://nlp.stanford.edu/projects/glove/,28349.00,GloVe: Global Vectors for Word Representation,2014-01-01,Stanford University,Academia,120000000.00,400k vocab * 300 vector dimensions,,"""The total run-time is split between populating X
and training the model. The former depends on
many factors, including window size, vocabulary
size, and corpus size. Though we did not do so,
this step could easily be parallelized across multiple machines (see, e.g., Lebret and Collobert
(2014) for some benchmarks). Using a single
thread of a dual 2.1GHz Intel Xeon E5-2658 machine, populating X with a 10 word symmetric
context window, a 400,000 word vocabulary, and
a 6 billion token corpus takes about 85 minutes.
Given X, the time it takes to train the model depends on the vector size and the number of iterations. For 300-dimensional vectors with the above settings (and using all 32 cores of the above machine), a single iteration takes 14 minutes. See Fig. 4 for a plot of the learning curve""

""We run 50 iterations for vectors smaller than
300 dimensions, and 100 iterations otherwise (see
Section 4.6 for more details about the convergence
rate).""

Details of dual 2.1GHz Intel Xeon E5-2658 machine:
https://www.intel.com/content/www/us/en/products/sku/61428/intel-xeon-processor-e52658-20m-2-10-ghz-8-0-gts-intel-qpi/specifications.html",Gigaword5 + Wikipedia2014,,6000000000,"""We trained our model on five corpora of varying sizes: a 2010 Wikipedia dump with 1 billion tokens; a 2014 Wikipedia dump with 1.6 billion tokens; Gigaword 5 which has 4.3 billion tokens; the combination Gigaword5 + Wikipedia2014, which has 6 billion tokens; and on 42 billion tokens of web data, from Common Crawl

[To demonstrate the scalability of the model, we also trained it on a much larger sixth corpus, containing 840 billion tokens of web data, but in this case we did not lowercase the vocabulary, so the results are not directly comparable.]""",,,Embeddings are precalculated,,"Section 4.6 in original paper (https://nlp.stanford.edu/pubs/glove.pdf)

85 min to populate coocurrence matrix
+ 25 training iterations

Each iteration takes 14 minutes on 32 cores ",,,,,,Academia,,,2023-12-05 04:33,Robi Rahman,,,United States of America,Stanford University,,,,GloVe (6B),,,
GloVe (32B),Language,Semantic embedding,"J Pennington, R Socher, CD Manning",Highly cited,,,https://nlp.stanford.edu/projects/glove/,28349.00,GloVe: Global Vectors for Word Representation,2014-01-01,Stanford University,Academia,120000000.00,400k vocab * 300 vector dimensions,,"""The total run-time is split between populating X
and training the model. The former depends on
many factors, including window size, vocabulary
size, and corpus size. Though we did not do so,
this step could easily be parallelized across multiple machines (see, e.g., Lebret and Collobert
(2014) for some benchmarks). Using a single
thread of a dual 2.1GHz Intel Xeon E5-2658 machine, populating X with a 10 word symmetric
context window, a 400,000 word vocabulary, and
a 6 billion token corpus takes about 85 minutes.
Given X, the time it takes to train the model depends on the vector size and the number of iterations. For 300-dimensional vectors with the above settings (and using all 32 cores of the above machine), a single iteration takes 14 minutes. See Fig. 4 for a plot of the learning curve""

""We run 50 iterations for vectors smaller than
300 dimensions, and 100 iterations otherwise (see
Section 4.6 for more details about the convergence
rate).""

But we are interested in the 42B token model",Common Crawl,,42000000000,"""We trained our model on five corpora of varying sizes: a 2010 Wikipedia dump with 1 billion tokens; a 2014 Wikipedia dump with 1.6 billion tokens; Gigaword 5 which has 4.3 billion tokens; the combination Gigaword5 + Wikipedia2014, which has 6 billion tokens; and on 42 billion tokens of web data, from Common Crawl

[To demonstrate the scalability of the model, we also trained it on a much larger sixth corpus, containing 840 billion tokens of web data, but in this case we did not lowercase the vocabulary, so the results are not directly comparable.]""",,,Embeddings are precalculated,,"Section 4.6 in original paper (https://nlp.stanford.edu/pubs/glove.pdf)

85 min to populate coocurrence matrix
+ 25 training iterations

Each iteration takes 14 minutes on 32 cores ",,,,,,Academia,,,2023-12-05 04:33,Robi Rahman,,,United States of America,Stanford University,,,,GloVe (32B),,,
SPN-4+KN5,Language,,"W. Cheng, Stanley Kok, Hoai Vu Pham, Hai Leong Chieu, K. M. A. Chai",SOTA Improvement,"""Our empirical comparisons with
six previous language models indicate that our SPN has superior
performance""",,https://spn.cs.washington.edu/papers/is14.pdf,102.00,Language modeling with sum-product networks,2014-01-01,"Singapore University of Technology & Design,DSO National Laboratories",,5000000.00,,44000000000000000.00,,Penn TreeBank,,,,,,,,,,,,,,,,,2023-11-26 03:25,Robi Rahman,SPN-4+KN5,,"Singapore,Singapore","Singapore University of Technology & Design, DSO National Laboratories",,,,SPN-4+KN5,,,
SPN-4,Language,,,,,,,,,2014-01-01,,,,,,,,,,,,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,SPN-4,1,,,,,,,,,
DBN for NLP,Language,Text classification,"R Sarikaya, GE Hinton, A Deoras",,,,https://ieeexplore.ieee.org/document/6737243,445.00,Application of Deep Belief Networks for Natural Language Understanding,2014-02-11,"Microsoft,University of Toronto",Industry - Academia Collaboration,1021535.00,"Assuming 1000 input features, 35 classes and 3 hidden layers of 500 units each",,,,,178000,The training data has 27K automatically transcribed utterances amounting to 178K words.,,,,,,,,,,,Industry,,,2023-11-21 01:39,Robi Rahman,,,"Multinational,Canada","Microsoft, University of Toronto",,,,,,,
HyperNEAT,Games,Atari Games,"M Hausknecht, J Lehman",SOTA Improvement,"""Neuroevolution ameliorates these problems and evolved policies achieve state-of-the-art results, even surpassing human high scores on three games""",,https://ieeexplore.ieee.org/abstract/document/6756960,195.00,A Neuroevolution Approach to General Atari Game Playing,2014-03-05,University of Texas at Austin,Academia,239712.00,"""The ANN consists of three layers (Fig. 3): a substrate layer inwhich information from the game screen (raw pixels, objects, ornoise) is given as input to the network; a processing layer whichadds a nonlinear internal representation; and a nonlinear outputlayer from which actions are read and conveyed to the Atari em-ulator. Both the input and output layers are fully connected tothe processing layer. The substrate dimensionality of the inputand processinglayers is 810 in the case of the object repre-sentation and 1621 for the pixel and noise representations.3The output layer consists of a 33 substrate mirroring the ninepossible directions of the Atari 2600 joystick and a single noderepresenting thefire button""",,,,,,,,,,,,,,,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,United States of America,University of Texas at Austin,,,,HyperNEAT,,,
Dropout (2014),,,"Nitish Shrivasta, Geoffrey Hinton, Alex Krizhevsky",Highly cited,,,https://jmlr.org/papers/v15/srivastava14a.html,35145.00,Dropout: A Simple Way to Prevent Neural Networks from Overfitting,2014-06-01,University of Toronto,Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-12-05 04:33,Robi Rahman,,,Canada,University of Toronto,,,,Dropout (2014),,,
GRUs,Language,,"K Cho, B Van Merriënboer, C Gulcehre",Highly cited,,,https://arxiv.org/abs/1406.1078,19663.00,Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation,2014-06-03,"University of Montreal,Jacobs University,University of Maine",Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-12-05 04:33,Robi Rahman,,,"Canada,Germany,United States of America","University of Montreal, Jacobs University, University of Maine",,,,GRUs,,,
Two-stream ConvNets for action recognition,Video,Video classification,"Karen Simonyan, Andrew Zisserman",Highly cited,,,https://arxiv.org/abs/1406.2199,6752.00,Two-Stream Convolutional Networks for Action Recognition in Videos,2014-06-09,University of Oxford,Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-12-05 04:33,Robi Rahman,,,United Kingdom of Great Britain and Northern Ireland,University of Oxford,,,,Two-stream ConvNets for action recognition,,,
GANs,Drawing,Image generation,"Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio",Highly cited,,,https://arxiv.org/abs/1406.2661,36870.00,Generative Adversarial Networks,2014-06-10,Universite de Montréal,Academia,,The paper outlines the G-D framework but doesn't provide information about the structures of their generator and discriminator.,518400000000000000.00,"From https://openai.com/blog/ai-and-compute/ Appendix

""Less than 0.006 pfs-days""
(86400*10^15*0.006)

Seems extremely speculative, unless someone at OpenAI privately corresponded with the authors. There is no information about compute or training in the GANs paper.",CIFAR-10,,60000,"""We trained adversarial nets an a range of datasets including MNIST[23], the Toronto Face Database (TFD) [28], and CIFAR-10 [21].""

MNIST has 60k images 
https://en.wikipedia.org/wiki/MNIST_database

TFD seems to have 2925 examples (?)
https://www.cs.toronto.edu/~urtasun/courses/CSC411/hw3-411.pdf

CIFAR-10 has 60k images
https://www.cs.toronto.edu/~kriz/cifar.html

",,,,,,,,,$6.09,,Academia,Speculative,,2023-11-26 03:25,Robi Rahman,,,Canada,Universite de Montréal,,,,GANs,,,
SPPNet,Vision,Image classification,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun",Highly cited,,,https://arxiv.org/abs/1406.4729,8981.00,Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition,2014-06-18,"Microsoft,Xi’an Jiaotong University,University of Science and Technology of China",Industry - Academia Collaboration,,,3411072000000000000.00,"""All networks in this paper can be
trained on a single GeForce GTX Titan GPU (6 GB memory) within two to four weeks.""
4.7e12 FLOP/s * 4* 7*24*60*60 seconds * 0.3 utilisation",Imagenet-1k,,1280000,"Section 3.1: ""We train the networks on the 1000-category training
set of ImageNet 2012.""",,,,672.0,"""All networks in this paper can be trained on a single GeForce GTX Titan GPU (6 GB memory) within two to four weeks.""",NVIDIA GeForce GTX TITAN,NVIDIA GeForce GTX TITAN,,$65.07,,Industry,,,2023-12-05 04:33,Robi Rahman,,,"Multinational,China,China","Microsoft, Xi’an Jiaotong University, University of Science and Technology of China",,,,SPPNet,,,
RNN-WER,Speech,Speech recognition,"Alex Graves, Navdeep Jaitly","SOTA Improvement,Highly cited","""Finally, by combining the new model with a baseline, we
have achieved state-of-the-art accuracy on the Wall Street
Journal corpus for speaker independent recognition.""",,https://proceedings.mlr.press/v32/graves14.html,2805.00,Towards End-To-End Speech Recognition with Recurrent Neural Networks,2014-06-22,"DeepMind,University of Toronto",,26500000.00,"""The network had five levels of bidirectional LSTM hidden layers, with 500 cells in each layer, giving a total of ∼ 26.5M weights.""",,,WSJ,"""The experiments were carried out on the Wall Street Journal (WSJ) corpus (available as LDC corpus LDC93S6B
and LDC94S13B). The RNN was trained on both the 14
hour subset ‘train-si84’ and the full 81 hour set""

",1100000,"dataset is 81 hours

At 228 wpm (https://docs.google.com/document/d/1G3vvQkn4x_W71MKg0GmHVtzfd9m0y3_Ofcoew0v902Q/edit)
that's 81*228*60 = 1,108,080

another source says WSJ contains 37k sentences, so this would be ~30 words per sentence which seems high but roughly right: https://www.arxiv-vanity.com/papers/1903.00216/",,,,,,,,Supervised,,,,Likely,"This paper presents a speech recognition system that directly transcribes audio data with text, without requiring an intermediate phonetic representation. The system is based on a combination of the deep bidirectional LSTM recurrent neural network architecture and the Connectionist Temporal Classification objective function. A modification to the objective function is introduced that trains the network to minimise the expectation of an arbitrary transcription loss function. This allows a direct optimisation of the word error rate, even in the absence of a lexicon or language model. The system achieves a word error rate of 27.3% on the Wall Street Journal corpus with no prior linguistic information, 21.9% with only a lexicon of allowed words, and 8.2% with a trigram language model. Combining the network with a baseline system further reduces the error rate to 6.7%.",2023-12-14 07:26,Anonymous,,0,"United Kingdom of Great Britain and Northern Ireland,Canada","DeepMind, University of Toronto",,,,,,,
Multiresolution CNN,Video,Video classification,"A Karpathy, G Toderici, S Shetty, T Leung",Highly cited,,,https://ieeexplore.ieee.org/document/6909619,5915.00,Large-Scale Video Classification with Convolutional Neural Networks,2014-06-23,"Google,Stanford University",Industry - Academia Collaboration,126125568.00,"""Using shorthand notation, the full [single frame] architecture is C(96, 11, 3)-N-P-C(256, 5, 1)-N-P-C(384, 3, 1)-C(384, 3, 1)-C(256, 3, 1)-P-FC(4096)-FC(4096), where C(d, f, s) indicates a convolutional layer with d filters of spatial size f ×f, applied to the input with stride s""

Two such single-frame architectures are concatenated as shown in figure 2

""Since the input is only of half the
spatial size as the full-frame models, we take out the last
pooling layer to ensure that both streams still terminate in a
layer of size 7×7×256. ""

We assume the input are T=10 frames with C=3 color channels each

2*(256*(10*3*5*5+1) + 384*(256*3*3+1) + 384*(384*3*3+1) + 256*(384*3*3+1)) + (2*7*7*256 + 1)*4096 + (4096+1)*4096



",,,,,50000000,"""We further estimate the size of our dataset of sampled frames to be on the order of 50 million examples and that our networks have each seen approximately 500 million examples throughout the training period in total.""

So 5e+7 datapoints and 10 epochs.",,,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,"Multinational,United States of America","Google, Stanford University",,,,Multiresolution CNN,,,
DeepFace,Vision,Face verification,"Y Taigman, M Yang, MA Ranzato",Highly cited,,,https://ieeexplore.ieee.org/document/6909616,5772.00,DeepFace: Closing the Gap to Human-Level Performance in Face Verification,2014-06-23,"Tel Aviv University,Facebook",Industry - Academia Collaboration,,,,,,,,,,,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,"Israel,Multinational","Tel Aviv University, Facebook",,,,DeepFace,,,
SmooCT,Games,,"Johannes Heinrich, David Silver",SOTA improvement,First RL system to achieve superhuman level at Poker Limit Texas Hold Em,,https://www.semanticscholar.org/paper/Self-play-Monte-Carlo-tree-search-in-computer-poker-Heinrich-Silver/7b687599b4425aa959036071030e1212a3b359c7,16.00,Self-Play Monte-Carlo Tree Search in Computer Poker,2014-07-01,University College London (UCL),Academia,,,69000000000000000.00,"""Each three-player agent was trained for about 12 billion episodes, requiring about 48 hours of training time [...] on a modern computer without using parallelization""

Assume an Intel i7 so 400e9 FLOP/s.
6.9e16 = 400e9*60*60*48",,,12000000000,"""Each three-player agentwas trained for about 12 billion episodes""

An episode seems to be a round of betting.",,,,48.0,,,,,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,United Kingdom of Great Britain and Northern Ireland,University College London (UCL),,,,SmooCT,,,
AdClickNet,,,"X He, J Pan, O Jin, T Xu, B Liu, T Xu, Y Shi",,,,https://dl.acm.org/doi/10.1145/2648584.2648589,798.00,Practical Lessons from Predicting Clicks on Ads at Facebook,2014-08-24,Facebook,Industry,,,,,,,,,,,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,Multinational,Facebook,,,,,,,
RNNsearch-50*,Language,Translation,"D Bahdanau, K Cho, Y Bengio",Highly cited,,,https://arxiv.org/abs/1409.0473,24155.00,Neural Machine Translation by Jointly Learning to Align and Translate,2014-09-01,"Universite de Montréal,Jacobs University Bremen",Academia,,,1555200000000000000.00,"From https://openai.com/blog/ai-and-compute/ Appendix.

0.018 pfs-days
(86400*10^15*0.018)

252 hours in a Quadro K-6000 GPU",WMT'14 + selection,,348000000,"[WORDS]
""WMT ’14 contains the following English-French parallel corpora: Europarl (61M words), news
commentary (5.5M), UN (421M) and two crawled corpora of 90M and 272.5M words respectively,
totaling 850M words. Following the procedure described in Cho et al. (2014a), we reduce the size of
the combined corpus to have 348M words using the data selection method by Axelrod et al. (2011).""",,,,,,,,,$81.48,,Academia,,,2023-12-05 04:33,Robi Rahman,,,"Canada,Germany","Universite de Montréal, Jacobs University Bremen",,,,RNNsearch-50*,,,
VGG16,Vision,,Karen Simonyan; Andrew Zisserman,Highly cited,,,https://arxiv.org/abs/1409.1556,83998.00,Very Deep Convolutional Networks for Large-Scale Image Recognition,2014-09-04,University of Oxford,Academia,138000000.00,"Source: Table 2
https://arxiv.org/abs/1409.1556",9253440000000000000.00,"2.5 weeks * 4 Titan Black GPUs * 0.30 utilization

Section 3.3: ""On a system equipped with
four NVIDIA Titan Black GPUs, training a single net took 2–3 weeks depending on the architecture.""



",ILSVRC-2012,,1300000,"""In this section, we present the image classification results achieved by the described
ConvNet architectures on the ILSVRC-2012 dataset (which was used for ILSVRC 2012–2014 challenges). The dataset includes images of 1000 classes, and is split into three sets: training (1.3M images), validation (50K images), and testing (100K images with held-out class labels).""",,15300000000.00,"""Remarkably, although the depth is significantly increased, the 152-layer ResNet (11.3 billion FLOPs) still has lower complexity than VGG-16/19 nets (15.3/19.6 billion FLOPs).""

Source: https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf",,,NVIDIA GTX Titan Black,NVIDIA GTX Titan Black,,$82.80,,Academia,,,2023-12-05 04:33,Robi Rahman,,,United Kingdom of Great Britain and Northern Ireland,University of Oxford,,,,VGG16,,,
VGG19,Vision,,"K Simonyan, A Zisserman",Highly cited,,,https://arxiv.org/abs/1409.1556,83998.00,Very Deep Convolutional Networks for Large-Scale Image Recognition,2014-09-04,University of Oxford,Academia,144000000.00,"Source: Table 2
https://arxiv.org/abs/1409.1556",,,ILSVRC-2012,,1300000,"""In this section, we present the image classification results achieved by the described
ConvNet architectures on the ILSVRC-2012 dataset (which was used for ILSVRC 2012–2014 challenges). The dataset includes images of 1000 classes, and is split into three sets: training (1.3M images), validation (50K images), and testing (100K images with held-out class labels).""",,19600000000.00,"""Remarkably, although the depth is significantly increased, the 152-layer ResNet (11.3 billion FLOPs) still has lower complexity than VGG-16/19 nets (15.3/19.6 billion FLOPs).""

Source: https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf",,,,,,,,Academia,,,2023-12-05 04:33,Robi Rahman,,,United Kingdom of Great Britain and Northern Ireland,University of Oxford,,,,VGG19,,,
Large regularized LSTM,Language,,"Wojciech Zaremba, Ilya Sutskever, Oriol Vinyals",Highly cited,,,https://arxiv.org/abs/1409.2329,3224.00,Recurrent Neural Network Regularization,2014-09-08,"New York University (NYU),Google Brain",,66000000.00,,91000000000000000.00,,Penn TreeBank,,,,55.00,,,,,,,,,,,,,2023-11-26 03:25,Robi Rahman,Large regularized LSTM,,"United States of America,Multinational","New York University (NYU), Google Brain",,,,Large regularized LSTM,,,
Seq2Seq LSTM,Language,Translation,"I Sutskever, O Vinyals, QV Le",Highly cited,,,https://arxiv.org/abs/1409.3215,18102.00,Sequence to Sequence Learning with Neural Networks,2014-09-10,Google,Industry,1920000000.00,"The resulting LSTM has 384M parameters of which 64M are pure recurrent connections (32M for the “encoder” LSTM and 32M
for the “decoder” LSTM).
The paper uses an ensemble of 5 LSTMs.",56000000000000000000.00,"384E+6 parameters * 2 FLOP/parameter * (348E+6 + 304E+6 points per epoch) * 7.5 epochs * 3 FLOP/point ~= 1.126656e+19 FLOP
Times 5 independent models in ensemble => 5.6E+19 FLOP

If we assume NVIDIA K40 (in use at the time): 10 days * 24 * 60 * 60 seconds/day * 8 GPUs * 33% * 5e12 FLOP/s * 5 models in ensemble ~= 5.7E+19 FLOP",WMT'14 dataset,,652000000,"[WORDS]
""We used the WMT’14 English to French dataset. We trained our models on a subset of 12M sentences consisting of 348M French words and 304M English words, which is a clean “selected”
subset from [29].""",,,,,,,,,$79.60,,Industry,,,2023-12-05 04:33,Robi Rahman,,,Multinational,Google,,,,Seq2Seq LSTM,,,
Deeply-supervised nets,Vision,Image classification,"Chen-Yu Lee, Saining Xie, Patrick Gallagher, Zhengyou Zhang, Zhuowen Tu","Highly cited,SOTA improvement",,,https://arxiv.org/abs/1409.5185,2509.00,Deeply-Supervised Nets,2014-09-18,Microsoft Research,Industry - Academia Collaboration (Academia leaning),,,,,"MNIST, CIFAR-10, CIFAR-100, SVHN","According to the paper, the Deeply-Supervised Nets (DSN) model was trained and evaluated on these image classification datasets:

MNIST - handwritten digits dataset with 60,000 training images and 10,000 test images.
CIFAR-10 - 60,000 32x32 color images across 10 classes, with 50,000 for training and 10,000 for testing.
CIFAR-100 - similar to CIFAR-10 but with 100 classes and 600 images per class.
SVHN - Street View House Numbers dataset with over 600,000 images of digits for training and 26,000 images for testing.",870000,60000+50000+60000+600000,,,,,,,,,,,Industry,,,2023-11-26 03:25,Robi Rahman,,,United States of America,Microsoft Research,,,,Deeply-supervised nets,,,
Spatially-Sparse CNN,Vision,Image Classification,Benjamin Graham,SOTA Improvement,SOTA per https://paperswithcode.com/sota/image-classification-on-cifar-10,,https://arxiv.org/abs/1409.6070v1,260.00,Spatially-sparse convolutional neural networks,2014-09-23,University of Warwick,Academia,,Parameter count not stated but is probably derivable from the paper.,,,CIFAR-10,,,,,,,,,,,,,,,Likely,"Convolutional neural networks (CNNs) perform well on problems such as handwriting recognition and image classification. However, the performance of the networks is often limited by budget and time constraints, particularly when trying to train deep networks.
Motivated by the problem of online handwriting recognition, we developed a CNN for processing spatially-sparse inputs; a character drawn with a one-pixel wide pen on a high resolution grid looks like a sparse matrix. Taking advantage of the sparsity allowed us more efficiently to train and test large, deep CNNs. On the CASIA-OLHWDB1.1 dataset containing 3755 character classes we get a test error of 3.82%.
Although pictures are not sparse, they can be thought of as sparse by adding padding. Applying a deep convolutional network using sparsity has resulted in a substantial reduction in test error on the CIFAR small picture datasets: 6.28% on CIFAR-10 and 24.30% for CIFAR-100.",2023-11-26 03:25,Anonymous,,,United Kingdom of Great Britain and Northern Ireland,University of Warwick,,,,Spatially-Sparse CNN,,,
LRCN,Vision,Video description,"Jeff Donahue, Lisa Anne Hendricks, Marcus Rohrbach, Subhashini Venugopalan, Sergio Guadarrama, Kate Saenko, Trevor Darrell",Highly cited,,,https://arxiv.org/abs/1411.4389,5625.00,Long-term Recurrent Convolutional Networks for Visual Recognition and Description,2014-11-07,"UT Austin,University of Massachusetts Lowell,UC Berkeley",Academia,142552000.00,"1st model: CaffeNet fc6 feature extractor (4096-length vectors) -> LSTM with 1024 hidden units

2nd model: CaffeNet fc6 feature extractor (4096-length vectors) -> 2 layer LSTM with 1000 hidden units

3rd mode: Like the second, but has encoder and decoder LSTMs (both with 2 layers)

AlexNet (close relative to CaffeNet) has 61M params.

LSTM RNN number of parameters is given by L*(n*m + n^2 + n) where L:= Number of layers, n:= hidden units, m:= input vector length
",,,TaCoS,"Largest model is for image captioning:
Pretrained with ILSVRC 2021 (1.2M images)
Trained on 40k video-sentence pairs from TaCoS",40000,,,,,,,,,Reinforcement learning,,,Academia,,,2023-12-05 04:33,Robi Rahman,,,"United States of America,United States of America,United States of America","UT Austin, University of Massachusetts Lowell, UC Berkeley",,,,LRCN,,,
Fully Convolutional Networks,Vision,Image segmentation,"J Long, E Shelhamer, T Darrell",Highly cited,,,https://arxiv.org/abs/1411.4038,32283.00,Fully Convolutional Networks for Semantic Segmentation,2014-11-14,UC Berkeley,Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-12-05 04:33,Robi Rahman,,,United States of America,UC Berkeley,,,,Fully Convolutional Networks,,,
Cascaded LNet-ANet,Vision,,"Z Liu, P Luo, X Wang, X Tang",Highly cited,,,https://arxiv.org/abs/1411.7766,6578.00,Deep Learning Face Attributes in the Wild,2014-11-28,Chinese University of Hong Kong,Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-12-05 04:33,Robi Rahman,,,Hong Kong,Chinese University of Hong Kong,,,,Cascaded LNet-ANet,,,
NTM,,,"Alex Graves, Greg Wayne, Ivo Danihelka",Highly cited,,,https://arxiv.org/abs/1410.5401,2042.00,Neural Turing Machines,2014-12-10,Google DeepMind,Industry,,,,,,,,,,,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,Multinational,Google DeepMind,,,,NTM,,,
Fractional Max-Pooling,Vision,Image Classification,Benjamin Graham,SOTA Improvement,"""for instance, we improve on the state-of-the art for CIFAR-100 without even using dropout.""",,https://arxiv.org/abs/1412.6071v4,672.00,Fractional Max-Pooling,2014-12-18,University of Warwick,Academia,27000000.00,27M weights in largest CIFAR-100 model,100000000000000000.00,"For the 12M param model, training required ""18 hours on a GeForce GTX 780"". So would be somewhat larger for 27M.

4 TFLOPS * 18 * 3600 * 0.4 = 1e17",CIFAR-100,,,,250.00,,,18.0,,NVIDIA GeForce GTX 780,NVIDIA GeForce GTX 780,,,,,Likely,"Convolutional networks almost always incorporate some form of spatial pooling, and very often it is alpha times alpha max-pooling with alpha=2. Max-pooling act on the hidden layers of the network, reducing their size by an integer multiplicative factor alpha. The amazing by-product of discarding 75% of your data is that you build into the network a degree of invariance with respect to translations and elastic distortions. However, if you simply alternate convolutional layers with max-pooling layers, performance is limited due to the rapid reduction in spatial size, and the disjoint nature of the pooling regions. We have formulated a fractional version of max-pooling where alpha is allowed to take non-integer values. Our version of max-pooling is stochastic as there are lots of different ways of constructing suitable pooling regions. We find that our form of fractional max-pooling reduces overfitting on a variety of datasets: for instance, we improve on the state-of-the art for CIFAR-100 without even using dropout.",2023-11-26 03:25,Anonymous,,,United Kingdom of Great Britain and Northern Ireland,University of Warwick,,,,Fractional Max-Pooling,,,
ADAM (CIFAR-10),Vision,Image classification,"DP Kingma, J Ba",Highly cited,,,https://arxiv.org/abs/1412.6980,127821.00,Adam: A Method for Stochastic Optimization,2014-12-22,"University of Amsterdam,OpenAI,University of Toronto",Industry - Academia Collaboration,,CIFAR-10 with c64-c64-c128-1000 architecture,60480000000000000.00,"From https://openai.com/blog/ai-and-compute/ Appendix

less than 0.0007 pfs-days (86400*10^15*0.0007)",,,,,,,,,,,,,$0.60,,Industry,,,2023-12-05 04:33,Robi Rahman,,,"Netherlands,United States of America,Canada","University of Amsterdam, OpenAI, University of Toronto",,,,ADAM (CIFAR-10),,,
DeepLab,Vision,Image segmentation,"Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, Alan L. Yuille",Highly cited,,,https://arxiv.org/abs/1412.7062,4249.00,Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs,2014-12-22,"Google,University of California Los Angeles (UCLA)",Industry - Academia Collaboration,,,,,,,,,,,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,"Multinational,United States of America","Google, University of California Los Angeles (UCLA)",,,,DeepLab,,,
4-gram + 8 DENN,Language,,"Kartik Audhkhasi, Abhinav Sethy, Bhuvana Ramabhadran",,,,https://arxiv.org/pdf/1412.7063,1.00,Diverse Embedding Neural Network Language Models,2014-12-22,,,16100000.00,,,,,,,,,,,,,,,,,,,,,2023-11-21 01:39,Robi Rahman,4-gram + 8 DENN,,,,,,,,,,
SCRN(Structurally Constrained Recurrent Network),Language,,"Tomas Mikolov, Armand Joulin, Sumit Chopra, Michael Mathieu, Marc'Aurelio Ranzato",,,,https://arxiv.org/abs/1412.7753,306.00,Learning Longer Memory in Recurrent Neural Networks,2014-12-24,,,26500000.00,,,,,,,,,,,,,,,,,,,,,2023-11-21 01:39,Robi Rahman,SCRN(Structurally Constrained Recurrent Network),,,,,,,,,,
N-gram+Cache,Language,,,,,,,,,2014-12-24,,,,,,,,,,,,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,N-gram+Cache,1,,,,,,,,,
N-gram,Language,,,,,,,,,2014-12-24,,,,,,,,,,,,,,,,,,,,,,Unverified,,2023-12-08 01:01,Robi Rahman,N-gram,1,,,,,,,,,
"MSRA (C, PReLU)",Vision,Image classification,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun",Highly cited,,,https://arxiv.org/abs/1502.01852,20078.00,Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification,2015-02-06,Microsoft Research,Industry,87048800.00,"I used the architecture in table 3
I ignored biases, and assumed a SPP bin size of 256

",23974030080000000000.00,"""training C on eight K40 GPUs, takes about 3-4 weeks""
0.33 util rate
(From Imagenet paper-data, Besiroglu et al., forthcoming) ",ILSVRC 2012,"They won ILSVRC 2015, but actually the classification dataset is the same as 2012",1280000,"""We perform the experiments on the 1000-class ImageNet 2012 dataset"", paper; ImageNet 2012 train set size from https://huggingface.co/datasets/imagenet-1k",,,,588.0,,,,,$2166.22,,Industry,,"Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on our PReLU networks (PReLU-nets), we achieve 4.94% top-5 test error on the ImageNet 2012 classification dataset. This is a 26% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66%). To our knowledge, our result is the first to surpass human-level performance (5.1%, Russakovsky et al.) on this visual recognition challenge.",2023-11-26 03:25,Robi Rahman,,,United States of America,Microsoft Research,,,,"""MSRA (C, PReLU)""",,,
CRF-RNN,Vision,Image segmentation,"Shuai Zheng, Sadeep Jayasumana, Bernardino Romera-Paredes, Vibhav Vineet, Zhizhong Su, Dalong Du, Chang Huang, Philip H. S. Torr",Highly cited,,,https://arxiv.org/abs/1502.03240,2661.00,Conditional Random Fields as Recurrent Neural Networks,2015-02-11,"University of Oxford,Stanford University,Baidu",Industry - Academia Collaboration,,,,,,,,,,,,,,,,,,,Industry,,,2023-11-26 03:25,Robi Rahman,,,"United Kingdom of Great Britain and Northern Ireland,United States of America,China","University of Oxford, Stanford University, Baidu",,,,CRF-RNN,,,
DQN-2015,Games,Atari Games,"Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, Demis Hassabis ",Highly cited,,,https://www.nature.com/articles/nature14236,22125.00,Human-level control through deep reinforcement learning,2015-02-25,Google,Industry,1693362.00,"""The input to the neural network consists of an 84x84x4 image produced by the preprocess-ing mapw. The first hidden layer convolves 32 filters of 8x8 with stride 4 with theinput image and applies a rectifier nonlinearity. The second hidden layer con-volves 64 filters of 4x4 with stride 2, again followed by a rectifier nonlinearity.This is followedby a thirdconvolutional layer thatconvolves 64 filtersof 3x3 withstride 1 followed by a rectifier. The final hidden layer is fully-connected and con-sists of 512 rectifier units. The output layer is a fully-connected linear layer with asingle output for each valid action. The number of valid actions varied between 4 and 18 on the games we considered.""

Example num params here: https://colab.research.google.com/drive/1Ty6SFYWd7EcKoxJohucL2OdiLR_3oXnI?usp=sharing",,"This should be calculatable, just needs careful reasoning about compute per frame.",,,50000000,"Methods: ""we trained for a total of 50 million frames""",,,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,Multinational,Google,,,,DQN-2015,,,
Constituency-Tree LSTM,Language,Semantic embedding,"KS Tai, R Socher, CD Manning",Highly cited,,,https://arxiv.org/abs/1503.00075,2936.00,Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks,2015-02-28,"MetaMind Inc,Stanford University",Industry - Academia Collaboration,205190.00,"Table 1

https://arxiv.org/abs/1503.00075",,,,,,,,,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,"United States of America,United States of America","MetaMind Inc, Stanford University",,,,Constituency-Tree LSTM,,,
Stack RNN,Language,,"Armand Joulin, Tomas Mikolov",,,,https://arxiv.org/abs/1503.01007,440.00,Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets,2015-03-03,,,2010000.00,,,,,,,,,,,,,,,,,,,,,2023-11-21 01:39,Robi Rahman,Stack RNN,,,,,,,,,,
genCNN + dyn eval,Language,,"Mingxuan Wang, Zhengdong Lu, Hang Li, Wenbin Jiang, Qun Liu",SOTA Improvement,"""genCNN outperforms the state-ofthe-arts with big margins.""",,https://aclanthology.org/P15-1151/,33.00,genCNN: A Convolutional Architecture for Word Sequence Prediction,2015-03-17,"Chinese Academy of Sciences,Huawei Noah's Ark Lab,Dublin City University",,8000000.00,,73000000000000000.00,,Penn TreeBank,,,,,,,,,,,,,,,,,2023-11-26 03:25,Robi Rahman,genCNN + dyn eval,,"China,China,Ireland","Chinese Academy of Sciences, Huawei Noah's Ark Lab, Dublin City University",,,,genCNN + dyn eval,,,
Fast R-CNN,Vision,Object detection,R Girshick,Highly cited,,,https://arxiv.org/abs/1504.08083,19746.00,Fast R-CNN,2015-04-30,Microsoft Research,Industry,,,,,,,,,,,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,United States of America,Microsoft Research,,,,Fast R-CNN,,,
Deep LSTM for video classification,Vision,Video,"Joe Yue-Hei Ng, Matthew Hausknecht, Sudheendra Vijayanarasimhan, Oriol Vinyals, Rajat Monga, George Toderici",Highly cited,,,https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Ng_Beyond_Short_Snippets_2015_CVPR_paper.html,2260.00,Beyond Short Snippets: Deep Networks for Video Classification,2015-05-01,"University of Texas at Austin,Google",Industry - Academia Collaboration,,,,,,,,,,,,,,,,,,,Industry,,,2023-11-26 03:25,Robi Rahman,,,"United States of America,Multinational","University of Texas at Austin, Google",,,,Deep LSTM for video classification,,,
2nd order FOFE-FNNLM,Language,,"Shiliang Zhang, Hui Jiang, Mingbin Xu, Junfeng Hou, Lirong Dai",,,,https://arxiv.org/abs/1505.01504,18.00,A Fixed-Size Encoding Method for Variable-Length Sequences with its Application to Neural Network Language Models,2015-05-06,,,6000000.00,,,,Penn TreeBank,,,,,,,,,,,,,,,,,2023-11-21 01:39,Robi Rahman,2nd order FOFE-FNNLM,,,,,,,,,,
Trajectory-pooled conv nets,Vision,Action recognition,"Limin Wang, Yu Qiao, Xiaoou Tang",Highly cited,,,https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Wang_Action_Recognition_With_2015_CVPR_paper.html,3786.00,Action Recognition with Trajectory-Pooled Deep-Convolutional Descriptors,2015-05-19,"Chinese University of Hong Kong,Chinese Academy of Sciences",Academia,9106245.00,"The input layer takes either a single RGB frame (224x224x3) for the spatial stream or a stack of 10 optical flow frames (224x224x20) for the temporal stream.
The first convolutional layer has 96 filters of size 7x7 with stride 2.
This is followed by max pooling with size 3x3 and stride 2.
The second convolutional layer has 256 filters of size 5x5 with stride 2.
After that is another max pooling layer (3x3, stride 2).
The third convolutional layer has 512 filters of size 3x3 with stride 1.
The fourth convolutional layer has 512 filters of size 3x3 with stride 1.
The fifth convolutional layer has 512 filters of size 3x3 with stride 1.
Next is a max pooling layer (3x3, stride 2).
The fully-connected layers have 4096, 2048, and 101 neurons respectively.

(7*7*20+1)*96 + (5*5*20+1)*256 + (3*3*20+1)*512 + (3*3*20+1)*512 + (3*3*20+1)*512 + 2*4096 + (4096+1)*2048 + (2048+1)*101 = 9106245",,,"ImageNet, UCF101",,,"They pretrain on ImageNet, and use UCF101 for actions. Its paper says ""We introduce UCF101 which is currently the largest dataset of human actions. It consists of 101 action classes, over 13k clips and 27 hours of video data"".",,,,,,,,,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,"Hong Kong,China","Chinese University of Hong Kong, Chinese Academy of Sciences",,,,Trajectory-pooled conv nets,,,
Faster R-CNN,Vision,Object detection,"S Ren, K He, R Girshick, J Sun",Highly cited,,,https://arxiv.org/abs/1506.01497,48575.00,Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks,2015-06-04,Microsoft Research,Industry,,,,,,,,,,,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,United States of America,Microsoft Research,,,,Faster R-CNN,,,
GoogLeNet / InceptionV1,Vision,Image classification,"Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich",Highly cited,,,https://arxiv.org/abs/1409.4842,38016.00,Going deeper with convolutions,2015-06-07,"Google,University of Michigan,University of North Carolina",Industry - Academia Collaboration (Industry leaning),6797700.00,Computed summing the parameters on table 1 of section 5,1557140125176000000.00,Extracted from AI and Compute (https://openai.com/blog/ai-and-compute/) charts by using https://automeris.io/WebPlotDigitizer/.,ILSVRC 2014,,1200000,"""The ILSVRC 2014 classification challenge involves the
task of classifying the image into one of 1000 leaf-node categories in the Imagenet hierarchy. There are about 1.2 million images for training, 50,000 for validation and 100,000 images for testing
[...]
We participated in the challenge with no external data
used for training.
""",,,,,,,,,$14.16,,Industry,,,2023-12-05 04:33,Robi Rahman,,,"Multinational,United States of America,United States of America","Google, University of Michigan, University of North Carolina",,,,GoogLeNet / InceptionV1,,,
YOLO,Vision,Object detection,"Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi",Highly cited,,,https://arxiv.org/abs/1506.02640,26672.00,"You Only Look Once: Unified, Real-Time Object Detection",2015-06-08,"University of Washington,Allen Institute for AI,Facebook AI Research",Industry - Academia Collaboration,271684800.00,"Calculation based on figure 3 of the paper:
7 * 7 * 3 * 64 + 3 * 3 * 64 * 192 + 1 * 1 * 192 * 128 + 3 * 3 * 128 * 256 + 1 * 1 * 256 * 256 + 3 * 3 * 256 * 512 + 4 * (1 * 1 * 512 * 256 + 3 * 3 * 256 * 512) + 1 * 1 * 512 * 512 + 3 * 3 * 512 * 1024 + 2 * (1 * 1 * 1024 * 512 + 3 * 3 * 512 * 1024) + 4 * (3 * 3 * 1024 * 1024) + 7 * 7 * 1024 * 4096 + 4096 * 7 * 7 * 30",,,,,,,,,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,"United States of America,United States of America,Multinational","University of Washington, Allen Institute for AI, Facebook AI Research",,,,YOLO,,,
BatchNorm,,,"S Ioffe, C Szegedy",Highly cited,,,https://arxiv.org/abs/1502.03167,37990.00,Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,2015-06-15,Google,Industry,,,,,,,,,,,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,Multinational,Google,,,,BatchNorm,,,
Search-Proven Best LSTM,Language,,"R. Józefowicz, Wojciech Zaremba, Ilya Sutskever",Highly cited,,,https://proceedings.mlr.press/v37/jozefowicz15.pdf,2207.00,An Empirical Exploration of Recurrent Network Architectures,2015-07-06,"Google,New York University (NYU),Facebook",,20000000.00,,3340000000000000.00,,,,,,30.00,,,,,,,,,,,,,2023-11-26 03:25,Robi Rahman,Search-Proven Best LSTM,,"Multinational,United States of America,Multinational","Google, New York University (NYU), Facebook",,,,Search-Proven Best LSTM,,,
"Listen, Attend and Spell",,,"William Chan, Navdeep Jaitly, Quoc Le, and Oriol Vinyals",Highly cited,,,https://ieeexplore.ieee.org/document/7472621,2010.00,"Listen, attend and spell: A neural network for large vocabulary conversational speech recognition",2015-08-20,"Google,Carnegie Mellon University (CMU)",Industry - Academia Collaboration,,,,,,,,,,,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,"Multinational,United States of America","Google, Carnegie Mellon University (CMU)",,,,"""Listen, Attend and Spell""",,,
LSTM-Char-Large,Language,,"Yoon Kim, Yacine Jernite, David Sontag, Alexander M. Rush",Highly cited,,,https://arxiv.org/abs/1508.06615,2033.00,Character-Aware Neural Language Models,2015-08-26,"Harvard University,New York University (NYU)",,19000000.00,,2650000000000000.00,,Penn TreeBank,,,,25.00,,,,,,,,,,,,,2023-11-26 03:25,Robi Rahman,LSTM-Char-Large,,"United States of America,United States of America","Harvard University, New York University (NYU)",,,,LSTM-Char-Large,,,
BPE,Language,Translation,"R Sennrich, B Haddow, A Birch",Highly cited,,,https://arxiv.org/abs/1508.07909,6331.00,Neural Machine Translation of Rare Words with Subword Units,2015-08-31,University of Edinburgh,Academia,,,,,WMT'15,,37500000,"[WORDS]
""We perform experiments on data from the shared translation task of WMT 2015. For English→German, our training set consists of 4.2 million sentence pairs, or approximately 100 million tokens. For English→Russian, the training set consists of 2.6 million sentence pairs, or approximately 50 million tokens""

100M tokens, around half will be in English, 0.75 words per token

",,,,,,,,,,,Academia,,,2023-12-05 04:33,Robi Rahman,,,United Kingdom of Great Britain and Northern Ireland,University of Edinburgh,,,,BPE,,,
Deep Deterministic Policy Gradients,,,"TP Lillicrap, JJ Hunt, A Pritzel, N Heess, T Erez",Highly cited,,,https://arxiv.org/abs/1509.02971,10179.00,Continuous control with deep reinforcement learning,2015-09-09,Google DeepMind,Industry,,,,,,,,,,,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,Multinational,Google DeepMind,,,,Deep Deterministic Policy Gradients,,,
AlphaGo Fan,Games,Go,"David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, Demis Hassabis",SOTA improvement,,,https://www.nature.com/articles/nature24270.epdf?author_access_token=VJXbVjaSHxFoctQQ4p2k4tRgN0jAjWel9jnR3ZoTv0PVW4gB86EEpGqTRDtpIz-2rmo8-KG06gqVobU5NSCFeHILHcVFUeMsbvwS-lxjqQGg98faovwjxeTUgZAUMnRQ,14389.00,Mastering the game of Go with deep neural networks and tree search,2015-10-01,Google DeepMind,Industry,8209984.00,"The input to the policy network is a 19 × 19 × 48 image stack consisting of 48 feature planes. The first hidden layer zero pads the input into a 23 × 23 image, then convolves k filters of kernel size 5 × 5 with stride 1 with the input image and applies a rectifier nonlinearity. Each of the subsequent hidden layers 2 to 12 zero pads the respective previous hidden layer into a 21 × 21 image, then convolves k filters of kernel size 3 × 3 with stride 1, again followed by a rectifier nonlinearity. The final layer convolves 1 filter of kernel size 1 × 1 with stride 1, with a different bias for each position, and applies a softmax function. The match version of AlphaGo used k = 192 filters; Fig. 2b and Extended Data Table 3 additionally show the results of training with k = 128, 256 and 384 filters.

The input to the value network is also a 19 × 19 × 48 image stack, with an additional binary feature plane describing the current colour to play. Hidden layers 2 to 11 are identical to the policy network, hidden layer 12 is an additional convolution layer, hidden layer 13 convolves 1 filter of kernel size 1 × 1 with stride 1, and hidden layer 14 is a fully connected linear layer with 256 rectifier units. The output layer is a fully connected linear layer with a single tanh unit.",380000000000000070000.00,"Assume 0.3 utilisation rate, 1e13 GPU FLOP/s [single precision]. Trained in three stages using 50 GPUs over 3 weeks + 1 day + 1 week

Training compute = (50 GPUs)(29 days)(86400s/day)(0.3 utilisation rate)(1e13 FLOP/s) = 3.8e20 FLOPs",,,,Supervised learning + self-play,,,"Distributed: 176 GPUs + 1202 PUs + 40 search threads
Single machine: 8 GPUs + 48 CPUs 

https://www.nature.com/articles/nature16961",,,,,,$3076.07,,Industry,,"A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo’s own move selections and also the winner of AlphaGo’s games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100–0 against the previously published, champion-defeating AlphaGo.",2023-12-05 04:33,Robi Rahman,,,Multinational,Google DeepMind,,,,AlphaGo Fan,,,AlphaGo Fan
Multi-scale Dilated CNN,Vision,Image segmentation,"Fisher Yu, Vladlen Koltun",Highly cited,,,https://arxiv.org/abs/1511.07122,7217.00,Multi-Scale Context Aggregation by Dilated Convolutions,2015-11-23,"Princeton University,Intel Labs",Industry - Academia Collaboration,,,,,,,,,,,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,"United States of America,Multinational","Princeton University, Intel Labs",,,,Multi-scale Dilated CNN,,,
Netflix Recommender System,Recommendation,,"CA Gomez-Uribe, N Hunt",Highly cited,,,https://dl.acm.org/doi/pdf/10.1145/2843948,1092.00,"The Netflix Recommender System: Algorithms, Business Value, and Innovation",2015-12-01,Netflix,Industry,,,,,,,,,,,,,,,,,,,Industry,,,2023-11-26 03:25,Robi Rahman,,,United States of America,Netflix,,,,Netflix Recommender System,,,
Inception v3,Vision,Image classification,"Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, Zbigniew Wojna",Highly cited,,,https://arxiv.org/abs/1512.00567,22351.00,Rethinking the inception architecture for computer vision.,2015-12-02,"Google,University College London (UCL)",Industry - Academia Collaboration (Industry leaning),23626728.00,Table 3 from Xception paper,,,ILSVRC 2012,,1200000,"The full dataset is a lot larger and has far more categories. When people say ""ImageNet"" they're usually referring to the subset of the full dataset with 1000 categories and 1.2million images, found here: https://image-net.org/challenges/LSVRC/2012/",,114830000000.00,"Rados (FLOPs)
https://drive.google.com/drive/folders/1bhy5z6hh1n3wCHx6528Xb7xB1KhYdAL1",,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,"Multinational,United Kingdom of Great Britain and Northern Ireland","Google, University College London (UCL)",,,,Inception v3,,,
DeepSpeech2 (English),Speech,Speech recognition,"Dario Amodei, Rishita Anubhai, Eric Battenberg, Carl Case, Jared Casper, Bryan Catanzaro, Jingdong Chen, Mike Chrzanowski, Adam Coates, Greg Diamos, Erich Elsen, Jesse Engel, Linxi Fan, Christopher Fougner, Tony Han, Awni Hannun, Billy Jun, Patrick LeGresley, Libby Lin, Sharan Narang, Andrew Ng, Sherjil Ozair, Ryan Prenger, Jonathan Raiman, Sanjeev Satheesh, David Seetapun, Shubho Sengupta, Yi Wang, Zhiqian Wang, Chong Wang, Bo Xiao, Dani Yogatama, Jun Zhan, Zhenyao Zhu",Highly cited,,,https://arxiv.org/abs/1512.02595,2715.00,Deep Speech 2: End-to-End Speech Recognition in English and Mandarin,2015-12-08,Baidu Research - Silicon Valley AI Lab,Industry,38000000.00,All networks have 38 million parameters.,26000000000000000000.00,"1 timestep = (1280 hidden units)^2 * (7 RNN layers * 4 matrices for bidirectional + 2 DNN layers) * (2 for doubling parameters from 36M to 72M) = 98 MFLOP
20 epochs * 12,000 hours * 3600 seconds/hour * 50 samples/sec * 98 MFLOP * 3 add-multiply * 2 backprop 
= 26,000 PF = 0.30 pfs-days

See also AI and Compute by Dario Amodei and OpenAI https://openai.com/research/ai-and-compute",,,163339200,"""Our English speech system is trained on 11,940 hours of speech, while the Mandarin system is trained on 9,400 hours.""

11,940 * 13,680 = 163339200",,1800000000.00,,,,"NVIDIA GTX Titan X,NVIDIA Quadro K1200","NVIDIA GTX Titan X, NVIDIA Quadro K1200",,$150.78,,Industry,,"We show that an end-to-end deep learning approach can be used to recognize either English or Mandarin Chinese speech--two vastly different languages. Because it replaces entire pipelines of hand-engineered components with neural networks, end-to-end learning allows us to handle a diverse variety of speech including noisy environments, accents and different languages. Key to our approach is our application of HPC techniques, resulting in a 7x speedup over our previous system. Because of this efficiency, experiments that previously took weeks now run in days. This enables us to iterate more quickly to identify superior architectures and algorithms. As a result, in several cases, our system is competitive with the transcription of human workers when benchmarked on standard datasets. Finally, using a technique called Batch Dispatch with GPUs in the data center, we show that our system can be inexpensively deployed in an online setting, delivering low latency when serving users at scale.",2023-12-05 04:33,Robi Rahman,,,United States of America,Baidu Research - Silicon Valley AI Lab,,,,DeepSpeech2 (English),,,
ResNet-152 (ImageNet),Vision,Image classification,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun",Highly cited,,,https://arxiv.org/abs/1512.03385,147465.00,Deep Residual Learning for Image Recognition,2015-12-10,Microsoft,Industry,60000000.00,Taken from https://arxiv.org/abs/1605.07146,12100000000000000000.00,"(11.4 *10^9) mult-adds per forward pass
2 FLOPS/ mult-add
3.5 for forward & backward pass
1.2 * 10^6 examples in dataset
128 epochs

Source:x",ILSVRC 2012,"They won ILSVRC 2015, but actually the classification dataset is the same as 2012",1280000,"""We evaluate our method on the ImageNet 2012 classification dataset [36] that consists of 1000 classes. The models are trained on the 1.28 million training images""",,22600000000.00,Table 1,,,,,,$92.03,,Industry,,"Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.",2023-12-05 04:33,Robi Rahman,,,Multinational,Microsoft,,,,ResNet-152 (ImageNet),,,
ResNet-110 (CIFAR-10),Vision,Image classification,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun",Highly cited,,,https://arxiv.org/abs/1512.03385,147465.00,Deep Residual Learning for Image Recognition,2015-12-10,Microsoft,Industry,1700000.00,Table 6,,,,,,,,,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,Multinational,Microsoft,,,,ResNet-110 (CIFAR-10),,,
BPL,Drawing,Image generation,"BM Lake, R Salakhutdinov, JB Tenenbaum",Highly cited,,,https://science.sciencemag.org/content/350/6266/1332/,2634.00,Human-level concept learning through probabilistic program induction,2015-12-11,"University of Toronto,New York University (NYU),Massachusetts Institute of Technology (MIT)",Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-12-05 04:33,Robi Rahman,,,"Canada,United States of America,United States of America","University of Toronto, New York University (NYU), Massachusetts Institute of Technology (MIT)",,,,BPL,,,
Advantage Learning,Games,Atari Games,"MG Bellemare, G Ostrovski, A Guez",SOTA improvement,,,http://arxiv.org/abs/1512.04860v1,139.00,Increasing the Action Gap: New Operators for Reinforcement Learning,2015-12-15,Google DeepMind,Industry,,,,,,,,,,,,,,,,,,,Industry,,"This paper introduces new optimality-preserving operators on Q-functions. We first describe an operator for tabular representations, the consistent Bellman operator, which incorporates a notion of local policy consistency. We show that this local consistency leads to an increase in the action gap at each state; increasing this gap, we argue, mitigates the undesirable effects of approximation and estimation errors on the induced greedy policies. This operator can also be applied to discretized continuous space and time problems, and we provide empirical results evidencing superior performance in this context. Extending the idea of a locally consistent operator, we then derive sufficient conditions for an operator to preserve optimality, leading to a family of operators which includes our consistent Bellman operator. As corollaries we provide a proof of optimality for Baird's advantage learning algorithm and derive other gap-increasing operators with interesting properties. We conclude with an empirical study on 60 Atari 2600 games illustrating the strong potential of these new operators.",2023-12-05 04:33,Robi Rahman,,,Multinational,Google DeepMind,,,,Advantage Learning,,,
"Variational (untied weights, MC) LSTM (Large)",Language,,"Yarin Gal, Zoubin Ghahramani","Highly cited,SOTA Improvement","""The new approach outperforms existing techniques, and to the best of our knowledge improves on the single model state-of-the-art in language modelling with the Penn Treebank (73.4 test perplexity)""",,https://arxiv.org/abs/1512.05287?context=stat,1838.00,A Theoretically Grounded Application of Dropout in Recurrent Neural Networks,2015-12-16,University of Cambridge,,66000000.00,,5620000000000000.00,,Penn TreeBank,,,,16.00,,,,,,,,,,,,,2023-11-26 03:25,Robi Rahman,"""Variational (untied weights, MC) LSTM (Large)""",,United Kingdom of Great Britain and Northern Ireland,University of Cambridge,,,,"""Variational (untied weights, MC) LSTM (Large)""",,,
AlphaGo Lee,Games,Go,"David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, Demis Hassabis",Highly cited,,,https://www.nature.com/articles/nature16961,14389.00,Mastering the game of Go with deep neural networks and tree search,2016-01-27,DeepMind,Industry,,,1.9e+21,"This number is pretty uncertain. I expect it to be right to around a factor of 3, at least compared to AlphaGo Fan.

The architecture used was pretty much the same as AlphaGo Fan, but it was ""trained for longer"" and had around 5.33x the number of convolutional layers of AlphaGo Fan (256/48 = 5.33). 

The convolutional layers are the major contributor to the training compute, so I somewhat arbitrarily just multiply the compute for AlphaGo Fan by 5. Thus 3.8e20 * 5 = 1.9e21

Otherwise there has been little said about this model specifically - I've mainly relied on the source for AlphaGo Zero and AlphaGo Fan, linked below

AlphaGo Fan: https://www.nature.com/articles/nature16961

AlphaGo Zero: https://www.nature.com/articles/nature24270.epdf?author_access_token=VJXbVjaSHxFoctQQ4p2k4tRgN0jAjWel9jnR3ZoTv0PVW4gB86EEpGqTRDtpIz-2rmo8-KG06gqVobU5NSCFeHILHcVFUeMsbvwS-lxjqQGg98faovwjxeTUgZAUMnRQ",,,29400000,"We trained the policy network pσ to classify positions according to expert moves played in the KGS data set. This data set contains 29.4 million positions from 160,000 games played by KGS 6 to 9 dan human players; 35.4% of the games are handicap games.",,,,,,,,,$14041.80,,Industry,Speculative,"The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses ‘value networks’ to evaluate board positions and ‘policy networks’ to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.",2023-12-05 04:33,Robi Rahman,,,United Kingdom of Great Britain and Northern Ireland,DeepMind,,,,AlphaGo Lee,,,AlphaGo Lee
Convolutional Pose Machines,Vision,Pose estimation,"Shih-En Wei, Varun Ramakrishna, Takeo Kanade, Yaser Sheikh",Highly cited,,,https://arxiv.org/abs/1602.00134,2467.00,Convolutional Pose Machines,2016-01-30,Carnegie Mellon University (CMU),Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-12-05 04:33,Robi Rahman,,,United States of America,Carnegie Mellon University (CMU),,,,Convolutional Pose Machines,,,
A3C FF hs,Games,Atari Games,"V Mnih, AP Badia, M Mirza, A Graves",SOTA improvement,,,http://arxiv.org/abs/1602.01783v2,7233.00,Asynchronous Methods for Deep Reinforcement Learning,2016-02-04,"Google,University of Montreal",Industry - Academia Collaboration (Industry leaning),,,,,,,,,,,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,"Multinational,Canada","Google, University of Montreal",,,,A3C FF hs,,,
Inception-ResNet-V2,Vision,Image classification,"Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi",Highly cited,,,https://arxiv.org/abs/1602.07261,11630.00,"Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning",2016-02-23,Google,Industry,56000000.00,,,,,,,,,2638000000.00,"Rados (FLOPs)
https://drive.google.com/drive/folders/1bhy5z6hh1n3wCHx6528Xb7xB1KhYdAL1",,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,Multinational,Google,,,,Inception-ResNet-V2,,,
Inceptionv4,Vision,Image classification,"Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi",Highly cited,,,https://arxiv.org/abs/1602.07261,11630.00,"Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning",2016-02-23,Google,Industry,43000000.00,"""The folks from Google strike again with Inception-v4, 43M parameters.""

https://towardsdatascience.com/illustrated-10-cnn-architectures-95d78ace614d",,,,,,,,24600000000.00,"Rados (FLOPs)
https://drive.google.com/drive/folders/1bhy5z6hh1n3wCHx6528Xb7xB1KhYdAL1",,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,Multinational,Google,,,,Inceptionv4,,,
SqueezeNet,Vision,Image classification,"Forrest N. Iandola, Song Han, Matthew W. Moskewicz, Khalid Ashraf, William J. Dally, Kurt Keutzer",Highly cited,,,https://arxiv.org/abs/1602.07360,5964.00,SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size,2016-02-24,"DeepScale,UC Berkeley,Stanford University",Industry - Academia Collaboration,1200000.00,"The paper says ""SqueezeNet achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters.""

AlexNet has 60 million parameters.",,,,,,,,,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,"United States of America,United States of America,United States of America","DeepScale, UC Berkeley, Stanford University",,,,SqueezeNet,,,
Binarized Neural Network (MNIST),Vision,Image Classification,"Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-Yaniv, Yoshua Bengio",Highly cited,,,https://arxiv.org/abs/1602.02830,3299.00,Binarized Neural Networks: Training Neural Networks with Weights and Activations Constrained to +1 or −1,2016-03-17,"Université de Montréal,Technion - Israel Institute of Technology,Columbia University,CIFAR AI Research",Academia,37000000.00,"Parameter count is not explicitly stated, but they give details:

""The MLP we train on MNIST consists of 3 hidden layers of 4096 binary units (see Section 1) and a L2-SVM output layer""

Approximately 37m, based on 784 pixels * 4096 + 2 * 4096^2",,,MNIST,,60000,"60k training images, 10k test in MNIST",1000.00,,,,,,,,,,,Likely,"We introduce a method to train Binarized Neural Networks (BNNs) - neural networks with binary weights and activations at run-time. At training-time the binary weights and activations are used for computing the parameters gradients. During the forward pass, BNNs drastically reduce memory size and accesses, and replace most arithmetic operations with bit-wise operations, which is expected to substantially improve power-efficiency. To validate the effectiveness of BNNs we conduct two sets of experiments on the Torch7 and Theano frameworks. On both, BNNs achieved nearly state-of-the-art results over the MNIST, CIFAR-10 and SVHN datasets. Last but not least, we wrote a binary matrix multiplication GPU kernel with which it is possible to run our MNIST BNN 7 times faster than with an unoptimized GPU kernel, without suffering any loss in classification accuracy. The code for training and running our BNNs is available on-line.",2023-11-26 03:25,Anonymous,,,"Canada,Israel,United States of America,Canada","Université de Montréal, Technion - Israel Institute of Technology, Columbia University, CIFAR AI Research",,,,Binarized Neural Network (MNIST),,,
Symmetric Residual Encoder-Decoder Net,Vision,Image super-resolution,"Xiao-Jiao Mao, Chunhua Shen, Yu-Bin Yang",Highly cited,,,https://arxiv.org/abs/1603.09056v2,1184.00,Image Restoration Using Very Deep Convolutional Encoder-Decoder Networks with Symmetric Skip Connections,2016-03-30,"Nanjing University,University of Adelaide",Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,"China,Australia","Nanjing University, University of Adelaide",,,,Symmetric Residual Encoder-Decoder Net,,,
Gated HORNN (3rd order),Language,,"Rohollah Soltani, Hui Jiang",SOTA Improvement,"""Both FOFEbased pooling and gated HORNNs have achieved the stateof-the-art performance, i.e., 100 in perplexity on this task.
To the best of our knowledge, this is the best reported performance on PTB under the same training condition.""",,https://arxiv.org/pdf/1605.00064,77.00,Higher Order Recurrent Neural Networks,2016-04-30,York University,,8970000.00,,,,Penn TreeBank,,,,,,,,,,,,,,,,,2023-11-26 03:25,Robi Rahman,Gated HORNN (3rd order),,Canada,York University,,,,Gated HORNN (3rd order),,,
Spatiotemporal fusion ConvNet,Vision,Video,"Christoph Feichtenhofer, Axel Pinz, Andrew Zisserman",Highly cited,,,https://openaccess.thecvf.com/content_cvpr_2016/html/Feichtenhofer_Convolutional_Two-Stream_Network_CVPR_2016_paper.html,2376.00,Convolutional Two-Stream Network Fusion for Video Action Recognition,2016-06-01,"Graz University of Technology,University of Oxford",Academia,,,,,UCF101,,97200,"[SECONDS OF VIDEO]

They use UCF101, whose paper says
""We introduce UCF101 which is currently the largest dataset of human actions. It consists of 101 action classes, over 13k clips and 27 hours of video data""",,,,,,,,,,,Academia,,,2023-12-05 04:33,Robi Rahman,,,"Austria,United Kingdom of Great Britain and Northern Ireland","Graz University of Technology, University of Oxford",,,,Spatiotemporal fusion ConvNet,,,
DMN,Language,,"Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer, James Bradbury, Ishaan Gulrajani, Victor Zhong, Romain Paulus, Richard Socher",Highly cited,,,https://arxiv.org/abs/1506.07285,1187.00,Ask Me Anything: Dynamic Memory Networks for Natural Language Processing,2016-06-20,Salesforce,Industry,,,,,,,,,,,,,,,,,,,Industry,,,2023-11-26 03:25,Robi Rahman,,,United States of America,Salesforce,,,,DMN,,,
R-FCN,Vision,Object detection,"Jifeng Dai, Y. Li, Kaiming He, and Jian Sun",Highly cited,,,https://arxiv.org/abs/1605.06409,5033.00,R-fcn: Object detection via region-based fully convolutional networks.,2016-06-21,"Tsinghua University,Microsoft Research",Industry - Academia Collaboration (Industry leaning),,,61492939793999990.00,"1,464  images in 2012 VOC (https://paperswithcode.com/dataset/pascal-voc)/
9,963 images in 2007 VOC (https://www.tensorflow.org/datasets/catalog/voc)
83K training images in MS COCO  (https://paperswithcode.com/dataset/coco)

They used a Nvidia K40 GPU and report training time/image in seconds (table 3)

Assumed a 0.33 util rate",PASCAL VOC (2007 and 2012 vesrions) + MS COCO,,94427,,,,,,,,,,$5.51,,Industry,,,2023-12-05 04:33,Robi Rahman,,,"China,United States of America","Tsinghua University, Microsoft Research",,,,R-FCN,,,
Wide & Deep,Recommendation,,"HT Cheng, L Koc, J Harmsen, T Shaked",Highly cited,,,https://arxiv.org/abs/1606.07792,2879.00,Wide & Deep Learning for Recommender Systems,2016-06-24,Google,Industry,,,,,,,,,,,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,Multinational,Google,,,,Wide & Deep,,,
fastText,Language,,"A Joulin, E Grave, P Bojanowski, T Mikolov",Highly cited,,,https://arxiv.org/abs/1607.01759,4003.00,Bag of Tricks for Efficient Text Classification,2016-07-06,Facebook AI Research,Industry,,,,,,,,,,,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,Multinational,Facebook AI Research,,,,fastText,,,
VD-RHN,Language,,"Julian Georg Zilly, Rupesh Kumar Srivastava, Jan Koutník, Jürgen Schmidhuber",SOTA Improvement,"""On the larger Wikipedia datasets for character prediction (text8 and enwik8), RHNs outperform all previous results and achieve an entropy of 1.27 bits per character.""",,https://arxiv.org/abs/1607.03474,493.00,Recurrent Highway Networks,2016-07-12,"ETH Zurich,IDSIA",,32000000.00,,3570000000000000.00,,Penn TreeBank,,,,20.00,,,,,,,,,,,,,2023-11-26 03:25,Robi Rahman,VD-RHN,,"Switzerland,Switzerland","ETH Zurich, IDSIA",,,,VD-RHN,,,
Variational RHN + WT,Language,,,,,,,,,2016-07-12,,,,,,,,,,,20.00,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,Variational RHN + WT,1,,,,,,,,,
Character-enriched word2vec,Language,,"P Bojanowski, E Grave, A Joulin",Highly cited,,,https://arxiv.org/abs/1607.04606,8563.00,Enriching Word Vectors with Subword Information,2016-07-15,Facebook AI Research,Industry,,,,,,,,,,,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,Multinational,Facebook AI Research,,,,Character-enriched word2vec,,,
Part-of-sentence tagging model,Language,POS tagging,"Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton",Highly cited,,,https://arxiv.org/abs/1607.06450,6928.00,Layer Normalization,2016-07-21,University of Toronto,Academia,,,145411200000000000.00,"12 hours of training for POS tagging
GeForce GTX TITAN X GPU
0.33 utilization rate
",,,,,,,,12.0,,NVIDIA GeForce GTX TITAN X,NVIDIA GeForce GTX TITAN X,,$0.97,,Academia,Wrong,,2023-12-05 04:33,Robi Rahman,,,Canada,University of Toronto,,,,Part-of-sentence tagging model,,,
Named Entity Recognition model,Language,Named Entity Recognition model,"Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin- ton",Highly cited,,,https://arxiv.org/abs/1607.06450,6928.00,Layer Normalization,2016-07-21,University of Toronto,Academia,,,96940800000000000.00,"8 hours of training for NER
GeForce GTX TITAN X GPU
0.33 utilization rate
",,,,,,,,8.0,,NVIDIA GeForce GTX TITAN X,NVIDIA GeForce GTX TITAN X,,$0.63,,Academia,Wrong,,2023-12-05 04:33,Robi Rahman,,,Canada,University of Toronto,,,,Named Entity Recognition model,,,
Attend-Infer-Repeat,Vision,Object recognition,"SM Ali Eslami, Nicolas Heess, Theophane Weber, Yuval Tassa, David Szepesvari, Geoffrey E Hinton",,,,https://arxiv.org/pdf/1603.08575.pdf,488.00,"Attend, Infer, Repeat: Fast Scene Understanding with Generative Models",2016-08-12,Google DeepMind,Industry,,,,,MNIST,images of multiple MNIST digits,60000,60000 MNIST images,,21000000000.00,"Executed on Nvidia Quadro K4000 GPU (1244 gFLOPs), took 17 milliseconds per image of three digits. 1244*0.017 = 21 gFLOP",48.0,"48 hours for MNIST model, 72 hours for 3D scenes model",,,,,,,Unverified,"We present a framework for efficient inference in structured image models that explicitly reason about objects. We achieve this by performing probabilistic inference using a recurrent neural network that attends to scene elements and processes them one at a time. Crucially, the model itself learns to choose the appropriate number of inference steps. We use this scheme to learn to perform inference in partially specified 2D models (variable-sized variational auto-encoders) and fully specified 3D models (probabilistic renderers). We show that such models learn to identify multiple objects – counting, locating and classifying the elements of a scene – without any supervision, e.g., decomposing 3D images with various numbers of objects in a single forward pass of a neural network at unprecedented speed. We
further show that the networks produce accurate inferences when compared to supervised counterparts, and that their structure leads to improved generalization.",2023-12-05 04:33,Anonymous,,,Multinational,Google DeepMind,,,,,,,
SimpleNet,Vision,Image Classification,"Seyyed Hossein Hasanpour, Mohammad Rouhani, Mohsen Fayyaz, Mohammad Sabokrou",SOTA Improvement,"""We achieved state-of-theart result on CIFAR10 outperforming several heavier architectures""",,https://arxiv.org/abs/1608.06037,117.00,"Lets keep it simple, Using simple architectures to outperform deeper and more complex architectures",2016-08-22,"Sensifai,Islamic Azad University,Technicolor R&I,Institute for Research in Fundamental Sciences (IPM)",Industry - Academia Collaboration (Industry leaning),5480000.00,SOTA CIFAR-10 model was 5.48m params,,,"CIFAR-10, ImageNet","""We experimented on CIFAR-10/100 Krizhevsky & Hinton (2009), SVHN Netzer et al. (2011), MNIST
Lecun et al. (1998) and ILSVRC 2012 classification task Russakovsky et al. (2015) datasets in order
to evaluate and compare our architecture""",,,,,,,,NVIDIA GeForce GTX 980,NVIDIA GeForce GTX 980,,,,,Confident,"Major winning Convolutional Neural Networks (CNNs), such as AlexNet, VGGNet, ResNet, GoogleNet, include tens to hundreds of millions of parameters, which impose considerable computation and memory overhead. This limits their practical use for training, optimization and memory efficiency. On the contrary, light-weight architectures, being proposed to address this issue, mainly suffer from low accuracy. These inefficiencies mostly stem from following an ad hoc procedure. We propose a simple architecture, called SimpleNet, based on a set of designing principles, with which we empirically show, a well-crafted yet simple and reasonably deep architecture can perform on par with deeper and more complex architectures. SimpleNet provides a good tradeoff between the computation/memory efficiency and the accuracy. Our simple 13-layer architecture outperforms most of the deeper and complex architectures to date such as VGGNet, ResNet, and GoogleNet on several well-known benchmarks while having 2 to 25 times fewer number of parameters and operations. This makes it very handy for embedded systems or systems with computational and memory limitations. We achieved state-of-theart result on CIFAR10 outperforming several heavier architectures, near state of the art on MNIST and highly competitive results on CIFAR100 and SVHN. We also outperformed the much larger and deeper architectures such as VGGNet and popular variants of ResNets among others on the ImageNet dataset. Models are made available at: https://github.com/Coderx7/SimpleNet",2023-11-26 03:25,Anonymous,,,"Belgium,Iran (Islamic Republic of),France,Iran (Islamic Republic of)","Sensifai, Islamic Azad University, Technicolor R&I, Institute for Research in Fundamental Sciences (IPM)",,,,SimpleNet,,,
DenseNet-264,Vision,Image classification,"G Huang, Z Liu, L Van Der Maaten",Highly cited,,,https://arxiv.org/abs/1608.06993,28620.00,Densely Connected Convolutional Networks,2016-08-25,"Tsinghua University,Facebook AI Research,Cornell University",Industry - Academia Collaboration (Academia leaning),34000000.00,,,,,,,,,,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,"China,Multinational,United States of America","Tsinghua University, Facebook AI Research, Cornell University",,,,DenseNet-264,,,
Multi-task Cascaded CNN,Vision,Face detection,"Kaipeng Zhang, Zhanpeng Zhang, Zhifeng Li, Yu Qiao",Highly cited,,,https://arxiv.org/abs/1604.02878,4164.00,Joint Face Detection and Alignment using Multitask cascaded convolutional networks,2016-08-26,"Chinese Academy of Sciences,Chinese University of Hong Kong",Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-12-05 04:33,Robi Rahman,,,"China,Hong Kong","Chinese Academy of Sciences, Chinese University of Hong Kong",,,,Multi-task Cascaded CNN,,,
WaveNet,Speech,,"A Oord, S Dieleman, H Zen, K Simonyan",Highly cited,,,https://arxiv.org/abs/1609.03499,6172.00,WaveNet: A Generative Model for Raw Audio,2016-09-12,Google DeepMind,Industry,,,,,,,,,,,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,Multinational,Google DeepMind,,,,WaveNet,,,
Youtube recommendation model,Recommendation,,"Paul Covington, Jay Adams, and Emre Sargin",Highly cited,,,https://research.google/pubs/pub45530/,2462.00,Deep Neural Networks for YouTube Recommendations,2016-09-15,Google,Industry,,,,,,,,,,,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,Multinational,Google,,,,Youtube recommendation model,,,
MS-CNN,Vision,Object detection,"Zhaowei Cai, Quanfu Fan, Rogerio S. Feris, Nuno Vasconcelos",Highly cited,,,https://link.springer.com/chapter/10.1007/978-3-319-46493-0_22,1405.00,A Unified Multi-scale Deep Convolutional Neural Network for Fast Object Detection,2016-09-17,"IBM,UC San Diego",Industry - Academia Collaboration,,,,,,,,,,,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,"Multinational,United States of America","IBM, UC San Diego",,,,MS-CNN,,,
Stacked hourglass network,Vision,Pose estimation,"Alejandro Newell, Kaiyu Yang, Jia Deng",Highly cited,,,https://link.springer.com/chapter/10.1007/978-3-319-46484-8_29,4398.00,Stacked Hourglass Networks for Human Pose Estimation,2016-09-17,University of Michigan,Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-12-05 04:33,Robi Rahman,,,United States of America,University of Michigan,,,,Stacked hourglass network,,,
TSN,Vision,Action recognition,"Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoou Tang, Luc Van Gool",Highly cited,,,https://link.springer.com/chapter/10.1007/978-3-319-46484-8_2,3260.00,Temporal Segment Networks: Towards Good Practices for Deep Action Recognition,2016-09-17,"ETH Zurich,Shenzhen Institute of Advanced Technology,Chinese University of Hong Kong",Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-12-05 04:33,Robi Rahman,,,"Switzerland,China,Hong Kong","ETH Zurich, Shenzhen Institute of Advanced Technology, Chinese University of Hong Kong",,,,TSN,,,
ResNet-1001,Vision,Image classification,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun",Highly cited,,,https://link.springer.com/chapter/10.1007/978-3-319-46493-0_38,8618.00,Identity Mappings in Deep Residual Networks,2016-09-17,Microsoft,Industry,10200000.00,,,"""On CIFAR, ResNet-1001 takes about 27 h to train on 2 GPUs""",CIFAR 10 and CIFAR 100,,,,,,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,Multinational,Microsoft,,,,ResNet-1001,,,
ResNet-200,Vision,Image Classification,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun",Highly cited,,,https://link.springer.com/chapter/10.1007/978-3-319-46493-0_38,8618.00,Identity Mappings in Deep Residual Networks,2016-09-17,Microsoft,Industry,,,,"""ResNet-200 takes about 3 weeks to train on 8 GPUs"". didn't specify which GPU",ImageNet,,,,,,,500.0,"""about 3 weeks""",,,,,,,Unverified,"Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR10 (4.62 % error) and CIFAR-100, and a 200-layer ResNet on ImageNet.",2023-12-05 04:33,Anonymous,,,Multinational,Microsoft,,,,ResNet-200,,,
Wide Residual Network,Vision,Image classification,"Sergey Zagoruyko, Nikos Komodakis",Highly cited,,,https://arxiv.org/abs/1605.07146,6308.00,Wide Residual Networks,2016-09-19,Université Paris-Est,Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-12-05 04:33,Robi Rahman,,,France,Université Paris-Est,,,,Wide Residual Network,,,
GNMT,Language,Translation,"Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Łukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, Jeffrey Dean",Highly cited,,,https://arxiv.org/abs/1609.08144,5948.00,Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation,2016-09-26,Google,Industry,278000000.00,"Table 5 in 'Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer'

https://arxiv.org/abs/1701.06538",6.899999999999999e+21,"sqrt(10 * 100) factor added because production model used 2-3 orders of magnitude more data, but only 1 epoch rather than 10.
96 K80 GPU’s * 9 days * 8.5 TFLOPS * 0.33 utilization * sqrt(10 * 100)  
= 6.9e6 PF = 79 pfs-days

source: https://openai.com/blog/ai-and-compute/",,,360000000,"[WORDS]
"" On WMT En→Fr, the training set contains 36M sentence pairs. On WMT En→De, the training set contains 5M sentence pairs.""

36M sentences * 10 words/sentence",,,,4320.0,,NVIDIA Tesla K80,NVIDIA Tesla K80,,$307573.50,,Industry,,"Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference. Also, most NMT systems have difficulty with rare words. These issues have hindered NMT's use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google's Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using attention and residual connections. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units (""wordpieces"") for both input and output. This method provides a good balance between the flexibility of ""character""-delimited models and the efficiency of ""word""-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. On the WMT'14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60% compared to Google's phrase-based production system.",2023-12-05 04:33,Robi Rahman,,,Multinational,Google,,,,GNMT,96,,GNMT
Pointer Sentinel-LSTM (medium),Language,,"Stephen Merity, Caiming Xiong, James Bradbury, Richard Socher",SOTA Improvement,"""Our pointer sentinel-LSTM model achieves state of the art language modeling performance on the Penn Treebank (70.9 perplexity) while using far fewer parameters than a standard softmax LSTM""",,https://arxiv.org/abs/1609.07843,1646.00,Pointer Sentinel Mixture Models,2016-09-26,"MetaMind Inc,Salesforce",,21000000.00,,7490000000000000.00,,Penn TreeBank,,,,64.00,,,,,,,,,,,,,2023-12-05 04:33,Robi Rahman,Pointer Sentinel-LSTM (medium),,"United States of America,United States of America","MetaMind Inc, Salesforce",,,,Pointer Sentinel-LSTM (medium),,,
Zoneout + Variational LSTM (WT2),Language,,"Stephen Merity, Caiming Xiong, James Bradbury, Richard Socher",Highly cited,,,https://arxiv.org/abs/1609.07843,1646.00,Pointer Sentinel Mixture Models,2016-09-26,"MetaMind Inc,Salesforce",,21000000.00,,16800000000000000.00,,WikiText-2,,,,64.00,,,,,,,,,,,,,2023-12-05 04:33,Robi Rahman,Zoneout + Variational LSTM (WT2),,"United States of America,United States of America","MetaMind Inc, Salesforce",,,,Zoneout + Variational LSTM (WT2),,,
Byte-mLSTM+emb+WN+VD,Language,,"Ben Krause, Liang Lu, Iain Murray, Steve Renals",,,,https://arxiv.org/pdf/1609.07959,216.00,Multiplicative LSTM for sequence modelling,2016-09-26,,,46000000.00,,,,,,,,,,,,,,,,,,,,,2023-11-21 01:39,Robi Rahman,Byte-mLSTM+emb+WN+VD,,,,,,,,,,
Zoneout + Variational LSTM (PTB),Language,,,,,,,,,2016-09-26,,,,,,,,,,,64.00,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,Zoneout + Variational LSTM (PTB),1,,,,,,,,,
Pointer Sentinel-LSTM,Language,,,,,,,,,2016-09-26,,,,,,,,,,,64.00,,,,,,,,,,,Unverified,,2023-12-08 01:01,Robi Rahman,Pointer Sentinel-LSTM,1,,,,,,,,,
Xception,Vision,Image classification,François Chollet,Highly cited,,,https://arxiv.org/abs/1610.02357,10821.00,Xception: Deep Learning with Depthwise Separable Convolutions,2016-10-07,Google,Industry,22855952.00,Table 3,43623360000000000000.00,"60 K80 GPUs * 3 days * 8.5 TFLOPS/GPU * 0.33 utilization 
= 4.4e4 PF = 0.44 pfs-days",JFT,"Also ImageNet, but JFT is significantly larger",350000000,"""JFT is an internal Google dataset for large-scale image classification dataset, first introduced by Hinton et al. in [5], which comprises over 350 million high-resolution images annotated with labels from a set of 17,000 classes. To evaluate the performance of a model trained on JFT, we use an auxiliary dataset, FastEval14k""",,16800000000.00,"Rados (FLOPs)
https://drive.google.com/drive/folders/1bhy5z6hh1n3wCHx6528Xb7xB1KhYdAL1",,,NVIDIA Tesla K80,NVIDIA Tesla K80,,$1961.34,,Industry,,"We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters.",2023-12-05 04:33,Robi Rahman,,,Multinational,Google,,,,Xception,,,
Differentiable neural computer,,,"Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka Grabska-Barwińska, Sergio Gómez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou, Adrià Puigdomènech Badia, Karl Moritz Hermann, Yori Zwols, Georg Ostrovski, Adam Cain, Helen King, Christopher Summerfield, Phil Blunsom, Koray Kavukcuoglu & Demis Hassabis",Highly cited,,,https://www.nature.com/articles/nature20101,1428.00,Hybrid computing using a neural network with dynamic external memory,2016-10-12,Google DeepMind,Industry,,,,,,,,,,,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,Multinational,Google DeepMind,,,,Differentiable neural computer,,,
VD-LSTM+REAL Large,Language,,"Hakan Inan, Khashayar Khosravi, Richard Socher",SOTA Improvement,"""Our framework leads to state of the art performance on the Penn Treebank""",,https://arxiv.org/abs/1611.01462,397.00,Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling,2016-11-04,"Salesforce Research,Stanford University",,51000000.00,,21300000000000000.00,,Penn TreeBank,,,,75.00,,,,,,,,,,,,,2023-11-26 03:25,Robi Rahman,VD-LSTM+REAL Large,,"United States of America,United States of America","Salesforce Research, Stanford University",,,,VD-LSTM+REAL Large,,,
VD-LSTM+REAL Medium,Language,,,,,,,,,2016-11-04,,,,,,,,,,,,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,VD-LSTM+REAL Medium,1,,,,,,,,,
VD-LSTM+REAL Small,Language,,,,,,,,,2016-11-04,,,,,,,,,,,60.00,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,VD-LSTM+REAL Small,1,,,,,,,,,
NASv3 (CIFAR-10),Vision,,"Barret Zoph, Quoc V. Le",Highly cited,,,https://arxiv.org/abs/1611.01578,4569.00,Neural Architecture Search with Reinforcement Learning,2016-11-05,Google Brain,Industry,37400000.00,Table 1,2.2e+21,"50 epochs * 50,000 images * 10.0 GFLOPSs * 12800 networks * 2 add-multiply * 3 backward pass 
= 1.9e6 PF = 22 pfs-days

source: https://openai.com/blog/ai-and-compute/",,,,,,,,,,,,,$13069.35,,Industry,Likely,"Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines. Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.",2023-12-05 04:33,Robi Rahman,,0,Multinational,Google Brain,,,,NASv3 (CIFAR-10),800,,NASv3 (CIFAR-10)
Neural Architecture Search with base 8 and shared embeddings,Language,,"Barret Zoph, Quoc V. Le",Highly cited,,,https://arxiv.org/abs/1611.01578,4569.00,Neural Architecture Search with Reinforcement Learning,2016-11-05,Google Brain,Industry,54000000.00,,10500000000000000.00,,Penn TreeBank,,,,35.00,,,,,,,,,,,,,2023-12-05 04:33,Robi Rahman,Neural Architecture Search with base 8 and shared embeddings,,Multinational,Google Brain,,,,Neural Architecture Search with base 8 and shared embeddings,,,
Deeply-recursive ConvNet,Vision,Image super-resolution,"Jiwon Kim, Jung Kwon Lee, Kyoung Mu Lee",Highly cited,,,https://arxiv.org/abs/1511.04491,2192.00,Deeply-Recursive Convolutional Network for Image Super-Resolution,2016-11-11,Seoul National University,Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-12-05 04:33,Robi Rahman,,,Korea (Republic of),Seoul National University,,,,Deeply-recursive ConvNet,,,
ResNeXt-50,Vision,Image classification,"Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, Kaiming He",Highly cited,,,https://arxiv.org/abs/1611.05431,8346.00,Aggregated Residual Transformations for Deep Neural Networks,2016-11-16,"UC San Diego,Facebook",Industry - Academia Collaboration,25000000.00,"""If you’re thinking about ResNets, yes, they are related. ResNeXt-50 has 25M parameters (ResNet-50 has 25.5M).""

https://towardsdatascience.com/illustrated-10-cnn-architectures-95d78ace614d",,,,,,,,8400000000.00,"Rados  (FLOPs)
https://drive.google.com/drive/folders/1bhy5z6hh1n3wCHx6528Xb7xB1KhYdAL1",,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,"United States of America,Multinational","UC San Diego, Facebook",,,,ResNeXt-50,,,
PolyNet,Vision,Image classification,"X Zhang, Z Li, C Change Loy",SOTA Improvement,"""The Very Deep PolyNet, designed following this direction, demonstrates substantial improvements over the state-of-the-art on the ILSVRC 2012 benchmark. Compared to Inception-ResNet-v2, it reduces the top-5 validation error on single crops from 4.9% to 4.25%, and that on multi-crops from 3.7% to 3.45%.""",,https://arxiv.org/abs/1611.05725,282.00,PolyNet: A Pursuit of Structural Diversity in Very Deep Networks,2016-11-17,Chinese University of Hong Kong,Academia,92000000.00,,64000000000000000000.00,"Section 5: ""ResNet-500 [has] similar computation
costs to our Very Deep PolyNet"".

ResNet-152 has 11.3e9 FLOP per forward pass (https://arxiv.org/abs/1512.03385, Table 1). Hence ResNet-500 has approx 3.7e10 = 11.3e9*500/152 FLOP per forward pass.

560k iterations, batch size 512:
Train compute = 3.7e10*3*2*560e3 * 512 = 6.4e19",ImageNet,Section 4,1280000,,,,,,,,,,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,Hong Kong,Chinese University of Hong Kong,,,,PolyNet,,,
RefineNet,Vision,Object detection,"Guosheng Lin, Anton Milan, Chunhua Shen, Ian Reid",Highly cited,,,https://arxiv.org/abs/1611.06612v3,2469.00,RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation,2016-11-20,"University of Adelaide,Australian Centre for Robotic Vision",Industry - Academia Collaboration,,,,,,,,,,,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,"Australia,Australia","University of Adelaide, Australian Centre for Robotic Vision",,,,RefineNet,,,
Image-to-image cGAN,,,"P Isola, JY Zhu, T Zhou",Highly cited,,,https://arxiv.org/abs/1611.07004,15965.00,Image-to-Image Translation with Conditional Adversarial Networks,2016-11-21,UC Berkeley,Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-12-05 04:33,Robi Rahman,,,United States of America,UC Berkeley,,,,Image-to-image cGAN,,,
Elastic weight consolidation,,,"J Kirkpatrick, R Pascanu",Highly cited,,,https://arxiv.org/abs/1612.00796,4724.00,Overcoming catastrophic forgetting in neural networks,2016-12-02,DeepMind,Industry,,,,,,,,,,,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,United Kingdom of Great Britain and Northern Ireland,DeepMind,,,,Elastic weight consolidation,,,
PointNet,Other,3d segmentation,"CR Qi, H Su, K Mo, LJ Guibas",Highly cited,,,https://arxiv.org/abs/1612.00593,10186.00,PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation,2016-12-02,Stanford University,Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-12-05 04:33,Robi Rahman,,,United States of America,Stanford University,,,,PointNet,,,
GAN-Advancer,Vision,Image classification,"Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi Chen",Highly cited,,,https://dl.acm.org/doi/10.5555/3157096.3157346,7355.00,Improved Techniques for Training GANs,2016-12-05,OpenAI,Industry,,,,,,,,,,,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,United States of America,OpenAI,,,,GAN-Advancer,,,
Diabetic Retinopathy Detection Net,Vision,,"V Gulshan, L Peng, M Coram, MC Stumpe, D Wu",Highly cited,,,https://jamanetwork.com/journals/jama/article-abstract/2588763,3540.00,Development and Validation of a Deep Learning Algorithm for Detection of Diabetic Retinopathy in Retinal Fundus Photographs,2016-12-13,"UT Austin,UC Berkeley,Google",Industry - Academia Collaboration,,,,,,,,,,,,,,,,,,,Industry,,,2023-11-26 03:25,Robi Rahman,,,"United States of America,United States of America,Multinational","UT Austin, UC Berkeley, Google",,,,Diabetic Retinopathy Detection Net,,,
Neural cache model (size=2000) (300M),Language,,"Edouard Grave, Armand Joulin, Nicolas Usunier",,,,https://arxiv.org/abs/1612.04426,302.00,Improving Neural Language Models with a Continuous Cache,2016-12-13,,,300000000.00,,,,WikiText-103,,,,,,,,,,,,,,,,,2023-11-21 01:39,Robi Rahman,Neural cache model (size=2000) (300M),,,,,,,,,,
Neural cache model (size=2000),Language,,,,,,,,,2016-12-13,,,,,,,,,,,,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,Neural cache model (size=2000),1,,,,,,,,,
LSTM (PTB),Language,,,,,,,,,2016-12-13,,,,,,,,,,,,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,LSTM (PTB),1,,,,,,,,,
LSTM (WT2),Language,,,,,,,,,2016-12-13,,,,,,,,,,,,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,LSTM (WT2),1,,,,,,,,,
LSTM (WT103),Language,,,,,,,,,2016-12-13,,,,,,,,,,,,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,LSTM (WT103),1,,,,,,,,,
"GCRN-M1, dropout",Language,,"Youngjoo Seo, Michaël Defferrard, Pierre Vandergheynst, Xavier Bresson",,,,https://arxiv.org/pdf/1612.07659,674.00,Structured Sequence Modeling with Graph Convolutional Recurrent Networks,2016-12-22,,,42000000.00,,3040000000000000.00,,,,,,13.00,,,,,,,,,,,,,2023-11-23 06:53,Robi Rahman,"""GCRN-M1, dropout""",,,,,,,,,,
GCNN-14,Language,,"Yann N. Dauphin, Angela Fan, Michael Auli, David Grangier",Highly cited,,,https://arxiv.org/abs/1612.08083,2176.00,Language Modeling with Gated Convolutional Networks,2016-12-23,Facebook AI Research,,,,,,WikiText-103,,,,35.00,,,,,,,,,,,,,2023-11-26 03:25,Robi Rahman,"GCNN-14,GCNN-14",,Multinational,Facebook AI Research,,,,GCNN-14,,,
YOLOv2,Vision,Object detection,"Joseph Redmon, Ali Farhadi",Highly cited,,,https://arxiv.org/abs/1612.08242,12167.00,"YOLO9000: Better, Faster, Stronger",2016-12-25,"University of Washington,Allen Institute for AI",Industry - Academia Collaboration,51000000.00,Source: https://resources.wolframcloud.com/NeuralNetRepository/resources/YOLO-V2-Trained-on-MS-COCO-Data_1,,,,,,,,,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,"United States of America,United States of America","University of Washington, Allen Institute for AI",,,,YOLOv2,,,
Libratus,Games,Poker,"N Brown, T Sandholm, S Machine",SOTA improvement,Claims to be first ML system to reach superhuman level at No Limit Poker Texas Hold Em,,https://www.cs.cmu.edu/~noamb/papers/17-IJCAI-Libratus.pdf,97.00,Libratus: The Superhuman AI for No-Limit Poker,2017-01-01,Carnegie Mellon University (CMU),Academia,,,551000000000000000000.00,"""In total, Libratus used about 25 million core hours. Of those, about 13 million core hours were used for exploratory experiments and evaluation. About 6 million core hours were spent on the initial abstraction and equilibrium finding component, another 3 million were used for nested subgame solving, and about 3 million were used on the self-improvement algorithm.""

""Like many data-centric supercomputers, Bridges offers a relatively a modest number of FLOPS, but lots of memory: 895 teraflops and 130 TB, respectively.""

I just used the first bullet point (as those are usually independent systems and you only benchmark one of them).
The first system has 752 nodes a 2CPUs a 14cores each.

source: https://www.top500.org/news/bridges-supercomputer-boots-up-at-pittsburgh/



1. 12M core hours for 196 cores
2. We have  895 TFLOPS for 752 nodes a 2 CPUs a 14 cores
2.1 That's 42.5 GFLOPS per core.
3. Running this for 12M h
3.1 12 * 10^6 * 60 * 60 * 42.5 * 10^9 FLOP/S = 1.823e21 FLOPs
4. Assuming 30% utilization
 1.823e21 * 0.3
→ 5.51e20 FLOPs",,,,,,,,,"In total, Libratus used about 25 million core hours. Of those,
about 13 million core hours were used for exploratory experiments and evaluation. About 6 million core hours were spent
on the initial abstraction and equilibrium finding component,
another 3 million were used for nested subgame solving, and
about 3 million were used on the self-improvement algorithm.",,,,$6253.49,,Academia,,"No-limit Texas Hold’em is the most popular variant of poker in the world. Heads-up no-limit Texas Hold’em is the main benchmark challenge for AI in imperfect-information games. We present Libratus, the first—and so far only—AI to defeat top human professionals in that game. Libratus’s architecture features three main modules, each of which has new algorithms: pre-computing a solution to an abstraction of the game which provides a high-level blueprint for the strategy of the AI, a new nested subgame-solving algorithm which repeatedly calculates a more detailed strategy as play progresses, and a self-improving module which augments the pre-computed blueprint over time.",2023-11-26 03:25,Robi Rahman,,,United States of America,Carnegie Mellon University (CMU),,,,Libratus,,,
AlphaGo Master,Games,Go,"D Silver, J Schrittwieser, K Simonyan, I Antonoglou",Highly cited,,,https://www.researchgate.net/publication/320473480_Mastering_the_game_of_Go_without_human_knowledge,7831.00,Mastering the game of Go without human knowledge,2017-01-01,DeepMind,Industry,,,1.5e+23,"This is a guess. There was no single journal publication that accompanied this model, that gave information about architecture/model training time etc. All I could find was that it has the same architecture as AlphaGo Zero, and that it had roughly the same power consumption as AGZ. See for instance: 
https://deepmind.com/blog/article/alphago-zero-starting-scratch

Since AGZ reaches the ELO of AlphaGo Master in about 20 days (half of the total training time), I estimate the compute to be around half that of AGZ. I round this down to 1.5e23, and I expect this to only be accurate within an OOM.",,,,,,,,,,Google TPU v1,Google TPU v1,,$852748.08,,Industry,,"A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo's own move selections and also the winner of AlphaGo's games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100-0 against the previously published, champion-defeating AlphaGo. © 2017 Macmillan Publishers Limited, part of Springer Nature. All rights reserved.",2023-12-05 04:33,Robi Rahman,,,United Kingdom of Great Britain and Northern Ireland,DeepMind,,,,AlphaGo Master,,,AlphaGo Master
Fisher Kernel GMM,Vision,Image classification,"Florent Perronnin, Christopher Dance",Highly cited,,,https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.71.7388&rep=rep1&type=pdf,1915.00,Fisher kernels on visual vocabularies for image categorization,2017-01-01,Xerox,Industry,,,,,,in-house image dataset of 19 object/scene categories,30000,,,,,2.5,,,,Supervised,,,,Unverified,"Within the field of pattern classification, the Fisher kernel is a powerful framework which combines the strengths of generative and discriminative approaches. The idea is to
characterize a signal with a gradient vector derived from a
generative probability model and to subsequently feed this
representation to a discriminative classifier. We propose to
apply this framework to image categorization where the input signals are images and where the underlying generative
model is a visual vocabulary: a Gaussian mixture model
which approximates the distribution of low-level features in
images. We show that Fisher kernels can actually be understood as an extension of the popular bag-of-visterms. Our approach demonstrates excellent performance on two challenging databases: an in-house database of 19 object/scene categories and the recently released VOC 2006 database. It is also very practical: it has low computational needs both at training and test time and vocabularies trained on one set of categories can be applied to another set without any significant loss in performance.",2023-11-26 03:25,Anonymous,,,United States of America,Xerox,,,,Fisher Kernel GMM,,,
DeepStack,Games,Poker,"Matej Moravčík, Martin Schmid, Neil Burch, Viliam Lisý, Dustin Morrill, Nolan Bard, Trevor Davis, Kevin Waugh, Michael Johanson, Michael Bowling",SOTA Improvement,"first human-competitive poker AI, confirmed by website: https://www.deepstack.ai/",,https://arxiv.org/abs/1701.01724,762.00,DeepStack: Expert-Level Artificial Intelligence in No-Limit Poker,2017-01-06,"University of Alberta,Charles University,Czech Technical University",Academia,2500000.00,"Figure 3, p.9

source: https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389",14463360000000000000.00,"The largest source of compute necessary for training seems to be the data generation job on 20 GPUs. We count this towards the training compute because it requires simulation using the network. This is analogous to the AlphaGo systems simulating Go games.

From p.26: ""For the flop network, one million poker flop situations (from after the flop cards are dealt) were generated and solved. These situations were solved using DeepStack’s depth limited solver with the turn network used for the counterfactual values at public states immediately after the turn card. We used a cluster of 20 GPUS and one-half of a GPU year of computation time.""

Assume they used P100 GPUs because they were common at the time (P100 was released in 2016 and this paper was published in 2017).

But assume low utilization of 10% to hedge on (a) lower-performing GPUs being used, (b) non-FLOP computations taking up a lot of the data generation job.

Calculation:
6 months * 30 days * 24 hours * 3600 seconds * 9.3e12 FLOP/s * 0.1 utilization = 1.446336e+19 FLOP.",,,10000000,"""The turn network was trained by solving 10 million randomly generated poker turn
games. These turn games used randomly generated ranges, public cards, and a random pot
size (10).""",,,,,,,,,$0.00,,Academia,Speculative,,2023-12-05 04:33,Robi Rahman,,,"Canada,Czechia,Czechia","University of Alberta, Charles University, Czech Technical University",,,,DeepStack,,,
OR-WideResNet,Vision,Image Classification,"Yanzhao Zhou, Qixiang Ye, Qiang Qiu and Jianbin Jiao",SOTA Improvement,"""In Sec. 4.3, we upgrade the VGG [38], ResNet [18], and the
WideResNet [45] to ORNs, and train them on CIFAR10 and
CIFAR100 [22], showing the state-of-the-art performance
on the natural image classification task.""",,https://arxiv.org/abs/1701.01833v2,285.00,Oriented Response Networks,2017-01-07,"Duke University,University of Chinese Academy of Sciences",Academia,18200000.00,18.2M for largest OR-WideResNet model.,,,CIFAR-10,,,,,,,,,NVIDIA Tesla K80,NVIDIA Tesla K80,,,,,Likely,"Deep Convolution Neural Networks (DCNNs) are capable of learning unprecedentedly effective image representations. However, their ability in handling significant local and global image rotations remains limited. In this paper, we propose Active Rotating Filters (ARFs) that actively rotate during convolution and produce feature maps with location and orientation explicitly encoded. An ARF acts as a virtual filter bank containing the filter itself and its multiple unmaterialised rotated versions. During back-propagation, an ARF is collectively updated using errors from all its rotated versions. DCNNs using ARFs, referred to as Oriented Response Networks (ORNs), can produce within-class rotation-invariant deep features while maintaining inter-class discrimination for classification tasks. The oriented response produced by ORNs can also be used for image and object orientation estimation tasks. Over multiple state-of-the-art DCNN architectures, such as VGG, ResNet, and STN, we consistently observe that replacing regular filters with the proposed ARFs leads to significant reduction in the number of network parameters and improvement in classification performance. We report the best results on several commonly used benchmarks.",2023-11-26 03:25,Anonymous,,,"United States of America,China","Duke University, University of Chinese Academy of Sciences",,,,OR-WideResNet,,,
MoE,Language,Language modelling / Machine translation,"N Shazeer, A Mirhoseini, K Maziarz, A Davis",SOTA Improvement,"""On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost""",,https://arxiv.org/abs/1701.06538,1356.00,Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer,2017-01-23,"Jagiellonian University,Google Brain",Industry - Academia Collaboration (Industry leaning),8700000000.00,"Table 5

https://arxiv.org/abs/1701.06538",93939056640000000000.00,"12 days 
64 NVIDIA K40 GPUS (see hardware data sheet for performance)
0.33 util rate
 ",,,100000000000,"[WORDS]

""We constructed a similar training set consisting of shuffled unique sentences from Google’s internal
news corpus, totalling roughly 100 billion words""",,,,,,NVIDIA Tesla K40t,NVIDIA Tesla K40t,,$8484.35,,Industry,,"The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.",2023-12-05 04:33,Robi Rahman,,,"Poland,Multinational","Jagiellonian University, Google Brain",,,,MoE,,,
DnCNN,Vision,Image super-resolution,"Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, Lei Zhang",Highly cited,,,https://ieeexplore.ieee.org/abstract/document/7839189,5458.00,Beyond a Gaussian Denoiser: Residual Learning of Deep CNN for Image Denoising,2017-02-01,"Harbin Institute of Technology,Hong Kong Polytechnic University,ULSee Inc.,Xi’an Jiaotong University",Industry - Academia Collaboration (Academia leaning),,,,,,,,,,,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,"China,Hong Kong,China,China","Harbin Institute of Technology, Hong Kong Polytechnic University, ULSee Inc., Xi’an Jiaotong University",,,,DnCNN,,,
Prototypical networks,Vision,Image classification," Jake Snell, Kevin Swersky, Richard S. Zemel",Highly cited,,,https://arxiv.org/abs/1703.05175,5877.00,Prototypical Networks for Few-shot Learning,2017-03-15,"University of Toronto,Twitter",Industry - Academia Collaboration,,,,,,,,,,,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,"Canada,United States of America","University of Toronto, Twitter",,,,Prototypical networks,,,
Word-Independent-SRNN+KN5,Language,,"Youssef Oualil, Clayton Greenberg, Mittul Singh, Dietrich Klakow",,,,https://arxiv.org/pdf/1703.08068,7.00,Sequential Recurrent Neural Networks for Language Modeling,2017-03-23,,,5320000.00,,,,,,,,,,,,,,,,,,,,,2023-11-21 01:39,Robi Rahman,Word-Independent-SRNN+KN5,,,,,,,,,,
Mask R-CNN,Vision,Image segmentation,"Kaiming He, Georgia Gkioxari, Piotr Dollár, Ross Girshick",Highly cited,,,https://arxiv.org/abs/1703.06870,18725.00,Mask R-CNN,2017-03-30,Facebook AI Research,Industry,,,,,COCO,,,,,,,,"Training with
ResNet-50-FPN on COCO trainval35k takes 32 hours
in our synchronized 8-GPU implementation (0.72s per 16-
image mini-batch), and 44 hours with ResNet-101-FPN",,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,Multinational,Facebook AI Research,,,,Mask R-CNN,,,
WGAN-GP,Vision,Image generation,"Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, Aaron Courville",Highly cited,,,https://arxiv.org/abs/1704.00028,7802.00,Improved Training of Wasserstein GANs,2017-03-31,"Montreal Institute for Learning Algorithms,Courant Institute of Mathematical Sciences",Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-12-05 04:33,Robi Rahman,,,"Canada,United States of America","Montreal Institute for Learning Algorithms, Courant Institute of Mathematical Sciences",,,,WGAN-GP,,,
MobileNet,Vision,,"AG Howard, M Zhu, B Chen, D Kalenichenko",Highly cited,,,https://arxiv.org/abs/1704.04861,15872.00,MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications,2017-04-17,Google,Industry,4200000.00,,,,,,,,,1140000000.00,"Rados (FLOPs)
https://drive.google.com/drive/folders/1bhy5z6hh1n3wCHx6528Xb7xB1KhYdAL1",,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,Multinational,Google,,,,MobileNet,,,
DeepLab (2017),Vision,Image segmentation,"Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, Alan L. Yuille",Highly cited,,,https://ieeexplore.ieee.org/abstract/document/7913730,14134.00,"DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs",2017-04-27,"Johns Hopkins University,Google,University College London (UCL)",Industry - Academia Collaboration,,,,,,,,,,,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,"United States of America,Multinational,United Kingdom of Great Britain and Northern Ireland","Johns Hopkins University, Google, University College London (UCL)",,,,DeepLab (2017),,,
Low-Cost Collaborative Network,Vision,Image classification,"Xuanyi Dong, Junshi Huang, Yi Yang, and Shuicheng Yan",,,,https://arxiv.org/pdf/1703.08651.pdf,258.00,More is Less: A More Complicated Network with Less Inference Complexity,2017-05-15,"National University of Singapore,University of Technology Sydney,Qihoo 360 AI Institute",Academia,,,,,"CIFAR-10, CIFAR-100, ImageNet",,1280000,,,,,,,,,,,,,Unverified,"In this paper, we present a novel and general network structure towards accelerating the inference process of convolutional neural networks, which is more complicated in
network structure yet with less inference complexity. The
core idea is to equip each original convolutional layer
with another low-cost collaborative layer (LCCL), and the
element-wise multiplication of the ReLU outputs of these
two parallel layers produces the layer-wise output. The
combined layer is potentially more discriminative than the
original convolutional layer, and its inference is faster for
two reasons: 1) the zero cells of the LCCL feature maps will
remain zero after element-wise multiplication, and thus it is
safe to skip the calculation of the corresponding high-cost
convolution in the original convolutional layer; 2) LCCL
is very fast if it is implemented as a 1 × 1 convolution or
only a single filter shared by all channels. Extensive experiments on the CIFAR-10, CIFAR-100 and ILSCRC-2012
benchmarks show that our proposed network structure can
accelerate the inference process by 32% on average with
negligible performance drop.",2023-12-05 04:33,Anonymous,,,"Singapore,Australia,China","National University of Singapore, University of Technology Sydney, Qihoo 360 AI Institute",,,,,,,
SRGAN,Vision,Image super-resolution,"Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, Wenzhe Shi",Highly cited,,,https://openaccess.thecvf.com/content_cvpr_2017/html/Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper.html,11032.00,Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network,2017-05-25,Twitter,Industry,,,,,,,,,,,,,,,,,,,Industry,,,2023-11-26 03:25,Robi Rahman,,,United States of America,Twitter,,,,SRGAN,,,
Inflated 3D ConvNet,Vision,Action recognition,"Joao Carreira, Andrew Zisserman",Highly cited,,,https://arxiv.org/abs/1705.07750,6140.00,"Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset",2017-06-01,"DeepMind,University of Oxford",Industry - Academia Collaboration,,,,,,,,,,,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,"United Kingdom of Great Britain and Northern Ireland,United Kingdom of Great Britain and Northern Ireland","DeepMind, University of Oxford",,,,Inflated 3D ConvNet,,,
PointNet++,,3D segmentation,"Charles R. Qi, Li Yi, Hao Su, Leonidas J. Guibas",Highly cited,,,https://arxiv.org/abs/1706.02413,7405.00,PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space,2017-06-07,Stanford University,Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-12-05 04:33,Robi Rahman,,,United States of America,Stanford University,,,,PointNet++,,,
EDSR,Vision,Image super-resolution,"Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, Kyoung Mu Lee",Highly cited,,,https://arxiv.org/abs/1707.02921,4420.00,Enhanced Deep Residual Networks for Single Image Super-Resolution,2017-06-10,Seoul National University,Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-12-05 04:33,Robi Rahman,,,Korea (Republic of),Seoul National University,,,,EDSR,,,
Transformer,Language,Translation,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin",Highly cited,,,https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf,75692.00,Attention Is All You Need,2017-06-12,"Google Research,Google Brain",Industry,213000000.00,"This page suggests the transformer has 213M parameters.

""Although there are others architectures that make use of attention layers, none achieves so good results so fast. Not only that, but the only model that can compite against Transformer is the Slicenet22, proposed just fifteen days before. It takes much longer to train, due to the huge amount of parameters it requires (348 million against the 213 millions of Transformer), and the BLEU scores it achieves are slightly worse on average. In short, up to date it offers no profit over Transformer.""

https://ricardokleinklein.github.io/2017/11/16/Attention-is-all-you-need.html",7424524800000000000.00,"""The model was trained during 300000 steps, roughly 3.5 days, using 8 NVIDIA P100 GPUs.""

source: https://ricardokleinklein.github.io/2017/11/16/Attention-is-all-you-need.html

NVIDIA Tesla P100 has 9.3 teraFLOPS single-precision performance

source: https://www.nvidia.com/en-gb/data-center/tesla-p100/

We assume 0.33 utilization performance, in line with OpenAI's ""AI and compute"" article

source: https://openai.com/blog/ai-and-compute/",,,360000000,"[WORDS]

""For English-French, we used the significantly larger WMT
2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [31]""",,54000000000.00,"Source: rados dataset (FLOPs)
https://drive.google.com/drive/folders/1bhy5z6hh1n3wCHx6528Xb7xB1KhYdAL1",,"We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using
the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the
bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps
(3.5 days).",NVIDIA P100,NVIDIA P100,Self-supervised learning,$111.17,,Industry,,,2023-12-05 04:33,Robi Rahman,,,"Multinational,Multinational","Google Research, Google Brain",,,,Transformer,,,
HRA,Games,Ms Pacman,"H Van Seijen, M Fatemi, J Romoff, R Laroche",SOTA Improvement,"""With the best combination, HRA not only outperforms the state-of-the-art on both metrics, it
also significantly outperforms the human score, convincingly demonstrating the strength of HRA.""",,https://arxiv.org/abs/1706.04208,222.00,Hybrid Reward Architecture for Reinforcement Learning,2017-06-13,"Maluuba,Microsoft",Industry - Academia Collaboration (Industry leaning),,,,,,,,,,,,,,,,,,,Industry,,,2023-11-26 03:25,Robi Rahman,,,"Canada,Multinational","Maluuba, Microsoft",,,,HRA,,,
DeepLabV3,Vision,Semantic segmentation,"Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, Hartwig Adam",Highly cited,,,https://arxiv.org/abs/1706.05587,6474.00,Rethinking Atrous Convolution for Semantic Image Segmentation,2017-06-17,Google,Industry,,,,,,,,,,,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,Multinational,Google,,,,DeepLabV3,,,
NoisyNet-Dueling,Games,Atari Games,"M Fortunato, MG Azar, B Piot, J Menick",SOTA improvement,,,https://arxiv.org/abs/1706.10295v3,741.00,Noisy Networks for Exploration,2017-06-30,DeepMind,Industry,,,,,,,,,,,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,United Kingdom of Great Britain and Northern Ireland,DeepMind,,,,NoisyNet-Dueling,,,
ShuffleNet v1,Vision,,"X Zhang, X Zhou, M Lin, J Sun",Highly cited,,,https://arxiv.org/abs/1707.01083,5192.00,ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices,2017-07-03,Megvii Inc,Industry,2430000.00,,,,,,,,,140000000.00,"Table 4 (ShuffleNet 1x, g=8)

https://arxiv.org/abs/1707.01083",,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,China,Megvii Inc,,,,ShuffleNet v1,,,
JFT,Vision,,"Chen Sun, Abhinav Shrivastava, Saurabh Singh, Abhinav Gupta",Highly cited,,,https://arxiv.org/abs/1707.02968,1923.00,Revisiting Unreasonable Effectiveness of Data in Deep Learning Era.,2017-07-10,"Google Research,Carnegie Mellon University (CMU)",Industry - Academia Collaboration,,,843000000000000000000.00,"Tesla K80 performance: 8.13 TFLOP/s

Assume 40% utilization

60 days * 50 GPUs * 40% utilization * 8.13 TFLOP/s/GPU = 8.43*10^20 FLOP",JFT-300M,,300000000,,,,,1440.0,,NVIDIA Tesla K80,NVIDIA Tesla K80,,$21396.42,,Industry,,"The success of deep learning in vision can be attributed to: (a) models with high capacity; (b) increased computational power; and (c) availability of large-scale labeled data. Since 2012, there have been significant advances in representation capabilities of the models and computational capabilities of GPUs. But the size of the biggest dataset has surprisingly remained constant. What will happen if we increase the dataset size by 10x or 100x? This paper takes a step towards clearing the clouds of mystery surrounding the relationship between `enormous data' and visual deep learning. By exploiting the JFT-300M dataset which has more than 375M noisy labels for 300M images, we investigate how the performance of current vision tasks would change if this data was used for representation learning. Our paper delivers some surprising (and some expected) findings. First, we find that the performance on vision tasks increases logarithmically based on volume of training data size. Second, we show that representation learning (or pre-training) still holds a lot of promise. One can improve performance on many vision tasks by just training a better base model. Finally, as expected, we present new state-of-the-art results for different vision tasks including image classification, object detection, semantic segmentation and human pose estimation. Our sincere hope is that this inspires vision community to not undervalue the data and develop collective efforts in building larger datasets.",2023-12-05 11:18,Robi Rahman,,,"Multinational,United States of America","Google Research, Carnegie Mellon University (CMU)",,,,JFT,50,,JFT
AWD-LSTM,Language,,"Gábor Melis, Chris Dyer, Phil Blunsom",SOTA Improvement,"""We establish a new state of the art on the Penn Treebank and Wikitext-2 corpora""",,https://arxiv.org/abs/1707.05589,555.00,On the State of the Art of Evaluation in Neural Language Models,2017-07-18,"DeepMind,University of Oxford",,24000000.00,,,,WikiText-2,,,,,,,,,,,,,,,,,2023-11-26 03:25,Robi Rahman,AWD-LSTM,,"United Kingdom of Great Britain and Northern Ireland,United Kingdom of Great Britain and Northern Ireland","DeepMind, University of Oxford",,,,AWD-LSTM,,,
Densely Connected LSTM + Var. Dropout,Language,,"Fréderic Godin, Joni Dambre, Wesley De Neve",,,,https://arxiv.org/pdf/1707.06130,7.00,Improving Language Modeling using Densely Connected Recurrent Neural Networks,2017-07-19,,,23000000.00,,12800000000000000.00,,,,,,100.00,,,,,,,,,,,,,2023-11-23 06:53,Robi Rahman,Densely Connected LSTM + Var. Dropout,,,,,,,,,,
4 layer Densely Connected LSTM,Language,,,,,,,,,2017-07-19,,,,,,,,,,,100.00,,,,,,,,,,,Unverified,,2023-12-08 01:01,Robi Rahman,4 layer Densely Connected LSTM,1,,,,,,,,,
NASNet-A,Vision,Image classification,"B Zoph, V Vasudevan, J Shlens",Highly cited,,,https://arxiv.org/abs/1707.07012,4797.00,Learning Transferable Architectures for Scalable Image Recognition,2017-07-21,Google Brain,Industry,89000000.00,,,,,,,,,,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,Multinational,Google Brain,,,,NASNet-A,,,
PSPNet,Vision,Image segmentation,"Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, Jiaya Jia",Highly cited,,,https://ieeexplore.ieee.org/document/8100143,9002.00,Pyramid Scene Parsing Network,2017-07-21,Chinese University of Hong Kong,Industry - Academia Collaboration (Academia leaning),,,,,,,,,,,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,Hong Kong,Chinese University of Hong Kong,,,,PSPNet,,,
RetinaNet-R50,Vision,Object detection,"TY Lin, P Goyal, R Girshick, K He",Highly cited,,,https://arxiv.org/abs/1708.02002,16437.00,Focal loss for dense object detection,2017-08-07,Facebook AI Research,Industry,34000000.00,source: table 2 in https://arxiv.org/pdf/1911.09070.pdf,,,,,,,,97000000000.00,source: table 2 in https://arxiv.org/pdf/1911.09070.pdf,,,,,,,,Industry,,,2023-11-26 03:25,Robi Rahman,,,Multinational,Facebook AI Research,,,,RetinaNet-R50,,,
RetinaNet-R101,Vision,Object detection,"TY Lin, P Goyal, R Girshick, K He, P Dollar",Highly cited,,,https://arxiv.org/abs/1708.02002,16437.00,Focal loss for dense object detection,2017-08-07,Facebook AI Research,Industry,53000000.00,source: table 2 in https://arxiv.org/pdf/1911.09070.pdf,2065392000000000000.00,"""We use synchronized SGD over 8 GPUs with a total of 16 images per minibatch (2 images per GPU). Unless otherwise specified, all models are trained for 90k iterations with an initial learning rate of 0.01, which is then divided by 10 at 60k and again at 80k iterations. We use horizontal image flipping as the only form of data augmentation unless otherwise noted. Weight decay of 0.0001 and momentum of 0.9 are used. The training loss is the sum the focal loss and the standard smooth L1 loss used for box regression [10]. Training time ranges between 10 and 35 hours for the models in Table 1e.""

NVIDIA M40 GPU

35*60**2*0.3*8*6.83E+12 = 2.07e18",COCO,,135000,trainval135k split,,127000000000.00,source: table 2 in https://arxiv.org/pdf/1911.09070.pdf,,,NVIDIA M40,NVIDIA M40,,,,Industry,,,2023-11-26 03:25,Robi Rahman,,,Multinational,Facebook AI Research,,,,RetinaNet-R101,,,
AWD-LSTM - 3-layer LSTM (tied) + continuous cache pointer (WT2),Language,,"Stephen Merity, Nitish Shirish Keskar, Richard Socher","SOTA Improvement,Highly cited","""we achieve an
even lower state-of-the-art perplexity of 52.8 on Penn Treebank and 52.0 on WikiText-2.""",,https://arxiv.org/abs/1708.02182,1176.00,Regularizing and Optimizing LSTM Language Models,2017-08-07,Salesforce Research,,33000000.00,,309000000000000000.00,,WikiText-2,,,,750.00,,,,,,,,,,,,,2023-11-26 03:25,Robi Rahman,AWD-LSTM - 3-layer LSTM (tied) + continuous cache pointer (WT2),,United States of America,Salesforce Research,,,,AWD-LSTM - 3-layer LSTM (tied) + continuous cache pointer (WT2),,,
AWD-LSTM - 3-layer LSTM (tied) + continuous cache pointer (PTB),Language,,,,,,,,,2017-08-07,,,,,,,,,,,500.00,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,AWD-LSTM - 3-layer LSTM (tied) + continuous cache pointer (PTB),1,,,,,,,,,
OpenAI TI7 DOTA 1v1,Games,DOTA,,"SOTA Improvement,Historical significance",,,https://openai.com/research/dota-2,0.00,Dota 2,2017-08-11,OpenAI,Industry,,"Section 4 states: ""we used a model with over 150 million parameters"" but this is for the 5v5 agent, not the 1v1.",604609522259200200000.00,Extracted from AI and Compute (https://openai.com/blog/ai-and-compute/) charts by using https://automeris.io/WebPlotDigitizer/.,,,,,,,,,,,,,$2873.99,,Industry,,"We’ve created a bot which beats the world’s top professionals at 1v1 matches of Dota 2 under standard tournament rules. The bot learned the game from scratch by self-play, and does not use imitation learning or tree search. This is a step towards building AI systems which accomplish well-defined goals in messy, complicated situations involving real humans.",2023-12-05 11:15,Robi Rahman,,,United States of America,OpenAI,,,,OpenAI TI7 DOTA 1v1,,,OpenAI TI7 DOTA 1v1
EI-REHN-1000D,Language,,"Hyunsin Park, Chang D. Yoo",SOTA Improvement,"""The proposed networks showed better performance than other state-of-the-art recurrent networks in all three experiments.""",,https://arxiv.org/pdf/1708.04116,6.00,Early Improving Recurrent Elastic Highway Network,2017-08-14,Korea Advanced Institute of Science and Technology (KAIST),,19000000.00,,10600000000000000.00,,,,,,100.00,,,,,,,,,,,,,2023-11-26 03:25,Robi Rahman,EI-REHN-1000D,,Korea (Republic of),Korea Advanced Institute of Science and Technology (KAIST),,,,EI-REHN-1000D,,,
EI-REHN-1200D,Language,,,,,,,,,2017-08-14,,,,,,,,,,,100.00,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,EI-REHN-1200D,1,,,,,,,,,
Cutout-regularized net,Vision,Image classification," Terrance DeVries, Graham W. Taylor",Highly cited,,,https://arxiv.org/abs/1708.04552,2849.00,Improved Regularization of Convolutional Neural Networks with Cutout,2017-08-15,"University of Guelph,Canadian Institute for Advanced Research,Vector Institute",Industry - Academia Collaboration,,,,,,,,,,,,,,,,,,https://www.yuzeh.com/data/agz-cost.html,Industry,,,2023-12-05 04:33,Robi Rahman,,,"Canada,Canada,Canada","University of Guelph, Canadian Institute for Advanced Research, Vector Institute",,,,Cutout-regularized net,,,
NeuMF (Pinterest),Recommendation,Collaborative filtering,"X He, L Liao, H Zhang, L Nie, X Hu",Highly cited,,,https://arxiv.org/abs/1708.05031,4485.00,Neural Collaborative Filtering,2017-08-16,"Shandong University,Texas A&M,National University of Singapore,Columbia University",Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-12-05 04:33,Robi Rahman,,,"China,United States of America,Singapore,United States of America","Shandong University, Texas A&M, National University of Singapore, Columbia University",,,,NeuMF (Pinterest),,,
GRU + p-tHSM (pretrain via Brown) (WT103),Language,,"Nan Jiang, Wenge Rong, Min Gao, Yikang Shen, Zhang Xiong",,,,https://www.researchgate.net/profile/Yikang-Shen-2/publication/318830618_Exploration_of_Tree-based_Hierarchical_Softmax_for_Recurrent_Language_Models/links/5b2c050aa6fdcc8506bc6f4a/Exploration-of-Tree-based-Hierarchical-Softmax-for-Recurrent-Language-Models.pdf,6.00,Exploration of Tree-based Hierarchical Softmax for Recurrent Language Models,2017-08-19,,Industry,206000000.00,,,,WikiText-103,,,,,,,,,,,,,,,,,2023-11-21 01:39,Robi Rahman,GRU + p-tHSM (pretrain via Brown) (WT103),,,,,,,,,,
GRU + p-tHSM (pretrain via Brown) (PTB),Language,,,,,,,,,2017-08-19,,,,,,,,,,,,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,GRU + p-tHSM (pretrain via Brown) (PTB),1,,,,,,,,,
GRU + p-tHSM (pretrain via Brown) (WT2),Language,,,,,,,,,2017-08-19,,,,,,,,,,,,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,GRU + p-tHSM (pretrain via Brown) (WT2),1,,,,,,,,,
D-LSRC(200)+KN5,Language,,"Youssef Oualil, Mittul Singh, Clayton Greenberg, Dietrich Klakow",,,,https://arxiv.org/pdf/1708.06555,19.00,Long-Short Range Context Neural Networks for Language Modeling,2017-08-22,,,7160000.00,,,,Penn TreeBank,,,,,,,,,,,,,,,,,2023-11-21 01:39,Robi Rahman,D-LSRC(200)+KN5,,,,,,,,,,
D-LSRC(100)+KN5,Language,,,,,,,,,2017-08-22,,,,,,,,,,,,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,D-LSRC(100)+KN5,1,,,,,,,,,
NMM(LSTM+RNN),Language,,"Youssef Oualil, Dietrich Klakow",,,,https://arxiv.org/pdf/1708.06989,10.00,A Neural Network Approach for Mixing Language Models,2017-08-23,,,5180000.00,,,,,,,,,,,,,,,,,,,,,2023-11-21 01:39,Robi Rahman,NMM(LSTM+RNN),,,,,,,,,,
GL-LWGC-AWD-MoS-LSTM + dynamic evaluation (WT2),Language,,"Ziv Aharoni, Gal Rattner, Haim Permuter",SOTA Improvement,"""Our GL-LSTM model
overcame the state-of-the-art results with only two layers and 19M parameters, and further improved
the state-of-the-art results with the third layer phase""",,https://arxiv.org/abs/1708.08863,4.00,Gradual Learning of Recurrent Neural Networks,2017-08-29,Ben-Gurion University of the Negev,,38000000.00,,474000000000000000.00,,WikiText-2,,,,1000.00,,,,,,,,,,,,,2023-11-26 03:25,Robi Rahman,GL-LWGC-AWD-MoS-LSTM + dynamic evaluation (WT2),,United States of America,Ben-Gurion University of the Negev,,,,GL-LWGC-AWD-MoS-LSTM + dynamic evaluation (WT2),,,
GL-LWGC-AWD-MoS-LSTM + dynamic evaluation (PTB),Language,,,,,,,,,2017-08-29,,,,,,,,,,,1000.00,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,GL-LWGC-AWD-MoS-LSTM + dynamic evaluation (PTB),1,,,,,,,,,
SENet (ImageNet),Vision,Image classification,"Jie Hu, Li Shen, Samuel Albanie, Gang Sun, Enhua Wu",Highly cited,,,https://arxiv.org/abs/1709.01507,18271.00,Squeeze-and-Excitation Networks,2017-09-05,"Chinese Academy of Sciences,University of Oxford",Academia,28100000.00,Table 16,,,ImageNet,,,,,3870000000.00,Table 16,,,,,,,,Academia,,,2023-12-05 04:33,Robi Rahman,,,"China,United Kingdom of Great Britain and Northern Ireland","Chinese Academy of Sciences, University of Oxford",,,,SENet (ImageNet),,,
PyramidNet,Vision,Image Classification,"Dongyoon Han, Jiwhan Kim, Junmo Kim",SOTA Improvement,"""In tests using CIFAR-10, CIFAR-100, and ImageNet1k datasets, our PyramidNets outperform all previous state-
of-the-art deep network architectures.""",,https://arxiv.org/abs/1610.02915v4,718.00,Deep Pyramidal Residual Networks,2017-09-06,Korea Advanced Institute of Science and Technology (KAIST),Academia,26000000.00,best model had 26M params,,,CIFAR-10,,,,300.00,,,,,,,,,,,Likely,"Deep convolutional neural networks (DCNNs) have shown remarkable performance in image classification tasks in recent years. Generally, deep neural network architectures are stacks consisting of a large number of convolutional layers, and they perform downsampling along the spatial dimension via pooling to reduce memory usage. Concurrently, the feature map dimension (i.e., the number of channels) is sharply increased at downsampling locations, which is essential to ensure effective performance because it increases the diversity of high-level attributes. This also applies to residual networks and is very closely related to their performance. In this research, instead of sharply increasing the feature map dimension at units that perform downsampling, we gradually increase the feature map dimension at all units to involve as many locations as possible. This design, which is discussed in depth together with our new insights, has proven to be an effective means of improving generalization ability. Furthermore, we propose a novel residual unit capable of further improving the classification accuracy with our new network architecture. Experiments on benchmark CIFAR-10, CIFAR-100, and ImageNet datasets have shown that our network architecture has superior generalization ability compared to the original residual networks. Code is available at this https URL}",2023-11-26 03:25,Anonymous,,,Korea (Republic of),Korea Advanced Institute of Science and Technology (KAIST),,,,PyramidNet,,,
ISS,Language,,"Wei Wen, Yuxiong He, Samyam Rajbhandari, Minjia Zhang, Wenhan Wang, Fang Liu, Bin Hu, Yiran Chen, Hai Li",SOTA Improvement,"""Moreover, ISS learning can find a
smaller RHN model with width 726, meanwhile improve the state-of-the-art perplexity as shown by the second entry in Table 2.""",,https://arxiv.org/pdf/1709.05027,146.00,Learning Intrinsic Sparse Structures within Long Short-Term Memory,2017-09-15,"Duke University,Microsoft",Industry - Academia Collaboration,11100000.00,,3400000000000000.00,,,,,,55.00,,,,,,,,,,,,,2023-11-26 03:25,Robi Rahman,ISS,,"United States of America,Multinational","Duke University, Microsoft",,,,ISS,,,
LSTM + dynamic eval,Language,,"Ben Krause, Emmanuel Kahembwe, Iain Murray, Steve Renals",SOTA Improvement,"""Dynamic evaluation outperforms existing adaptation approaches in our comparisons. Dynamic evaluation improves the state-of-the-art word-level perplexities on the Penn Treebank and WikiText-2 datasets to 51.1 and 44.3 respectively""",,https://arxiv.org/abs/1709.07432,130.00,Dynamic Evaluation of Neural Sequence Models,2017-09-21,University of Edinburgh,,50000000.00,,,,Penn TreeBank,,,,,,,,,,,,,,,,,2023-11-26 03:25,Robi Rahman,LSTM + dynamic eval,,United Kingdom of Great Britain and Northern Ireland,University of Edinburgh,,,,LSTM + dynamic eval,,,
AWD-LSTM + dynamic eval (PTB),Language,,,,,,,,,2017-09-21,,,,,,,,,,,,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,AWD-LSTM + dynamic eval (PTB),1,,,,,,,,,
AWD-LSTM + dynamic eval (WT2),Language,,,,,,,,,2017-09-21,,,,,,,,,,,,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,AWD-LSTM + dynamic eval (WT2),1,,,,,,,,,
AWD-LSTM+WT+Cache+IOG (WT2),Language,,"Sho Takase, Jun Suzuki, Masaaki Nagata",SOTA Improvement,"""IOG achieves comparable scores to the state-of-the-art on the Penn Treebank
dataset and outperforms the WikiText-2 dataset""",,https://arxiv.org/pdf/1709.08907,7.00,Input-to-Output Gate to Improve RNN Language Models,2017-09-26,NTT Communication Science Laboratories,,53000000.00,,3310000000000000.00,,,,,,5.00,,,,,,,,,,,,,2023-11-26 03:25,Robi Rahman,AWD-LSTM+WT+Cache+IOG (WT2),,Japan,NTT Communication Science Laboratories,,,,AWD-LSTM+WT+Cache+IOG (WT2),,,
AWD-LSTM+WT+Cache+IOG (PTB),Language,,,,,,,,,2017-09-26,,,,,,,,,,,5.00,,,,,,,,,,,Unverified,,2023-12-08 01:01,Robi Rahman,AWD-LSTM+WT+Cache+IOG (PTB),1,,,,,,,,,
AlphaGo Zero,Games,Go,"D Silver, J Schrittwieser, K Simonyan, I Antonoglou",Highly cited,,,https://www.researchgate.net/publication/320473480_Mastering_the_game_of_Go_without_human_knowledge,7831.00,Mastering the game of Go without human knowledge,2017-10-18,DeepMind,Industry,46400244.00,Quick calculation,3.41e+23,"source: https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389


AGZ had two models, one of which was small and another of which was large. The compute for AGZ is for the large model, which has 40 residual blocks instead of 20.

A second way of looking at this... we believe multiple TPUs were used for training. 29 million games * 211 moves per game on average * 0.8 seconds per move = 4.8952E+09 seconds of player-time across all TPUs.

4.8952E+09 seconds of player-time / (40 days * 24 * 60 * 60 seconds of real time) ~= 1,416 players

4 TPUs per player => 4.8952E+09 * 4 = 1.95808E+10 TPU-seconds
Total compute = 1.95808E+10 TPU-seconds * 92E+12 FLOP/(TPU-second) * 0.4 = 7.2e23 FLOP

So similar to the Cotra and Davidson estimate (within a factor of 2).",,,5800000000,"""Over the course of training, 29 million games of self-play were generated""

Approx 200 moves per Go game on average

https://homepages.cwi.nl/~aeb/go/misc/gostat.html

Thus 200 * 29e6 = 5.8e9",,,,480.0,,Google TPU v1,Google TPU v1,Self-supervised learning,$1544149.42,,Industry,,"A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo's own move selections and also the winner of AlphaGo's games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100-0 against the previously published, champion-defeating AlphaGo. © 2017 Macmillan Publishers Limited, part of Springer Nature. All rights reserved.",2023-12-05 04:33,Robi Rahman,,,United Kingdom of Great Britain and Northern Ireland,DeepMind,,,,AlphaGo Zero,,,AlphaGo Zero
LRSO-GAN,Vision,Person re-identification,"Zhedong Zheng, Liang Zheng, Yi Yang",Highly cited,,,https://arxiv.org/abs/1701.07717,1652.00,Unlabeled Samples Generated by GAN Improve the Person Re-identification Baseline in vitro,2017-10-22,University of Technology Sydney,Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-12-05 04:33,Robi Rahman,,,Australia,University of Technology Sydney,,,,LRSO-GAN,,,
CapsNet (MultiMNIST),Vision,Character recognition,"S Sabour, N Frosst, GE Hinton",Highly cited,,,https://arxiv.org/abs/1710.09829,3887.00,Dynamic Routing Between Capsules,2017-10-26,Google Brain,Industry,11360000.00,"""This model has 24.56M parameters which is 2 times more parameters
than CapsNet with 11.36M parameters.""",,,,,,,,,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,Multinational,Google Brain,,,,CapsNet (MultiMNIST),,,
CapsNet (MNIST),Vision,Character recognition,"S Sabour, N Frosst, GE Hinton",Highly cited,,,https://arxiv.org/abs/1710.09829,3887.00,Dynamic Routing Between Capsules,2017-10-26,Google Brain,Industry,8200000.00,"""In terms of number of parameters the baseline has 35.4M while CapsNet
has 8.2M parameters and 6.8M parameters without the reconstruction subnetwork""",,"It should be feasible to estimate this from the information in the paper, but it would require carefully checking the FLOP involved for capsules.",MNIST,,60000,Section 5: The dataset has 60K and 10K images for training and testing respectively.,,,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,Multinational,Google Brain,,,,CapsNet (MNIST),,,
ProgressiveGAN,Vision,Image generation,"Tero Karras, Timo Aila, Samuli Laine, Jaakko Lehtinen",Highly cited,,,https://arxiv.org/abs/1710.10196,5795.00,"Progressive Growing of GANs for Improved Quality, Stability, and Variation",2017-10-27,NVIDIA,Industry,,,,,,,,,,,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,United States of America,NVIDIA,,,,ProgressiveGAN,,,
Fraternal dropout + AWD-LSTM 3-layer (WT2),Language,,"Konrad Zolna, Devansh Arpit, Dendi Suhubdy, Yoshua Bengio",SOTA Improvement,"""We evaluate our model and achieve state-of-the-art results in sequence
modeling tasks on two benchmark datasets – Penn Treebank and Wikitext-2""",,https://arxiv.org/abs/1711.00066,55.00,Fraternal Dropout,2017-10-31,"Jagiellonian University,Mila- Quebec AI,University of Montreal",,34000000.00,,98500000000000000.00,,WikiText-2,,,,520.00,,,,,,,,,,,,,2023-11-26 03:25,Robi Rahman,Fraternal dropout + AWD-LSTM 3-layer (WT2),,"Poland,Canada,Canada","Jagiellonian University, Mila- Quebec AI, University of Montreal",,,,Fraternal dropout + AWD-LSTM 3-layer (WT2),,,
Fraternal dropout + AWD-LSTM 3-layer (PTB),Language,,,,,,,,,2017-10-31,,,,,,,,,,,520.00,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,Fraternal dropout + AWD-LSTM 3-layer (PTB),1,,,,,,,,,
"AWD-LSTM-MoS + dynamic evaluation (WT2, 2017)",Language,,"Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, William W. Cohen",SOTA Improvement,"""Experimental results confirm that the
proposed method significantly improves state-of-the-art language models, achieving a perplexity of 55.31 and 62.89 on
the test set of Penn Treebank and WikiText-2""",,https://arxiv.org/abs/1711.03953,358.00,Breaking the Softmax Bottleneck: A High-Rank RNN Language Model,2017-11-10,Carnegie Mellon University (CMU),,35000000.00,,437000000000000000.00,,WikiText-2,,,,1000.00,,,,,,,,,,,,,2023-11-26 03:25,Robi Rahman,"""AWD-LSTM-MoS + dynamic evaluation (WT2, 2017)""",,United States of America,Carnegie Mellon University (CMU),,,,"""AWD-LSTM-MoS + dynamic evaluation (WT2, 2017)""",,,
"AWD-LSTM-MoS + dynamic evaluation (PTB, 2017)",Language,,,,,,,,,2017-11-10,,,,,,,,,,,1000.00,,,,,,,,,,,Unverified,,2023-12-08 01:01,Robi Rahman,"""AWD-LSTM-MoS + dynamic evaluation (PTB, 2017)""",1,,,,,,,,,
TriNet,Vision,Person re-identification,"Alexander Hermans, Lucas Beyer, Bastian Leibe",Highly cited,,,https://arxiv.org/abs/1703.07737,2715.00,In Defense of the Triplet Loss for Person Re-Identification,2017-11-21,"Visual Computing Institute,Aachen University",Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-12-05 04:33,Robi Rahman,,,"Germany,Germany","Visual Computing Institute, Aachen University",,,,TriNet,,,
PNAS-net,Vision,Image classification,"C Liu, B Zoph, M Neumann, J Shlens",Highly cited,,,https://arxiv.org/abs/1712.00559,1785.00,Progressive Neural Architecture Search,2017-12-02,"Johns Hopkins University,Google AI,Stanford University",Industry - Academia Collaboration (Industry leaning),86000000.00,,,,,,,,,,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,"United States of America,Multinational,United States of America","Johns Hopkins University, Google AI, Stanford University",,,,PNAS-net,,,
PNASNet-5,,,"C Liu, B Zoph, M Neumann, J Shlens",Highly cited,,,https://arxiv.org/abs/1712.00559,1785.00,Progressive Neural Architecture Search,2017-12-02,"Johns Hopkins University,Google AI,Stanford University",Industry - Academia Collaboration (Industry leaning),,,66290400000000010000.00,"8 times less compute than Zoph (2018), which used 500 p100s for 4 days.
(From Imagenet paper-data, Besiroglu et al., forthcoming) ",Imagenet-1k,,1280000,,,,,,,,,,$991.48,,Industry,,"We propose a new method for learning the structure of convolutional neural networks (CNNs) that is more efficient than recent state-of-the-art methods based on reinforcement learning and evolutionary algorithms. Our approach uses a sequential model-based optimization (SMBO) strategy, in which we search for structures in order of increasing complexity, while simultaneously learning a surrogate model to guide the search through structure space. Direct comparison under the same search space shows that our method is up to 5 times more efficient than the RL method of Zoph et al. (2018) in terms of number of models evaluated, and 8 times faster in terms of total compute. The structures we discover in this way achieve state of the art classification accuracies on CIFAR-10 and ImageNet.",2023-12-05 04:33,Robi Rahman,,,"United States of America,Multinational,United States of America","Johns Hopkins University, Google AI, Stanford University",,,,PNASNet-5,,,
AlphaZero,Games,,"D Silver, T Hubert, J Schrittwieser, I Antonoglou",Highly cited,,,https://arxiv.org/abs/1712.01815,1343.00,Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm,2017-12-05,DeepMind,Industry,,,3.6679273004682866e+22,Extracted from AI and Compute (https://openai.com/blog/ai-and-compute/) charts by using https://automeris.io/WebPlotDigitizer/.,,,700000,"""We trained a separate instance of AlphaZero for each game. Training proceeded
for 700,000 steps""",,,"This post claims 0.8 seconds per move for the 40-day training version of AlphaGo Zero
https://www.yuzeh.com/data/agz-cost.html
Compare move time for AlphaZero",,,Google TPU v2,Google TPU v2,Self-supervised learning,$162054.70,,Industry,,"The game of chess is the most widely-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. In contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go, by tabula rasa reinforcement learning from games of self-play. In this paper, we generalise this approach into a single AlphaZero algorithm that can achieve, tabula rasa, superhuman performance in many challenging domains. Starting from random play, and given no domain knowledge except the game rules, AlphaZero achieved within 24 hours a superhuman level of play in the games of chess and shogi (Japanese chess) as well as Go, and convincingly defeated a world-champion program in each case.",2023-12-05 04:33,Robi Rahman,,0,United Kingdom of Great Britain and Northern Ireland,DeepMind,,,,AlphaZero,64,,AlphaZero
2-layer-LSTM+Deep-Gradient-Compression,Language,,"Yujun Lin, Song Han, Huizi Mao, Yu Wang, William J. Dally",Highly cited,,,https://arxiv.org/pdf/1712.01887,1270.00,Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training,2017-12-05,"Tsinghua University,Stanford University,NVIDIA",Academia,6020000.00,,1340000000000000.00,,,,,,40.00,,,,,,,,,,,,,2023-11-26 03:25,Robi Rahman,2-layer-LSTM+Deep-Gradient-Compression,,"China,United States of America,United States of America","Tsinghua University, Stanford University, NVIDIA",,,,2-layer-LSTM+Deep-Gradient-Compression,,,
RNNLM + Dynamic KL Regularization (WT2),Language,,"Thanapon Noraset, David Demeter, Doug Downey",,,,https://ojs.aaai.org/index.php/AAAI/article/view/11993,6.00,Controlling Global Statistics in Recurrent Neural Network Text Generation,2018-01-01,,,87600000.00,,21900000000000000.00,,,,,,20.00,,,,,,,,,,,,,2023-11-23 06:53,Robi Rahman,RNNLM + Dynamic KL Regularization (WT2),,,,,,,,,,
RNNLM + Dynamic KL Regularization,Language,,,,,,,,,2018-01-01,,,,,,,,,,,20.00,,,,,,,,,,,Unverified,,2023-12-08 01:01,Robi Rahman,RNNLM + Dynamic KL Regularization,1,,,,,,,,,
Refined Part Pooling,Vision,Person retrieval,"Yifan Sun, Liang Zheng, Yi Yang, Qi Tian, Shengjin Wang",Highly cited,,,https://arxiv.org/abs/1711.09349,1820.00,Beyond Part Models: Person Retrieval with Refined Part Pooling (and a Strong Convolutional Baseline),2018-01-09,"Tsinghua University,University of Technology Sydney,University of Texas at San Antonio",Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-12-05 04:33,Robi Rahman,,,"China,Australia,United States of America","Tsinghua University, University of Technology Sydney, University of Texas at San Antonio",,,,Refined Part Pooling,,,
ULM-FiT,Language,Text classification,"J Howard, S Ruder",Highly cited,,,https://arxiv.org/abs/1801.06146,1940.00,Universal Language Model Fine-tuning for Text Classification,2018-01-18,"University of San Francisco,Insight Centre NUI Galway",Industry - Academia Collaboration,,,,,,,,,,,,,,,,,,,Industry,,,2023-11-26 03:25,Robi Rahman,,,"United States of America,Ireland","University of San Francisco, Insight Centre NUI Galway",,,,ULM-FiT,,,
ELMo,Language,,"ME Peters, M Neumann, M Iyyer, M Gardner",Highly cited,,,https://arxiv.org/abs/1802.05365,10194.00,Deep contextualized word representations,2018-02-01,"University of Washington,Allen Institute for AI",Industry,94000000.00,,,3300e12 - https://github.com/amirgholami/ai_and_memory_wall,,,,,,26000000000.00,"Rados dataset (FLOPs)
https://drive.google.com/drive/folders/1bhy5z6hh1n3wCHx6528Xb7xB1KhYdAL1",,,,,Self-supervised learning,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,"United States of America,United States of America","University of Washington, Allen Institute for AI",,,,ELMo,,,
QRNN,Language,,"Stephen Merity, Nitish Shirish Keskar, James Bradbury, Richard Socher",SOTA Improvement,"""we reduce our per-epoch time substantially and achieve a new state-of-the-art on WikiText-103 despite training for 14 epochs""",,https://mlsys.org/Conferences/doc/2018/50.pdf,4.00,Scalable Language Modeling: WikiText-103 on a Single GPU in 12 hours,2018-02-01,Salesforce Research,,135000000.00,,360000000000000000.00,,WikiText-103,,,,14.00,,,,,,,,,,,,,2023-11-26 03:25,Robi Rahman,QRNN,,United States of America,Salesforce Research,,,,QRNN,,,
AmoebaNet-A (F=448),Vision,Image classification,"Esteban Real, Alok Aggarwal, Yanping Huang, Quoc V Le",Highly cited,,,https://arxiv.org/abs/1802.01548,2505.00,Regularized Evolution for Image Classifier Architecture Search,2018-02-05,Google Brain,Industry,469000000.00,Table 2,385296912000000000000.00,"450 K40 GPUs for 20k models (approx. 7 days).
(From Imagenet paper-data, Besiroglu et al., forthcoming) ",Imagenet-1k,,1280000,,,,,168.0,"""Each experiment ran on 450 K40 GPUs for 20k models (approx. 7 days).""",NVIDIA Tesla K40s,NVIDIA Tesla K40s,,$5858.75,,Industry,,"The effort devoted to hand-crafting neural network image classifiers has motivated the use of architecture search to discover them automatically. Although evolutionary algorithms have been repeatedly applied to neural network topologies, the image classifiers thus discovered have remained inferior to human-crafted ones. Here, we evolve an image classifier---AmoebaNet-A---that surpasses hand-designs for the first time. To do this, we modify the tournament selection evolutionary algorithm by introducing an age property to favor the younger genotypes. Matching size, AmoebaNet-A has comparable accuracy to current state-of-the-art ImageNet models discovered with more complex architecture-search methods. Scaled to larger size, AmoebaNet-A sets a new state-of-the-art 83.9% / 96.6% top-5 ImageNet accuracy. In a controlled comparison against a well known reinforcement learning algorithm, we give evidence that evolution can obtain results faster with the same hardware, especially at the earlier stages of the search. This is relevant when fewer compute resources are available. Evolution is, thus, a simple method to effectively discover high-quality architectures.",2023-12-05 11:03,Robi Rahman,,,Multinational,Google Brain,,,,AmoebaNet-A (F=448),450,,AmoebaNet-A (F=448)
AmoebaNet-A (F=190),Vision,Image classification,"E Real, A Aggarwal, Y Huang, QV Le",Highly cited,,,https://arxiv.org/abs/1802.01548,2505.00,Regularized Evolution for Image Classifier Architecture Search,2018-02-05,Google Brain,Industry,87000000.00,Table 2,,,,,,,,,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,Multinational,Google Brain,,,,AmoebaNet-A (F=190),,,
IMPALA,Games,Atari,"Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymir Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, Koray Kavukcuoglu",SOTA Improvement,"""IMPALA is able to achieve better performance than previous agents with less data""",,https://arxiv.org/abs/1802.01561,1288.00,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,2018-02-05,DeepMind,Industry,1600000.00,"""Figure 3 in the paper states that the large architecture has 1.6 million parameters. I am using the large model because it was the only one trained on all the Atari games at once, which seems like the most impressive task in the suite.""

Source: https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389",168000000000000000000.00,"Source: Ajeya Cotra and Tom Davidson, https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389",,,240000000000,"From fig 6, there were 1e10 environment frames, and 24 agents. Thus we note down 2.4e11 for the ""dataset size""",,,,100.0,Maximum training time for IMPALA is 100 hours according to Figure 6. This seems to refer to the 1 GPU model. The 8 GPU model looks to have been trained about 1/8 as long.,NVIDIA P100,NVIDIA P100,Self-supervised learning,,,Industry,,"In this work we aim to solve a large collection of tasks using a single reinforcement learning agent with a single set of parameters. A key challenge is to handle the increased amount of data and extended training time. We have developed a new distributed agent IMPALA (Importance Weighted Actor-Learner Architecture) that not only uses resources more efficiently in single-machine training but also scales to thousands of machines without sacrificing data efficiency or resource utilisation. We achieve stable learning at high throughput by combining decoupled acting and learning with a novel off-policy correction method called V-trace. We demonstrate the effectiveness of IMPALA for multi-task reinforcement learning on DMLab-30 (a set of 30 tasks from the DeepMind Lab environment (Beattie et al., 2016)) and Atari-57 (all available Atari games in Arcade Learning Environment (Bellemare et al., 2013a)). Our results show that IMPALA is able to achieve better performance than previous agents with less data, and crucially exhibits positive transfer between tasks as a result of its multi-task approach.",2023-12-05 11:59,Robi Rahman,,,United Kingdom of Great Britain and Northern Ireland,DeepMind,,,,IMPALA,1,,IMPALA
DeepLabV3+,Vision,Semantic segmentation,"Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, Hartwig Adam",Highly cited,,,https://arxiv.org/abs/1802.02611v3,8791.00,Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation,2018-02-07,Google,Industry,,,,,,,,,,,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,Multinational,Google,,,,DeepLabV3+,,,
ENAS,Language,,"Hieu Pham, Melody Y. Guan, Barret Zoph, Quoc V. Le, Jeff Dean",Highly cited,,,https://arxiv.org/abs/1802.03268,2760.00,Efficient Neural Architecture Search via Parameter Sharing,2018-02-09,"Google Brain,Carnegie Mellon University (CMU),Stanford University",Academia,24000000.00,,20099999999999996.00,,Penn TreeBank,,,,150.00,,,,,,,,,,,,,2023-11-26 03:25,Robi Rahman,ENAS,,"Multinational,United States of America,United States of America","Google Brain, Carnegie Mellon University (CMU), Stanford University",,,,ENAS,,,
Multipop Adaptive Continuous Stack (WT2),Language,,"Dani Yogatama, Yishu Miao, Gabor Melis, Wang Ling, Adhiguna Kuncoro, Chris Dyer, Phil Blunsom",,,,https://openreview.net/forum?id=SkFqf0lAZ,62.00,Memory Architectures in Recurrent Neural Network Language Models,2018-02-15,,,26000000.00,,,,,,,,,,,,,,,,,,,,,2023-11-21 01:39,Robi Rahman,Multipop Adaptive Continuous Stack (WT2),,,,,,,,,,
TCN (148M),Language,,"Shaojie Bai, J. Zico Kolter, Vladlen Koltun",,,,https://openreview.net/forum?id=rk8wKk-R-,64.00,Convolutional Sequence Modeling Revisited,2018-02-15,"Carnegie Mellon University (CMU),Intel Labs",,148000000.00,,,,WikiText-103,,,,,,,,,,,,,,,,,2023-11-21 01:39,Robi Rahman,TCN (148M),,"United States of America,Multinational","Carnegie Mellon University (CMU), Intel Labs",,,,,,,
TCN (13M),Language,,,,,,,,,2018-02-15,,,,,,,Penn TreeBank,,,,,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,TCN (13M),1,,,,,,,,,
Multipop Adaptive Continuous Stack (PTB),Language,,,,,,,,,2018-02-15,,,,,,,,,,,,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,Multipop Adaptive Continuous Stack (PTB),1,,,,,,,,,
TCN (P-MNIST),Language,Image Classification,"Shaojie Bai, J. Zico Kolter, Vladlen Koltun",SOTA Improvement,"""For the permuted sequential MNIST, TCNs
outperform state of the art results using recurrent nets (95.9%) with Zoneout+Recurrent BatchNorm
(Cooijmans et al., 2016; Krueger et al., 2017), a highly optimized method for regularizing RNNs""",,https://openreview.net/forum?id=rk8wKk-R-,64.00,Convolutional Sequence Modeling Revisited,2018-02-15,"Carnegie Mellon University (CMU),Intel Labs",,42000.00,,,,P-MNIST,,,,,,,,,,,,,,,Unverified,,2023-11-26 03:25,Epoch Artificialintelligence,TCN (148M),,"United States of America,Multinational","Carnegie Mellon University (CMU), Intel Labs",,,,TCN (P-MNIST),,,
Spectrally Normalized GAN,Vision,Image generation,"Takeru Miyato, Toshiki Kataoka, Masanori Koyama, Yuichi Yoshida",Highly cited,,,https://arxiv.org/abs/1802.05957,3747.00,Spectral Normalization for Generative Adversarial Networks,2018-02-16,"Preferred Networks Inc,Ritsumeikan University,National Institute of Informatics",Industry - Academia Collaboration,,,,,,,,,,,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,"Japan,Japan,Japan","Preferred Networks Inc, Ritsumeikan University, National Institute of Informatics",,,,Spectrally Normalized GAN,,,
Residual Dense Network,Vision,Image super-resolution," Yulun Zhang, Yapeng Tian, Yu Kong, Bineng Zhong, Yun Fu",Highly cited,,,https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Residual_Dense_Network_CVPR_2018_paper.html,2628.00,Residual Dense Network for Image Super-Resolution,2018-02-24,"Northeastern University,University of Rochester",Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-12-05 04:33,Robi Rahman,,,"United States of America,United States of America","Northeastern University, University of Rochester",,,,Residual Dense Network,,,
Chinese - English translation,Language,Translation,"H Hassan, A Aue, C Chen, V Chowdhary",SOTA Improvement,"""We find that our latest neural machine translation system has reached a new state-of-the-art, and that the translation quality is at human parity when compared to professional human translations""",,https://www.microsoft.com/en-us/research/publication/achieving-human-parity-on-automatic-chinese-to-english-news-translation/,543.00,Achieving Human Parity on Automatic Chinese to English News Translation,2018-03-01,Microsoft,Industry,,,,,,,,,,,,,,,,Self-supervised learning,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,Multinational,Microsoft,,,,Chinese - English translation,,,
LSTM (2018),Language,,"Shaojie Bai, J. Zico Kolter, Vladlen Koltun",Highly cited,,,https://arxiv.org/abs/1803.01271,4024.00,An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling,2018-03-04,"Intel Labs,Carnegie Mellon University (CMU)",,13000000.00,,,,Penn TreeBank,,,,,,,,,,,,,,,,,2023-11-26 03:25,Robi Rahman,LSTM (2018),,"Multinational,United States of America","Intel Labs, Carnegie Mellon University (CMU)",,,,LSTM (2018),,,
Rotation,Drawing,Image completion,"Spyros Gidaris, Praveer Singh, Nikos Komodakis",Highly cited,,,https://arxiv.org/abs/1803.07728,2668.00,Unsupervised Representation Learning by Predicting Image Rotations,2018-03-21,École des Ponts ParisTech,Academia,86000000.00,https://openai.com/blog/image-gpt/#rfref53,,,,,,,,,,,,,,,,,Academia,,,2023-12-05 04:33,Robi Rahman,,,France,École des Ponts ParisTech,,,,Rotation,,,
4 layer QRNN (h=2500),Language,,"Stephen Merity, Nitish Shirish Keskar, Richard Socher",SOTA Improvement,"""QRNNs achieve stateof-the-art results on character-level (Penn Treebank, enwik8) and word-level (WikiText-103)
datasets, respectively""",,https://arxiv.org/abs/1803.08240,183.00,An Analysis of Neural Language Modeling at Multiple Scales,2018-03-22,Salesforce Research,,26000000.00,,240000000000000000.00,,WikiText-103,,,,14.00,,,,,,,,,,,,,2023-11-26 03:25,Robi Rahman,4 layer QRNN (h=2500),,United States of America,Salesforce Research,,,,4 layer QRNN (h=2500),,,
"LSTM (Hebbian, Cache, MbPA)",Language,,"Jack W Rae, Chris Dyer, Peter Dayan, Timothy P Lillicrap",SOTA Improvement,"""We also show improved performance for word-based language models on news reports (GigaWord), books (Project Gutenberg) and Wikipedia articles (WikiText-103) --- the latter achieving a state-of-the-art perplexity of 29.2.""",,https://arxiv.org/abs/1803.10049,45.00,Fast Parametric Learning with Activation Memorization,2018-03-27,"DeepMind,University College London (UCL)",,45200000.00,,24000000000000000000.00,,WikiText-103,,,,90.00,,,,,,,,,,,,,2023-12-05 04:33,Robi Rahman,"""LSTM (Hebbian, Cache, MbPA)""",,"United Kingdom of Great Britain and Northern Ireland,United Kingdom of Great Britain and Northern Ireland","DeepMind, University College London (UCL)",,,,"""LSTM (Hebbian, Cache, MbPA)""",,,
YOLOv3,Vision,Object detection,"Joseph Redmon, Ali Farhadi",Highly cited,,,https://arxiv.org/abs/1804.02767,15299.00,YOLOv3: An Incremental Improvement,2018-04-08,University of Washington,Academia,56933216.00,"Feature extractor (ignoring biases)
32*3*3*3 +
64*3*3*32 +
32*1*1*64 +
64*3*3*32 +
128*3*3*64 +
2*(64*1*1*128 +
128*3*3*64) +
256*3*3*128 +
8*(128*1*1*256 +
256*3*3*128) +
512*3*3*256 + 
8*(256*1*1*512 + 
512*3*3*256) + 
1024*3*3*512 + 
4*(512*1*1*1024 +
1024*3*3*512) +
4*4*1024*1000

source: table 1
This is assuming the average pooling step changes the output size from 8x8 to 4x4.

The weights file is 237MB. If the weights are saved as float32, 4 bytes per weight, then there are approximately 237M/4=59M parameters, consistent with the calculation above.",50939199919999990000.00,"We use the formula training_compute = ops_per_forward_pass * 3.5 * n_epochs * n_examples

Assuming 160 epochs of training as in https://arxiv.org/pdf/1612.08242.pdf",ImageNet,,1281167,Source: https://image-net.org/download.php,,18700000000.00,"Table 2, Darknet-53. Note that the inference compute depends on the image resolution..",,,"NVIDIA M40,NVIDIA GTX Titan X","NVIDIA M40, NVIDIA GTX Titan X",,$295.76,,Academia,,"We present some updates to YOLO! We made a bunch of little design changes to make it better. We also trained this new network that's pretty swell. It's a little bigger than last time but more accurate. It's still fast though, don't worry. At 320x320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but three times faster. When we look at the old .5 IOU mAP detection metric YOLOv3 is quite good. It achieves 57.9 mAP@50 in 51 ms on a Titan X, compared to 57.5 mAP@50 in 198 ms by RetinaNet, similar performance but 3.8x faster. As always, all the code is online at this https URL",2023-12-05 04:33,Robi Rahman,,,United States of America,University of Washington,,,,YOLOv3,,,
TF-LM-discourse LSTM (WT2),Language,,"Lyan Verwimp, Hugo Van hamme, Patrick Wambacq",,,,https://aclanthology.org/L18-1470.pdf,10.00,TF-LM: TensorFlow-based Language Modeling Toolkit,2018-05-01,,,,,,,,,,,39.00,,,,,,,,,,,,,2023-11-21 01:39,Robi Rahman,TF-LM-discourse LSTM (WT2),,,,,,,,,,
TF-LM-discourse LSTM (PTB),Language,,"Lyan Verwimp, Hugo Van hamme, Patrick Wambacq",,,,https://aclanthology.org/L18-1470.pdf,10.00,TF-LM: TensorFlow-based Language Modeling Toolkit,2018-05-01,,,,,,,,,,,39.00,,,,,,,,,,,,,2023-11-21 01:39,Robi Rahman,TF-LM-discourse LSTM (PTB),,,,,,,,,,
ResNeXt-101 32x48d,Vision,Image classification,"Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin Bharambe, Laurens van der Maaten",SOTA Improvement,"""We show improvements on several image classification and object detection tasks, and report the highest ImageNet-1k single-crop, top-1 accuracy to date: 85.4%",,https://arxiv.org/abs/1805.00932,1176.00,Exploring the Limits of Weakly Supervised Pretraining,2018-05-02,Facebook,Industry,829000000.00,"Table 6
",8.74395e+21,"Table 6: 153e9 mult-adds.
Section 2.4: ""minibatches of 8,064 images"".

Compute = 2 * 3 * mult-adds * dataset size = 2 * 3 * 153e9 * 9525e6 = 8.74e21 FLOP","ImageNet, Instagram","Instagram images, captioned with hashtags",9525000000,Table 3: (300+1925+300+7000) million images,,31200000000.00,"Rados (FLOPs)
https://drive.google.com/drive/folders/1bhy5z6hh1n3wCHx6528Xb7xB1KhYdAL1",,"""Mahajan et al. (2018) required 19
GPU years to train their ResNeXt101-32x48d""
https://arxiv.org/abs/2103.00020",,,,,,Industry,Likely,,2023-12-05 04:33,Robi Rahman,,,Multinational,Facebook,,,,ResNeXt-101 32x48d,336,,ResNeXt-101 32x48d
Dropout-LSTM+Noise(Bernoulli) (WT2),Language,,"Adji B. Dieng, Rajesh Ranganath, Jaan Altosaar, David M. Blei",SOTA Improvement,"this is the best model in this paper per Table 4
""On language modeling benchmarks, Noisin improves over dropout by as much as 12.2% on the Penn Treebank and 9.4% on the Wikitext-2 dataset""",,https://arxiv.org/pdf/1805.01500,26.00,Noisin: Unbiased Regularization for Recurrent Neural Networks,2018-05-03,"Columbia University,New York University (NYU),Princeton University",,51000000.00,,127000000000000000.00,,,,,,200.00,,,,,,,,,,,,,2023-11-26 03:25,Robi Rahman,Dropout-LSTM+Noise(Bernoulli) (WT2),,"United States of America,United States of America,United States of America","Columbia University, New York University (NYU), Princeton University",,,,Dropout-LSTM+Noise(Bernoulli) (WT2),,,
LSTM+Noise(Beta),Language,,"Adji B. Dieng, Rajesh Ranganath, Jaan Altosaar, David M. Blei",,not the best model in this paper,,https://arxiv.org/pdf/1805.01500,26.00,Noisin: Unbiased Regularization for Recurrent Neural Networks,2018-05-03,,,51000000.00,,127000000000000000.00,,,,,,200.00,,,,,,,,,,,,,2023-11-23 06:53,Robi Rahman,LSTM+Noise(Beta),,,,,,,,,,
AWD-LSTM-MoS+Noisin+dynamic evaluation ,Language,,,,,,,,,2018-05-03,,,,,,,,,,,400.00,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,AWD-LSTM-MoS+Noisin+dynamic evaluation ,1,,,,,,,,,
Dropout-LSTM+Noise(Laplace),Language,,,,,,,,,2018-05-03,,,,,,,,,,,200.00,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,Dropout-LSTM+Noise(Laplace),1,,,,,,,,,
Dropout-LSTM+Noise(Bernoulli) (PTB),Language,,,,,,,,,2018-05-03,,,,,,,,,,,200.00,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,Dropout-LSTM+Noise(Bernoulli) (PTB),1,,,,,,,,,
aLSTM(depth-2)+RecurrentPolicy (WT2),Language,,"Sebastian Flennerhag, Hujun Yin, John Keane, Mark Elliot",SOTA Improvement,"""Without tuning for WT2, both outperform previously published results in 150 epochs (table 3) and converge to new state of the art performance in 190 epochs""",,https://arxiv.org/pdf/1805.08574,12.00,Breaking the Activation Function Bottleneck through Adaptive Parameterization,2018-05-22,"University of Manchester,Alan Turing Institute",,32000000.00,,75900000000000000.00,,,,,,190.00,,,,,,,,,,,,,2023-11-26 03:25,Robi Rahman,aLSTM(depth-2)+RecurrentPolicy (WT2),,"United Kingdom of Great Britain and Northern Ireland,United Kingdom of Great Britain and Northern Ireland","University of Manchester, Alan Turing Institute",,,,aLSTM(depth-2)+RecurrentPolicy (WT2),,,
aLSTM(depth-2)+RecurrentPolicy (PTB),Language,,,,,,,,,2018-05-22,,,,,,,,,,,180.00,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,aLSTM(depth-2)+RecurrentPolicy (PTB),1,,,,,,,,,
2-layer skip-LSTM + dropout tuning (WT2),Language,,"Gábor Melis, Charles Blundell, Tomáš Kočiský, Karl Moritz Hermann, Chris Dyer, Phil Blunsom",,,,https://arxiv.org/abs/1805.09208,14.00,Pushing the bounds of dropout,2018-05-23,,,5400000.00,,,,WikiText-2,,,,,,,,,,,,,,,,,2023-11-21 01:39,Robi Rahman,2-layer skip-LSTM + dropout tuning (WT2),,,,,,,,,,
RHN+HSG(depth=40),Language,,"Ron Shoham, Haim Permuter",,,,https://arxiv.org/pdf/1805.09238,0.00,Highway State Gating for Recurrent Highway Networks: improving information flow through time,2018-05-23,,Academia,,,,,,,,,300.00,,,,,,,,,,,,,2023-11-21 01:39,Robi Rahman,RHN+HSG(depth=40),,,,,,,,,,
RHN(depth=40),Language,,"Ron Shoham, Haim Permuter",,,,https://arxiv.org/pdf/1805.09238,0.00,Highway State Gating for Recurrent Highway Networks: improving information flow through time,2018-05-23,,Industry - Academia Collaboration,,,,,,,,,300.00,,,,,,,,,,,,,2023-11-21 01:39,Robi Rahman,RHN(depth=40),,,,,,,,,,
2-layer skip-LSTM + dropout tuning (PTB),Language,,,,,,,,,2018-05-23,,,,,,,,,,,,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,2-layer skip-LSTM + dropout tuning (PTB),1,,,,,,,,,
GPT,Language,,"A Radford, K Narasimhan, T Salimans, I Sutskever",Highly cited,,,https://openai.com/blog/language-unsupervised/,7000.00,Improving Language Understanding by Generative Pre-Training,2018-06-01,OpenAI,Industry,117000000.00,"""The model had 117M parameters in total.""

source: https://medium.com/walmartglobaltech/the-journey-of-open-ai-gpt-models-32d95b7b7fb2",17578125000000000000.00,"COMPUTE = FORWARD COMPUTE PER TOKEN * 3 BACKWARD FORWARD ADJUSTMENT * EPOCHS * DATASET SIZE

""We train for 100 epochs on minibatches of 64 randomly sampled, contiguous sequences of 512 tokens.""
",BooksCorpus,"""We use the BooksCorpus dataset [71] for training the language model""",1000000000,"""BookCorpus is a large collection of free novel books written by unpublished authors, which contains 11,038 books (around 74M sentences and 1G words) of 16 different sub-genres (e.g., Romance, Historical, Adventure, etc.).""
https://paperswithcode.com/dataset/bookcorpus

BookCorpus seems to have about 5000MB of content
source: https://huggingface.co/datasets/bookcorpusopen

Assuming a byte-pair encoder similar to GPT-2, there are 8 bytes / token.

So approximately 5000MB / 8 bytes / token = 5e9 / 8 tokens",,120000000000.00,"https://github.com/amirgholami/ai_and_memory_wall estimates 9.6e+10. The ELECTRA paper (https://arxiv.org/pdf/2003.10555.pdf) Table 1 reports 3.0E+10 FLOP for GPT. But: ""Infer FLOPs assumes a single length-128 input"". If we instead assume 512 tokens as per GPT's training process, then I think the calculation would be 4x larger, i.e. 1.2E+11. This is closer to the estimate of 9.6E+10 in the link.",,,,,Self-supervised learning,$68.72,,Industry,,"Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classification. Although large unlabeled text corpora are abundant, labeled data for learning these specific tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task. In contrast to previous approaches, we make use of task-aware input transformations during fine-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures specifically crafted for each task, significantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9% on commonsense reasoning (Stories Cloze Test), 5.7% on question answering (RACE), and 1.5% on textual entailment (MultiNLI).",2023-12-05 04:33,Robi Rahman,,,United States of America,OpenAI,,,,GPT,,,
Relational Memory Core,Language,,"Adam Santoro, Ryan Faulkner, David Raposo, Jack Rae, Mike Chrzanowski, Theophane Weber, Daan Wierstra, Oriol Vinyals, Razvan Pascanu, Timothy Lillicrap",SOTA Improvement,"""Finally, we test the RMC on a suite of tasks that may profit from more capable relational reasoning across sequential information, and show large gains in RL domains (e.g. Mini PacMan), program evaluation, and language modeling, achieving state-of-the-art results on the WikiText-103, Project Gutenberg, and GigaWord datasets.""",,https://arxiv.org/abs/1806.01822,235.00,Relational recurrent neural networks,2018-06-05,"DeepMind,University College London (UCL)",Industry - Academia Collaboration,,,,,WikiText-103,,,,,,,,,,,,,,,,,2023-11-26 03:25,Robi Rahman,Relational Memory Core,,"United Kingdom of Great Britain and Northern Ireland,United Kingdom of Great Britain and Northern Ireland","DeepMind, University College London (UCL)",,,,Relational Memory Core,,,
MobileNetV2,Vision,,"M Sandler, A Howard, M Zhu",Highly cited,,,https://ieeexplore.ieee.org/document/8578572,13328.00,MobileNetV2: Inverted Residuals and Linear Bottlenecks,2018-06-18,Google,Industry,3400000.00,Rados,,,,,,,,600000000.00,"Rados (FLOPs)
https://drive.google.com/drive/folders/1bhy5z6hh1n3wCHx6528Xb7xB1KhYdAL1",,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,Multinational,Google,,,,MobileNetV2,,,
DARTS,Language,,"Hanxiao Liu, Karen Simonyan, Yiming Yang",Highly cited,,,https://arxiv.org/abs/1806.09055,3990.00,DARTS: Differentiable Architecture Search,2018-06-24,"DeepMind,Carnegie Mellon University (CMU)",,33000000.00,,11000000000000000.00,,WikiText-2,,,,300.00,,,,,,,,,,,,,2023-11-26 03:25,Robi Rahman,DARTS,,"United Kingdom of Great Britain and Northern Ireland,United States of America","DeepMind, Carnegie Mellon University (CMU)",,,,DARTS,,,
DARTS (second order),Language,,,,,,,,,2018-06-24,,,,,,,,,,,300.00,,,,,,,,,,,Unverified,,2023-12-08 01:01,Robi Rahman,DARTS (second order),1,,,,,,,,,
S + I-Attention (3),Language,,"Artyom Gadetsky, Ilya Yakubovskiy, Dmitry Vetrov",,,,https://arxiv.org/abs/1806.10090,56.00,Conditional Generators of Words Definitions,2018-06-26,,Academia,,,,,Oxford Dictionary,,,,35.00,,,,,,,,,,,,,2023-11-21 01:39,Robi Rahman,S + I-Attention (3),,,,,,,,,,
ShuffleNet v2,Vision,,"N Ma, X Zhang, HT Zheng",Highly cited,,,https://arxiv.org/abs/1807.11164,3464.00,ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design,2018-06-30,"Tsinghua University,Megvii Inc",Industry - Academia Collaboration,2280000.00,,,,,,,,,300000000.00,"Rados (FLOPs)
https://drive.google.com/drive/folders/1bhy5z6hh1n3wCHx6528Xb7xB1KhYdAL1",,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,"China,China","Tsinghua University, Megvii Inc",,,,ShuffleNet v2,,,
FTW,Games,Capture the flag,"Max Jaderberg, Wojciech M. Czarnecki, Iain Dunning, Luke Marris, Guy Lever, Antonio Garcia Castaneda, Charles Beattie, Neil C. Rabinowitz, Ari S. Morcos, Avraham Ruderman, Nicolas Sonnerat, Tim Green, Louise Deason, Joel Z. Leibo, David Silver, Demis Hassabis, Koray Kavukcuoglu, Thore Graepel",SOTA Improvement,"""In this work, we demonstrate for the first time that an agent can achieve human-level in a popular 3D multiplayer first-person video game, Quake III Arena Capture the Flag (28), using only pixels and game points as input.""",,https://arxiv.org/abs/1807.01281,571.00,Human-level performance in first-person multiplayer games with population-based deep reinforcement learning,2018-07-03,DeepMind,Industry,126001330.00,"Architecture described in figure S11 of the supplement

The architecture includes modules for visual embedding, reward prediction, recurrent processing, policy, baseline and pixel control.

Input is 84x84x3 pixels as seen in figure S10 of the supplement

""We elected to use a resolution of 84x84 pixels as in previous related work in this environment. Each pixel is represented by a triple of three bytes""

Visual embedding (84x84x3 -> 256)
32*(8*8*3+1)+64*(4*4*32+1)+64*(3*3*64+1)+64*(3*3*64+1) + (84/(S^4)*84/(S^4)*64+1)*256
Note there is no information about the stride S used in the convolutions; we assume S = 1

Reward prediction (256 -> 3)
(256+1)*128 + (128+1)*3

Recurrent processing (n-> 512)
VU1 (256 -> 512)
4*(799+2*32)*((512+(32*2) + 3*32 + 5*2 + 3)+(799+2*32)+1) + 2*(256+1)*256

VU2 (512 -> 512)
4*(512+2*32)*((512+(32*2) + 3*32 + 5*2 + 3)+(512+2*32)+1) + 2*(256+1)*256

LSTMs usually have 4*(n*m+n*n+n) parameters, where n=input size and m=output size.

This DNS + LSTM takes as input the concatenation of the previous layer of size n and R read vectors of size W=32; and outputs m units plus an interface vector of size (W*R) + 3*W + 5*R + 3, for a total of about 4*(n+R*W)*((m+(W*R) + 3*W + 5*R + 3)+(n+R*32)+1) parameters

I assume R=2 since that seems implied by the previous paper (?)

The first VU has as input the visual embedding (size 256), the previous action (size 540) and the previous reward (size 3), for a total size of 256+540+3 = 799. The output is size 512.

The second VU has input size 512 and output size 512

The DNC memory architecture is described in https://www.nature.com/articles/nature20101.epdf

Policy (512 -> 5x3x3x3x2x2)
6*(512+1)*256 + (256+1)*5 + 3*(256+1)*3 + 2*(256+1)*2

Baseline
(512+1)*256 + (256+1)*1

Pixel control
(512+1)*32*7*7 + 32*(9*9+1) + 5*(4*4+1) + 3*2*(4*4+1) + 2*2*(4*4+1) + 1*(4*4+1)
""we trained independent pixel control policies for each of the six action groups""",7.26e+21,"We assume that most operations happen in the visual embedding.

2* 84^2*84^2 * 32 * 3 / 1^2 = 9.5 *10^9
new image size: 76 x 76 x 32
ignore ReLU/additions becaue probably very little influence 
2 * 76^2 * 76^2 * 10* 64 = 4 *10^10
new image size: 72 x 72 x 64
2 * 72^2 *72^2 * 64 * 64 * 3=  6.6 * 10^11
new image size: 69 x 69 x 64
2 * 69^2 *69^2 * 64 * 64 * 3=  5.5 * 10^11
new image size: 66 x 66 x 64
Linear layer: 2* ( 66*66*64)*256 = 1.4*10^8
Total aprox: 1.21e+12 FLOP/forward pass

",,,,,,1210000000000.00,,,,,,Self-supervised learning,$21045.02,,Industry,,,2023-12-05 10:58,Robi Rahman,,,United Kingdom of Great Britain and Northern Ireland,DeepMind,,,,FTW,,,FTW
Population-based DRL,Games,Capture the flag,"Max Jaderberg, Wojciech M. Czarnecki, Iain Dunning, Luke Marris, Guy Lever, Antonio Garcia Castaneda, Charles Beattie, Neil C. Rabinowitz, Ari S. Morcos, Avraham Ruderman, Nicolas Sonnerat, Tim Green, Louise Deason, Joel Z. Leibo, David Silver, Demis Hassabis, Koray Kavukcuoglu, Thore Graepel",SOTA Improvement,"Qualitatively clearly SOTA: ""In this work, we demonstrate for the first time that an agent can achieve human-level in a popular 3D multiplayer first-person video game, Quake III Arena Capture the Flag (28), using only pixels and game points as input... proved far stronger than existing state-of-the-art agents""",,https://arxiv.org/abs/1807.01281,571.00,Human-level performance in first-person multiplayer games with population-based deep reinforcement learning,2018-07-03,DeepMind,Industry,122000000.00,"Calculated from the architecture schematic in Figure S11 on pg 55 of the Capture the Flag supplementary materials. This is dominated by the size of the vision module, which is 116 million parameters, followed by the temporal processors which is 4.3 million parameters. The RL policy itself is only 0.79 million parameters. Also, I'm pretty uncertain if I'm right about how I calculated these parameters.

Source: 
https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389",34900000000000000000.00,"Source: 
https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389",,,,,,60000000000.00,"""Agents were trained for two billion steps, corresponding to approximately 450K
games.""

""We train a
population of 30 different agents together, which provides a diverse set of teammates and opponents to
play with, and is also used to evolve the internal rewards and hyperparameters of agents and learning
process""

30 * 2e9 = 6e10",,,,,Self-supervised learning,$130.36,,Industry,,"Recent progress in artificial intelligence through reinforcement learning (RL) has shown great success on increasingly complex single-agent environments and two-player turn-based games. However, the real-world contains multiple agents, each learning and acting independently to cooperate and compete with other agents, and environments reflecting this degree of complexity remain an open challenge. In this work, we demonstrate for the first time that an agent can achieve human-level in a popular 3D multiplayer first-person video game, Quake III Arena Capture the Flag, using only pixels and game points as input. These results were achieved by a novel two-tier optimisation process in which a population of independent RL agents are trained concurrently from thousands of parallel matches with agents playing in teams together and against each other on randomly generated environments. Each agent in the population learns its own internal reward signal to complement the sparse delayed reward from winning, and selects actions using a novel temporally hierarchical representation that enables the agent to reason at multiple timescales. During game-play, these agents display human-like behaviours such as navigating, following, and defending based on a rich learned representation that is shown to encode high-level game knowledge. In an extensive tournament-style evaluation the trained agents exceeded the win-rate of strong human players both as teammates and opponents, and proved far stronger than existing state-of-the-art agents. These results demonstrate a significant jump in the capabilities of artificial agents, bringing us closer to the goal of human-level intelligence.",2023-12-05 04:33,Robi Rahman,,,United Kingdom of Great Britain and Northern Ireland,DeepMind,,,,Population-based DRL,,,
RCAN,Vision,Image super-resolution," Yulun Zhang, Kunpeng Li, Kai Li, Lichen Wang, Bineng Zhong, Yun Fu",Highly cited,,,https://openaccess.thecvf.com/content_ECCV_2018/html/Yulun_Zhang_Image_Super-Resolution_Using_ECCV_2018_paper.html,3167.00,Image Super-Resolution Using Very Deep Residual Channel Attention Networks,2018-07-08,Northeastern University,Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-12-05 04:33,Robi Rahman,,,United States of America,Northeastern University,,,,RCAN,,,
Big-Little Net,Vision,Image classification,"Chun-Fu Chen, Quanfu Fan, Neil Mallinar, Tom Sercu, and Rogerio Feris",SOTA Improvement,"""On object recognition task, we demonstrated that our approach provides approximately 2× speedup over baselines while
improving accuracy, and the result significantly outperforms the state-of-the-art networks by a large
margin in terms of accuracy and FLOPs reduction""",,https://arxiv.org/pdf/1807.03848.pdf,75.00,Big-Little Net: An Efficient Multi-Scale Feature Representation for Visual and Speech Recognition,2018-07-10,IBM,Industry,77360000.00,,,,ImageNet,,1280000,,110.00,582500000.00,9320000000 FLOPs per batch of 16 images,,,NVIDIA Tesla K80,NVIDIA Tesla K80,,,,,Unverified,"In this paper, we propose a novel Convolutional Neural Network (CNN) architecture for learning multi-scale feature representations with good tradeoffs between speed and accuracy. This is achieved by using a multi-branch network, which has different computational complexity at different branches. Through frequent merging of features from branches at distinct scales, our model obtains multi-scale features while using less computation. The proposed approach demonstrates improvement of model efficiency and performance on both object recognition and speech recognition tasks,using popular architectures including ResNet and ResNeXt. For object recognition, our approach reduces computation by 33% on object recognition while improving accuracy with 0.9%. Furthermore, our model surpasses state-of-the-art CNN acceleration approaches by a large margin in accuracy and FLOPs reduction. On the task of speech recognition, our proposed multi-scale CNNs save 30% FLOPs with slightly better word error rates, showing good generalization across domains.",2023-12-05 04:33,Anonymous,,,Multinational,IBM,,,,Big-Little Net,,,
RGC+ASQ (WT2),Language,,"Jiarui Fang, Haohuan Fu, Guangwen Yang, Cho-Jui Hsieh",,,,https://arxiv.org/pdf/1808.04357,28.00,RedSync : Reducing Synchronization Traffic for Distributed Deep Learning,2018-08-13,,Industry,209000000.00,,,,,,,,,,,,,,,,,,,,,2023-11-21 01:39,Robi Rahman,RGC+ASQ (WT2),,,,,,,,,,
RGC+ASQ (PTB),Language,,,,,,,,,2018-08-13,,,,,,,,,,,40.00,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,RGC+ASQ (PTB),1,,,,,,,,,
AWD-LSTM-MoS+PDR + dynamic evaluation (WT2),Language,,Siddhartha Brahma,SOTA Improvement,"""our Past Decode Regularization (PDR) method achieves a word level perplexity of 55.6 on the Penn Treebank and 63.5 on the WikiText-2 datasets using a single softmax. We also show gains by using PDR in combination with a mixture-of-softmaxes, achieving a word level perplexity of 53.8 and 60.5 on these datasets. In addition, our method achieves 1.169 bits-per-character on the Penn Treebank Character dataset for character level language modeling. These results constitute a new state-of-the-art in their respective settings.""",,https://arxiv.org/abs/1808.05908,5.00,Improved Language Modeling by Decoding the Past,2018-08-14,IBM,,35000000.00,,,,WikiText-2,,,,,,,,,,,,,,,,,2023-11-26 03:25,Robi Rahman,AWD-LSTM-MoS+PDR + dynamic evaluation (WT2),,Multinational,IBM,,,,AWD-LSTM-MoS+PDR + dynamic evaluation (WT2),,,
AWD-LSTM-MoS+PDR + dynamic evaluation (PTB),Language,,,,,,,,,2018-08-14,,,,,,,,,,,1200.00,,,,,,,,,,,Unverified,,2023-12-08 01:01,Robi Rahman,AWD-LSTM-MoS+PDR + dynamic evaluation (PTB),1,,,,,,,,,
(ensemble): AWD-LSTM-DOC (fin) × 5 (WT2),Language,,"Sho Takase, Jun Suzuki, Masaaki Nagata",SOTA Improvement,"""The proposed method improves the current state-of-the-art language model and achieves the best score on the Penn Treebank and WikiText-2, which are the standard benchmark datasets""",,https://arxiv.org/abs/1808.10143,36.00,Direct Output Connection for a High-Rank Language Model,2018-08-30,"NTT Communication Science Laboratories,Tohoku University",,185000000.00,,693000000000000000.00,,WikiText-2,,,,300.00,,,,,,,,,,,,,2023-11-26 03:25,Robi Rahman,(ensemble): AWD-LSTM-DOC (fin) × 5 (WT2),,"Japan,Japan","NTT Communication Science Laboratories, Tohoku University",,,,(ensemble): AWD-LSTM-DOC (fin) × 5 (WT2),,,
(ensemble): AWD-LSTM-DOC (fin) × 5 (PTB),Language,,,,,,,,,2018-08-30,,,,,,,,,,,300.00,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,(ensemble): AWD-LSTM-DOC (fin) × 5 (PTB),1,,,,,,,,,
AWD-LSTM-DOC (fin) (37M),Language,,,,,,,,,2018-08-30,,,,,,,,,,,300.00,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,AWD-LSTM-DOC (fin) (37M),1,,,,,,,,,
AWD-LSTM-DOC (fin) (23M),Language,,,,,,,,,2018-08-30,,,,,,,,,,,300.00,,,,,,,,,,,Unverified,,2023-12-08 01:01,Robi Rahman,AWD-LSTM-DOC (fin) (23M),1,,,,,,,,,
ESRGAN,Vision,Image super-resolution,"Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Chen Change Loy, Yu Qiao, Xiaoou Tang",Highly cited,,,https://arxiv.org/abs/1809.00219,2593.00,ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks,2018-09-01,"Chinese University of Hong Kong,Chinese Academy of Sciences,Nanyang Technological University",Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-12-05 04:33,Robi Rahman,,,"Hong Kong,China,Singapore","Chinese University of Hong Kong, Chinese Academy of Sciences, Nanyang Technological University",,,,ESRGAN,,,
"AWD-LSTM-MoS + dynamic evaluation (WT2, 2018)",Language,,"Chengyue Gong, Di He, Xu Tan, Tao Qin, Liwei Wang, Tie-Yan Liu",SOTA Improvement,"""Specifically, in language modeling and machine translation, we achieve better performance than the state-of-the-art results on PTB, WT2
and WMT14 English-German datasets.""",,https://arxiv.org/abs/1809.06858,152.00,FRAGE: Frequency-Agnostic Word Representation,2018-09-18,"Peking University,Microsoft Research Asia",,35000000.00,,,,WikiText-2,,,,,,,,,,,,,,,,,2023-11-26 03:25,Robi Rahman,"""AWD-LSTM-MoS + dynamic evaluation (WT2, 2018)""",,"China,China","Peking University, Microsoft Research Asia",,,,"""AWD-LSTM-MoS + dynamic evaluation (WT2, 2018)""",,,
"AWD-LSTM-MoS + dynamic evaluation (PTB, 2018)",Language,,,,,,,,,2018-09-18,,,,,,,,,,,,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,"""AWD-LSTM-MoS + dynamic evaluation (PTB, 2018)""",1,,,,,,,,,
LSTM+NeuralCache,Language,,"Lyan Verwimp, Joris Pelemans, Hugo Van hamme, Patrick Wambacq",SOTA Improvement,"""We obtain a 29.9%/32.1% (validation/test set) relative improvement in perplexity with respect to a baseline LSTM LM on the WikiText-2 dataset, outperforming previous work on neural cache LMs"" 
... 

""we observe that neural cache models
consistently outperform regular cache models on this dataset.""",,https://arxiv.org/pdf/1809.08826,3.00,Information-Weighted Neural Cache Language Models for ASR,2018-09-24,"KU Leuven,ESAT - PSI,Apple",,2100000.00,,1020000000000000.00,,,,,,39.00,,,,,,,,,,,,,2023-11-26 03:25,Robi Rahman,LSTM+NeuralCache,,"Belgium,Belgium,United States of America","KU Leuven, ESAT - PSI, Apple",,,,LSTM+NeuralCache,,,
BigGAN-deep 512x512,Drawing,Image generation,"A Brock, J Donahue, K Simonyan",Highly cited,,Permissive license,https://arxiv.org/abs/1809.11096,4163.00,Large Scale GAN Training for High Fidelity Natural Image Synthesis,2018-09-28,"Heriot-Watt University,DeepMind",Industry - Academia Collaboration,112694781.00,"I used the publicly available implementation available at [1]

There I loaded the biggan-deep512/1 model, and ran script [2] to compute the number of parameters

[1] https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/biggan_generation_with_tf_hub.ipynb

[2]
n_params = 0
for var in module.variables:
  n_params += np.prod(var.shape.as_list())
  pass

print(n_params)",3.00000000001e+21,"3e21, estimate taken from:

https://www.lesswrong.com/posts/wfpdejMWog4vEDLDg/ai-and-compute-trend-isn-t-predictive-of-what-is-happening",JFT-300M,,292000000,"""To confirm that our design choices are effective for even larger and more complex and diverse datasets, we also present results of our system on a subset of JFT-300M (Sun et al., 2017). The full JFT-300M dataset contains 300M real-world images labeled with 18K categories. Since the category distribution is heavily long-tailed, we subsample the dataset to keep only images with the 8.5K most common labels. The resulting dataset contains 292M images – two orders of magnitude larger than ImageNet. """,,,,48.0,"""We train on a Google TPU v3 Pod, with the number of cores proportional to the resolution: 128 for 128×128, 256 for 256×256, and 512 for 512×512. Training takes between 24 and 48 hours for most models""",Google TPU v3,Google TPU v3,,$10448.44,,Industry,Likely,"Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as ImageNet remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities specific to such scale. We find that applying orthogonal regularization to the generator renders it amenable to a simple ""truncation trick,"" allowing fine control over the trade-off between sample fidelity and variety by reducing the variance of the Generator's input. Our modifications lead to models which set the new state of the art in class-conditional image synthesis. When trained on ImageNet at 128x128 resolution, our models (BigGANs) achieve an Inception Score (IS) of 166.5 and Frechet Inception Distance (FID) of 7.4, improving over the previous best IS of 52.52 and FID of 18.6.",2023-12-05 04:33,Robi Rahman,,0,"United Kingdom of Great Britain and Northern Ireland,United Kingdom of Great Britain and Northern Ireland","Heriot-Watt University, DeepMind",,,,BigGAN-deep 512x512,256,,BigGAN-deep 512x512
Transformer (Adaptive Input Embeddings),Language,,"Alexei Baevski, Michael Auli",SOTA Improvement,"""On the WikiText-103 benchmark we achieve 18.7 perplexity, an improvement of 10.5 perplexity compared to the previously best published result""",,https://arxiv.org/abs/1809.10853,337.00,Adaptive Input Representations for Neural Language Modeling,2018-09-28,Facebook AI Research,,247000000.00,,7300000000000000000.00,,WikiText-103,,,,180.00,,,,,,,,,,,,,2023-11-26 03:25,Robi Rahman,Transformer (Adaptive Input Embeddings),,Multinational,Facebook AI Research,,,,Transformer (Adaptive Input Embeddings),,,
BERT-Large,Language,Next sentence prediction,"J Devlin, MW Chang, K Lee, K Toutanova",Highly cited,,Fully open-source,https://arxiv.org/abs/1810.04805,64593.00,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,Google,Industry,340000000.00,,285000000000000000000.00,more info here https://docs.google.com/document/d/1B8x6XYcmB1u6Tmq3VcbAtj5bzhDaj2TcIPyK6Wpupx4/edit?usp=sharing,,,3300000000,"""For the pre-training corpus we
use the BooksCorpus (800M words) (Zhu et al., 2015) and English Wikipedia (2,500M words)""",,79000000000.00,"Rados (FLOPs)
https://drive.google.com/drive/folders/1bhy5z6hh1n3wCHx6528Xb7xB1KhYdAL1",,,,,Self-supervised learning,$999.93,,Industry,,"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",2023-12-05 04:33,Robi Rahman,,,Multinational,Google,,,,BERT-Large,,,
MetaMimic,Games,,"Tom Le Paine, Sergio Gomez",SOTA Improvement,"""By retaining and taking advantage of all its experiences,
MetaMimic also substantially outperforms the state-of-the-art D4PG RL agent, when D4PG
uses only the current task experiences.""",,https://arxiv.org/abs/1810.05017,24.00,One-Shot High-Fidelity Imitation: Training Large-Scale Deep Nets with RL,2018-10-11,Google,Industry,22000000.00,"""This representational demand motivates the introduction of high-capacity deep neural networks. We found the architecture, shown in Figure 3, with residual connections, 20 convolution layers with 512 channels
for a total of 22 million parameters, and instance normalization to drastically improve performance, as shown in Figure 6 of the Experiments section.""",,,,,,,,,,,,,,Reinforcement learning,,,Industry,,,2023-11-26 03:25,Robi Rahman,,,Multinational,Google,,,,MetaMimic,,,
TrellisNet,Language,,"Shaojie Bai, J. Zico Kolter, Vladlen Koltun",SOTA Improvement,"""Experiments demonstrate that trellis networks outperform the current state of the art methods on a variety of challenging benchmarks, including word-level language modeling and character-level language modeling
tasks""",,https://arxiv.org/abs/1810.06682,132.00,Trellis Networks for Sequence Modeling,2018-10-15,"Carnegie Mellon University (CMU),Bosch Center for Artificial Intelligence,Intel Labs",,180000000.00,,2780000000000000000.00,,WikiText-103,,,,25.00,,,,,,,,,,,,,2023-11-26 03:25,Robi Rahman,TrellisNet,,"United States of America,Germany,Multinational","Carnegie Mellon University (CMU), Bosch Center for Artificial Intelligence, Intel Labs",,,,TrellisNet,,,
TrellisNet-MoS (1.4x larger),Language,,,,,,,,,2018-10-15,,,,,,,,,,,25.00,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,TrellisNet-MoS (1.4x larger),1,,,,,,,,,
Fine-tuned-AWD-LSTM-DOC(fin),Language,,"Vadim Popov, Mikhail Kudinov",SOTA Improvement,"""The novel approach that we propose allows us to reach state-of-theart quality on Penn Treebank: perplexity decreases from 52.4 to 52.1.""",,https://arxiv.org/pdf/1811.04623,2.00,Fine-tuning of Language Models with Discriminator,2018-11-12,Samsung R&D Institute Russia,,23000000.00,,1920000000000000.00,,,,,,15.00,,,,,,,,,,,,,2023-11-26 03:25,Robi Rahman,Fine-tuned-AWD-LSTM-DOC(fin),,Russia,Samsung R&D Institute Russia,,,,Fine-tuned-AWD-LSTM-DOC(fin),,,
Multi-cell LSTM,Language,,"Thomas Cherian, Akshay Badola, Vineet Padmanabhan",SOTA Improvement,"""The proposed multi-cell LSTM language models outperform the state-of-the-art results on well-known Penn Treebank (PTB) setup""",,https://arxiv.org/pdf/1811.06477,6.00,Multi-cell LSTM Based Neural Language Model,2018-11-15,University of Hyderabad,,7200000.00,,2009999999999999.75,,,,,,50.00,,,,,,,,,,,,,2023-11-26 03:25,Robi Rahman,Multi-cell LSTM,,India,University of Hyderabad,,,,Multi-cell LSTM,,,
GPipe (Transformer),Language,Translation,"Y Huang, Y Cheng, A Bapna, O Firat","SOTA Improvement,Highly cited","""We train a single 6-billion-parameter,
128-layer Transformer model on a corpus spanning over 100 languages and achieve better quality than all bilingual models.""",,https://arxiv.org/abs/1811.06965,1218.00,GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism,2018-11-16,Google,Industry,6000000000.00,Section 5: ,,,,,20000000000,"[WORDS]

Section 5: ""We use a
corpus of parallel documents over 102 languages and English, containing a total of 25 billion training examples, ranging from 10^4 to 10^9 per language""

10^9 sentences * 20 words per sentence",,,,,,,,Self-supervised learning,,,Industry,,,2023-11-26 03:25,Robi Rahman,,,Multinational,Google,,,,GPipe (Transformer),,,
GPipe (Amoeba),Vision,Image classification,"Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Mia Xu Chen, Dehao Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V. Le, Yonghui Wu, Zhifeng Chen",Highly cited,,,https://arxiv.org/abs/1811.06965,1218.00,GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism,2018-11-16,Google,Industry,557000000.00,Section 4,,,ImageNet,,1281167,Table 4,,,,,,,,,,,Industry,,,2023-11-26 03:25,Robi Rahman,,,Multinational,Google,,,,GPipe (Amoeba),,,
Transformer ELMo,Language,Language modelling,"ME Peters, M Neumann, L Zettlemoyer, W Yih",SOTA Improvement,"""Our model is the Reconciled Span Parser (RSP; Joshi et al., 2018), which, using ELMo representations, achieved state of the art performance for this
task. As shown in Table 2, the LSTM based models demonstrate the best performance with a 0.2% and 1.0% improvement over the Transformer and CNN models, respectively""",,https://www.semanticscholar.org/paper/Dissecting-Contextual-Word-Embeddings%3A-Architecture-Peters-Neumann/ac11062f1f368d97f4c826c317bf50dcc13fdb59,340.00,Dissecting Contextual Word Embeddings: Architecture and Representation,2019-01-01,"Allen Institute for AI,University of Washington",Industry - Academia Collaboration (Industry leaning),56000000.00,,,,,More info on this is extractable with some time,,,,,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,"United States of America,United States of America","Allen Institute for AI, University of Washington",,,,Transformer ELMo,,,
Transformer + Average Attention Network,Language,,"Jian Guo Zhang, Jian Ping Li, Huang Li",,,,https://ieeexplore.ieee.org/abstract/document/9067534,126.00,Language Modeling with Transformer,2019-01-01,,Industry,,,,,WikiText-103,,,,,,,,,,,,,,,,,2023-11-21 01:39,Robi Rahman,Transformer + Average Attention Network,,,,,,,,,,
Decoupled weight decay regularization,Vision,Image classification,Ilya Loshchilov and Frank Hutter,Highly cited,,,https://arxiv.org/abs/1711.05101,2061.00,Decoupled weight decay regularization.,2019-01-04,University of Freiburg,Academia,36500000.00,"From author communication

WideResNet 28-10 models with 36.5 million parameters (3.65E+07)",2470000000000000000.00,"From author communication

Per image: 5.24 billion FLOPs (5.24E+09)  Per training run: 50k times 5.24E+09 times 1800 epochs = 2.47E+18 FLOPs",CIFAR-10,,50000,,,1730000000.00,"From author communication

Best estimate: 1.73E+09",,,,,,$8.07,,Academia,,,2023-11-26 03:25,Robi Rahman,,,Germany,University of Freiburg,,,,Decoupled weight decay regularization,,,
Transformer-XL Large,Language,,"Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov",Highly cited,,,https://arxiv.org/abs/1901.02860,3155.00,Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context,2019-01-09,"Carnegie Mellon University (CMU),Google Brain",,257000000.00,,10900000000000000000.00,,WikiText-103,,,,,,,,,,,,,,,,,2023-11-26 03:25,Robi Rahman,Transformer-XL Large,,"United States of America,Multinational","Carnegie Mellon University (CMU), Google Brain",,,,Transformer-XL Large,,,
Transformer-XL-ptb,Language,,,,,,,,,2019-01-09,,,,,,,,,,,,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,Transformer-XL-ptb,1,,,,,,,,,
H-LSTM+wg+rcp+rcg+wp,Language,,"Hongxu Yin, Guoyang Chen, Yingmin Li, Shuai Che, Weifeng Zhang, Niraj K. Jha",,,,https://arxiv.org/pdf/1901.10997,10.00,"Hardware-Guided Symbiotic Training for Compact, Accurate, yet Execution-Efficient LSTM",2019-01-30,,Academia,800000.00,,,,,,,,,,,,,,,,,,,,,2023-11-21 01:39,Robi Rahman,H-LSTM+wg+rcp+rcg+wp,,,,,,,,,,
MT-DNN,Language,,"X Liu, P He, W Chen, J Gao","Highly cited,SOTA Improvement","""MT-DNN extends the model proposed in Liu et al. (2015) by incorporating a pre-trained bidirectional transformer language model, known as BERT (Devlin et al., 2018). MT-DNN obtains new state-of-the-art results on ten NLU tasks, including SNLI, SciTail, and eight out of nine GLUE tasks, pushing the GLUE benchmark to 82.7% (2.2% absolute improvement)""",,https://arxiv.org/abs/1901.11504,1217.00,Multi-Task Deep Neural Networks for Natural Language Understanding,2019-01-31,Microsoft,Industry,330000000.00,,,,,,,,,,,,,,,,,,Industry,,,2023-11-26 03:25,Robi Rahman,,,Multinational,Microsoft,,,,MT-DNN,,,
TSLM+MoS (WT2),Language,,"Lipeng Zhang, Peng Zhang, Xindian Ma, Shuqin Gu, Zhan Su, Dawei Song",,,,https://arxiv.org/pdf/1901.11167,21.00,A Generalized Language Model in Tensor Space,2019-01-31,,,9120000.00,,,,,,,,,,,,,,,,,,,,,2023-11-21 01:39,Robi Rahman,TSLM+MoS (WT2),,,,,,,,,,
TSLM+MoS (PTB),Language,,,,,,,,,2019-01-31,,,,,,,,,,,,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,TSLM+MoS (PTB),1,,,,,,,,,
Hanabi 4 player,Games,Hanabi,,Historical significance,Adapted some SOTA RL algorithms to a new task that posed research challenges,,https://arxiv.org/abs/1902.00506,229.00,The Hanabi Challenge: A New Frontier for AI Research,2019-02-01,"DeepMind,University of Oxford,Carnegie Mellon University (CMU),Google Brain",Industry - Academia Collaboration (Industry leaning),764000.00,source: https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389,4300000000000000000.00,14.13e+12 FLOP/s * 7 days * 86400 s/day * 0.50 utilization = 4.3e+18 FLOP,,,,,,,,,,,,,$0.34,,Industry,,,2023-11-26 03:25,Robi Rahman,,,"United Kingdom of Great Britain and Northern Ireland,United Kingdom of Great Britain and Northern Ireland,United States of America,Multinational","DeepMind, University of Oxford, Carnegie Mellon University (CMU), Google Brain",,,,Hanabi 4 player,,,
Compress-LSTM (66M),Language,,"Artem M. Grachev, Dmitry I. Ignatov, Andrey V. Savchenko",,,,"https://arxiv.org/abs/1902.02380#:~:text=Compression%20of%20Recurrent%20Neural%20Networks%20for%20Efficient%20Language%20Modeling,-Artem%20M.&text=Recurrent%20neural%20networks%20have%20proved,real%2Dtime%20offline%20mobile%20applications.",37.00,Compression of Recurrent Neural Networks for Efficient Language Modeling,2019-02-06,,Academia,66000000.00,,33100000000000000.00,,,,,,90.00,,,,,,,,,,,,,2023-11-23 06:53,Robi Rahman,Compress-LSTM (66M),,,,,,,,,,
Compress-LSTM (4.6M),Language,,,,,,,,,2019-02-06,,,,,,,,,,,90.00,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,Compress-LSTM (4.6M),1,,,,,,,,,
GPT-2 (1542M),Language,,"A Radford, J Wu, R Child, D Luan, D Amodei",Highly cited,,,https://openai.com/blog/better-language-models/,13164.00,Language Models are Unsupervised Multitask Learners,2019-02-14,OpenAI,Industry,1500000000.00,"""GPT-2 is a large transformer-based language model with 1.5 billion parameters""",1.494140625e+21,"We use COMPUTE = FORWARD COMPUTE PER TOKEN * 3 BACKWARD FORWARD ADJUSTMENT* N EPOCHS * N TOKENS IN TRAINING DATASET

The number of epochs is not reported, but this other paper [1] claims in table 1 that it is 20 or 100 epochs. 100 epochs is consistent with the original GPT paper.

[1] https://arxiv.org/abs/1906.06669",,,3000000000,"“All results presented in this paper use a preliminary version of WebText which does not include links created after Dec 2017 and which after de-duplication and some heuristic based cleaning contains slightly over 8 million documents for a total of 40 GB of text.”
40GB is approximately 3e9 words.
",20.00,3400000000000.00,"Rados (FLOPs)
https://drive.google.com/drive/folders/1bhy5z6hh1n3wCHx6528Xb7xB1KhYdAL1",,,,,Self-supervised learning,$4692.89,"https://en.wikipedia.org/wiki/GPT-2#:~:text=The%20cloud%20compute%20costs%20for,full%201.5%20billion%20parameter%20model).",Industry,,"Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.",2023-12-05 04:33,Robi Rahman,GPT-2 (1542M),,United States of America,OpenAI,,,,GPT-2 (1542M),,,
GPT-2 (117M),Language,,,,,,,,,2019-02-14,,,,,,,,,,,100.00,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,GPT-2 (117M),1,,,,,,,,,
GPT-2 (762M),Language,,,,,,,,,2019-02-14,,,,,,,,,,,100.00,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,GPT-2 (762M),1,,,,,,,,,
GPT-2 (345M),Language,,,,,,,,,2019-02-14,,,,,,,,,,,100.00,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,GPT-2 (345M),1,,,,,,,,,
ProxylessNAS,Vision,,"Han Cai, Ligeng Zhu, and Song Han",Highly cited,,,https://arxiv.org/abs/1812.00332,1806.00,ProxylessNAS: Direct neural architecture search on target task and hardware,2019-02-23,Massachusetts Institute of Technology (MIT),Academia,,,37065600000000000000.00,"For their searched Imagenet models, they used 200 GPU hours on a V100 GPU.

At FP32, a V100 GPU has a peak performance of 1.56E+14 FLOPS.

Utilization rate of 0.33.",ImageNet,,1280000,,,262548000000.00,"5.1 Miliseconds on a V100 GPU
",,,,,,$135.04,,Academia,,"Neural architecture search (NAS) has a great impact by automatically designing effective neural network architectures. However, the prohibitive computational demand of conventional NAS algorithms (e.g. 104 GPU hours) makes it difficult to \emph{directly} search the architectures on large-scale tasks (e.g. ImageNet). Differentiable NAS can reduce the cost of GPU hours via a continuous representation of network architecture but suffers from the high GPU memory consumption issue (grow linearly w.r.t. candidate set size). As a result, they need to utilize~\emph{proxy} tasks, such as training on a smaller dataset, or learning with only a few blocks, or training just for a few epochs. These architectures optimized on proxy tasks are not guaranteed to be optimal on the target task. In this paper, we present \emph{ProxylessNAS} that can \emph{directly} learn the architectures for large-scale target tasks and target hardware platforms. We address the high memory consumption issue of differentiable NAS and reduce the computational cost (GPU hours and GPU memory) to the same level of regular training while still allowing a large candidate set. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of directness and specialization. On CIFAR-10, our model achieves 2.08\% test error with only 5.7M parameters, better than the previous state-of-the-art architecture AmoebaNet-B, while using 6× fewer parameters. On ImageNet, our model achieves 3.1\% better top-1 accuracy than MobileNetV2, while being 1.2× faster with measured GPU latency. We also apply ProxylessNAS to specialize neural architectures for hardware with direct hardware metrics (e.g. latency) and provide insights for efficient CNN architecture design.",2023-11-26 03:25,Robi Rahman,,,United States of America,Massachusetts Institute of Technology (MIT),,,,ProxylessNAS,,,
KataGo,Games,Go,David J. Wu,SOTA Improvement,Better than ELF OpenGo while using 1/50th the compute.,,https://arxiv.org/abs/1902.10565,70.00,Accelerating Self-Play Learning in Go,2019-02-27,Jane Street,Industry,2500000.00,https://arxiv.org/abs/2210.00849 gives parameter count for AlphaZero in Fig 1b.,23200000000000000000.00,"""[KataGo] surpasses the strength of ELF OpenGo after training on about 27 V100 GPUs for 19 days""
14.13 teraFLOP/s * 19 days = 2.32e+19 FLOP",,"Self-play: ""In total, KataGo’s main run lasted for 19 days using a maximum of 28 V100 GPUs at any time (averaging 26-27) and generated about 241 million training samples across 4.2 million games.""",241000000,241 million training samples across 4.2 million games,,,,456.0,27 processors for 19 days,NVIDIA Tesla V100 DGXS 16 GB,NVIDIA Tesla V100 DGXS 16 GB,Self-supervised learning,,,Industry,Speculative,"By introducing several improvements to the AlphaZero process and architecture, we greatly accelerate self-play learning in Go, achieving a 50x reduction in computation over comparable methods. Like AlphaZero and replications such as ELF OpenGo and Leela Zero, our bot KataGo only learns from neural-net-guided Monte Carlo tree search self-play. But whereas AlphaZero required thousands of TPUs over several days and ELF required thousands of GPUs over two weeks, KataGo surpasses ELF's final model after only 19 days on fewer than 30 GPUs. Much of the speedup involves non-domain-specific improvements that might directly transfer to other problems. Further gains from domain-specific techniques reveal the remaining efficiency gap between the best methods and purely general methods such as AlphaZero. Our work is a step towards making learning in state spaces as large as Go possible without large-scale computational resources.",2023-11-26 03:25,Anonymous,,,Multinational,Jane Street,,,,KataGo,,,
DOC + Finetune∗ + Partial Shuffle (WT2),Language,,Ofir Press,,,,https://arxiv.org/abs/1903.04167,5.00,Partially Shuffling the Training Data to Improve Language Models,2019-03-11,University of Washington,,67300000.00,,,,WikiText-2,,,,,,,,,,,,,,,,,2023-11-21 01:39,Robi Rahman,DOC + Finetune∗ + Partial Shuffle (WT2),,United States of America,University of Washington,,,,,,,
DOC + Finetune∗ + Partial Shuffle (PTB),Language,,,,,,,,,2019-03-11,,,,,,,,,,,,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,DOC + Finetune∗ + Partial Shuffle (PTB),1,,,,,,,,,
FAIRSEQ Adaptive Inputs,Language,,"Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, Michael Auli",Highly cited,,,https://arxiv.org/abs/1904.01038,2539.00,"fairseq: A Fast, Extensible Toolkit for Sequence Modeling",2019-04-01,"Facebook AI Research,Google Brain",Industry,247000000.00,,7300000000000000000.00,,WikiText-103,,,,,,,,,,,,,,,,,2023-12-05 04:33,Robi Rahman,FAIRSEQ Adaptive Inputs,,"Multinational,Multinational","Facebook AI Research, Google Brain",,,,FAIRSEQ Adaptive Inputs,,,
Cross-lingual alignment,,,"Tal Schuster, Ori Ram, Regina Barzilay, and Amir Globerson.",SOTA Improvement,"""our method consistently outperforms the previous state-of-the-art on 6 tested languages""",,https://arxiv.org/abs/1902.09492,184.00,"Cross-lingual alignment of contextual word embeddings, with applications to zero- shot dependency parsing.",2019-04-04,"Tel Aviv University,Massachusetts Institute of Technology (MIT)",Academia,,,2560000000000000000.00,"From author communication:

Precision: float32

Hardware: 4 GPU  NVIDIA 1080Ti

NVIDIA 1080Ti: 1.06E+13

Compute: 7 GPU-days

0.4 * 1.06E+13 FLOP/s * 7 days * 24h/day * 3600s/h
= 2.56E+18",,,,,,3660000000000.00,"From author communication:

Precision: float32

Hardware: 4 GPU  NVIDIA 1080Ti

NVIDIA 1080Ti: 1.06E+13

Compute (Estimate): 0.00001 GPU Days


0.4 * 1.06E+13 FLOP/s * 0.00001 days * 24h/day * 3600s/h
= 3.66E+12



",,,,,,$7.83,,Academia,,,2023-12-05 04:33,Robi Rahman,,,"Israel,United States of America","Tel Aviv University, Massachusetts Institute of Technology (MIT)",,,,Cross-lingual alignment,,,
True-Regularization+Finetune,Language,,"Yangyang Shi, Mei-Yuh Hwang, Xin Lei, Haoyu Sheng",,not the best model in this paper,,https://arxiv.org/pdf/1904.04163,24.00,Knowledge Distillation For Recurrent Neural Network Language Modeling With Trust Regularization,2019-04-08,,,7000000.00,,,,,,,,,,,,,,,,,,,,,2023-11-21 01:39,Robi Rahman,True-Regularization+Finetune,,,,,,,,,,
True-Regularization+Finetune+Dynamic-Eval,Language,,"Yangyang Shi, Mei-Yuh Hwang, Xin Lei, Haoyu Sheng",SOTA Improvement,"""In the first experiment, the student model achieves state-of-the-art perplexity results on the Penn Treebank dataset [1] with a model size one third of that of the
previously published best model""",,https://arxiv.org/abs/1904.04163,24.00,Knowledge Distillation For Recurrent Neural Network Language Modeling With Trust Regularization,2019-04-08,"Mobvoi,Williams College",,7000000.00,,,,,,,,,,,,,,,,,,,,,2023-11-26 03:25,Robi Rahman,True-Regularization+Finetune+Dynamic-Eval,,"China,United States of America","Mobvoi, Williams College",,,,True-Regularization+Finetune+Dynamic-Eval,,,
WeNet (WT2),Language,,"Zhiheng Huang, Bing Xiang",,,,https://arxiv.org/abs/1904.03819,5.00,WeNet: Weighted Networks for Recurrent Network Architecture Search,2019-04-08,Amazon,,33000000.00,,,,WikiText-2,,,,6000.00,,,,,,,,,,,,,2023-11-21 01:39,Robi Rahman,WeNet (WT2),,Multinational,Amazon,,,,,,,
WeNet (PTB),Language,,,,,,,,,2019-04-08,,,,,,,,,,,,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,WeNet (PTB),1,,,,,,,,,
WeNet (Penn Treebank),Language,,"Zhiheng Huang, Bing Xiang",SOTA Improvement,"""We show that an architecture found by WeNets
arXiv:1904.03819v1 [cs.NE] 8 Apr 2019
WeNet: Weighted Networks for Recurrent Network Architecture Search
achieves state-of-the-art results on the Penn Treebank
language dataset""",,https://arxiv.org/abs/1904.03819,5.00,WeNet: Weighted Networks for Recurrent Network Architecture Search,2019-04-08,Amazon,,23000000.00,,,,Penn TreeBank,,,,6000.00,,,,,,,,,,,Unverified,,2023-11-26 03:25,Epoch Artificialintelligence,WeNet (WT2),,Multinational,Amazon,,,,WeNet (Penn Treebank),,,
Transformer-XL + RMS dynamic eval,Language,,"Ben Krause, Emmanuel Kahembwe, Iain Murray, Steve Renals",SOTA Improvement,"""By applying dynamic evaluation to Transformer-XL models, we improve the state of the art on enwik8 from 0.99 to 0.94 bits/char, text8 from 1.08 to 1.04 bits/char, and WikiText-103 from 18.3 to 16.4 perplexity points.""",,https://arxiv.org/abs/1904.08378,40.00,Dynamic Evaluation of Transformer Language Models,2019-04-17,University of Edinburgh,,257000000.00,,,,WikiText-103,,,,,,,,,,,,,,,,,2023-11-26 03:25,Robi Rahman,Transformer-XL + RMS dynamic eval,,United Kingdom of Great Britain and Northern Ireland,University of Edinburgh,,,,Transformer-XL + RMS dynamic eval,,,
SpecAugment,Language,Speech recognition," Daniel S. Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, Ekin D. Cubuk, Quoc V. Le",Highly cited,,,https://arxiv.org/abs/1904.08779,2704.00,SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition,2019-04-18,Google Brain,Industry,,,,,,,,,,,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,Multinational,Google Brain,,,,SpecAugment,,,
LTM,Language,,"Anupiya Nugaliyadde, Kok Wai Wong, Ferdous Sohel, Hong Xie",,,,https://arxiv.org/pdf/1904.08936,19.00,Language Modeling through Long Term Memory Network,2019-04-18,,Industry - Academia Collaboration,,,,,,,,,,,,,,,,,,,,,,2023-11-21 01:39,Robi Rahman,LTM,,,,,,,,,,
BERT-Large-CAS (PTB+WT2+WT103),Language,,"Chenguang Wang, Mu Li, Alexander J. Smola",SOTA Improvement,"""CAS achieves perplexities between 20.42 and 34.11 on all problems, i.e. on average an improvement of 12.0 perplexity units compared to state-of-the-art LSTMs""",,https://arxiv.org/abs/1904.09408,110.00,Language Models with Transformers,2019-04-20,Amazon,,395000000.00,,521000000000000000000.00,,Penn TreeBank; WikiText-2; WikiText-103,,,,50.00,,,,,,,,,,,,,2023-11-26 03:25,Robi Rahman,BERT-Large-CAS (PTB+WT2+WT103),,Multinational,Amazon,,,,BERT-Large-CAS (PTB+WT2+WT103),,,
BERT-Large-CAS (WT103),Language,,,,,,,,,2019-04-20,,,,,,,,,,,50.00,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,BERT-Large-CAS (WT103),1,,,,,,,,,
BERT-Large-CAS (WT2),Language,,,,,,,,,2019-04-20,,,,,,,,,,,50.00,,,,,,,,,,,Unverified,,2023-12-08 01:01,Robi Rahman,BERT-Large-CAS (WT2),1,,,,,,,,,
DANet,Vision,Semantic segmentation,"Jun Fu, Jing Liu, Haijie Tian, Yong Li, Yongjun Bao, Zhiwei Fang, Hanqing Lu",Highly cited,,,https://openaccess.thecvf.com/content_CVPR_2019/html/Fu_Dual_Attention_Network_for_Scene_Segmentation_CVPR_2019_paper.html,3798.00,Dual Attention Network for Scene Segmentation,2019-04-21,Chinese Academy of Sciences,Industry - Academia Collaboration (Academia leaning),,,,,,,,,,,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,China,Chinese Academy of Sciences,,,,DANet,,,
ResNet-50 Billion-scale,Vision,Image classification,"I. Zeki Yalniz, Hervé Jégou, Kan Chen, Manohar Paluri, Dhruv Mahajan",Highly cited,,,https://arxiv.org/abs/1905.00546,371.00,Billion-scale semi-supervised learning for image classification,2019-05-02,Facebook AI,Industry,25000000.00,25M parameters vanilla ResNet50,,,,,,,,,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,Multinational,Facebook AI,,,,ResNet-50 Billion-scale,,,
ResNeXt-101 Billion-scale,Vision,Image classification,"IZ Yalniz, H Jégou, K Chen, M Paluri",SOTA Improvement,"""We demonstrate the performance of our method on popular classification benchmarks for both images and videos and significantly outperforms the state of the art.""",,https://arxiv.org/abs/1905.00546,371.00,Billion-scale semi-supervised learning for image classification,2019-05-02,Facebook AI,Industry,193000000.00,,,,,,,,,,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,Multinational,Facebook AI,,,,ResNeXt-101 Billion-scale,,,
AWD-LSTM-DRILL + dynamic evaluation† (WT2),Language,,"Nikolaos Pappas, James Henderson",SOTA Improvement,"""our models improve over the state-of-the-art by +1.6 perplexity on PennTreebank and by +3.9 perplexity on
Wikitext-2""",,https://arxiv.org/abs/1905.05513,7.00,Deep Residual Output Layers for Neural Language Generation,2019-05-14,IDIAP,,34000000.00,,424000000000000000.00,,WikiText-2,,,,1000.00,,,,,,,,,,,,,2023-11-26 03:25,Robi Rahman,AWD-LSTM-DRILL + dynamic evaluation† (WT2),,Switzerland,IDIAP,,,,AWD-LSTM-DRILL + dynamic evaluation† (WT2),,,
AWD-LSTM-DRILL + dynamic evaluation† (PTB),Language,,,,,,,,,2019-05-14,,,,,,,,,,,1000.00,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,AWD-LSTM-DRILL + dynamic evaluation† (PTB),1,,,,,,,,,
CPC v2,Drawing,Image completion,,SOTA Improvement,"""this unsupervised representation substantially improves transfer learning to object detection on the
PASCAL VOC dataset, surpassing fully supervised pre-trained ImageNet classifiers""",,https://arxiv.org/abs/1905.09272,491.00,Data-Efficient Image Recognition with Contrastive Predictive Coding,2019-05-22,"DeepMind,UC Berkeley",Industry - Academia Collaboration (Industry leaning),303000000.00,source: https://openai.com/blog/image-gpt/#rfref25d,,,,,,,,,,,,,,,,,Industry,,,2023-11-26 03:25,Robi Rahman,,,"United Kingdom of Great Britain and Northern Ireland,United States of America","DeepMind, UC Berkeley",,,,CPC v2,,,
EfficientNet-L2,Vision,Image classification,"M Tan, Q Le",Highly cited,,,https://arxiv.org/abs/1905.11946,11523.00,EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks,2019-05-28,Google,Industry,480000000.00,,,,,,,,,390000000.00,"table 2: using efficientnet_b0
",,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,Multinational,Google,,,,EfficientNet-L2,,,
RSM,Language,,"David Rawlinson, Abdelrahman Ahmed, Gideon Kowadlo",,,,https://arxiv.org/pdf/1905.11589,3.00,Learning distant cause and effect using only local and immediate credit assignment,2019-05-28,,,,,,,,,,,,,,,,,,,,,,,,2023-11-21 01:39,Robi Rahman,RSM,,,,,,,,,,
Grover-Mega,Language,,"R Zellers, A Holtzman, H Rashkin, Y Bisk",,,,https://arxiv.org/abs/1905.12616,700.00,Defending Against Neural Fake News,2019-05-29,University of Washington,Industry - Academia Collaboration (Academia leaning),1500000000.00,,,,,,,,,,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,United States of America,University of Washington,,,,,,,
MnasNet-A3,Vision,Performing image classification and object detection on mobile devices,"Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, Quoc V. Le",Highly cited,,,https://arxiv.org/abs/1807.11626,2409.00,MnasNet: Platform-Aware Neural Architecture Search for Mobile,2019-05-29,Google,Industry,5200000.00,From https://arxiv.org/pdf/1807.11626.pdf,1.5e+21,"""each architecture search takes 4.5 days on 64 TPUv2 devices""
This seems to be referring to a TPUv2 pod, consisting of 64 four-chip modules. The total performance is 11.5 petaFLOPS.
https://en.wikipedia.org/wiki/Tensor_Processing_Unit#Second_generation_TPU
Assuming a 33% utilization rate:

4.5 days * 64 * 180 teraFLOPS * 0.33 = 1.48*10^21 FLOP

However, it is unclear if ""64 TPUv2 devices"" refers to chips or modules, so the true compute might be 1/4 of this amount.",ImageNet,,1280000,"""In this paper, we directly perform our architecture search on the ImageNet training set but with fewer training steps (5 epochs). As a common practice, we reserve randomly selected 50K images from the training set as the fixed validation set. """,,806000000.00,"Table 1: 403M mult-adds per image
1 multiply-add = 2 FLOP",,,Google TPU v3,Google TPU v3,,$4331.00,,Industry,Speculative,"Designing convolutional neural networks (CNN) for mobile devices is challenging because mobile models need to be small and fast, yet still accurate. Although significant efforts have been dedicated to design and improve mobile CNNs on all dimensions, it is very difficult to manually balance these trade-offs when there are so many architectural possibilities to consider. In this paper, we propose an automated mobile neural architecture search (MNAS) approach, which explicitly incorporate model latency into the main objective so that the search can identify a model that achieves a good trade-off between accuracy and latency. Unlike previous work, where latency is considered via another, often inaccurate proxy (e.g., FLOPS), our approach directly measures real-world inference latency by executing the model on mobile phones. To further strike the right balance between flexibility and search space size, we propose a novel factorized hierarchical search space that encourages layer diversity throughout the network. Experimental results show that our approach consistently outperforms state-of-the-art mobile CNN models across multiple vision tasks. On the ImageNet classification task, our MnasNet achieves 75.2% top-1 accuracy with 78ms latency on a Pixel phone, which is 1.8x faster than MobileNetV2 [29] with 0.5% higher accuracy and 2.3x faster than NASNet [36] with 1.2% higher accuracy. Our MnasNet also achieves better mAP quality than MobileNets for COCO object detection. Code is at this https URL",2023-12-05 04:33,Robi Rahman,,,Multinational,Google,,,,MnasNet-A3,,,
MnasNet-A1 + SSDLite,Vision,Performing image classification and object detection on mobile devices,"Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, Quoc V. Le",Highly cited,,,https://arxiv.org/abs/1807.11626,2409.00,MnasNet: Platform-Aware Neural Architecture Search for Mobile,2019-05-29,Google,Industry,4900000.00,From https://arxiv.org/pdf/1807.11626.pdf,1.5e+21,"""each architecture search takes 4.5 days on 64 TPUv2 devices""
This seems to be referring to a TPUv2 pod, consisting of 64 four-chip modules. The total performance is 11.5 petaFLOPS.
https://en.wikipedia.org/wiki/Tensor_Processing_Unit#Second_generation_TPU
Assuming a 33% utilization rate:

4.5 days * 64 * 180 teraFLOPS * 0.33 = 1.48*10^21 FLOP

However, it is unclear if ""64 TPUv2 devices"" refers to chips or modules, so the true compute might be 1/4 of this amount.",MS COCO,,118000,,,1600000000.00,"Table 3: 0.8B mult-adds per image
1 multiply-add = 2 FLOP",,,Google TPU v3,Google TPU v3,,$4331.00,,Industry,Speculative,"Designing convolutional neural networks (CNN) for mobile devices is challenging because mobile models need to be small and fast, yet still accurate. Although significant efforts have been dedicated to design and improve mobile CNNs on all dimensions, it is very difficult to manually balance these trade-offs when there are so many architectural possibilities to consider. In this paper, we propose an automated mobile neural architecture search (MNAS) approach, which explicitly incorporate model latency into the main objective so that the search can identify a model that achieves a good trade-off between accuracy and latency. Unlike previous work, where latency is considered via another, often inaccurate proxy (e.g., FLOPS), our approach directly measures real-world inference latency by executing the model on mobile phones. To further strike the right balance between flexibility and search space size, we propose a novel factorized hierarchical search space that encourages layer diversity throughout the network. Experimental results show that our approach consistently outperforms state-of-the-art mobile CNN models across multiple vision tasks. On the ImageNet classification task, our MnasNet achieves 75.2% top-1 accuracy with 78ms latency on a Pixel phone, which is 1.8x faster than MobileNetV2 [29] with 0.5% higher accuracy and 2.3x faster than NASNet [36] with 1.2% higher accuracy. Our MnasNet also achieves better mAP quality than MobileNets for COCO object detection. Code is at this https URL",2023-12-05 04:33,Robi Rahman,,,Multinational,Google,,,,MnasNet-A1 + SSDLite,,,
DLRM-2020,Recommendation,,"M Naumov, D Mudigere, HJM Shi, J Huang",SOTA Improvement,"""In this paper, we develop a state-of-the-art deep learning recommendation model
(DLRM)""",,https://arxiv.org/abs/1906.00091,486.00,Deep Learning Recommendation Model for Personalization and Recommendation Systems,2019-05-31,Facebook AI,Industry,100000000000.00,"Figure 1

https://arxiv.org/abs/2104.05158",4000000000000000000.00,"Figure 1

https://arxiv.org/abs/2104.05158",,,,,,,,,,,,,$14.60,,Industry,,,2023-12-05 04:33,Robi Rahman,,,Multinational,Facebook AI,,,,DLRM-2020,,,
XLNet,Language,,"Z Yang, Z Dai, Y Yang, J Carbonell",Highly cited,,,https://arxiv.org/abs/1906.08237,6615.00,XLNet: Generalized Autoregressive Pretraining for Language Understanding,2019-06-01,"Carnegie Mellon University (CMU),Google Brain",Industry - Academia Collaboration,360268800.00,,,,,,,,,,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,"United States of America,Multinational","Carnegie Mellon University (CMU), Google Brain",,,,XLNet,,,
XLM,Language,,"G Lample, A Conneau",SOTA Improvement,"""On supervised machine translation, we obtain a new state of the art of 38.5 BLEU on WMT’16 Romanian-English, outperforming the previous best approach by more
than 4 BLEU""",,https://arxiv.org/abs/1901.07291,2220.00,Cross-lingual Language Model Pretraining,2019-06-01,Facebook,Industry,665000000.00,,,,,,,,,,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,Multinational,Facebook,,,,XLM,,,
AMDIM,Drawing,Image completion,"Philip Bachman, R Devon Hjelm, William Buchwalter",Highly cited,,,https://arxiv.org/abs/1906.00910,1307.00,Learning Representations by Maximizing Mutual Information Across Views,2019-06-03,Microsoft Research,Industry,626000000.00,source: https://openai.com/blog/image-gpt/#rfref13e,,,,,,,,,,,,,,,,,Industry,,,2023-11-26 03:25,Robi Rahman,,,United States of America,Microsoft Research,,,,AMDIM,,,
Transformer-XL Large + Phrase Induction,Language,,"Hongyin Luo, Lan Jiang, Yonatan Belinkov, James Glass",SOTA Improvement,"""We achieved a new state-of-the-art performance of 17.4 perplexity on the Wikitext-103 dataset""",,https://arxiv.org/abs/1906.01702,12.00,"Improving Neural Language Models by Segmenting, Attending, and Predicting the Future",2019-06-04,"Massachusetts Institute of Technology (MIT),University of Illinois Urbana-Champaign (UIUC)",,257000000.00,,7300000000000000000.00,,WikiText-103,,,,1.00,,,,,,,,,,,,,2023-11-26 03:25,Robi Rahman,Transformer-XL Large + Phrase Induction,,"United States of America,United States of America","Massachusetts Institute of Technology (MIT), University of Illinois Urbana-Champaign (UIUC)",,,,Transformer-XL Large + Phrase Induction,,,
AWD-LSTM + Phrase Induction + finetuning,Language,,,,,,,,,2019-06-04,,,,,,,,,,,,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,AWD-LSTM + Phrase Induction + finetuning,1,,,,,,,,,
AWD-LSTM + MoS + Partial Shuffled,Language,,"Dilin Wang, Chengyue Gong, Qiang Liu",SOTA Improvement,"""our method improves on the single model state-of-the-art results for language modeling on Penn Treebank (PTB) and Wikitext-2, achieving test perplexity scores of 46.01 and 38.07, respectively""",,https://arxiv.org/abs/1906.03805,96.00,Improving Neural Language Modeling via Adversarial Training,2019-06-10,University of Texas at Austin,,35000000.00,,328000000000000000.00,,WikiText-2,,,,750.00,,,,,,,,,,,,,2023-12-05 04:33,Robi Rahman,AWD-LSTM + MoS + Partial Shuffled,,United States of America,University of Texas at Austin,,,,AWD-LSTM + MoS + Partial Shuffled,,,
AdvSoft + 4 layer QRNN + dynamic evaluation,Language,,,,,,,,,2019-06-10,,,,,,,,,,,,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,AdvSoft + 4 layer QRNN + dynamic evaluation,1,,,,,,,,,
Adversarial + AWD-LSTM-MoS + partial shuffled,Language,,,,,,,,,2019-06-10,,,,,,,,,,,450.00,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,Adversarial + AWD-LSTM-MoS + partial shuffled,1,,,,,,,,,
4 layer QRNN + dynamic evaluation,Language,,,,,,,,,2019-06-10,,,,,,,,,,,,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,4 layer QRNN + dynamic evaluation,1,,,,,,,,,
Char-CNN-BiLSTM,Language,,"Chris Larson, Tarek Lahlou, Diana Mingels, Zachary Kulis, Erik Mueller",SOTA Improvement,"""Notably, our language model achieves a test perplexity of 37.49 on PTB, which to our knowledge is state-of-the-art among models trained only on PTB.""",,https://arxiv.org/pdf/1906.05678,2.00,Telephonetic: Making Neural Language Models Robust to ASR and Semantic Noise,2019-06-13,Capital One,Industry - Academia Collaboration,,,,,,,,,,,,,,,,,,,,,,2023-12-05 04:33,Robi Rahman,Char-CNN-BiLSTM,,United States of America,Capital One,,,,Char-CNN-BiLSTM,,,
FixRes ResNeXt-101 WSL,Vision,Image classification,"H Touvron, A Vedaldi, M Douze, H Jégou",SOTA Improvement,"""To the best of our knowledge our ResNeXt-101 32x48d surpasses all other models available in the literature""",,https://arxiv.org/abs/1906.06423,405.00,Fixing the train-test resolution discrepancy,2019-06-14,Facebook AI,Industry,829000000.00,,,,,,940000000,"""Conversely, when training a ResNeXt-101 32x48d pre-trained in weakly-supervised fashion on 940 million public images at resolution 224x224 and further optimizing for test resolution 320x320, we obtain a test top-1 accuracy of 86.4% (top-5: 98.0%) (single-crop)""",,,,,,,,,,"https://medium.com/swlh/deepmind-achieved-starcraft-ii-grandmaster-level-but-at-what-cost-32891dd990e4#:~:text=According%20to%20the%20analysis%20by,Source%3A%20DeepMind.",Industry,,,2023-11-26 03:25,Robi Rahman,,,Multinational,Facebook AI,,,,FixRes ResNeXt-101 WSL,,,
PG-SWGAN,Drawing,Image generation,"Jiqing Wu, Zhiwu Huang, Dinesh Acharya, Wen Li, Janine Thoma, Danda Pani Paudel, Luc Van Gool",SOTA Improvement,"""For fair comparison, we equip the same progressive growing architecture with our proposed SWGAN objective and its dual
SWD blocks (PG-SWGAN). As shown in Fig. 3 (Right)
and Fig. 5, our PG-SWGAN can outperform PG-WGAN in
terms of both qualitative and quantitative comparison on the
CelebA-HQ and LSUN datasets""",,https://openaccess.thecvf.com/content_CVPR_2019/html/Wu_Sliced_Wasserstein_Generative_Models_CVPR_2019_paper.html,115.00,Sliced Wasserstein Generative Models,2019-06-15,ETH Zurich,Academia,,,,,"CIFAR-10, LSUN, Celeb-A",,,,,,,,,,,,,,,Likely,"In generative modeling, the Wasserstein distance (WD) has emerged as a useful metric to measure the discrepancy between generated and real data distributions. Unfortunately, it is challenging to approximate the WD of high-dimensional distributions. In contrast, the sliced Wasserstein distance (SWD) factorizes high-dimensional distributions into their multiple one-dimensional marginal distributions and is thus easier to approximate. In this paper, we introduce novel approximations of the primal and dual SWD. Instead of using a large number of random projections, as it is done by conventional SWD approximation methods, we propose to approximate SWDs with a small number of parameterized orthogonal projections in an end-to-end deep learning fashion. As concrete applications of our SWD approximations, we design two types of differentiable SWD blocks to equip modern generative frameworks---Auto-Encoders (AE) and Generative Adversarial Networks (GAN). In the experiments, we not only show the superiority of the proposed generative models on standard image synthesis benchmarks, but also demonstrate the state-of-the-art performance on challenging high resolution image and video generation in an unsupervised manner.",2023-11-26 03:25,Anonymous,,,Switzerland,ETH Zurich,,,,PG-SWGAN,,,
Stacked-LSTM+Pruning,Language,,"Liangjian Wen, Xuanyang Zhang, Haoli Bai, Zenglin Xu",,,,https://arxiv.org/pdf/1906.06847,34.00,Structured Pruning of Recurrent Neural Networks through Neuron Selection,2019-06-17,,Academia,6160000.00,,,,,,,,,,,,,,,,,,,,,2023-11-21 01:39,Robi Rahman,Stacked-LSTM+Pruning,,,,,,,,,,
LaNet-L (CIFAR-10),Vision,Image Classification,"Linnan Wang, Saining Xie, Teng Li, Rodrigo Fonseca, Yuandong Tian",SOTA Improvement,"""In practice, LaNAS finds a network that achieves SOTA 99.0% accuracy on CIFAR-10""",,https://arxiv.org/abs/1906.06832,42.00,Sample-Efficient Neural Architecture Search by Learning Action Space,2019-06-17,"Brown University,Facebook",Industry - Academia Collaboration (Industry leaning),44100000.00,44.1M,,"LaNet-L was trained on 150 GPU-days, however the GPU was not specified",CIFAR-10,,,,600.00,,,,,,,Supervised,,,,Likely,"Neural Architecture Search (NAS) has emerged as a promising technique for automatic neural network design. However, existing MCTS based NAS approaches often utilize manually designed action space, which is not directly related to the performance metric to be optimized (e.g., accuracy), leading to sample-inefficient explorations of architectures. To improve the sample efficiency, this paper proposes Latent Action Neural Architecture Search (LaNAS), which learns actions to recursively partition the search space into good or bad regions that contain networks with similar performance metrics. During the search phase, as different action sequences lead to regions with different performance, the search efficiency can be significantly improved by biasing towards the good regions. On three NAS tasks, empirical results demonstrate that LaNAS is at least an order more sample efficient than baseline methods including evolutionary algorithms, Bayesian optimizations, and random search. When applied in practice, both one-shot and regular LaNAS consistently outperform existing results. Particularly, LaNAS achieves 99.0% accuracy on CIFAR-10 and 80.8% top1 accuracy at 600 MFLOPS on ImageNet in only 800 samples, significantly outperforming AmoebaNet with 33x fewer samples. Our code is publicly available at this https URL.",2023-12-05 04:33,Anonymous,,,"United States of America,Multinational","Brown University, Facebook",,,,LaNet-L (CIFAR-10),,,
Walking Minotaur robot,Robotics,,"Tuomas Haarnoja, Sehoon Ha, Aurick Zhou, Jie Tan, George Tucker, Sergey Levine",SOTA Improvement,,,https://arxiv.org/abs/1812.11103,377.00,Learning to Walk via Deep Reinforcement Learning,2019-06-19,"UC Berkeley,Google Brain",Industry - Academia Collaboration (Industry leaning),,,,,,,,,,,,,,,,Reinforcement learning,,,Industry,Speculative,"Deep reinforcement learning (deep RL) holds the promise of automating the acquisition of complex controllers that can map sensory inputs directly to low-level actions. In the domain of robotic locomotion, deep RL could enable learning locomotion skills with minimal engineering and without an explicit model of the robot dynamics. Unfortunately, applying deep RL to real-world robotic tasks is exceptionally difficult, primarily due to poor sample complexity and sensitivity to hyperparameters. While hyperparameters can be easily tuned in simulated domains, tuning may be prohibitively expensive on physical systems, such as legged robots, that can be damaged through extensive trial-and-error learning. In this paper, we propose a sample-efficient deep RL algorithm based on maximum entropy RL that requires minimal per-task tuning and only a modest number of trials to learn neural network policies. We apply this method to learning walking gaits on a real-world Minitaur robot. Our method can acquire a stable gait from scratch directly in the real world in about two hours, without relying on any model or simulation, and the resulting policy is robust to moderate variations in the environment. We further show that our algorithm achieves state-of-the-art performance on simulated benchmarks with a single set of hyperparameters. Videos of training and the learned policy can be found on the project website.",2023-11-26 03:25,Anonymous,,,"United States of America,Multinational","UC Berkeley, Google Brain",,,,Walking Minotaur robot,,,
TAPE Transformer,Language,Proteins,"Roshan Rao, Nicholas Bhattacharya, Neil Thomas, Yan Duan, Xi Chen, John Canny, Pieter Abbeel, Yun S. Song",,,,https://arxiv.org/abs/1906.08230,617.00,Evaluating Protein Transfer Learning with TAPE,2019-06-19,"UC Berkeley,Covariant",Industry - Academia Collaboration (Industry leaning),38000000.00,"""We use a 12-layer Transformer with a hidden size of 512 units and 8 attention heads, leading to a 38M-parameter model""",90700000000000000000.00,"""All self-supervised models are trained on four NVIDIA V100 GPUs for one week""

4* 125 teraFLOP/s * 7 * 24 * 3600 * 0.3 (utilization assumption) = 9.07e19",Pfam,"""We use Pfam [33], a database of thirty-one million protein domains used extensively in bioinformatics, as the pretraining corpus for TAPE""",,,,,,168.0,,,,Self-supervised learning,,,,Likely,"Machine learning applied to protein sequences is an increasingly popular area
of research. Semi-supervised learning for proteins has emerged as an important
paradigm due to the high cost of acquiring supervised protein labels, but the current
literature is fragmented when it comes to datasets and standardized evaluation
techniques. To facilitate progress in this field, we introduce the Tasks Assessing
Protein Embeddings (TAPE), a set of five biologically relevant semi-supervised
learning tasks spread across different domains of protein biology. We curate
tasks into specific training, validation, and test splits to ensure that each task tests
biologically relevant generalization that transfers to real-life scenarios. We benchmark a range of approaches to semi-supervised protein representation learning,
which span recent work as well as canonical sequence learning techniques. We
find that self-supervised pretraining is helpful for almost all models on all tasks,
more than doubling performance in some cases. Despite this increase, in several
cases features learned by self-supervised pretraining still lag behind features extracted by state-of-the-art non-neural techniques. This gap in performance suggests
a huge opportunity for innovative architecture design and improved modeling
paradigms that better capture the signal in biological sequences. TAPE will help
the machine learning community focus effort on scientifically relevant problems.
Toward this end, all data and code used to run these experiments are available at
https://github.com/songlab-cal/tape.",2023-12-09 05:07,Anonymous,,,"United States of America,Multinational","UC Berkeley, Covariant",,,,,,,
Tensorized Transformer (257M),Language,,"Xindian Ma, Peng Zhang, Shuai Zhang, Nan Duan, Yuexian Hou, Ming Zhou, Dawei Song",SOTA Improvement,"""Table 2: Results and compression with state-of-the-art results on PTB and WikiText-103""",,https://arxiv.org/abs/1906.09777,126.00,A Tensorized Transformer for Language Modeling,2019-06-24,"Tianjin University,Microsoft Research Asia,Beijing Institute of Technology",,257000000.00,,4760000000000000000.00,,WikiText-103,,,,30.00,,,,,,,,,,,,,2023-11-26 03:25,Robi Rahman,Tensorized Transformer (257M),,"China,China,China","Tianjin University, Microsoft Research Asia, Beijing Institute of Technology",,,,Tensorized Transformer (257M),,,
Tensorized Transformer (core-2),Language,,,,,,,,,2019-06-24,,,,,,,,,,,30.00,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,Tensorized Transformer (core-2),1,,,,,,,,,
Tensorized Transformer (small),Language,,,,,,,,,2019-06-24,,,,,,,,,,,30.00,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,Tensorized Transformer (small),1,,,,,,,,,
Tensorized Transformer (large PTB),Language,,,,,,,,,2019-06-24,,,,,,,,,,,30.00,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,Tensorized Transformer (large PTB),1,,,,,,,,,
Tensorized Transformer (151M),Language,,,,,,,,,2019-06-24,,,,,,,,,,,30.00,,,,,,,,,,,Unverified,,2023-12-08 01:01,Robi Rahman,Tensorized Transformer (151M),1,,,,,,,,,
RoBERTa Large,Language,,"Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov",Highly cited,,,https://arxiv.org/abs/1907.11692,15901.00,RoBERTa: A Robustly Optimized BERT Pretraining Approach,2019-07-01,"Facebook,University of Washington",Industry,355000000.00,,4.15383552e+21,"Section 5: We pretrain our model using 1024 V100 GPUs for approximately one day.

Note this is the base pretraining comparable to BERT, 100k steps. Subsequently they do more: ""increasing the number of pretraining steps
from 100K to 300K, and then further to 500K"".

So assume 5x the 1024 V100 GPUs for 1d estimate. Mixed precision.

C=5*1024*3.13E+13*60**2*24*0.3 = 4.2e21",,,32000000000,160GB*200M words/GB = 3.2e10 words,,,"Authors of KEPLER say their model has the same inference compute as RoBERTa, so if we calculate this we may use it for KEPLER, too

""It depends on the length of the input sequences. The inference computation of KEPLER is the same as RoBERTa (base) and you may estimate it with this.""",120.0,"First the model is pretrained for 100k steps on 1024 GPUs for 1 day, then pretraining is increased to 500k steps, so assuming they used the same number of GPUs, this would have taken 5 days.",NVIDIA Tesla V100 DGXS 32 GB,NVIDIA Tesla V100 DGXS 32 GB,,,,Industry,Likely,,2023-12-05 11:53,Robi Rahman,,0,"Multinational,United States of America","Facebook, University of Washington",,,,RoBERTa Large,1024,,RoBERTa Large
All-attention network + adaptive span,Language,,"Sainbayar Sukhbaatar, Edouard Grave, Guillaume Lample, Herve Jegou, Armand Joulin",,,,https://arxiv.org/abs/1907.01470,98.00,Augmenting Self-attention with Persistent Memory,2019-07-02,,,133000000.00,,46000000000000000000.00,,WikiText-103,,,,,,,,,,,,,,,,,2023-12-05 04:33,Robi Rahman,All-attention network + adaptive span,,,,,,,,,,
BigBiGAN,Drawing,Image completion,"Spyros Gidaris, Praveer Singh, Nikos Komodakis",SOTA Improvement,"""BigBiGAN, an unsupervised learning approach based purely on generative models, achieves state-of-the-art results in image representation learning on ImageNet""",,https://arxiv.org/abs/1907.02544,465.00,Large Scale Adversarial Representation Learning,2019-07-04,Google,Industry,86000000.00,https://openai.com/blog/image-gpt/#rfref53,,,,,,,,,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,Multinational,Google,,,,BigBiGAN,,,
Pluribus,Games,Poker,"Noam Brown, Tuomas Sandholm",SOTA Improvement,"first to beat humans at multiplayer poker: ""Developing a superhuman AI for multiplayer poker was the widely,recognized main remaining milestone. In this paper we describe Pluribus, an AI capable of defeating elite human professionals in six-player no-limit Texas hold’em poker, the most commonly played poker format in the world.""",,https://www.science.org/cms/asset/910714a7-ee2a-486e-9970-42fb893b08d9/pap.pdf,575.00,Superhuman AI for multiplayer poker,2019-07-11,Facebook AI Research,Industry - Academia collaboration,,,66000000000000000.00,"Trained in 8 days on a 64 core CPU
https://ai.facebook.com/blog/pluribus-first-ai-to-beat-pros-in-6-player-poker/

""We trained the blueprint strategy for Pluribus in eight days on a 64-core server and required less than 512 GB of RAM. No GPUs were used. At typical cloud computing instance rates, it would cost less than $150 to train.""

Guess: trained on i7 Intel CPU, approx 5e9 FLOP/s for each core.

 https://epochai.org/blog/estimating-training-compute
8 days, 64 cores, 5e9 FLOP/s, 30% utilization",,,,,,,,,,,,,,,,,,2023-11-26 03:25,Robi Rahman,,,Multinational,Facebook AI Research,,,,Pluribus,,,
R-Transformer,Language,,"Zhiwei Wang, Yao Ma, Zitao Liu, Jiliang Tang",,,,https://arxiv.org/abs/1907.05572,93.00,R-Transformer: Recurrent Neural Network Enhanced Transformer,2019-07-12,,,15800000.00,,8400000000000000.00,,Penn TreeBank,,,,100.00,,,,,,,,,,,,,2023-11-23 06:53,Robi Rahman,R-Transformer,,,,,,,,,,
RNN Baseline,Language,,,,,,,,,2019-07-14,,,,,,,,,,,,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,RNN Baseline,1,,,,,,,,,
RNN + char2-MS-vec,Language,,,,,,,,,2019-07-15,,,,,,,,,,,,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,RNN + char2-MS-vec,1,,,,,,,,,
RNN + char3-MS-vec,Language,,,,,,,,,2019-07-16,,,,,,,,,,,,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,RNN + char3-MS-vec,1,,,,,,,,,
RNN + char4-MS-vec,Language,,"Sho Takase, Jun Suzuki, Masaaki Nagata",,,,https://ojs.aaai.org/index.php/AAAI/article/view/4440,26.00,Character n-Gram Embeddings to Improve RNN Language Models,2019-07-17,,,226000000.00,,,,WikiText-103,,,,,,,,,,,,,,,,,2023-11-21 01:39,Robi Rahman,RNN + char4-MS-vec,,,,,,,,,,
LSTM+Adam+Lookahead,Language,,"Michael R. Zhang, James Lucas, Geoffrey Hinton, Jimmy Ba",,,,https://arxiv.org/pdf/1907.08610,612.00,"Lookahead Optimizer: k steps forward, 1 step back",2019-07-19,,,7190000.00,,,,,,,,,,,,,,,,,,,,,2023-11-21 01:39,Robi Rahman,LSTM+Adam+Lookahead,,,,,,,,,,
EN^2AS with performance reward,Language,,"Miao Zhang, Huiqi Li, Shirui Pan, Taoping Liu, Steven Su",SOTA Improvement,"""The best architecture obtained by our algorithm with
the same search space achieves the state-of-the-art test error rate of 2.51% on CIFAR-10""",,https://arxiv.org/pdf/1907.09109,1.00,Efficient Novelty-Driven Neural Architecture Search,2019-07-22,"Beijing Institute of Technology,University of Technology Sydney,Monash University",Industry,23000000.00,,,,,,,,,,,,,,,,,,,,,2023-11-26 03:25,Robi Rahman,EN^2AS with performance reward,,"China,Australia,Australia","Beijing Institute of Technology, University of Technology Sydney, Monash University",,,,EN^2AS with performance reward,,,
L_UL-seq,Language,,"Sean Welleck, Ilia Kulikov, Stephen Roller, Emily Dinan, Kyunghyun Cho, Jason Weston",,"""We empirically showed that
unlikelihood training - both at the token and sequence levels - substantially reduced degeneration
according to automatic metrics, and outperformed likelihood-trained models with various decoding
methods according to human evaluation, being superior to the current state-of-the-art approaches.""",,https://arxiv.org/abs/1908.04319,405.00,Neural Text Generation with Unlikelihood Training,2019-08-12,,,247000000.00,,,,WikiText-103,,,,,,,,,,,,,,,,,2023-12-05 04:33,Robi Rahman,L_UL-seq,,,,,,,,,,
LSTM-Large+Behaviorial-Gating,Language,,"Prashanth Gurunath Shivakumar, Shao-Yen Tseng, Panayiotis Georgiou, Shrikanth Narayanan",,,,https://arxiv.org/pdf/1909.00107,3.00,Behavior Gated Language Models,2019-08-31,,,67000000.00,,,,,,,,,,,,,,,,,,,,,2023-11-21 01:39,Robi Rahman,LSTM-Large+Behaviorial-Gating,,,,,,,,,,
AWD-LSTM+Behaviorial-Gating,Language,,,,,,,,,2019-08-31,,,,,,,,,,,,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,AWD-LSTM+Behaviorial-Gating,1,,,,,,,,,
LSTM-Medium+Behaviorial-Gating,Language,,,,,,,,,2019-08-31,,,,,,,,,,,,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,LSTM-Medium+Behaviorial-Gating,1,,,,,,,,,
"DEQ-Transformer (Medium, Adaptive Embedding)",Language,,"Shaojie Bai, J. Zico Kolter, Vladlen Koltun",,,,https://arxiv.org/abs/1909.01377,496.00,Deep Equilibrium Models,2019-09-03,,,110000000.00,,816000000000000000.00,,WikiText-103,,,,12.00,,,,,,,,,,,,,2023-11-23 06:53,Robi Rahman,"""DEQ-Transformer (Medium, Adaptive Embedding)""",,,,,,,,,,
DEQ-TrellisNet,Language,,,,,,,,,2019-09-03,,,,,,,,,,,12.00,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,DEQ-TrellisNet,1,,,,,,,,,
"Mogrifier (d2, MoS2, MC) + dynamic eval",Language,,"Gábor Melis, Tomáš Kočiský, Phil Blunsom",SOTA Improvement,"""We establish a new state of the art on all datasets with the exception of Enwik8""",,https://arxiv.org/abs/1909.01792,109.00,Mogrifier LSTM,2019-09-04,"DeepMind,University of Oxford",,35000000.00,,,,WikiText-2,,,,145.00,,,,,,,,,,,,,2023-11-26 03:25,Robi Rahman,"""Mogrifier (d2, MoS2, MC) + dynamic eval""",,"United Kingdom of Great Britain and Northern Ireland,United Kingdom of Great Britain and Northern Ireland","DeepMind, University of Oxford",,,,"""Mogrifier (d2, MoS2, MC) + dynamic eval""",,,
"Mogrifier (d2, MC) + dynamic eval",Language,,,,,,,,,2019-09-04,,,,,,,,,,,145.00,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,"""Mogrifier (d2, MC) + dynamic eval""",1,,,,,,,,,
ObjectNet,Vision,Object recognition,"Andrei Barbu, David Mayo, Julian Alverio, William Luo, Christopher Wang, Dan Gutfre- und, Josh Tenenbaum, and Boris Katz",Highly cited,,,https://papers.nips.cc/paper/2019/file/97af07a14cacba681feacf3012730892-Paper.pdf,2393.00,Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models,2019-09-06,Massachusetts Institute of Technology (MIT),Academia,38000000.00,,19400000000000000000.00,"3-5 days of training (say, 4.5), 50 teraFLOP/second at 50% utilization rate (reported) = 1.94E19",Internal data,,50000,"In total, 95,824 images were collected from 5,982 workers out of which 50,000 images were retained
after validation and included in the dataset",,,,,,,,,$50.79,,Academia,,"We collect a large real-world test set, ObjectNet, for object recognition with controls where object backgrounds, rotations, and imaging viewpoints are random. Most scientific experiments have controls, confounds which are removed from the data, to ensure that subjects cannot perform a task by exploiting trivial correlations in the data. Historically, large machine learning and computer vision datasets have lacked such controls. This has resulted in models that must be fine-tuned for new datasets and perform better on datasets than in real-world applications. When tested on ObjectNet, object detectors show a 40-45% drop in performance, with respect to their performance on other benchmarks, due to the controls for biases. Controls make ObjectNet robust to fine-tuning showing only small performance increases. We develop a highly automated platform that enables gathering datasets with controls by crowdsourcing image capturing and annotation. ObjectNet is the same size as the ImageNet test set (50,000 images), and by design does not come paired with a training set in order to encourage generalization. The dataset is both easier than ImageNet – objects are largely centered and unoccluded – and harder, due to the controls. Although we focus on object recognition here, data with controls can be gathered at scale using automated tools throughout machine learning to generate datasets that exercise models in new ways thus providing valuable feedback to researchers. This work opens up new avenues for research in generalizable, robust, and more human-like computer vision and in creating datasets where results are predictive of real-world performance.",2023-11-26 03:25,Robi Rahman,,,United States of America,Massachusetts Institute of Technology (MIT),,,,ObjectNet,,,
Megatron-BERT,Language,,"Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, Bryan Catanzaro",SOTA Improvement,"""Our BERT model achieves SOTA results on the RACE dataset""",,https://arxiv.org/abs/1909.08053,1003.00,Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism,2019-09-17,NVIDIA,Industry,3900000000.00,"Source: https://lair.lighton.ai/akronomicon/

Archive on GitHub: https://github.com/lightonai/akronomicon/tree/main/akrodb",6.02700000000001e+23,"A source: https://lair.lighton.ai/akronomicon/ claims 5.7e22

The authors report experimenting on 1 V100 GPU and achieving throughput of 39 TFLOPS which is 30% of the peak throughput. Therefore the GPU has a peak throughput of 130 TFLOPS so it is specifically the NVIDIA V100S PCIe.
https://images.nvidia.com/content/technologies/volta/pdf/volta-v100-datasheet-update-us-1165301-r5.pdf

Param-based calculation:
6ND = 6*3.9e9*2e6*1024*1024 = 4.8e22 FLOP

Time-based calculation:
The 8.3B GPT-like arch took 2.1 days per epoch on 512 GPUs, batch size 512. An epoch was 68.5k iterations.
BERT: batch size 1024, 2e6 iterations total.
So we should expect 4B => 1.0 days per epoch (69e3*512 examples)
=> 2e6*1024/(69e3*512) = 58 days training

On 512 GPUs they achieve a peak throughput of 15.1 PFLOPS.
C=15.1 PFLOPS * 58 days = 7.6e22 FLOP.

The param and time calculations seem more trustworthy. Geometric mean is 6.027e22 FLOP",,,34800000000,"""The resulting aggregate corpus contains 174 GB of deduplicated text.""",,,,1392.0,"The 8.3B GPT-like arch took 2.1 days per epoch on 512 GPUs, batch size 512. An epoch was 68.5k iterations.
BERT: batch size 1024, 2e6 iterations total.
So we should expect 4B => 1.0 days per epoch (69e3*512 examples)
=> 2e6*1024/(69e3*512) = 58 days training",NVIDIA Tesla V100S PCIe 32 GB,NVIDIA Tesla V100S PCIe 32 GB,Self-supervised learning,$208034.39,,Industry,Likely,"Recent work in language modeling demonstrates that training large transformer models advances the state of the art in Natural Language Processing applications. However, very large models can be quite difficult to train due to memory constraints. In this work, we present our techniques for training very large transformer models and implement a simple, efficient intra-layer model parallel approach that enables training transformer models with billions of parameters. Our approach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model parallelism, and can be fully implemented with the insertion of a few communication operations in native PyTorch. We illustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 GPUs. We sustain 15.1 PetaFLOPs across the entire application with 76% scaling efficiency when compared to a strong single GPU baseline that sustains 39 TeraFLOPs, which is 30% of peak FLOPs. To demonstrate that large language models can further advance the state of the art (SOTA), we train an 8.3 billion parameter transformer language model similar to GPT-2 and a 3.9 billion parameter model similar to BERT. We show that careful attention to the placement of layer normalization in BERT-like models is critical to achieving increased performance as the model size grows. Using the GPT-2 model we achieve SOTA results on the WikiText103 (10.8 compared to SOTA perplexity of 15.8) and LAMBADA (66.5% compared to SOTA accuracy of 63.2%) datasets. Our BERT model achieves SOTA results on the RACE dataset (90.9% compared to SOTA accuracy of 89.4%).",2023-12-05 04:33,Robi Rahman,,0,United States of America,NVIDIA,,,,Megatron-BERT,512,0.2269,Megatron-BERT
Megatron-LM (8.3B),Language,,"Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, Bryan Catanzaro",SOTA Improvement,"""Using the GPT-2 model we achieve SOTA results on the WikiText103 (10.8 compared to SOTA perplexity of 15.8) and LAMBADA"" 

GPT-2 model here meaning model similar to GPT-2",,https://arxiv.org/abs/1909.08053,1003.00,Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism,2019-09-17,NVIDIA,Industry,8300000000.00,"Source: https://lair.lighton.ai/akronomicon/

Archived source: https://web.archive.org/web/20211220142906/https://lair.lighton.ai/akronomicon/

Data also available on GitHub: https://github.com/lightonai/akronomicon/blob/main/akrodb/NVIDIA/Megatron-LM.json",9.100000000000001e+21,"source: https://lair.lighton.ai/akronomicon/

archived: https://github.com/lightonai/akronomicon/tree/main/akrodb",,,34800000000,"""The resulting aggregate
corpus contains 174 GB of deduplicated text.""",4.40,18000000000000.00,"Rados (FLOPs)
https://drive.google.com/drive/folders/1bhy5z6hh1n3wCHx6528Xb7xB1KhYdAL1",327.0,"Reported throughput is 15.1 teraFLOPS per GPU on 512 GPUs
Assume total compute is 9.1e21 FLOP.
Then training time is 327 hours.
https://www.wolframalpha.com/input?i=9.1*10%5E21+FLOP+%2F+%28512*15.1+teraFLOPS%29",NVIDIA Tesla V100 DGXS 32 GB,NVIDIA Tesla V100 DGXS 32 GB,Self-supervised learning,$33212.51,,Industry,,"Recent work in language modeling demonstrates that training large transformer models advances the state of the art in Natural Language Processing applications. However, very large models can be quite difficult to train due to memory constraints. In this work, we present our techniques for training very large transformer models and implement a simple, efficient intra-layer model parallel approach that enables training transformer models with billions of parameters. Our approach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model parallelism, and can be fully implemented with the insertion of a few communication operations in native PyTorch. We illustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 GPUs. We sustain 15.1 PetaFLOPs across the entire application with 76% scaling efficiency when compared to a strong single GPU baseline that sustains 39 TeraFLOPs, which is 30% of peak FLOPs. To demonstrate that large language models can further advance the state of the art (SOTA), we train an 8.3 billion parameter transformer language model similar to GPT-2 and a 3.9 billion parameter model similar to BERT. We show that careful attention to the placement of layer normalization in BERT-like models is critical to achieving increased performance as the model size grows. Using the GPT-2 model we achieve SOTA results on the WikiText103 (10.8 compared to SOTA perplexity of 15.8) and LAMBADA (66.5% compared to SOTA accuracy of 63.2%) datasets. Our BERT model achieves SOTA results on the RACE dataset (90.9% compared to SOTA accuracy of 89.4%).",2023-12-05 10:41,Robi Rahman,Megatron-LM (8.3B),,United States of America,NVIDIA,,,,Megatron-LM (8.3B),512,0.1162,Megatron-LM (8.3B)
Hide and Seek,Games,Hide and Seek,"B Baker, I Kanitscheider, T Markov, Y Wu",,,,https://openai.com/blog/emergent-tool-use/,521.00,Emergent Tool Use From Multi-Agent Autocurricula,2019-09-17,OpenAI,Industry,1600000.00,"""The default model, which uses a batch size of 64,000 and 1.6 million parameters,..."" pg. 7 of the Hide and Seek paper, stored in ""RL papers""

source: https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389",1150000000000000000.00,"1.6e6 * 2 * 120e9 * 3 ~= 1.15e18 FLOP. We assume the single convolution on the lidar input is negligible, and the rest of the model (which consists of MLPs, self-attention and LSTM) has roughly 1 multiply-add per parameter. The following source has a similar estimate but does not adjust for the full number of episodes: https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389",,,120000000000,"""The default model, which uses a batch size of 64,000 and 1.6 million parameters, requires 132.3 million episodes (31.7 billion frames) over 34 hours of training to reach stage 4 of the skill progression, i.e. ramp defense."" But the full number of episodes, e.g. Figure 1, is 500 million. 500 / 132.3 * 31.7B ~= 120B.",,,,,,,,,$0.80,,Industry,,,2023-12-05 04:33,Robi Rahman,,,United States of America,OpenAI,,,,,,,
Megatron-LM (2.5B),Language,,,,,,,,,2019-09-17,,,,,,,,,,,4.40,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,Megatron-LM (2.5B),1,,,,,,,,,
Megatron-LM (355M),Language,,,,,,,,,2019-09-17,,,,,,,,,,,4.40,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,Megatron-LM (355M),1,,,,,,,,,
Alleviated TOI 10 (WT103),Language,,"Noémien Kocher, Christian Scuito, Lorenzo Tarantino, Alexandros Lazaridis, Andreas Fischer, Claudiu Musat",,,,https://arxiv.org/abs/1909.08700,0.00,Alleviating Sequence Information Loss with Data Overlapping and Prime Batch Sizes,2019-09-18,,,,,,,WikiText-103,,,,1000.00,,,,,,,,,,,,,2023-11-21 01:39,Robi Rahman,Alleviated TOI 10 (WT103),,,,,,,,,,
Alleviated TOI 10 (PTB),Language,,,,,,,,,2019-09-18,,,,,,,,,,,1000.00,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,Alleviated TOI 10 (PTB),1,,,,,,,,,
Alleviated TOI 10 (WT2),Language,,,,,,,,,2019-09-18,,,,,,,,,,,1000.00,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,Alleviated TOI 10 (WT2),1,,,,,,,,,
Transformer-XL+AdamGapAware(GA),Language,,"Saar Barkai, Ido Hakimi, Assaf Schuster",,,,https://arxiv.org/pdf/1909.10802,13.00,Gap Aware Mitigation of Gradient Staleness,2019-09-24,,,257000000.00,,,,,,,,,,,,,,,,,,,,,2023-12-05 04:33,Robi Rahman,Transformer-XL+AdamGapAware(GA),,,,,,,,,,
Adaptive Inputs + LayerDrop,Language,,"Angela Fan, Edouard Grave, Armand Joulin",SOTA Improvement,"""In neural machine translation on newstest2014, our 12 encoder layer Transformer model with LayerDrop further improves the state of the art, reaching 30.2 BLEU""",,https://arxiv.org/abs/1909.11556,435.00,Reducing Transformer Depth on Demand with Structured Dropout,2019-09-25,"Facebook AI Research,LORIA",,423000000.00,,,,WikiText-103,,,,,,,,,,,,,,,,,2023-11-26 03:25,Robi Rahman,Adaptive Inputs + LayerDrop,,"Multinational,France","Facebook AI Research, LORIA",,,,Adaptive Inputs + LayerDrop,,,
ALBERT,Language,,"Z Lan, M Chen, S Goodman, K Gimpel",Highly cited,,,https://arxiv.org/abs/1909.11942,4795.00,ALBERT: A Lite BERT for Self-supervised Learning of Language Representations,2019-09-26,"Toyota Technological Institute at Chicago,Google Research",Industry - Academia Collaboration,18000000.00,Section 3.2 of paper,,,,,3300000000,"Pretraining same as for BERT - Wikipedia and BookCorpus

""For the pre-training corpus we
use the BooksCorpus (800M words) (Zhu et al., 2015) and English Wikipedia (2,500M words)""",,22500000000.00,"Rados dataset  (FLOPs)
https://drive.google.com/drive/folders/1bhy5z6hh1n3wCHx6528Xb7xB1KhYdAL1",,,Google TPU v3,Google TPU v3,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,"United States of America,Multinational","Toyota Technological Institute at Chicago, Google Research",,,,ALBERT,,,
AlphaX-1,Vision,Neural architecture search for computer vision,"Linnan Wang, Yiyang Zhao, Yuu Jinnai, Yuandong Tian, Rodrigo Fonseca1",SOTA Improvement,"""In 12 GPU days and 1000 samples, AlphaX found an architecture that reaches 97.84\% top-1 accuracy on CIFAR-10, and 75.5\% top-1 accuracy on ImageNet, exceeding SOTA NAS methods in both the accuracy and sampling efficiency""",,https://arxiv.org/abs/1903.11059,81.00,AlphaX: eXploring Neural Architectures with Deep Neural Networks and Monte Carlo Tree Search,2019-10-02,"Facebook AI Research,Brown University",Industry - Academia Collaboration (Academia leaning),579000000.00,,7600000000000000000.00,,ImageNet,,,,,,,,,NVIDIA Geforce GTX1080 Ti,NVIDIA Geforce GTX1080 Ti,,$24.10,,Industry,,,2023-12-05 04:33,Robi Rahman,,,"Multinational,United States of America","Facebook AI Research, Brown University",,,,AlphaX-1,,,
DistilBERT,Language,Text autocompletion,"Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf",Highly cited,,,https://arxiv.org/abs/1910.01108,4583.00,"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",2019-10-02,Hugging Face,Industry,66000000.00,Table 3,12441600000000000000.00,"Section 3: DistilBERT was trained on 8 16GB V100 GPUs for approximately 90 hours.

1.6e13*8*60**2*90*0.3 = 1.2e19",,"Section 3: We train DistilBERT on the same corpus as the original BERT model: a concatenation of English Wikipedia and Toronto Book Corpus [Zhu et al., 2015].",,,,,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,Multinational,Hugging Face,,,,DistilBERT,,,
dense-IndRNN+dynamic eval,Language,,"Shuai Li, Wanqing Li, Chris Cook, Yanbo Gao",,,,https://arxiv.org/abs/1910.06251,45.00,Deep Independently Recurrent Neural Network (IndRNN),2019-10-11,,,44100000.00,,,,Penn TreeBank,,,,,,,,,,,,,,,,,2023-11-21 01:39,Robi Rahman,dense-IndRNN+dynamic eval,,,,,,,,,,
Rubik's cube ADR robot,Robotics,,"Ilge Akkaya, Marcin Andrychowicz, Maciek Chociej, Mateusz Litwin, Bob McGrew, Arthur Petron, Alex Paino, Matthias Plappert, Glenn Powell, Raphael Ribas, Jonas Schneider, Nikolas Tezak, Jerry Tworek, Peter Welinder, Lilian Weng, Qiming Yuan, Wojciech Zaremba, Lei Zhang
",,,,https://arxiv.org/abs/1910.07113,914.00,Solving Rubik’s Cube with a Robot Hand,2019-10-15,OpenAI,Industry,27769565.00,"Table 13 on pg. 44 of the Cube paper, saved in ""RL papers"" folder. Sum of all the trainable parameters (dominated by the value and policy networks).

source: https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389",854000000000000000000.00,source: https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389,,,62400000,""" The cumulative amount of experience over that period used for training on the
Rubik’s cube is roughly 13 thousand years, which is on the same order of magnitude as the 40 thousand years used by
OpenAI Five""

13/40 * 1.92e8 = 6.24e7",,,,,,NVIDIA Tesla V100 DGXS 32 GB,NVIDIA Tesla V100 DGXS 32 GB,,$3102.27,,Industry,,"We demonstrate that models trained only in simulation can be used to solve a manipulation problem of unprecedented complexity on a real robot. This is made possible by two key components: a novel algorithm, which we call automatic domain randomization (ADR) and a robot platform built for machine learning. ADR automatically generates a distribution over randomized environments of ever-increasing difficulty. Control policies and vision state estimators trained with ADR exhibit vastly improved sim2real transfer. For control policies, memory-augmented models trained on an ADR-generated distribution of environments show clear signs of emergent meta-learning at test time. The combination of ADR with our custom robot platform allows us to solve a Rubik's cube with a humanoid robot hand, which involves both control and state estimation problems. Videos summarizing our results are available: this https URL",2023-12-05 04:33,Robi Rahman,,,United States of America,OpenAI,,,,,,,
LSTM(large)+Sememe+cell,Language,,"Yujia Qin, Fanchao Qi, Sicong Ouyang, Zhiyuan Liu, Cheng Yang, Yasheng Wang, Qun Liu, Maosong Sun",,,,https://arxiv.org/pdf/1910.08910,19.00,Improving Sequence Modeling Ability of Recurrent Neural Networks via Sememes,2019-10-20,,,48000000.00,,24000000000000000.00,,,,,,40.00,,,,,,,,,,,,,2023-11-23 06:53,Robi Rahman,LSTM(large)+Sememe+cell,,,,,,,,,,
LSTM(medium)+Sememe+cell,Language,,,,,,,,,2019-10-20,,,,,,,,,,,40.00,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,LSTM(medium)+Sememe+cell,1,,,,,,,,,
T5-3B,Language,Text autocompletion,"Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu",Highly cited,,,https://arxiv.org/abs/1910.10683,10800.00,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,2019-10-23,Google,Industry,2800000000.00,"page 37, 3B and 11B. ""To further explore what kind of performance is possible when using larger models, we consider two additional variants. In both cases, we use d_model = 1024, a 24 layer encoder and decoder, and dkv = 128. For the “3B” variant, we use dff = 16,384 with 32-headed attention, which results in around 2.8 billion parameters; for “11B” we use dff = 65,536 with 128-headed attention producing a model with about 11 billion parameters""",865865406873600000000.00,"Akronomicon states 1.04e+22 FLOP. Archived source: https://github.com/lightonai/akronomicon/tree/main/akrodb
However, this seems dubiously high.

""We pre-train each model for 2^19 = 524,288 steps on C4 before fine-tuning.""
""In total, this batch size and number of steps corresponds to pre-training on 2^35 ≈ 34B tokens.""
""To compare these mixing strategies on equal footing with our baseline pre-train-then-fine-tune results, we train multi-task models for the same total number of steps: 2^19 + 2^18 = 786,432""
Using the 6DN approximation gives: 6 FLOP/token/param * 2^35 pretrain tokens * (1+1/2 finetune tokens per pretrain token) * 1 iteration of training data* 2.8 billion parameters = 8.659e20 FLOP
https://www.wolframalpha.com/input?i=6+*+2%5E35+*+2.8+billion+*+1.5",C4,,25500000000,"""This produces a collection of text that is not only orders of magnitude larger than most data sets used for pre-training (about 750 GB) but also
comprises reasonably clean and natural English text. We dub this data set the “Colossal Clean Crawled Corpus” (or C4 for short) and release it as part of TensorFlow Datasets""
750GB * 200M word/GB = 1.5e11

""In total, this batch size and number of steps corresponds to pre-training on 2^35 ≈ 34B tokens.""
""Note that 2^35 tokens only covers a fraction of the entire C4 data set, so we never repeat any data during pre-training.""
The fraction is 25.5 billion / 150 billion = 0.17 epochs.",0.17,,,,,Google TPU v3,Google TPU v3,Self-supervised learning,,,Industry,,"Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.",2023-12-05 10:33,Robi Rahman,,,Multinational,Google,,,,T5-3B,,,T5-3B
T5-11B,Language,Text autocompletion,"Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu",Highly cited,,,https://arxiv.org/abs/1910.10683,10800.00,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,2019-10-23,Google,Industry,11000000000.00,The full 11-billion parameter model,4.05e+22,"https://arxiv.org/ftp/arxiv/papers/2104/2104.10350.pdf
Table 4",C4,,150000000000,"""This produces a collection of text that is not only
orders of magnitude larger than most data sets used for pre-training (about 750 GB) but also
comprises reasonably clean and natural English text. We dub this data set the “Colossal
Clean Crawled Corpus” (or C4 for short) and release it as part of TensorFlow Datasets""

750GB * 200M word/GB = 1.5e11",,,,481.9,"4.05*10^22 FLOP at 37.073% utilization on 512 TPU v3 chips (123 TFLOPS) -> 482 hours
https://www.wolframalpha.com/input?i=4.05*10%5E22+seconds+%2F+%28512*123*10%5E12%29+*%28123%2F45.6%29",Google TPU v3,Google TPU v3,Self-supervised learning,$105686.20,,Industry,Confident,"Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.",2023-12-05 04:33,Robi Rahman,,,Multinational,Google,,,,T5-11B,512,0.3707,T5-11B
Amended-DARTS,Language,,"Kaifeng Bi, Changping Hu, Lingxi Xie, Xin Chen, Longhui Wei, Qi Tian",,,,https://arxiv.org/pdf/1910.11831,48.00,Stabilizing DARTS with Amended Gradient Estimation on Architectural Parameters,2019-10-25,,Industry - Academia Collaboration,23000000.00,,,,,,,,,,,,,,,,,,,,,2023-11-21 01:39,Robi Rahman,Amended-DARTS,,,,,,,,,,
BART-large,Language,,"Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer",Highly cited,,,https://arxiv.org/abs/1910.13461,6655.00,"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",2019-10-29,Facebook AI,Industry,406291456.00,"""In total, BART contains roughly 10% more parameters than the equivalently sized BERT model.""

I counted the parameters in the huggingface model
https://huggingface.co/facebook/bart-large/tree/main

from transformers import AutoTokenizer, AutoModel
tokenizer = AutoTokenizer.from_pretrained(""facebook/bart-large"")
model = AutoModel.from_pretrained(""facebook/bart-large"")
sum(p.numel() for p in model.parameters() if p.requires_grad)",,,,,,,,,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,Multinational,Facebook AI,,,,BART-large,,,
AlphaStar,Games,StarCraft,"Oriol Vinyals,Igor Babuschkin,Wojciech M. Czarnecki,Michaël Mathieu,Andrew Dudzik,Junyoung Chung,David H. Choi,Richard Powell,Timo Ewalds,Petko Georgiev,Junhyuk Oh,Dan Horgan,Manuel Kroiss,Ivo Danihelka,Aja Huang,Laurent Sifre,Trevor Cai,John P. Agapiou,Max Jaderberg,Alexander S. Vezhnevets,Rémi Leblond,Tobias Pohlen,Valentin Dalibard,David Budden,Yury Sulsky,James Molloy,Tom L. Paine,Caglar Gulcehre,Ziyu Wang,Tobias Pfaff,Yuhuai Wu,Roman Ring,Dani Yogatama,Dario Wünsch,Katrina McKinney,Oliver Smith,Tom Schaul,Timothy Lillicrap,Koray Kavukcuoglu,Demis Hassabis,Chris Apps,David Silver",Highly cited,,,https://www.deepmind.com/blog/alphastar-grandmaster-level-in-starcraft-ii-using-multi-agent-reinforcement-learning,2681.00,Grandmaster level in StarCraft II using multi-agent reinforcement learning,2019-10-30,DeepMind,Industry,139000000.00,"AlphaStar has 139 million weights, but only 55 million weights are required during inference.",5.9250000000001e+22,"384 TPUv3 chips for 44 days. Assume 33% utilization.
https://www.wolframalpha.com/input?i=123+teraFLOPS+*+384+*+0.33+*+44+days",,,,"Multiple data types. First supervised learning, then other stuff",,,,1056.0,"""Each agent was trained using 32 third-generation tensor
processing units (TPUs) over 44 days""",Google TPU v3,Google TPU v3,,$512765.27,,Industry,,"Many real-world applications require artificial agents to compete and coordinate with other agents in complex environments. As a stepping stone to this goal, the domain of StarCraft has emerged as an important challenge for artificial intelligence research, owing to its iconic and enduring status among the most difficult professional esports and its relevance to the real world in terms of its raw complexity and multi-agent challenges. Over the course of a decade and numerous competitions1–3, the strongest agents have simplified important aspects of the game, utilized superhuman capabilities, or employed hand-crafted sub-systems4. Despite these advantages, no previous agent has come close to matching the overall skill of top StarCraft players. We chose to address the challenge of StarCraft using generalpurpose learning methods that are in principle applicable to other complex domains: a multi-agent reinforcement learning algorithm that uses data from both human and agent games within a diverse league of continually adapting strategies and counter-strategies, each represented by deep neural networks5,6. We evaluated our agent, AlphaStar, in the full game of StarCraft II, through a series of online games against human players. AlphaStar was rated at Grandmaster level for all three StarCraft races and above 99.8% of officially ranked human players.",2023-12-05 04:33,Robi Rahman,,,United Kingdom of Great Britain and Northern Ireland,DeepMind,,,,AlphaStar,384,,AlphaStar
Base LM + kNN LM + Continuous Cache,Language,,"Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, Mike Lewis",SOTA Improvement,"""GNN-LM achieves a new state-of-the-art perplexity of 14.8 on WikiText-103""",,https://arxiv.org/abs/1911.00172,446.00,Generalization through Memorization: Nearest Neighbor Language Models,2019-11-01,"Stanford University,Facebook AI Research",Academia,247000000.00,,7300000000000000000.00,,WikiText-103,,,,200.00,,,,,,,,,,,,,2023-12-05 04:33,Robi Rahman,Base LM + kNN LM + Continuous Cache,,"United States of America,Multinational","Stanford University, Facebook AI Research",,,,Base LM + kNN LM + Continuous Cache,,,
Sandwich Transformer,Language,,"Ofir Press, Noah A. Smith, Omer Levy",SOTA Improvement,"""Table 4 shows that the sandwich transformer transfers well to the books domain, improving
performance by 1.06 perplexity, achieving similar performance to the datastore-augmented kNN-LM
(Khandelwal et al., 2019), which is the state of the art on WikiText-103"" (Sandwich is slightly better per table)",,https://arxiv.org/abs/1911.03864,65.00,Improving Transformer Models by Reordering their Sublayers,2019-11-10,"Allen Institute for AI,Facebook AI Research",,209000000.00,,158000000000000000000.00,,Toronto Books Corpus,,,,180.00,,,,,,,,,,,,,2023-12-05 04:33,Robi Rahman,Sandwich Transformer,,"United States of America,Multinational","Allen Institute for AI, Facebook AI Research",,,,Sandwich Transformer,,,
Noisy Student (L2),Vision,Image classification,"Q Xie, MT Luong, E Hovy, QV Lee",SOTA Improvement,"""Noisy Student Training achieves 88.4% top-1 accuracy on ImageNet, which is 2.0% better than the state-of-the-art model""",,https://paperswithcode.com/paper/self-training-with-noisy-student-improves/review/,1833.00,Self-training with Noisy Student improves ImageNet classification,2019-11-11,"Carnegie Mellon University (CMU),Google",Industry - Academia Collaboration (Industry leaning),480000000.00,,849346560000000000000.00,"""Our largest model, EfficientNet-L2, needs to be trained for 6 days on a Cloud TPU v3 Pod, which has 2048 cores, if the unlabeled batch size is 14x the labeled batch size""

2048*4.00E+12*60**2*24*4*0.3 = 8.5e20","ImageNet, JFT",,81000000,"""Due to duplications, there are only 81M unique images among these 130M images.""",,1040000000000.00,"Rados (FLOPs)
https://drive.google.com/drive/folders/1bhy5z6hh1n3wCHx6528Xb7xB1KhYdAL1",144.0,"""Xie et al. (2020) required 33 TPUv3 core-years to train their Noisy Student EfficientNet-L2""

https://arxiv.org/abs/2103.00020",,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,"United States of America,Multinational","Carnegie Mellon University (CMU), Google",,,,Noisy Student (L2),,,
MoCo,Drawing,Image completion,"Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xe, Ross Girshick",Highly cited,,,https://arxiv.org/abs/1911.05722,7973.00,Momentum Contrast for Unsupervised Visual Representation Learning,2019-11-13,Facebook AI,Industry,375000000.00,https://openai.com/blog/image-gpt/#rfref53,,,,,,,,,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,Multinational,Facebook AI,,,,MoCo,,,
Compressive Transformers for Long-Range Sequence Modelling,Language,,"Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Timothy P. Lillicrap",,,,https://arxiv.org/abs/1911.05507,357.00,Compressive Transformers for Long-Range Sequence Modelling,2019-11-13,,Industry,,,160000000000000000000.00,,WikiText-103,,,,328.32,,,,,,,,,,,,,2023-12-05 04:33,Robi Rahman,Compressive Transformers for Long-Range Sequence Modelling,,,,,,,,,,
MuZero,Games,Atari Games,"J Schrittwieser, I Antonoglou, T Hubert, K Simonyan",SOTA improvement,,,https://arxiv.org/abs/1911.08265v2,1357.00,Mastering Atari Go Chess and Shogi by Planning with a Learned Model,2019-11-19,DeepMind,Industry,36864000.00,"Both the representation and dynamics function use the same architecture asAlphaZero, but with 16 instead of20 residual blocks [15]. We use 3x3 kernels and 256 hidden planes for each convolution.

Previous downsampling:
•  1 convolution with stride 2 and 128 output planes, output resolution 48x48.•  2 residual blocks with 128 planes•  1 convolution with stride 2 and 256 output planes, output resolution 24x24.•  3 residual blocks with 256 planes.•  Average pooling with stride 2, output resolution 12x12.•  3 residual blocks with 256 planes.•  Average pooling with stride 2, output resolution 6x6.",48000000000000000000.00,"third-generation Google Cloud TPU
(For each board game, we used 16 TPUs for training and 1000 TPUs for self-play)
For each game in Atari, we used 8 TPUs for training and 32 TPUs for self-play
Training for 12 hours (for Atari)
Data from Parameter, Compute and Data Trends in Machine Learning
Google v3 TPU: 1.23E+14 FLOP/s (although with the caveat that it might be not applicable)
Utilization rate 
In LaMDA: Language Models for Dialog Applications, they report for TPU V3: 56.5%
Calculations for Atari:
12 hours → 43200 seconds
(8 TPUs for training) * (1.23*10^14 FLOP/s) * (43.2 *10^3 s) * (0.565 utilization rate) = 2.4017472 * 10^19 FLOP
Training time missing for boardgames
Assumption also 12 hours 
Also: 2.4017472 * 10^19 FLOP
Total cost ≈ 4.8 * 10^19 FLOP",,,20000000000,"Table 1
https://arxiv.org/pdf/1911.08265.pdf",,,,,,,,Self-supervised learning,$121.18,,Industry,,"Constructing agents with planning capabilities has long been one of the main challenges in the pursuit of artificial intelligence. Tree-based planning methods have enjoyed huge success in challenging domains, such as chess and Go, where a perfect simulator is available. However, in real-world problems the dynamics governing the environment are often complex and unknown. In this work we present the MuZero algorithm which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging and visually complex domains, without any knowledge of their underlying dynamics. MuZero learns a model that, when applied iteratively, predicts the quantities most directly relevant to planning: the reward, the action-selection policy, and the value function. When evaluated on 57 different Atari games - the canonical video game environment for testing AI techniques, in which model-based planning approaches have historically struggled - our new algorithm achieved a new state of the art. When evaluated on Go, chess and shogi, without any knowledge of the game rules, MuZero matched the superhuman performance of the AlphaZero algorithm that was supplied with the game rules.",2023-12-05 04:33,Robi Rahman,,,United Kingdom of Great Britain and Northern Ireland,DeepMind,,,,MuZero,,,
Transformer - LibriVox + Decoding/Rescoring,Speech,Speech recognition,"Gabriel Synnaeve, Qiantong Xu, Jacob Kahn, Tatiana Likhomanenko, Edouard Grave, Vineel Pratap, Anuroop Sriram, Vitaliy Liptchinsky, Ronan Collobert",SOTA Improvement,"""Results with decoding/rescoring are shown in Table 2, where we reach 2.09% and 4.11% on test-clean and test-other , respectively, and are further improvements on the state-of-the-art.""",,https://arxiv.org/abs/1911.08460v3,233.00,End-to-end ASR: from Supervised to Semi-Supervised Learning with Modern Architectures,2019-11-19,Facebook,Industry,296000000.00,,,"""Models are trained on 64 GPUs each with an overall batch size of 256 for ResNet and TDS and 320 for Transformer. With only LIBRISPEECH, all models converged in under a week; with pseudo-labels from LIBRIVOX, training required 2-3 weeks""

GPU not specified","LibriSpeech, LibriVox",,,,,,,,,,,,,,,Likely,"We study pseudo-labeling for the semi-supervised training of ResNet, Time-Depth Separable ConvNets, and Transformers for speech recognition, with either CTC or Seq2Seq loss functions. We perform experiments on the standard LibriSpeech dataset, and leverage additional unlabeled data from LibriVox through pseudo-labeling. We show that while Transformer-based acoustic models have superior performance with the supervised dataset alone, semi-supervision improves all models across architectures and loss functions and bridges much of the performance gaps between them. In doing so, we reach a new state-of-the-art for end-to-end acoustic models decoded with an external language model in the standard supervised learning setting, and a new absolute state-of-the-art with semi-supervised training. Finally, we study the effect of leveraging different amounts of unlabeled audio, propose several ways of evaluating the characteristics of unlabeled audio which improve acoustic modeling, and show that acoustic models trained with more audio rely less on external language models.",2023-11-26 03:25,Anonymous,,,Multinational,Facebook,,,,Transformer - LibriVox + Decoding/Rescoring,,,
Photo-Geometric Autoencoder,Vision,,"Shangzhe Wu, Christian Rupprecht, Andrea Vedaldi
",SOTA Improvement,"""Our model outperforms a
current state-of-the-art 3D reconstruction method that uses 2D keypoint supervision""",,https://arxiv.org/abs/1911.11130,251.00,Unsupervised Learning of Probably Symmetric Deformable 3D Objects From Images in the Wild,2019-11-25,University of Oxford,Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-12-05 04:33,Robi Rahman,,,United Kingdom of Great Britain and Northern Ireland,University of Oxford,,,,Photo-Geometric Autoencoder,,,
Transformer-XL DeFINE (141M),Language,,"Sachin Mehta, Rik Koncel-Kedziorski, Mohammad Rastegari, Hannaneh Hajishirzi",SOTA Improvement,"""Compared to state-of-the-art methods including adaptive input representations,
this technique results in a 6% to 20% drop in perplexity""",,https://arxiv.org/abs/1911.12385,21.00,DeFINE: DEep Factorized INput Token Embeddings for Neural Sequence Modeling,2019-11-27,"University of Washington,Allen Institute for AI",,141000000.00,,6200000000000000000.00,,WikiText-103; Penn Treebank,,,,20.00,,,,,,,,,,,,,2023-11-26 03:25,Robi Rahman,Transformer-XL DeFINE (141M),,"United States of America,United States of America","University of Washington, Allen Institute for AI",,,,Transformer-XL DeFINE (141M),,,
Transformer-XL DeFINE (107M),Language,,,,,,,,,2019-11-27,,,,,,,,,,,20.00,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,Transformer-XL DeFINE (107M),1,,,,,,,,,
Adaptive LSTM + DeFINE,Language,,,,,,,,,2019-11-27,,,,,,,,,,,20.00,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,Adaptive LSTM + DeFINE,1,,,,,,,,,
AWD-LSTM + DeFINE,Language,,,,,,,,,2019-11-27,,,,,,,,,,,20.00,,,,,,,,,,,Unverified,,2023-12-08 01:01,Robi Rahman,AWD-LSTM + DeFINE,1,,,,,,,,,
bRSM + cache,Language,,"Jeremy Gordon, David Rawlinson, Subutai Ahmad",,,,https://arxiv.org/abs/1912.01116,4.00,Long Distance Relationships without Time Travel: Boosting the Performance of a Sparse Predictive Autoencoder in Sequence Modeling,2019-12-02,,,2550000.00,,213000000000000.00,,,,,,15.00,,,,,,,,,,,,,2023-11-23 06:53,Robi Rahman,bRSM + cache,,,,,,,,,,
StarGAN v2,Drawing,,"Yunjey Choi, Youngjung Uh, Jaejun Yoo, Jung-Woo Ha",SOTA Improvement,"""Votes from AMT workers for the most preferred method
regarding visual quality and style reflection (%). StarGAN v2 outperforms the baselines with remarkable margins in all aspects.""",,https://arxiv.org/abs/1912.01865,1318.00,StarGAN v2: Diverse Image Synthesis for Multiple Domains,2019-12-04,"NAVER,Yonsei University,Swiss Federal Institute of Technology",Industry - Academia Collaboration (Industry leaning),,,,,,,,,,,,,,,,,,,Industry,,,2023-11-26 03:25,Robi Rahman,,,"Korea (Republic of),Korea (Republic of),Switzerland","NAVER, Yonsei University, Swiss Federal Institute of Technology",,,,StarGAN v2,,,
MMLSTM,Language,,"Kai Shuang, Rui Li, Mengyu Gu, Jonathan Loo, Sen Su",SOTA Improvement,"""In experiments, we demonstrate the language model with MMLSTMs surpasses the existing state-of-the-art model on Penn Treebank (PTB) and WikiText-2 (WT2) datasets""",,http://repository.uwl.ac.uk/id/eprint/6490/1/Loo_etal_IEEE_TNNLS_2019_Major-minor_long_short-term_memory_for_word-level_language_model.pdf,14.00,Major–Minor Long Short-Term Memory for Word-Level Language Model,2019-12-05,"Beijing University of Posts and Telecommunications,University of West London",,75000000.00,,2320000000000000000.00,,WikiText-103,,,,50.00,,,,,,,,,,,,,2023-11-26 03:25,Robi Rahman,MMLSTM,,"China,United Kingdom of Great Britain and Northern Ireland","Beijing University of Posts and Telecommunications, University of West London",,,,MMLSTM,,,
OpenAI Five,Games,Dota 2,"Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemysław Dębiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, Rafal Józefowicz, Scott Gray, Catherine Olsson, Jakub Pachocki, Michael Petrov, Henrique P. d.O. Pinto, Jonathan Raiman, Tim Salimans, Jeremy Schlatter, Jonas Schneider, Szymon Sidor, Ilya Sutskever, Jie Tang, Filip Wolski, Susan Zhang",SOTA improvement,"""On April 13th, 2019, OpenAI Five became the first AI system to defeat the world champions at an esports game.""",,https://arxiv.org/abs/1912.06680,1303.00,Dota 2 with Large Scale Deep Reinforcement Learning,2019-12-13,OpenAI,Industry,159000000.00,"""We define a policy (π) as a function from the history of observations to a probability distribution over actions, which we parameterize as a recurrent neural network with approximately 159 million parameters (θ)."" pg. 3 of paper

source: https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389",6.7e+22,"""770±50 PFlops/s·days of compute"" for the model that played against world champions. They did a single training run that took 10 months.

While the model was playing against world champions, they continued training for a few days, so that the resulting model used even more training compute: 820±50 PFlops/s·days.

Finally, they also trained a Rerun model with 150±5 PFlops/s·days of compute.

Source: Dota 2 with Large Scale Deep Reinforcement Learning
https://arxiv.org/abs/1912.06680",,,454321373184,"""Although the Dota 2 engine runs at 30 frames per second, OpenAI Five only acts on every 4th
frame which we call a timestep""
--> 7.5 timesteps/s

""OpenAI Five is a single training run that ran from June 30th, 2018 to April 22nd, 2019. "" --> 296 days

296 * 24*3600 * 7.5 = 1.92e8

This number seems a little low? The DQN paper had 1e7 timesteps. Might be to do with sample efficiency?

EDIT 14/06/2022
Multiple copies of OpenAI Five were trained in parallel, so the total training time is much higher than 296 days.
Table 1 shows 220,000 GPU iterations, each iteration has a batch size of between 1M and 3M timesteps (Table 2), so the total number of episodes is on the order of 2e11",,,,7104.0,"""OpenAI Five is a single training run that ran from June 30th, 2018 to April 22nd, 2019. "" --> 296 days",,,Self-supervised learning,$166042.11,"Cannot multiply the hardware quantity by training time to get the quantity of GPU-hours! Page 5: "" the number of GPUs (up to 1536 at the peak)""",Industry,Confident,"On April 13th, 2019, OpenAI Five became the first AI system to defeat the world champions at an esports game. The game of Dota 2 presents novel challenges for AI systems such as long time horizons, imperfect information, and complex, continuous state-action spaces, all challenges which will become increasingly central to more capable AI systems. OpenAI Five leveraged existing reinforcement learning techniques, scaled to learn from batches of approximately 2 million frames every 2 seconds. We developed a distributed training system and tools for continual training which allowed us to train OpenAI Five for 10 months. By defeating the Dota 2 world champion (Team OG), OpenAI Five demonstrates that self-play reinforcement learning can achieve superhuman performance on a difficult task.
",2023-12-05 04:33,Robi Rahman,,,United States of America,OpenAI,,,,OpenAI Five,1536,,OpenAI Five
OpenAI Five Rerun,Games,Dota 2,"Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung,
Przemysław “Psyho"" Dębiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, Rafal Józefowicz, Scott Gray, Catherine Olsson, Jakub Pachocki, Michael Petrov, Henrique Pondé de Oliveira Pinto, Jonathan Raiman, Tim Salimans, Jeremy Schlatter, Jonas Schneider, Szymon Sidor, Ilya Sutskever, Jie Tang, Filip Wolski, Susan Zhang",SOTA improvement,"""On April 13th, 2019, OpenAI Five became the first AI system to defeat the world champions at an esports game.""",,https://cdn.openai.com/dota-2.pdf,1303.00,Dota 2 with Large Scale Deep Reinforcement Learning,2019-12-13,OpenAI,Industry,159000000.00,"""We define a policy (π) as a function from the history of observations to a probability distribution
over actions, which we parameterize as a recurrent neural network with approximately 159 million
parameters (θ)."" pg. 3 of paper

source: https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389",1.3e+22,"THIS CALCULATION IS FOR RERUN

source: https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389",,,53084160000,"54k iterations (Fig 7)
with a batch size of 983040 (Table 2)",,,,,,,,Self-supervised learning,$32217.13,,Industry,,"On April 13th, 2019, OpenAI Five became the first AI system to defeat the world champions at an esports game. The game of Dota 2 presents novel challenges for AI systems such as long time horizons, imperfect information, and complex, continuous state-action spaces, all challenges which will become increasingly central to more capable AI systems. OpenAI Five leveraged existing reinforcement learning techniques, scaled to learn from batches of approximately 2 million frames every 2 seconds. We developed a distributed training system and tools for continual training which allowed us to train OpenAI Five for 10 months. By defeating the Dota 2 world champion (Team OG), OpenAI Five demonstrates that self-play reinforcement learning can achieve superhuman performance on a difficult task.",2023-12-05 04:33,Robi Rahman,,,United States of America,OpenAI,,,,OpenAI Five Rerun,512,,OpenAI Five Rerun
DD-PPO,Robotics,Object detection,"Erik Wijmans, Abhishek Kadian, Ari Morcos, Stefan Lee, Irfan Essa, Devi Parikh, Manolis Savva, Dhruv Batra",SOTA Improvement,"""This agent achieves state-of-art on the Habitat Challenge 2019 RGB track (rank 2 entry has 0.89 SPL).""",,https://openreview.net/forum?id=H1gX8C4YPr,326.00,DD-PPO: Learning Near-Perfect PointGoal Navigators from 2.5 Billion Frames,2019-12-19,"Georgia Institute of Technology,Facebook AI Research,Oregon State University,Simon Fraser University",Industry - Academia Collaboration (Industry leaning),,"no parameter count but some architecture details: ""The policy is parameterized by a 2-layer LSTM with a 512-dimensional hidden state. It takes three inputs: the previous action, the target relative to the current state, and the output of the visual encoder. The LSTM’s output is used to produce a softmax distribution over the action space and an estimate of the value function. See Appendix C for full details.""",780000000000000000000.00,"""Using DD-PPO, we train agents for 2.5 Billion steps of experience with 64 Tesla V100 GPUs in 2.75 days – 180 GPU-days of training""

125 teraFLOP/s (exact V100 model not specified) * 180 * 24 * 3600 * 0.4 (assumed utilization) = 7.8e20",,"""We experiment with several different sources of data. First, we utilize the training data released
as part of the Habitat Challenge 2019, consisting of 72 scenes from the Gibson dataset (Xia et al.,
2018). We then augment this with all 90 scenes in the Matterport3D dataset (Chang et al., 2017) to
create a larger training set (note that Matterport3D meshes tend to be larger and of better quality).2
Furthermore, Savva et al. (2019) curated the Gibson dataset by rating every mesh reconstruction on
a quality scale of 0 to 5 and then filtered all splits such that each only contains scenes with a rating of
4 or above (Gibson-4+), leaving all scenes with a lower rating previously unexplored. We examine
training on the 332 scenes from the original train split with a rating of 2 or above (Gibson-2+).""",,,,,,66.0,2.75 days,NVIDIA V100,NVIDIA V100,Reinforcement learning,,,,Likely,"We present Decentralized Distributed Proximal Policy Optimization (DD-PPO), a method for distributed reinforcement learning in resource-intensive simulated environments. DD-PPO is distributed (uses multiple machines), decentralized (lacks a centralized server), and synchronous (no computation is ever ""stale""), making it conceptually simple and easy to implement. In our experiments on training virtual robots to navigate in Habitat-Sim, DD-PPO exhibits near-linear scaling -- achieving a speedup of 107x on 128 GPUs over a serial implementation. We leverage this scaling to train an agent for 2.5 Billion steps of experience (the equivalent of 80 years of human experience) -- over 6 months of GPU-time training in under 3 days of wall-clock time with 64 GPUs. ",2023-12-05 04:33,Anonymous,,,"United States of America,Multinational,United States of America,Canada","Georgia Institute of Technology, Facebook AI Research, Oregon State University, Simon Fraser University",,,,DD-PPO,,,
Big Transfer (BiT-L),Vision,Image classification,"A Kolesnikov, L Beyer, X Zhai, J Puigcerver, J Yung",SOTA Improvement,"""We transfer BiT to many diverse tasks... These tasks include ImageNet’s ILSVRC-2012 [10], CIFAR-10/100 [27], Oxford-IIIT Pet [41], Oxford
Flowers-102 [39] (including few-shot variants), and the 1000-sample VTAB-1k benchmark [66], which consists of 19 diverse datasets. BiT-L attains state-ofthe-art performance on many of these tasks",,https://arxiv.org/abs/1912.11370,935.00,Large scale learning of general visual representations for transfer,2019-12-24,Google Brain,Industry,928000000.00,,,,,,,,40.00,,,,,Google TPU v3,Google TPU v3,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,Multinational,Google Brain,,,,Big Transfer (BiT-L),,,
"MicroNet (Adaptive, Cache)",Language,,"Zhongxia Yan, Hanrui Wang, Demi Guo, Song Han",,,,http://proceedings.mlr.press/v123/yan20a.html?ref=https://githubhelp.com,8.00,MicroNet for Efficient Language Modeling,2020-01-01,,Industry - Academia Collaboration,8300000.00,,,,WikiText-103,,,,,,,,,,,,,,,,,2023-11-21 01:39,Robi Rahman,"""MicroNet (Adaptive, Cache)""",,,,,,,,,,
AlphaFold,Other,Protein folding prediction,"Andrew W. Senior, Richard Evans, John Jumper, James Kirkpatrick, Laurent Sifre, Tim Green, Chongli Qin, Augustin Žídek, Alexander W. R. Nelson, Alex Bridgland, Hugo Penedones, Stig Petersen, Karen Simonyan, Steve Crossan, Pushmeet Kohli, David T. Jones, David Silver, Koray Kavukcuoglu, Demis Hassabis",SOTA improvement,,,https://www.nature.com/articles/s41586-019-1923-7,1957.00,Improved protein structure prediction using potentials from deep learning,2020-01-15,DeepMind,Industry,44826624.00,"p.13 of https://discovery.ucl.ac.uk/id/eprint/10089234/1/343019_3_art_0_py4t4l_convrt.pdf

“7 × 4 Blocks with 256 channels, cycling through dilations 1, 2, 4, 8”
“48 × 4 Blocks with 128 channels, cycling through dilations 1, 2, 4, 8”

Dilations don't change the number of parameters in each filter

Unclear what the ""projection"" layers are - just count convolution layer parameters.

Approximation: 7 * 4 * 256 * 3 * 3 * 256 + 48 * 4 * 128 * 3 * 3 * 128 = 44826624
",100000000000000000000.00,"Estimated in the blogpost below

https://www.lesswrong.com/posts/wfpdejMWog4vEDLDg/ai-and-compute-trend-isn-t-predictive-of-what-is-happening",,,,Multiple tasks! Different units,,,,,,,,Self-supervised learning,$241.59,,Industry,Speculative,"Protein structure prediction can be used to determine the three-dimensional shape of a protein from its amino acid sequence1. This problem is of fundamental importance as the structure of a protein largely determines its function2; however, protein structures can be difficult to determine experimentally. Considerable progress has recently been made by leveraging genetic information. It is possible to infer which amino acid residues are in contact by analysing covariation in homologous sequences, which aids in the prediction of protein structures3. Here we show that we can train a neural network to make accurate predictions of the distances between pairs of residues, which convey more information about the structure than contact predictions. Using this information, we construct a potential of mean force4 that can accurately describe the shape of a protein. We find that the resulting potential can be optimized by a simple gradient descent algorithm to generate structures without complex sampling procedures. The resulting system, named AlphaFold, achieves high accuracy, even for sequences with fewer homologous sequences. In the recent Critical Assessment of Protein Structure Prediction5 (CASP13)—a blind assessment of the state of the field—AlphaFold created high-accuracy structures (with template modelling (TM) scores6 of 0.7 or higher) for 24 out of 43 free modelling domains, whereas the next best method, which used sampling and contact information, achieved such accuracy for only 14 out of 43 domains. AlphaFold represents a considerable advance in protein-structure prediction. We expect this increased accuracy to enable insights into the function and malfunction of proteins, especially in cases for which no structures for homologous proteins have been experimentally determined7.",2023-12-05 04:33,Robi Rahman,,,United Kingdom of Great Britain and Northern Ireland,DeepMind,,,,AlphaFold,,,
ContextNet + Noisy Student,Speech,Speech recognition,"Daniel S. Park, Yu Zhang, Ye Jia, Wei Han, Chung-Cheng Chiu, Bo Li, Yonghui Wu, Quoc V. Le",SOTA Improvement,"""We are thus able to improve upon the previous state-of-the-art clean/noisy test WERs achieved on LibriSpeech 100h (4.74%/12.20%) and LibriSpeech (1.9%/4.1%)""",,https://arxiv.org/abs/2005.09629v2,217.00,Improved Noisy Student Training for Automatic Speech Recognition,2020-01-19,Google,Industry,,,5.4e+21,"""Five generations of models numbered 0 to 4, are trained,
where the baseline model is taken to be the generation-zero
model. The baseline ContextNet model has the same encoder
as CN-2, but has a one-layer RNN decoder with dimension 640.
Meanwhile, CN-w with w=1.25, 1.75, 2.25 and 2.5 have been
set to be the ASR model from generation 1 through 4. Each
generation is trained on 128 Google Cloud TPU chips for 1-3
days""

Roughly assuming each generation is an average of 2 days. The TPU version is likely v3 given this is a 2020 paper.

we get 128 * 10 * 24 * 3600 * 123 tflops * 0.4  (assumed utilization) = 5.4e21","LibriSpeech, LibriLight ",,,,,,,240.0,roughly 10 days,Google TPU v3,Google TPU v3,,,,,Likely,"Recently, a semi-supervised learning method known as ""noisy student training"" has been shown to improve image classification performance of deep networks significantly. Noisy student training is an iterative self-training method that leverages augmentation to improve network performance. In this work, we adapt and improve noisy student training for automatic speech recognition, employing (adaptive) SpecAugment as the augmentation method. We find effective methods to filter, balance and augment the data generated in between self-training iterations. By doing so, we are able to obtain word error rates (WERs) 4.2%/8.6% on the clean/noisy LibriSpeech test sets by only using the clean 100h subset of LibriSpeech as the supervised set and the rest (860h) as the unlabeled set. Furthermore, we are able to achieve WERs 1.7%/3.4% on the clean/noisy LibriSpeech test sets by using the unlab-60k subset of LibriLight as the unlabeled set for LibriSpeech 960h. We are thus able to improve upon the previous state-of-the-art clean/noisy test WERs achieved on LibriSpeech 100h (4.74%/12.20%) and LibriSpeech (1.9%/4.1%).",2023-11-26 03:25,Anonymous,,,Multinational,Google,,,,ContextNet + Noisy Student,,,
Meena,Language,Text autocompletion,"Dongling Xiao, Han Zhang, Yukun Li, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang",SOTA Improvement,"""We also propose a human evaluation metric called Sensibleness and
Specificity Average (SSA)... the full version of Meena (with a filtering mechanism and
tuned decoding) scores 79% SSA, 23% higher in absolute SSA than the existing chatbots we evaluated""",,https://arxiv.org/abs/2001.09977,747.00,Towards a Human-like Open-Domain Chatbot,2020-01-28,Google Brain,Industry,2600000000.00,"""We present Meena, a multi-turn open-domain chatbot trained end-to-end on data mined and filtered from public domain social media conversations. This 2.6B parameter neural network is simply trained to minimize perplexity of the next token.""",1.12e+23,"https://arxiv.org/ftp/arxiv/papers/2104/2104.10350.pdf
Table 4",,,40000000000,"""The final Meena dataset contains 341GB of text
(40B words)""

Converting from GB to words yields 6.8e10, which is in the same OOM",,,,720.0,"We trained our best model for 30 days on a TPUv3 Pod (2,048 TPU cores)",Google TPU v3,Google TPU v3,Self-supervised learning,$263099.94,,Industry,,"We present Meena, a multi-turn open-domain chatbot trained end-to-end on data mined and filtered from public domain social media conversations. This 2.6B parameter neural network is simply trained to minimize perplexity of the next token. We also propose a human evaluation metric called Sensibleness and Specificity Average (SSA), which captures key elements of a human-like multi-turn conversation. Our experiments show strong correlation between perplexity and SSA. The fact that the best perplexity end-to-end trained Meena scores high on SSA (72% on multi-turn evaluation) suggests that a human-level SSA of 86% is potentially within reach if we can better optimize perplexity. Additionally, the full version of Meena (with a filtering mechanism and tuned decoding) scores 79% SSA, 23% higher in absolute SSA than the existing chatbots we evaluated.",2023-12-05 04:33,Robi Rahman,,,Multinational,Google Brain,,,,Meena,1024,0.3439,Meena
Theseus 6/768,Language,Text autocompletion,"Canwen Xu, Wangchunshu Zhou, Tao Ge, Furu Wei, Ming Zhou",SOTA Improvement,"""Our approach outperforms existing knowledge distillation approaches on GLUE benchmark""",,https://arxiv.org/abs/2002.02925,160.00,BERT-of-Theseus: Compressing BERT by Progressive Module Replacing,2020-02-07,"UC San Diego,Beihang University,Microsoft",Industry - Academia Collaboration,66000000.00,"Rados, also specified in Table 1 in the paper",,,,,,,,11300000000.00,"Rados (FLOPs)
https://drive.google.com/drive/folders/1bhy5z6hh1n3wCHx6528Xb7xB1KhYdAL1",,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,"United States of America,China,Multinational","UC San Diego, Beihang University, Microsoft",,,,Theseus 6/768,,,
Perceiver IO,Multimodal,,"Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, Olivier Hénaff,
Matthew M. Botvinick, Andrew Zisserman, Oriol Vinyals, João Carreira",SOTA Improvement,"""Perceiver IO... achieves state-of-the-art performance on Sintel optical flow estimation""",,https://arxiv.org/abs/2107.14795,343.00,Perceiver IO: A General Architecture for Structured Inputs & Outputs,2020-02-08,DeepMind,Industry,,,,,,,,,,,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,United Kingdom of Great Britain and Northern Ireland,DeepMind,,,,Perceiver IO,,,
TaLK Convolution,Language,,"Vasileios Lioutas, Yuhong Guo",SOTA Improvement,"""[We] set a new state-of-the-art result on the
IWSLT De-En and CNN-DailyMail datasets""",,https://arxiv.org/abs/2002.03184,28.00,Time-aware Large Kernel Convolutions,2020-02-08,Carleton University,,240000000.00,,27800000000000000000.00,,WikiText-103,,,,187.43,,,,,,,,,,,,,2023-11-26 03:25,Robi Rahman,TaLK Convolution,,Canada,Carleton University,,,,TaLK Convolution,,,
ALBERT-xxlarge,Language,,"Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut",Highly cited,,,https://arxiv.org/abs/1909.11942,4795.00,ALBERT: A Lite BERT for Self-supervised Learning of Language Representations.,2020-02-09,"Toyota Technological Institute at Chicago,Google",Industry - Academia Collaboration,235000000.00,,2.39e+21,"32 hours of training
512 TPU V3s
0.33 utilization rate
",,,3300000000,"Pretraining same as for BERT - Wikipedia and BookCorpus

""For the pre-training corpus we
use the BooksCorpus (800M words) (Zhu et al., 2015) and English Wikipedia (2,500M words)""",,2500000000000.00,Source: https://github.com/amirgholami/ai_and_memory_wall,,,,,Self-supervised learning,$5924.43,,Industry,,"Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and \squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at this https URL.",2023-12-05 04:33,Robi Rahman,,,"United States of America,Multinational","Toyota Technological Institute at Chicago, Google",,,,ALBERT-xxlarge,,,
SimCLR,Drawing,Image completion,"Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton",Highly cited,,,https://arxiv.org/abs/2002.05709,11558.00,A Simple Framework for Contrastive Learning of Visual Representations,2020-02-13,Google Brain,Industry,375000000.00,source: https://openai.com/blog/image-gpt/,,,,,,,1000.00,,,,,Google TPU v3,Google TPU v3,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,Multinational,Google Brain,,,,SimCLR,,,
Turing-NLG,Language,Text autocompletion,Corby Rosset,SOTA Improvement,"from paper: ""Turing Natural Language Generation (T-NLG) is a 17 billion parameter language model by Microsoft that outperforms the state of the art on many downstream NLP tasks""",,https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/,114.00,Turing-NLG: A 17-billion-parameter language model by Microsoft,2020-02-13,Microsoft,Industry,17000000000.00,,1.57e+22,"source: https://lair.lighton.ai/akronomicon/

archived: https://github.com/lightonai/akronomicon/tree/main/akrodb",,,34800000000,"Authors say they pretrain on the same data as for Megatron-LM. 

From the Megatron-LM paper: https://arxiv.org/pdf/1909.08053.pdf

""The resulting aggregate
corpus contains 174 GB of deduplicated text.""

174GB * 2e8words/GB = 3.48e10 words",3.39,36000000000000.00,"Rados (FLOPs)
https://drive.google.com/drive/folders/1bhy5z6hh1n3wCHx6528Xb7xB1KhYdAL1",,,NVIDIA Tesla V100 DGXS 32 GB,NVIDIA Tesla V100 DGXS 32 GB,Self-supervised learning,$58395.62,,Industry,Likely,,2023-12-02 04:23,Robi Rahman,Turing-NLG,,Multinational,Microsoft,,,,Turing-NLG,256,,Turing-NLG
Feedback Transformer,Language,,"Angela Fan, Thibaut Lavril, Edouard Grave, Armand Joulin, Sainbayar Sukhbaatar",SOTA Improvement,"""As shown in Table 4, the Feedback
Transformer model achieves a new SOTA performance (on Enwiki8) of 0.96 bit-per-byte despite its small size.""",,https://arxiv.org/abs/2002.09402,41.00,Addressing Some Limitations of Transformers with Feedback Memory,2020-02-21,"LORIA,University of Lorraine,Facebook AI Research",,126000000.00,,44100000000000000000.00,,WikiText-103,,,,267.23,,,,,,,,,,,,,2023-11-26 03:25,Robi Rahman,Feedback Transformer,,"France,France,Multinational","LORIA, University of Lorraine, Facebook AI Research",,,,Feedback Transformer,,,
Temporal Convolutional Attention-based Network(TCAN) (WT2),Language,,"Hongyan Hao, Yan Wang, Yudi Xia, Jian Zhao, Furao Shen",SOTA Improvement,"""We improve the state-of-theart results of ... 9.20 on WikiText-2""",,https://arxiv.org/pdf/2002.12530,33.00,Temporal Convolutional Attention-based Network For Sequence Modeling,2020-02-28,"Nanjing University,Ant Group",,33000000.00,,,,,,,,,,,,,,,,,,,,,2023-11-26 03:25,Robi Rahman,Temporal Convolutional Attention-based Network(TCAN) (WT2),,"China,China","Nanjing University, Ant Group",,,,Temporal Convolutional Attention-based Network(TCAN) (WT2),,,
Temporal Convolutional Attention-based Network(TCAN) (PTB),Language,,,,,,,,,2020-02-28,Ant Group,,,,,,,,,,,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,Temporal Convolutional Attention-based Network(TCAN) (PTB),1,China,Ant Group,,,,,,,
LSTM-3-layer+Gadam,Language,,"Diego Granziol, Xingchen Wan, Samuel Albanie, Stephen Roberts",,,,https://arxiv.org/pdf/2003.01247,5.00,Iterative Averaging in the Quest for Best Test Error,2020-03-02,,Academia,24000000.00,,26800000000000000.00,,,,,,200.00,,,,,,,,,,,,,2023-11-23 06:53,Robi Rahman,LSTM-3-layer+Gadam,,,,,,,,,,
TransformerXL + spectrum control,Language,,"Lingxiao Wang, Jing Huang, Kevin Huang, Ziniu Hu, Guangtao Wang, Quanquan Gu",SOTA Improvement,"""We demonstrate that our spectrum control method outperforms the state-of-the-art Transformer-XL modeling for language model""",,https://openreview.net/forum?id=ByxY8CNtvr,55.00,Improving Neural Language Generation with Spectrum Control,2020-03-11,"University of California Los Angeles (UCLA),JD.com",,151000000.00,,459999999999999940.00,,WikiText-103,,,,250.00,,,,,,,,,,,,,2023-11-26 03:25,Robi Rahman,TransformerXL + spectrum control,,"United States of America,China","University of California Los Angeles (UCLA), JD.com",,,,TransformerXL + spectrum control,,,
Routing Transformer,Language,,"Aurko Roy, Mohammad Saffar, Ashish Vaswani, David Grangier",SOTA Improvement,"""Additionally, we set a new state-of-the-art on the newly released PG-19 data-set, obtaining a test perplexity of 33.2 with a 22 layer Routing Transformer model trained on sequences of length 8192""",,https://arxiv.org/abs/2003.05997,376.00,Efficient Content-Based Sparse Attention with Routing Transformers,2020-03-12,Google Research,,79500000.00,,,,WikiText-103,,,,,,,,,,,,,,,,,2023-12-05 04:33,Robi Rahman,Routing Transformer,,Multinational,Google Research,,,,Routing Transformer,,,
Local Transformer,Language,,,,,,,,,2020-03-12,,,,,,,,,,,,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,Local Transformer,1,,,,,,,,,
ProGen,Other,Protein generation,"Ali Madani, Bryan McCann, Nikhil Naik, Nitish Shirish Keskar, Namrata Anand, Raphael R. Eguchi,  View ORCID ProfilePo-Ssu Huang, Richard Socher",,,,https://www.biorxiv.org/content/10.1101/2020.03.07.982272v2,198.00,ProGen: Language Modeling for Protein Generation,2020-03-13,"Salesforce Research,Stanford University",Industry - Academia Collaboration,1200000000.00,"""We train a 1.2B-parameter language model, ProGen, on ∼280M protein sequences""",370000000000000000000.00,"Our model was implemented in TensorFlow (Abadi et al.,
2016) and trained with a global batch size of 64 distributed
across 256 cores of a Cloud TPU v3 Pod for 1M iterations. Training took approximately two weeks using Adagrad (Duchi et al., 2011)

4.00E+12*256*60**2*24*14*0.3 = 3.7e20",,,,,5.00,,,,,,,Self-supervised learning,$623.75,,Industry,,"Generative modeling for protein engineering is key to solving fundamental problems in synthetic biology, medicine, and material science. We pose protein engineering as an unsupervised sequence generation problem in order to leverage the exponentially growing set of proteins that lack costly, structural annotations. We train a 1.2B-parameter language model, ProGen, on ∼280M protein sequences conditioned on taxonomic and keyword tags such as molecular function and cellular component. This provides ProGen with an unprecedented range of evolutionary sequence diversity and allows it to generate with fine-grained control as demonstrated by metrics based on primary sequence similarity, secondary structure accuracy, and conformational energy.",2023-12-05 04:33,Robi Rahman,,,"United States of America,United States of America","Salesforce Research, Stanford University",,,,,,,
Tensor-Transformer(1core)+PN (WT103),Language,,"Sheng Shen, Zhewei Yao, Amir Gholami, Michael W. Mahoney, Kurt Keutzer",SOTA Improvement,"""The results are reported in Table 1. In the first section of
rows, we report state-of-the-art results for these two tasks with comparable model sizes""",,https://arxiv.org/pdf/2003.07845,60.00,PowerNorm: Rethinking Batch Normalization in Transformers,2020-03-17,UC Berkeley,,85300000.00,,1580000000000000000.00,,,,,,30.00,,,,,,,,,,,,,2023-11-26 03:25,Robi Rahman,Tensor-Transformer(1core)+PN (WT103),,United States of America,UC Berkeley,,,,Tensor-Transformer(1core)+PN (WT103),,,
Tensor-Transformer(1core)+PN (PTB),Language,,,,,,,,,2020-03-17,,,,,,,,,,,30.00,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,Tensor-Transformer(1core)+PN (PTB),1,,,,,,,,,
ELECTRA,Language,Text autocompletion,"Kevin Clark, Minh-Thang Luong, Quoc V. Le, Christopher D. Manning",Highly cited,,,https://arxiv.org/abs/2003.10555v1,2968.00,ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators,2020-03-23,"Stanford University,Google,Google Brain",Industry - Academia Collaboration,335000000.00,https://github.com/google-research/electra,3.0999999999999995e+21,"Table 8: ""ELECTRA-1.75M"" used 3.1e21 train FLOPs. Note that the actual parameter count is 335M. The 1.75M refers to the number of training steps.",,,,,,79000000000.00,"Rados (FLOPs)
https://drive.google.com/drive/folders/1bhy5z6hh1n3wCHx6528Xb7xB1KhYdAL1",,,,,Self-supervised learning,,,Industry,,"Masked language modeling (MLM) pre-training methods such as BERT corrupt the input by replacing some tokens with [MASK] and then train a model to reconstruct the original tokens. While they produce good results when transferred to downstream NLP tasks, they generally require large amounts of compute to be effective. As an alternative, we propose a more sample-efficient pre-training task called replaced token detection. Instead of masking the input, our approach corrupts it by replacing some tokens with plausible alternatives sampled from a small generator network. Then, instead of training a model that predicts the original identities of the corrupted tokens, we train a discriminative model that predicts whether each token in the corrupted input was replaced by a generator sample or not. Thorough experiments demonstrate this new pre-training task is more efficient than MLM because the task is defined over all input tokens rather than just the small subset that was masked out. As a result, the contextual representations learned by our approach substantially outperform the ones learned by BERT given the same model size, data, and compute. The gains are particularly strong for small models; for example, we train a model on one GPU for 4 days that outperforms GPT (trained using 30x more compute) on the GLUE natural language understanding benchmark. Our approach also works well at scale, where it performs comparably to RoBERTa and XLNet while using less than 1/4 of their compute and outperforms them when using the same amount of compute.",2023-11-26 03:25,Robi Rahman,,,"United States of America,Multinational,Multinational","Stanford University, Google, Google Brain",,,,ELECTRA,,,
MetNet,Other,Weather prediction,"Casper Kaae Sønderby, Lasse Espeholt, Jonathan Heek, Mostafa Dehghani, Avital Oliver, Tim Salimans, Shreya Agrawal, Jason Hickey, Nal Kalchbrenner",SOTA Improvement,"""MetNet improves upon the current operational NWP system HRRR for up to 8 hours of lead time""
... 
""Numerical Weather Prediction is the most successful framework to perform medium- and longrange (up to 6 days with high confidence) forecast to date (Bauer et al., 2015).""",,https://arxiv.org/abs/2003.12140,196.00,MetNet: A Neural Weather Model for Precipitation Forecasting,2020-03-24,Google,Industry,,,,,,,,,,,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,Multinational,Google,,,,MetNet,,,
Agent57,Games,Atari,"AP Badia, B Piot, S Kapturowski",SOTA Improvement,"""We propose Agent57, the first deep RL agent that outperforms the standard human benchmark on all 57 Atari games""",,https://arxiv.org/abs/2003.13350,397.00,Agent57: Outperforming the Atari Human Benchmark,2020-03-30,DeepMind,Industry,,,,,,,,,,,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,United Kingdom of Great Britain and Northern Ireland,DeepMind,,,,Agent57,,,
DiffStk-MRNN,Language,,"Ankur Mali, Alexander Ororbia, Daniel Kifer, Clyde Lee Giles",,,,https://arxiv.org/pdf/2004.07623,12.00,Recognizing Long Grammatical Sequences Using Recurrent Networks Augmented With An External Differentiable Stack,2020-04-04,,,1010000.00,,282000000000000.00,,,,,,50.00,,,,,,,,,,,,,2023-12-05 04:33,Robi Rahman,DiffStk-MRNN,,,,,,,,,,
MobileBERT,Language,Text autocompletion,"Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, Denny Zhou",,,,https://arxiv.org/abs/2004.02984,563.00,MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices,2020-04-06,"Carnegie Mellon University (CMU),Google Brain",Industry - Academia Collaboration (Industry leaning),25300000.00,Rados,,,,,,,,5360000000.00,"Rados (FLOPs)
https://drive.google.com/drive/folders/1bhy5z6hh1n3wCHx6528Xb7xB1KhYdAL1",,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,"United States of America,Multinational","Carnegie Mellon University (CMU), Google Brain",,,,,,,
CURL,Games,Atari Games,"A Srinivas, M Laskin, P Abbeel",SOTA improvement,,,https://arxiv.org/abs/2004.04136v4,767.00,CURL: Contrastive Unsupervised Representations for Reinforcement Learning,2020-04-08,UC Berkeley,Academia,907264.00,,,,,,,,,,,,,,,,,,Academia,,,2023-12-05 04:33,Robi Rahman,,,United States of America,UC Berkeley,,,,CURL,,,
Go-explore,Games,Atari,"A Ecoffet, J Huizinga, J Lehman, KO Stanley, J Clune",SOTA Improvement,"""GoExplore solves all heretofore unsolved Atari games (meaning those for which algorithms could not previously
outperform humans when evaluated following current community standards for Atari3) and surpasses the state
of the art on all hard-exploration games""",,https://arxiv.org/abs/2004.12919,240.00,"First return, then explore",2020-04-27,"Uber AI,OpenAI",Industry,,,,,,,,,,,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,"Multinational,United States of America","Uber AI, OpenAI",,,,Go-explore,,,
Once for All,Vision,,"Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, and Song Han",SOTA Improvement,"""In particular, OFA achieves a new SOTA 80.0% ImageNet top-1 accuracy under the mobile setting""",,https://arxiv.org/abs/1908.09791,918.00,Once for all: Train one network and specialize it for efficient deployment.,2020-04-29,"MIT-IBM Watson AI Lab,Massachusetts Institute of Technology (MIT),IBM",Industry,7700000.00,,1.7842809599999997e+21,"4.2k V100-hours (table 1)
0.33 utilization rate
",ImageNet,,,,,,,,,,,,$6569.51,,Industry,,"We address the challenging problem of efficient inference across many devices and resource constraints, especially on edge devices. Conventional approaches either manually design or use neural architecture search (NAS) to find a specialized neural network and train it from scratch for each case, which is computationally prohibitive (causing CO2 emission as much as 5 cars' lifetime) thus unscalable. In this work, we propose to train a once-for-all (OFA) network that supports diverse architectural settings by decoupling training and search, to reduce the cost. We can quickly get a specialized sub-network by selecting from the OFA network without additional training. To efficiently train OFA networks, we also propose a novel progressive shrinking algorithm, a generalized pruning method that reduces the model size across many more dimensions than pruning (depth, width, kernel size, and resolution). It can obtain a surprisingly large number of sub-networks (>1019) that can fit different hardware platforms and latency constraints while maintaining the same level of accuracy as training independently. On diverse edge devices, OFA consistently outperforms state-of-the-art (SOTA) NAS methods (up to 4.0% ImageNet top1 accuracy improvement over MobileNetV3, or same accuracy but 1.5x faster than MobileNetV3, 2.6x faster than EfficientNet w.r.t measured latency) while reducing many orders of magnitude GPU hours and CO2 emission. In particular, OFA achieves a new SOTA 80.0% ImageNet top-1 accuracy under the mobile setting (<600M MACs). OFA is the winning solution for the 3rd Low Power Computer Vision Challenge (LPCVC), DSP classification track and the 4th LPCVC, both classification track and detection track. Code and 50 pre-trained models (for many devices & many latency constraints) are released at this https URL.",2023-12-05 04:33,Robi Rahman,,,"United States of America,United States of America,Multinational","MIT-IBM Watson AI Lab, Massachusetts Institute of Technology (MIT), IBM",,,,Once for All,,,
"Segatron XL large, M=384",Language,,"He Bai, Peng Shi, Jimmy Lin, Yuqing Xie, Luchen Tan, Kun Xiong, Wen Gao, Ming Li",,,,https://arxiv.org/abs/2004.14996,13.00,Segatron: Segment-Aware Transformer for Language Modeling and Understanding,2020-04-30,,,257000000.00,,26500000000000000000.00,,WikiText-103,,,,167.02,,,,,,,,,,,,,2023-11-23 06:53,Robi Rahman,"""Segatron XL large, M=384""",,,,,,,,,,
"Segatron XL base, M=384",Language,,,,,,,,,2020-04-30,,,,,,,,,,,18.64,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,"""Segatron XL base, M=384""",1,,,,,,,,,
NAS+ESS (156M),Language,,"Yinqiao Li, Chi Hu, Yuhao Zhang, Nuo Xu, Yufan Jiang, Tong Xiao, Jingbo Zhu, Tongran Liu, Changliang Li",SOTA Improvement,"""Our ESS method
achieves state-of-the-art result on the PTB task""",,https://arxiv.org/pdf/2005.02593,12.00,Learning Architectures from an Extended Search Space for Language Modeling,2020-05-06,"Northeastern University (China),Chinese Academy of Sciences,NiuTrans Research,Kingsoft",Industry,156000000.00,,2890000000000000000.00,,,,,,30.00,,,,,,,,,,,,,2023-11-26 03:25,Robi Rahman,NAS+ESS (156M),,"China,China,China,China","Northeastern University (China), Chinese Academy of Sciences, NiuTrans Research, Kingsoft",,,,NAS+ESS (156M),,,
NAS+ESS (23M),Language,,,,,,,,,2020-05-06,"Northeastern University (China),NiuTrans Research,Kingsoft",,,,,,,,,,30.00,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,NAS+ESS (23M),1,"China,China,China","Northeastern University (China), NiuTrans Research, Kingsoft",,,,,,,
ContextNet,Speech,Speech recognition,"Wei Han, Zhengdong Zhang, Yu Zhang, Jiahui Yu, Chung-Cheng Chiu, James Qin, Anmol Gulati, Ruoming Pang, Yonghui Wu",SOTA Improvement,"""We demonstrate that on the widely used Librispeech
benchmark, ContextNet achieves a word error rate (WER) of
2.1%/4.6% without external language model (LM), 1.9%/4.1%
with LM and 2.9%/7.0% with only 10M parameters on the
clean/noisy LibriSpeech test sets. This compares to the
best previously published model of 2.0%/4.6% with LM and
3.9%/11.3% with 20M parameters""",,https://arxiv.org/abs/2005.03191v3,233.00,ContextNet: Improving Convolutional Neural Networks for Automatic Speech Recognition with Global Context,2020-05-07,Google,Industry,112700000.00,Table 5,,,Librispeech,,,970 hours of speech,,1000000000.00,"2.647 billion FLOP to process one second of audio. Our database guide says words are the standard datapoint for speech, and speech is usually a few words per second so I put down 1 billion, but unsure what exact ratio we should use.",,,,,,,,,Likely,"Convolutional neural networks (CNN) have shown promising results for end-to-end speech recognition, albeit still behind other state-of-the-art methods in performance. In this paper, we study how to bridge this gap and go beyond with a novel CNN-RNN-transducer architecture, which we call ContextNet. ContextNet features a fully convolutional encoder that incorporates global context information into convolution layers by adding squeeze-and-excitation modules. In addition, we propose a simple scaling method that scales the widths of ContextNet that achieves good trade-off between computation and accuracy. We demonstrate that on the widely used LibriSpeech benchmark, ContextNet achieves a word error rate (WER) of 2.1%/4.6% without external language model (LM), 1.9%/4.1% with LM and 2.9%/7.0% with only 10M parameters on the clean/noisy LibriSpeech test sets. This compares to the previous best published system of 2.0%/4.6% with LM and 3.9%/11.3% with 20M parameters. The superiority of the proposed ContextNet model is also verified on a much larger internal dataset.",2023-11-26 03:25,Anonymous,,,Multinational,Google,,,,ContextNet,,,
ONLSTM-SYD,Language,,"Wenyu Du, Zhouhan Lin, Yikang Shen, Timothy J. O'Donnell, Yoshua Bengio, Yue Zhang",,,,https://arxiv.org/pdf/2005.05864,15.00,Exploiting Syntactic Structure for Better Language Modeling: A Syntactic Distance Approach,2020-05-12,,,25000000.00,,138999999999999980.00,,,,,,1000.00,,,,,,,,,,,,,2023-11-23 06:53,Robi Rahman,ONLSTM-SYD,,,,,,,,,,
Conformer,Speech,Speech recognition,"Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, Ruoming Pang","SOTA Improvement,Highly cited","""Conformer significantly outperforms the previous Transformer and CNN based models achieving state-of-the-art accuracies. On the widely used LibriSpeech benchmark, our model achieves WER of 2.1%/4.3% without using a language model and 1.9%/3.9% with an external language model on test/testother""",,https://arxiv.org/abs/2005.08100v1,2085.00,Conformer: Convolution-augmented Transformer for Speech Recognition,2020-05-16,Google,Industry,118800000.00,118.8M for Conformer(L),,,LibriSpeech,,,,,,,,,,,,,,,Confident,"Recently Transformer and Convolution neural network (CNN) based models have shown promising results in Automatic Speech Recognition (ASR), outperforming Recurrent neural networks (RNNs). Transformer models are good at capturing content-based global interactions, while CNNs exploit local features effectively. In this work, we achieve the best of both worlds by studying how to combine convolution neural networks and transformers to model both local and global dependencies of an audio sequence in a parameter-efficient way. To this regard, we propose the convolution-augmented transformer for speech recognition, named Conformer. Conformer significantly outperforms the previous Transformer and CNN based models achieving state-of-the-art accuracies. On the widely used LibriSpeech benchmark, our model achieves WER of 2.1%/4.3% without using a language model and 1.9%/3.9% with an external language model on test/testother. We also observe competitive performance of 2.7%/6.3% with a small model of only 10M parameters.",2023-11-26 03:25,Anonymous,,,Multinational,Google,,,,Conformer,,,
Decay RNN,Language,,"Gantavya Bhatt, Hritik Bansal, Rishubh Singh, Sumeet Agarwal",,,,https://arxiv.org/abs/2005.08199,7.00,How much complexity does an RNN architecture need to learn syntax-sensitive dependencies?,2020-05-17,,Industry - Academia Collaboration,1400000.00,,,,Lizen et al 2016,,,,,,,,,,,,,,,,,2023-11-21 01:39,Robi Rahman,Decay RNN,,,,,,,,,,
rTop-k(distributed setting),Language,,"Leighton Pate Barnes, Huseyin A. Inan, Berivan Isik, Ayfer Ozgur",,,,https://arxiv.org/pdf/2005.10761,41.00,rTop-k: A Statistical Estimation Approach to Distributed SGD,2020-05-21,,Academia,69000000.00,,14600000000000000.00,,,,,,38.00,,,,,,,,,,,,,2023-11-23 06:53,Robi Rahman,rTop-k(distributed setting),,,,,,,,,,
DETR,Vision,Object detection,"Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko",Highly cited,,,https://arxiv.org/abs/2005.12872,8594.00,End-to-End Object Detection with Transformers,2020-05-26,Facebook,Industry,60000000.00,60M per Table 1,400000000000000000000.00,"""Training the baseline model for 300 epochs on 16 V100 GPUs
takes 3 days, with 4 images per GPU (hence a total batch size of 64). For the
longer schedule used to compare with Faster R-CNN we train for 500 epochs
with learning rate drop after 400 epochs. This schedule adds 1.5 AP compared
to the shorter schedule.""

48 V100-days for baseline DETR model. Larger model had 1.5x the params and 5/3 as many epochs, so required ~2.5x as much training compute.

125 teraflop/s * 2.5 * 48 * 24 * 3600 * 0.3 (assumed utilization) ~ 4e20",COCO 2017,"""We perform experiments on COCO 2017 detection and panoptic segmentation datasets [24,18], containing 118k training images and 5k validation images""",123000,,500.00,25300000000.00,"253 GFLOPS inference cost, for 10 FPS, so 25.3 billion FLOP per image. ",,,NVIDIA V100,NVIDIA V100,Supervised,,,,Likely,"Abstract. We present a new method that views object detection as a
direct set prediction problem. Our approach streamlines the detection
pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation
that explicitly encode our prior knowledge about the task. The main
ingredients of the new framework, called DEtection TRansformer or
DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given
a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output
the final set of predictions in parallel. The new model is conceptually
simple and does not require a specialized library, unlike many other
modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster RCNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation
in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at
https://github.com/facebookresearch/detr.",2023-11-26 03:25,Anonymous,,,Multinational,Facebook,,,,DETR,,,
GPT-3 175B (davinci),Language,Text autocompletion,"Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei",Highly cited,,API accessible,https://arxiv.org/abs/2005.14165,18144.00,Language models are Few-Shot Learners,2020-05-28,OpenAI,Industry,175000000000.00,"""we train GPT-3, an autoregressive language model with 175 billion parameters""",3.14e+23,"Table D.1
https://arxiv.org/abs/2005.14165",CommonCrawl; WebText2; Books1; Books2; Wikipedia,Table 2.2 (other datasets also used),374000000000,"From table 2.2, we determine that there are 410 + 19 + 12 + 55 + 3 = 499 billion tokens. 

We multiply this by 0.75 to give 374B words. 

3.74e11

========================
[Anson: I think the calculation below doesn't look at all the data, the CommonCrawl data only constitutes 60% of the data. Multiplying by 5/3 gives 4.75e11]

""The CommonCrawl data was downloaded from 41 shards of monthly CommonCrawl covering 2016 to 2019, constituting 45TB of compressed plaintext before filtering and 570GB after filtering, roughly equivalent to 400 billion byte-pair-encoded tokens. ""

Converted to words using 
http://extraconversion.com/data-storage/gigabits/gigabits-to-words.html

2.85e11",0.60,740000000000000.00,"Rados (FLOPs)
https://drive.google.com/drive/folders/1bhy5z6hh1n3wCHx6528Xb7xB1KhYdAL1",355.2,14.8 days according to https://arxiv.org/ftp/arxiv/papers/2104/2104.10350.pdf,NVIDIA Tesla V100 DGXS 32 GB,NVIDIA Tesla V100 DGXS 32 GB,Self-supervised learning,$1131415.12,,Industry,,"Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",2023-12-05 04:33,Robi Rahman,GPT-3 175B (davinci),,United States of America,OpenAI,,,,GPT-3 175B (davinci),10000,0.2196,GPT-3 175B (davinci)
GPT3-6.7B (rerun of original),Language,,"Greg Yang, Edward J. Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ryder, Jakub Pachocki, Weizhu Chen, Jianfeng Gao",,,,https://web.archive.org/web/20221014063419/https://arxiv.org/pdf/2203.03466.pdf,76.00,Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer,2020-05-28,,,6700000000.00,,1.2e+22,,,,,,1.00,,,,,,,,,,,,,2023-12-05 04:33,Robi Rahman,GPT3-6.7B (rerun of original),,,,,,,,,,
6-Layer-Tensor-Transformer+AdaHessian,Language,,"Zhewei Yao, Amir Gholami, Sheng Shen, Mustafa Mustafa, Kurt Keutzer, Michael W. Mahoney",,"""We show that AdaHessian achieves new state-of-the-art results by a large margin as compared
to other adaptive optimization methods, including variants of Adam""",,https://arxiv.org/pdf/2006.00719,155.00,ADAHESSIAN: An Adaptive Second Order Optimizer for Machine Learning,2020-06-01,,Academia,85300000.00,,1580000000000000000.00,,,,,,30.00,,,,,,,,,,,,,2023-12-05 04:33,Robi Rahman,6-Layer-Tensor-Transformer+AdaHessian,,,,,,,,,,
3-Layer-Tensor-Transformer+AdaHessian,Language,,,,,,,,,2020-06-01,,,,,,,,,,,30.00,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,3-Layer-Tensor-Transformer+AdaHessian,1,,,,,,,,,
SqueezeBERT,Language,Text autocompletion,"Forrest N. Iandola, Albert E. Shaw, Ravi Krishna, Kurt W. Keutzer",,,,https://arxiv.org/abs/2006.11316,99.00,SqueezeBERT: What can computer vision teach NLP about efficient neural networks?,2020-06-10,UC Berkeley,Academia,51100000.00,Rados,,,,,,,,7420000000.00,"Rados (FLOPs)
https://drive.google.com/drive/folders/1bhy5z6hh1n3wCHx6528Xb7xB1KhYdAL1",,,,,,,,Academia,,,2023-12-05 04:33,Robi Rahman,,,United States of America,UC Berkeley,,,,,,,
Transformer-XL+AdamP,Language,,"Byeongho Heo, Sanghyuk Chun, Seong Joon Oh, Dongyoon Han, Sangdoo Yun, Gyuwan Kim, Youngjung Uh, Jung-Woo Ha",,,,https://arxiv.org/pdf/2006.08217,114.00,AdamP: Slowing Down the Slowdown for Momentum Optimizers on Scale-invariant Weights,2020-06-15,,,257000000.00,,,,,,,,,,,,,,,,,,,,,2023-11-21 01:39,Robi Rahman,Transformer-XL+AdamP,,,,,,,,,,
Transformer-XL+WN+AdamP,Language,,"Byeongho Heo, Sanghyuk Chun, Seong Joon Oh, Dongyoon Han, Sangdoo Yun, Gyuwan Kim, Youngjung Uh, Jung-Woo Ha",,,,https://arxiv.org/pdf/2006.08217,114.00,AdamP: Slowing Down the Slowdown for Momentum Optimizers on Scale-invariant Weights,2020-06-15,,,257000000.00,,,,,,,,,,,,,,,,,,,,,2023-11-21 01:39,Robi Rahman,Transformer-XL+WN+AdamP,,,,,,,,,,
iGPT-XL,Drawing,Image completion,"Mark Chen, Alec Radford, Rewon Child, Jeff Wu, Heewoo Jun, Prafulla Dhariwal, David Luan, Ilya Sutskever",Highly cited,,,https://openai.com/research/image-gpt,1190.00,Generative Pretraining from Pixels,2020-06-17,OpenAI,Industry,6801000000.00,source: https://openai.com/blog/image-gpt/#rfref53,3.3e+22,"Taken from here
https://www.lesswrong.com/posts/wfpdejMWog4vEDLDg/ai-and-compute-trend-isn-t-predictive-of-what-is-happening",ILSVRC 2012,,9600000,"""We use the ImageNet ILSVRC 2012 training dataset, splitting off 4% as our experimental validation set and report results on the ILSVRC 2012 validation set as our test set.""

https://image-net.org/challenges/LSVRC/2012/

""The goal of this competition is to estimate the content of photographs for the purpose of retrieval and automatic annotation using a subset of the large hand-labeled ImageNet dataset (10,000,000 labeled images depicting 10,000+ object categories) as training.""
",,,,,,NVIDIA Tesla V100 DGXS 32 GB,NVIDIA Tesla V100 DGXS 32 GB,Self-supervised learning,$120440.96,,Industry,,"Inspired by progress in unsupervised representation learning for natural language, we examine whether similar models can learn useful representations for images. We train a sequence Transformer to auto-regressively predict pixels, without incorporating knowledge of the 2D input structure. Despite training on low-resolution ImageNet without labels, we find that a GPT-2 scale model learns strong image representations as measured by linear probing, fine-tuning, and low-data classification. On CIFAR-10, we achieve 96.3% accuracy with a linear probe, outperforming a supervised Wide ResNet, and 99.0% accuracy with full finetuning, matching the top supervised pre-trained models. An even larger model trained on a mixture of ImageNet and web images is competitive with self-supervised benchmarks on ImageNet, achieving 72.0% top-1 accuracy on a linear probe of our features.",2023-11-26 03:25,Robi Rahman,,,United States of America,OpenAI,,,,iGPT-XL,,,
iGPT-L,Drawing,Image completion,"Mark Chen, Alec Radford, Rewon Child, Jeff Wu, Heewoo Jun, Prafulla Dhariwal, David Luan, Ilya Sutskever",,,,https://openai.com/blog/image-gpt/,1190.00,Generative Pretraining from Pixels,2020-06-17,OpenAI,Industry,1362000000.00,source: https://openai.com/blog/image-gpt/#rfref53,8.91e+21,"We have that ""iGPT-L was trained for roughly 2500 V100-days"" [1]

I assume this is the NVIDIA Tesla V100 GPU. In the specifications, the NVIDIA Tesla V100 has 7 to 8.2 TFLOPS of peak double precision performance and 14 to 16.4 TFLOPS of peak single precision performance and 112 to 130 TFLOPS of peak tensor performance [2].

I suppose the one that makes sense using if peak tensor performance, for ~125 TFLOPS peak tensor performance more or less.
Following OpenAIs AI and compute we apply a 0.33 utitilization factor [3].

In total we get 2500 V100-days * (24*60*60) seconds/day * 125 TFLOPS * 0.33 = 8.91e+21 FLOPS = 89.1 PF-days.

[1] https://openai.com/blog/image-gpt/
[2] https://images.nvidia.com/content/technologies/volta/pdf/volta-v100-datasheet-update-us-1165301-r5.pdf
[3] https://openai.com/blog/ai-and-compute/",ILSVRC 2012,,9600000,"""We use the ImageNet ILSVRC 2012 training dataset, splitting off 4% as our experimental validation set and report results on the ILSVRC 2012 validation set as our test set.""

https://image-net.org/challenges/LSVRC/2012/

""The goal of this competition is to estimate the content of photographs for the purpose of retrieval and automatic annotation using a subset of the large hand-labeled ImageNet dataset (10,000,000 labeled images depicting 10,000+ object categories) as training.""
",,,,,,NVIDIA Tesla V100 DGXS 32 GB,NVIDIA Tesla V100 DGXS 32 GB,Self-supervised learning,$32482.56,,Industry,,"Inspired by progress in unsupervised representation learning for natural language, we examine whether similar models can learn useful representations for images. We train a sequence Transformer to auto-regressively predict pixels, without incorporating knowledge of the 2D input structure. Despite training on low-resolution ImageNet without labels, we find that a GPT-2 scale model learns strong image representations as measured by linear probing, fine-tuning, and low-data classification. On CIFAR-10, we achieve 96.3% accuracy with a linear probe, outperforming a supervised Wide ResNet, and 99.0% accuracy with full finetuning, matching the top supervised pre-trained models. An even larger model trained on a mixture of ImageNet and web images is competitive with self-supervised benchmarks on ImageNet, achieving 72.0% top-1 accuracy on a linear probe of our features.",2023-11-23 06:53,Robi Rahman,,,United States of America,OpenAI,,,,,,,
GShard (dense),Language,Translation,"Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, Zhifeng Chen",SOTA Improvement,"""such a giant model can efficiently be trained on 2048 TPU v3 accelerators in 4 days to achieve far superior quality for translation from 100 languages to English compared to the prior art""",,https://arxiv.org/abs/2006.16668,523.00,GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding,2020-06-30,Google,Industry,2300000000.00,"""Our best quality dense single Transformer model (2.3B parameters) achieving ∆BLEU of 6.1, was trained with GPipe [15] on 2048 TPU v3 cores for 6 weeks or total of 235.5 TPU v3 core-years.""",1.3702000000001001e+23,"2048 TPU v3 cores for 6 weeks or 235.5 TPU v3 core-years

Assume 30% utilization. 2 TPU v3 cores = 1 TPU v3 chip.
TPU v3 performance is 123 teraFLOPS per chip

2048 TPU cores * (1 chip / 2 cores) * 123 TFLOPS/chip * 0.30 = 1.3702e23 FLOP
https://www.wolframalpha.com/input?i=123+teraFLOPS+%2F+2+*+235.5+years+*+0.30

Effective model FLOPs utilization could have been lower since this model has very high training compute compared to parameter count. (Compare to Chinchilla-optimal?)",,,260000000000,"""We focus on improving the translation quality (measured in terms of BLEU score [48]) from all 100 languages to English. This resulted in approximately 13 billion training examples to be used for model training""

Each example is a sentence pair. Assuming 20 words per sentence, that is 13*20 billion words.",,,,1008.0,6 weeks = 1008 hours,Google TPU v3,Google TPU v3,Self-supervised learning,,,Industry,Confident,"Neural network scaling has been critical for improving the model quality in many real-world machine learning applications with vast amounts of training data and compute. Although this trend of scaling is affirmed to be a sure-fire approach for better model quality, there are challenges on the path such as the computation cost, ease of programming, and efficient implementation on parallel devices. GShard is a module composed of a set of lightweight annotation APIs and an extension to the XLA compiler. It provides an elegant way to express a wide range of parallel computation patterns with minimal changes to the existing model code. GShard enabled us to scale up multilingual neural machine translation Transformer model with Sparsely-Gated Mixture-of-Experts beyond 600 billion parameters using automatic sharding. We demonstrate that such a giant model can efficiently be trained on 2048 TPU v3 accelerators in 4 days to achieve far superior quality for translation from 100 languages to English compared to the prior art.",2023-12-05 11:48,Robi Rahman,,,Multinational,Google,,,,GShard (dense),1024,,GShard (dense)
GShard (600B),Language,Translation,"Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, Zhifeng Chen",,this is the smaller model in this paper,,https://arxiv.org/abs/2006.16668,523.00,GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding,2020-06-30,Google,Industry,600000000000.00,"""The 600B parameters model that achieved the best translation quality was trained with 2048 TPU v3 cores for 4 days, a total cost of 22 TPU v3 core-years.""",1.33e+22,"https://arxiv.org/ftp/arxiv/papers/2104/2104.10350.pdf
Table 4",,,260000000000,"""We focus on improving the translation quality (measured in terms of BLEU score [48]) from all 100 languages to English. This resulted in approximately 13 billion training examples to be used for model training""

Each example is a sentence pair. Assuming 20 words per sentence, that is 13*20 billion words.",,,,,,,,Self-supervised learning,$27609.81,,Industry,,"Neural network scaling has been critical for improving the model quality in many real-world machine learning applications with vast amounts of training data and compute. Although this trend of scaling is affirmed to be a sure-fire approach for better model quality, there are challenges on the path such as the computation cost, ease of programming, and efficient implementation on parallel devices. GShard is a module composed of a set of lightweight annotation APIs and an extension to the XLA compiler. It provides an elegant way to express a wide range of parallel computation patterns with minimal changes to the existing model code. GShard enabled us to scale up multilingual neural machine translation Transformer model with Sparsely-Gated Mixture-of-Experts beyond 600 billion parameters using automatic sharding. We demonstrate that such a giant model can efficiently be trained on 2048 TPU v3 accelerators in 4 days to achieve far superior quality for translation from 100 languages to English compared to the prior art.",2023-12-05 04:33,Robi Rahman,,,Multinational,Google,,,,,,,
DLRM-2021,Recommendation,,"D Mudigere, Y Hao, J Huang, A Tulloch",,,,https://www.arxiv-vanity.com/papers/2104.05158/,10.00,"High-performance, Distributed Training of Large scale Deep Learning Recommendation Models",2020-07-01,Facebook AI,Industry,1000000000000.00,"Figure 1

https://arxiv.org/abs/2104.05158",300000000000000000000.00,"Figure 1

https://arxiv.org/abs/2104.05158",,,,,,,,,,,,,$1094.92,"https://bdtechtalks.com/2020/02/03/google-meena-chatbot-ai-language-model/
",Industry,,"Deep learning recommendation models (DLRMs) are used across many business-critical services at Facebookand are the single largest AI application in terms of infrastructure demand in its data-centers. In this paper we discuss the SW/HW co-designed solution for high-performance distributed training of large-scale DLRMs. We introduce a high-performance scalable software stack based on PyTorch and pair it with the new evolution of Zion platform, namely ZionEX. We demonstrate the capability to train very large DLRMs with up to 12 Trillion parameters and show that we can attain 40 × speedup in terms of time to solution over previous systems. We achieve this by (i) designing the ZionEX platform with dedicated scale-out network, provisioned with high bandwidth, optimal topology and efficient transport (ii) implementing an optimized PyTorch-based training stack supporting both model and data parallelism (iii) developing sharding algorithms capable of hierarchical partitioning of the embedding tables along row, column dimensions and load balancing them across multiple workers; (iv) adding high-performance core operators while retaining flexibility to support optimizers with fully deterministic updates (v) leveraging reduced precision communications, multi-level memory hierarchy (HBM+DDR+SSD) and pipelining. Furthermore, we develop and briefly comment on distributed data ingestion and other supporting services that are required for the robust and efficient end-to-end training in production environments.",2023-11-23 06:53,Robi Rahman,,,Multinational,Facebook AI,,,,,,,
SemExp,Robotics,Object detection,"Devendra Singh Chaplot, Dhiraj Gandhi, Abhinav Gupta, Ruslan Salakhutdinov",SOTA Improvement,"""Our method achieves state-of-the-art performance on the object goal navigation task and won the CVPR2020 Habitat ObjectNav challenge""",,https://proceedings.neurips.cc/paper/2020/file/2c75cf2681788adaca63aa95ae028b22-Paper.pdf,317.00,Object Goal Navigation using Goal-Oriented Semantic Exploration,2020-07-02,"Carnegie Mellon University (CMU),Facebook AI Research",Industry - Academia Collaboration (Industry leaning),,,,," Gibson, Matterport3D (MP3D)","""We use the Gibson [46] and Matterport3D (MP3D) [6] datasets""",,,,,,,,,,Reinforcement learning,,,,Likely,"This work studies the problem of object goal navigation which involves navigating to an instance of the given object category in unseen environments. End-to-end learning-based navigation methods struggle at this task as they are ineffective at exploration and long-term planning. We propose a modular system called, ‘GoalOriented Semantic Exploration’ which builds an episodic semantic map and uses it to explore the environment efficiently based on the goal object category. Empirical results in visually realistic simulation environments show that the proposed model outperforms a wide range of baselines including end-to-end learning-based methods as well as modular map-based methods and led to the winning entry of the CVPR2020 Habitat ObjectNav Challenge. Ablation analysis indicates that the proposed model learns semantic priors of the relative arrangement of objects in a scene, and uses them to explore efficiently. Domain-agnostic module design allows us to transfer our model to a mobile robot platform and achieve similar performance for object goal navigation in the real-world.",2023-11-26 03:25,Anonymous,,,"United States of America,Multinational","Carnegie Mellon University (CMU), Facebook AI Research",,,,SemExp,,,
1-layer-LSTM,Language,,"H. T. Kung, Bradley McDanel, Sai Qian Zhang",,,,https://arxiv.org/pdf/2007.06389,9.00,Term Revealing: Furthering Quantization at Run Time on Quantized DNNs,2020-07-13,,,86500000.00,,,,,,,,,,,,,,,,,,,,,2023-11-21 01:39,Robi Rahman,1-layer-LSTM,,,,,,,,,,
Hopfield Networks (2020),Other,,"Hubert Ramsauer, Bernhard Schäfl, Johannes Lehner, Philipp Seidl, Michael Widrich, Thomas Adler, Lukas Gruber, Markus Holzleitner, Milena Pavlović, Geir Kjetil Sandve, Victor Greiff, David Kreil, Michael Kopp, Günter Klambauer, Johannes Brandstetter, Sepp Hochreiter",SOTA Improvement,"""Hopfield layers yielded a new state-ofthe-art when compared to different machine learning methods. Finally, Hopfield
layers achieved state-of-the-art on two drug design datasets""",,https://arxiv.org/abs/2008.02217,233.00,Hopfield Networks is All You Need,2020-07-16,"Johannes Kepler University Linz,Institute of Advanced Research in Artificial Intelligence,University of Oslo",Academia,,,,,,,,,,,,,,,,,,,Academia,,,2023-12-05 04:33,Robi Rahman,,,"Austria,Austria,Norway","Johannes Kepler University Linz, Institute of Advanced Research in Artificial Intelligence, University of Oslo",,,,Hopfield Networks (2020),,,
EfficientDet,Vision,Object detection,"Mingxing Tan, Ruoming Pang, Quoc V. Le",SOTA Improvement,"""EfficientDet-D7 achieves stateof-the-art 55.1 AP on COCO test-dev with 77M parameters and 410B FLOPs""",,https://openaccess.thecvf.com/content_CVPR_2020/html/Tan_EfficientDet_Scalable_and_Efficient_Object_Detection_CVPR_2020_paper.html,4701.00,EfficientDet: Scalable and Efficient Object Detection,2020-07-27,Google Brain,Industry,77000000.00,"""EfficientDet-D7 achieves stateof-the-art 55.1 AP on COCO test-dev with 77M parameters and 410B FLOPs""",,,,,,,,410000000000.00,"""In particular, with single model and single-scale, our EfficientDet-D7 achieves state-of-the-art 55.1 AP on COCO test-dev with 77M parameters and 410B FLOPs""",,,,,,,,Industry,,,2023-11-26 03:25,Robi Rahman,,,Multinational,Google Brain,,,,EfficientDet,,,
TransformerXL-LayerFusion-CA,Language,,"James O' Neill, Greg Ver Steeg, Aram Galstyan",,,,https://arxiv.org/pdf/2007.14917,5.00,Compressing Deep Neural Networks via Layer Fusion,2020-07-29,,,,,,,,,,,,,,,,,,,,,,,,2023-11-21 01:39,Robi Rahman,TransformerXL-LayerFusion-CA,,,,,,,,,,
GPT2-LayerFusion-WS,Language,,"James O' Neill, Greg Ver Steeg, Aram Galstyan",,,,https://arxiv.org/pdf/2007.14917,5.00,Compressing Deep Neural Networks via Layer Fusion,2020-07-29,,,,,,,,,,,,,,,,,,,,,,,,2023-11-21 01:39,Robi Rahman,GPT2-LayerFusion-WS,,,,,,,,,,
Grown to Prune Two-layer stacked LSTM,Language,,"Xin Yuan, Pedro Savarese, Michael Maire",,,,https://arxiv.org/pdf/2007.15353,37.00,Growing Efficient Deep Networks by Structured Continuous Sparsification,2020-07-30,,Industry - Academia Collaboration,,,,,,,,,,,,,,,,,,,,,,2023-11-21 01:39,Robi Rahman,Grown to Prune Two-layer stacked LSTM,,,,,,,,,,
DeLight,Language,,"Sachin Mehta, Marjan Ghazvininejad, Srinivasan Iyer, Luke Zettlemoyer, Hannaneh Hajishirzi",SOTA Improvement,"""Comparison with state-of-the-art methods on machine translation corpora. DeLighT delivers
similar or better performance than state-of-the-art models with fewer parameters.""",,https://arxiv.org/abs/2008.00623,98.00,DeLighT: Deep and Light-weight Transformer,2020-08-03,"University of Washington,Allen Institute for AI,Facebook AI Research",,99000000.00,,24000000000000000000.00,,WikiText-103,,,,62.14,,,,,,,,,,,,,2023-11-26 03:25,Robi Rahman,DeLight,,"United States of America,United States of America,Multinational","University of Washington, Allen Institute for AI, Facebook AI Research",,,,DeLight,,,
ERNIE-GEN (large),Language,Language Generation,"Dongling Xiao, Han Zhang, Yukun Li, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang",SOTA Improvement,"""Empirically, ERNIE-GEN is particularly effective and
achieves state-of-the-art results on a range of NLG tasks
including abstractive summarization (Gigaword and CNN/DailyMail), question generation (SQuAD), dialogue response generation (Persona-Chat) and generative question answering (CoQA)""",,https://arxiv.org/abs/2001.11314,95.00,ERNIE-GEN: An Enhanced Multi-Flow Pre-training and Fine-tuning Framework for Natural Language Generation,2020-08-06,Baidu,Industry,340000000.00,"""We train a base model ERNIEGENBASE (L=12, H=768, A=12, Total Parameters=110M)1
and a large model ERNIE-GENLARGE (L=24, H=1024,
A=16, Total Parameters=340M) with parameters initialized
by BERTBASE and BERTLARGE respectively""",,,,,,,,,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,China,Baidu,,,,ERNIE-GEN (large),,,
Transformer+Recurrent Windows of Context,Language,,"Davis Yoshida, Allyson Ettinger, Kevin Gimpel",,,,https://arxiv.org/pdf/2008.07027,4.00,Adding Recurrence to Pretrained Transformers for Improved Efficiency and Context Size,2020-08-16,,,124000000.00,,116999999999999980000.00,,,,,,2.00,,,,,,,,,,,,,2023-11-23 06:53,Robi Rahman,Transformer+Recurrent Windows of Context,,,,,,,,,,
ProBERTa,Language,Proteins,"Ananthan Nambiar, Maeve Heflin, Simon Liu, Sergei Maslov, Mark Hopkins, Anna Ritz",SOTA Improvement,"""Furthermore, we used embeddings from PRoBERTa for a fundamentally different problem, PPI prediction, using two different
datasets generated from the HIPPIE database and found that with
sufficient data, it substantially outperforms the current state-of-theart method in the conservative scenario.""",,https://dl.acm.org/doi/10.1145/3388440.3412467,97.00,"Transforming the Language of Life: Transformer Neural
Networks for Protein Prediction Tasks",2020-09-01,"University of Illinois Urbana-Champaign (UIUC),Reed College",Academia,44000000.00,"""In total, our model has approximately 44M trainable parameters.""",9720000000000000000.00,"""we pre-train PRoBERTa on 4 NVIDIA V100 GPUs in 18 hours""
4 * 125 tFLOP/s * 18 * 3600 * 0.3 (assumed utilization) = 9.72e18",UniProtKB/Swiss-Prot,,,,,,,18.0,,NVIDIA V100,NVIDIA V100,,,,Academia,Likely,"The scientific community is rapidly generating protein sequence information, but only a fraction of these proteins can be experimentally characterized. While promising deep learning approaches for protein prediction tasks have emerged, they have computational limitations or are designed to solve a specific task. We present a Transformer neural network that pre-trains task-agnostic sequence representations. This model is fine-tuned to solve two different protein prediction tasks: protein family classification and protein interaction prediction. Our method is comparable to existing state-of-the-art approaches for protein family classification while being much more general than other architectures. Further, our method outperforms all other approaches for protein interaction prediction. These results offer a promising framework for fine-tuning the pre-trained sequence representations for other protein prediction tasks.",2023-11-26 03:25,Anonymous,,,"United States of America,United States of America","University of Illinois Urbana-Champaign (UIUC), Reed College",,,,ProBERTa,,,
PAR Transformer Large,Language,,"Swetha Mandava, Szymon Migacz, Alex Fit Florea",,,,https://arxiv.org/abs/2009.04534,11.00,Pay Attention when Required,2020-09-09,,Academia,,,,,WikiText-103,,,,,,,,,,,,,,,,,2023-11-21 01:39,Robi Rahman,PAR Transformer Large,,,,,,,,,,
Integer Transformer,Language,,"Ye Lin, Yanyang Li, Tengbo Liu, Tong Xiao, Tongran Liu, Jingbo Zhu",,,,https://arxiv.org/pdf/2009.08034,74.00,Towards Fully 8-bit Integer Inference for the Transformer Model,2020-09-17,,,247000000.00,,,,,,,,,,,,,,,,,,,,,2023-12-05 04:33,Robi Rahman,"Integer Transformer,Integer Transformer",,,,,,,,,,
Frage-AWD-LSTM-MemoryAug-NeuralCache (WT2),Language,,"Ke Li, Daniel Povey, Sanjeev Khudanpur",,,,https://arxiv.org/pdf/2009.13774,4.00,Neural Language Modeling With Implicit Cache Pointers,2020-09-29,,,33000000.00,,,,,,,,,,,,,,,,,,,,,2023-11-21 01:39,Robi Rahman,Frage-AWD-LSTM-MemoryAug-NeuralCache (WT2),,,,,,,,,,
Frage-AWD-LSTM-MemoryAug-NeuralCache (PTB),Language,,,,,,,,,2020-09-29,,,,,,,,,,,,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,Frage-AWD-LSTM-MemoryAug-NeuralCache (PTB),1,,,,,,,,,
LSTM-MemoryAug (WT2),Language,,,,,,,,,2020-09-29,,,,,,,,,,,,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,LSTM-MemoryAug (WT2),1,,,,,,,,,
LSTM-MemoryAug (PTB),Language,,,,,,,,,2020-09-29,,,,,,,,,,,,,,,,,,,,,,Unverified,,2023-12-08 01:01,Robi Rahman,LSTM-MemoryAug (PTB),1,,,,,,,,,
Memformer (4 encoder + 16 decoder),Language,,"Qingyang Wu, Zhenzhong Lan, Kun Qian, Jing Gu, Alborz Geramifard, Zhou Yu",,,,https://arxiv.org/abs/2010.06891,16.00,Memformer: A Memory-Augmented Transformer for Sequence Modeling,2020-10-14,,Industry,76200000.00,,12000000000000000000.00,,WikiText-103,,,,11.93,,,,,,,,,,,,,2023-12-05 04:33,Robi Rahman,Memformer (4 encoder + 16 decoder),,,,,,,,,,
Conformer + Wav2vec 2.0 + Noisy Student ,Speech,Speech recognition,"Yu Zhang, James Qin, Daniel S. Park, Wei Han, Chung-Cheng Chiu, Ruoming Pang, Quoc V. Le, Yonghui Wu",SOTA Improvement,"""By doing so, we are able to achieve
word-error-rates (WERs) 1.4%/2.6% on the LibriSpeech test/test-other sets against
the current state-of-the-art WERs 1.7%/3.3%.""",,https://arxiv.org/abs/2010.10504v2,280.00,Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition,2020-10-20,Google,Industry,1000000000.00,1B for XXL model,1.5e+22,"""We train with global batch size 2048 on 256/512 Google TPU V3 cores for 3-4 days for the XL/XXL models respectively...
We fine-tune the pre-trained checkpoints (400k steps) with global batch
size 1024/512 on 256/512 Google TPU v3 cores for 1-3 days for the XL/XXL models""

512 * 7 days * 24 * 3600 * 123 tflops * 0.4 (assumed utilization) = 1.5e22","LibriSpeech, Libri-Light","""We aim to improve the LibriSpeech task [1] by utilizing the unlabeled audio in the ""unlab-60k""
subset of the Libri-Light [2] dataset. The 960h of transcribed audio of the LibriSpeech dataset is
used as the supervised data""",,,,,,168.0,7 days,Google TPU v3,Google TPU v3,,,,,Likely,"We employ a combination of recent developments in semi-supervised learning for automatic speech recognition to obtain state-of-the-art results on LibriSpeech utilizing the unlabeled audio of the Libri-Light dataset. More precisely, we carry out noisy student training with SpecAugment using giant Conformer models pre-trained using wav2vec 2.0 pre-training. By doing so, we are able to achieve word-error-rates (WERs) 1.4%/2.6% on the LibriSpeech test/test-other sets against the current state-of-the-art WERs 1.7%/3.3%.",2023-11-26 03:25,Anonymous,,,Multinational,Google,,,,Conformer + Wav2vec 2.0 + Noisy Student ,,,
mT5-XXL,Language,Language modelling,"Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, Colin Raffel","SOTA Improvement,Highly cited","""Table 2 presents our main results, with perlanguage breakdowns for each task given in Appendix B. Our largest model mT5-XXL exceeds
state-of-the-art on all classification and QA tasks
and is near SOTA on NER (69.2 vs. 70.1).""",,https://aclanthology.org/2021.naacl-main.41/,1338.00,mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer,2020-10-20,Google,Industry,13000000000.00,13 billion,7.8e+22,"""We pre-train our mT5 model variants for 1 million steps on batches of 1024 length-1024 input
sequences, corresponding to roughly 1 trillion input tokens total.""

1 trillion tokens * 13 billion params * 6 = 7.8e22",mC4,"""The C4 dataset was explicitly designed to be
English only: any page that was not given a
probability of at least 99% of being English by
langdetect2 was discarded. In contrast, for
mC4 we use cld33 to identify over 100 languages.
Since some of these languages are relatively scarce
on the internet, we make use of all of the 71
monthly web scrapes released so far by Common
Crawl. This is dramatically more source data
than was used for C4, for which the April 2019
web scrape alone was enough to provide plenty of
English-language data.""",5000000000000,"""totaling 6.6B pages and 6.3T tokens""

It's multilingual so we don't have a standard word:token ratio, but using the 0.75 for English that's ~5trillion",0.16,,,,,,,,,,,Likely,"The recent “Text-to-Text Transfer Transformer” (T5) leveraged a unified text-to-text format and scale to attain state-of-the-art results on a wide variety of English-language NLP tasks. In this paper, we introduce mT5, a multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We detail the design and modified training of mT5 and demonstrate its state-of-the-art performance on many multilingual benchmarks. We also describe a simple technique to prevent “accidental translation” in the zero-shot setting, where a generative model chooses to (partially) translate its prediction into the wrong language. All of the code and model checkpoints used in this work are publicly available.",2023-11-30 06:28,Anonymous,,0,Multinational,Google,,,,,,,
ViT-Huge/14,Vision,Image representation,"Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby",Highly cited,,,https://arxiv.org/abs/2010.11929,20003.00,An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale,2020-10-22,Google Brain,Industry,632000000.00,Table 1 https://arxiv.org/pdf/2010.11929.pdf,1.2826e+22,"Table 5
They also report TPUv3 days, which aligns with the number on table 5
(From Imagenet paper-data, Besiroglu et al., forthcoming) ",Imagenet-1k,,1280000,,,,,,,,,,,,Industry,,"While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.",2023-12-06 04:23,Robi Rahman,,,Multinational,Google Brain,,,,ViT-Huge/14,,,
wave2vec 2.0 LARGE,Speech,Speech completion,"Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli",SOTA Improvement,"Arguably an ""important"" paper? 

Abstract: 
""We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler.""",,https://arxiv.org/abs/2006.11477,2948.00,wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations,2020-10-22,Facebook,Industry,317000000.00,"Section 5.1:
""We consider two model sizes: BASE (95m parameters) and LARGE (317m parameters)
",1.9e+21,"From surveying the authors:

We trained the base model on 64 V100 GPUs for 400k updates. This takes about 3 days to complete. The large model is trained on 128 V100 GPUs for 1 million updates, and this takes about 7 days to complete.

V100 GPU peak: 125TFLOP/s (https://www.nvidia.com/en-gb/data-center/tesla-v100/)
Assume 40% utilization based on default for non-Language domain (https://epochai.org/blog/estimating-training-compute)

64 GPUs * 40% * 125TFLOP/s * 7 days * 24h/day * 3600s/h
~= 1.9E+21 FLOP",LibriSpeech,,727776000,"pg 4, section 4.1

""As unlabeled data we consider the Librispeech corpus [40] without transcriptions containing 960 hours of audio (LS-960) or the audio data from LibriVox (LV-60k). For the latter we follow the preprocessing of [27] resulting in 53.2k hours of audio.""

53.2k h * 13,680 words/h = 727776000 words",,,,,,NVIDIA Tesla V100 DGXS 32 GB,NVIDIA Tesla V100 DGXS 32 GB,Self-supervised learning,$1569.38,,Industry,,"We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the clean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state of the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and pre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech recognition with limited amounts of labeled data.",2023-12-05 04:33,Robi Rahman,,,Multinational,Facebook,,,,wave2vec 2.0 LARGE,,,
ViT-Base/32,Vision,Image representation,"Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby",Highly cited,,,https://arxiv.org/abs/2010.11929,20003.00,An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale,2020-10-22,Google Brain,Industry,86000000.00,Table 1 https://arxiv.org/pdf/2010.11929.pdf,,,,,,,,,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,Multinational,Google Brain,,,,ViT-Base/32,,,
CryptoGRU,Language,,"Bo Feng, Qian Lou, Lei Jiang, Geoffrey C. Fox",,improves latency but not accuracy compared to other cryptographic models,,https://arxiv.org/pdf/2010.11796,12.00,CryptoGRU: Low Latency Privacy-Preserving Text Analysis With GRU,2020-10-22,,,,,,,,,,,,,,,,,,,,,,,,2023-11-21 01:39,Robi Rahman,CryptoGRU,,,,,,,,,,
SimCLRv2,,,"Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey Hinton",Highly cited,,,https://arxiv.org/abs/2006.10029,1791.00,Big self- supervised models are strong semi-supervised learners.,2020-10-26,Google Brain,Industry,795000000.00,"From author communication

We trained different model sizes (from 24M to 795M), and they're summarized in Table 1 of the paper (https://arxiv.org/pdf/2006.10029.pdf).",,,,,1280000,,,,,,,,,,,,Industry,,,2023-11-26 03:25,Robi Rahman,,,Multinational,Google Brain,,,,SimCLRv2,,,
AWD-FWM (WT2),Language,,"Imanol Schlag, Tsendsuren Munkhdalai, Jürgen Schmidhuber",,,,https://arxiv.org/abs/2011.07831,29.00,Learning Associative Inference Using Fast Weight Memory,2020-11-16,,,37000000.00,,739000000000000000.00,,WikiText-2,,,,1600.00,,,,,,,,,,,,,2023-11-23 06:53,Robi Rahman,AWD-FWM (WT2),,,,,,,,,,
AWD-FWM (PTB),Language,,,,,,,,,2020-11-16,,,,,,,,,,,1000.00,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,AWD-FWM (PTB),1,,,,,,,,,
KEPLER,Language,Relation Extraction,"Xiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhiyuan Liu, Juanzi Li, and Jian Tang.",SOTA Improvement,"""Experimental results show that KEPLER achieves state-of-the-art performances
on various NLP tasks""",,https://arxiv.org/abs/1911.06136,400.00,KEPLER: A Unified Model for Knowledge Embedding and Pre- trained Language Representation.,2020-11-23,"Tsinghua University,Mila- Quebec AI,University de Montreal,HEC,CIFAR AI Research,Princeton University",Academia,110000000.00,,124000000000000000000.00,"From author communication

""About 128 GPU-days using Nvidia V100 (16GB). ""

precision: float16

V100 GPU for float16: 28000000000000 (2.8E+13)

0.4 * 28TFLOP/s * 128 GPU-days * 24h/day * 3600s/h
= 1.24E+20


","Wikipedia, BookCorpus","From author communication

    For the language modeling objective, we use Wikipedia+BookCorpus datasets (about 13GB).    For the knowledge embedding objective, we use Wikidata5m (about 1GB).",3300000000,"For BookCorpus + English Wikipedia: 800M + 2500M

For Wikidata5M: 20614279
See table 1. Contains ""entities"", ""relations"", and ""triplets""",,,"From author communication

It depends on the length of the input sequences. The inference computation of KEPLER is the same as RoBERTa (base) and you may estimate it with this.",,,,,Self-supervised learning,$437.97,,Academia,,"Pre-trained language representation models (PLMs) cannot well capture factual knowledge from text. In contrast, knowledge embedding (KE) methods can effectively represent the relational facts in knowledge graphs (KGs) with informative entity embeddings, but conventional KE models cannot take full advantage of the abundant textual information. In this paper, we propose a unified model for Knowledge Embedding and Pre-trained LanguagE Representation (KEPLER), which can not only better integrate factual knowledge into PLMs but also produce effective text-enhanced KE with the strong PLMs. In KEPLER, we encode textual entity descriptions with a PLM as their embeddings, and then jointly optimize the KE and language modeling objectives. Experimental results show that KEPLER achieves state-of-the-art performances on various NLP tasks, and also works remarkably well as an inductive KE model on KG link prediction. Furthermore, for pre-training and evaluating KEPLER, we construct Wikidata5M, a large-scale KG dataset with aligned entity descriptions, and benchmark state-of-the-art KE methods on it. It shall serve as a new KE benchmark and facilitate the research on large KG, inductive KE, and KG with text. The source code can be obtained from this https URL.",2023-12-05 04:33,Robi Rahman,,,"China,Canada,Canada,France,Canada,United States of America","Tsinghua University, Mila- Quebec AI, University de Montreal, HEC, CIFAR AI Research, Princeton University",,,,KEPLER,,,
AlphaFold 2,Other,Protein folding prediction,"John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Kathryn Tunyasuvunakool, Olaf Ronneberger, Russ Bates, Augustin Žídek, Alex Bridgland, Clemens Meyer, Simon A A Kohl, Anna Potapenko, Andrew J Ballard, Andrew Cowie, Bernardino Romera-Paredes, Stanislav Nikolov, Rishub Jain, Jonas Adler, Trevor Back, Stig Petersen, David Reiman, Martin Steinegger, Michalina Pacholska, David Silver, Oriol Vinyals, Andrew W Senior, Koray Kavukcuoglu, Pushmeet Kohli, Demis Hassabis.",Historical significance,,,https://www.nature.com/articles/s41586-021-03819-2,24.00,High Accuracy Protein Structure Prediction Using Deep Learning,2020-11-30,DeepMind,Industry,,,2.99e+21,"123 teraFLOPS / TPU v3 chip * 128 cores * (1 chip / 2 cores) * 11 days * 40% utilization = 2.99e21 FLOP
https://www.wolframalpha.com/input?i=123+teraFLOPS+*+128+*+11+days+*+0.4",,170000 protein structures,170000,,,,,264.0,7 days pretrain and 4 days finetune,Google TPU v3,Google TPU v3,,,,Industry,,,2023-11-26 03:25,Robi Rahman,,,United Kingdom of Great Britain and Northern Ireland,DeepMind,,,,AlphaFold 2,,,
CPM-Large,Language,,"Z Zhang, X Han, H Zhou, P Ke, Y Gu, D Ye, Y Qin, Y Su",SOTA Improvement,"""CPM outperforms CDial-GPT with a large margin in the few-shot experiment, showing the generalization ability of our model.""",,https://arxiv.org/abs/2012.00413,86.00,CPM: A Large-scale Generative Chinese Pre-trained Language Model,2020-12-01,"Tsinghua University,Beijing Academy of Artificial Intelligence",Academia,2600000000.00,"""To the best of our knowledge, CPM, with 2.6 billion parameters and 100GB Chinese training data, is the largest Chinese pre-trained language mode""",1.8e+21,"source: https://lair.lighton.ai/akronomicon/

archived: https://github.com/lightonai/akronomicon/tree/main/akrodb",,,16700000000,"""language model, with 2.6 billion parameters and 100GB Chinese training data.""

We use the conversion factor 1GB ~ 167M words",,,,,,,,Self-supervised learning,$6569.51,"https://towardsdatascience.com/the-future-of-ai-is-decentralized-848d4931a29a#:~:text=Training%20GPT%2D3%20reportedly%20cost,a%20single%20training%20run%C2%B9.",Academia,,"Pre-trained Language Models (PLMs) have proven to be beneficial for various downstream NLP tasks. Recently, GPT-3, with 175 billion parameters and 570GB training data, drew a lot of attention due to the capacity of few-shot (even zero-shot) learning. However, applying GPT-3 to address Chinese NLP tasks is still challenging, as the training corpus of GPT-3 is primarily English, and the parameters are not publicly available. In this technical report, we release the Chinese Pre-trained Language Model (CPM) with generative pre-training on large-scale Chinese training data. To the best of our knowledge, CPM, with 2.6 billion parameters and 100GB Chinese training data, is the largest Chinese pre-trained language model, which could facilitate several downstream Chinese NLP tasks, such as conversation, essay generation, cloze test, and language understanding. Extensive experiments demonstrate that CPM achieves strong performance on many NLP tasks in the settings of few-shot (even zero-shot) learning. The code and parameters are available at this https URL.",2023-12-05 04:33,Robi Rahman,,,"China,China","Tsinghua University, Beijing Academy of Artificial Intelligence",,,,CPM-Large,,,
ESM-1,Language,"Proteins,Protein folding prediction","Alexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, Myle Ott, C. Lawrence Zitnick, Jerry Ma, and Rob Fergus","SOTA Improvement,Highly cited","""We apply the representations to a range of prediction tasks
and find that they improve state-of-art features across the
applications.""",,https://www.pnas.org/doi/abs/10.1073/pnas.2016239118,1148.00,Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences,2020-12-16,"Facebook AI Research,New York University (NYU)",Industry - Academia Collaboration (Industry leaning),669200000.00,,,,UniParc,,,,,,,,,,,,,,,Confident,"In the field of artificial intelligence, a combination of scale in data
and model capacity enabled by unsupervised learning has led to
major advances in representation learning and statistical generation. In the life sciences, the anticipated growth of sequencing
promises unprecedented data on natural sequence diversity. Protein language modeling at the scale of evolution is a logical step
toward predictive and generative artificial intelligence for biology.
To this end, we use unsupervised learning to train a deep contextual
language model on 86 billion amino acids across 250 million protein
sequences spanning evolutionary diversity. The resulting model
contains information about biological properties in its representations. The representations are learned from sequence data alone.
The learned representation space has a multiscale organization
reflecting structure from the level of biochemical properties of
amino acids to remote homology of proteins. Information about
secondary and tertiary structure is encoded in the representations
and can be identified by linear projections. Representation learning
produces features that generalize across a range of applications,
enabling state-of-the-art supervised prediction of mutational effect
and secondary structure and improving state-of-the-art features for
long-range contact prediction.",2023-11-26 03:25,Anonymous,,,"Multinational,United States of America","Facebook AI Research, New York University (NYU)",,,,ESM-1,,,
VQGAN + CLIP,Drawing,Text-to-image,"Patrick Esser, Robin Rombach, Björn Ommer",SOTA Improvement,,,https://arxiv.org/abs/2012.09841,1238.00,Taming Transformers for High-Resolution Image Synthesis,2020-12-17,Heidelberg University,Academia,,,,,,,,I'm confused - I guess they pretrained on several different datasets? I think the model is also able to do zero-shot learning,,,,,,,,,,,Academia,,,2023-12-05 04:33,Robi Rahman,,,Germany,Heidelberg University,,,,VQGAN + CLIP,,,
CT-MoS (WT2),Language,,"Pei-Hsin Wang, Sheng-Iou Hsieh, Shih-Chieh Chang, Yu-Ting Chen, Jia-Yu Pan, Wei Wei, Da-Chang Juan",SOTA Improvement,"""Experimental results confirm that the
proposed method significantly improves state-of-the-art language models, achieving a perplexity of 55.31 and 62.89 on
the test set of Penn Treebank and WikiText-2""",,https://arxiv.org/pdf/2012.13575,6.00,Contextual Temperature for Language Modeling,2020-12-25,"Google,National Tsing Hua University",,45000000.00,,562000000000000000.00,,,,,,1000.00,,,,,,,,,,,,,2023-11-26 03:25,Robi Rahman,CT-MoS (WT2),,"Multinational,Taiwan","Google, National Tsing Hua University",,,,CT-MoS (WT2),,,
CT-MoS + DynamicEval (WT2),Language,,"Pei-Hsin Wang, Sheng-Iou Hsieh, Shih-Chieh Chang, Yu-Ting Chen, Jia-Yu Pan, Wei Wei, Da-Chang Juan",,"other model in the paper (without DynamicEval) was better, per Table 2",,https://arxiv.org/pdf/2012.13575,6.00,Contextual Temperature for Language Modeling,2020-12-25,,,45000000.00,,562000000000000000.00,,,,,,1000.00,,,,,,,,,,,,,2023-11-23 06:53,Robi Rahman,CT-MoS + DynamicEval (WT2),,,,,,,,,,
CT-MoS (PTB),Language,,,,,,,,,2020-12-25,,,,,,,,,,,1000.00,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,CT-MoS (PTB),1,,,,,,,,,
CT-MoS + DynamicEval (PTB),Language,,,,,,,,,2020-12-25,,,,,,,,,,,1000.00,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,CT-MoS + DynamicEval (PTB),1,,,,,,,,,
AraGPT2-Mega,Language,,"W Antoun, F Baly, H Hajj",,,,https://arxiv.org/abs/2012.15520,53.00,AraGPT2: Pre-Trained Transformer for Arabic Language Generation,2020-12-31,American University of Beirut,Academia,1500000000.00,"source: https://lair.lighton.ai/akronomicon/

archived: https://github.com/lightonai/akronomicon/tree/main/akrodb",1.9999999999999997e+21,"source: https://github.com/lightonai/akronomicon/blob/10adaca9c74afa7d11f196947e410d248f25abe9/akrodb/American%20University%20of%20Beirut/AraGPT2-Mega.json

Akronomicon uses units of petaflop/s-days. 20 petaflop/s-days ~= 2e21 FLOP.

Our own validation of this estimate is below.

For the Mega model: 9 days on a TPUv3-128, bfloat16 precision  (from author communication)

A TPUv3-128 has 128 cores (you can infer this from footnote 9 on p.4 of the paper - 128 * 16GB = 2TB). TPUv3 has 2 cores per chip. So 64 chips.

TPUv3 FLOP/s: 1.23E+14

Utilization: use default value of 30% for Language domain (https://epochai.org/blog/estimating-training-compute)

64 chips * 30% * 1.23E+14 FLOP/s * 9 days * 24h/day * 3600s/h
~= 2e21 FLOP",,,8800000000,"""The total dataset size is 77GB with 8.8B words [word count was done after preprocessing, where a white
space is inserted before and after punctuations, brackets, numbers... which increased the total word count]""",,,,,,,,Self-supervised learning,$3685.43,,Academia,,"Recently, pre-trained transformer-based architectures have proven to be very efficient at language modeling and understanding, given that they are trained on a large enough corpus. Applications in language generation for Arabic are still lagging in comparison to other NLP advances primarily due to the lack of advanced Arabic language generation models. In this paper, we develop the first advanced Arabic language generation model, AraGPT2, trained from scratch on a large Arabic corpus of internet text and news articles. Our largest model, AraGPT2-mega, has 1.46 billion parameters, which makes it the largest Arabic language model available. The Mega model was evaluated and showed success on different tasks including synthetic news generation, and zero-shot question answering. For text generation, our best model achieves a perplexity of 29.8 on held-out Wikipedia articles. A study conducted with human evaluators showed the significant success of AraGPT2-mega in generating news articles that are difficult to distinguish from articles written by humans. We thus develop and release an automatic discriminator model with a 98% percent accuracy in detecting model-generated text. The models are also publicly available, hoping to encourage new research directions and applications for Arabic NLP.",2023-12-05 04:33,Robi Rahman,,,Lebanon,American University of Beirut,,,,,,,
Shortformer,Language,,"Ofir Press, Noah A. Smith, Mike Lewis",,,,https://arxiv.org/abs/2012.15832,57.00,Shortformer: Better Language Modeling using Shorter Inputs,2020-12-31,,,24000000.00,,3040000000000000000.00,,WikiText-103,,,,205.00,,,,,,,,,,,,,2023-12-05 04:33,Robi Rahman,Shortformer,,,,,,,,,,
ERNIE-Doc (247M),Language,,"Siyu Ding, Junyuan Shang, Shuohuan Wang, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang",SOTA Improvement,"""ERNIE-DOC improved the state-of-the-art language modeling result of perplexity to 16.8 on WikiText103""",,https://arxiv.org/pdf/2012.15688,35.00,ERNIE-Doc: A Retrospective Long-Document Modeling Transformer,2020-12-31,Baidu,,247000000.00,,29100000000000000000.00,,,,,,190.88,,,,,,,,,,,,,2023-11-26 03:25,Robi Rahman,ERNIE-Doc (247M),,China,Baidu,,,,ERNIE-Doc (247M),,,
ERNIE-Doc (151M),Language,,,,,,,,,2020-12-31,,,,,,,,,,,190.88,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,ERNIE-Doc (151M),1,,,,,,,,,
Subformer (122M),Language,,"Machel Reid, Edison Marrese-Taylor, Yutaka Matsuo",,,,https://arxiv.org/abs/2101.00234,20.00,Subformer: Exploring Weight Sharing for Parameter Efficiency in Generative Transformers,2021-01-01,,,122000000.00,,5300000000000000000.00,,WikiText-103,,,,70.29,,,,,,,,,,,,,2023-12-05 04:33,Robi Rahman,Subformer (122M),,,,,,,,,,
Subformer (83M),Language,,,,,,,,,2021-01-01,,,,,,,,,,,70.29,,,,,,,,,,,Unverified,,2023-12-08 01:01,Robi Rahman,Subformer (83M),1,,,,,,,,,
Subformer (96M),Language,,,,,,,,,2021-01-01,,,,,,,,,,,70.29,,,,,,,,,,,Unverified,,2023-12-08 01:01,Robi Rahman,Subformer (96M),1,,,,,,,,,
DALL-E,Drawing,Text-to-image,"Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, Ilya Sutskever",Significant use,,,https://openai.com/blog/dall-e/,2381.00,Zero-Shot Text-to-Image Generation,2021-01-05,OpenAI,Industry,12000000000.00,DALL·E is a 12-billion parameter version of GPT-3 trained to generate images from text descriptions,4.7e+22,"source: https://lair.lighton.ai/akronomicon/

archived: https://github.com/lightonai/akronomicon/tree/main/akrodb",,,250000000,"""To scale up to 12-billion parameters, we created a dataset of a similar scale to JFT-300M (Sun et al., 2017) by collecting
250 million text-images pairs from the internet. """,,,,,,,,Self-supervised learning,$171537.13,,Industry,,"Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.",2023-12-05 04:33,Robi Rahman,,,United States of America,OpenAI,,,,DALL-E,,,
CLIP (ViT L/14@336px),Multimodal,Zero-shot image classification,"Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever",SOTA Improvement,,,https://arxiv.org/abs/2103.00020,9359.00,Learning Transferable Visual Models From Natural Language Supervision,2021-01-05,OpenAI,Industry,370000000.00,"Image encoder
Vision Transformer
Table 1 in https://arxiv.org/pdf/2010.11929.pdf
Authors fine-tuned ViT L/14 at additional 336px resolution, hence the @336 (See ViT)
307M params

Text encoder
~Transformer (from paper)
63M params",1.05e+22,https://docs.google.com/document/d/156miAJkFN9DDX06C3s03UDsretCtymCKiGDddLBCgQE/edit?usp=sharing,,Custom image-text pairs from the internet,400000000,,,110000000.00,Figure 10 https://arxiv.org/pdf/2103.00020.pdf,,"""In the end, our best performing CLIP model trains on 256 GPUs for 2 weeks which is similar to existing large scale image models.""",,,Self-supervised learning,$40146.99,"https://www.kdnuggets.com/2021/03/beginners-guide-clip-model.html
",Industry,,"State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at this https URL.",2023-12-05 04:33,Robi Rahman,,,United States of America,OpenAI,,,,CLIP (ViT L/14@336px),,,
CLIP (ResNet-50),Multimodal,Zero-shot image classification,"Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever",SOTA Improvement,,,https://arxiv.org/abs/2103.00020,9359.00,Learning Transferable Visual Models From Natural Language Supervision,2021-01-05,OpenAI,Industry,88600000.00,"Image encoder
~ResNet-50 (from paper)
25.6M params

Text encoder
~Transformer (from paper)
63M params",,,,Custom image-text pairs from the internet,400000000,,,7000000.00,Figure 10 https://arxiv.org/pdf/2103.00020.pdf,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,United States of America,OpenAI,,,,CLIP (ResNet-50),,,
Transformer-XL + AutoDropout (WT2),Language,,"Hieu Pham, Quoc V. Le",,,,https://arxiv.org/abs/2101.01761,45.00,AutoDropout: Learning Dropout Patterns to Regularize Deep Networks,2021-01-05,,,35000000.00,,,,WikiText-2,,,,,,,,,,,,,,,,,2023-11-21 01:39,Robi Rahman,Transformer-XL + AutoDropout (WT2),,,,,,,,,,
Transformer-XL + AutoDropout (PTB),Language,,,,,,,,,2021-01-05,,,,,,,,,,,,,,,,,,,,,,Unverified,,2023-12-08 01:01,Robi Rahman,Transformer-XL + AutoDropout (PTB),1,,,,,,,,,
BigSSL,Speech,Audio speech recognition,"Yu Zhang,  Daniel S. Park, Wei Han,James Qin, Anmol Gulati, Joel Shor, Aren Jansen, Yuanzhong Xu, Yanping Huang, Shibo Wang, Zongwei Zhou, Bo Li, Min Ma, William Chan, Jiahui Yu, Yongqiang Wang, Liangliang Cao, Khe Chai Sim, Bhuvana Ramabhadran, Tara N. Sainath, Françoise Beaufays, Zhifeng Chen, Quoc V. Le, Chung-Cheng Chiu, Ruoming Pang and Yonghui Wu",SOTA Improvement,"""In particular, on an ASR task with 34k hours of labeled data, by fine-tuning an 8 billion parameter pre-trained Conformer model we can match state-of-the-art (SoTA) performance with only 3% of the training data and significantly improve SoTA with the full training set""",,https://arxiv.org/abs/2109.13226,109.00,BigSSL: Exploring the Frontier of Large-Scale Semi-Supervised Learning for Automatic Speech Recognition,2021-01-10,"Google,Apple",Industry,8000000000.00,"""... we study the utility of large models, with the parameter count ranging from 600M to 8B...""",,,,,42626880000,"Sum all values in Table VII, and add 34k for English VAD, and 926k for English Youtube = 3116k hours

Note this involves significant self-training: ""Noisy student training (NST) [23], [41] is a self-training
method where a teacher model generates pseudo-labels for a
large unlabeled dataset, which is in turn used to train a student
model with augmentation.""

1 hour ~ 13,680 words
13680 * 3116000 = 42626880000",,,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,"Multinational,United States of America","Google, Apple",,,,BigSSL,,,
Switch,Language,Text autocompletion,"William Fedus, Barret Zoph, Noam Shazeer",SOTA Improvement,""" On ANLI (Nie et al., 2019), Switch XXL improves over the prior state-of-the-art to get a 65.7
accuracy versus the prior best of 49.4 (Yang et al., 2020)... Finally, we also conduct an early examination of the model’s knowledge with three closed-book knowledge-based tasks: Natural
Questions, WebQuestions and TriviaQA, without additional pre-training using Salient Span
Masking (Guu et al., 2020). In all three cases, we observe improvements over the prior stateof-the-art T5-XXL model (without SSM)",,https://arxiv.org/abs/2101.03961,967.00,Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity,2021-01-11,Google,Industry,1600000000000.00,"""Combining expert, model and data parallelism, we design two large Switch Transformer models, one
with 395 billion and 1.6 trillion parameters""",8.22e+22,"Table 4
https://arxiv.org/ftp/arxiv/papers/2104/2104.10350.pdf",,,432000000000,"""In our protocol we pre-train with 220 (1,048,576) tokens
per batch for 550k steps amounting to 576B total tokens.""

1 token ~ 0.75 words",,,,,,,,Self-supervised learning,$149825.60,,Industry,,"In deep learning, models typically reuse the same parameters for all inputs. Mixture of Experts (MoE) defies this and instead selects different parameters for each incoming example. The result is a sparsely-activated model -- with outrageous numbers of parameters -- but a constant computational cost. However, despite several notable successes of MoE, widespread adoption has been hindered by complexity, communication costs and training instability -- we address these with the Switch Transformer. We simplify the MoE routing algorithm and design intuitive improved models with reduced communication and computational costs. Our proposed training techniques help wrangle the instabilities and we show large sparse models may be trained, for the first time, with lower precision (bfloat16) formats. We design models based off T5-Base and T5-Large to obtain up to 7x increases in pre-training speed with the same computational resources. These improvements extend into multilingual settings where we measure gains over the mT5-Base version across all 101 languages. Finally, we advance the current scale of language models by pre-training up to trillion parameter models on the ""Colossal Clean Crawled Corpus"" and achieve a 4x speedup over the T5-XXL model.
",2023-12-05 04:33,Robi Rahman,,,Multinational,Google,,,,Switch,,,
Wu Dao - Wen Yuan,Language,,,,,,https://web.archive.org/web/20230409001523/https://mp.weixin.qq.com/s/BUQWZ5EdR19i40GuFofpBg,0.00,"Tencent: Facing cognition, Zhiyuan Research Institute and several units released a super-large-scale new pre-training model ""Enlightenment·Wenhui""",2021-01-11,Beijing Academy of Artificial Intelligence,Industry - Academia Collaboration (Academia Leaning),2600000000.00,"""It has 2.6 billion parameters and is capable of performing cognitive activities such as memorization, comprehension, retrieval, numerical calculation, multi-language, etc.""

https://medium.com/syncedreview/chinas-gpt-3-baai-introduces-superscale-intelligence-model-wu-dao-1-0-98a573fc4d70",650280960000000000000.00,"64 Nvidia V100 GPUs for two weeks

64 GPUs * 2.8e13 FLOP/s /GPU * 14*24*60*60s * 0.3 [utilization rate]

",,,,,,,,,,,,Self-supervised learning,,,Industry,,,2023-11-23 06:53,Robi Rahman,,,China,Beijing Academy of Artificial Intelligence,,,,,,,
Selfish-RNN (SNT-ASGD) Stacked LSTMs,Language,,"Shiwei Liu, Decebal Constantin Mocanu, Yulong Pei, Mykola Pechenizkiy",,"SOTA for sparse networks, but presumably not in general",,https://arxiv.org/pdf/2101.09048,33.00,Selfish Sparse RNN Training,2021-01-22,,,25200000.00,,14000000000000000.00,,,,,,100.00,,,,,,,,,,,,,2023-12-05 04:33,Robi Rahman,Selfish-RNN (SNT-ASGD) Stacked LSTMs,,,,,,,,,,
Selfish-RNN (ON-LSTM),Language,,,,,,,,,2021-01-22,,,,,,,,,,,1000.00,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,Selfish-RNN (ON-LSTM),1,,,,,,,,,
Selfish-RNN (SNT-ASGD)RHNs,Language,,,,,,,,,2021-01-22,,,,,,,,,,,500.00,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,Selfish-RNN (SNT-ASGD)RHNs,1,,,,,,,,,
Selfish-RNN (AWD-LSTM-MoS),Language,,,,,,,,,2021-01-22,,,,,,,,,,,1000.00,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,Selfish-RNN (AWD-LSTM-MoS),1,,,,,,,,,
top-down frozen classifier,Language,,"Shucong Zhang, Cong-Thanh Do, Rama Doddipatla, Erfan Loweimi, Peter Bell, Steve Renals",SOTA Improvement,"""Table 2 demonstrates that, to the best of our knowledge, top-down training results in state-of-the art character error rates for LSTM-based endto-end models on WSJ""",,https://arxiv.org/pdf/2102.04697,2.00,Train your classifier first: Cascade Neural Networks Training from upper layers to lower layers,2021-02-09,"University of Edinburgh,Toshiba Cambridge Research Laboratory",,,,,,,,,,,,,,,,,,,,,,,2023-11-26 03:25,Robi Rahman,top-down frozen classifier,,"United Kingdom of Great Britain and Northern Ireland,United Kingdom of Great Britain and Northern Ireland","University of Edinburgh, Toshiba Cambridge Research Laboratory",,,,top-down frozen classifier,,,
Rational DQN Average,Games,Atari Games,"Q Delfosse, P Schramowski, A Molina",SOTA improvement,,,https://openreview.net/forum?id=gnRmI8TatHV,4.00,Recurrent Rational Networks,2021-02-18,TU Darmstadt,Academia,1683456.00,See figure 7,,,,,,,,,,,,,,,,,Academia,,,2023-11-26 03:25,Robi Rahman,,,Germany,TU Darmstadt,,,,Rational DQN Average,,,
Linear Transformer (large),Language,,"Imanol Schlag, Kazuki Irie, Jürgen Schmidhuber",,,,https://arxiv.org/pdf/2102.11174.pdf,105.00,Linear Transformers Are Secretly Fast Weight Programmers,2021-02-22,,,90000000.00,,3890000000000000000.00,,,,,,70.00,,,,,,,,,,,,,2023-12-05 04:33,Robi Rahman,Linear Transformer (large),,,,,,,,,,
Linear Transformer (small),Language,,,,,,,,,2021-02-22,,,,,,,,,,,120.00,,,,,,,,,,,Unverified,,2023-12-08 01:01,Robi Rahman,Linear Transformer (small),1,,,,,,,,,
SRU++ Large,Language,,Tao Lei,SOTA Improvement,"""our model achieves a state-of-the-art result on the ENWIK8 dataset using 1.6 days of training on an 8-GPU machine. """,,https://arxiv.org/abs/2102.12459,36.00,When Attention Meets Fast Recurrence: Training Language Models with Reduced Compute,2021-02-24,ASAPP,,234000000.00,,11000000000000000000.00,,WikiText-103,,,,34.08,,,,,,,,,,,,,2023-12-05 04:33,Robi Rahman,SRU++ Large,,United States of America,ASAPP,,,,SRU++ Large,,,
SRU++ Base,Language,,,,,,,,,2021-02-24,,,,,,,,,,,25.56,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,SRU++ Base,1,,,,,,,,,
SRU++ Large only 2 attention layers (k=5),Language,,,,,,,,,2021-02-24,,,,,,,,,,,34.08,,,,,,,,,,,Unverified,,2023-12-08 01:01,Robi Rahman,SRU++ Large only 2 attention layers (k=5),1,,,,,,,,,
Wu Dao - Wen Hui,Multimodal,,,,,,https://medium.com/syncedreview/chinas-gpt-3-baai-introduces-superscale-intelligence-model-wu-dao-1-0-98a573fc4d70,0.00,China's GPT-3? BAAI Introduces Superscale Intelligence Model 'Wu Dao 1.0',2021-03-01,Beijing Academy of Artificial Intelligence,Non-profit,11300000000.00,"""Wu Dao — Wen Hui has reached 11.3 billion parameters, and through simple fine-tuning can generate poetry, make videos, draw pictures, retrieve text, perform complex reasoning, etc.""

https://medium.com/syncedreview/chinas-gpt-3-baai-introduces-superscale-intelligence-model-wu-dao-1-0-98a573fc4d70",116121600000000000000.00,"64 Nvidia V100 GPUs for 2.5 days

64 GPUs * 2.8e13 FLOP/s /GPU * 2.5*24*60*60s* 0.3 [utilization rate]

",,,,,,,,,,,,Self-supervised learning,,,Industry,,,2023-11-23 06:53,Robi Rahman,,,China,Beijing Academy of Artificial Intelligence,,,,,,,
Wu Dao - Wen Lan,Multimodal,,,,,,https://medium.com/syncedreview/chinas-gpt-3-baai-introduces-superscale-intelligence-model-wu-dao-1-0-98a573fc4d70,0.00,China's GPT-3? BAAI Introduces Superscale Intelligence Model 'Wu Dao 1.0',2021-03-01,Beijing Academy of Artificial Intelligence,Non-profit,1000000000.00,"""Currently, the model has 1 billion parameters and is trained on 50 million graphic pairs collected from open sources.""

https://medium.com/syncedreview/chinas-gpt-3-baai-introduces-superscale-intelligence-model-wu-dao-1-0-98a573fc4d70",7.1995392e+21,"128 Nvidia A100 GPUs for 7 days

128 GPUs * 3.1e14 FLOP/s /GPU * 7*24*60*60s* 0.3 [utilization rate]

",,,,,,,,,,,,Self-supervised learning,,,Industry,,,2023-11-23 06:53,Robi Rahman,,,China,Beijing Academy of Artificial Intelligence,,,,,,,
Wu Dao - Wen Su,Other,Proteins,,,,,https://medium.com/syncedreview/chinas-gpt-3-baai-introduces-superscale-intelligence-model-wu-dao-1-0-98a573fc4d70,0.00,China's GPT-3? BAAI Introduces Superscale Intelligence Model 'Wu Dao 1.0',2021-03-01,Beijing Academy of Artificial Intelligence,Non-profit,,,,,,,,,,,,,,,,Self-supervised learning,,,Industry,,,2023-11-21 01:39,Robi Rahman,,,China,Beijing Academy of Artificial Intelligence,,,,,,,
Meta Pseudo Labels,Vision,Image Classification,"Hieu Pham, Zihang Dai, Qizhe Xie, Minh-Thang Luong, and Quoc V. Le",SOTA Improvement,,,https://arxiv.org/abs/2003.10580,467.00,Meta pseudo labels,2021-03-01,Google Brain,Industry,480000000.00,"Table 4
 480M",4.79e+22,"From communication with author:

22671 TPU days on specific hardware.

Which hardware did you use and in which configuration?
2048 cores of TPU v3.

Precision: Mixed. bfloat16 for activations, float32 for weights and optimizer slots.

2048 TPUv3 cores means 1024 TPUv3 chips, and the spec is 123e12 FLOP/second per chip with bfloat16 precision (Source: https://cloud.google.com/tpu/docs/system-architecture-tpu-vm)

So the compute estimate is:
1024 chips * 123e12 FLOP/second * 0.4 utilization * 11 days * 24 * 60 * 60 = 4.788191232e+22 FLOP",ImageNet,,130000000,"Section 4
Datasets. For this experiment, we use the entire ImageNet
training set as labeled data, and use the JFT dataset as unlabeled data. The JFT dataset has 300 million images, and
then is filtered down to 130 million images by Noisy Student
using confidence thresholds and up-sampling [77]. We use
the same 130 million images as Noisy Student",,,,,,,,Self-supervised learning,$369462.82,,Industry,,"We present Meta Pseudo Labels, a semi-supervised learning method that achieves a new state-of-the-art top-1 accuracy of 90.2% on ImageNet, which is 1.6% better than the existing state-of-the-art. Like Pseudo Labels, Meta Pseudo Labels has a teacher network to generate pseudo labels on unlabeled data to teach a student network. However, unlike Pseudo Labels where the teacher is fixed, the teacher in Meta Pseudo Labels is constantly adapted by the feedback of the student's performance on the labeled dataset. As a result, the teacher generates better pseudo labels to teach the student. Our code will be available at this https URL.",2023-12-05 04:33,Robi Rahman,,,Multinational,Google Brain,,,,Meta Pseudo Labels,,,
M6-10B,Multimodal,,"J Lin, R Men, A Yang, C Zhou, M Ding, Y Zhang",,,,https://arxiv.org/abs/2103.00823,109.00,M6: A Chinese Multimodal Pretrainer,2021-03-01,"Tsinghua University,Alibaba",Industry - Academia Collaboration (Industry leaning),10000000000.00,"""We scale the
model size up to 10 billion and 100 billion parameters, and build
the largest pretrained model in Chinese.""",,"""We implement M6-100B with around 100 billion parameters
on 128 Nvidia A100s and the speed of pretraining achieves 1440
samples/s (for samples of the sequence length of 272).""

Their response to our email doesn't say enough to tell us what the compute is for this paper, but allows us to determine the compute for the follow-up paper with the M6-10T model (but we knew this already)",,,1900000000000,"""1.9TB images and 292GB texts""

TODO: figure out what to do for multimodal pretraining datasets",,,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,"China,China","Tsinghua University, Alibaba",,,,,,,
M6-100B,Multimodal,,"J Lin, R Men, A Yang, C Zhou, M Ding, Y Zhang",,,,https://arxiv.org/abs/2103.00823,109.00,M6: A Chinese Multimodal Pretrainer,2021-03-01,"Tsinghua University,Alibaba",Industry - Academia Collaboration (Industry leaning),100000000000.00,"""We scale the
model size up to 10 billion and 100 billion parameters, and build
the largest pretrained model in Chinese.""",,,,,1900000000000,"""1.9TB images and 292GB texts""

TODO: figure out what to do for multimodal pretraining datasets",,,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,"China,China","Tsinghua University, Alibaba",,,,,,,
RFA-GATE-Gaussian-Stateful Big,Language,,"Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah A. Smith, Lingpeng Kong",,,,https://arxiv.org/abs/2103.02143,225.00,Random Feature Attention,2021-03-03,,,242000000.00,,7140000000000000000.00,,WikiText-103,,,,47.72,,,,,,,,,,,,,2023-11-23 06:53,Robi Rahman,RFA-GATE-Gaussian-Stateful Big,,,,,,,,,,
M6-T,Multimodal,,"An Yang, Junyang Lin, Rui Men, Chang Zhou, Le Jiang, Xianyan Jia, Ang Wang, Jie Zhang, Jiamang Wang, Yong Li, Di Zhang, Wei Lin, Lin Qu, Jingren Zhou, Hongxia Yang",SOTA Improvement,"Improves on hardware SOTA for similar problems

Abstract: 
""We push the model
scale to over 1 trillion parameters and implement it on solely 480 NVIDIA V100-32GB GPUs, in comparison with the recent SOTAs [11; 6] on 2048 TPU cores.""",,https://arxiv.org/abs/2105.15082,76.00,M6-T: Exploring Sparse Expert Models and Beyond,2021-03-05,Alibaba,Industry,1000000000000.00,"Section 4, pg 8:
""Due to limited computational resources, we attempt to figure out solutions to implement a 1-trillion-parameter model on solely 480 NVIDIA V100-32GB GPUs.""",5.5e+21,Estimate taken from https://www.governance.ai/research-paper/recent-trends-chinas-llm-landscape,M6-Corpus,,1900000000000,Images,,,,,,NVIDIA Tesla V100 DGXS 32 GB,NVIDIA Tesla V100 DGXS 32 GB,Self-supervised learning,,,Industry,Likely,"Mixture-of-Experts (MoE) models can achieve promising results with outrageous large amount of parameters but constant computation cost, and thus it has become a trend in model scaling. Still it is a mystery how MoE layers bring quality gains by leveraging the parameters with sparse activation. In this work, we investigate several key factors in sparse expert models. We observe that load imbalance may not be a significant problem affecting model quality, contrary to the perspectives of recent studies, while the number of sparsely activated experts k and expert capacity C in top-k routing can significantly make a difference in this context. Furthermore, we take a step forward to propose a simple method called expert prototyping that splits experts into different prototypes and applies k top-1 routing. This strategy improves the model quality but maintains constant computational costs, and our further exploration on extremely large-scale models reflects that it is more effective in training larger models. We push the model scale to over 1 trillion parameters and implement it on solely 480 NVIDIA V100-32GB GPUs, in comparison with the recent SOTAs on 2048 TPU cores. The proposed giant model achieves substantial speedup in convergence over the same-size baseline.",2023-11-26 03:25,Robi Rahman,,,China,Alibaba,,,,M6-T,,,
Generative BST,Language,,"Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Kurt Shuster, Eric M. Smith, Y-Lan Boureau, Jason Weston",SOTA Improvement,"Abstract:
""Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.""",,https://arxiv.org/abs/2004.13637,750.00,Recipes for building an open-domain chatbot,2021-03-05,Facebook AI Research,Industry,9400000000.00,"Abstract:
""We build variants of these recipes with 90M, 2.7B and 9.4B parameter models""",,"Unclear - no mention of GPUs used, or training time, and the architecture is terribly complicated",,"Section 6:
Pushshfit.io Reddit, ConvAI 2, Wizard of Wikipedia",,,,,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,Multinational,Facebook AI Research,,,,Generative BST,,,
GLM-10B,Language,,"Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, Jie Tang",,smaller version of the model in this paper,,https://arxiv.org/abs/2103.10360,331.00,GLM: General Language Model Pretraining with Autoregressive Blank Infilling,2021-03-18,,,10000000000.00,,3.79e+22,,,,,,1.00,,,,,,,,,,,,,2023-12-05 04:33,Robi Rahman,GLM-10B,,,,,,,,,,
GLM-2B,Language,,,,,,,,,2021-03-18,,,,,,,,,,,1.00,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,GLM-2B,1,,,,,,,,,
GLM-10B-bidirectional,Language,,,,,,,,,2021-03-18,,,,,,,,,,,1.00,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,GLM-10B-bidirectional,1,,,,,,,,,
GLM-10B-unidirectional,Language,,,,,,,,,2021-03-18,,,,,,,,,,,1.00,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,GLM-10B-unidirectional,1,,,,,,,,,
GPT-Neo,Language,,,,,,https://www.eleuther.ai/projects/gpt-neo/,0.00,GPT-Neo,2021-03-21,EleutherAI,Research collective,2700000000.00,"source: https://www.eleuther.ai/projects/gpt-neo/

Note: Directory of LLMs (https://docs.google.com/spreadsheets/d/1gc6yse74XCwBx028HV_cvdxwXkmXejVjkO-Mz2uwE0k/edit#gid=0) gives a somewhat lower estimate (2e9)",7.9e+21,source: https://www.aitracker.org/,The Pile,,885837004800,"""In aggregate, the Pile consists of over 825GiB of raw text data""

(see GPT-NeoX)",,,,,,,,Self-supervised learning,$13685.99,,Industry,,,2023-11-23 06:53,Robi Rahman,,,Multinational,EleutherAI,,,,,,,
GPT-Neo-2.7B,Language,,"Sid Black, Leo Gao, Phil Wang, Connor Leahy, Stella Biderman",,,,https://github.com/EleutherAI/gpt-neo,,GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow,2021-03-21,,,2700000000.00,,6.48e+21,,,,,,1.00,,,,,,,,,,,,,2023-11-23 06:53,Robi Rahman,GPT-Neo-2.7B,,,,,,,,,,
GPT-Neo-2.7B (finetuned),Language,,,,,,,,,2021-03-21,,,,,,,,,,,1.00,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,GPT-Neo-2.7B (finetuned),1,,,,,,,,,
GPT-Neo-2.7B (finetuned on PTB),Language,,,,,,,,,2021-03-21,,,,,,,,,,,1.00,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,GPT-Neo-2.7B (finetuned on PTB),1,,,,,,,,,
GPT-Neo-125M,Language,,,,,,,,,2021-03-21,,,,,,,,,,,1.00,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,GPT-Neo-125M,1,,,,,,,,,
GPT-Neo-1.3B,Language,,,,,,,,,2021-03-21,,,,,,,,,,,1.00,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,GPT-Neo-1.3B,1,,,,,,,,,
GPT-Neo-125M(finetuned),Language,,,,,,,,,2021-03-21,,,,,,,,,,,1.00,,,,,,,,,,,Unverified,,2023-12-08 01:01,Robi Rahman,"GPT-Neo-125M(finetuned),GPT-Neo-125M(finetuned)",1,,,,,,,,,
GPT-Neo-1.3B (finetuned),Language,,,,,,,,,2021-03-21,,,,,,,,,,,1.00,,,,,,,,,,,Unverified,,2023-12-08 01:01,Robi Rahman,GPT-Neo-1.3B (finetuned),1,,,,,,,,,
T2R + Random Init,Language,,"Jungo Kasai, Hao Peng, Yizhe Zhang, Dani Yogatama, Gabriel Ilharco, Nikolaos Pappas, Yi Mao, Weizhu Chen, Noah A. Smith",,,,https://arxiv.org/abs/2103.13076,38.00,Finetuning Pretrained Transformers into RNNs,2021-03-24,,Industry - Academia Collaboration,450000000.00,,61000000000000000000.00,,,,,,205.48,,,,,,,,,,,,,2023-11-23 06:53,Robi Rahman,T2R + Random Init,,,,,,,,,,
T2R 75% + Pretrain,Language,,,,,,,,,2021-03-24,,,,,,,,,,,34.47,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,T2R 75% + Pretrain,1,,,,,,,,,
T2R + Pretrain,Language,,,,,,,,,2021-03-24,,,,,,,,,,,34.47,,,,,,,,,,,Unverified,,2023-12-08 01:01,Robi Rahman,T2R + Pretrain,1,,,,,,,,,
TransfoRNN(d=1024)(2-layer) (WT2),Language,,"Tze Yuang Chong, Xuyang Wang, Lin Yang, Junjie Wang",,,,https://arxiv.org/pdf/2104.01572,0.00,TransfoRNN: Capturing the Sequential Information in Self-Attention Representations for Language Modeling,2021-04-04,,,97600000.00,,,,,,,,,,,,,,,,,,,,,2023-11-21 01:39,Robi Rahman,TransfoRNN(d=1024)(2-layer) (WT2),,,,,,,,,,
TransfoRNN(d=1024)(2-layer) (PTB),Language,,,,,,,,,2021-04-04,,,,,,,,,,,,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,TransfoRNN(d=1024)(2-layer) (PTB),1,,,,,,,,,
Transformer-C,Language,,"Simeng Sun, Mohit Iyyer",,,,https://arxiv.org/abs/2104.03474,10.00,Revisiting Simple Neural Probabilistic Language Models,2021-04-08,,,148000000.00,,21000000000000000.00,,WikiText-103,,,,19.88,,,,,,,,,,,,,2023-11-23 06:53,Robi Rahman,Transformer-C,,,,,,,,,,
Megatron-LM (1T),Language,Text autocompletion,"Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Patwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan Catanzaro, Amar Phanishayee, Matei Zaharia",,Improved SOTA efficiency at distributed training.,,https://arxiv.org/abs/2104.04473,217.00,Efficient Large-Scale Language Model Training on GPU Clusters,2021-04-09,"Microsoft Research,NVIDIA,Stanford University",Industry - Academia Collaboration (Industry leaning),1000000000000.00,"[NOTE: They didn't train the model fully end-to-end, probably just to obtain enough information to gauge the ability to do model parallelisation]

""Our approach allows us to perform training iterations on a model with 1 trillion parameters at 502 petaFLOP/s on 3072 GPUs with achieved per-GPU throughput of 52% of theoretical peak.""",,"NOTE: They didn't train the model fully end-to-end, probably just to obtain enough information to gauge the ability to do model parallelisation.

We calculate below the FLOP required for a full training, but we do not populate it in the Training Compute column.

“For the 1 trillion parameter model, we assume that 450 billion tokens are needed for end-to-end training. With 3072 A100 GPUs, we can achieve a per-GPU throughput of 163 teraFLOP/s, and end-to-end training time of 84 days. We believe these training times (using a reasonable number of GPUs) are practical.”

Table 1 gives a utilisation rate of 52%

Plugging this into the calculator: https://epochai.org/blog/estimating-training-compute
84 days, 3072 GPUs, NVIDIA A100, FP16, 52% utilisation rate --> 3.6e24 FLOP",,Dataset information not provided.,,,,,,,,NVIDIA A100,NVIDIA A100,Self-supervised learning,,,Industry,Likely,"Large language models have led to state-of-the-art accuracies across a range of tasks. However, training these models efficiently is challenging for two reasons: a) GPU memory capacity is limited, making it impossible to fit large models on even a multi-GPU server, and b) the number of compute operations required to train these models can result in unrealistically long training times. Consequently, new methods of model parallelism such as tensor and pipeline parallelism have been proposed. Unfortunately, naive usage of these methods leads to fundamental scaling issues at thousands of GPUs, e.g., due to expensive cross-node communication or devices spending significant time waiting on other devices to make progress.
In this paper, we show how different types of parallelism methods (tensor, pipeline, and data parallelism) can be composed to scale to thousands of GPUs and models with trillions of parameters. We survey techniques for pipeline parallelism and propose a novel interleaved pipeline parallelism schedule that can improve throughput by 10+% with memory footprint comparable to existing approaches. We quantitatively study the trade-offs between tensor, pipeline, and data parallelism, and provide intuition as to how to configure distributed training of a large model. Our approach allows us to perform training iterations on a model with 1 trillion parameters at 502 petaFLOP/s on 3072 GPUs with achieved per-GPU throughput of 52% of theoretical peak.",2023-11-21 01:39,Robi Rahman,,,"United States of America,United States of America,United States of America","Microsoft Research, NVIDIA, Stanford University",,,,,,,
DLRM-12T,Recommendation,Recommender system,"Dheevatsa Mudigere, Yuchen Hao, Jianyu Huang, Zhihao Jia, Andrew Tulloch, Srinivas Sridharan, Xing Liu, Mustafa Ozdal, Jade Nie, Jongsoo Park, Liang Luo, Jie Amy Yang, Leon Gao, Dmytro Ivchenko, Aarti Basant, Yuxi Hu, Jiyan Yang, Ehsan K. Ardestani, Xiaodong Wang, Rakesh Komuravelli, Ching-Hsiang Chu, Serhat Yilmaz, Huayu Li, Jiyuan Qian, Zhuobo Feng, Yinbin Ma, Junjie Yang, Ellie Wen, Hong Li, Lin Yang, Chonglin Sun, Whitney Zhao, Dimitry Melts, Krishna Dhulipala, KR Kishore, Tyler Graf, Assaf Eisenman, Kiran Kumar Matam, Adi Gangidi, Guoqiang Jerry Chen, Manoj Krishnan, Avinash Nayak, Krishnakumar Nair, Bharath Muthiah, Mahmoud khorashadi, Pallab Bhattacharya, Petr Lapukhov, Maxim Naumov, Ajit Mathews, Lin Qiao, Mikhail Smelyanskiy, Bill Jia, Vijay Rao",,,,https://arxiv.org/abs/2104.05158,72.00,Software-Hardware Co-design for Fast and Scalable Training of Deep Learning Recommendation Models,2021-04-12,"Meta AI,Carnegie Mellon University (CMU)",Industry - Academia Collaboration (Industry leaning),12000000000000.00,They instantiated a 12T-parameter model to show that their hardware setup can train it despite the huge memory requirements.,,No training details provided.,,No training details provided.,,No training details provided.,,,No inference details provided.,,No training details provided.,NVIDIA A100,NVIDIA A100,,,No training details provided.,,Confident,"Deep learning recommendation models (DLRMs) are used across many business-critical services at Facebook and are the single largest AI application in terms of infrastructure demand in its data-centers. In this paper we discuss the SW/HW co-designed solution for high-performance distributed training of large-scale DLRMs. We introduce a high-performance scalable software stack based on PyTorch and pair it with the new evolution of Zion platform, namely ZionEX. We demonstrate the capability to train very large DLRMs with up to 12 Trillion parameters and show that we can attain 40X speedup in terms of time to solution over previous systems. We achieve this by (i) designing the ZionEX platform with dedicated scale-out network, provisioned with high bandwidth, optimal topology and efficient transport (ii) implementing an optimized PyTorch-based training stack supporting both model and data parallelism (iii) developing sharding algorithms capable of hierarchical partitioning of the embedding tables along row, column dimensions and load balancing them across multiple workers; (iv) adding high-performance core operators while retaining flexibility to support optimizers with fully deterministic updates (v) leveraging reduced precision communications, multi-level memory hierarchy (HBM+DDR+SSD) and pipelining. Furthermore, we develop and briefly comment on distributed data ingestion and other supporting services that are required for the robust and efficient end-to-end training in production environments.",2023-12-05 04:33,Anonymous,,,"Multinational,United States of America","Meta AI, Carnegie Mellon University (CMU)",,,,,,,
PLUG,Language,,,SOTA Improvement,Was a SOTA in CLUE 1.0 https://www.cluebenchmarks.com/classification10.html,,https://mp.weixin.qq.com/s/DAQomIkDa52Sef-ruyH5qg,0.00,,2021-04-19,Alibaba,Industry,27000000000.00,,3.5997695999999996e+22,128 Nvidia A100 for 35 days,,,,,,,,,,,,,,,,,,2023-11-26 03:25,Robi Rahman,,,China,Alibaba,,,,PLUG,,,
DiffQ Transformer (16L),Language,,"Alexandre Défossez, Yossi Adi, Gabriel Synnaeve",,,,https://arxiv.org/abs/2104.09987,24.00,Differentiable Model Compression via Pseudo Quantization Noise,2021-04-20,,,257000000.00,,3360000000000000000.00,,WikiText-103,,,,,,,,,,,,,,,,,2023-12-05 04:33,Robi Rahman,DiffQ Transformer (16L),,,,,,,,,,
PanGu-α,Language,,"Wei Zeng, Xiaozhe Ren, Teng Su, Hui Wang, Yi LiaoZhiwei WangXin JiangZhenzhang YangKaisheng WangXiaoda ZhangChen LiZiyan GongYifan YaoXinjing HuangJun WangJianfeng YuQi GuoYue YuYan ZhangJin WangHengtao TaoDasen YanZexuan YiFang PengFangqing JiangHan ZhangLingfeng DengYehong ZhangZhe LinChao ZhangShaojie ZhangMingyue GuoShanzhi GuGaojun FanYaowei WangXuefeng JinQun LiuYonghong Tian",,,,https://arxiv.org/abs/2104.12369,138.00,PanGu-α: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation,2021-04-25,Huawei Noah's Ark Lab,Industry,207000000000.00,"Table in https://git.openi.org.cn/PCL-Platform.Intelligence/PanGu-Alpha

Note: Directory of LLMs (https://docs.google.com/spreadsheets/d/1gc6yse74XCwBx028HV_cvdxwXkmXejVjkO-Mz2uwE0k/edit#gid=0)
gives a slightly lower estimate, not sure about source",5.83e+22,"source: https://lair.lighton.ai/akronomicon/

archived: https://github.com/lightonai/akronomicon/blob/main/akrodb/Huawei/PanGu-%CE%B1.json",,Custom dataset,200000000000,"""The composition of our corpus and the processing steps adopted to each data source is shown in Table 3.2.
Based on the new corpus, we construct two training datasets with 100GB and 1TB text data for our medium (2.6B and 13B) and large (200B) models, respectively""

1 TB = 1000 GB
1 GB ~ 200M words",,,,,,,,Self-supervised learning,$97802.06,,Industry,,"Large-scale Pretrained Language Models (PLMs) have become the new paradigm for Natural Language Processing (NLP). PLMs with hundreds of billions parameters such as GPT-3 have demonstrated strong performances on natural language understanding and generation with \textit{few-shot in-context} learning. In this work, we present our practice on training large-scale autoregressive language models named PanGu-α, with up to 200 billion parameters. PanGu-α is developed under the MindSpore and trained on a cluster of 2048 Ascend 910 AI processors. The training parallelism strategy is implemented based on MindSpore Auto-parallel, which composes five parallelism dimensions to scale the training task to 2048 processors efficiently, including data parallelism, op-level model parallelism, pipeline model parallelism, optimizer model parallelism and rematerialization. To enhance the generalization ability of PanGu-α, we collect 1.1TB high-quality Chinese data from a wide range of domains to pretrain the model. We empirically test the generation ability of PanGu-α in various scenarios including text summarization, question answering, dialogue generation, etc. Moreover, we investigate the effect of model scales on the few-shot performances across a broad range of Chinese NLP tasks. The experimental results demonstrate the superior capabilities of PanGu-α in performing various tasks under few-shot or zero-shot settings.",2023-12-05 04:33,Robi Rahman,,,China,Huawei Noah's Ark Lab,,,,,,,
SPALM + kNN,Language,,"Dani Yogatama, Cyprien de Masson d’Autume, Lingpeng Kong",,"paper says ""Our implementation produces results that are in the same range as state-of-the-art numbers, demonstrating the strength of our baselines"" I think this suggests it's close to, but not reaching SOTA?",,https://web.archive.org/web/20230210050534/https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00371/100688/Adaptive-Semiparametric-Language-Models,70.00,Adaptive Semiparametric Language Models,2021-04-26,,Industry - Academia Collaboration,,,,,,,,,,,,,,,,,,,,,,2023-11-21 01:39,Robi Rahman,SPALM + kNN,,,,,,,,,,
GPT-J-6B,Language,,Aran Komatsuzaki,,,Fully open-source,https://arankomatsuzaki.wordpress.com/2021/06/04/gpt-j/,0.00,GPT-J-6B: 6B JAX-Based Transformer,2021-05-01,,Research collective,6053381344.00,source: model details table in GitHub,1.5e+22,source: zero shot evaluation table in GitHub,,,160000000000,"""The model was trained on 400B tokens from The Pile dataset with 800GB text.""

1 GB ~ 200M words",1.00,,,,,,,Self-supervised learning,$25176.80,,Industry,,,2023-11-23 06:53,Robi Rahman,GPT-J-6B,,,,,,,,,,
Transformer-XL + SIS,Language,,"Sagar Verma, Jean-Christophe Pesquet",,,,https://web.archive.org/web/20220122141508/http://proceedings.mlr.press/v139/verma21b/verma21b.pdf,10.00,Sparsifying Networks via Subdifferential Inclusion,2021-05-03,,Industry - Academia Collaboration,246000000.00,,10400000000000000000.00,,,,,,,,,,,,,,,,,,,2023-11-23 06:53,Robi Rahman,Transformer-XL + SIS,,,,,,,,,,
ProtT5-XXL,Other,Proteins,"A Elnaggar, M Heinzinger, C Dallago, G Rihawi",SOTA Improvement,"""For the per-residue predictions the transfer of the most informative embeddings (ProtT5) for the first time outperformed the state-of-the-art
without using evolutionary information""",,https://www.biorxiv.org/content/10.1101/2020.07.12.199554v3,396.00,ProtTrans: Towards Cracking the Language of Life’s Code Through Self-Supervised Learning,2021-05-04,"Technical University of Munich,Med AI Technology,NVIDIA,Oak Ridge National Laboratory,Google",Industry - Academia Collaboration,11000000000.00,"source: https://lair.lighton.ai/akronomicon/

archived: https://github.com/lightonai/akronomicon/tree/main/akrodb",7.370000000000001e+22,"source: https://lair.lighton.ai/akronomicon/

archived: https://github.com/lightonai/akronomicon/tree/main/akrodb",UniRef; BDF,,393000000000,"""Here, we trained two auto-regressive models (Transformer-XL, XLNet) and four auto-encoder models (BERT, Albert, Electra, T5) on data from UniRef and BFD containing up to 393 billion amino acids.""",,,,,,,,Self-supervised learning,$123918.36,,Industry,,"Computational biology and bioinformatics provide vast data gold-mines from protein sequences, ideal for Language Models taken from NLP. These LMs reach for new prediction frontiers at low inference costs. Here, we trained two auto-regressive models (Transformer-XL, XLNet) and four auto-encoder models (BERT, Albert, Electra, T5) on data from UniRef and BFD containing up to 393 billion amino acids. The LMs were trained on the Summit supercomputer using 5616 GPUs and TPU Pod up-to 1024 cores. Dimensionality reduction revealed that the raw protein LM-embeddings from unlabeled data captured some biophysical features of protein sequences. We validated the advantage of using the embeddings as exclusive input for several subsequent tasks. The first was a per-residue prediction of protein secondary structure (3-state accuracy Q3=81%-87%); the second were per-protein predictions of protein sub-cellular localization (ten-state accuracy: Q10=81%) and membrane vs. water-soluble (2-state accuracy Q2=91%). For the per-residue predictions the transfer of the most informative embeddings (ProtT5) for the first time outperformed the state-of-the-art without using evolutionary information thereby bypassing expensive database searches. Taken together, the results implied that protein LMs learned some of the grammar of the language of life. To facilitate future work, we released our models at https://github.com/agemagician/ProtTrans.",2023-11-26 03:25,Robi Rahman,,,"Germany,China,United States of America,United States of America,Multinational","Technical University of Munich, Med AI Technology, NVIDIA, Oak Ridge National Laboratory, Google",,,,ProtT5-XXL,,,
ADM,Drawing,Image generation,"Prafulla Dhariwal, Alex Nichol","Highly cited,SOTA Improvement","""We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models""",,https://arxiv.org/abs/2105.05233,2573.00,Diffusion Models Beat GANs on Image Synthesis,2021-05-11,OpenAI,Industry,,,499999999999999930000.00,"For LSUN horse, they report 116 V100-days in pre-training.

125 teraFLOP/s * 116 * 24 * 3600 * 0.4 = 5e20",LSUN,"""To evaluate our improved model architecture on unconditional image generation, we train separate diffusion models on three LSUN [71] classes: bedroom, horse, cat""",,,,,,,,NVIDIA V100,NVIDIA V100,,,,,Likely,"We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128×128, 4.59 on ImageNet 256×256, and 7.72 on ImageNet 512×512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256×256 and 3.85 on ImageNet 512×512.",2023-12-05 04:33,Anonymous,,,United States of America,OpenAI,,,,ADM,,,
Fairseq + UID: variance,Language,,"Jason Wei, Clara Meister, Ryan Cotterell",,,,https://web.archive.org/web/20221010230611/https://arxiv.org/pdf/2105.07144.pdf,16.00,A Cognitive Regularizer for Language Modeling,2021-05-15,,Academia,,,,,,,,,,,,,,,,,,,,,,2023-12-05 04:33,Robi Rahman,Fairseq + UID: variance,,,,,,,,,,
ConSERT,Language,Language modelling,"Yuanmeng Yan, Rumei Li, Sirui Wang, Fuzheng Zhang, Wei Wu, Weiran Xu",SOTA Improvement,Trains an effective BERT model on small sample sizes and achieves an 8% improvement over previous SOTA on STA datasets.,,https://arxiv.org/abs/2105.11741,344.00,ConSERT: A contrastive framework for self-supervised sentence representation transfer,2021-05-25,"Meituan University,Beijing University of Posts and Telecommunications",Academia,345000000.00,,280000000000000000000.00,"Fine-tuning was done using a single Nvidia V100 GPU for a few minutes -> 1.0E+15 to 5.0E+15 (2 to 10 min)

Foundation model is BeRT with 2.8e+20 FLOP.

So total compute is 2.8e+20.",,,,,,,,0.1,,NVIDIA Tesla V100S PCIe 32 GB,NVIDIA Tesla V100S PCIe 32 GB,,,,,Likely,"Learning high-quality sentence representations benefits a wide range of natural language processing tasks. Though BERT-based pre-trained language models achieve high performance on many downstream tasks, the native derived sentence representations are proved to be collapsed and thus produce a poor performance on the semantic textual similarity (STS) tasks. In this paper, we present ConSERT, a Contrastive Framework for Self-Supervised Sentence Representation Transfer, that adopts contrastive learning to fine-tune BERT in an unsupervised and effective way. By making use of unlabeled texts, ConSERT solves the collapse issue of BERT-derived sentence representations and make them more applicable for downstream tasks. Experiments on STS datasets demonstrate that ConSERT achieves an 8\% relative improvement over the previous state-of-the-art, even comparable to the supervised SBERT-NLI. And when further incorporating NLI supervision, we achieve new state-of-the-art performance on STS tasks. Moreover, ConSERT obtains comparable results with only 1000 samples available, showing its robustness in data scarcity scenarios.",2023-12-05 04:33,Anonymous,,,"China,China","Meituan University, Beijing University of Posts and Telecommunications",,,,ConSERT,,,
Transformer local-attention (NesT-B),Vision,,"Zizhao Zhang, Han Zhang, Long Zhao, Ting Chen, Sercan Arık, Tomas Pfister",Highly cited,,,https://arxiv.org/abs/2105.12723v4,5734.00,"Nested Hierarchical Transformer: Towards Accurate, Data-Efficient and Interpretable Visual Understanding",2021-05-26,"Google Cloud,Google Research",Industry,90100000.00,"Table A2, NesT-B is the largest size.",24057600000000000000.00,"17.9 GFLOPS per forward pass
300 epochs
1.28M training examples
3.5 f_to_b pass ratio
(From Imagenet paper-data, Besiroglu et al., forthcoming) ",Imagenet-1k,,1280000,,,,,,,,,Self-supervised learning,$39.51,,Industry,,"Hierarchical structures are popular in recent vision transformers, however, they require sophisticated designs and massive datasets to work well. In this paper, we explore the idea of nesting basic local transformers on non-overlapping image blocks and aggregating them in a hierarchical way. We find that the block aggregation function plays a critical role in enabling cross-block non-local information communication. This observation leads us to design a simplified architecture that requires minor code changes upon the original vision transformer. The benefits of the proposed judiciously-selected design are threefold: (1) NesT converges faster and requires much less training data to achieve good generalization on both ImageNet and small datasets like CIFAR; (2) when extending our key ideas to image generation, NesT leads to a strong decoder that is 8× faster than previous transformer-based generators; and (3) we show that decoupling the feature learning and abstraction processes via this nested hierarchy in our design enables constructing a novel method (named GradCAT) for visually interpreting the learned model. Source code is available this https URL.",2023-11-26 03:25,Robi Rahman,,,"Multinational,Multinational","Google Cloud, Google Research",,,,Transformer local-attention (NesT-B),,,
CogView,Drawing,Text-to-image,"M Ding, Z Yang, W Hong, W Zheng, C Zhou",SOTA Improvement,"""CogView achieves the state-of-the-art FID on the blurred MS COCO dataset, outperforming previous GAN-based models and a recent similar work DALL-E""",,https://arxiv.org/abs/2105.13290,399.00,CogView: Mastering Text-to-Image Generation via Transformers,2021-05-26,"Tsinghua University,DAMO Academy",Industry - Academia Collaboration (Academia leaning),4000000000.00,"""We propose CogView, a 4-billion-parameter Transformer with VQ-VAE tokenizer to advance this problem.""",2.68e+22,"source: https://lair.lighton.ai/akronomicon/

archived: https://github.com/lightonai/akronomicon/tree/main/akrodb",,"""We collected about 30 million text-image pairs from multiple channels, and built a 2.5TB new dataset (after tokenization, the size becomes about 250GB).""",50000000000,"""We collected about 30 million text-image pairs from multiple channels, and built a 2.5TB new dataset (after tokenization, the size becomes about 250GB).""

250GB * (1 word / 5 bytes) = 50 billion words or 67 billion tokens

So 30M text-image pairs and 50 billion words",,,,,,NVIDIA Tesla V100 DGXS 16 GB,NVIDIA Tesla V100 DGXS 16 GB,Self-supervised learning,$44452.39,,Industry,,"Text-to-Image generation in the general domain has long been an open problem, which requires both a powerful generative model and cross-modal understanding. We propose CogView, a 4-billion-parameter Transformer with VQ-VAE tokenizer to advance this problem. We also demonstrate the finetuning strategies for various downstream tasks, e.g. style learning, super-resolution, text-image ranking and fashion design, and methods to stabilize pretraining, e.g. eliminating NaN losses. CogView achieves the state-of-the-art FID on the blurred MS COCO dataset, outperforming previous GAN-based models and a recent similar work DALL-E.",2023-12-05 04:33,Robi Rahman,,,"China,China","Tsinghua University, DAMO Academy",,,,CogView,,,
ByT5-XXL,Language,Language modelling,"Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, Colin Raffel",SOTA Improvement,"""On the most realistic in-language setting, where some gold training data is available in all languages, ByT5 surpasses the previous state-of-art mT5 on all tasks and model sizes""",Permissive license,https://arxiv.org/abs/2105.13626,235.00,ByT5: Towards a token-free future with pre-trained byte-to-byte models,2021-05-28,Google,Industry,12900000000.00,"12.9B, from Table 1",8.1e+22,"""Like mT5, we set our sequence
length to 1024 (bytes rather than tokens), and train
for 1 million steps over batches of 2^20 tokens.""

12.9 billion * 1 million * 2^20 * 6 = ~8.1e22",mC4,,,,,,,,,Google TPU v3,Google TPU v3,,,,,Likely,"Most widely-used pre-trained language models operate on sequences of tokens corresponding to word or subword units. By comparison, token-free models that operate directly on raw text (bytes or characters) have many benefits: they can process text in any language out of the box, they are more robust to noise, and they minimize technical debt by removing complex and error-prone text preprocessing pipelines. Since byte or character sequences are longer than token sequences, past work on token-free models has often introduced new model architectures designed to amortize the cost of operating directly on raw text. In this paper, we show that a standard Transformer architecture can be used with minimal modifications to process byte sequences. We characterize the trade-offs in terms of parameter count, training FLOPs, and inference speed, and show that byte-level models are competitive with their token-level counterparts. We also demonstrate that byte-level models are significantly more robust to noise and perform better on tasks that are sensitive to spelling and pronunciation. As part of our contribution, we release a new set of pre-trained byte-level Transformer models based on the T5 architecture, as well as all code and data used in our experiments.",2023-12-05 04:33,Anonymous,,0,Multinational,Google,,,,,,,
Wu Dao 2.0,Multimodal,,,,,,https://www.engadget.com/chinas-gigantic-multi-modal-ai-is-no-one-trick-pony-211414388.html,0.00,China's gigantic multi-modal AI is no one-trick pony,2021-05-31,Beijing Academy of Artificial Intelligence,Non-profit,1750000000000.00,"""It's been trained on 1.75 trillion parameters""",,"It's a mixture-of-experts model, so 1.75 trillion params likely overstates how much compute was required:

""The parameter scale of Enlightenment 2.0 reached a record-breaking 1.75 trillion. According to reports, the new generation FastMoE technology is the key to the realization of the ""Trillion Model"" cornerstone of Enlightenment 2.0.""

",WuDao Corpora,"WuDao Corpora, as of version 2.0, was a large dataset constructed for training Wu Dao 2.0. It contains 3 terabytes of text scraped from web data, 90 terabytes of graphical data (incorporating 630 million text/image pairs), and 181 gigabytes of Chinese dialogue (incorporating 1.4 billion dialogue rounds).
https://en.wikipedia.org/wiki/Wu_Dao#WuDao_Corpora",,,,,,,,,,,,,Industry,,,2023-12-13 07:35,Robi Rahman,,,China,Beijing Academy of Artificial Intelligence,,,,,,,
CODA,Language,,"Lin Zheng, Zhiyong Wu, Lingpeng Kong",,,,https://arxiv.org/pdf/2105.14850,2.00,Cascaded Head-colliding Attention,2021-05-31,,,247000000.00,,,,,,,,,,,,,,,,,,,,,2023-11-21 01:39,Robi Rahman,CODA,,,,,,,,,,
GPT2-Large+LHOPT,Language,,"Diogo Almeida, Clemens Winter, Jie Tang, Wojciech Zaremba",,,,https://web.archive.org/web/20221027150413/https://arxiv.org/pdf/2106.00958.pdf,19.00,A Generalizable Approach to Learning Optimizers,2021-06-02,,,760000000.00,,1.6e+21,,,,,,1.00,,,,,,,,,,,,,2023-12-05 04:33,Robi Rahman,GPT2-Large+LHOPT,,,,,,,,,,
AFP+FPI (PTB),Language,,"Zhengxiong Wang, Anton Ragni",,,,https://arxiv.org/pdf/2106.02417,2.00,Approximate Fixed-Points in Recurrent Neural Networks,2021-06-04,,,2040000.00,,227000000000000.00,,Penn TreeBank,,,,20.00,,,,,,,,,,,,,2023-12-05 04:33,Robi Rahman,AFP+FPI (PTB),,,,,,,,,,
AFP+FPI (WT2),Language,,,,,,,,,2021-06-04,,,,,,,,,,,40.00,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,AFP+FPI (WT2),1,,,,,,,,,
ViT-G/14,Vision,,"X Zhai, A Kolesnikov, N Houlsby, L Beyer",SOTA Improvement,"""we successfully train a ViT model with two billion parameters, which attains a new state-of-the-art on ImageNet of 90.45% top-1 accuracy""",,https://arxiv.org/abs/2106.04560,622.00,Scaling Vision Transformers,2021-06-08,Google Brain,Industry,1843000000.00,Table 2 of paper,3.4e+21,"source: https://lair.lighton.ai/akronomicon/

archived: https://github.com/lightonai/akronomicon/tree/main/akrodb


Alternatively: per paper, ViT-G required between 20-30k TPUv3-days to train (from eyeballing the tick marks in Figure 9)

123 trillion * 25,000 * 3600 * 0.4 = 4.4e21

",JFT-3B,,3000000000,"""For this study, we use the proprietary JFT-3B dataset, a larger version of the JFT-300M dataset used
in many previous works on large-scale computer vision models [31, 18, 11]. This dataset consists of
nearly 3 billion images, annotated with a class-hierarchy of around 30k labels via a semi-automatic
pipeline""",,,,,,Google TPU v3,Google TPU v3,Self-supervised learning,$5541.84,,Industry,Likely,"Attention-based neural networks such as the Vision Transformer (ViT) have recently attained state-of-the-art results on many computer vision benchmarks. Scale is a primary ingredient in attaining excellent results, therefore, understanding a model's scaling properties is a key to designing future generations effectively. While the laws for scaling Transformer language models have been studied, it is unknown how Vision Transformers scale. To address this, we scale ViT models and data, both up and down, and characterize the relationships between error rate, data, and compute. Along the way, we refine the architecture and training of ViT, reducing memory consumption and increasing accuracy of the resulting models. As a result, we successfully train a ViT model with two billion parameters, which attains a new state-of-the-art on ImageNet of 90.45% top-1 accuracy. The model also performs well for few-shot transfer, for example, reaching 84.86% top-1 accuracy on ImageNet with only 10 examples per class.",2023-12-05 04:33,Robi Rahman,,,Multinational,Google Brain,,,,ViT-G/14,,,
CoAtNet,Vision,Image Classification,"Zihang Dai, Hanxiao Liu, Quoc V. Le, Mingxing Tan",SOTA Improvement,"""Notably, when we further scale up CoAtNet with JFT-3B, it achieves 90.88% top-1 accuracy on ImageNet, establishing a new state-of-the-art result.""",,https://arxiv.org/abs/2106.04803v2,777.00,"CoAtNet: Marrying Convolution and Attention
for All Data Sizes",2021-06-09,Google,Industry,2440000000.00,,8.5e+22,"20.1K TPU-v3 core-days

123 teraflop/s * 20100 * 24 * 3600 * 0.4 (utilization assumption) = 8.5e22","ImageNet, JFT",,,,,2586000000000.00,"2586B FLOP, per table 5, for 512x512 image",,,Google TPU v3,Google TPU v3,,,,,Likely,"Transformers have attracted increasing interests in computer vision, but they still fall behind state-of-the-art convolutional networks. In this work, we show that while Transformers tend to have larger model capacity, their generalization can be worse than convolutional networks due to the lack of the right inductive bias. To effectively combine the strengths from both architectures, we present CoAtNets (pronounced “coat” nets), a family of hybrid models built from two key insights: (1) depthwise Convolution and self-Attention can be naturally unified via simple relative attention; (2) vertically stacking convolution layers and attention layers in a principled way is surprisingly effective in improving generalization, capacity and efficiency. Experiments show that our CoAtNets achieve state-of-the-art performance under different resource constraints across various datasets: Without extra data, CoAtNet achieves 86.0% ImageNet top-1 accuracy; When pre-trained with 13M images from ImageNet-21K, our CoAtNet achieves 88.56% top-1 accuracy, matching ViT-huge pre-trained with 300M images from JFT-300M while using 23x less data; Notably, when we further scale up CoAtNet with JFT-3B, it achieves 90.88% top-1 accuracy on ImageNet, establishing a new state-of-the-art result.",2023-11-26 03:25,Anonymous,,,Multinational,Google,,,,CoAtNet,,,
DeBERTa,Language,,"Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen",SOTA Improvement,"""DeBERTa significantly outperforms all existing PLMs of similar size on MNLI and creates a new state of the art""",,https://arxiv.org/abs/2006.03654,1357.00,DeBERTa: Decoding-enhanced BERT with Disentangled Attention,2021-06-10,Microsoft,Industry,1500000000.00,"""...we scale up DeBERTa by training a larger version that consists of 48 Transform layers with 1.5 billion parameters""

Other versions are smaller and use a smaller pre-training dataset. These are distinguished in the paper (e.g. DeBERTa1.5B is the version of DeBERTa with 1.5 billion parameters).",6.00000000001e+21,"From section 5.1.1: ""We use 6 DGX-2 machines (96 V100 GPUs) to train the models. A single model trained with 2K batch size and 1M steps takes about 20 days."" 

This specifically refers to the largest models referred to in the paper, and smaller models are described elsewhere, but I'm assuming the large models are what we care about here. 

Apparently there are multiple types of GPUs referred to as V100s. I'm guessing these are NVIDIA Tesla SMX2s.",,,15600000000,""" DeBERTa is pretrained on 78G training data""

1GB ~ 200M words",,,,,,,,Self-supervised learning,,,Industry,,"Recent progress in pre-trained neural language models has significantly improved the performance of many natural language processing (NLP) tasks. In this paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT with disentangled attention) that improves the BERT and RoBERTa models using two novel techniques. The first is the disentangled attention mechanism, where each word is represented using two vectors that encode its content and position, respectively, and the attention weights among words are computed using disentangled matrices on their contents and relative positions, respectively. Second, an enhanced mask decoder is used to incorporate absolute positions in the decoding layer to predict the masked tokens in model pre-training. In addition, a new virtual adversarial training method is used for fine-tuning to improve models' generalization. We show that these techniques significantly improve the efficiency of model pre-training and the performance of both natural language understanding (NLU) and natural langauge generation (NLG) downstream tasks. Compared to RoBERTa-Large, a DeBERTa model trained on half of the training data performs consistently better on a wide range of NLP tasks, achieving improvements on MNLI by +0.9% (90.2% vs. 91.1%), on SQuAD v2.0 by +2.3% (88.4% vs. 90.7%) and RACE by +3.6% (83.2% vs. 86.8%). Notably, we scale up DeBERTa by training a larger version that consists of 48 Transform layers with 1.5 billion parameters. The significant performance boost makes the single DeBERTa model surpass the human performance on the SuperGLUE benchmark (Wang et al., 2019a) for the first time in terms of macro-average score (89.9 versus 89.8), and the ensemble DeBERTa model sits atop the SuperGLUE leaderboard as of January 6, 2021, out performing the human baseline by a decent margin (90.3 versus 89.8).",2023-12-05 04:33,Robi Rahman,,,Multinational,Microsoft,,,,DeBERTa,,,
ALIGN,Multimodal,Representation Learning,"Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yunhsuan Sung, Zhen Li, Tom Duerig",SOTA Improvement,"""The aligned visual and language representations... set new state-of-the-art results on Flickr30K and MSCOCO image-text retrieval benchmarks""",,https://arxiv.org/abs/2102.05918,1766.00,Scaling up visual and vision-language representation learning with noisy text supervision,2021-06-11,Google Research,Industry,820000000.00,"From author communication

 480M (image tower) + 340 M (text tower)",2.5986700000009994e+22,"From author communication
14.82K TPUv3 core-days
Precision: bfloat16

Estimation
TPUv3 at float16: 123 TFLOPS/chip

123*10^12 TFLOPS/chip * (1 chip / 2 cores) * 14820 TPU core-days * 86400 s/day * 33% utilization = 2.599*10^22 FLOP
https://www.wolframalpha.com/input?i=14820+days+*+123+teraFLOPS+%2F+2+*+0.33",,,1600000000,"Dataset contains 1.8B image-text pairs, then some duplicates are removed.",,,,347.3,14820 TPU core-hours / 1024 TPU cores = 347.3 hours,Google TPU v3,Google TPU v3,Self-supervised learning,$357760.33,,Industry,Likely,"Pre-trained representations are becoming crucial for many NLP and perception tasks. While representation learning in NLP has transitioned to training on raw text without human annotations, visual and vision-language representations still rely heavily on curated training datasets that are expensive or require expert knowledge. For vision applications, representations are mostly learned using datasets with explicit class labels such as ImageNet or OpenImages. For vision-language, popular datasets like Conceptual Captions, MSCOCO, or CLIP all involve a non-trivial data collection (and cleaning) process. This costly curation process limits the size of datasets and hence hinders the scaling of trained models. In this paper, we leverage a noisy dataset of over one billion image alt-text pairs, obtained without expensive filtering or post-processing steps in the Conceptual Captions dataset. A simple dual-encoder architecture learns to align visual and language representations of the image and text pairs using a contrastive loss. We show that the scale of our corpus can make up for its noise and leads to state-of-the-art representations even with such a simple learning scheme. Our visual representation achieves strong performance when transferred to classification tasks such as ImageNet and VTAB. The aligned visual and language representations enables zero-shot image classification and also set new state-of-the-art results on Flickr30K and MSCOCO image-text retrieval benchmarks, even when compared with more sophisticated cross-attention models. The representations also enable cross-modality search with complex text and text + image queries.",2023-12-05 11:37,Robi Rahman,,,Multinational,Google Research,,,,ALIGN,512,,ALIGN
Denoising Diffusion Probabilistic Models (LSUN Bedroom),Drawing,,"Jonathan Ho, Ajay Jain, Pieter Abbeel",SOTA Improvement,"Novel approach to image synthesis that yields SOTA results on datasets like CIFAR-10

Abstract: 
""On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. """,,https://arxiv.org/abs/2006.11239,4819.00,Denoising Diffusion Probabilistic Models,2021-06-11,UC Berkeley,Academia,256000000.00,"Appendix B: 
"" Our CIFAR10 model has 35.7 million parameters, and our LSUN and
CelebA-HQ models have 114 million parameters. We also trained a larger variant of the LSUN Bedroom model with approximately 256 million parameters by increasing filter count.""",95000000000000020000.00,"Numbers in Appendix B
10.6h for the CIFAR model (batch size 128, 21 step/s)
2.2 step/s for the LSUN model, 1.15M steps so 702.8 hours

1.25E14 FLOP/s * 702.8h * 3600s/h * 0.3 = 9.5e19",LSUN Bedroom,,3033042,"""We trained on CelebA-HQ for 0.5M steps, LSUN Bedroom for 2.4M steps, LSUN Cat for 1.8M steps, and LSUN Church for 1.2M steps.""

""The CelebA-HQ dataset is a high-quality version of CelebA that consists of 30,000 images at 1024×1024 resolution.""
https://paperswithcode.com/dataset/celeba-hq

LSUN bedroom has 3,033,042 examples. LSUN cat has 1,657,266 examples. LSUN church has 126,227 examples.
https://www.tensorflow.org/datasets/catalog/lsun
",,,,,,Google TPU v3,Google TPU v3,,$2.60,,Academia,,,2023-12-05 04:33,Robi Rahman,,,United States of America,UC Berkeley,,,,Denoising Diffusion Probabilistic Models (LSUN Bedroom),,,
Delta RNN (+ full context),Language,,"Kazuki Irie, Imanol Schlag, Róbert Csordás, Jürgen Schmidhuber",,,,https://proceedings.neurips.cc/paper/2021/file/3f9e3767ef3b10a0de4c256d7ef9805d-Paper.pdf,42.00,Going Beyond Linear Transformers with Recurrent Fast Weight Programmers,2021-06-11,,,44600000.00,,1100000000000000100.00,,WikiText-103,,,,40.00,,,,,,,,,,,,,2023-11-23 06:53,Robi Rahman,Delta RNN (+ full context),,,,,,,,,,
EfficientNetV2,Vision,Image Classification,"Mingxing Tan, Quoc V. Le","SOTA Improvement,Highly cited","""EfficientNetV2 achieves 87.3% top-1 accuracy on ImageNet ILSVRC2012, outperforming the recent ViT by 2.0% accuracy while 
 training 5x-11x faster using the same computing resources.""",,EfficientNetV2: Smaller Models and Faster Training,1387.00,EfficientNetV2: Smaller Models and Faster Training,2021-06-23,Google,Industry,208000000.00,"Table 7, page 7",95600000000000000000.00,"Table 7, page 7: 45 hours on 32 TPUv3 cores.

""Each v3 TPU chip contains two TensorCores.""
TPU performance per chip = 123e12 FLOP/s
32 cores = 16 chips

123e12 FLOP/s per chip * (32 cores / 2 cores per chip) * 45 hours * 3600 seconds/hour * 0.30 utilization = 9.56e19 FLOP

https://www.wolframalpha.com/input?i=123+terahertz+*+16+*+45+hours+*+0.3",ImageNet21k,,14197122,,,,,45.0,Table 7,Google TPU v3,Google TPU v3,Supervised,,,Industry,Likely,"This paper introduces EfficientNetV2, a new family of convolutional networks that have faster training speed and better parameter efficiency than previous models. To develop this family of models, we use a combination of training-aware neural architecture search and scaling, to jointly optimize training speed and parameter efficiency. The models were searched from the search space enriched with new ops such as Fused-MBConv. Our experiments show that EfficientNetV2 models train much faster than state-of-the-art models while being up to 6.8x smaller.
Our training can be further sped up by progressively increasing the image size during training, but it often causes a drop in accuracy. To compensate for this accuracy drop, we propose to adaptively adjust regularization (e.g., dropout and data augmentation) as well, such that we can achieve both fast training and good accuracy.
With progressive learning, our EfficientNetV2 significantly outperforms previous models on ImageNet and CIFAR/Cars/Flowers datasets. By pretraining on the same ImageNet21k, our EfficientNetV2 achieves 87.3% top-1 accuracy on ImageNet ILSVRC2012, outperforming the recent ViT by 2.0% accuracy while training 5x-11x faster using the same computing resources. Code will be available at this https URL.",2023-11-26 03:25,Anonymous,,,Multinational,Google,,,,EfficientNetV2,,,
CPM-2,Language,Language Generation,"Zhengyan Zhang, Yuxian Gu, Xu Han, Shengqi Chen, Chaojun Xiao, Zhenbo Sun, Yuan Yao, Fanchao Qi, Jian Guan, Pei Ke, Yanzheng Cai, Guoyang Zeng, Zhixing Tan, Zhiyuan Liu, Minlie Huang, Wentao Han, Yang Liu, Xiaoyan Zhu, Maosong Sun",,,,https://arxiv.org/abs/2106.10715,64.00,CPM-2: Large-scale Cost-effective Pre-trained Language Models,2021-06-24,"Tsinghua University,Beijing Academy of Artificial Intelligence",Academia,11000000000.00,11B,,,WuDaoCorpus,,444100000000,"""We pre-train our model on WuDaoCorpus (Yuan et al., 2021), which contains 2.3TB cleaned Chinese data as well as 300GB cleaned English data.""

2300*167 million + 300*200 million = 444,100,000,000 (444 billion)

https://docs.google.com/document/d/1G3vvQkn4x_W71MKg0GmHVtzfd9m0y3_Ofcoew0v902Q/edit#heading=h.ieihc08p8dn0",,,,,,,,,,,,Likely,"In recent years, the size of pre-trained language models (PLMs) has grown by leaps and bounds. However, efficiency issues of these large-scale PLMs limit their utilization in real-world scenarios. We present a suite of cost-effective techniques for the use of PLMs to deal with the efficiency issues of pre-training, fine-tuning, and inference. (1) We introduce knowledge inheritance to accelerate the pre-training process by exploiting existing PLMs instead of training models from scratch. (2) We explore the best practice of prompt tuning with large-scale PLMs. Compared with conventional fine-tuning, prompt tuning significantly reduces the number of task-specific parameters. (3) We implement a new inference toolkit, namely InfMoE, for using large-scale PLMs with limited computational resources. Based on our cost-effective pipeline, we pre-train two models: an encoder-decoder bilingual model with 11 billion parameters (CPM-2) and its corresponding MoE version with 198 billion parameters. In our experiments, we compare CPM-2 with mT5 on downstream tasks. Experimental results show that CPM-2 has excellent general language intelligence. Moreover, we validate the efficiency of InfMoE when conducting inference of large-scale models having tens of billions of parameters on a single GPU. All source code and model parameters are available at this https URL.",2023-12-05 08:10,Anonymous,,0,"China,China","Tsinghua University, Beijing Academy of Artificial Intelligence",,,,,,,
DEQ-Transformer (Post-LN) + Jacobian Regularisation,Language,,"Shaojie Bai, Vladlen Koltun, J. Zico Kolter",,,,https://arxiv.org/abs/2106.14342,45.00,Stabilizing Equilibrium Models by Jacobian Regularization,2021-06-28,,,98000000.00,,29000000000000000000.00,,WikiText-103,,,,23.00,,,,,,,,,,,,,2023-11-23 06:53,Robi Rahman,DEQ-Transformer (Post-LN) + Jacobian Regularisation,,,,,,,,,,
Adaptive Input Transformer + RD,Language,,"Xiaobo Liang, Lijun Wu, Juntao Li, Yue Wang, Qi Meng, Tao Qin, Wei Chen, Min Zhang, Tie-Yan Liu",SOTA Improvement,"""In particular, it yields substantial
improvements when applied to fine-tune large-scale pre-trained models, e.g., ViT, RoBERTa-large, and BART, and achieves state-of-the-art (SOTA) performances with the vanilla Transformer model """,,https://arxiv.org/abs/2106.14448,246.00,R-Drop: Regularized Dropout for Neural Networks,2021-06-28,"Microsoft Research Asia,Soochow University",,247000000.00,,82000000000000000000.00,,,,,,,,,,,,,,,,,,,2023-12-05 04:33,Robi Rahman,Adaptive Input Transformer + RD,,"China,Taiwan","Microsoft Research Asia, Soochow University",,,,Adaptive Input Transformer + RD,,,
ERNIE 3.0,Language,,"Y Sun, S Wang, S Feng, S Ding, C Pang",SOTA Improvement,"""ERNIE 3.0 achieved new state-of-the-art results across 54 Chinese NLP tasks""",,http://research.baidu.com/Blog/index-view?id=160,199.00,ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation,2021-07-05,Baidu,Industry,10000000000.00,"""We trained the model with 10 billion parameters on a 4TB corpus consisting of plain texts and a large-scale knowledge graph.""",2.25e+22,"Section 3.3.3: 
""""The model is trained for
a total of 375 billion tokens""

Total compute approximated as 6*N*D",,,668000000000,"""To ensure the success of the pre-training of ERNIE 3.0, we construct a large-scale, wide-variety and high-quality Chinese text corpora amounting to 4TB storage size in 11 different categories.""

1 GB ~ 167M chinese words",,,,,,,,Self-supervised learning,$3.83,,Industry,,,2023-12-05 04:33,Robi Rahman,,,China,Baidu,,,,ERNIE 3.0,,,
Codex,Language,Code autocompletion,"Mark Chen , Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger,  Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji,  Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, Wojciech Zaremba ",Significant use,,,https://openai.com/blog/openai-codex/,1784.00,Evaluating Large Language Models Trained on Code,2021-07-07,OpenAI,Industry,12000000000.00,"""With just a single sample, a 12B parameter Codex solves 28.8% of these problems, and a 300M parameter Codex solves 13.2% of these problems""",,"""The original training of GPT-3-12B consumed hundreds of petaflop/sdays of compute, while fine-tuning it to create Codex-12B
consumed a similar amount of compute.""
",,,31800000000,"""Our training dataset was collected in May 2020 from 54 million public software repositories hosted on GitHub, containing 179 GB of unique Python files under 1 MB. We filtered out files which were likely auto-generated, had average line
length greater than 100, had maximum line length greater
than 1000, or contained a small percentage of alphanumeric
characters. After filtering, our final dataset totaled 159 GB.""

1 GB ~ 200M words",,,,,,,,Self-supervised learning,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,United States of America,OpenAI,,,,Codex,,,
FNetAR Medium,Language,,"Tim Lou, Michael Park, Mohammad Ramezanali, Vincent Tang",,not SOTA per https://paperswithcode.com/sota/language-modelling-on-wikitext-103,,https://arxiv.org/abs/2107.10932,2.00,FNetAR: Mixing Tokens with Autoregressive Fourier Transforms,2021-07-22,,,34300000.00,,,,WikiText-103,,,,,,,,,,,,,,,,,2023-11-21 01:39,Robi Rahman,FNetAR Medium,,,,,,,,,,
GOAT,Games,Open ended play,Open- Ended Learning Team,SOTA Improvement,likely qualitatively SOTA,,https://deepmind.com/blog/article/generally-capable-agents-emerge-from-open-ended-play,127.00,Open-Ended Learning Leads to Generally Capable Agents,2021-07-27,DeepMind,Industry,3500000.00,estimate described here: https://docs.google.com/document/d/1S9xZyCeITDOs-P1W_-liNW0WgVN-OLsSudVrPXMaLqw/edit?usp=sharing,7.8e+22,"[Final calculation]
(8 TPUs)(4.20e14 FLOP/s)(0.1 utilisation rate)(32 agents)(7.3e6 s/agent) = 7.8e22 FLOPs

==========================
NOTES BELOW

[Hardware]
- ""Each agent is trained using 8 TPUv3s and consumes approximately 50,000 agent steps (observations) per second.""
- TPUv3 (half precision): 4.2e14 FLOP/s
- Number of TPUs: 8
- Utilisation rate: 0.1

[Timesteps]
- Figure 16 shows steps per generation and agent. In total there are 1.5e10 + 4.0e10 + 2.5e10 + 1.1e11 + 2e11 = 3.9e11 steps per agent.
- 3.9e11 / 5e4 = 8e6 s → ~93 days
- 100 million steps is equivalent to 30 minutes of wall-clock time in our setup. (pg 29, fig 27)
- 1e8 steps → 0.5h
- 3.9e11 steps → 1950h → 7.0e6 s → ~82 days
- Both of these seem like overestimates, because:
“Finally, on the largest timescale (days), generational training iteratively improves population performance by bootstrapping off previous generations, whilst also iteratively updating the validation normalised percentile metric itself.” (pg 16)
- Suggests that the above is an overestimate of the number of days needed, else they would have said (months) or (weeks)?
- Final choice (guesstimate): 85 days = 7.3e6 s

[Population size]
- 8 agents? (pg 21) → this is describing the case where they’re not using PBT, so ignore this number
- The original PBT paper uses 32 agents for one task https://arxiv.org/pdf/1711.09846.pdf (in general it uses between 10 and 80)
- (Guesstimate) Average population size: 32",XLand,,390000000000,Figure 16 shows steps per generation and agent. In total there are 1.5e10 + 4.0e10 + 2.5e10 + 1.1e11 + 2e11 = 3.9e11 steps per agent.,,,,,,,,Self-supervised learning,$122418.97,,Industry,,"In this work we create agents that can perform well beyond a single, individual task, that exhibit much wider generalisation of behaviour to a massive, rich space of challenges. We define a universe of tasks within an environment domain and demonstrate the ability to train agents that are generally capable across this vast space and beyond. The environment is natively multi-agent, spanning the continuum of competitive, cooperative, and independent games, which are situated within procedurally generated physical 3D worlds. The resulting space is exceptionally diverse in terms of the challenges posed to agents, and as such, even measuring the learning progress of an agent is an open research problem. We propose an iterative notion of improvement between successive generations of agents, rather than seeking to maximise a singular objective, allowing us to quantify progress despite tasks being incomparable in terms of achievable rewards. We show that through constructing an open-ended learning process, which dynamically changes the training task distributions and training objectives such that the agent never stops learning, we achieve consistent learning of new behaviours. The resulting agent is able to score reward in every one of our humanly solvable evaluation levels, with behaviour generalising to many held-out points in the universe of tasks. Examples of this zero-shot generalisation include good performance on Hide and Seek, Capture the Flag, and Tag. Through analysis and hand-authored probe tasks we characterise the behaviour of our agent, and find interesting emergent heuristic behaviours such as trial-and-error experimentation, simple tool use, option switching, and cooperation. Finally, we demonstrate that the general capabilities of this agent could unlock larger scale transfer of behaviour through cheap finetuning.",2023-12-05 04:33,Robi Rahman,,,United Kingdom of Great Britain and Northern Ireland,DeepMind,,,,GOAT,,,
HuBERT,Language,,"Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed",SOTA Improvement,"Abstract: 
"" the
HuBERT model either matches or improves upon the state-ofthe-art wav2vec 2.0 performance on the Librispeech (960h) and
Libri-light (60,000h) benchmarks with 10min, 1h, 10h, 100h, and
960h fine-tuning subsets.""",,https://arxiv.org/abs/2106.07447,1224.00,HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units,2021-07-27,Facebook AI Research,Industry,1000000000.00,"From abstract:
""Using a 1B parameter model, HuBERT shows up to 19% and 13% relative WER reduction on the more challenging dev-other and test-other evaluation subsets""",5.54e+21,"GPU NOT SPECIFIED - for the sake of argument I assume something on the order of 1 TFLOP/s

Numbers from Section IV part C
0.1 * (960h * 32GPUs + 60000h * 256 GPUs) * 3600s/h * 1 TFLOP/s/GPU",LibriSpeech,,820800000,"""When the HuBERT model is pre-trained on either the standard Librispeech 960h [24] or the Libri-Light 60k hours [25], it either matches or improves upon the state-of-theart wav2vec 2.0 [6] performance on all fine-tuning subsets of 10mins, 1h, 10h, 100h, and 960h.""

1h ~ 13,680 words
13,680 * 60,000 = 820800000",,,,,,,,Self-supervised learning,$8632.11,,Industry,,"Self-supervised approaches for speech representation learning are challenged by three unique problems: (1) there are multiple sound units in each input utterance, (2) there is no lexicon of input sound units during the pre-training phase, and (3) sound units have variable lengths with no explicit segmentation. To deal with these three problems, we propose the Hidden-Unit BERT (HuBERT) approach for self-supervised speech representation learning, which utilizes an offline clustering step to provide aligned target labels for a BERT-like prediction loss. A key ingredient of our approach is applying the prediction loss over the masked regions only, which forces the model to learn a combined acoustic and language model over the continuous inputs. HuBERT relies primarily on the consistency of the unsupervised clustering step rather than the intrinsic quality of the assigned cluster labels. Starting with a simple k-means teacher of 100 clusters, and using two iterations of clustering, the HuBERT model either matches or improves upon the state-of-the-art wav2vec 2.0 performance on the Librispeech (960h) and Libri-light (60,000h) benchmarks with 10min, 1h, 10h, 100h, and 960h fine-tuning subsets. Using a 1B parameter model, HuBERT shows up to 19% and 13% relative WER reduction on the more challenging dev-other and test-other evaluation subsets.",2023-12-05 04:33,Robi Rahman,,,Multinational,Facebook AI Research,,,,HuBERT,,,
SEER,Vision,,"Priya Goyal, Mathilde Caron, Benjamin Lefaudeux, Min Xu, Pengchao Wang, Vivek Pai, Mannat Singh, Vitaliy Liptchinsky, Ishan Misra, Armand Joulin, Piotr Bojanowski",SOTA Improvement,"SOTA for self-supervised models on ImageNet, which seems fair to consider a different benchmark than ImageNet for supervised models.

""Our final SElf-supERvised (SEER) model,
a RegNetY with 1.3B parameters trained on 1B random
images with 512 GPUs achieves 84.2% top-1 accuracy,
surpassing the best self-supervised pretrained model by 1%""",,https://arxiv.org/abs/2103.01988,202.00,Self-supervised Pretraining of Visual Features in the Wild,2021-07-29,"Facebook AI Research,INRIA",Industry - Academia Collaboration (Industry leaning),1300000000.00,"From abstract:
"" Our final SElf-supERvised (SEER) model, a RegNetY with 1.3B parameters...""",4.42e+21,"Numbers from section 3.2

512 GPUs * 0.1 * 8days * 24h/day * 3600s/h * 125 TFLOP/s",,"Section 3.3:
""For our billion scale pretraining, we consider a dataloader that directly samples random, public, and non-EU images from Instagram""

Note the dataset is not static - it is refreshed every 90 days",1000000000,"""Overall, we train
on 1B images for a total of 122K iterations.""",,,,,,NVIDIA Tesla V100 DGXS 32 GB,NVIDIA Tesla V100 DGXS 32 GB,Self-supervised learning,$16058.80,,Industry,,"Recently, self-supervised learning methods like MoCo, SimCLR, BYOL and SwAV have reduced the gap with supervised methods. These results have been achieved in a control environment, that is the highly curated ImageNet dataset. However, the premise of self-supervised learning is that it can learn from any random image and from any unbounded dataset. In this work, we explore if self-supervision lives to its expectation by training large models on random, uncurated images with no supervision. Our final SElf-supERvised (SEER) model, a RegNetY with 1.3B parameters trained on 1B random images with 512 GPUs achieves 84.2% top-1 accuracy, surpassing the best self-supervised pretrained model by 1% and confirming that self-supervised learning works in a real world setting. Interestingly, we also observe that self-supervised models are good few-shot learners achieving 77.9% top-1 with access to only 10% of ImageNet. Code: this https URL",2023-12-05 04:33,Robi Rahman,,,"Multinational,France","Facebook AI Research, INRIA",,,,SEER,,,
6-Act Tether,Robotics,Object detection,"Joel Ye, Dhruv Batra, Abhishek Das, Erik Wijmans",SOTA Improvement,"""Our agents achieve 24.5% success and 8.1% SPL, a 37% and 8% relative improvement over prior state-of-the-art, respectively, on the Habitat ObjectNav Challenge""",,https://openaccess.thecvf.com/content/ICCV2021/html/Ye_Auxiliary_Tasks_and_Exploration_Enable_ObjectGoal_Navigation_ICCV_2021_paper.html,48.00,Auxiliary Tasks and Exploration Enable ObjectGoal Navigation,2021-08-03,"Facebook AI Research,Georgia Institute of Technology",Industry - Academia Collaboration (Industry leaning),5000000.00,"""Agent parameter counts were all 5 − 6 million parameters, excluding parameters in auxiliary modules""",,"""In our experiments, we train each of our agents for 8 GPU-weeks (192 GPU-hours)"". No GPU specified.",Matterport,"""We experiment on the Matterport dataset (MP3D [4]), which has 90 scenes and 40 labeled semantic object categories.""",,,,,,,,,,Reinforcement learning,,,,Confident,,2023-11-26 03:25,Anonymous,,,"Multinational,United States of America","Facebook AI Research, Georgia Institute of Technology",,,,6-Act Tether,,,
FMMformer (2-kernel fast weight + Band20),Language,,"Tan M. Nguyen, Vai Suliafu, Stanley J. Osher, Long Chen, Bao Wang",,,,https://web.archive.org/web/20220803154831/https://proceedings.neurips.cc/paper/2021/file/f621585df244e9596dc70a39b579efb1-Paper.pdf,27.00,FMMformer: Efficient and Flexible Transformer via Decomposed Near-field and Far-field Attention,2021-08-05,,,40000000.00,,430000000000000000.00,,,,,,,,,,,,,,,,,,,2023-12-05 04:33,Robi Rahman,FMMformer (2-kernel fast weight + Band20),,,,,,,,,,
W2v-BERT,Speech,Speech recognition,"Yu-An Chung, Yu Zhang, Wei Han, Chung-Cheng Chiu, James Qin, Ruoming Pang, Yonghui Wu",SOTA Improvement,"""Our experiments show that w2v-BERT achieves competitive results
compared to current state-of-the-art pre-trained models on the LibriSpeech benchmarks when using the Libri-Light 60k corpus as the
unsupervised data. In particular, when compared to published models such as conformer-based wav2vec 2.0 and HuBERT, our model
shows 5% to 10% relative WER reduction on the test-clean and
test-other subsets""",,https://arxiv.org/abs/2108.06209v2,216.00,W2v-BERT: Combining Contrastive Learning and Masked Language Modeling for Self-Supervised Speech Pre-Training,2021-08-07,"Google Brain,Massachusetts Institute of Technology (MIT)",Industry - Academia Collaboration (Industry leaning),1000000000.00,1B for XXL model,,,Libri-Light,"""We use the Libri-Light unlab-60k subset [34], which contains
about 60,000 hours of unannotated speech audio, for pre-training
w2v-BERT models""",,,,,,,,,,,,,,Likely,"Motivated by the success of masked language modeling~(MLM) in pre-training natural language processing models, we propose w2v-BERT that explores MLM for self-supervised speech representation learning. w2v-BERT is a framework that combines contrastive learning and MLM, where the former trains the model to discretize input continuous speech signals into a finite set of discriminative speech tokens, and the latter trains the model to learn contextualized speech representations via solving a masked prediction task consuming the discretized tokens. In contrast to existing MLM-based speech pre-training frameworks such as HuBERT, which relies on an iterative re-clustering and re-training process, or vq-wav2vec, which concatenates two separately trained modules, w2v-BERT can be optimized in an end-to-end fashion by solving the two self-supervised tasks~(the contrastive task and MLM) simultaneously. Our experiments show that w2v-BERT achieves competitive results compared to current state-of-the-art pre-trained models on the LibriSpeech benchmarks when using the Libri-Light~60k corpus as the unsupervised data. In particular, when compared to published models such as conformer-based wav2vec~2.0 and HuBERT, our model shows~5\% to~10\% relative WER reduction on the test-clean and test-other subsets. When applied to the Google's Voice Search traffic dataset, w2v-BERT outperforms our internal conformer-based wav2vec~2.0 by more than~30\% relatively.",2023-11-26 03:25,Anonymous,,,"Multinational,United States of America","Google Brain, Massachusetts Institute of Technology (MIT)",,,,W2v-BERT,,,
Jurassic-1-Jumbo,Language,,"Opher Lieber, Or Sharir, Barak Lenz, Yoav Shoham",,,,https://uploads-ssl.webflow.com/60fd4503684b466578c0d307/61138924626a6981ee09caf6_jurassic_tech_paper.pdf,55.00,Jurassic-1: Technical Details and Evaluation,2021-08-11,AI21 Labs,Industry,178000000000.00,"""Jurassic-1 models come in two sizes, where the Jumbo version, at 178B parameters, is the largest and most sophisticated language model ever released for general use by developers.""",3.7e+23,see here https://docs.google.com/document/d/1B8x6XYcmB1u6Tmq3VcbAtj5bzhDaj2TcIPyK6Wpupx4/edit,,,225000000000,"""Our model was trained with the conventional self-supervised auto-regressive training objective on 300B tokens drawn from publicly available resources""

1 token ~ 0.75 words",,,,,,,,Self-supervised learning,$805277.01,,Industry,,"Jurassic-1 is a pair of auto-regressive language models recently released by AI21 Labs, consisting of J1-Jumbo, a 178B-parameter model, and J1-Large, a 7B-parameter model. We describe their architecture and training, and evaluate their performance relative to GPT-3. The evaluation is in terms of perplexity, as well as zero-shot and few-shot learning. To that end, we developed a zeroshot and few-shot test suite, which we made publicly available (https://github.com/ai21labs/ lm-evaluation) as a shared resource for the evaluation of mega language models.",2023-11-23 06:53,Robi Rahman,,,Israel,AI21 Labs,,,,,,,
Zidong Taichu,Multimodal,,,Historical significance,"The world’s first image, language, and audio trimodal pre-trained model.",,https://gitee.com/zidongtaichu/multi-modal-models,0.00,Zidong Ancestral multi-modal large model,2021-08-11,Chinese Academy of Sciences,,100000000000.00,,,,,,,,,,,,,,,,,,,Likely,,2023-11-26 03:25,Anonymous,,,China,Chinese Academy of Sciences,,,,Zidong Taichu,,,
"GPT-2 (1.5B, Curriculum Learning 45K)",Language,,"Conglong Li, Minjia Zhang, Yuxiong He",,,,https://arxiv.org/abs/2108.06084,20.00,Curriculum Learning: A Regularization Method for Efficient and Stable Billion-Scale GPT Model Pre-Training,2021-08-13,,,1500000000.00,,600000000000000000000.00,,Wikipedia; CC-Stories; RealNews; OpenWebtext,,,,2.20,,,,,,,,,,,,,2023-11-23 06:53,Robi Rahman,"""GPT-2 (1.5B, Curriculum Learning 45K)""",,,,,,,,,,
"GPT-2 (117M, SLW 110K)",Language,,,,,,,,,2021-08-13,,,,,,,,,,,1.10,,,,,,,,,,,Unverified,,2023-12-08 01:01,Robi Rahman,"""GPT-2 (117M, SLW 110K)""",1,,,,,,,,,
XLMR-XXL,Language,,"Naman Goyal, Jingfei Du, Myle Ott, Giri Anantharaman, Alexis Conneau",SOTA Improvement,"Abstract:
""Our model also outperforms
the RoBERTa-Large model on several English tasks of the GLUE benchmark by 0.3% on average while handling 99 more languages.""",,https://arxiv.org/abs/2105.00572,65.00,Larger-Scale Transformers for Multilingual Masked Language Modeling,2021-08-17,Facebook AI Research,Industry,10700000000.00,"Section 2.1:
"" ...XLM-RXXL (L= 48, H = 4096, A = 32, 10.7B params)""",,,CC100,,125250000000,"""We pretrain the models on the CC100 dataset, which corresponds to 167B tokens in 100 languages.""

1 token ~ 0.7 words",,,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,Multinational,Facebook AI Research,,,,XLMR-XXL,,,
ProteinLM,Language,"Protein generation,Proteins","Yijia Xiao, Jiezhong Qiu, Ziang Li, Chang-Yu Hsieh, Jie Tang",,,,https://arxiv.org/abs/2108.07435,15.00,Modeling Protein Using Large-scale Pretrain Language Model,2021-08-17,"Tsinghua University,Tencent,Beijing Academy of Artificial Intelligence",Industry - Academia Collaboration (Industry leaning),3000000000.00,"""We have trained multiple largescale models on the PFAM[7] dataset, the largest with
3 billion parameters""",1.5999999999999998e+22,"""We pretrained two large models on a 480 GPUs (TeslaV100-32GB) cluster for about three weeks""

21 * 24* 3600 * 480 * 125 teraFLOP/s * 0.3 (utilization) * 0.5 (two models) = 1.6e22",PFAM,"""PFAM[7] is a widely-used database consisting of more than 32 million protein sequences""",,,,,,,,NVIDIA V100,NVIDIA V100,Unsupervised,,,Industry,Likely,"Protein is linked to almost every life process. Therefore, analyzing the biological structure and property of protein sequences is critical to the exploration of life, as well as disease detection and drug discovery. Traditional protein analysis methods tend to be labor-intensive and time-consuming. The emergence of deep learning models makes modeling data patterns in large quantities of data possible. Interdisciplinary researchers have begun to leverage deep learning methods to model large biological datasets, e.g. using long short-term memory and convolutional neural network for protein sequence classification. After millions of years of evolution, evolutionary information is encoded in protein sequences. Inspired by the similarity between natural language and protein sequences, we use large-scale language models to model evolutionary-scale protein sequences, encoding protein biology information in representation. Significant improvements are observed in both tokenlevel and sequence-level tasks, demonstrating that our large-scale model can accurately capture evolution information from pretraining on evolutionary-scale individual sequences. Our code and model are available at https://github.com/THUDM/ProteinLM.",2023-12-05 04:33,Anonymous,,,"China,Multinational,China","Tsinghua University, Tencent, Beijing Academy of Artificial Intelligence",,,,,,,
"ALiBi (L=3072, Lvalid = 3072)",Language,,"Ofir Press, Noah A. Smith, Mike Lewis",,,,https://arxiv.org/abs/2108.12409,229.00,"Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",2021-08-27,,,1300000000.00,,180000000000000000000.00,,WikiText-103,,,,205.00,,,,,,,,,,,,,2023-12-05 04:33,Robi Rahman,"""ALiBi (L=3072, Lvalid = 3072)""",,,,,,,,,,
$\infty$-former (SM),Language,,"Pedro Henrique Martins, Zita Marinho, André F. T. Martins",,,,https://arxiv.org/abs/2109.00301,31.00,$\infty$-former: Infinite Memory Transformer,2021-09-01,,,117000000.00,,1.2e+22,,WikiText-103,,,,1.00,,,,,,,,,,,,,2023-11-23 06:53,Robi Rahman,$\infty$-former (SM),,,,,,,,,,
FLAN 137B,Language,Language modelling,"Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le",SOTA Improvement,"Abstract: 
""FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 datasets that we evaluate.""",,https://arxiv.org/abs/2109.01652,1420.00,Finetuned Language Models Are Zero-Shot Learners,2021-09-03,Google Research,Industry,137000000000.00,"Abstract:
""We take a 137B parameter pretrained language model and instruction tune it on
over 60 NLP datasets verbalized via natural language instruction templates. We
evaluate this instruction-tuned model, which we call FLAN, on unseen task types.""

Many models seem to be using the same 137B base transformer model?",4.896e+22,"From section 2.4: ""60 hours on a TPUv3 with 128 cores."" I assume that ""128 cores"" = 128 TPUv3s. Which took less than 2% of total time (see environmental considerations section)",,"Abstract: ""We take a 137B parameter pretrained language model and instruction tune it on
over 60 NLP datasets""",1870000000000,"""Model architecture and pretraining. In our experiments, we use LaMDA-PT, a dense left-to-right, decoder-only transformer language model of 137B parameters (Thoppilan et al., 2022). This model is pretrained on a collection of web documents (including those with computer code), dialog data, and Wikipedia, tokenized into 2.49T BPE tokens with a 32k vocabulary using the  SentencePiece library (Kudo & Richardson, 2018). Around 10% of the pretraining data was non-English. Note that LaMDA-PT only has language model pretraining (c.f. LaMDA, which was finetuned for dialog).""

2.49e12 tokens ~= 1.87e12 words",,,,,,,,Self-supervised learning,,,Industry,,"This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning—finetuning language models on a collection of datasets described via instructions—substantially improves zeroshot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction tune it on over 60 NLP datasets verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 datasets that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning.",2023-12-05 04:33,Robi Rahman,,,Multinational,Google Research,,,,FLAN 137B,,,
MEB,Search,,"W Liu, Z Wang, X Liu, N Zeng, Y Liu, FE Alsaadi",Significant use,"""MEB is running in production for 100 percent of Bing searches, in all regions and languages.""",,https://www.microsoft.com/en-us/research/blog/make-every-feature-binary-a-135b-parameter-sparse-neural-network-for-massively-improved-search-relevance/,26.00,Make Every feature Binary: A 135B parameter sparse neural network for massively improved search relevance,2021-09-04,Microsoft,Industry,135000000000.00,See paper title,,,,,,"""MEB uses three years of search logs from Bing as training data."" TODO convert",,,,,,,,,,,Industry,,,2023-11-26 03:25,Robi Rahman,,,Multinational,Microsoft,,,,MEB,,,
RNS-RNN,Language,,"Brian DuSell, David Chiang",,,,https://arxiv.org/pdf/2109.01982,10.00,Learning Hierarchical Structures with Differentiable Nondeterministic Stacks,2021-09-05,,,5660000.00,,3150000000000000.00,,,,,,100.00,,,,,,,,,,,,,2023-12-05 04:33,Robi Rahman,RNS-RNN,,,,,,,,,,
PermuteFormer,Language,,Peng Chen,SOTA Improvement,"""Results show that
PermuteFormer uniformly improves the performance of Performer, accelerates convergence, and
achieves state-of-the-art on some tasks.""",,https://arxiv.org/pdf/2109.02377,13.00,PermuteFormer: Efficient Relative Position Encoding for Long Sequences,2021-09-06,Peking University,,33000000.00,,3100000000000000000.00,,,,,,30.00,,,,,,,,,,,,,2023-12-05 04:33,Robi Rahman,PermuteFormer,,China,Peking University,,,,PermuteFormer,,,
NLM,Language,,"Junxian He, Graham Neubig, Taylor Berg-Kirkpatrick",,,,https://arxiv.org/abs/2109.04212,60.00,Efficient Nearest Neighbor Language Models,2021-09-09,,Industry - Academia Collaboration,515000000.00,,7360000000000000000.00,,WikiText-103,,,,,,,,,,,,,,,,,2023-11-23 06:53,Robi Rahman,NLM,,,,,,,,,,
HyperClova,Language,,"Boseop Kim, HyoungSeok Kim, Sang-Woo Lee, Gichang Lee, Donghyun Kwak, Dong Hyeon Jeon, Sunghyun Park, Sungju Kim, Seonhoon Kim, Dongpil Seo, Heungsub Lee, Minyoung Jeong, Sungjae Lee, Minsub Kim, Suk Hyun Ko, Seokhun Kim, Taeyong Park, Jinuk Kim, Soyoung Kang, Na-Hyeon Ryu, Kang Min Yoo, Minsuk Chang, Soobin Suh, Sookyo In, Jinseong Park, Kyungduk Kim, Hiun Kim, Jisu Jeong, Yong Goo Yeo, Donghoon Ham, Dongju Park, Min Young Lee, Jaewook Kang, Inho Kang, Jung-Woo Ha, Woomyoung Park, Nako Sung",SOTA Improvement,"""HyperCLOVA with our training configuration shows state-of-the-art in-context zero-shot and few-shot learning performances on various downstream tasks in Korean""",,https://arxiv.org/abs/2109.04650,76.00,What Changes Can Large-scale Language Models Bring? Intensive Study on HyperCLOVA: Billions-scale Korean Generative Pretrained Transformers,2021-09-10,"NAVER,Search Solutions",Industry,82000000000.00,"""We introduce a Korean in-context large-scale LM with 82B parameters, i.e., HyperCLOVA. This is the first discovery on near
100B-scale non-English LM.""",1.476e+23,"""For experiments in Section 4, the model trained with 150B is used for fair comparison, because not all models are finished training at the same iteration. However, experiments in Section 5.2 use the model trained with 300B tokens, as HyperCLOVA Studio provided the 39B and 82B models trained with 300B tokens.""

82e9 connections * 2 FLOP/connection * 300e9 tokens * 3 backward pass = 1.476e23 FLOP

Calculation using GPU time corroborates this:
- ""Our model is based on megatron-LM (Shoeybi et al., 2019) and trained on the NVIDIA Superpod, which includes 128 strongly clustered DGX servers with 1,024 A100 GPUs.""
- ""It takes 13.4 days to train a model with 82B parameters with 150B tokens."" Assume 300B tokens takes twice as long, 26.8 days.
- Assume the default of 30% utilization rate for large language models.

1024 A100 GPUs * 312e12 FLOP/second * 0.3 utilization * 26.8 days * 24 * 60 * 60 seconds/day = 2.219e+23 FLOP",,,190000000000,"""We introduce HyperCLOVA, a large-scale
Korean in-context learning-based LM with
nearly 100B parameters, by constructing a
large Korean-centric corpus of 560B tokens.""

Based on tokenizing the Hyperclova article itself using OpenAI's tiktoken BPE tokenizer (https://github.com/openai/tiktoken), there are 3285 tokens for 1069 words - about 3 tokens per word.

This work uses a special tokenizer, but based on Figure 5 in Appendix E, the number of tokens seems similar between different tokenization methods.

Based on that, 5.6e11 Korean tokens ~= 1.9e11 words",,,,,,,,Self-supervised learning,$103802.31,,Industry,,,2023-12-05 04:33,Robi Rahman,,,"Korea (Republic of),Korea (Republic of)","NAVER, Search Solutions",,,,HyperClova,,,
DLRM-2022,Recommendation,,"D Mudigere, Y Hao, J Huang, A Tulloch",,,,https://arxiv.org/abs/2104.05158,72.00,Software-Hardware Co-design for Fast and Scalable Training of Deep Learning Recommendation Models,2021-09-15,Facebook,Industry,3000000000000.00,"Figure 1

https://arxiv.org/abs/2104.05158",1.1e+21,"Figure 1

https://arxiv.org/abs/2104.05158",,,,,,,,,,"NVIDIA Tesla V100 DGXS 32 GB,NVIDIA A100","NVIDIA Tesla V100 DGXS 32 GB, NVIDIA A100",,$2394.07,,Industry,,"Deep learning recommendation models (DLRMs) are used across many business-critical services at Facebook and are the single largest AI application in terms of infrastructure demand in its data-centers. In this paper we discuss the SW/HW co-designed solution for high-performance distributed training of large-scale DLRMs. We introduce a high-performance scalable software stack based on PyTorch and pair it with the new evolution of Zion platform, namely ZionEX. We demonstrate the capability to train very large DLRMs with up to 12 Trillion parameters and show that we can attain 40X speedup in terms of time to solution over previous systems. We achieve this by (i) designing the ZionEX platform with dedicated scale-out network, provisioned with high bandwidth, optimal topology and efficient transport (ii) implementing an optimized PyTorch-based training stack supporting both model and data parallelism (iii) developing sharding algorithms capable of hierarchical partitioning of the embedding tables along row, column dimensions and load balancing them across multiple workers; (iv) adding high-performance core operators while retaining flexibility to support optimizers with fully deterministic updates (v) leveraging reduced precision communications, multi-level memory hierarchy (HBM+DDR+SSD) and pipelining. Furthermore, we develop and briefly comment on distributed data ingestion and other supporting services that are required for the robust and efficient end-to-end training in production environments.",2023-12-05 04:33,Robi Rahman,,,Multinational,Facebook,,,,,,,
PLATO-XL,Language,Language modelling,"Siqi Bao, Huang He, Fan Wang, Hua Wu, Haifeng Wang, Wenquan Wu, Zhihua Wu, Zhen Guo, Hua Lu, Xinxian Huang, Xin Tian, Xinchao Xu, Yingzhan Lin, Zheng-Yu Niu",SOTA Improvement,,,https://arxiv.org/abs/2109.09519,47.00,PLATO-XL: Exploring the Large-scale Pre-training of Dialogue Generation,2021-09-20,Baidu,Industry,11000000000.00,,,,,,,,,,,,,NVIDIA Tesla V100 DGXS 32 GB,NVIDIA Tesla V100 DGXS 32 GB,,,,Industry,Likely,"To explore the limit of dialogue generation pre-training, we present the models of PLATO-XL with up to 11 billion parameters, trained on both Chinese and English social media conversations. To train such large models, we adopt the architecture of unified transformer with high computation and parameter efficiency. In addition, we carry out multi-party aware pre-training to better distinguish the characteristic information in social media conversations. With such designs, PLATO-XL successfully achieves superior performances as compared to other approaches in both Chinese and English chitchat. We further explore the capacity of PLATO-XL on other conversational tasks, such as knowledge grounded dialogue and task-oriented conversation. The experimental results indicate that PLATO-XL obtains state-of-the-art results across multiple conversational tasks, verifying its potential as a foundation model of conversational AI.",2023-12-05 04:33,Anonymous,,,China,Baidu,,,,PLATO-XL,,,
M6-10T,Multimodal,,"Junyang Lin, An Yang, Jinze Bai, Chang Zhou, Le Jiang, Xianyan Jia, Ang Wang, Jie Zhang, Yong Li, Wei Lin, Jingren Zhou, Hongxia Yang",,,,https://arxiv.org/abs/2110.03888,27.00,M6-10T: A Sharing-Delinking Paradigm for Efficient Multi-Trillion Parameter Pretraining,2021-10-08,Alibaba,Industry,10000000000000.00,"""We demonstrate a practice of pretraining unprecedented 10-trillion-parameter model, an order of magnitude larger than the state-of-the-art, on solely 512 GPUs within 10 days""",5.53e+21,"512 GPUs in 10 days - using NVIDIA V100 GPUs

Using the NVIDIA V100 Specifications this works out to be: 
0.30 * 125E12 * 512 * 10 * 86400 = 1.66E22

(Assuming 30% utilisation, and 125 TFLOPS)",BookCorpus; English Wikipedia,,8000000000,"""We conduct experiments for pretraining and finetuning to analyze model competence in upstream and
downstream tasks. Following the classical data setup for pretraining and finetuning, we pretrain the model on BookCorpus [52] and English Wikipedia [9], which are corpora with around 16GB of plain
texts.""

I used http://extraconversion.com/data-storage/gigabits/gigabits-to-words.html for the conversion to number of words",,,,,512 GPUs * 10 days * 24 h/day,,,Self-supervised learning,$20073.49,,Industry,,"Recent expeditious developments in deep learning algorithms, distributed training, and even hardware design for large models have enabled training extreme-scale models, say GPT-3 and Switch Transformer possessing hundreds of billions or even trillions of parameters. However, under limited resources, extreme-scale model training that requires enormous amounts of computes and memory footprint suffers from frustratingly low efficiency in model convergence. In this paper, we propose a simple training strategy called ""Pseudo-to-Real"" for high-memory-footprint-required large models. Pseudo-to-Real is compatible with large models with architecture of sequential layers. We demonstrate a practice of pretraining unprecedented 10-trillion-parameter model, an order of magnitude larger than the state-of-the-art, on solely 512 GPUs within 10 days. Besides demonstrating the application of Pseudo-to-Real, we also provide a technique, Granular CPU offloading, to manage CPU memory for training large model and maintain high GPU utilities. Fast training of extreme-scale models on a decent amount of resources can bring much smaller carbon footprint and contribute to greener AI.",2023-12-05 04:33,Robi Rahman,,,China,Alibaba,,,,,,,
Megatron-Turing NLG 530B,Language,Language modelling,"Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, Elton Zhang, Rewon Child, Reza Yazdani Aminabadi, Julie Bernauer, Xia Song, Mohammad Shoeybi, Yuxiong He, Michael Houston, Saurabh Tiwary, Bryan Catanzaro",SOTA improvement,"The 105-layer, transformer-based MT-NLG improved upon the prior state-of-the-art models in zero-, one-, and few-shot settings",,https://arxiv.org/abs/2201.11990,460.00,"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model",2021-10-11,"Microsoft,NVIDIA",Industry,530000000000.00,,1.17e+24,https://www.lesswrong.com/posts/bGuMrzhJdENCo8BxX/nvidia-and-microsoft-releases-530b-parameter-transformer?commentId=HSJSNspKp94tFcSCx,"Common Crawl, The Pile"," In addition to Common Crawl data, we leveraged a number of other previously generated datasets. From The Pile, we selected Books3, OpenWebText2, Stack Exchange, PubMed Abstracts,
Wikipedia, Gutenberg (PG-19), BookCorpus2, NIH ExPorter, and Pile-CC datasets. We also included the
CC-Stories and RealNews datasets used to train Megatron",202500000000,"""Our training dataset consists of 339 billion tokens and we
trained MT-NLG on 270 billions tokens by blending the 15 training datasets as described above. We also set aside 2% of our data for validation.""

1 token ~ 0.75 words",,,,770.0,"Total compute was 1.17*10^24 FLOP.
They don't directly report the utilization and training speed when using the full Selene supercomputer with 560 DGX * 8 A100/DGX = 4480 GPUs. See section 2.3 Hardware Setup.

At 280 DGX, the utilization is 126/312 = 40% and a batch takes 60 seconds; at 350, it is 39% for 50 seconds; at 420, it is 36% for 44 seconds.

The overall utilization was 30.2% and the full cluster has 560 DGX. Dividing the total compute by the total performance of 4480 A100 at 30.2% utilization gives 770 hours.",NVIDIA A100 SXM4 80 GB,NVIDIA A100 SXM4 80 GB,Self-supervised learning,$3046994.09,,Industry,,"Pretrained general-purpose language models can achieve state-of-the-art accuracies in various natural language processing domains by adapting to downstream tasks via zero-shot, few-shot and fine-tuning techniques. Because of their success, the size of these models has increased rapidly, requiring high-performance hardware, software, and algorithmic techniques to enable training such large models. As the result of a joint effort between Microsoft and NVIDIA, we present details on the training of the largest monolithic transformer based language model, Megatron-Turing NLG 530B (MT-NLG), with 530 billion parameters. In this paper, we first focus on the infrastructure as well as the 3D parallelism methodology used to train this model using DeepSpeed and Megatron. Next, we detail the training process, the design of our training corpus, and our data curation techniques, which we believe is a key ingredient to the success of the model. Finally, we discuss various evaluation results, as well as other interesting observations and new properties exhibited by MT-NLG. We demonstrate that MT-NLG achieves superior zero-, one-, and few-shot learning accuracies on several NLP benchmarks and establishes new state-of-the-art results. We believe that our contributions will help further the development of large-scale training infrastructures, large-scale language models, and natural language generations.",2023-12-05 04:33,Robi Rahman,,,"Multinational,United States of America","Microsoft, NVIDIA",,,,Megatron-Turing NLG 530B,4480,0.3020,Megatron-Turing NLG 530B
Yuan 1.0,Language,Language modelling,"Shaohua Wu, Xudong Zhao, Tong Yu, Rongguo Zhang, Chong Shen, Hongli Liu, Feng Li, Hong Zhu, Jiangang Luo, Liang Xu, Xuanwei Zhang, Jun Liu",SOTA Improvement,"""The zero-shot average scores of both LM and PLM are superior to the SOTA one. On Csldcp, Tnews and Iflytek tasks, we surpass the zero-shot SOTA by a large margin""",,https://arxiv.org/abs/2110.04725,34.00,Yuan 1.0: Large-Scale Pre-trained Language Model in Zero-Shot and Few-Shot Learning,2021-10-12,Inspur,Industry,245730000000.00,"Table 2: Parameters of Yuan models.
""Parameters (billion)""",3.5380000000001e+23,"Table 9: 4095 petaFLOPS-days which equals 3.538*10^23 FLOP

https://www.wolframalpha.com/input?i=4095+petaFLOPS+*+1+day
",,"""A Chinese corpus with 5TB high-quality text is built, which is sufficient to train Yuan 245B model without sampling the dataset twice.""",1000000000000,"""Yuan 1.0 was trained on a new Chinese dataset of 5TB high-quality text that was built on 850TB raw data from Internet.""

1 GB ~ 167M words in English or 333M words in Chinese. For a mixed dataset of mostly Chinese, 5TB may be equivalent to around 1T words.",0.22,,,,,,,Self-supervised learning,$606364.75,,Industry,Confident,"Recent work like GPT-3 has demonstrated excellent performance of Zero-Shot and Few-Shot learning on many natural language processing (NLP) tasks by scaling up model size, dataset size and the amount of computation. However, training a model like GPT-3 requires huge amount of computational resources which makes it challengeable to researchers. In this work, we propose a method that incorporates large-scale distributed training performance into model architecture design. With this method, Yuan 1.0, the current largest singleton language model with 245B parameters, achieves excellent performance on thousands GPUs during training, and the state-of-the-art results on NLP tasks. A data processing method is designed to efficiently filter massive amount of raw data. The current largest high-quality Chinese corpus with 5TB high quality texts is built based on this method. In addition, a calibration and label expansion method is proposed to improve the Zero-Shot and Few-Shot performance, and steady improvement is observed on the accuracy of various tasks. Yuan 1.0 presents strong capacity of natural language generation, and the generated articles are difficult to distinguish from the human-written ones.",2023-12-05 04:33,Robi Rahman,,,China,Inspur,,,,Yuan 1.0,2128,0.4500,Yuan 1.0
T0-XXL,Language,Language modelling,"Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao,  Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal V. Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng-Xin Yong, Harshit Pandey, Michael McKenna, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Tali Bers, Stella Biderman, Leo Gao, Thomas Wolf, Alexander M. Rush",,"""we compare T0 to the zero-shot performance of the largest language models available as of writing, i.e., various GPT-3 models up to 175B parameters...
We find that T0 matches or exceeds the performance
of all GPT-3 models on 9 out of 11 held-out datasets""",,https://arxiv.org/abs/2110.08207,969.00,Multitask Prompted Training Enables Zero-Shot Task Generalization,2021-10-15,"Hugging Face,Brown University",Industry - Academia collaboration,11000000000.00,"""Unless specified otherwise, we use the XXL version which
has 11B parameters.""",1.792e+22,"From section B.1: ""These training runs corresponded to about 270 total hours of training on a v3-512 Cloud TPU device."" (512 cores for 270 hours)",,,,"Multitask - 12 tasks, 62 datasets. See fig 2 for details. 

This is going to be a nightmare to figure out! TODO figure out the sizes of each of these 62 datasets!

All datasets from here: https://arxiv.org/pdf/2109.02846.pdf",,,,,,,,,,,Industry,,"Large language models have recently been shown to attain reasonable zero-shot generalization on a diverse set of tasks (Brown et al., 2020). It has been hypothesized that this is a consequence of implicit multitask learning in language models' pretraining (Radford et al., 2019). Can zero-shot generalization instead be directly induced by explicit multitask learning? To test this question at scale, we develop a system for easily mapping any natural language tasks into a human-readable prompted form. We convert a large set of supervised datasets, each with multiple prompts with diverse wording. These prompted datasets allow for benchmarking the ability of a model to perform completely held-out tasks. We fine-tune a pretrained encoder-decoder model (Raffel et al., 2020; Lester et al., 2021) on this multitask mixture covering a wide variety of tasks. The model attains strong zero-shot performance on several standard datasets, often outperforming models up to 16x its size. Further, our approach attains strong performance on a subset of tasks from the BIG-bench benchmark, outperforming models up to 6x its size. All trained models are available at this https URL and all prompts are available at this https URL.",2023-12-05 04:33,Robi Rahman,,,"Multinational,United States of America","Hugging Face, Brown University",,,,,,,
KnGPT2,Language,,"Ali Edalati, Marzieh Tahaei, Ahmad Rashid, Vahid Partovi Nia, James J. Clark, Mehdi Rezagholizadeh",,,,https://web.archive.org/web/20221111092612/https://arxiv.org/pdf/2110.08152.pdf,17.00,Kronecker Decomposition for GPT Compression,2021-10-15,,Academia,83000000.00,,124000000000000000000.00,,,,,,1.00,,,,,,,,,,,,,2023-12-05 04:33,Robi Rahman,KnGPT2,,,,,,,,,,
PAGnol-XL,Language,Language modelling,"Julien Launay, E.L. Tommasone, Baptiste Pannier, François Boniface, Amélie Chatelain, Alessandro Cappelli, Iacopo Poli, Djamé Seddah",,,,https://arxiv.org/abs/2110.08554,3.00,PAGnol: An Extra-Large French Generative Model,2021-10-16,"LightOn,Laboratoire de Physique de l'Ecole Normale (LPENS),INRIA",Industry - Academia Collaboration,1500000000.00,,259200000000000000000.00,"They report their compute directly.

From section 8: ""About 62k GPU-hours on the Jean Zay HPC Cluster."" Jean Zay uses both A100 and V100 GPUs, and maybe other stuff as well?

Note they explicitly call out V100 in their Appendix A.

https://www.hpcwire.com/2021/11/17/frances-jean-zay-supercomputer-gets-ai-boost-from-hpe-nvidia/",CCNet,They mostly use CCNet but also OSCAR for a comparison.,24000000000,Section 4.1: 32G tokens => 32e9*0.75 = 24e9 words,,,,,,NVIDIA Tesla V100 SXM2,NVIDIA Tesla V100 SXM2,Self-supervised learning,,,,,"Access to large pre-trained models of varied architectures, in many different languages, is central to the democratization of NLP. We introduce PAGnol, a collection of French GPT models. Using scaling laws, we efficiently train PAGnol-XL (1.5B parameters) with the same computational budget as CamemBERT, a model 13 times smaller. PAGnol-XL is the largest model trained to date for the French language. We plan to train increasingly large and performing versions of PAGnol, exploring the capabilities of French extreme-scale models. For this first release, we focus on the pretraining and scaling calculations underlining PAGnol. We fit a scaling law for compute for the French language, and compare it with its English counterpart. We find the pre-training dataset significantly conditions the quality of the outputs, with common datasets such as OSCAR leading to low-quality offensive text. We evaluate our models on discriminative and generative tasks in French, comparing to other state-of-the-art French and multilingual models, and reaching the state of the art in the abstract summarization task. Our research was conducted on the public GENCI Jean Zay supercomputer, and our models up to the Large are made publicly available.",2023-11-23 06:53,Robi Rahman,,,"France,France,France","LightOn, Laboratoire de Physique de l'Ecole Normale (LPENS), INRIA",,,,,,,
GPT-2 (fine-tuned with HYDRA),Language,,"Kabir Nagrecha, Arun Kumar",,,,https://arxiv.org/abs/2110.08633,4.00,Hydra: A System for Large Multi-Model Deep Learning,2021-10-16,,,1540000000.00,,19200000000000000.00,,WikiText-2,,,,1.00,,,,,,,,,,,,,2023-11-23 06:53,Robi Rahman,GPT-2 (fine-tuned with HYDRA),,,,,,,,,,
MGK 4 heads (medium),Language,,"Tam Nguyen, Tan M. Nguyen, Dung D. Le, Duy Khuong Nguyen, Viet-Anh Tran, Richard G. Baraniuk, Nhat Ho, Stanley J. Osher",,,,https://arxiv.org/pdf/2110.08678,16.00,Improving Transformers with Probabilistic Attention Keys,2021-10-16,,Industry,90000000.00,,6670000000000000000.00,,,,,,120.00,,,,,,,,,,,,,2023-12-05 04:33,Robi Rahman,MGK 4 heads (medium),,,,,,,,,,
MGK 8 heads (small),Language,,,,,,,,,2021-10-16,,,,,,,,,,,120.00,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,MGK 8 heads (small),1,,,,,,,,,
base LM+GNN+kNN,Language,,"Yuxian Meng, Shi Zong, Xiaoya Li, Xiaofei Sun, Tianwei Zhang, Fei Wu, Jiwei Li",SOTA Improvement,,,https://arxiv.org/abs/2110.08743,27.00,GNN-LM: Language Modeling based on Global Contexts via GNN,2021-10-17,"Shannon.AI,Nanjing University,Nanyang Technological University,Zhejiang University",Industry,274000000.00,,7300000000000000000.00,,WikiText-103,,,,,,,,,,,,,,,,,2023-12-05 04:33,Robi Rahman,base LM+GNN+kNN,,"China,China,Singapore,China","Shannon.AI, Nanjing University, Nanyang Technological University, Zhejiang University",,,,base LM+GNN+kNN,,,
base LM+GNN,Language,,,,,,,,,2021-10-17,,,,,,,,,,,,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,base LM+GNN,1,,,,,,,,,
WD+LR+M,Language,,"Ross M. Clarke, Elre T. Oldewage, José Miguel Hernández-Lobato",,,,https://arxiv.org/pdf/2110.10461,9.00,Scalable One-Pass Optimisation of High-Dimensional Weight-Update Hyperparameters by Implicit Differentiation,2021-10-20,,Industry - Academia Collaboration,,,,,,,,,72.00,,,,,,,,,,,,,2023-12-05 04:33,Robi Rahman,WD+LR+M,,,,,,,,,,
Cloob,Multimodal,,"Andreas Fürst ∗Elisabeth Rumetshofer ∗Johannes Lehner,Viet Tran,Fei Tang, Hubert Ramsauer, David Kreil, Michael Kopp, Günter Klambauer, Angela Bitto-Nemling, Sepp Hochreiter",,"improves on CLIP, which was SOTA at the time 1.5 years ago. but this paper doesn't claim to be SOTA, so I think it's unlikely to be so.",,https://arxiv.org/abs/2110.11316,59.00,CLOOB: Modern Hopfield Networks with InfoLOOB Outperform CLIP,2021-10-21,"Johannes Kepler University,HERE Technologies,Institute of Advanced Research in Artificial Intelligence",Industry - Academia Collaboration (Academia Leaning),,,,,,,15000000,"[Image-text pairs]
""To be comparable to the CLIP results, we use the same subset of 15 million samples from the YFCC100M dataset""",,,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,"Austria,Switzerland,Austria","Johannes Kepler University, HERE Technologies, Institute of Advanced Research in Artificial Intelligence",,,,,,,
Scatterbrain,Language,,"Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri Rudra, Christopher Ré",,,,https://web.archive.org/web/20220808053741/https://arxiv.org/pdf/2110.15343.pdf,16.00,Scatterbrain: Unifying Sparse and Low-rank Attention Approximation,2021-10-28,,Industry,,,,,,,,,30.00,,,,,,,,,,,,,2023-12-05 04:33,Robi Rahman,Scatterbrain,,,,,,,,,,
EfficientZero,Games,,"Weirui Ye, Shaohuai Liu, Thanard Kurutach, Pieter Abbeel, Yang Gao",SOTA Improvement,"""Our method is 176% and 163% better
than the previous SoTA performance, in mean and median human normalized score respectively""",,https://arxiv.org/abs/2111.00210,112.00,Mastering Atari Games with Limited Data,2021-10-30,"Tsinghua University,UC Berkeley,Shanghai Qi Zhi institute",Academia,,,,"""Our implementation is computationally friendly. To train an Atari agent for 100k steps, it only needs 4 GPUs to train 7 hours.""",,,,,,,,,,,,,,,Academia,,,2023-12-05 04:33,Robi Rahman,,,"China,United States of America,China","Tsinghua University, UC Berkeley, Shanghai Qi Zhi institute",,,,EfficientZero,,,
S4,Language,,"Albert Gu, Karan Goel, Christopher Ré",SOTA Improvement,"""S4 achieves strong empirical results across a diverse range of established benchmarks, including... SoTA on every task from the Long Range Arena benchmark""",,https://arxiv.org/abs/2111.00396,248.00,Efficiently Modeling Long Sequences with Structured State Spaces,2021-10-31,Stanford University,,249000000.00,,600000000000000000000.00,,WikiText-103,,,,509.02,,,,,,,,,,,,,2023-12-05 04:33,Robi Rahman,S4,,United States of America,Stanford University,,,,S4,,,
CodeT5-base,Language,Code generation,"Yue Wang, Weishi Wang, Shafiq Joty, Steven C.H. Hoi",SOTA Improvement,"""Extensive experiments show that CodeT5 yields state-of-the-art results on the fourteen sub-tasks in CodeXGLUE.""",,https://aclanthology.org/2021.emnlp-main.685/,631.00,CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation,2021-11-01,"Salesforce,Nanyang Technological University",Industry - Academia Collaboration (Industry leaning),220000000.00,"""We build CodeT5 based on Huggingface’s T5 (Raffel et al., 2020) PyTorch implementation and employ two sizes of CodeT5-small (60M) and CodeT5-base (220M)""",1.56e+21,"""We pre-train the model with the denoising objective for 100 epochs and bimodal dual training for further 50 epochs on a cluster of 16 NVIDIA A100 GPUs with 40G memory. The total training time for CodeT5-small and CodeT5- base is 5 and 12 days, respectively""

16 * 312 teraFLOP/s * 12 * 24 * 3600 * 0.3 (utilization assumption) = 1.56e21",CodeSearchNet,"""We follow Feng et al. (2020) to employ CodeSearchNet (Husain et al., 2019) to pre-train CodeT5, which consists of six PLs with both unimodal and bimodal data. Apart from that, we additionally collect two datasets of C/CSharp from
BigQuery1 to ensure that all downstream tasks have overlapped PLs with the pre-training data. In total, we employ around 8.35 million instances for pretraining""",,"""In total, we employ around 8.35 million instances for pretraining"" 
Instances meaning code snippets/examples, not tokens.",150.00,,,288.0,"""The total training time for CodeT5-small and CodeT5- base is 5 and 12 days, respectively""",NVIDIA A100,NVIDIA A100,,,,,Likely,"""Pre-trained models for Natural Languages (NL) like BERT and GPT have been recently shown to transfer well to Programming Languages (PL) and largely benefit a broad set of code-related tasks. Despite their success, most current methods either rely on an encoder-only (or decoder-only) pre-training that is suboptimal for generation (resp. understanding) tasks or process the code snippet in the same way as NL, neglecting the special characteristics of PL such as token types. We present CodeT5, a unified pre-trained encoder-decoder Transformer model that better leverages the code semantics conveyed from the developer-assigned identifiers. Our model employs a unified framework to seamlessly support both code understanding and generation tasks and allows for multi-task learning. Besides, we propose a novel identifier-aware pre-training task that enables the model to distinguish which code tokens are identifiers and to recover them when they are masked. Furthermore, we propose to exploit the user-written code comments with a bimodal dual generation task for better NL-PL alignment. Comprehensive experiments show that CodeT5 significantly outperforms prior methods on understanding tasks such as code defect detection and clone detection, and generation tasks across various directions including PL-NL, NL-PL, and PL-PL. Further analysis reveals that our model can better capture semantic information from code. Our code and pre-trained models are released at https://github.com/salesforce/CodeT5.""",2023-12-05 04:33,Anonymous,,,"United States of America,Singapore","Salesforce, Nanyang Technological University",,,,CodeT5-base,,,
Projected GAN,Drawing,Image generation,"Axel Sauer, Kashyap Chitta, Jens Müller, Andreas Geiger",SOTA Improvement,"""It is further compatible with resolutions of up to one Megapixel and advances the state-of-the-art Fréchet Inception Distance (FID) on twenty-two benchmark datasets""",,https://proceedings.neurips.cc/paper/2021/hash/9219adc5c42107c4911e249155320648-Abstract.html,128.00,Projected GANs Converge Faster,2021-11-01,Heidelberg University,Academia,,,,"""With this setting, each experiment takes roughly 100-200 GPU hours on a NVIDIA V100,
for more details we refer to the appendix.""

""We conduct our experiments on an internal cluster with several nodes, each with up to 8 Quadro RTX
6000 or NVIDIA V100 using PyTorch 1.7.1 and CUDA 11.0.""",,They experiment with 22 image datasets.,,,,,"""On images at resolution 2562, the wall-clock training times
measured in sec/kimg using 8 Quadro RTX 6000 are shown in
Table 10.""",,,"NVIDIA V100,NVIDIA Quadro RTX 6000","NVIDIA V100, NVIDIA Quadro RTX 6000",,,,,Likely,"Generative Adversarial Networks (GANs) produce high-quality images but are
challenging to train. They need careful regularization, vast amounts of compute,
and expensive hyper-parameter sweeps. We make significant headway on these issues by projecting generated and real samples into a fixed, pretrained feature space.
Motivated by the finding that the discriminator cannot fully exploit features from
deeper layers of the pretrained model, we propose a more effective strategy that
mixes features across channels and resolutions. Our Projected GAN improves image quality, sample efficiency, and convergence speed. It is further compatible with
resolutions of up to one Megapixel and advances the state-of-the-art Fréchet Inception Distance (FID) on twenty-two benchmark datasets. Importantly, Projected
GANs match the previously lowest FIDs up to 40 times faster, cutting the wall-clock
time from 5 days to less than 3 hours given the same computational resources.",2023-12-05 04:33,Anonymous,,,Germany,Heidelberg University,,,,Projected GAN,,,
GPT2+CoreLM+Fine-Tuning,Language,,"Nikolaos Stylianou, Ioannis Vlahavas",,,,https://arxiv.org/pdf/2111.02687,2.00,CoreLM: Coreference-aware Language Model Fine-Tuning,2021-11-04,,,132000000.00,,31700000000000000.00,,,,,,10.00,,,,,,,,,,,,,2023-11-23 06:53,Robi Rahman,GPT2+CoreLM+Fine-Tuning,,,,,,,,,,
Japanese dialog transformers,Language,,"Hiroaki Sugiyama, Masahiro Mizukami, Tsunehiro Arimoto, Hiromi Narimatsu, Yuya Chiba, Hideharu Nakajima, Toyomi Meguro",,,,https://arxiv.org/abs/2109.05217,31.00,Empirical Analysis of Training Strategies of Transformer-based Japanese Chit-chat Systems,2021-11-09,NTT Communication Science Laboratories,Industry,1600000000.00,"""We examined the improvement in model size in detail by considering four model sizes: 0.35B, 0.7B, 1.1B, and 1.6B parameters""",,,,,2100000000,"[Pairs of text]

""We obtained 2.1 billion (521 GB) pairs by this method. The average number of utterances in the input context was 2.913, and the average number of characters was 62.3 for the input context and 20.3 for the target utterance""",,,,,,,,,,,Industry,,,2023-11-21 01:39,Robi Rahman,,,Japan,NTT Communication Science Laboratories,,,,,,,
ViT-G/14 (LiT),Vision,Image Classification,"Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner, Daniel Keysers, Alexander Kolesnikov, Lucas Beyer",SOTA Improvement,"""For example, it achieves 82.5% accuracy on the challenging ObjectNet test set [1], outperforming the previous state-of-the-art
method [46] by 10.2%.""",,https://arxiv.org/abs/2111.07991v3,287.00,Zero-Shot Transfer with Locked-image Text Tuning,2021-11-15,Google,Industry,3005000000.00,Table 7,,"They start with the ViT-G/14 image model and train their own text model. ViT-G/14 is 3.4e21. 

They also say ""We use 128 TPU cores by default for the above experiments, and 256 TPU cores for our best run with 18 billion seen image-text pairs"" which may be relevant.",,"CC12M, YFCC100m, and their novel dataset:
""Our dataset. We collect 4 billion image and alt-text
pairs following the same process as ALIGN [31], with the
same image-based filtering but simpler text-based filtering.
Appendix L shows that reducing text filtering does not harm
performance. To avoid misleading evaluation results, we
remove from our dataset near-duplicate images of all splits
from all datasets we evaluate on. We do not consider the
creation of our dataset a main contribution of this paper; we
just simplify the data collection process in ALIGN [31] to
demonstrate the efficacy of our methods at scale.""",4000000000,"Largest dataset is ""4 billion image and alt-text pairs"". This is rounded down slightly; the other datasets are much smaller.",4.50,,,,,Google TPU v3,Google TPU v3,,,,,Likely,"This paper presents contrastive-tuning, a simple method employing contrastive training to align image and text models while still taking advantage of their pre-training. In our empirical study we find that locked pre-trained image models with unlocked text models work best. We call this instance of contrastive-tuning ""Locked-image Tuning"" (LiT), which just teaches a text model to read out good representations from a pre-trained image model for new tasks. A LiT model gains the capability of zero-shot transfer to new vision tasks, such as image classification or retrieval. The proposed LiT is widely applicable; it works reliably with multiple pre-training methods (supervised and unsupervised) and across diverse architectures (ResNet, Vision Transformers and MLP-Mixer) using three different image-text datasets. With the transformer-based pre-trained ViT-g/14 model, the LiT model achieves 85.2% zero-shot transfer accuracy on the ImageNet test set, and 82.5% on the challenging out-of-distribution ObjectNet test set.",2023-12-05 04:33,Anonymous,,,Multinational,Google,,,,ViT-G/14 (LiT),,,
BASIC-L,Vision,Image Classification,"Hieu Pham, Zihang Dai, Golnaz Ghiasi, Kenji Kawaguchi, Hanxiao Liu, Adams Wei Yu, Jiahui Yu, Yi-Ting Chen, Minh-Thang Luong, Yonghui Wu, Mingxing Tan, Quoc V. Le",SOTA Improvement,"SOTA on ImageNet for a model that was not trained on ImageNet images:
""We present a combined scaling method – named BASIC – that achieves 85.7% top-1 accuracy on the ImageNet ILSVRC-2012 validation set without learning from any labeled ImageNet example. This accuracy
surpasses best-published similar models – CLIP and ALIGN – by 9.3%""",,https://arxiv.org/abs/2111.10050,107.00,Combined Scaling for Zero-shot Transfer Learning,2021-11-19,Google,Industry,3070000000.00,2.4B image model + 670M text model,8.3e+22,"6.9k + 1k + 0.8k = TPUv4-days for BASIC-L, per Table 8

275 teraflops * 6900 * 24 * 3600 * 0.4 (assumed utilization) = 8.3e22",,"""Starting from the ALIGN dataset, which contains 1.7B weakly-aligned image-text pairs (Jia et al., 2021), we collect 5B more image-text pairs, hence expanding the dataset size by roughly 4 times. We acquire
these 5B image-text pairs from the JFT dataset""",6700000000,6.7B image-text pairs,3.00,708000000000.00,"per Table 5, if I'm understanding correctly",,,Google TPU v4,Google TPU v4,,,,,Likely,"We present a combined scaling method - named BASIC - that achieves 85.7% top-1 accuracy on the ImageNet ILSVRC-2012 validation set without learning from any labeled ImageNet example. This accuracy surpasses best published similar models - CLIP and ALIGN - by 9.3%. Our BASIC model also shows significant improvements in robustness benchmarks. For instance, on 5 test sets with natural distribution shifts such as ImageNet-{A,R,V2,Sketch} and ObjectNet, our model achieves 84.3% top-1 average accuracy, only a small drop from its original ImageNet accuracy. To achieve these results, we scale up the contrastive learning framework of CLIP and ALIGN in three dimensions: data size, model size, and batch size. Our dataset has 6.6B noisy image-text pairs, which is 4x larger than ALIGN, and 16x larger than CLIP. Our largest model has 3B weights, which is 3.75x larger in parameters and 8x larger in FLOPs than ALIGN and CLIP. Finally, our batch size is 65536 which is 2x more than CLIP and 4x more than ALIGN. We encountered two main challenges with the scaling rules of BASIC. First, the main challenge with implementing the combined scaling rules of BASIC is the limited memory of accelerators, such as GPUs and TPUs. To overcome the memory limit, we propose two simple methods which make use of gradient checkpointing and model parallelism. Second, while increasing the dataset size and the model size has been the defacto method to improve the performance of deep learning models like BASIC, the effect of a large contrastive batch size on such contrastive-trained image-text models is not well-understood. To shed light on the benefits of large contrastive batch sizes, we develop a theoretical framework which shows that larger contrastive batch sizes lead to smaller generalization gaps for image-text models such as BASIC.",2023-12-05 04:33,Anonymous,,,Multinational,Google,,,,BASIC-L,,,
Florence,Vision,,"Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, Ce Liu, Mengchen Liu, Zicheng Liu, Yumao Lu, Yu Shi, Lijuan Wang, JianFeng Wang, Bin Xiao, Zhen Xiao, Jianwei Yang, Michael Zeng, Luowei Zhou, Pengchuan Zhang","SOTA Improvement,Historical significance",,,https://arxiv.org/pdf/2111.11432v1.pdf,501.00,Florence: A New Foundation Model for Computer Vision,2021-11-22,Microsoft,Industry,893000000.00,"""Our Florence pretrained model has in total 893M parameters, including the language transformer with 256M parameters and the CoSwin-H transformer with 637M parameters.""",4.831e+22,"""The model takes 10 days to train on 512 NVIDIA A100 GPUs with 40GB memory per GPU.""
512 * 312 teraFLOPS * 10 days * 35% utilization = 4.831e22 FLOP",,900 million image-text pairs curated from internet images and descriptions,900000000,,,,,240.0,10 days on 512 A100 40GB,NVIDIA A100 SXM4 40 GB,NVIDIA A100 SXM4 40 GB,Supervised,,,Industry,Confident,"Automated visual understanding of our diverse and open world demands computer vision models to generalize well with minimal customization for specific tasks, similar to human vision. Computer vision foundation models, which are trained on diverse, large-scale dataset and can be adapted to a wide range of downstream tasks, are critical for this mission to solve real-world computer vision applications. While existing vision foundation models such as CLIP, ALIGN, and Wu Dao 2.0 focus mainly on mapping images and textual representations to a cross-modal shared representation, we introduce a new computer vision foundation model, Florence, to expand the representations from coarse (scene) to fine (object), from static (images) to dynamic (videos), and from RGB to multiple modalities (caption, depth). By incorporating universal visual-language representations from Web-scale image-text data, our Florence model can be easily adapted for various computer vision tasks, such as classification, retrieval, object detection, VQA, image caption, video retrieval and action recognition. Moreover, Florence demonstrates outstanding performance in many types of transfer learning: fully sampled fine-tuning, linear probing, few-shot transfer and zero-shot transfer for novel images and objects. All of these properties are critical for our vision foundation model to serve general purpose vision tasks. Florence achieves new state-of-the-art results in majority of 44 representative benchmarks, e.g., ImageNet-1K zero-shot classification with top-1 accuracy of 83.74 and the top-5 accuracy of 97.18, 62.4 mAP on COCO fine tuning, 80.36 on VQA, and 87.8 on Kinetics-600.",2023-12-05 04:33,Anonymous,,,Multinational,Microsoft,,,,Florence,,,
Persia,Recommendation,Recommender system,"Xiangru Lian, Binhang Yuan, Xuefeng Zhu, Yulong Wang, Yongjun He, Honghuan Wu, Lei Sun, Haodong Lyu, Chengjun Liu, Xing Dong, Yiqiao Liao, Mingnan Luo, Congfei Zhang, Jingru Xie, Haonan Li, Lei Chen, Renjie Huang, Jianying Lin, Chengchun Shu, Xuezhong Qiu, Zhishan Liu, Dongying Kong, Lei Yuan, Hai Yu, Sen Yang, Ce Zhang, Ji Liu",,,Permissive license,https://arxiv.org/abs/2111.05897,13.00,"Persia: An Open, Hybrid System Scaling Deep Learning-based Recommenders up to 100 Trillion Parameters",2021-11-23,"ETH Zurich,Kuaishou Technology",Industry - Academia Collaboration,100000000000000.00,100 trillion,,,,,,,,,,,,NVIDIA V100,NVIDIA V100,,,,,Confident,"Deep learning based models have dominated the current landscape of production recommender systems. Furthermore, recent years have witnessed an exponential growth of the model scale--from Google's 2016 model with 1 billion parameters to the latest Facebook's model with 12 trillion parameters. Significant quality boost has come with each jump of the model capacity, which makes us believe the era of 100 trillion parameters is around the corner. However, the training of such models is challenging even within industrial scale data centers. This difficulty is inherited from the staggering heterogeneity of the training computation--the model's embedding layer could include more than 99.99% of the total model size, which is extremely memory-intensive; while the rest neural network is increasingly computation-intensive. To support the training of such huge models, an efficient distributed training system is in urgent need. In this paper, we resolve this challenge by careful co-design of both the optimization algorithm and the distributed system architecture. Specifically, in order to ensure both the training efficiency and the training accuracy, we design a novel hybrid training algorithm, where the embedding layer and the dense neural network are handled by different synchronization mechanisms; then we build a system called Persia (short for parallel recommendation training system with hybrid acceleration) to support this hybrid training algorithm. Both theoretical demonstration and empirical study up to 100 trillion parameters have conducted to justified the system design and implementation of Persia. We make Persia publicly available (at this https URL) so that anyone would be able to easily train a recommender model at the scale of 100 trillion parameters.",2023-12-05 04:33,Anonymous,,0,"Switzerland,China","ETH Zurich, Kuaishou Technology",,,,,,,
NÜWA,Multimodal,,"Chenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang, Daxin Jiang, Nan Duan",SOTA Improvement,"""NÜWA achieves state-of-the-art results on text-to-image generation, text-to-video generation, video prediction, etc""",,https://arxiv.org/abs/2111.12417,167.00,NÜWA: Visual Synthesis Pre-training for Neural visUal World creAtion,2021-11-24,"Microsoft Research,Peking University",Industry,870000000.00,Section 4.1,4.8384e+21,"From AI Tracker:
""Compute cost: End of Sec 4.1: ""We pre-train on 64 A100 GPUs for two weeks"". Info sheet from NVIDIA (https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet.pdf) gives single precision TensorFloat 32 performance of 156 TFLOPs/s. So we get 64 x 14 x 156 = 140,000 TFLOPs/s x days.""

Multiply by seconds/day and 30% utilization","Conceptual Captions, Moments in Time, VATEX",,,"we first pre-train N  ̈UWA on three
datasets: Conceptual Captions [22] for text-to-image (T2I)
generation, which includes 2.9M text-image pairs, Mo-
ments in Time [26] for video prediction (V2V), which in-
cludes 727K videos, and VATEX dataset [43] for text-to-
video (T2V) generation, which includes 241K text-video
pairs.",,,,,,,,Self-supervised learning,$10446.84,,Industry,,"This paper presents a unified multimodal pre-trained model called NÜWA that can generate new or manipulate existing visual data (i.e., images and videos) for various visual synthesis tasks. To cover language, image, and video at the same time for different scenarios, a 3D transformer encoder-decoder framework is designed, which can not only deal with videos as 3D data but also adapt to texts and images as 1D and 2D data, respectively. A 3D Nearby Attention (3DNA) mechanism is also proposed to consider the nature of the visual data and reduce the computational complexity. We evaluate NÜWA on 8 downstream tasks. Compared to several strong baselines, NÜWA achieves state-of-the-art results on text-to-image generation, text-to-video generation, video prediction, etc. Furthermore, it also shows surprisingly good zero-shot capabilities on text-guided image and video manipulation tasks. Project repo is this https URL.",2023-12-05 04:33,Robi Rahman,,,"United States of America,China","Microsoft Research, Peking University",,,,NÜWA,,,
Quantized ADMM,Language,,"Junhao Xu, Xie Chen, Shoukang Hu, Jianwei Yu, Xunying Liu, Helen Meng",,,,https://arxiv.org/pdf/2111.14836,9.00,Low-bit Quantization of Recurrent Neural Network Language Models Using Alternating Direction Methods of Multipliers,2021-11-29,,,,,,,,,,,50.00,,,,,,,,,,,,,2023-11-21 01:39,Robi Rahman,Quantized ADMM,,,,,,,,,,
Transformer LM + MinSen,Language,,"Junhao Xu, Shoukang Hu, Jianwei Yu, Xunying Liu, Helen Meng",,,,https://arxiv.org/pdf/2112.11540,9.00,Mixed Precision of Quantization of Transformer Language Models for Speech Recognition,2021-11-29,,Academia,,,,,,,,,,,,,,,,,,,,,,2023-11-21 01:39,Robi Rahman,Transformer LM + MinSen,,,,,,,,,,
GPT-2-Medium+Pixelfly,Language,,"Tri Dao, Beidi Chen, Kaizhao Liang, Jiaming Yang, Zhao Song, Atri Rudra, Christopher Ré",,,,https://arxiv.org/pdf/2112.00029,48.00,Pixelated Butterfly: Simple and Efficient Sparse training for Neural Network Models,2021-11-30,,,203000000.00,,834000000000000000000.00,,,,,,100.00,,,,,,,,,,,,,2023-12-05 04:33,Robi Rahman,GPT-2-Medium+Pixelfly,,,,,,,,,,
GPT-2-Small+Pixelfly,Language,,,,,,,,,2021-11-30,,,,,,,,,,,100.00,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,GPT-2-Small+Pixelfly,1,,,,,,,,,
Player of Games,Games,,"Martin Schmid, Matej Moravcik, Neil Burch, Rudolf Kadlec, Josh Davidson, Kevin Waugh, Nolan Bard, Finbarr Timbers, Marc Lanctot, Zach Holland, Elnaz Davoodi, Alden Christianson, Michael Bowling",SOTA Improvement,"""Player of Games reaches strong performance in chess and Go, beats the strongest openly available agent in heads-up no-limit Texas hold'em poker (Slumbot), and defeats the state-of-the-art agent in Scotland Yard""",,https://arxiv.org/abs/2112.03178,9.00,Player of Games,2021-12-06,DeepMind,Industry,,,,,,,,,,,,,,,,,,,Industry,,,2023-11-26 03:25,Robi Rahman,,,United Kingdom of Great Britain and Northern Ireland,DeepMind,,,,Player of Games,,,
DeBERTaV3-large + KEAR ,Language,,"Yichong Xu, Chenguang Zhu, Shuohang Wang, Siqi Sun, Hao Cheng, Xiaodong Liu, Jianfeng Gao, Pengcheng He, Michael Zeng, Xuedong Huang",SOTA Improvement,"""The proposed system, Knowledgeable External Attention for commonsense Reasoning (KEAR), reaches human parity on the open CommonsenseQA research benchmark with an accuracy of 89.4\% in comparison to the human accuracy of 88.9\%.""

SOTA per https://paperswithcode.com/sota/common-sense-reasoning-on-commonsenseqa",,https://arxiv.org/abs/2112.03254v3,42.00,Human Parity on CommonsenseQA: Augmenting Self-Attention with External Attention,2021-12-06,Microsoft,Industry,418000000.00,"DeBERTaV3-large had 418M params, per Table 2",,this is a fine-tuned version of DeBERTaV3-large,,"""We present details of the 17 datasets that we use for training
data retrieval in Table 1. All the datasets are multiple-choice
or classification datasets related to commonsense reasoning,
and we include dataset details in the appendix.""",,,,,,,,,,,,,,Likely,"Most of today's AI systems focus on using self-attention mechanisms and transformer architectures on large amounts of diverse data to achieve impressive performance gains. In this paper, we propose to augment the transformer architecture with an external attention mechanism to bring external knowledge and context to bear. By integrating external information into the prediction process, we hope to reduce the need for ever-larger models and increase the democratization of AI systems. We find that the proposed external attention mechanism can significantly improve the performance of existing AI systems, allowing practitioners to easily customize foundation AI models to many diverse downstream applications. In particular, we focus on the task of Commonsense Reasoning, demonstrating that the proposed external attention mechanism can augment existing transformer models and significantly improve the model's reasoning capabilities. The proposed system, Knowledgeable External Attention for commonsense Reasoning (KEAR), reaches human parity on the open CommonsenseQA research benchmark with an accuracy of 89.4\% in comparison to the human accuracy of 88.9\%.",2023-12-05 04:33,Anonymous,,,Multinational,Microsoft,,,,DeBERTaV3-large + KEAR ,,,
Gopher (280B),Language,Language modelling,"Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d’Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu and Geoffrey Irving",SOTA Improvement,"""These models are evaluated on 152 diverse tasks, achieving state-of-the-art performance across the majority""",,https://arxiv.org/abs/2112.11446,736.00,"Scaling Language Models: Methods, Analysis & Insights from Training Gopher",2021-12-08,DeepMind,Industry,280000000000.00,"Language modelling provides a step towards intelligent communication systems by harnessing large repositories of written human knowledge to better predict and understand the world. In this paper, we present an analysis of Transformer-based language model performance across a wide range of model scales -- from models with tens of millions of parameters up to a 280 billion parameter model called Gopher. These models are evaluated on 152 diverse tasks, achieving state-of-the-art performance across the majority. Gains from scale are largest in areas such as reading comprehension, fact-checking, and the identification of toxic language, but logical and mathematical reasoning see less benefit. We provide a holistic analysis of the training dataset and model's behaviour, covering the intersection of model scale with bias and toxicity. Finally we discuss the application of language models to AI safety and the mitigation of downstream harms.",6.31e+23,"See table A24
6.31E+08 Train PFLOPs",,,225000000000,"""We train all models for 300 billion tokens with a 2048 token context window, using the Adam (Kingma and Ba, 2014) optimiser.""

1 token ~ 0.75 words",1.00,,,920.0,"""We trained Gopher for 920 hours in November and December 2020 in Google’s Georgia datacentre. The PUE of the datacenter at this time was 1.08; the net tCO2e per MWh in October 2020 was 0.33. Using an estimate of 283W drawn per chip, this leads to a total of 380 net tCO2e""",Google TPU v3,Google TPU v3,Self-supervised learning,$891638.80,,Industry,Confident,"We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a 2 trillion token database, our Retrieval-Enhanced Transformer (RETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25× fewer parameters. After fine-tuning, RETRO performance translates to downstream knowledge-intensive tasks such as question answering. RETRO combines a frozen Bert retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training. We typically train RETRO from scratch, yet can also rapidly RETROfit pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale.",2023-12-05 04:33,Robi Rahman,Gopher (280B),0,United Kingdom of Great Britain and Northern Ireland,DeepMind,,,,Gopher (280B),4096,0.3780,Gopher (280B)
Gopher (7.1B),Language,,,,,,,,,2021-12-08,,,,,,,,,,,1.00,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,Gopher (7.1B),1,,,,,,,,,
Engin-XL(NE),Language,,"Zhongping Zhang, Yiwen Gu, Bryan A. Plummer",,,,https://arxiv.org/pdf/2112.05917,0.00,Show and Write: Entity-aware Article Generation with Image Information,2021-12-11,,,1500000000.00,,,,,,,,3.00,,,,,,,,,,,,,2023-11-21 01:39,Robi Rahman,Engin-XL(NE),,,,,,,,,,
Engin-Base (NE),Language,,,,,,,,,2021-12-11,,,,,,,,,,,3.00,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,Engin-Base (NE),1,,,,,,,,,
Engin-Medium(NE),Language,,,,,,,,,2021-12-11,,,,,,,,,,,3.00,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,Engin-Medium(NE),1,,,,,,,,,
GLaM,Language,,"Nan Du, Yanping Huang, Andrew M. Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, Barret Zoph, Liam Fedus, Maarten Bosma, Zongwei Zhou, Tao Wang, Yu Emma Wang, Kellie Webster, Marie Pellat, Kevin Robinson, Kathleen Meier-Hellstern, Toju Duke, Lucas Dixon, Kun Zhang, Quoc V Le, Yonghui Wu, Zhifeng Chen, Claire Cui",SOTA Improvement,"""As shown in Table 5, GLaM (64B/64E) is better than the dense model and outperforms the previous finetuned state-of-the-art (SOTA) on this dataset in the open-domain setting""",,https://arxiv.org/abs/2112.06905,314.00,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,2021-12-13,Google,Industry,1200000000000.00,1.2 trillion parameters,3.74e+23,"from paper: ""GLaM (64B/64E) training after 600B tokens consumes 456 MWh, about 1/3 of the energy cost of 1287 MWh used by GPT-3. Moreover, to reach similar (and slightly exceeded) scores as GPT-3, we train using 1,024 TPU-v4 chips for 574 hours (with 280B tokens). This consumes 213 MWh or 1/6 of the GPT-3 energy cost""

600/280 is almost exactly 456/213 (2.14) so the later tokens have the same per-token energy cost. 
2.14*574*1024 = 1,257,840 TPU-v4 hours
TPU-v4s are 275 teraFLOP/s. 
Using our usual 0.3 utilization assumption, 275 trillion * 1,257,840 * 3600 * 0.3 = 3.74e23

Later they say they measured 326W power usage per chip, which could maybe be used to estimate utilization.",,"""To train our model, we build a high-quality dataset of 1.6 trillion tokens that are representative of a wide range of natural language use cases. Web pages constitute the vast quantity of data in our unlabeled dataset. However, their
quality ranges from professional writing to low-quality comment and forum pages.""",800000000000,"The dataset is made of 1.6 trillion tokens, but later in the paper they say they only train the largest model for 600b tokens. 600b / 0.75 words/token = 800b words.",,180000000000.00,"180 GFLOPs/token, per Table 1",1366.0,"Note that they give several energy estimates. Use the complete training figures for 600B tokens, not the GPT-3 comparison values with 280B tokens.

""326W measured system power per TPU-v4 chip""
""The complete GLaM training using 600B tokens consumes only
456 MWh""
1024 TPU v4 chips
(456 MWh) / (326W/chip * 1024 chips) = 1366 hours",Google TPU v4,Google TPU v4,,,,,Likely,,2023-12-05 04:33,Anonymous,,,Multinational,Google,,,,GLaM,1024,,GLaM
HSO,Language,,"Davis Yoshida, Kevin Gimpel",,,,https://web.archive.org/web/20230220145200/https://arxiv.org/pdf/2112.08653.pdf,1.00,Reconsidering the Past: Optimizing Hidden States in Language Models,2021-12-16,,,345000000.00,,345000000000000000000.00,,,,,,,,,,,,,,,,,,,2023-11-23 06:53,Robi Rahman,HSO,,,,,,,,,,
XGLM,Language,,"Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O’Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov, Xian Li",SOTA improvement,"""Our largest model (XGLM7.5B) sets a new state of the art performance for few-shot learning in more than 20 representative languages (including medium- and low-resource languages) for the tasks of commonsense reasoning, natural language inference and machine translation.""",,https://arxiv.org/abs/2112.10668,110.00,Few-shot Learning with Multilingual Language Models,2021-12-20,Meta AI,Industry,7500000000.00,,,,Subset of CC100-XL,,1740000000,,,,,,,,,Self-supervised learning,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,Multinational,Meta AI,,,,XGLM,,,
GLIDE,Drawing,,"Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam Pamela Mishkin Bob McGrew IlyaSutskever MarkChen",Highly cited,,,https://arxiv.org/abs/2112.10741,1407.00,GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models,2021-12-20,OpenAI,Industry,3500000000.00,"""Samples from a 3.5 billion parameter text-conditional diffusion model using classifier-free guidance are favored by human evaluators to those from DALL-E, even when the latter uses expensive CLIP reranking""",,"""Note that GLIDE was
trained with roughly the same training compute as DALL-E
but with a much smaller model (3.5 billion vs. 12 billion
parameters)""",,,250000000,"Section 4:
""We train our model on the same dataset as DALL-E (Ramesh
et al., 2021)""

This paper used 250M image-text pairs
https://arxiv.org/pdf/2102.12092.pdf",,,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,United States of America,OpenAI,,,,GLIDE,,,
MoE-1.1T,Language,,"Mikel Artetxe, Shruti Bhosale, Naman Goyal, Todor Mihaylov, Myle Ott, Sam Shleifer, Xi Victoria Lin, Jingfei Du, Srinivasan Iyer, Ramakanth Pasunuru, Giri Anantharaman, Xian Li, Shuohui Chen, Halil Akin, Mandeep Baines, Louis Martin, Xing Zhou, Punit Singh Koura, Brian O'Horo, Jeff Wang, Luke Zettlemoyer, Mona Diab, Zornitsa Kozareva, Ves Stoyanov",,,,https://arxiv.org/abs/2112.10684,94.00,Efficient Large Scale Language Modeling with Mixtures of Experts,2021-12-20,Meta AI,Industry,1100000000000.00,,2.227e+22,Reported directly in paper. Authors calculate FLOPs analytically in appendix G,,"""We pretrain our models on a union of six Englishlanguage datasets, including the five datasets used to pretrain RoBERTa (Liu et al., 2019) and the
English subset of CC100, totalling 112B tokens corresponding to 453GB:
BookCorpus (Zhu et al., 2019) consists of more
than 10K unpublished books (4GB);
• English Wikipedia, excluding lists, tables and
headers (12GB);
• CC-News (Nagel, 2016) contains 63 millions English news articles crawled between September
2016 and February 2019 (76GB);
• OpenWebText (Gokaslan and Cohen, 2019), an
open source recreation of the WebText dataset
used to train GPT-2 (38GB);
• CC-Stories (Trinh and Le, 2018) contains a subset of CommonCrawl data filtered to match the
story-like style of Winograd schemas (31GB);
• English CC100 (Wenzek et al., 2020), a dataset
extracted from CommonCrawl snapshots between January 2018 and December 2018, filtered
to match the style of Wikipedia (292GB)""",84000000000,"112B tokens, or 84B words at 0.75 English words/token. 
""We pretrain our models on a union of six Englishlanguage datasets, including the five datasets used to pretrain RoBERTa (Liu et al., 2019) and the
English subset of CC100, totalling 112B tokens""",2.68,,,,,NVIDIA A100,NVIDIA A100,,,,,Confident,"Mixture of Experts layers (MoEs) enable efficient scaling of language models through conditional computation. This paper presents a detailed empirical study of how autoregressive MoE language models scale in comparison with dense models in a wide range of settings: in- and out-of-domain language modeling, zero- and few-shot priming, and full-shot fine-tuning. With the exception of fine-tuning, we find MoEs to be substantially more compute efficient. At more modest training budgets, MoEs can match the performance of dense models using ∼4 times less compute. This gap narrows at scale, but our largest MoE model (1.1T parameters) consistently outperforms a compute-equivalent dense model (6.7B parameters). Overall, this performance gap varies greatly across tasks and domains, suggesting that MoE and dense models generalize differently in ways that are worthy of future study. We make our code and models publicly available for research use.",2023-12-05 04:33,Anonymous,,,Multinational,Meta AI,,,,,,,
Fairseq-dense 13B,Language,,"Mikel Artetxe, Shruti Bhosale, Naman Goyal, Todor Mihaylov, Myle Ott, Sam Shleifer, Xi Victoria Lin, Jingfei Du, Srinivasan Iyer, Ramakanth Pasunuru, Giri Anantharaman, Xian Li, Shuohui Chen, Halil Akin, Mandeep Baines, Louis Martin, Xing Zhou, Punit Singh Koura, Brian O'Horo, Jeff Wang, Luke Zettlemoyer, Mona Diab, Zornitsa Kozareva, Ves Stoyanov",,,Permissive license,https://arxiv.org/abs/2112.10684,31.00,Efficient Large Scale Language Modeling with Mixtures of Experts,2021-12-20,Meta AI,Industry,13000000000.00,,3.267e+22,Table 1,,"""We pretrain our models on a union of six Englishlanguage datasets, including the five datasets used to pretrain RoBERTa (Liu et al., 2019) and the
English subset of CC100, totalling 112B tokens corresponding to 453GB:
BookCorpus (Zhu et al., 2019) consists of more
than 10K unpublished books (4GB);
• English Wikipedia, excluding lists, tables and
headers (12GB);
• CC-News (Nagel, 2016) contains 63 millions English news articles crawled between September
2016 and February 2019 (76GB);
• OpenWebText (Gokaslan and Cohen, 2019), an
open source recreation of the WebText dataset
used to train GPT-2 (38GB);
• CC-Stories (Trinh and Le, 2018) contains a subset of CommonCrawl data filtered to match the
story-like style of Winograd schemas (31GB);
• English CC100 (Wenzek et al., 2020), a dataset
extracted from CommonCrawl snapshots between January 2018 and December 2018, filtered
to match the style of Wikipedia (292GB)""",84000000000,"112B tokens, or 84B words at 0.75 English words/token. 
""We pretrain our models on a union of six Englishlanguage datasets, including the five datasets used to pretrain RoBERTa (Liu et al., 2019) and the English subset of CC100, totalling 112B tokens""
...
""All models are trained for 300B tokens with a sequence length of 2048 tokens.""",2.68,,,,,NVIDIA A100,NVIDIA A100,,,,,Likely,"Mixture of Experts layers (MoEs) enable efficient scaling of language models through conditional computation. This paper presents a detailed empirical study of how autoregressive MoE language models scale in comparison with dense models in a wide range of settings: in- and out-of-domain language modeling, zero- and few-shot priming, and full-shot fine-tuning. With the exception of fine-tuning, we find MoEs to be substantially more compute efficient. At more modest training budgets, MoEs can match the performance of dense models using ∼4 times less compute. This gap narrows at scale, but our largest MoE model (1.1T parameters) consistently outperforms a compute-equivalent dense model (6.7B parameters). Overall, this performance gap varies greatly across tasks and domains, suggesting that MoE and dense models generalize differently in ways that are worthy of future study. We make our code and models publicly available for research use.",2023-12-05 08:19,Anonymous,,0,Multinational,Meta AI,,,,,,,
ERNIE 3.0 Titan,Language,,"Shuohuan Wang, Yu Sun, Yang Xiang, Zhihua Wu, Siyu Ding, Weibao Gong, Shikun Feng",SOTA Improvement,"""Empirical results show that the ERNIE 3.0 Titan outperforms the state-of-the-art models on 68 NLP datasets.""",,https://arxiv.org/abs/2112.12731,40.00,ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training for Language Understanding and Generation,2021-12-23,"Baidu,Peng Cheng Laboratory",Industry,260000000000.00,"""[We] developed... distributed training technology, including fine-grained parallelism, heterogeneous hardware-aware training, and fault tolerance mechanism to train the 260B model on both Nvidia V100 GPU and Ascend 910 NPU clusters.""
See also:
https://twitter.com/BaiduResearch/status/1468633977242243078?t=6q4zuLNdTSc4GUBe9OM5Aw&s=19",1.0421e+24,"The paper suggests that ERNIE 3.0 Titan uses more compute than GPT-3. This is consistent with the 6ND approximation.

C = 6ND = 6 (FLOP/param/token) * (260B params) * (668B tokens) = 1.0421*10^24 FLOP", ERNIE 3.0 Corpus,,668000000000,"""To ensure the success of the pre-training of ERNIE 3.0 Titan, we utilize the ERNIE 3.0 Corpus [ 2 ], a large-scale, wide-variety, and high-quality Chinese text corpora amounting to 4TB""

Assuming 167M words per GB",,,,,,"Huawei Ascend 910,NVIDIA Tesla V100 DGXS 32 GB","Huawei Ascend 910, NVIDIA Tesla V100 DGXS 32 GB",,,,Industry,Likely,"Pre-trained language models have achieved state-of-the-art results in various Natural Language Processing (NLP) tasks. GPT-3 has shown that scaling up pre-trained language models can further exploit their enormous potential. A unified framework named ERNIE 3.0 was recently proposed for pre-training large-scale knowledge enhanced models and trained a model with 10 billion parameters. ERNIE 3.0 outperformed the state-of-the-art models on various NLP tasks. In order to explore the performance of scaling up ERNIE 3.0, we train a hundred-billion-parameter model called ERNIE 3.0 Titan with up to 260 billion parameters on the PaddlePaddle platform. Furthermore, we design a self-supervised adversarial loss and a controllable language modeling loss to make ERNIE 3.0 Titan generate credible and controllable texts. To reduce the computation overhead and carbon emission, we propose an online distillation framework for ERNIE 3.0 Titan, where the teacher model will teach students and train itself simultaneously. ERNIE 3.0 Titan is the largest Chinese dense pre-trained model so far. Empirical results show that the ERNIE 3.0 Titan outperforms the state-of-the-art models on 68 NLP datasets.",2023-12-05 04:33,Robi Rahman,,,"China,China","Baidu, Peng Cheng Laboratory",,,,ERNIE 3.0 Titan,1920,,ERNIE 3.0 Titan
ERNIE-ViLG,Multimodal,Vision-language generation,"Han Zhang, Weichong Yin, Yewei Fang, Lanxin Li, Boqiang Duan, Zhihua Wu, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang",SOTA Improvement,"""we train a 10-billion parameter ERNIE-ViLG model on a large-scale dataset of 145 million (Chinese) image-text pairs which achieves state-of-the-art performance for both text-to-image and image-to-text tasks""",,https://arxiv.org/abs/2112.15283,40.00,ERNIE-ViLG: Unified Generative Pre-training for Bidirectional Vision-Language Generation,2021-12-31,Baidu,Industry,10000000000.00,"""To explore the landscape of large-scale pre-training for bidirectional text-image generation, we pre-train a 10-billion parameter model on a large-scale dataset of 145 million high-quality Chinese image-text pairs.""",,,,,145000000,"To explore the landscape of large-scale pre-training for bidirectional text-image generation,
we pre-train a 10-billion parameter model on a large-scale dataset of 145 million high-quality Chinese image-text pairs.",,,,,,,,Self-supervised learning,,,,,,2023-12-05 04:33,Robi Rahman,,,China,Baidu,,,,ERNIE-ViLG,,,
data2vec (language),Language,,"Alexei Baevski,  Wei-Ning Hsu,  Qiantong Xu , Arun Babu,  Jiatao Gu,  Michael Auli",SOTA Improvement,"""Experiments on the major benchmarks of speech recognition, image classification, and natural lan guage understanding demonstrate a new state of the art or competitive performance to predominant approaches""",,https://ai.facebook.com/research/data2vec-a-general-framework-for-self-supervised-learning-in-speech-vision-and-language/,460.00,"Data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language",2022-01-20,Meta AI,Industry,705134592.00,"Section 4: ""We experiment with two model sizes: data2vec Base and
data2vec Large, containing either L = 12 or L = 24 Trans-
former blocks with H = 768 or H = 1024 hidden dimen-
sion (with 4 × H feed-forward inner-dimension)""
",,,"Books Corpus, English Wikipedia",,3300000000,"Section 5.3: ""we
adopt the same training setup as BERT (Devlin et al., 2019)
by pre-training on the Books Corpus (Zhu et al., 2015) and
English Wikipedia data over 1M updates and a batch size
of 256 sequences.""",,,,,,,,Self-supervised learning,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,Multinational,Meta AI,,,,data2vec (language),,,
data2vec (speech),Speech,,"Alexei Baevski,  Wei-Ning Hsu,  Qiantong Xu , Arun Babu,  Jiatao Gu,  Michael Auli",SOTA Improvement,"""Experiments on the major benchmarks of speech recognition, image classification, and natural lan guage understanding demonstrate a new state of the art or competitive performance to predominant approaches""",,https://ai.facebook.com/research/data2vec-a-general-framework-for-self-supervised-learning-in-speech-vision-and-language/,460.00,"Data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language",2022-01-20,Meta AI,Industry,705134592.00,"Section 4: ""We experiment with two model sizes: data2vec Base and
data2vec Large, containing either L = 12 or L = 24 Trans-
former blocks with H = 768 or H = 1024 hidden dimen-
sion (with 4 × H feed-forward inner-dimension)""
",,,LS-960,,13132800,"Section 5.2:
""we pre-train data2vec on the 960
hours of speech audio data from Librispeech (LS-960)""

13,680 words per hour",,,,,,,,Self-supervised learning,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,Multinational,Meta AI,,,,data2vec (speech),,,
data2vec (vision),Vision,,"Alexei Baevski,  Wei-Ning Hsu,  Qiantong Xu , Arun Babu,  Jiatao Gu,  Michael Auli",SOTA Improvement,"""Experiments on the major benchmarks of speech recognition, image classification, and natural lan guage understanding demonstrate a new state of the art or competitive performance to predominant approaches""",,https://ai.facebook.com/research/data2vec-a-general-framework-for-self-supervised-learning-in-speech-vision-and-language/,460.00,"Data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language",2022-01-20,Meta AI,Industry,705134592.00,"Section 4: ""We experiment with two model sizes: data2vec Base and
data2vec Large, containing either L = 12 or L = 24 Trans-
former blocks with H = 768 or H = 1024 hidden dimen-
sion (with 4 × H feed-forward inner-dimension)""
",,,ImageNet,,1281167,"Section 5.1: 
""we pretrain data2vec on the images of the ImageNet-1K training
set""",,,,,,,,Self-supervised learning,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,Multinational,Meta AI,,,,data2vec (vision),,,
Primer,Language,,"DavidR.So, WojciechMan ́ke, HanxiaoLiu, ZihangDai, NoamShazeer, QuocV.Le",,,,https://arxiv.org/abs/2109.08668,58.00,Primer: Searching for Efficient Transformers for Language Modeling,2022-01-24,Google Brain,Industry,1900000000.00,"""For instance, in a 1.9B parameter configuration similar to GPT-3 XL, Primer uses 1/3 of the training compute to achieve the same one-shot performance as Transformer""",7.1e+21,"From the email they claim to have use 72K TPUv4 hours for training

Thus: 
72000 h * 0.1 * 275e12 FLOP/s 3600s/h = 7.1e21 FLOP",C4,,173284750600,"In GB - TODO convert to words

""Dataset size: 806.92 GiB""
https://www.tensorflow.org/datasets/catalog/c4

This was the largest dataset that the authors used 
""These benefits are robust and hold across model sizes (20M
to 1.9B parameters), across compute scales (10 to 105
accelerator hours), across datasets (LM1B,
C4, PG19 [22])""

802.92 GiB ~ 866.42 GB
1 GB ~ 200M words",,,,,,,,,$9690.72,,Industry,,"Large Transformer models have been central to recent advances in natural language processing. The training and inference costs of these models, however, have grown rapidly and become prohibitively expensive. Here we aim to reduce the costs of Transformers by searching for a more efficient variant. Compared to previous approaches, our search is performed at a lower level, over the primitives that define a Transformer TensorFlow program. We identify an architecture, named Primer, that has a smaller training cost than the original Transformer and other variants for auto-regressive language modeling. Primer's improvements can be mostly attributed to two simple modifications: squaring ReLU activations and adding a depthwise convolution layer after each Q, K, and V projection in self-attention. Experiments show Primer's gains over Transformer increase as compute scale grows and follow a power law with respect to quality at optimal model sizes. We also verify empirically that Primer can be dropped into different codebases to significantly speed up training without additional tuning. For example, at a 500M parameter size, Primer improves the original T5 architecture on C4 auto-regressive language modeling, reducing the training cost by 4X. Furthermore, the reduced training cost means Primer needs much less compute to reach a target one-shot performance. For instance, in a 1.9B parameter configuration similar to GPT-3 XL, Primer uses 1/3 of the training compute to achieve the same one-shot performance as Transformer. We open source our models and several comparisons in T5 to help with reproducibility.",2023-12-05 04:33,Robi Rahman,,,Multinational,Google Brain,,,,,,,
SPALM + RelationLM,Language,,"Qi Liu, Dani Yogatama, Phil Blunsom",,,,https://arxiv.org/pdf/2201.09680,21.00,Relational Memory-Augmented Language Models,2022-01-24,,Industry - Academia Collaboration,124000000.00,,,,WikiText-103,,,,,,,,,,,,,,,,,2023-11-21 01:39,Robi Rahman,SPALM + RelationLM,,,,,,,,,,
TransformerXL+RelationLM,Language,,"Qi Liu, Dani Yogatama, Phil Blunsom",,,,https://arxiv.org/pdf/2201.09680,21.00,Relational Memory-Augmented Language Models,2022-01-24,,Industry - Academia Collaboration,124000000.00,,3.2e+21,,WikiText-103,,,,,,,,,,,,,,,,,2023-11-23 06:53,Robi Rahman,TransformerXL+RelationLM,,,,,,,,,,
InstructGPT,Language,,"Long Ouyang, Pamela Mishkin, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright,John Schulman Amanda Askell, Fraser Kelton Peter Welinder, Luke Miller Maddie Simens Paul Christiano,Ryan Lowe,Chong Zhang Jacob Hilton, Sandhini Agarwal Katarina Slama Alex Ray, Jan Leike",Historical significance,,,https://cdn.openai.com/papers/Training_language_models_to_follow_instructions_with_human_feedback.pdf,3650.00,Training language models to follow instructions with human feedback,2022-01-27,OpenAI,Industry,,,,,,,374000033207,"Table 6 - describes **number of prompts**

26584 + 6623 = 33207

This is added to GPT-3 dataset size.",,,,,,,,Self-supervised learning,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,United States of America,OpenAI,,,,InstructGPT,,,
DARK,Language,"Protein generation,Proteins","Lewis Moffat, Shaun M. Kandathil, David T. Jones",,,,https://www.biorxiv.org/content/10.1101/2022.01.27.478087v1.full,20.00,Design in the DARK: Learning Deep Generative Models for De Novo Protein Design,2022-01-28,University College London (UCL),Academia,,"""DARK3, uses a Transformer decoder with 12 layers, 12 heads, and a feedforward dimension of 768.""",9700000000000000000.00,"""Rounding up to the nearest day, if we were to re-perform DARK from nothing to having a trained DARK3 it would take 12 days when parallelized across ten V100 GPUS. Of that time, model training constitutes just over 3 days and only requires 1 GPU""

3 * 24 * 3600 * 125 teraFLOP/s * 0.3 (utilization) = 9.7e18",,"Iteratively trained by generating synthetic data: ""To build this tool we develop a framework, called DARK, that trains the underlying generative model on an iteratively expanding set of synthetic sequences""",,,,,,,,NVIDIA V100,NVIDIA V100,Unsupervised,,,,Speculative,"The design of novel protein sequences is providing paths towards the development of novel therapeutics and materials. At the forefront is the challenging field of de novo protein design, which looks to design protein sequences unlike those found in nature using general design methodologies. In this work, we develop a tool for de novo design, based on a deep generative sequence model, that rapidly samples novel protein sequences with diverse and ordered structures. To build this tool we develop a framework, called DARK, that trains the underlying generative model on an iteratively expanding set of synthetic sequences. The resulting model generalizes where models trained on natural sequences struggle and greatly improves on the efficiency of comparable sampling-based approaches. We further show how it can generate high quality candidates for de novo design problems and aid in the development of further novel design methods, in all, providing another step, amongst others, towards truly automated and intelligent protein design.",2023-11-23 06:53,Anonymous,,,United Kingdom of Great Britain and Northern Ireland,University College London (UCL),,,,,,,
AlphaCode,Language,Code generation,"Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson d'Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, Oriol Vinyals",SOTA improvement,,,https://arxiv.org/abs/2203.07814,526.00,Competition-Level Code Generation with AlphaCode,2022-02-02,DeepMind,Industry,41100000000.00,Table 3,1.568160000001e+23,"Figure 7 (a) shows a maximum training compute budget of approx 20000 TPU-days per model.
20000 days * 275 TFLOPS * 0.33 utilization = 1.6e23 FLOP
https://www.wolframalpha.com/input?i=20000+*+275+teraFLOPS+*+1+day+*+0.33",,,,Appendix part A has answers for pretraining.,,,,,,"Google TPU v4,Google TPU v4i","Google TPU v4, Google TPU v4i",Self-supervised learning,,,Industry,,"Programming is a powerful and ubiquitous problem-solving tool. Developing systems that can assist programmers or even generate programs independently could make programming more productive and
accessible, yet so far incorporating innovations in AI has proven challenging. Recent large-scale language models have demonstrated an impressive ability to generate code, and are now able to complete
simple programming tasks. However, these models still perform poorly when evaluated on more complex, unseen problems that require problem-solving skills beyond simply translating instructions into
code. For example, competitive programming problems which require an understanding of algorithms
and complex natural language remain extremely challenging. To address this gap, we introduce AlphaCode, a system for code generation that can create novel solutions to these problems that require deeper
reasoning. In simulated evaluations on recent programming competitions on the Codeforces platform,
AlphaCode achieved on average a ranking of top 54.3% in competitions with more than 5,000 participants. We found that three key components were critical to achieve good and reliable performance:
(1) an extensive and clean competitive programming dataset for training and evaluation, (2) large and
efficient-to-sample transformer-based architectures, and (3) large-scale model sampling to explore the
search space, followed by filtering based on program behavior to a small set of submissions.
",2023-12-05 04:33,Robi Rahman,,,United Kingdom of Great Britain and Northern Ireland,DeepMind,,,,AlphaCode,3750,,AlphaCode
RETRO-7B,Language,,"Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego de Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack W. Rae‡, Erich Elsen, Laurent Sifre",SOTA Improvement,"""Our largest model obtains state-of-the-art results on a range of downstream evaluation
datasets including Wikitext103""",,https://arxiv.org/abs/2112.04426,431.00,Improving language models by retrieving from trillions of tokens,2022-02-07,DeepMind,Industry,7500000000.00,"""Retro provides a constant gain for models ranging from 150M to 7B parameters, and Retro can be improved at evaluation time by increasing the database size and the number of retrieved neighbours. """,,,,,315000000000,"""we train for 419,430,400,000 training tokens"" ~= 315B words.",,,,,,,,Self-supervised learning,,,Industry,,,2023-12-05 04:33,Robi Rahman,RETRO-7B,,United Kingdom of Great Britain and Northern Ireland,DeepMind,,,,RETRO-7B,,,
GPT-NeoX-20B,Language,,"Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, Samuel Weinbach",Historical significance,,Fully open-source,https://arxiv.org/abs/2204.06745,367.00,GPT-NeoX-20B: An Open-Source Autoregressive Language Model,2022-02-09,EleutherAI,Research Collective,20000000000.00,,9.31627008e+22,Trained for 3 months on 96 A100s (according to correspondence with author). Let's say 0.4 utilization rate.,The Pile,,177167400000,"""In aggregate, the Pile consists of over 825GiB of raw text data""

1 GB ~ 200M words",1.00,,,,,,,Self-supervised learning,$202407.46,,Industry,,,2023-12-05 04:33,Robi Rahman,GPT-NeoX-20B,,Multinational,EleutherAI,,,,GPT-NeoX-20B,,,
LaMDA,Language,Language modelling,"Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Kathleen Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak, Ed Chi, Quoc Le",Historical significance,,,https://arxiv.org/abs/2201.08239,891.00,LaMDA: Language Models for Dialog Applications,2022-02-10,Google,Industry,137000000000.00,"""LaMDA is a family of Transformer-based neural language models specialized for dialog, which have up to 137B parameters""",3.55e+23,"""The total FLOPS is 56.5% * 123 TFLOPS/s * 1024 chips * 57.7 days
= 3.55E+23""
From https://arxiv.org/pdf/2201.08239.pdf p.18
",Infiniset,"LaMDA's underlying dataset is called 'Infiniset', and besides the dialogue also involves common crawl, wikipedia, a mixture of english and non-english web documents, and data from programming-related sites (so LaMDA models can also dabble in code).",1560000000000,"""and are pre-trained on 1.56T words of public dialog data and web text""",,,,1385.0,57.7 days * 24,Google TPU v3,Google TPU v3,Self-supervised learning,$484957.20,,Industry,Confident,"We present LaMDA: Language Models for Dialog Applications. LaMDA is a family of Transformer-based neural language models specialized for dialog, which have up to 137B parameters and are pre-trained on 1.56T words of public dialog data and web text. While model scaling alone can improve quality, it shows less improvements on safety and factual grounding. We demonstrate that fine-tuning with annotated data and enabling the model to consult external knowledge sources can lead to significant improvements towards the two key challenges of safety and factual grounding. The first challenge, safety, involves ensuring that the model's responses are consistent with a set of human values, such as preventing harmful suggestions and unfair bias. We quantify safety using a metric based on an illustrative set of human values, and we find that filtering candidate responses using a LaMDA classifier fine-tuned with a small amount of crowdworker-annotated data offers a promising approach to improving model safety. The second challenge, factual grounding, involves enabling the model to consult external knowledge sources, such as an information retrieval system, a language translator, and a calculator. We quantify factuality using a groundedness metric, and we find that our approach enables the model to generate responses grounded in known sources, rather than responses that merely sound plausible. Finally, we explore the use of LaMDA in the domains of education and content recommendations, and analyze their helpfulness and role consistency.",2023-12-05 11:29,Robi Rahman,,,Multinational,Google,,,,LaMDA,1024,0.5650,LaMDA
ProteinBERT,Language,"Proteins,Protein generation","Nadav Brandes, Dan Ofer, Yam Peleg, Nadav Rappoport, Michal Linial",SOTA Improvement,"""ProteinBERT obtains near state-of-the-art performance, and sometimes exceeds it, on multiple benchmarks covering diverse protein properties (including protein structure, post-translational modifications and biophysical attributes)""",,https://academic.oup.com/bioinformatics/article/38/8/2102/6502274,212.00,ProteinBERT: a universal deep-learning model of protein sequence and function,2022-02-10,"Hebrew University of Jerusalem,Ben-Gurion University of the Negev,Deep Trading",Industry - Academia Collaboration (Industry leaning),16000000.00,"""Altogether, it includes ∼16M trainable parameters, making it substantially smaller than other protein language models""",65000000000000000000.00,"""Pretraining speed on a single GPU (Nvidia Quadro RTX 5000) was 280 protein records per second. We trained the model for 28 days over ∼670M records""

28 * 24 * 3600 * 89 tFLOP/s * 0.3 (assumed utilization) = 6.5e19",UniRef90,"""ProteinBERT was pretrained on ∼106M UniRef90 records for ∼6.4 epochs""",,,6.40,,,672.0,28 days,NVIDIA Quadro RTX 5000,NVIDIA Quadro RTX 5000,Self-supervised learning,,,,Likely,"Self-supervised deep language modeling has shown unprecedented success across natural language tasks, and has recently been repurposed to biological sequences. However, existing models and pretraining methods are designed and optimized for text analysis. We introduce ProteinBERT, a deep language model specifically designed for proteins. Our pretraining scheme combines language modeling with a novel task of Gene Ontology (GO) annotation prediction. We introduce novel architectural elements that make the model highly efficient and flexible to long sequences. The architecture of ProteinBERT consists of both local and global representations, allowing end-to-end processing of these types of inputs and outputs. ProteinBERT obtains near state-of-the-art performance, and sometimes exceeds it, on multiple benchmarks covering diverse protein properties (including protein structure, post-translational modifications and biophysical attributes), despite using a far smaller and faster model than competing deep-learning methods. Overall, ProteinBERT provides an efficient framework for rapidly training protein predictors, even with limited labeled data.",2023-11-26 03:25,Anonymous,,,"Israel,United States of America,United States of America","Hebrew University of Jerusalem, Ben-Gurion University of the Negev, Deep Trading",,,,ProteinBERT,,,
MuZero VP9,Other,Video compression,"Amol Mandhane, Anton Zhernov, Maribeth Rauh, Chenjie Gu, Miaosen Wang, Flora Xue, Wendy Shang, Derek Pang, Rene Claus, Ching-Han Chiang, Cheng Chen, Jingning Han, Angie Chen, Daniel J. Mankowitz, Jackson Broshear, Julian Schrittwieser, Thomas Hubert, Oriol Vinyals, Timothy Mann",,,,https://arxiv.org/abs/2202.06626,27.00,MuZero with Self-competition for Rate Control in VP9 Video Compression,2022-02-14,DeepMind,Industry,,,,,,,,,,,,,,,,,,,,,,2023-12-05 04:33,Robi Rahman,,,United Kingdom of Great Britain and Northern Ireland,DeepMind,,,,,,,
Midjourney V1,Drawing,Image generation,,"Significant use,Historical significance","Significant historical usage and controversy as one of the first generative AI art models. For example:
https://www.nytimes.com/2022/09/02/technology/ai-artificial-intelligence-artists.html",,,,,2022-02-15,Midjourney,Research Collective,,,,,,,,,,,,,,,,,,,,Speculative,,2023-11-26 03:25,Anonymous,,,United States of America,Midjourney,,,,Midjourney V1,,,
ST-MoE,Language,,"Barret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yanping Huang, Jeff Dean, Noam Shazeer, William Fedus",SOTA Improvement,"""ST-MoE-32B improves the current state-of-the-art on the test server submissions for both ARC Easy (92.7 → 94.8) and ARC Challenge (81.4 → 86.5).""",,https://arxiv.org/abs/2202.08906v2,38.00,ST-MoE: Designing Stable and Transferable Sparse Expert Models,2022-02-17,Google,Industry,269000000000.00,269B. it's called ST-MoE-32B because it's equivalent to a 32B dense model.,,,,"""The pre-training dataset used to train our Sparse 32B model is a mix of C4 (Raffel et al., 2019) and the dataset introduced in GLaM (Du et al., 2021).""",13420000000000,"1790B tokens, or 1342B words at 0.75 words/token",0.84,,"2e12 FLOP per sequence, per Table 11. Earlier, the paper says ""The sequence length for the encoder was 512 and 114 for the decoder""",,,,,,,,,Likely,"Scale has opened new frontiers in natural language processing -- but at a high cost. In response, Mixture-of-Experts (MoE) and Switch Transformers have been proposed as an energy efficient path to even larger and more capable language models. But advancing the state-of-the-art across a broad set of natural language tasks has been hindered by training instabilities and uncertain quality during fine-tuning. Our work focuses on these issues and acts as a design guide. We conclude by scaling a sparse model to 269B parameters, with a computational cost comparable to a 32B dense encoder-decoder Transformer (Stable and Transferable Mixture-of-Experts or ST-MoE-32B). For the first time, a sparse model achieves state-of-the-art performance in transfer learning, across a diverse set of tasks including reasoning (SuperGLUE, ARC Easy, ARC Challenge), summarization (XSum, CNN-DM), closed book question answering (WebQA, Natural Questions), and adversarially constructed tasks (Winogrande, ANLI R3).",2023-12-05 04:33,Anonymous,,,Multinational,Google,,,,ST-MoE,,,
PolyCoder,Language,Code generation,"Frank F. Xu, Uri Alon, Graham Neubig, Vincent J. Hellendoorn",SOTA Improvement,"""In the C programming language, PolyCoder outperforms
all models including Codex""",,https://arxiv.org/abs/2202.13169,222.00,A Systematic Evaluation of Large Language Models of Code,2022-02-26,Carnegie Mellon University (CMU),Academia,2700000000.00,2.7B for largest model,1.1e+21,"""We use GPT-NeoX toolkit 11 to
train the model efficiently in parallel with 8 Nvidia RTX 8000 GPUs on a single machine. The wall
time used to train the largest 2.7B model is about 6 weeks""

8 * 130 TFLOP/s * 6 * 7 * 24 * 3600 * 0.3 (utilization) ~= 1.1e21",,"Code scraped from GitHub. ""249GB of code across 12 programming languages on a single machine.""",,"249GB

They trained on 39B tokens per Table 3, but I'm not sure how many epochs that is. May be <1. ",,,,1000.0,6 weeks,NVIDIA Quadro RTX 8000,NVIDIA Quadro RTX 8000,,,,Academia,Likely,"Large language models (LMs) of code have recently shown tremendous promise in completing code and synthesizing code from natural language descriptions. However, the current state-of-the-art code LMs (e.g., Codex (Chen et al., 2021)) are not publicly available, leaving many questions about their model and data design decisions. We aim to fill in some of these blanks through a systematic evaluation of the largest existing models: Codex, GPT-J, GPT-Neo, GPT-NeoX-20B, and CodeParrot, across various programming languages. Although Codex itself is not open-source, we find that existing open-source models do achieve close results in some programming languages, although targeted mainly for natural language modeling. We further identify an important missing piece in the form of a large open-source model trained exclusively on a multi-lingual corpus of code. We release a new model, PolyCoder, with 2.7B parameters based on the GPT-2 architecture, which was trained on 249GB of code across 12 programming languages on a single machine. In the C programming language, PolyCoder outperforms all models including Codex. Our trained models are open-source and publicly available at this https URL, which enables future research and application in this area.",2023-11-26 03:25,Anonymous,,,United States of America,Carnegie Mellon University (CMU),,,,PolyCoder,,,
DeepNet,Language,Language modelling,"Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Dongdong Zhang, Furu Wei",SOTA Improvement,"""Remarkably, on a multilingual benchmark with 7,482 translation directions, our 200-layer model with 3.2B parameters significantly outperforms the 48-layer state-of-the-art model with 12B parameters by 5 BLEU points""",,https://arxiv.org/abs/2203.00555,91.00,"DeepNet: Scaling Transformers to 1,000 Layers",2022-03-01,Microsoft Research,Industry,3200000000.00,"""Remarkably, on a multilingual benchmark with 7,482 translation directions, our 200-layer model with 3.2B parameters significantly outperforms the 48-layer state-of-the-art model with 12B parameters by 5 BLEU points, which indicates a promising scaling direction""

EDIT 05/05/2022: The 12B model was presented in an earlier paper. This paper presents a 3.2B model",,,,,12000000000,""" The final data consists of 102 languages, 1932 directions, and
12B sentence pairs.""",,,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,United States of America,Microsoft Research,,,,DeepNet,,,
Statement Curriculum Learning,Language,Automated theorem proving,"Stanislas Polu, Jesse Michael Han, Kunhao Zheng, Mantas Baksys, Igor Babuschkin, Ilya Sutskever ",SOTA Improvement,"""by applying this expert iteration to a manually curated set
of problem statements, we achieve state-of-the-art on the miniF2F benchmark, automatically solving
multiple challenging problems drawn from high school olympiads.""",,https://arxiv.org/abs/2202.01344,47.00,Formal Mathematics Statement Curriculum Learning,2022-03-02,OpenAI,Industry,,,,,"Common Crawl, WebMath","300 billion tokens from Common Crawl
72 billion tokens (220 GB) of code from WebMath
25000 theorems from mathlib
327 math problems from competitions and textbooks

The model was also trained on its own self-generated proofs",275000000000,"Table on p12 gives WebMath dataset size in GB of code. Uncompressed code probably has a similar number of tokens per gigabyte as natural language text, on the order of 3e8 tokens per GB.",,,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,United States of America,OpenAI,,,,Statement Curriculum Learning,,,
GPT3-6.7B + muP,Language,,"Greg Yang, Edward J. Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ryder, Jakub Pachocki, Weizhu Chen, Jianfeng Gao",,,,https://web.archive.org/web/20221014063419/https://arxiv.org/pdf/2203.03466.pdf,76.00,Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer,2022-03-07,,,6700000000.00,,1.28e+22,,,,,,1.00,,,,,,,,,,,,,2023-12-05 04:33,Robi Rahman,GPT3-6.7B + muP,,,,,,,,,,
ViT-G (model soup),Vision,Image Classification,"Mitchell Wortsman, Gabriel Ilharco, Samir Yitzhak Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari S. Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, Ludwig Schmidt",SOTA Improvement,"""When fine-tuning large pre-trained models such as CLIP, ALIGN, and a ViT-G pre-trained on JFT, our soup recipe provides significant improvements over the best model in a hyperparameter sweep on ImageNet. The resulting ViT-G model, which attains 90.94% top-1 accuracy on ImageNet, achieved a new state of the art.""",,https://arxiv.org/abs/2203.05482v3,344.00,Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time,2022-03-10,"University of Washington,Columbia University,Google,Meta AI,Tel Aviv University",Industry - Academia Collaboration (Industry leaning),1843000000.00,This is from the original ViT-G paper,3.4e+21,"This is a fine-tuned version of ViT-G, which required 3.4e21 to train per PCD/Akronomicon. Fine-tuning compute is likely minor in comparision.",,,,,,,,,,,,,,,,Likely,"The conventional recipe for maximizing model accuracy is to (1) train multiple models with various hyperparameters and (2) pick the individual model which performs best on a held-out validation set, discarding the remainder. In this paper, we revisit the second step of this procedure in the context of fine-tuning large pre-trained models, where fine-tuned models often appear to lie in a single low error basin. We show that averaging the weights of multiple models fine-tuned with different hyperparameter configurations often improves accuracy and robustness. Unlike a conventional ensemble, we may average many models without incurring any additional inference or memory costs -- we call the results ""model soups."" When fine-tuning large pre-trained models such as CLIP, ALIGN, and a ViT-G pre-trained on JFT, our soup recipe provides significant improvements over the best model in a hyperparameter sweep on ImageNet. The resulting ViT-G model, which attains 90.94% top-1 accuracy on ImageNet, achieved a new state of the art. Furthermore, we show that the model soup approach extends to multiple image classification and natural language processing tasks, improves out-of-distribution performance, and improves zero-shot performance on new downstream tasks. Finally, we analytically relate the performance similarity of weight-averaging and logit-ensembling to flatness of the loss and confidence of the predictions, and validate this relation empirically. Code is available at this https URL.",2023-11-26 03:25,Anonymous,,,"United States of America,United States of America,Multinational,Multinational,Israel","University of Washington, Columbia University, Google, Meta AI, Tel Aviv University",,,,ViT-G (model soup),,,
"Segatron-XL large, M=384 + HCP",Language,,"He Bai, Tong Wang, Alessandro Sordoni, Peng Shi",SOTA Improvement,"""Empirically, this curriculum learning strategy consistently improves perplexity over various large, highly-performant state-of-the-art Transformer-based models on two datasets, WikiText-103 and ARXIV""",,https://arxiv.org/abs/2203.10692,5.00,Better Language Model with Hypernym Class Prediction,2022-03-21,"Microsoft Research,University of Waterloo",,257000000.00,,26500000000000000000.00,,WikiText-103,,,,167.02,,,,,,,,,,,,,2023-12-05 04:33,Robi Rahman,"""Segatron-XL large, M=384 + HCP""",,"United States of America,Canada","Microsoft Research, University of Waterloo",,,,"""Segatron-XL large, M=384 + HCP""",,,
Transformer Large + HCP,Language,,"He Bai, Tong Wang, Alessandro Sordoni, Peng Shi",,other model in this paper has better performance,,https://arxiv.org/abs/2203.10692,5.00,Better Language Model with Hypernym Class Prediction,2022-03-21,,,257000000.00,,6060000000000000000.00,,WikiText-103,,,,38.18,,,,,,,,,,,,,2023-12-05 04:33,Robi Rahman,Transformer Large + HCP,,,,,,,,,,
"Segatron -XL base, M=150 + HCP",Language,,,,,,,,,2022-03-21,,,,,,,,,,,18.64,,,,,,,,,,,Unverified,,2023-12-08 01:01,Robi Rahman,"""Segatron -XL base, M=150 + HCP""",1,,,,,,,,,
MemSizer,Language,,"Yizhe Zhang, Deng Cai",,,,https://web.archive.org/web/20220327055642/https://arxiv.org/pdf/2203.12644.pdf,1.00,Linearizing Transformer with Key-Value Memory,2022-03-23,,,357000000.00,,7300000000000000000.00,,,,,,,,,,,,,,,,,,,2023-12-05 04:33,Robi Rahman,MemSizer,,,,,,,,,,
BaGuaLu,Multimodal,"Language modelling,Image classification","Zixuan Ma, Jiaao He, Jiezhong Qiu, Huanqi Cao, Yuanwei Wang, Zhenbo Sun, Liyan Zheng, Haojie Wang, Shizhi Tang, Tianyu Zheng, Junyang Lin, Guanyu Feng, Zeqiang Huang, Jie Gao, Aohan Zeng, Jianwei Zhang, Runxin Zhong, Tianhui Shi, Sha Liu, Weimin Zheng, Jie Tang, Hongxia Yang, Xin Liu, Jidong Zhai, Wenguang Chen",,,,https://dl.acm.org/doi/abs/10.1145/3503221.3508417,23.00,BaGuaLu: Targeting Brain Scale Pretrained Models with over 37 Million Cores,2022-03-28,"Tsinghua University,Zhejiang Lab,Beijing Academy of Artificial Intelligence,Alibaba",Industry - Academia Collaboration (Industry leaning),173900000000000.00,"Table 3, MoDa-174T has 173.9 trillion parameters",,"The 174T parameter system was not trained, the paper simply demonstrated they were able to train it for a few iterations.

Calculations below give some rough estimates of FLOP for full training.

From Table 5, sustained performance was 230 PFLOPS. Assuming they ran a 2-month training run, C=230e12*60**2*24*8=1.6e20 FLOP.

Using C=6ND with D=17.5B tokens and N=173.9T parameters, we get C=1.83e+25 FLOP. However, it's an MoE with 96e3 experts.

If we assume that scaling was perfect, then C=1.8e25/93e3=1.9e20 FLOP.

If we assume that they got similar utilisation to the Switch transformer, i.e. 0.10% of params active at a time, then C=1.8e25 * 0.001=1.8e22 FLOP.",M6-Corpus,"""The data are collected from different sources, including encyclopedias, ecommerce platforms, and other crawled web pages. The detailed statistics of the final processed dataset are reported in Table 4, where ""#Img"" refers to the number of distinct images, ""#Tok"" to the number of distinct tokens... after
the images transformed to features, the final product was a dataset of size 16 TB.""",13200000000,"17.5B tokens (in English, this is approximately 13.1B words, but the conversion may be different in Chinese) and 60.5M images.",,,,,,,,Self-supervised learning,,,Industry,,,2023-11-21 01:39,Robi Rahman,,,"China,China,China,China","Tsinghua University, Zhejiang Lab, Beijing Academy of Artificial Intelligence, Alibaba",,,,,,,
Chinchilla,Language,Language modelling,"Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals and Laurent Sifre",SOTA Improvement,"Proposes new scaling law, with good empirical results",,https://arxiv.org/abs/2203.15556,793.00,Training Compute-Optimal Large Language Models,2022-03-29,DeepMind,Industry,70000000000.00,"""We test this hypothesis by training a predicted compute-optimal model, \chinchilla, that uses the same compute budget as \gopher but with 70B parameters and 4× more more data. \chinchilla uniformly and significantly outperforms \Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks.""",5.76e+23,"""Both Chinchilla and Gopher have been trained for the same number of FLOPs but differ in the size of the model and the number of training tokens.""

We see the number of flops in table 3",,,1050000000000,"Table 1 shows Chinchilla was training on 1.4 trillion tokens

1 token ~ 0.75 words",1.00,,,,,Google TPU v4,Google TPU v4,Self-supervised learning,$753491.58,,Industry,,"We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over \nummodels language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, \chinchilla, that uses the same compute budget as \gopher but with 70B parameters and 4× more more data. \chinchilla uniformly and significantly outperforms \Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that \chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, \chinchilla reaches a state-of-the-art average accuracy of 67.5\% on the MMLU benchmark, greater than a 7\% improvement over \gopher.",2023-12-05 04:33,Robi Rahman,Chinchilla,,United Kingdom of Great Britain and Northern Ireland,DeepMind,,,,Chinchilla,,,Chinchilla
NoPos,Language,,"Adi Haviv, Ori Ram, Ofir Press, Peter Izsak, Omer Levy",,,,https://arxiv.org/abs/2203.16634,41.00,Transformer Language Models without Positional Encodings Still Learn Positional Information,2022-03-30,,,1300000000.00,,161000000000000000000.00,,The Pile (Subset),,,,199.92,,,,,,,,,,,,,2023-12-05 04:33,Robi Rahman,NoPos,,,,,,,,,,
Monarch-GPT-2-Medium,Language,,"Tri Dao, Beidi Chen, Nimit Sohoni, Arjun Desai, Michael Poli, Jessica Grogan, Alexander Liu, Aniruddh Rao, Atri Rudra, Christopher Ré",,,,https://arxiv.org/pdf/2204.00595,42.00,Monarch: Expressive Structured Matrices for Efficient and Accurate Training,2022-04-01,,Academia,165000000.00,,435999999999999930000.00,,,,,,110.00,,,,,,,,,,,,,2023-12-05 04:33,Robi Rahman,Monarch-GPT-2-Medium,,,,,,,,,,
Monarch-GPT-2-Small,Language,,,,,,,,,2022-04-01,,,,,,,,,,,110.00,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,Monarch-GPT-2-Small,1,,,,,,,,,
PaLM (540B),Language,Language modelling,"Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev,, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta ,Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, Noah Fiedel",SOTA Improvement,"Demonstrates continued benefits of scaling, as well as discontinuous improvements in performance",,https://arxiv.org/abs/2204.02311,2612.00,PaLM: Scaling Language Modeling with Pathways,2022-04-04,Google Research,Industry,540350000000.00,"""To further our understanding of the
impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer
language model, which we call Pathways Language Model (PaLM).""",2.5272e+24,"See Table 20: https://storage.googleapis.com/pathways-language-model/PaLM-paper.pdf

6144 TPUv4 for 1200 hours + 3072 TPUv4 for 336 hours.
Equivalent to 6144 TPUv4 for 1368 hours.

46.2% model FLOPs utilization",,,585000000000,"""The PaLM pretraining dataset consists of a high-quality corpus of 780 billion tokens that represent a wide range of natural language use cases.""

1 token ~ 0.75 words",,,,1368.0,"6144 TPUv4 for 1200 hours + 3072 TPUv4 for 336 hours.

Equivalent to 6144 TPUv4 for 1368 hours.",Google TPU v4,Google TPU v4,Self-supervised learning,$3232806.53,"Training compute and utilization rate exclude rematerialization FLOP, but cost should account for rematerialization.",Industry,,"Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.",2023-12-08 09:53,Robi Rahman,,,Multinational,Google Research,,,,PaLM (540B),6144,0.4620,PaLM (540B)
DALL·E 2,Drawing,Text-to-image,"Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen","SOTA Improvement,Highly cited",,,https://cdn.openai.com/papers/dall-e-2.pdf,2857.00,Hierarchical Text-Conditional Image Generation with CLIP Latents,2022-04-06,OpenAI,Industry,3500000000.00,"""Our decoder architecture is the 3.5 billion parameter GLIDE model""",,"Decoder architecture is similar to Imagen (1.46E+22), but trained on 1.6e9 datapoints (Table 3) rather than Imagen's 5.1e9 datapoints.

DALL-E 2 uses two models as priors. I estimate the prior model's FLOP as 6*N*D = 6 * 1e9 * 4096 * 1e6 = 2.5e19 FLOP. However, this seems low compared to CLIP.

So it may be possible to estimate DALL-E 2's compute by analogy to Imagen, but there is a lot of uncertainty and more research would be needed.","CLIP, DALL-E",,650000000,"""When training the encoder, we sample from the CLIP [39] and DALL-E [40] datasets (approximately 650M images in total) with equal probability""",,,,,,,,Self-supervised learning,,,Industry,Confident,,2023-12-05 04:33,Robi Rahman,,,United States of America,OpenAI,,,,DALL·E 2,,,
Stable Diffusion (LDM-KL-8-G),Drawing,Text-to-image,"Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer","Significant use,Highly cited",,,https://arxiv.org/abs/2112.10752,3962.00,High-Resolution Image Synthesis with Latent Diffusion Models,2022-04-13,"Runway,Ludwig Maximilian University",Industry,1450000000.00,See Table 1,5.0000000000000004e+22,"""I get 5e22 FLOP. 150k hours on A100 [1] gives 150*10^3 hours * 3600 seconds/hour * 3.12E+14 peak performance of A100 * 0.33 utilisation = 5e22  FLOP""

[1] https://twitter.com/EMostaque/status/1563870674111832066",LAION-400M,"Depends on the specific task; see sec 4

""we train a 1.45B parameter
KL-regularized LDM conditioned on language prompts on
LAION-400M""",400000000,,,,,,,,,Self-supervised learning,,,,,"By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at this https URL .",2023-12-05 04:33,Robi Rahman,,,"United States of America,Germany","Runway, Ludwig Maximilian University",,,,Stable Diffusion (LDM-KL-8-G),,,
Sparse all-MLP,Language,,"Ping Yu, Mikel Artexte, Myle Ott, Sam Shleifer, Hongyu Gong, Ves Stoyanov, Xian Li",SOTA Improvement,"Abstract:
""Our model also outperforms
the RoBERTa-Large model on several English tasks of the GLUE benchmark by 0.3% on average while handling 99 more languages.""",,https://arxiv.org/abs/2203.06850,5.00,Efficient Language Modeling with Sparse all-MLP,2022-04-14,Meta AI,Industry - Academia Collaboration (Industry leaning),9400000000.00,"Table 2: ""In Section 4.4, we run our large model (9.41B parameters)""",60770304000000000000.00,"112 hours on 32 V100 GPUs
assumed 0.33 util rate
32*112*60*60*0.3*1.57E+13
",RoBERTa dataset,,75000000000,100B tokens (Table 2) so 75B words.,,,,112.0,,,,Self-supervised learning,$320.00,,Industry,,"All-MLP architectures have attracted increasing interest as an alternative to attention-based models. In NLP, recent work like gMLP shows that all-MLPs can match Transformers in language modeling, but still lag behind in downstream tasks. In this work, we analyze the limitations of MLPs in expressiveness, and propose sparsely activated MLPs with mixture-of-experts (MoEs) in both feature and input (token) dimensions. Such sparse all-MLPs significantly increase model capacity and expressiveness while keeping the compute constant. We address critical challenges in incorporating conditional computation with two routing strategies. The proposed sparse all-MLP improves language modeling perplexity and obtains up to 2× improvement in training efficiency compared to both Transformer-based MoEs (GShard, Switch Transformer, Base Layers and HASH Layers) as well as dense Transformers and all-MLPs. Finally, we evaluate its zero-shot in-context learning performance on six downstream tasks, and find that it surpasses Transformer-based MoEs and dense Transformers.",2023-11-26 03:25,Robi Rahman,,,Multinational,Meta AI,,,,Sparse all-MLP,,,
LaMemo,Language,,"Haozhe Ji, Rongsheng Zhang, Zhenyu Yang, Zhipeng Hu, Minlie Huang",,,,https://web.archive.org/web/20220418055451/https://arxiv.org/pdf/2204.07341.pdf,2.00,LaMemo: Language Modeling with Look-Ahead Memory,2022-04-15,,,151000000.00,,,,,,,,79.53,,,,,,,,,,,,,2023-11-21 01:39,Robi Rahman,LaMemo,,,,,,,,,,
Flamingo,Multimodal,"Visual question answering,Image captioning","Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, Karen Simonyan",SOTA Improvement,"""For tasks lying anywhere on this spectrum, a single Flamingo model can achieve a new state of the art with few-shot learning, simply by prompting the model with task-specific examples. On numerous benchmarks, Flamingo outperforms models fine-tuned on thousands of times more task-specific data.""",,https://arxiv.org/abs/2204.14198,1111.00,Flamingo: a Visual Language Model for Few-Shot Learning,2022-04-29,DeepMind,Industry,80000000000.00,"""We obtain three models, Flamingo-3B, Flamingo-9B and Flamingo-80B""",2.7e+23,"1536 TPU v4 chips for 15 days. Assuming 50% utilization:
C = 1536 TPU * 275*10^12 FLOP/s/TPU * 15 day * 86400 s/day * 0.50 = 2.7*10^23 FLOP

All training and evaluation
was performed on TPUv4 instances. The largest model containing 80 billion parameters is trained on
QUSV chips for 15 days and sharded across 16 devices.

All trained parameters and optimizer accumulators are stored
and updated in float32; all activations and gradients are computed in bfloat16 after downcasting
of parameters from float32 to bfloat16","MultiModal MassiveWeb, LTIP, VTP, ALIGN",,,"Flamingo was trained on a mixture of web-scraped datasets:
43M pages of text with interleaved images (MultiModal MassiveWeb dataset)
312M image-text pairs (LTIP dataset)
27M video-text pairs (VTP dataset)
1.8B image-alt text pairs (ALIGN dataset)

Training dataset size is at least 2.1 billion.",,,,360.0,1536 TPU v4 chips for 15 days,Google TPU v4,Google TPU v4,Supervised,,,Industry,Likely,"Building models that can be rapidly adapted to novel tasks using only a handful of annotated examples is an open challenge for multimodal machine learning research. We introduce Flamingo, a family of Visual Language Models (VLM) with this ability. We propose key architectural innovations to: (i) bridge powerful pretrained vision-only and language-only models, (ii) handle sequences of arbitrarily interleaved visual and textual data, and (iii) seamlessly ingest images or videos as inputs. Thanks to their flexibility, Flamingo models can be trained on large-scale multimodal web corpora containing arbitrarily interleaved text and images, which is key to endow them with in-context few-shot learning capabilities. We perform a thorough evaluation of our models, exploring and measuring their ability to rapidly adapt to a variety of image and video tasks. These include open-ended tasks such as visual question-answering, where the model is prompted with a question which it has to answer; captioning tasks, which evaluate the ability to describe a scene or an event; and close-ended tasks such as multiple-choice visual question-answering. For tasks lying anywhere on this spectrum, a single Flamingo model can achieve a new state of the art with few-shot learning, simply by prompting the model with task-specific examples. On numerous benchmarks, Flamingo outperforms models fine-tuned on thousands of times more task-specific data.",2023-12-05 04:33,Anonymous,,,United Kingdom of Great Britain and Northern Ireland,DeepMind,,,,Flamingo,,,
OPT-175B,Language,Language modelling,"Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer",Significant use,https://ai.meta.com/blog/opt-175b-large-language-model-applications/,Fully open-source,https://ai.facebook.com/blog/democratizing-access-to-large-scale-language-models-with-opt-175b/,1443.00,OPT: Open Pre-trained Transformer Language Models,2022-05-02,Meta AI,Industry,175000000000.00,"""In line with Meta AI’s commitment to open science, we are sharing Open Pretrained Transformer (OPT-175B), a language model with 175 billion parameters trained on publicly available data sets""",4.3e+23,"https://github.com/facebookresearch/metaseq/blob/main/projects/OPT/chronicles/final_update.md

""As of yesterday, at 12:46pm PST on January 6, our 175B model finally completed its training run on 300B tokens. This required ~4.30E+23 FLOPs of compute""",,,135000000000,"""The training data contains 180B tokens corresponding to 800 GB of data""

1 token ~ 0.75 words",1.67,,,793.5,"4.3*10^23 FLOP / (147 TFLOPS) = 813000 A100-hours
https://www.wolframalpha.com/input?i=4.3*10%5E23+FLOP+%2F+%28147+TFLOPS%29

""As of yesterday, at 12:46pm PST on January 6, our 175B model finally completed its training run on 300B tokens. This required ~4.30E+23 FLOPs of compute, or roughly ~33 days of continuous training on 1024 80GB A100s (assuming no hardware issues, no numerical instabilities, etc.).""",NVIDIA A100 SXM4 80 GB,NVIDIA A100 SXM4 80 GB,Self-supervised learning,$1654082.50,,Industry,Confident,"Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3,1 while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models",2023-12-05 04:33,Robi Rahman,OPT-175B,0,Multinational,Meta AI,,,,OPT-175B,1024,0.4712,OPT-175B
Jurassic-X,Language,,"Ehud Karpas, Omri Abend, Yonatan Belinkov, Barak Lenz, Opher Lieber, Nir Ratner, Yoav Shoham, Hofit Bata, Yoav Levine, Kevin Leyton-Brown, Dor Muhlgay, Noam Rozen, Erez Schwartz, Gal Shachaf, Shai Shalev-Shwartz, Amnon Shashua, Moshe Tenenholtz",,,,https://www.ai21.com/blog/jurassic-x-crossing-the-neuro-symbolic-chasm-with-the-mrkl-system,33.00,"MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning",2022-05-03,AI21 Labs,Industry,7000000000.00,,,,,,,,,,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,Israel,AI21 Labs,,,,,,,
ASE,Robotics,,"Xue Bin Peng, Yunrong Guo, Lina Halper, Sergey Levine, Sanja Fidler",,,,https://arxiv.org/abs/2205.01906,56.00,ASE: Large-Scale Reusable Adversarial Skill Embeddings for Physically Simulated Characters,2022-05-05,"NVIDIA,UC Berkeley",Industry - Academia Collaboration,,,6104160000000000000.00,"Training was done using the Isaac Gym simulator on an NVIDIA V100 GPU. The model was trained on over 10 billion samples, which equates to 10 years of simulated experience time. Training took around 10 days on a single GPU.

14.13 TFLOP/s * 10 days * 86400 s/day * 0.50 utilization = 6.1e+18 FLOP",,"The model is trained on a dataset of 187 motion clips, about 30 minutes of human motion capture data, depicting locomotion and sword wielding motions.",,,,,,240.0,Training took around 10 days on a single GPU.,NVIDIA Tesla V100 PCIe 16 GB,NVIDIA Tesla V100 PCIe 16 GB,Reinforcement learning,,,Industry,Likely,"The incredible feats of athleticism demonstrated by humans are made possible in part by a vast repertoire of general-purpose motor skills, acquired through years of practice and experience. These skills not only enable humans to perform complex tasks, but also provide powerful priors for guiding their behaviors when learning new tasks. This is in stark contrast to what is common practice in physics-based character animation, where control policies are most typically trained from scratch for each task. In this work, we present a large-scale data-driven framework for learning versatile and reusable skill embeddings for physically simulated characters. Our approach combines techniques from adversarial imitation learning and unsupervised reinforcement learning to develop skill embeddings that produce life-like behaviors, while also providing an easy to control representation for use on new downstream tasks. Our models can be trained using large datasets of unstructured motion clips, without requiring any task-specific annotation or segmentation of the motion data. By leveraging a massively parallel GPU-based simulator, we are able to train skill embeddings using over a decade of simulated experiences, enabling our model to learn a rich and versatile repertoire of skills. We show that a single pre-trained model can be effectively applied to perform a diverse set of new tasks. Our system also allows users to specify tasks through simple reward functions, and the skill embedding then enables the character to automatically synthesize complex and naturalistic strategies in order to achieve the task objectives.",2023-11-23 06:53,Anonymous,,,"United States of America,United States of America","NVIDIA, UC Berkeley",,,,,,,
UL2,Language,,"Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Neil Houlsby, Donald Metzler",SOTA Improvement,"""by scaling our model up to 20B parameters, we achieve SOTA
performance on 50 well-established supervised NLP tasks""",,https://arxiv.org/abs/2205.05131v1,125.00,Unifying Language Learning Paradigms,2022-05-10,"Google Research,Google Brain",Industry,20000000000.00,Taken from Directory of LLMs,1.2e+23,"Trained on 1T tokens
20B * 1T * 6 = 1.2e23 ",C4,,,,,,,,,,,,,,Industry,,,2023-12-14 08:33,Robi Rahman,,,"Multinational,Multinational","Google Research, Google Brain",,,,UL2,,,
Gato,Multimodal,,"Scott Reed, Konrad Żołna, Emilio Parisotto, Sergio Gómez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Giménez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom Eccles, Jake Bruce, Ali Razavi, Ashley Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell, Oriol Vinyals, Mahyar Bordbar, Nando de Freitas",SOTA Improvement,"SOTA at Meta-World MT50 tasks (96.6%) page 14, section 5.5",,https://arxiv.org/abs/2205.06175,414.00,A Generalist Agent,2022-05-12,DeepMind,Industry,1180000000.00,"""This section focuses on in-simulation evaluation.
Figure 10 compares the full 1.18B parameter Gato"" p.10",5.44e+21,256 (16x16x) TPUv3 chips x 123e12 FLOPS/chip x 4 days x 86400 seconds/day * 0.5 utilization = 5.44e21 FLOPs,,,,,,,,,,,,,$6781.08,,Industry,,"Inspired by progress in large-scale language modeling, we apply a similar approach towards building a single generalist agent beyond the realm of text outputs. The agent, which we refer to as Gato, works as a multi-modal, multi-task, multi-embodiment generalist policy. The same network with the same weights can play Atari, caption images, chat, stack blocks with a real robot arm and much more, deciding based on its context whether to output text, joint torques, button presses, or other tokens. In this report we describe the model and the data, and document the current capabilities of Gato.",2023-12-05 04:33,Robi Rahman,,,United Kingdom of Great Britain and Northern Ireland,DeepMind,,,,Gato,,,
SimCSE,Language,Semantic embedding,"Tianyu Gao, Xingcheng Yao, Danqi Chen","SOTA Improvement,Highly cited",,,https://arxiv.org/abs/2104.08821,1722.00,SimCSE: Simple Contrastive Learning of Sentence Embeddings,2022-05-18,"Princeton University,Tsinghua University",Academia,,,,,,"""Training details. We start from pre-trained checkpoints of BERT (Devlin et al., 2019) (uncased) or RoBERTa (Liu et al., 2019) (cased) and take the [CLS] representation as the sentence embedding (see §6.3 for comparison between different pooling methods). We train unsupervised SimCSE on 106 randomly sampled sentences from English Wikipedia, and train supervised SimCSE on the combination of MNLI and SNLI datasets (314k). More training details can be found in Appendix A""",,,,,,,,,,,,,Academia,Confident,"This paper presents SimCSE, a simple contrastive learning framework that greatly advances state-of-the-art sentence embeddings. We first describe an unsupervised approach, which takes an input sentence and predicts itself in a contrastive objective, with only standard dropout used as noise. This simple method works surprisingly well, performing on par with previous supervised counterparts. We find that dropout acts as minimal data augmentation, and removing it leads to a representation collapse. Then, we propose a supervised approach, which incorporates annotated pairs from natural language inference datasets into our contrastive learning framework by using ""entailment"" pairs as positives and ""contradiction"" pairs as hard negatives. We evaluate SimCSE on standard semantic textual similarity (STS) tasks, and our unsupervised and supervised models using BERT base achieve an average of 76.3% and 81.6% Spearman's correlation respectively, a 4.2% and 2.2% improvement compared to the previous best results. We also show -- both theoretically and empirically -- that the contrastive learning objective regularizes pre-trained embeddings' anisotropic space to be more uniform, and it better aligns positive pairs when supervised signals are available.",2023-12-05 04:33,Anonymous,,,"United States of America,China","Princeton University, Tsinghua University",,,,SimCSE,,,
LSTM+GraB,Language,,"Yucheng Lu, Wentao Guo, Christopher De Sa",,,,https://arxiv.org/pdf/2205.10733,10.00,GraB: Finding Provably Better Data Permutations than Random Reshuffling,2022-05-22,,,,,,,,,,,50.00,,,,,,,,,,,,,2023-12-05 04:33,Robi Rahman,LSTM+GraB,,,,,,,,,,
Imagen,Drawing,Text-to-image,"Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li","Significant use,SOTA improvement",,,https://imagen.research.google/,2160.00,Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding,2022-05-23,Google Brain,Industry,3000000000.00,"2B 64x64 generation model, 600M 64->256 super-resolution model, 400M 256->1024 super-resolution model",1.4600000000000002e+22,"256 TPU v4 chips for 64x64, for 4 days
128 TPU v4 chips for 64->256, for 2 days
128 TPU v4 chips for 256->1024, for 2 days

256 TPUs * 275 teraFLOPS/TPU * 4 days + 2 * (128 TPUs * 275 teraFLOPS/TPU * 2 days) * 40% utilization = 1.46e+22 FLOP","LAION-400M, other",,860000000,"""We train on a combination of internal datasets, with ≈ 460M
image-text pairs, and the publicly available Laion dataset [61], with ≈ 400M image-text pairs.""",,,,,,,,Self-supervised learning,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,Multinational,Google Brain,,,,Imagen,,,
TRIMELMext (247M),Language,,"Zexuan Zhong, Tao Lei, Danqi Chen",,,,https://arxiv.org/abs/2205.12674,58.00,Training Language Models with Memory Augmentation,2022-05-25,,Industry - Academia Collaboration,247000000.00,,31200000000000000000.00,,WikiText-103,,,,204.72,,,,,,,,,,,,,2023-12-05 04:33,Robi Rahman,TRIMELMext (247M),,,,,,,,,,
TRIMELMext (7M),Language,,,,,,,,,2022-05-25,,,,,,,,,,,47.72,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,TRIMELMext (7M),1,,,,,,,,,
TRIMELMlong (150M),Language,,,,,,,,,2022-05-25,,,,,,,,,,,139.81,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,TRIMELMlong (150M),1,,,,,,,,,
Tranception,Language,Proteins,"Pascal Notin, Mafalda Dias, Jonathan Frazer, Javier Marchena-Hurtado, Aidan Gomez, Debora S. Marks, Yarin Gal",SOTA Improvement,"""We introduce Tranception, a novel transformer architecture leveraging autoregressive predictions and retrieval of homologous sequences at inference to achieve state-of-the-art fitness prediction performance""",,https://arxiv.org/abs/2205.13760,77.00,"Tranception: protein fitness prediction with autoregressive
transformers and inference-time retrieval",2022-05-27,"University of Oxford,Harvard University,Cohere,Harvard Medical School",Academia,700000000.00,,7.240000000000001e+21,"Trained using 64 A100 GPUs for two weeks.
64 * 312 teraFLOP/s * 14 days * 24 hours/day * 3600 seconds/hour * 0.3 utilization (assumption)
= 7.24e21",UniRef100,https://www.uniprot.org/help/uniref,,249 million sequences with a median length of 314 tokens. Not sure about mean length.,,,,336.0,Two weeks,NVIDIA A100,NVIDIA A100,Self-supervised learning,,,,Likely,"The ability to accurately model the fitness landscape of protein sequences is critical to a wide range of applications, from quantifying the effects of human variants on disease likelihood, to predicting immune-escape mutations in viruses and designing novel biotherapeutic proteins. Deep generative models of protein sequences trained on multiple sequence alignments have been the most successful approaches so far to address these tasks. The performance of these methods is however contingent on the availability of sufficiently deep and diverse alignments for reliable training. Their potential scope is thus limited by the fact many protein families are hard, if not impossible, to align. Large language models trained on massive quantities of non-aligned protein sequences from diverse families address these problems and show potential to eventually bridge the performance gap. We introduce Tranception, a novel transformer architecture leveraging autoregressive predictions and retrieval of homologous sequences at inference to achieve state-of-the-art fitness prediction performance. Given its markedly higher performance on multiple mutants, robustness to shallow alignments and ability to score indels, our approach offers significant gain of scope over existing approaches. To enable more rigorous model testing across a broader range of protein families, we develop ProteinGym – an extensive set of multiplexed assays of variant effects, substantially increasing both the number and diversity of assays compared to existing benchmarks.",2023-12-05 04:33,Anonymous,,,"United Kingdom of Great Britain and Northern Ireland,United States of America,Canada,United States of America","University of Oxford, Harvard University, Cohere, Harvard Medical School",,,,Tranception,,,
CogVideo,Multimodal,Video generation,"Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, Jie Tang",Historical significance,The world's largest and first opensource large-scale pre-trained text-to-video model.,,https://arxiv.org/abs/2205.15868,160.00,CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers,2022-05-29,"Tsinghua University,Beijing Academy of Artificial Intelligence",Academia,9000000000.00,,,,,,,,,,,,,,,,,,,Likely,"Large-scale pretrained transformers have created milestones in text (GPT-3) and text-to-image (DALL-E and CogView) generation. Its application to video generation is still facing many challenges: The potential huge computation cost makes the training from scratch unaffordable; The scarcity and weak relevance of text-video datasets hinder the model understanding complex movement semantics. In this work, we present 9B-parameter transformer CogVideo, trained by inheriting a pretrained text-to-image model, CogView2. We also propose multi-frame-rate hierarchical training strategy to better align text and video clips. As (probably) the first open-source large-scale pretrained text-to-video model, CogVideo outperforms all publicly available models at a large margin in machine and human evaluations.",2023-12-05 04:33,Anonymous,,,"China,China","Tsinghua University, Beijing Academy of Artificial Intelligence",,,,CogVideo,,,
B2T connection (16L),Language,,"Sho Takase, Shun Kiyono, Sosuke Kobayashi, Jun Suzuki",,,,https://web.archive.org/web/20220602013934/https://arxiv.org/pdf/2206.00330.pdf,4.00,On Layer Normalizations and Residual Connections in Transformers,2022-06-01,,,247000000.00,,28000000000000000000.00,,,,,,150.00,,,,,,,,,,,,,2023-11-23 06:53,Robi Rahman,B2T connection (16L),,,,,,,,,,
Diffusion-GAN,Drawing,Image generation,"Zhendong Wang, Huangjie Zheng, Pengcheng He, Weizhu Chen, Mingyuan Zhou",SOTA Improvement,"""We demonstrate the advantages of Diffusion-GAN over strong GAN
baselines on various datasets, showing that it can produce more realistic images with higher stability and data efficiency than state-of-the-art GANs.""",,https://arxiv.org/abs/2206.02262v4,76.00,Diffusion-GAN: Training GANs with Diffusion,2022-06-05,"UT Austin,Microsoft",Industry - Academia Collaboration (Industry leaning),,,,,,"They experimented with the following datasets: ""CIFAR-10 (Krizhevsky, 2009), STL-10 (Coates et al., 2011), LSUN-Bedroom (Yu et al., 2015), LSUN-Church
(Yu et al., 2015), AFHQ(Cat/Dog/Wild) (Choi et al., 2020), and FFHQ (Karras et al., 2019)""",,,,,,,,NVIDIA V100,NVIDIA V100,,,,,Likely,"Generative adversarial networks (GANs) are challenging to train stably, and a promising remedy of injecting instance noise into the discriminator input has not been very effective in practice. In this paper, we propose Diffusion-GAN, a novel GAN framework that leverages a forward diffusion chain to generate Gaussianmixture distributed instance noise. Diffusion-GAN consists of three components, including an adaptive diffusion process, a diffusion timestep-dependent discriminator, and a generator. Both the observed and generated data are diffused by the same adaptive diffusion process. At each diffusion timestep, there is a different noise-to-data ratio and the timestep-dependent discriminator learns to distinguish the diffused real data from the diffused generated data. The generator learns from the discriminator’s feedback by backpropagating through the forward diffusion chain, whose length is adaptively adjusted to balance the noise and data levels. We theoretically show that the discriminator’s timestep-dependent strategy gives consistent and helpful guidance to the generator, enabling it to match the true data distribution. We demonstrate the advantages of Diffusion-GAN over strong GAN baselines on various datasets, showing that it can produce more realistic images with higher stability and data efficiency than state-of-the-art GANs.",2023-12-05 04:33,Anonymous,,,"United States of America,Multinational","UT Austin, Microsoft",,,,Diffusion-GAN,,,
LiMoE,Multimodal,Image classification,"Basil Mustafa, Carlos Riquelme, Joan Puigcerver, Rodolphe Jenatton, Neil Houlsby",,"
",,https://arxiv.org/abs/2206.02770,65.00,Multimodal Contrastive Learning with LIMoE: the Language-Image Mixture of Experts,2022-06-06,Google,Industry,5600000000.00,"Section 1: ""We scale this up to a large 5.6B parameter LIMoE-H/14""",1.8e+22,"Section 3.2: ""The model contains 5.6B parameters in total, but only applies 675M parameters per token""

From Section A.3, ""batch size 21502 with resolution 288 and text sequence length16"". ""The model was trained for 700k steps pre-cooldown. There was one cooldown of length 125k steps
from the final step, and 3 of length 40k steps starting from step 650k"". Patch size 14 for images.

Assume C = 6*N*D. 
C = 6*675e6*21.5e3*1e6*(16+(288/14)**2)/2 = 1.8e22

This is broadly consistent with ViT-H/14's compute",,,7600000000,"Section 3: ""Training data. By default, all models are trained on paired image-text data used in [16], consisting of 3.6B images and alt-texts scraped from the web. For large LIMoE-H/14 experiment, we also co-train with JFT-4B [17]. """,,,,,,,,,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,Multinational,Google,,,,,,,
DITTO,Language,,"Jin Xu, Xiaojiang Liu, Jianhao Yan, Deng Cai, Huayang Li, Jian Li",SOTA Improvement,"Achieves SOTA on CNN/DailyMail by fine-tuning and improving on BART-large, which is SOTA",,https://arxiv.org/abs/2206.02369,24.00,Learning to Break the Loop: Analyzing and Mitigating Repetitions for Neural Text Generation,2022-06-06,"Tsinghua University,Apple,Westlake University,Chinese University of Hong Kong",,750000000.00,,11000000000000000000.00,,,,,,7.16,,,,,,,,,,,,,2023-12-05 04:33,Robi Rahman,DITTO,,"China,United States of America,China,Hong Kong","Tsinghua University, Apple, Westlake University, Chinese University of Hong Kong",,,,DITTO,,,
MetaLM,Multimodal,,"Yaru Hao, Haoyu Song, Li Dong, Shaohan Huang, Zewen Chi, Wenhui Wang, Shuming Ma, Furu Wei",SOTA Improvement,"Abstract: ""Experimental results across various language-only and vision-language benchmarks show that our model outperforms or is competitive with specialized models on finetuning, zero-shot generalization, and few-shot learning.""",,https://arxiv.org/abs/2206.06336v1,57.00,Language Models are General-Purpose Interfaces,2022-06-13,Microsoft Research,Industry,,,,,,,,,,,,,,,,Self-supervised learning,,,Industry,,,2023-12-05 04:33,Robi Rahman,,,United States of America,Microsoft Research,,,,MetaLM,,,
EGRU (WT2),Language,,"Anand Subramoney, Khaleelulla Khan Nazeer, Mark Schöne, Christian Mayr, David Kappel",,SOTA for recurrent networks but not in general,,https://arxiv.org/pdf/2206.06178v3.pdf,3.00,Efficient recurrent architectures through activity sparsity and sparse back-propagation through time,2022-06-13,,,74000000.00,,2310000000000000000.00,,,,,,2500.00,,,,,,,,,,,,,2023-12-05 04:33,Robi Rahman,EGRU (WT2),,,,,,,,,,
EGRU (PTB),Language,,,,,,,,,2022-06-13,,,,,,,,,,,2500.00,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,EGRU (PTB),1,,,,,,,,,
CoCa,Vision,Image Classification,"Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, Yonghui Wu",SOTA Improvement,"""Notably on ImageNet classification, CoCa obtains 86.3% zero-shot top-1 accuracy, 90.6% with a frozen encoder and learned classification head, and new state-of-the-art 91.0% top-1 accuracy on ImageNet with a finetuned encoder.""",,https://arxiv.org/abs/2205.01917v2,632.00,CoCa: Contrastive Captioners are Image-Text Foundation Models,2022-06-14,Google,Industry,2100000000.00,"""Our largest CoCa model (""CoCa"" in short) follows the ViT-giant setup in [21] with 1B-parameters in the image encoder and 2.1B-parameters altogether with the text decoder""",7.3e+22,"""Pretraining CoCa takes about 5 days on 2,048 CloudTPUv4 chips""

275 teraFLOP/s * 2048 * 5 * 24 * 3600 * 0.3 (assumed utilization) = 7.3e22","JFT-3B, ALIGN","""CoCa is pretrained from scratch in a single stage on both webscale alt-text data and annotated images by treating all labels simply as texts. We use the JFT-3B dataset [21] with label names as the paired texts, and the ALIGN dataset [13] with noisy alt-texts.""",4800000000,"JFT is 3 billion captioned images, ALIGN is 1.8 billion captioned images",7.50,,,120.0,5 days,Google TPU v4,Google TPU v4,Self-supervised learning,,,,Likely,"Exploring large-scale pretrained foundation models is of significant interest in computer vision because these models can be quickly transferred to many downstream tasks. This paper presents Contrastive Captioner (CoCa), a minimalist design to pretrain an image-text encoder-decoder foundation model jointly with contrastive loss and captioning loss, thereby subsuming model capabilities from contrastive approaches like CLIP and generative methods like SimVLM. In contrast to standard encoder-decoder transformers where all decoder layers attend to encoder outputs, CoCa omits cross-attention in the first half of decoder layers to encode unimodal text representations, and cascades the remaining decoder layers which cross-attend to the image encoder for multimodal image-text representations. We apply a contrastive loss between unimodal image and text embeddings, in addition to a captioning loss on the multimodal decoder outputs which predicts text tokens autoregressively. By sharing the same computational graph, the two training objectives are computed efficiently with minimal overhead. CoCa is pretrained end-to-end and from scratch on both web-scale alt-text data and annotated images by treating all labels simply as text, seamlessly unifying natural language supervision for representation learning. Empirically, CoCa achieves state-of-the-art performance with zero-shot transfer or minimal task-specific adaptation on a broad range of downstream tasks, spanning visual recognition (ImageNet, Kinetics-400/600/700, Moments-in-Time), crossmodal retrieval (MSCOCO, Flickr30K, MSR-VTT), multimodal understanding (VQA, SNLI-VE, NLVR2), and image captioning (MSCOCO, NoCaps). Notably on ImageNet classification, CoCa obtains 86.3% zero-shot top-1 accuracy, 90.6% with a frozen encoder and learned classification head, and new state-of-the-art 91.0% top-1 accuracy on ImageNet with a finetuned encoder.",2023-12-05 04:33,Anonymous,,,Multinational,Google,,,,CoCa,,,
OPT-2.7B (finetuned on PTB),Language,,,,,,,,,2022-06-21,,,,,,,,,,,1.67,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,OPT-2.7B (finetuned on PTB),1,,,,,,,,,
OPT-1.3B,Language,,,,,,,,,2022-06-21,,,,,,,,,,,1.67,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,OPT-1.3B,1,,,,,,,,,
OPT-1.3B (finetuned on PTB),Language,,,,,,,,,2022-06-21,,,,,,,,,,,1.67,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,OPT-1.3B (finetuned on PTB),1,,,,,,,,,
OPT-2.7B (finetuned on WT2),Language,,,,,,,,,2022-06-21,,,,,,,,,,,1.67,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,OPT-2.7B (finetuned on WT2),1,,,,,,,,,
OPT-125M (finetuned),Language,,,,,,,,,2022-06-21,,,,,,,,,,,1.67,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,OPT-125M (finetuned),1,,,,,,,,,
OPT-13B,Language,,,,,,,,,2022-06-21,,,,,,,,,,,1.67,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,OPT-13B,1,,,,,,,,,
OPT-6.7B,Language,,,,,,,,,2022-06-21,,,,,,,,,,,1.67,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,OPT-6.7B,1,,,,,,,,,
OPT-66B,Language,,,,,,,,,2022-06-21,,,,,,,,,,,1.67,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,OPT-66B,1,,,,,,,,,
OPT-350M,Language,,,,,,,,,2022-06-21,,,,,,,,,,,1.67,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,OPT-350M,1,,,,,,,,,
OPT-2.7B,Language,,,,,,,,,2022-06-21,,,,,,,,,,,1.67,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,OPT-2.7B,1,,,,,,,,,
OPT-125M (finetuned on PTB),Language,,,,,,,,,2022-06-21,,,,,,,,,,,1.67,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,OPT-125M (finetuned on PTB),1,,,,,,,,,
OPT-30B,Language,,,,,,,,,2022-06-21,,,,,,,,,,,1.67,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,OPT-30B,1,,,,,,,,,
OPT-1.3B (finetuned),Language,,,,,,,,,2022-06-21,,,,,,,,,,,1.67,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,OPT-1.3B (finetuned),1,,,,,,,,,
Parti,Drawing,Text-to-image,"Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, Yonghui Wu",SOTA Improvement,"""Second, we achieve consistent quality improvements by scaling the encoder-decoder Transformer model up to 20B parameters, with a new state-of-the-art zero-shot FID score of 7.23 and finetuned FID score of 3.22 on MS-COCO""",,https://arxiv.org/abs/2206.10789v1,512.00,Scaling Autoregressive Models for Content-Rich Text-to-Image Generation,2022-06-22,Google Research,Industry,20000000000.00,"Abstract: ""we achieve consistent quality improvements
by scaling the encoder-decoder Transformer model up to 20B parameters""",3.962895376192635e+23,"Calculated from architecture. Does not take into account the encoding and decoding of text and images, only the transformer stack.

Table 1 shows for the 20B model
16 encoder layers
64 decoder layers
Dmodel = 4096
Dhidden = 16384
Num heads = 64

Just below table 1:
""We use a maximum length of text tokens of 128, and the length of image tokens are fixed to 1024""

I take the length of the sequence to be 100 for the encoder stack and 1024 for the decoder stack.

Section 3, Training: ""a total
of 450,000 steps and final ratio of 0.025. We use a global batch size of 8192 during training.""","LAION-400M, FIT400M, JFT-4B ",,4800000000,,,,,,,,,Self-supervised learning,$486659.77,,Industry,,"We present the Pathways Autoregressive Text-to-Image (Parti) model, which generates high-fidelity photorealistic images and supports content-rich synthesis involving complex compositions and world knowledge. Parti treats text-to-image generation as a sequence-to-sequence modeling problem, akin to machine translation, with sequences of image tokens as the target outputs rather than text tokens in another language. This strategy can naturally tap into the rich body of prior work on large language models, which have seen continued advances in capabilities and performance through scaling data and model sizes. Our approach is simple: First, Parti uses a Transformer-based image tokenizer, ViT-VQGAN, to encode images as sequences of discrete tokens. Second, we achieve consistent quality improvements by scaling the encoder-decoder Transformer model up to 20B parameters, with a new state-of-the-art zero-shot FID score of 7.23 and finetuned FID score of 3.22 on MS-COCO. Our detailed analysis on Localized Narratives as well as PartiPrompts (P2), a new holistic benchmark of over 1600 English prompts, demonstrate the effectiveness of Parti across a wide variety of categories and difficulty aspects. We also explore and highlight limitations of our models in order to define and exemplify key areas of focus for further improvements. See https://parti.research.google/ for high-resolution images.",2023-12-05 04:33,Robi Rahman,,,Multinational,Google Research,,,,Parti,,,
CodeGeeX,Language,Code generation,,,,Permissive license,https://github.com/THUDM/CodeGeeX,,,2022-06-22,"Zhipu AI,Tsinghua University",,13000000000.00,"""We introduce CodeGeeX, a large-scale multilingual code generation model with 13 billion parameters""",6.630000000001e+22,"Assume 1 epoch on 850B tokens.
C=6DN=6*850B*13B
https://www.wolframalpha.com/input?i=6+*+13+billion+*+850+billion",,,637500000000,"As of June 22, 2022, CodeGeeX has been trained on more than 850 billion tokens",,,,156.1,"Assume 30% utilization on 1536 Ascend 910 calculating in FP16.
https://www.wolframalpha.com/input?i=6+*+13+billion+*+850+billion+FLOP+%2F+%280.30*1536*256+TFLOPS%29
If they used INT8 precision, the training time would be half of this.",Huawei Ascend 910,Huawei Ascend 910,Self-supervised learning,,,Industry,Likely,"We introduce CodeGeeX, a large-scale multilingual code generation model with 13 billion parameters, pre-trained on a large code corpus of more than 20 programming languages. As of June 22, 2022, CodeGeeX has been trained on more than 850 billion tokens on a cluster of 1,536 Ascend 910 AI Processors.",2023-12-07 07:46,Anonymous,,0,"China,China","Zhipu AI, Tsinghua University",,,,,,,
YaLM,Language,Language modelling,,,,,https://medium.com/yandex/yandex-publishes-yalm-100b-its-the-largest-gpt-like-neural-network-in-open-source-d1df53d0e9a6,0.00,,2022-06-23,Yandex,Industry,100000000000.00,,2.2e+23,"""It took us 65 days to train the model on a pool of 800 A100 graphics cards and 1.7 TB of online texts, books, and countless other sources.""",,,,,,,,,,,,,,,Industry,,,2023-11-23 06:53,Robi Rahman,,,Russia,Yandex,,,,,,,
GPT-SW3,Language,Language modelling,"Ariel Ekgren, Amaru Cuba Gyllensten, Evangelia Gogoulou, Alice Heiman, Severine Verlinden, Joey Ohman, Fredrik Carlsson, Magnus Sahlgren",,,,http://www.lrec-conf.org/proceedings/lrec2022/pdf/2022.lrec-1.376.pdf,6.00,Lessons Learned from GPT-SW3: Building the First Large-Scale Generative Language Model for Swedish,2022-06-25,"AI Sweden,RISE",Academia,3500000000.00,,1.3e+21,"From section 4: ""Training was performed on GPU resources from the Berzelius Superpod, which is currently the fastest super
computer in Sweden, equipped with 60 Nvidia DGX
A100 servers, each of which consists of 8 Nvidia A100
GPUs with 320 GB Total GPU memory. Our training
process took 2.5 days utilizing 16 of the DGX A100
servers (in total 128 GPUs).""

2.5*24*60**2 * 128 * 1.56E+14 * 0.3 = 1.3e21",,Novel Swedish 100GB corpus from news articles.,16700000000,"100GB Swedish corpus, assume Swedish has similar 167M words per GB as German.
100*167e6 = 1.67e10",,,,60.0,"""Our training process took 2.5 days utilizing 16 of the DGX A100 servers (in total 128 GPUs).""",NVIDIA A100,NVIDIA A100,Self-supervised learning,,,,,"Large-scale generative language models such as the GPT series (Radford and Narasimhan, 2018; Radford et al., 2019; Brown et al., 2020) have enjoyed considerable attention in recent years. This has been partly due to their unprecedented ability to generate coherent text, but also for their capacity for zero-shot performance - without any training examples, on a wide range of different tasks. A prerequisite for building such models is access to both large amounts of high-quality text data and powerful computational resources. This has proven to be a limiting factor for the development of large-scale models for languages other than English. With the goal of promoting the development of largescale generative models for other languages, we here present our work on developing and evaluating GPTSW3, a 3.5 billion parameter autoregressive language model, trained on a newly collected 100 GB Swedish corpus. To the best of our knowledge, this is the largest generative model for Swedish to date, and probably one of the bigger non-English models at the moment. In this paper, we collect the lessons learned by developing and evaluating this model, including challenges with data collection, training procedures, and validation activities.",2023-11-23 06:53,Robi Rahman,,,"Sweden,Sweden","AI Sweden, RISE",,,,,,,
ProGen2,Language,Protein generation,"Erik Nijkamp, Jeffrey Ruffolo, Eli N. Weinstein, Nikhil Naik, Ali Madani",SOTA Improvement,"""ProGen2 models show state-of-the-art performance in capturing the distribution of observed evolutionary sequences, generating novel viable sequences, and predicting protein fitness without additional finetuning""",,https://arxiv.org/pdf/2206.13517.pdf,77.00,ProGen2: Exploring the Boundaries of Protein Language Models,2022-06-27,"Salesforce,Johns Hopkins University,Columbia University",Industry - Academia Collaboration (Industry leaning),6400000000.00,"""We introduce a suite of protein language models, named ProGen2, that are scaled up to 6.4B parameters""",1.3e+22,"""350,000 steps x 1m batch size x 6.4 B “connections” x 6"" - Arb Research (https://arbresearch.com/files/gen_bio.pdf)

Steps and batches from Table 1. ","UniRef90, BFD30","""The standard PROGEN2 models are pretrained on a mixture of Uniref90 (Suzek et al., 2015) and BFD30 (Steinegger & Söding, 2018) databases""",,,,,,,,Google TPU v3,Google TPU v3,,,,Industry,Likely,"Attention-based models trained on protein sequences have demonstrated incredible success at classification and generation tasks relevant for artificial intelligence-driven protein design. However, we lack a sufficient understanding of how very large-scale models and data play a role in effective protein model development. We introduce a suite of protein language models, named ProGen2, that are scaled up to 6.4B parameters and trained on different sequence datasets drawn from over a billion proteins from genomic, metagenomic, and immune repertoire databases. ProGen2 models show state-of-the-art performance in capturing the distribution of observed evolutionary sequences, generating novel viable sequences, and predicting protein fitness without additional finetuning. As large model sizes and raw numbers of protein sequences continue to become more widely accessible, our results suggest that a growing emphasis needs to be placed on the data distribution provided to a protein sequence model. We release the ProGen2 models and code at this https URL.",2023-12-05 04:33,Anonymous,,,"United States of America,United States of America,United States of America","Salesforce, Johns Hopkins University, Columbia University",,,,ProGen2,,,
Minerva (540B),Language,Quantitative Reasoning Problems,"Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, Vedant Misra",SOTA Improvement,,,https://arxiv.org/abs/2206.14858,307.00,Solving Quantitative Reasoning Problems with Language Models,2022-06-29,Google,Industry,540350000000.00,"""To further our understanding of the
impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer
language model, which we call Pathways Language Model (PaLM).""

Our approach is to start with the PaLM pretrained decoder-only transformer language models Chowdhery
et al. (2022), and further train (finetune) them on our mathematical dataset using an autoregressive objective.
Table 2 contains the main model and training hyperparameters.

See Table 2",2.7414947368421056e+24,"Minerva was fine-tuned from PaLM using the same hardware. Assume the same model FLOPs utilization rate for pre-training and fine-tuning.

PaLM pretraining time: 6144 TPU for 1200 hours + 3072 TPU for 336 hours = @8404992 TPU-hours
Minerva finetuning time: 1024 TPU for 696 hours = 712704 TPU-hours
So fine-tuning added 8.5% more compute.

Minerva total compute = PaLM pretraining compute * (712704+8404992)/(8404992) = 2.7415*10^24 FLOP
https://www.wolframalpha.com/input?i=%28712704%2B8404992%29%2F%288404992%29+*+2.5272*10%5E24

",,"PaLM, finetuned on arxiv",613875000000,"""Our models were trained on a dataset of 38.5B tokens"" + PaLM",,,,696.0,,Google TPU v4,Google TPU v4,Self-supervised learning,$3267257.75,,Industry,,"Language models have achieved remarkable performance on a wide range of tasks that require natural language understanding. Nevertheless, state-of-the-art models have generally struggled with tasks that require quantitative reasoning, such as solving mathematics, science, and engineering problems at the college level. To help close this gap, we introduce Minerva, a large language model pretrained on general natural language data and further trained on technical content. The model achieves state-of-the-art performance on technical benchmarks without the use of external tools. We also evaluate our model on over two hundred undergraduate-level problems in physics, biology, chemistry, economics, and other sciences that require quantitative reasoning, and find that the model can correctly answer nearly a third of them.",2023-12-05 04:33,Robi Rahman,,,Multinational,Google,,2,,Minerva (540B),1024,,Minerva (540B)
BLOOM-7.1B,Language,,"Margaret Mitchell, Giada Pistilli, Yacine Jernite, Ezinwanne Ozoani, Marissa Gerchick, Nazneen Rajani, Sasha Luccioni, Irene Solaiman, Maraim Masoud, Somaieh Nikpoor, Carlos Muñoz Ferrandis, Stas Bekman, Christopher Akiki, Danish Contractor, David Lansky, Angelina McMillan-Major, Tristan Thrush, Suzana Ilić, Gérard Dupont, Shayne Longpre, Manan Dey, Stella Biderman, Douwe Kiela, Emi Baylor, Teven Le Scao, Aaron Gokaslan, Julien Launay, Niklas Muennighoff",,,,"https://huggingface.co/bigscience/bloom-3b#:~:text=Started%2011th%20March%2C%202022%2011,Ended%205th%20July%2C%202022",404.00,BigScience Language Open-science Open-access Multilingual (BLOOM) Language Model,2022-07-05,"Hugging Face,BigScience",Research Collective,7070000000.00,,1.48e+22,,,,,,1.00,,,,,,,,,,,,,2023-11-23 06:53,Robi Rahman,BLOOM-7.1B,,"Multinational,Multinational","Hugging Face, BigScience",,,,,,,
CodeT5-large,Language,Code generation,"Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, Steven C.H. Hoi ",SOTA Improvement,"""Our method not only achieves new SOTA results on the challenging APPS benchmark, but also shows strong zero-shot transfer capability with new SOTA results on the simpler MBPP benchmark.""",,https://arxiv.org/abs/2207.01780,95.00,CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning,2022-07-05,Salesforce,Industry,770000000.00,"""We pretrain a CodeT5-large model (770M) from scratch following T5-large’s architecture""",2.72e+21,"""We perform our experiments on a kubernetes with 16 A100-40G GPUs on Google Cloud Platform and the total pretraining duration is around 21 days""

16 * 312tFLOP/s * 21 * 24 * 3600 * 0.3 (utilization assumption) = 2.72e21",,"""We enlarge the Python pretraining dataset using the recently released
large-scale Github Code dataset5. We have compiled public, non-personal information from GitHub consisting of permissively licensed Python code (e.g. “mit”, “apache-2”, “bsd-3-clause”, “bsd-2- 126clause”, “cc0-1.0”, “unlicense”, “isc”). The resulting Python dataset (GCPY) has 10.5B tokens and is 10x larger than the CodeSearchNet (CSN) corpus [Husain et al., 2019] used in the original CodeT5 [Wang et al., 2021]""",,10.5b tokens,150.00,,,504.0,21 days,NVIDIA A100,NVIDIA A100,,,,,Likely,"""Program synthesis or code generation aims to generate a program that satisfies a problem specification. Recent approaches using large-scale pretrained language models (LMs) have shown promising results, yet they have some critical limitations. In particular, they often follow a standard supervised fine-tuning procedure to train a code generation model only from the pairs of natural-language problem descriptions and ground-truth programs. Such paradigm largely ignores some important but potentially useful signals in the problem specification such as unit tests, which thus often results in poor performance when solving complex unseen coding tasks. To address the limitations, we propose ""CodeRL"", a new framework for program synthesis tasks through pretrained LMs and deep reinforcement learning (RL). Specifically, during training, we treat the code-generating LM as an actor network, and introduce a critic network that is trained to predict the functional correctness of generated programs and provide dense feedback signals to the actor. During inference, we introduce a new generation procedure with a critical sampling strategy that allows a model to automatically regenerate programs based on feedback from example unit tests and critic scores. For the model backbones, we extended the encoder-decoder architecture of CodeT5 with enhanced learning objectives, larger model sizes, and better pretraining data. Our method not only achieves new SOTA results on the challenging APPS benchmark, but also shows strong zero-shot transfer capability with new SOTA results on the simpler MBPP benchmark.""",2023-12-05 04:33,Anonymous,,,United States of America,Salesforce,,,,CodeT5-large,,,
BLOOM-1.7B,Language,,"Margaret Mitchell, Giada Pistilli, Yacine Jernite, Ezinwanne Ozoani, Marissa Gerchick, Nazneen Rajani, Sasha Luccioni, Irene Solaiman, Maraim Masoud, Somaieh Nikpoor, Carlos Muñoz Ferrandis, Stas Bekman, Christopher Akiki, Danish Contractor, David Lansky, Angelina McMillan-Major, Tristan Thrush, Suzana Ilić, Gérard Dupont, Shayne Longpre, Manan Dey, Stella Biderman, Douwe Kiela, Emi Baylor, Teven Le Scao, Aaron Gokaslan, Julien Launay, Niklas Muennighoff",,,,,,,2022-07-05,"Hugging Face,BigScience",Research Collective,,,,,,,,,1.00,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,BLOOM-1.7B,1,"Multinational,Multinational","Hugging Face, BigScience",,,,,,,
BLOOM-560M,Language,,"Margaret Mitchell, Giada Pistilli, Yacine Jernite, Ezinwanne Ozoani, Marissa Gerchick, Nazneen Rajani, Sasha Luccioni, Irene Solaiman, Maraim Masoud, Somaieh Nikpoor, Carlos Muñoz Ferrandis, Stas Bekman, Christopher Akiki, Danish Contractor, David Lansky, Angelina McMillan-Major, Tristan Thrush, Suzana Ilić, Gérard Dupont, Shayne Longpre, Manan Dey, Stella Biderman, Douwe Kiela, Emi Baylor, Teven Le Scao, Aaron Gokaslan, Julien Launay, Niklas Muennighoff",,,,,,,2022-07-05,"Hugging Face,BigScience",Research Collective,,,,,,,,,1.00,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,BLOOM-560M,1,"Multinational,Multinational","Hugging Face, BigScience",,,,,,,
BLOOM-1B,Language,,"Margaret Mitchell, Giada Pistilli, Yacine Jernite, Ezinwanne Ozoani, Marissa Gerchick, Nazneen Rajani, Sasha Luccioni, Irene Solaiman, Maraim Masoud, Somaieh Nikpoor, Carlos Muñoz Ferrandis, Stas Bekman, Christopher Akiki, Danish Contractor, David Lansky, Angelina McMillan-Major, Tristan Thrush, Suzana Ilić, Gérard Dupont, Shayne Longpre, Manan Dey, Stella Biderman, Douwe Kiela, Emi Baylor, Teven Le Scao, Aaron Gokaslan, Julien Launay, Niklas Muennighoff",,,,,,,2022-07-05,"Hugging Face,BigScience",Research Collective,,,,,,,,,1.00,,,,,,,,,,,Unverified,,2023-12-08 01:01,Robi Rahman,BLOOM-1B,1,"Multinational,Multinational","Hugging Face, BigScience",,,,,,,
BLOOM-3B,Language,,"Margaret Mitchell, Giada Pistilli, Yacine Jernite, Ezinwanne Ozoani, Marissa Gerchick, Nazneen Rajani, Sasha Luccioni, Irene Solaiman, Maraim Masoud, Somaieh Nikpoor, Carlos Muñoz Ferrandis, Stas Bekman, Christopher Akiki, Danish Contractor, David Lansky, Angelina McMillan-Major, Tristan Thrush, Suzana Ilić, Gérard Dupont, Shayne Longpre, Manan Dey, Stella Biderman, Douwe Kiela, Emi Baylor, Teven Le Scao, Aaron Gokaslan, Julien Launay, Niklas Muennighoff",,,,,,,2022-07-05,"Hugging Face,BigScience",Research Collective,,,,,,,,,1.00,,,,,,,,,,,Unverified,,2023-12-08 01:01,Robi Rahman,BLOOM-3B,1,"Multinational,Multinational","Hugging Face, BigScience",,,,,,,
NLLB,Language,Translation,"Marta R. Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loic Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco (Paco) Guzmán, Philipp Koehn, Alexandre Mourachko, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, Jeff Wang",SOTA Improvement,"""Our model achieves an improvement of 44% BLEU relative to the previous state-of-the-art""",,https://research.facebook.com/publications/no-language-left-behind/,341.00,No Language Left Behind: Scaling Human-Centered Machine Translation,2022-07-06,Meta AI,Industry,54500000000.00,"Section 8.2.4: ""The model has a total of 54.5B parameters
and FLOPs similar to that of a 3.3B dense model""",1.751113728e+22,"Section 8.8:
"" To train NLLB-200, a cumulative
of 51968 GPU hours of computation was performed on hardware of type A100-SXM-80GB""
See also Table 48

Section 8.2.4 states they use FP16

NVIDIA Datasheet states 312TFLOPS for FP16
https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-nvidia-us-2188504-web.pdf

Assuming 0.3 utilization:

312e12*3600*51968*0.3

Also:
""Our final model is a Transformer
encoder-decoder model in which we replace the Feed Forward Network (FFN) layer in
every 4th Transformer block with a Sparsely Gated Mixture of Experts layer containing 128
experts. We use model dimension 2048, FFN dimension 8192, 16 attention heads, 24 encoder
layers and 24 decoder layers. We use Pre-LayerNorm (Xiong et al., 2020) as described in
Section 6.1.1. We share the embedding weights of the encoder input embedding, decoder
input embedding and decoder output embedding layers. We use an overall dropout of 0.3,
attention dropout 0.1 and EOM with peom=0.2. The model has a total of 54.5B parameters
and FLOPs similar to that of a 3.3B dense model.""",,,360000000000,"[WORDS]

Section 8.2.2: ""As we prepare to train on the final 202 language dataset comprising of over 18B sentence
pairs and 2440 language directions""

18B sentences * 20 words/sentence",,,,,,,,Self-supervised learning,$39175.64,,Industry,,"Driven by the goal of eradicating language barriers on a global scale, machine translation has solidified itself as a key focus of artificial intelligence research today. However, such efforts have coalesced around a small subset of languages, leaving behind the vast majority of mostly low-resource languages. What does it take to break the 200 language barrier while ensuring safe, high quality results, all while keeping ethical considerations in mind? In No Language Left Behind, we took on this challenge by first contextualizing the need for low-resource language translation support through exploratory interviews with native speakers. Then, we created datasets and models aimed at narrowing the performance gap between low and high-resource languages. More specifically, we developed a conditional compute model based on Sparsely Gated Mixture of Experts that is trained on data obtained with novel and effective data mining techniques tailored for low-resource languages. We propose multiple architectural and training improvements to counteract overfitting while training on thousands of tasks. Critically, we evaluated the performance of over 40,000 different translation directions using a human-translated benchmark, Flores-200, and combined human evaluation with a novel toxicity benchmark covering all languages in Flores-200 to assess translation safety. Our model achieves an improvement of 44% BLEU relative to the previous state-of-the-art, laying important groundwork towards realizing a universal translation system. Finally, we open source all contributions described in this work, accessible at https://github.com/facebookresearch/fairseq/tree/nllb.",2023-12-05 04:33,Robi Rahman,,,Multinational,Meta AI,,,,NLLB,,,
Transformer-XL + RMT,Language,,"Aydar Bulatov, Yuri Kuratov, Mikhail S. Burtsev",,,,https://web.archive.org/web/20220715153256/https://arxiv.org/pdf/2207.06881.pdf,39.00,Recurrent Memory Transformer,2022-07-14,,,247000000.00,,,,,,,,,,,,,,,,,,,,,2023-12-05 04:33,Robi Rahman,Transformer-XL + RMT,,,,,,,,,,
RITA,Language,"Protein generation,Proteins","Daniel Hesslow, Niccolò Zanichelli, Pascal Notin, Iacopo Poli, Debora Marks",,,,https://arxiv.org/abs/2205.05789,35.00,RITA: a Study on Scaling Up Generative Protein Sequence Models,2022-07-14,"LightOn,Harvard University,University of Oxford",Industry - Academia Collaboration (Industry leaning),1200000000.00,"""with up to 1.2 billion parameters""",3.4e+21,"""The models were trained for a total training time of over 25 thousand Nvidia-V100 GPU hours""

125 teraFLOP/s (uncertain which V100 model, tensor performance varies from 112-130tFLOP/s) * 25000 * 3600 * 0.3 (utilization) = 3.4e+21","UniRef100, MGnify, Metaclust","""We focus on three different pre-training corpora: UniRef100 (The UniProt Consortium, 2020), MGnify (Mitchell et al., 2020) and Metaclust (Steinegger & Soding ¨ , 2018), each providing a sufficient amount of tokens for model pretraining without having to repeat the data.""",,,1.00,,,,,,,,,,,Likely,"In this work we introduce RITA: a suite of autoregressive generative models for protein sequences, with up to 1.2 billion parameters, trained on over 280 million protein sequences belonging to the UniRef-100 database. Such generative models hold the promise of greatly accelerating protein design. We conduct the first systematic study of how capabilities evolve with model size for autoregressive transformers in the protein domain: we evaluate RITA models in next amino acid prediction, zero-shot fitness, and enzyme function prediction, showing benefits from increased scale. We release the RITA models openly, to the benefit of the research community.",2023-12-05 04:33,Anonymous,,,"France,United States of America,United Kingdom of Great Britain and Northern Ireland","LightOn, Harvard University, University of Oxford",,,,,,,
YuYan 11B,,,"Gongzheng Li, Yadong Xi, Jingzhen Ding, Duan Wang, Ziyang Luo, Rongsheng Zhang, Bai Liu, Changjie Fan, Xiaoxi Mao, Zeng Zhao",,,,https://arxiv.org/abs/2104.12470,4.00,Easy and Efficient Transformer: Scalable Inference Solution For Large NLP Model,2022-07-15,"Hong Kong Baptist University,NetEase",,11000000000.00,https://huggingface.co/FUXI/yuyan-11b,,,,,,,,,,,,"NVIDIA A100 PCIe,NVIDIA GeForce RTX 2080 Ti","NVIDIA A100 PCIe, NVIDIA GeForce RTX 2080 Ti",Self-supervised learning,,,Industry,Confident,"Recently, large-scale transformer-based models have been proven to be effective over various tasks across many domains. Nevertheless, applying them in industrial production requires tedious and heavy works to reduce inference costs. To fill such a gap, we introduce a scalable inference solution: Easy and Efficient Transformer (EET), including a series of transformer inference optimization at the algorithm and implementation levels. First, we design highly optimized kernels for long inputs and large hidden sizes. Second, we propose a flexible CUDA memory manager to reduce the memory footprint when deploying a large model. Compared with the state-of-the-art transformer inference library (Faster Transformer v4.0), EET can achieve an average of 1.40-4.20x speedup on the transformer decoder layer with an A100 GPU",2023-12-07 07:31,Anonymous,,0,"Hong Kong,China","Hong Kong Baptist University, NetEase",,,,,,,
ProtGPT2,Language,"Protein generation,Proteins","Noelia Ferruz, Steffen Schmidt, Birte Höcker",,,,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9329459/,145.00,ProtGPT2 is a deep unsupervised language model for protein design,2022-07-27,University of Bayreuth,Academia,738000000.00,"""The final model is a decoder-only architecture of 36 layers and 738 million parameters""",4.1e+21,"""The model trained on 128 NVIDIA A100s in 4 days""

128 * 4 * 24 * 3600 * 312 trillion FLOP/s * 0.3 = 4.1e21",UniRef50,"""We took Uniref50 version 2021_04 as the dataset for training, containing 49,874,565 sequences""",,,,,,96.0,,NVIDIA A100,NVIDIA A100,Unsupervised,,,Academia,Likely,"Protein design aims to build novel proteins customized for specific purposes,
thereby holding the potential to tackle many environmental and biomedical
problems. Recent progress in Transformer-based architectures has enabled
the implementation of language models capable of generating text with
human-like capabilities. Here, motivated by this success, we describe
ProtGPT2, a language model trained on the protein space that generates de
novo protein sequences following the principles of natural ones. The generated proteins display natural amino acid propensities, while disorder predictions indicate that 88% of ProtGPT2-generated proteins are globular, in line with natural sequences. Sensitive sequence searches in protein databases
show that ProtGPT2 sequences are distantly related to natural ones, and
similarity networks further demonstrate that ProtGPT2 is sampling unexplored regions of protein space. AlphaFold prediction of ProtGPT2-sequences
yields well-folded non-idealized structures with embodiments and large loops
and reveals topologies not captured in current structure databases. ProtGPT2
generates sequences in a matter of seconds and is freely available.",2023-12-05 04:33,Anonymous,,,Germany,University of Bayreuth,,,,,,,
AlexaTM 20B,Language,Language modelling,"Saleh Soltan, Shankar Ananthakrishnan, Jack FitzGerald, Rahul Gupta, Wael Hamza, Haidar Khan, Charith Peris, Stephen Rawls, Andy Rosenbaum, Anna Rumshisky, Chandana Satya Prakash, Mukund Sridhar, Fabian Triefenbach, Apurv Verma, Gokhan Tur, Prem Natarajan",SOTA Improvement,The Abstract reports SOTA improvement on multiple benchmarks.,,https://arxiv.org/abs/2208.01448,54.00,AlexaTM 20B: Few-Shot Learning Using a Large-Scale Multilingual Seq2Seq Model,2022-08-02,Amazon,Industry,19750000000.00,See Table 1 on p.3 of the paper,2.04374016e+23,"Training throughput is reported as 154 TFLOP/s - see p.5 of the paper.
""We relied on an internal and optimized version of DeepSpeed that we have since open-sourced (Chiu & Zheng, 2022) to obtain training throughput of up to 154 TFLOPS/GPU on 16 AWS p4d.24xlarge compute instances.""

Accelerator compute days are reported as 15,360 days - see Table 17 on p.18 of the paper.",mC4; Wikipedia,See Table 2 on p.3 of the paper.,,,,,,2880.0,"See p.5 of the paper: ""We trained AlexaTM 20B for 120 days on 128 A100 GPUs...""",,,,,,Industry,,"In this work, we demonstrate that multilingual large-scale sequence-to-sequence (seq2seq) models, pre-trained on a mixture of denoising and Causal Language Modeling (CLM) tasks, are more efficient few-shot learners than decoder-only models on various tasks. In particular, we train a 20 billion parameter multilingual seq2seq model called Alexa Teacher Model (AlexaTM 20B) and show that it achieves state-of-the-art (SOTA) performance on 1-shot summarization tasks, outperforming a much larger 540B PaLM decoder model. AlexaTM 20B also achieves SOTA in 1-shot machine translation, especially for low-resource languages, across almost all language pairs supported by the model (Arabic, English, French, German, Hindi, Italian, Japanese, Marathi, Portuguese, Spanish, Tamil, and Telugu) on Flores-101 dataset. We also show in zero-shot setting, AlexaTM 20B outperforms GPT3 (175B) on SuperGLUE and SQuADv2 datasets and provides SOTA performance on multilingual tasks such as XNLI, XCOPA, Paws-X, and XWinograd. Overall, our results present a compelling case for seq2seq models as a powerful alternative to decoder-only models for Large-scale Language Model (LLM) training. ",2023-12-05 04:33,Robi Rahman,,,Multinational,Amazon,,,,AlexaTM 20B,,,
GLM-130B,Language,,,SOTA Improvement,"""GLM-130B achieves an accuracy of 80.2% on zero-shot LAMBADA (En), while 76.2% for GPT-3 175B and 77.9% for the SOTA offered by PaLM 540B.""",,https://keg.cs.tsinghua.edu.cn/glm-130b/posts/glm-130b/,60.00,GLM-130B: An open bilingual pre-trained model,2022-08-04,Tsinghua University,Academia,130000000000.00,Dense model,3.778e+23,"""96 NVIDIA A100 (40G * 8) servers for 2 months""

312 TFLOPS/GPU * 96 servers * 8 GPU/server * 2 months * 30% utilization = 3.778*10^23 FLOP
https://www.wolframalpha.com/input?i=312+teraflops+*+96+*+8+*+2+months+*+30%25",,,,,1.00,,,,,,,,,,Industry,,,2023-11-26 03:25,Robi Rahman,GLM-130B,,China,Tsinghua University,,,,GLM-130B,,,
BlenderBot 3,Language,Chat,"Kurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju, Eric Michael Smith, Stephen Roller, Megan Ung, Moya Chen, Kushal Arora, Joshua Lane, Morteza Behrooz, William Ngan, Spencer Poff, Naman Goyal, Arthur Szlam, Y-Lan Boureau, Melanie Kambadur, Jason Weston",SOTA Improvement,"""Human evaluations show its superiority to existing open-domain dialogue agents, including its predecessors""",Permissive license,https://arxiv.org/abs/2208.03188,148.00,BlenderBot 3: a deployed conversational agent that continually learns to responsibly engage,2022-08-10,"McGill University,Meta AI,Mila- Quebec AI",Industry - Academia Collaboration,175000000000.00,,4.3e+23,(taken from OPT-175 base),,"Fine-tuned from OPT-175B.

""The fine-tuning data for BB3 comprises roughly 4 million source/target examples spread across the various
training modules. This corresponds to around 1.13B training tokens. When fine-tuning the OPT-based
BB3 models, we additionally included 600k examples ( 170m tokens) of pre-training data to help with
training stability. Table 16 and Table 17 enumerate the breakdown by module.""
",,,,,,,,NVIDIA A100,NVIDIA A100,,,,Industry,Likely,"We present BlenderBot 3, a 175B parameter dialogue model capable of open-domain conversation with access to the internet and a long-term memory, and having been trained on a large number of user defined tasks. We release both the model weights and code, and have also deployed the model on a public web page to interact with organic users. This technical report describes how the model was built (architecture, model and training scheme), and details of its deployment, including safety mechanisms. Human evaluations show its superiority to existing open-domain dialogue agents, including its predecessors (Roller et al., 2021; Komeili et al., 2022). Finally, we detail our plan for continual learning using the data collected from deployment, which will also be publicly released. The goal of this research program is thus to enable the community to study ever-improving responsible agents that learn through interaction.",2023-12-05 09:25,Anonymous,,0,"Canada,Multinational,Canada","McGill University, Meta AI, Mila- Quebec AI",OPT-175B,1,"""The 30B and 175B parameter BlenderBot 3 models were each trained for one epoch of the training data
on 64 (30B) or 128 (175B) x 40gb A100 GPUs; we found that the model (especially the 175B version)
overfit significantly when seeing the training data more than once. The 175B model was trained with
a batch size of 2^18 and the 30B model was trained with a batch size of 2^19, resulting in roughly 5600
updates and 2800 updates respectively.""

175b params * 5600 * 2^18 * 6 = 1.5e21
",,,,
Luminous-supreme,Language,Language Generation,,,,,https://docs.aleph-alpha.com/docs/introduction/model-card/,,Model Card Luminous,2022-08-15,Aleph Alpha,Industry,70000000000.00,"""~70B""",2.8e+23,"""~839000h"" GPU-hours on A100s, per Environmental Impact section of model card.

312 trillion * 839000 * 3600 * 0.3 = 2.8e23",,"""The Luminous family has been trained on a dataset compiled of sources in English, German, French, Spanish and Italian...""

more details in model card 
https://docs.aleph-alpha.com/docs/introduction/model-card/",,,,,,,,"NVIDIA A100 SXM4 40 GB,NVIDIA A100 SXM4 80 GB","NVIDIA A100 SXM4 40 GB, NVIDIA A100 SXM4 80 GB",,,,,Likely,"The Luminous series is a family of large language models. Large language models are powerful technological tools that can process and produce text. These capabilities emerge during model “training” where the model is exposed to significant amounts of human text data. Similar to a person who deliberately absorbs information while reading a whole library and half of the internet, large language models acquire structural understanding (and not necessarily also knowledge) of language and accumulated information about the world.

The Luminous family currently consists of three vanilla models, which differ in complexity and ability. They are, from the smallest to the largest, luminous-base, luminous-extended and luminous-supreme. All Luminous models are trained in the five most commonly spoken European languages: English, German, French, Italian and Spanish.",2023-11-23 06:53,Anonymous,,,Germany,Aleph Alpha,,,,,,,
PaLI,Vision,,"Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver, Nan Ding, Keran Rong, Hassan Akbari, Gaurav Mishra, Linting Xue, Ashish Thapliyal, James Bradbury, Weicheng Kuo, Mojtaba Seyedhosseini, Chao Jia, Burcu Karagol Ayan, Carlos Riquelme, Andreas Steiner, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, Radu Soricut",SOTA Improvement,"""PaLI achieves state-of-the-art in multiple vision and language tasks
(such as captioning, visual question-answering, scene-text understanding)""",,https://arxiv.org/abs/2209.06794v4,264.00,PaLI: A Jointly-Scaled Multilingual Language-Image Model,2022-09-14,Google,Industry,16900000000.00,"3.9b Image Encoder, 
14b Multimodal Encoder-Decoder",5.1e+22,"""The largest model, PaLI-17B, is pretrained using 1,024 GCP-TPUv4 chips for 7 days""

275 teraFLOP/s * 1024 * 7 * 24 * 3600 * 0.3 (utilization assumption) = 5.1e22",WebLI,"""we introduce WebLI, a multilingual imagelanguage dataset built from images and texts available on the public web... Due to the abundance of multilingual content on the internet, the collection process for the WebLI dataset can be scaled to cover 10 billion images and 12 billion alt-texts. In addition to annotation with web text, we use publicly available automatic service to extract OCR annotations on all images, resulting in 29 billion image-OCR pairs. To balance quality and retain scale, we filter the dataset to the highest quality subset retaining only the top 10% scoring of the original WebLI image-text pairs (about 1B examples), which we use to train PaLI""",,,1.00,,,168.0,7 days,Google TPU v4,Google TPU v4,,,,,Likely,"Effective scaling and a flexible task interface enable large language models to excel at many tasks. We present PaLI (Pathways Language and Image model), a model that extends this approach to the joint modeling of language and vision. PaLI generates text based on visual and textual inputs, and with this interface performs many vision, language, and multimodal tasks, in many languages. To train PaLI, we make use of large pre-trained encoder-decoder language models and Vision Transformers (ViTs). This allows us to capitalize on their existing capabilities and leverage the substantial cost of training them. We find that joint scaling of the vision and language components is important. Since existing Transformers for language are much larger than their vision counterparts, we train a large, 4-billion parameter ViT (ViT-e) to quantify the benefits from even larger-capacity vision models. To train PaLI, we create a large multilingual mix of pretraining tasks, based on a new image-text training set containing 10B images and texts in over 100 languages. PaLI achieves state-of-the-art in multiple vision and language tasks (such as captioning, visual question-answering, scene-text understanding), while retaining a simple, modular, and scalable design.",2023-12-05 04:33,Anonymous,,,Multinational,Google,,,,PaLI,,,
DistilProtBert,Language,Proteins,"Yaron Geffen, Yanay Ofran and Ron Unger",,,,https://academic.oup.com/bioinformatics/article/38/Supplement_2/ii95/6701995,13.00,DistilProtBert: a distilled protein language model used to distinguish between real proteins and their randomly shuffled counterparts,2022-09-18,Bar-Ilan University,Academia,230000000.00,"""we were able to reduce the number of DistilProtBert parameters by almost half, to 230 M""",190000000000000030000.00,"""Pretraining was done on five v100 32-GB Nvidia GPUs from a DGX
cluster with a local batch size of 16 examples... Every epoch run took approximately 4 days, resulting in total pretraining time of 12 days""

5 * 125 teraFLOP/s * 12 * 24 * 3600 * 0.3 (assumed utilization) = 1.9e20",UniRef50,"""DistilProtBert was pretrained on 43 M sequences from UniRef50 with length ranging from 20 to 512 amino acids""",,43M sequences,3.00,,,288.0,12 days,NVIDIA Tesla V100 DGXS 32 GB,NVIDIA Tesla V100 DGXS 32 GB,,,,Academia,Likely,"Recently, deep learning models, initially developed in the field of natural language processing (NLP), were applied successfully to analyze protein sequences. A major drawback of these models is their size in terms of the number of parameters needed to be fitted and the amount of computational resources they require. Recently, 'distilled' models using the concept of student and teacher networks have been widely used in NLP. Here, we adapted this concept to the problem of protein sequence analysis, by developing DistilProtBert, a distilled version of the successful ProtBert model. Implementing this approach, we reduced the size of the network and the running time by 50%, and the computational resources needed for pretraining by 98% relative to ProtBert model. Using two published tasks, we showed that the performance of the distilled model approaches that of the full model. We next tested the ability of DistilProtBert to distinguish between real and random protein sequences. The task is highly challenging if the composition is maintained on the level of singlet, doublet and triplet amino acids. Indeed, traditional machine-learning algorithms have difficulties with this task. Here, we show that DistilProtBert preforms very well on singlet, doublet and even triplet-shuffled versions of the human proteome, with AUC of 0.92, 0.91 and 0.87, respectively. Finally, we suggest that by examining the small number of false-positive classifications (i.e. shuffled sequences classified as proteins by DistilProtBert), we may be able to identify de novo potential natural-like proteins based on random shuffling of amino acid sequences.",2023-11-23 06:53,Anonymous,,,Israel,Bar-Ilan University,,,,,,,
Whisper,Speech,Audio Speech Recognition,"Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, Ilya Sutskever",SOTA Improvement,,,https://cdn.openai.com/papers/whisper.pdf,742.00,Robust Speech Recognition via Large-Scale Weak Supervision,2022-09-21,OpenAI,Industry,1550000000.00,Table 1,4.65e+22,"https://plotdigitizer.com/app

See figure 9",,,9302400000,"When scaled
to 680,000 hours of multilingual and multitask
supervision, the resulting models generalize well
to standard benchmarks and are often competitive
with prior fully supervised results but in a zeroshot transfer setting without the need for any finetuning. 


13,680 words/h * 680,000h = 9,302,400,000 words",3.00,,,,,,,Self-supervised learning,,,,,"We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zeroshot transfer setting without the need for any finetuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.",2023-12-12 07:40,Robi Rahman,,,United States of America,OpenAI,,,,Whisper,,,
Sparrow,Language,Chat,"Amelia Glaese, Nat McAleese, Maja Trębacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, Lucy Campbell-Gillingham, Jonathan Uesato, Po-Sen Huang, Ramona Comanescu, Fan Yang, Abigail See, Sumanth Dathathri, Rory Greig, Charlie Chen, Doug Fritz, Jaume Sanchez Elias, Richard Green, Soňa Mokrá, Nicholas Fernando, Boxi Wu, Rachel Foley, Susannah Young, Iason Gabriel, William Isaac, John Mellor, Demis Hassabis, Koray Kavukcuoglu, Lisa Anne Hendricks, Geoffrey Irving",,,,https://arxiv.org/abs/2209.14375,157.00,Improving alignment of dialogue agents via targeted human judgements,2022-09-28,DeepMind,,70000000000.00,70B,,,,,,,,,,,,,,,,,,Likely,"We present Sparrow, an information-seeking dialogue agent trained to be more helpful, correct, and harmless compared to prompted language model baselines. We use reinforcement learning from human feedback to train our models with two new additions to help human raters judge agent behaviour. First, to make our agent more helpful and harmless, we break down the requirements for good dialogue into natural language rules the agent should follow, and ask raters about each rule separately. We demonstrate that this breakdown enables us to collect more targeted human judgements of agent behaviour and allows for more efficient rule-conditional reward models. Second, our agent provides evidence from sources supporting factual claims when collecting preference judgements over model statements. For factual questions, evidence provided by Sparrow supports the sampled response 78% of the time. Sparrow is preferred more often than baselines while being more resilient to adversarial probing by humans, violating our rules only 8% of the time when probed. Finally, we conduct extensive analyses showing that though our model learns to follow our rules it can exhibit distributional biases.",2023-12-09 08:45,Anonymous,,0,United Kingdom of Great Britain and Northern Ireland,DeepMind,Chinchilla,,"Few clues from paper. Not clear how much fine-tuning data in total. Also they freeze some layers during fine-tuning:

""In all cases when fine-tuning, we freeze the bottom 64 transformer layers of Chinchilla, and
only fine-tune the final 16 layers; this allows sharing of the frozen layers between the rule model,
preference models, and the base LM/policy when reranking and during reinforcement learning
training, resulting in a reduced memory footprint (fig. 8).""",,,,
Make-A-Video,Text-to-Video,,"Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, Devi Parikh, Sonal Gupta, Yaniv Taigman",SOTA Improvement,,,https://arxiv.org/abs/2209.14792,406.00,Make-A-Video: Text-to-Video Generation without Text-Video Data,2022-09-29,Meta AI,Industry,,,,,,,,,,,,,,,,Self-supervised learning,,,,,,2023-12-05 04:33,Robi Rahman,,,Multinational,Meta AI,,,,Make-A-Video,,,
NMST+GPT-2,Language,,"Eugene Choi, Cheolhyoung Lee, Kyunghyun Cho",,,,https://web.archive.org/web/20230220171748/https://arxiv.org/pdf/2210.00660.pdf,0.00,A Non-monotonic Self-terminating Language Model,2022-10-03,,,124000000.00,,120000000000000000000.00,,,,,,2.98,,,,,,,,,,,,,2023-11-23 06:53,Robi Rahman,NMST+GPT-2,,,,,,,,,,
VRNS-RNN-3-3-5,Language,,"Brian DuSell, David Chiang",,,,https://arxiv.org/pdf/2210.01343,1.00,The Surprising Computational Power of Nondeterministic Stack RNNs,2022-10-04,,,1500000.00,,,,,,,,,,,,,,,,,,,,,2023-11-21 01:39,Robi Rahman,VRNS-RNN-3-3-5,,,,,,,,,,
Phenaki,Vision,Video generation,"Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, Hernan Moraldo, Han Zhang, Mohammad Taghi Saffar, Santiago Castro, Julius Kunze, Dumitru Erhan",SOTA Improvement,"""To the best of our knowledge, this is the first time a paper studies generating videos from time variable prompts""",,https://arxiv.org/abs/2210.02399,143.00,Phenaki: Variable Length Video Generation From Open Domain Textual Description,2022-10-05,"University College London (UCL),University of Michigan,Google Brain",Industry - Academia Collaboration (Industry leaning),1800000000.00,"Unless specified otherwise, we train a 1.8B parameter Phenaki model on a corpus of ∼15M textvideo pairs at 8 FPS mixed with ∼50M text-images plus ∼400M pairs of LAION-400M [41] (more
details in Appendix B.3). The model used in the visualisations in this paper was trained for 1 million
steps at a batch size of 512, which took less than 5 days. In this setup 80% of the training data came
from the video dataset and each image dataset contributed 10%.",,,,"Unless specified otherwise, we train a 1.8B parameter Phenaki model on a corpus of ∼15M textvideo pairs at 8 FPS mixed with ∼50M text-images plus ∼400M pairs of LAION-400M [41] (more
details in Appendix B.3). The model used in the visualisations in this paper was trained for 1 million
steps at a batch size of 512, which took less than 5 days. In this setup 80% of the training data came
from the video dataset and each image dataset contributed 10%.",,"Unless specified otherwise, we train a 1.8B parameter Phenaki model on a corpus of ∼15M textvideo pairs at 8 FPS mixed with ∼50M text-images plus ∼400M pairs of LAION-400M [41] (more
details in Appendix B.3). The model used in the visualisations in this paper was trained for 1 million
steps at a batch size of 512, which took less than 5 days. In this setup 80% of the training data came
from the video dataset and each image dataset contributed 10%.",,,,,,,,Self-supervised learning,,,,,,2023-12-05 04:33,Robi Rahman,,,"United Kingdom of Great Britain and Northern Ireland,United States of America,Multinational","University College London (UCL), University of Michigan, Google Brain",,,,Phenaki,,,
Imagen Video,Vision,Video generation,"Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P. Kingma, Ben Poole, Mohammad Norouzi, David J. Fleet, Tim Salimans",,,,https://arxiv.org/abs/2210.02303,444.00,Imagen Video: High Definition Video Generation with Diffusion Models,2022-10-05,Google Brain,Industry,11600000000.00,"Figure 6 summarizes the entire cascading pipeline of Imagen Video. In total, we have 1 frozen text
encoder, 1 base video diffusion model, 3 SSR (spatial super-resolution), and 3 TSR (temporal superresolution) models – for a total of 7 video diffusion models, with a total of 11.6B diffusion model
parameters",,,,,,"We train our models on a combination of an internal dataset consisting of 14 million video-text pairs
and 60 million image-text pairs, and the publicly available LAION-400M image-text dataset.",,,,,,,,,,,,,,2023-12-05 04:33,Robi Rahman,,,Multinational,Google Brain,,,,,,,
Decaying Fast Weights Transformer,Language,,Huanru Henry Mao,,,,https://arxiv.org/pdf/2210.04243.pdf,1.00,Fine-Tuning Pre-trained Transformers into Decaying Fast Weights,2022-10-09,,,242000000.00,,13000000000000000000.00,,,,,,192.12,,,,,,,,,,,,,2023-12-05 04:33,Robi Rahman,Decaying Fast Weights Transformer,,,,,,,,,,
TPM-LVD,Language,,"Anji Liu, Honghua Zhang, Guy Van den Broeck",,,,https://arxiv.org/pdf/2210.04398.pdf,12.00,Scaling up Probabilistic Circuits by Latent Variable Distillation,2022-10-10,,,1120000000.00,,,,,,,,,,,,,,,,,,,,,2023-12-05 04:33,Robi Rahman,TPM-LVD,,,,,,,,,,
Progressive LRD,Language,,"Habib Hajimolahoseini, Walid Ahmed, Mehdi Rezagholizadeh, Vahid Partovinia, Yang Liu",,,,https://web.archive.org/web/20221130215920/https://neurips2022-enlsp.github.io/papers/paper_33.pdf,0.00,Strategies for Applying Low Rank Decomposition to Transformer-Based Models,2022-10-12,,Industry,31000000.00,,62000000000000000000.00,,,,,,,,,,,,,,,,,,,2023-11-23 06:53,Robi Rahman,Progressive LRD,,,,,,,,,,
Characterizing Verbatim Short-Term Memory in Neural Language Models (182M),Language,,"Kristijan Armeni, Christopher Honey, Tal Linzen",,,,https://arxiv.org/pdf/2210.13569.pdf,3.00,Characterizing Verbatim Short-Term Memory in Neural Language Models,2022-10-24,"Johns Hopkins University,New York University (NYU)",Academia,182000000.00,Table 3,,,,,,,,,,,,,,,,,,,,2023-12-07 08:32,Robi Rahman,Characterizing Verbatim Short-Term Memory in Neural Language Models (182M),,"United States of America,United States of America","Johns Hopkins University, New York University (NYU)",,,,,,,
Characterizing Verbatim Short-Term Memory in Neural Language Models (117M),Language,,,,,,,,,2022-10-24,,,,,,,,,,,,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,Characterizing Verbatim Short-Term Memory in Neural Language Models (117M),1,,,,,,,,,
Characterizing Verbatim Short-Term Memory in Neural Language Models (108M),Language,,,,,,,,,2022-10-24,,,,,,,,,,,,,,,,,,,,,,Unverified,,2023-12-08 01:01,Robi Rahman,Characterizing Verbatim Short-Term Memory in Neural Language Models (108M),1,,,,,,,,,
ADP-FAIRSEQ+NGRAMRES,Language,,"Huayang Li, Deng Cai, Jin Xu, Taro Watanabe",,,,https://web.archive.org/web/20221027013457/https://arxiv.org/pdf/2210.14431.pdf,0.00,N-gram Is Back: Residual Learning of Neural Text Generation with n-gram Language Model,2022-10-26,,,247000000.00,,,,,,,,,,,,,,,,,,,,,2023-11-21 01:39,Robi Rahman,ADP-FAIRSEQ+NGRAMRES,,,,,,,,,,
ADP-FAIRSEQ,Language,,,,,,,,,2022-10-26,,,,,,,,,,,,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,ADP-FAIRSEQ,1,,,,,,,,,
retrieval-quality-kNN-LMs,Language,,"Andrew Drozdov, Shufan Wang, Razieh Rahimi, Andrew McCallum, Hamed Zamani, Mohit Iyyer",,,,https://arxiv.org/abs/2210.15859,8.00,"You can’t pick your neighbors, or can you? When and how to rely on retrieval in the kNN-LM",2022-10-28,University of Massachusetts Amherst,Academia,247000000.00,,,,,,,,,,,,,,,,,,,,,2023-12-07 08:34,Robi Rahman,retrieval-quality-kNN-LMs,,United States of America,University of Massachusetts Amherst,,,,,,,
Taiyi-Stable Diffusion,Drawing,Text-to-image,,Historical significance,"The first open-source, Chinese version of Stable Diffusion.",,https://huggingface.co/IDEA-CCNL/Taiyi-Stable-Diffusion-1B-Chinese-v0.1,0.00,,2022-10-31,IDEA CCNL,,1000000000.00,,5.1e+22,"Fine-tuning: 32 NVIDIA A100 GPUs for 100 hours
32 * 312e12 * 30% * 100 * 60 * 60 = 1.078272e+21 FLOP

Base model: Stable Diffusion, 5e+22 FLOP",,,,,,,,100.0,32 NVIDIA A100 GPUs for 100 hours,NVIDIA A100,NVIDIA A100,,,,,Likely,,2023-11-26 03:25,Anonymous,,,China,IDEA CCNL,,,,Taiyi-Stable Diffusion,,,
TransformerXL + PowerSGD + L-Greco,Language,,"Mohammadreza Alimohammadi, Ilia Markov, Elias Frantar, Dan Alistarh",,,,https://web.archive.org/web/20221101102609/https://arxiv.org/pdf/2210.17357.pdf,3.00,L-GreCo: An Efficient and General Framework for Layerwise-Adaptive Gradient Compression,2022-10-31,,Industry - Academia Collaboration,,,414000000000000000.00,,,,,,,,,,,,,,,,,,,2023-11-23 06:53,Robi Rahman,TransformerXL + PowerSGD + L-Greco,,,,,,,,,,
ESM-2,Language,Protein folding prediction,"Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, Allan dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Salvatore Candido, Alexander Rives",SOTA Improvement,"""The resulting ESM-2 model family significantly outperforms previously state-of-the-art ESM-1b (a ∼650 million parameter model) at a comparable number of parameters, and on structure prediction benchmarks it also outperforms other recent protein language models""",,https://www.biorxiv.org/content/biorxiv/early/2022/10/31/2022.07.20.500902.full.pdf,431.00,Evolutionary-scale prediction of atomic-level protein structure with a language model,2022-10-31,"Meta AI,New York University (NYU)",Industry - Academia Collaboration (Industry leaning),15000000000.00,"""we train models up to 15B parameters""",7.8e+22,"from the paper's Materials and Methods: 
""We trained each model over 512 NVIDIA V100 GPUs. ESM2 700M took 8 days to train. The 3B parameter LM took 30 days. The 15B model took 60 days.""

from Arb Research (https://arbresearch.com/files/gen_bio.pdf): ""ESM-2-15B: 270000 updates x 3.2M batch size x 15 B “connections” x 6. Alternatively, 60 days x 512 V100s x an imputed 30% utilization""

first method gets 7.8e22, the second ~1e23",UniRef50,"""UniRef50, September 2021 version, is used for the training of ESM models""",,,,,,1440.0,,,,Unsupervised,,,Industry,Likely,"""Artificial intelligence has the potential to open insight into the structure of proteins at the scale of evolution. It has only recently been possible to extend protein structure prediction to two hundred million cataloged proteins. Characterizing the structures of the exponentially growing billions of protein sequences revealed by large scale gene sequencing experiments would necessitate a breakthrough in the speed of folding. Here we show that direct inference of structure from primary sequence using a large language model enables an order of magnitude speed-up in high resolution structure prediction. Leveraging the insight that language models learn evolutionary patterns across millions of sequences, we train models up to 15B parameters, the largest language model of proteins to date. As the language models are scaled they learn information that enables prediction of the three-dimensional structure of a protein at the resolution of individual atoms. This results in prediction that is up to 60x faster than state-ofthe-art while maintaining resolution and accuracy. Building on this, we present the ESM Metagenomic Atlas. This is the first large-scale structural characterization of metagenomic proteins, with more than 617 million structures. The atlas reveals more than 225 million high confidence predictions, including millions whose structures are novel in comparison with experimentally determined structures, giving an unprecedented view into the vast breadth and diversity of the structures of some of the least understood proteins on earth.""",2023-11-26 03:25,Anonymous,,,"Multinational,United States of America","Meta AI, New York University (NYU)",,,,ESM-2,,,
Mogrifier RLSTM (WT2),Language,,Gábor Melis,SOTA Improvement,"""On top of these improvements, the RLSTM
outperformed the LSTM by a small margin, and we established a new state of the art on both datasets""",,https://arxiv.org/pdf/2211.01848,0.00,Circling Back to Recurrent Models of Language,2022-11-03,"DeepMind,University College London (UCL)",,35000000.00,,109000000000000000.00,,,,,,250.00,,,,,,,,,,,,,2023-11-26 03:25,Robi Rahman,Mogrifier RLSTM (WT2),,"United Kingdom of Great Britain and Northern Ireland,United Kingdom of Great Britain and Northern Ireland","DeepMind, University College London (UCL)",,,,Mogrifier RLSTM (WT2),,,
Mogrifier RLSTM (PTB),Language,,,,,,,,,2022-11-03,,,,,,,,,,,400.00,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,Mogrifier RLSTM (PTB),1,,,,,,,,,
BLOOM,Language,Language model,"Margaret Mitchell, Giada Pistilli, Yacine Jernite, Ezinwanne Ozoani, Marissa Gerchick, Nazneen Rajani, Sasha Luccioni, Irene Solaiman, Maraim Masoud, Somaieh Nikpoor, Carlos Muñoz Ferrandis, Stas Bekman, Christopher Akiki, Danish Contractor, David Lansky, Angelina McMillan-Major, Tristan Thrush, Suzana Ilić, Gérard Dupont, Shayne Longpre, Manan Dey, Stella Biderman, Douwe Kiela, Emi Baylor, Teven Le Scao, Aaron Gokaslan, Julien Launay, Niklas Muennighoff",Historical significance,"Was the largest open-source model at the time. 1000+ researchers, many from important orgs such as Microsoft and NVIDIA.",Permissive license,https://huggingface.co/bigscience/bloom,0.00,BigScience Large Open-science Open-access Multilingual Language Model,2022-11-08,"Hugging Face,BigScience",Research Collective,176000000000.00,,1.8e+23,"https://towardsdatascience.com/run-bloom-the-largest-open-access-ai-model-on-your-desktop-computer-f48e1e2a9a32

384 A100 GPUs * 116 days","""TB scale multilingual dataset""","In total, 1.6 terabytes of pre-processed text was converted into 350 billion unique tokens as BLOOM's training datasets.
 arXiv:2210.15424",262500000000,,,,,,,,,Self-supervised learning,,,,,,2023-11-26 03:25,Robi Rahman,,,"Multinational,Multinational","Hugging Face, BigScience",,,,BLOOM,,,
AltCLIP,Multimodal,,"Zhongzhi Chen, Guang Liu, Bo-Wen Zhang, Fulong Ye, Qinghong Yang, Ledell Wu",SOTA Improvement,"""We set new state-of-the-art performances on a bunch of tasks including ImageNet-CN, Flicker30kCN, COCO-CN and XTD""",,https://arxiv.org/abs/2211.06679,29.00,AltCLIP: Altering the Language Encoder in CLIP for Extended Language Capabilities,2022-11-12,Beijing Academy of Artificial Intelligence,Academia,,,,,,,,,10.00,,,,,,,,,,,Likely,"In this work, we present a conceptually simple and effective method to train a strong bilingual/multilingual multimodal representation model. Starting from the pre-trained multimodal representation model CLIP released by OpenAI, we altered its text encoder with a pre-trained multilingual text encoder XLM-R, and aligned both languages and image representations by a two-stage training schema consisting of teacher learning and contrastive learning. We validate our method through evaluations of a wide range of tasks. We set new state-of-the-art performances on a bunch of tasks including ImageNet-CN, Flicker30k-CN, COCO-CN and XTD. Further, we obtain very close performances with CLIP on almost all tasks, suggesting that one can simply alter the text encoder in CLIP for extended capabilities such as multilingual understanding. Our models and code are available at this https URL.",2023-12-05 04:33,Anonymous,,,China,Beijing Academy of Artificial Intelligence,,,,AltCLIP,,,
Galactica,Language,Language modelling,"Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, Robert Stojnic",SOTA Improvement,"""We outperform existing models on a range of scientific tasks. On technical knowledge probes such as LaTeX equations, Galactica outperforms the latest GPT-3 by 68.2% versus 49.0%. Galactica also performs well on reasoning, outperforming Chinchilla on mathematical MMLU by 41.3% to 35.7%, and PaLM 540B on MATH""",,https://galactica.org/static/paper.pdf,283.00,Galactica: A Large Language Model for Science,2022-11-15,Meta AI,Industry,120000000000.00,"""The largest 120B model we train runs on a single NVIDIA A100 node""",3.24e+23,"Authors state the model is trained on 450b tokens. Using 6 FLOP/token/parameter, this is 6*120b*450b = 3.24e23",Galactica Corpus,,,"""Total dataset size = 106 billion tokens""",4.00,,,,,NVIDIA A100 SXM4 80 GB,NVIDIA A100 SXM4 80 GB,Self-supervised learning,,,,Likely,,2023-12-05 04:33,Robi Rahman,,,Multinational,Meta AI,,,,Galactica,,,
AR-LDM,Multimodal,Text-to-image,"Xichen Pan, Pengda Qin, Yuhong Li, Hui Xue, Wenhu Chen",SOTA Improvement,"The first latent diffusion model for coherent visual story synthesizing.
""Quantitative results show that AR-LDM achieves SoTA FID scores on PororoSV, FlintstonesSV, and the newly introduced challenging dataset VIST containing natural images""",,https://arxiv.org/abs/2211.10950,15.00,Synthesizing Coherent Story with Auto-Regressive Latent Diffusion Models,2022-11-20,"Alibaba,University of Waterloo,Vector Institute",Industry - Academia Collaboration (Industry leaning),1500000000.00,Table 1,510000000000000000000.00,8 NVIDIA A100 GPUs for 8 days,,,,"PororoSV, FlintstonesSV and VIST. All storytelling datasets, sizes would be possible to look up.",50.00,,,194.0,8 NVIDIA A100 GPUs for 8 days,NVIDIA A100,NVIDIA A100,,,,,Confident,"Conditioned diffusion models have demonstrated state-of-the-art text-to-image synthesis capacity. Recently, most works focus on synthesizing independent images; While for real-world applications, it is common and necessary to generate a series of coherent images for story-stelling. In this work, we mainly focus on story visualization and continuation tasks and propose AR-LDM, a latent diffusion model auto-regressively conditioned on history captions and generated images. Moreover, AR-LDM can generalize to new characters through adaptation. To our best knowledge, this is the first work successfully leveraging diffusion models for coherent visual story synthesizing. Quantitative results show that AR-LDM achieves SoTA FID scores on PororoSV, FlintstonesSV, and the newly introduced challenging dataset VIST containing natural images. Large-scale human evaluations show that AR-LDM has superior performance in terms of quality, relevance, and consistency.",2023-12-05 04:33,Anonymous,,,"China,Canada,Canada","Alibaba, University of Waterloo, Vector Institute",,,,AR-LDM,,,
CICERO,Games,Diplomacy,,SOTA Improvement,"""We introduce Cicero, the first AI agent to achieve human-level performance in Diplomacy""",,https://www.science.org/doi/10.1126/science.ade9097,57.00,Human-level play in the game of Diplomacy by combining language models with strategic reasoning,2022-11-22,Meta AI,Industry,,"""We took R2C2 (22) as our base model – a 2.7B parameter Transformer-based (23) encoder-decoder model pre-trained on text from the Internet using a BART de-noising objective (24).""",,,WebDiplomacy,,,"""We obtained a dataset of 125,261 games of Diplomacy played online at webDiplomacy.net. Of these, 40,408 games contained dialogue, with a total of 12,901,662 messages exchanged between players. Player accounts were de-identified and automated redaction of personally identifiable information (PII) was performed by webDiplomacy. We refer to this dataset hereafter as WebDiplomacy .""",,,,,,,,,,,,,,2023-11-26 03:25,Robi Rahman,,,Multinational,Meta AI,,,,CICERO,,,
ALM 1.0,Language,Language modelling,,SOTA Improvement,SOTA results on Arabic-language benchmark ALUE.,,https://github.com/FlagAI-Open/FlagAI/blob/master/examples/ALM/README.md,0.00,ALM 1.0,2022-11-28,Beijing Academy of Artificial Intelligence,Academia,,,,,,,,,,,,,,,,,,,,Speculative,,2023-11-26 03:25,Anonymous,,,China,Beijing Academy of Artificial Intelligence,,,,ALM 1.0,,,
GPT-3.5 (text-davinci-003),Language,Language modelling,,"SOTA Improvement,Historical significance,Significant use",,API accessible,https://platform.openai.com/docs/models/gpt-3-5,,,2022-11-28,OpenAI,Industry,,"Parameter count may be 175B based on OpenAI's statements that text-davinci-003 is in the GPT-3.5 series of models. It was also stated to be 175B in the Microsoft CODEFUSION paper, but the paper was reportedly retracted because the authors did not know the parameter count.",2.578e+24,https://colab.research.google.com/drive/1QSxa8YCWjEBQU7mrXLhw6TP1VX5oqgdW#scrollTo=Gt6Z6oZ26clI,,,,,,,,,,NVIDIA A100 SXM4 40 GB,NVIDIA A100 SXM4 40 GB,Reinforcement learning,,,Industry,Speculative,,2023-12-02 08:18,Anonymous,,,United States of America,OpenAI,,,,GPT-3.5 (text-davinci-003),,,GPT-3.5 (text-davinci-003)
DiT-XL/2 + Discriminator Guidance,Vision,Image generation,"Dongjun Kim, Yeongmin Kim, Se Jung Kwon, Wanmo Kang, Il-Chul Moon",SOTA Improvement,"""Using our algorithm, we achive state-of-the-art results on ImageNet 256x256 with FID 1.83 and recall 0.64, similar to the validation data's FID (1.68) and recall (0.66)""",,https://arxiv.org/abs/2211.17091v4,28.00,Refining Generative Process with Discriminator Guidance in Score-based Diffusion Models,2022-11-28,"Korea Advanced Institute of Science and Technology (KAIST),NAVER",Industry - Academia Collaboration (Industry leaning),,,,,,,,,7.00,,,,,NVIDIA A100,NVIDIA A100,,,,,Confident,"The proposed method, Discriminator Guidance, aims to improve sample generation of pre-trained diffusion models. The approach introduces a discriminator that gives explicit supervision to a denoising sample path whether it is realistic or not. Unlike GANs, our approach does not require joint training of score and discriminator networks. Instead, we train the discriminator after score training, making discriminator training stable and fast to converge. In sample generation, we add an auxiliary term to the pre-trained score to deceive the discriminator. This term corrects the model score to the data score at the optimal discriminator, which implies that the discriminator helps better score estimation in a complementary way. Using our algorithm, we achive state-of-the-art results on ImageNet 256x256 with FID 1.83 and recall 0.64, similar to the validation data's FID (1.68) and recall (0.66). We release the code at this https URL.",2023-11-26 03:25,Anonymous,,,"Korea (Republic of),Korea (Republic of)","Korea Advanced Institute of Science and Technology (KAIST), NAVER",,,,DiT-XL/2 + Discriminator Guidance,,,
Discriminator Guidance,Vision,Image generation,"Dongjun Kim, Yeongmin Kim, Se Jung Kwon, Wanmo Kang, Il-Chul Moon",SOTA Improvement,"""Using our algorithm, we achive state-of-the-art results on ImageNet 256x256 with FID 1.83 and recall 0.64, similar to the validation data's FID (1.68) and recall (0.66).""
https://paperswithcode.com/paper/refining-generative-process-with",,https://arxiv.org/abs/2211.17091v4,28.00,Refining Generative Process with Discriminator Guidance in Score-based Diffusion Models,2022-11-28,"Korea Advanced Institute of Science and Technology (KAIST),NAVER",,,,215700000010000000000.00,481 hours * 312 TFLOPS (A100) * 40% utilization,,,,,,,,481.0,Table 6,NVIDIA A100 PCIe,NVIDIA A100 PCIe,,,,,Likely,"The proposed method, Discriminator Guidance, aims to improve sample generation of pre-trained diffusion models. The approach introduces a discriminator that gives explicit supervision to a denoising sample path whether it is realistic or not. Unlike GANs, our approach does not require joint training of score and discriminator networks. Instead, we train the discriminator after score training, making discriminator training stable and fast to converge. In sample generation, we add an auxiliary term to the pre-trained score to deceive the discriminator. This term corrects the model score to the data score at the optimal discriminator, which implies that the discriminator helps better score estimation in a complementary way. Using our algorithm, we achive state-of-the-art results on ImageNet 256x256 with FID 1.83 and recall 0.64, similar to the validation data's FID (1.68) and recall (0.66).",2023-12-07 05:47,Anonymous,,0,"Korea (Republic of),Korea (Republic of)","Korea Advanced Institute of Science and Technology (KAIST), NAVER",,,,,,,
ChatGPT (gpt-3.5-turbo),Language,,,"Historical significance,Significant use",https://www.reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01/,,,,,2022-11-30,OpenAI,Industry,20000000000.00,20B parameters according to Table 1 in Microsoft's CODEFUSION paper: https://arxiv.org/pdf/2310.17680.pdf,,,,,,,,,,,,,,,,,Industry,Speculative,,2023-11-26 03:25,Anonymous,,,United States of America,OpenAI,,,,ChatGPT (gpt-3.5-turbo),,,
Transformer + GFM,Language,,"Hao Yu, Jianxin Wu",,,,https://cs.nju.edu.cn/wujx/paper/AAAI2023_AFM.pdf,0.00,"Compressing Transformers: Features Are Low-Rank, but Weights Are Not",2022-12-01,,,185000000.00,,8039999999999999000.00,,,,,,,,,,,,,,,,,,,2023-11-23 06:53,Robi Rahman,Transformer + GFM,,,,,,,,,,
ZymCTRL,Language,Protein generation,"Geraldene Munsamy, Sebastian Lindner, Philipp Lorenz, Noelia Ferruz",,,,https://www.mlsb.io/papers_2022/ZymCTRL_a_conditional_language_model_for_the_controllable_generation_of_artificial_enzymes.pdf,3.00,ZymCTRL: a conditional language model for the controllable generation of artificial enzymes,2022-12-01,"Basecamp Research,Friedrich-Alexander-Universität,University of Girona",Industry - Academia Collaboration (Industry leaning),738000000.00,"""ZymCTRL contains 36 layers totalling 738M parameters""",5.05e+21,"""We trained for 179,000 steps on 48 NVIDIA A100s 80GB for about 15,000 GPU hours""

15000  * 3600 * 312 teraFLOPS * 0.3 (utilization assumption) = 5.05e21",BRENDA,"""ZymCTRL was trained on the BRENDA database, a dataset of 37M enzyme sequences classified according to their enzymatic class""",,,8.00,,,,,NVIDIA A100,NVIDIA A100,,,,,Likely,"""The design of custom-tailored proteins has the potential to provide novel and
groundbreaking solutions in many fields, including molecular medicine or environmental sciences. Among protein classes, enzymes are particularly attractive because their complex active sites can accelerate chemical transformations by several orders of magnitude. Since enzymes are biodegradable nanoscopic materials, they hold an unmatched promise as sustainable, large-scale industrial catalysts. Motivated by the enormous success of language models in designing novel yet nature-like proteins, we hypothesised that an enzyme-specific language model could provide new opportunities to design purpose-built artificial enzymes. Here, we describe ZymCTRL, a conditional language model trained on the BRENDA database of enzymes, which generates enzymes of a specific enzymatic class upon a user prompt. ZymCTRL generates artificial enzymes distant from natural ones while their intended functionality matches predictions from orthogonal methods. We release the model to the community.""",2023-11-23 06:53,Anonymous,,,"United Kingdom of Great Britain and Northern Ireland,Germany,Spain","Basecamp Research, Friedrich-Alexander-Universität, University of Girona",,,,,,,
TransformerXL + FWL,Language,,"Kevin Clark, Kelvin Guu, Ming-Wei Chang, Panupong Pasupat, Geoffrey Hinton, Mohammad Norouzi",,,,https://web.archive.org/web/20221207113900/https://arxiv.org/pdf/2212.02475.pdf,4.00,Meta-Learning Fast Weight Language Models,2022-12-05,,,257000000.00,,,,,,,,,,,,,,,,,,,,,2023-12-05 04:33,Robi Rahman,TransformerXL + FWL,,,,,,,,,,
OPT-IML (175B),Language,Language modelling,"Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov, Daniel Simig, Ping Yu, Kurt Shuster, Tianlu Wang, Qing Liu, Punit Singh Koura, Xian Li, Brian O'Horo, Gabriel Pereyra, Jeff Wang, Christopher Dewan, Asli Celikyilmaz, Luke Zettlemoyer, Ves Stoyanov",,,Permissive license,https://arxiv.org/abs/2212.12017,140.00,OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization,2022-12-22,Meta AI,Industry,175000000000.00,,,fine-tuned from OPT-175B with an estimate 2.1e21 FLOP for fine-tuning,OPT-IML Bench,"(fine-tuning dataset) ""To this end, we create OPT-IML Bench: a large benchmark for Instruction MetaLearning (IML) of 2000 NLP tasks consolidated into task categories from 8 existing benchmarks""",,,,,,,,NVIDIA A100,NVIDIA A100,,,,,Likely,"Recent work has shown that fine-tuning large pre-trained language models on a collection of tasks described via instructions, a.k.a. instruction-tuning, improves their zero and few-shot generalization to unseen tasks. However, there is a limited understanding of the performance trade-offs of different decisions made during the instruction-tuning process. These decisions include the scale and diversity of the instruction-tuning benchmark, different task sampling strategies, fine-tuning with and without demonstrations, training using specialized datasets for reasoning and dialogue, and finally, the fine-tuning objectives themselves. In this paper, we characterize the effect of instruction-tuning decisions on downstream task performance when scaling both model and benchmark sizes. To this end, we create OPT-IML Bench: a large benchmark for Instruction Meta-Learning (IML) of 2000 NLP tasks consolidated into task categories from 8 existing benchmarks, and prepare an evaluation framework to measure three types of model generalizations: to tasks from fully held-out categories, to held-out tasks from seen categories, and to held-out instances from seen tasks. Through the lens of this framework, we first present insights about instruction-tuning decisions as applied to OPT-30B and further exploit these insights to train OPT-IML 30B and 175B, which are instruction-tuned versions of OPT. OPT-IML demonstrates all three generalization abilities at both scales on four different evaluation benchmarks with diverse tasks and input formats -- PromptSource, FLAN, Super-NaturalInstructions, and UnifiedSKG. Not only does it significantly outperform OPT on all benchmarks but is also highly competitive with existing models fine-tuned on each specific benchmark. We release OPT-IML at both scales, together with the OPT-IML Bench evaluation framework.",2023-12-05 04:33,Anonymous,,0,Multinational,Meta AI,OPT-175B,2,"""We fine-tune all 30B models on 64 40GB A100s, and 175B models on 128 40GB A100s"", no timeframe specified

fine-tuned on 2B tokens. 2B * 175B * 6 = 2.1e21

",,,,
Hybrid H3-2.7B,Language,,"Daniel Y. Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, Christopher Ré",SOTA Improvement,Results table shows SOTA performance for some benchmarks,,https://arxiv.org/abs/2212.14052,52.00,Hungry Hungry Hippos: Towards Language Modeling with State Space Models,2022-12-28,"Stanford University,University at Buffalo",,2700000000.00,,849000000000000000000.00,,,,,,509.02,,,,,,,,,,,,,2023-12-05 04:33,Robi Rahman,Hybrid H3-2.7B,0,"United States of America,United States of America","Stanford University, University at Buffalo",,,,Hybrid H3-2.7B,,,
Hybrid H3-125M,Language,,,,,,https://arxiv.org/abs/2212.14052,52.00,Hungry Hungry Hippos: Towards Language Modeling with State Space Models,2022-12-28,,,,,,,,,,,509.02,,,,,,,,,,,Unverified,,2023-12-05 04:33,Robi Rahman,Hybrid H3-125M,1,,,,,,,,,
Hybrid H3-355M,Language,,,,,,https://arxiv.org/abs/2212.14052,52.00,Hungry Hungry Hippos: Towards Language Modeling with State Space Models,2022-12-28,,,,,,,,,,,509.02,,,,,,,,,,,Unverified,,2023-12-05 04:33,Robi Rahman,Hybrid H3-355M,1,,,,,,,,,
Hybrid H3-1.3B,Language,,,,,,https://arxiv.org/abs/2212.14052,52.00,Hungry Hungry Hippos: Towards Language Modeling with State Space Models,2022-12-28,,,,,,,,,,,509.02,,,,,,,,,,,Unverified,,2023-12-05 04:33,Robi Rahman,Hybrid H3-1.3B,1,,,,,,,,,
SparseOPT-175B,Language,,"Elias Frantar, Dan Alistarh",,,,https://arxiv.org/abs/2301.00774,91.00,SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot,2023-01-02,,,87500000000.00,,1.58e+23,,,,,,1.67,,,,,,,,,,,,,2023-12-05 04:33,Robi Rahman,SparseOPT-175B,,,,,,,,,,
SparseOPT-66B,Language,,,,,,,,,2023-01-02,,,,,,,,,,,1.67,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,SparseOPT-66B,1,,,,,,,,,
SparseOPT-13B,Language,,,,,,,,,2023-01-02,,,,,,,,,,,1.67,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,SparseOPT-13B,1,,,,,,,,,
SparseOPT-30B,Language,,,,,,,,,2023-01-02,,,,,,,,,,,1.67,,,,,,,,,,,Unverified,,2023-12-08 01:01,Robi Rahman,SparseOPT-30B,1,,,,,,,,,
SantaCoder,Language,Code generation,"Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, Logesh Kumar Umapathi, Carolyn Jane Anderson, Yangtian Zi, Joel Lamy Poirier, Hailey Schoelkopf, Sergey Troshin, Dmitry Abulkhanov, Manuel Romero, Michael Lappert, Francesco De Toni, Bernardo García del Río, Qian Liu, Shamik Bose, Urvashi Bhattacharyya, Terry Yue Zhuo, Ian Yu, Paulo Villegas, Marco Zocca, Sourab Mangrulkar, David Lansky, Huu Nguyen, Danish Contractor, Luis Villa, Jia Li, Dzmitry Bahdanau, Yacine Jernite, Sean Hughes, Daniel Fried, Arjun Guha, Harm de Vries, Leandro von Werra",,"""Our best model outperforms previous open-source multilingual code generation models (InCoder-6.7B and CodeGen-Multi-2.7B) in both left-to-right generation and infilling on the Java, JavaScript, and Python portions of MultiPL-E, despite being a substantially smaller model""",Permissive license,https://arxiv.org/abs/2301.03988,61.00,SantaCoder: don't reach for the stars!,2023-01-09,"Hugging Face,ServiceNow,Massachusetts Institute of Technology (MIT),Wellesley College,Saama,EleutherAI,Huawei Noah's Ark Lab,Carnegie Mellon University (CMU)",Industry - Academia Collaboration,1100000000.00,1.1B,2.1e+21,Each training run takes 3.1 days to complete on 96 Tesla V100 GPUs for a total of 1.05 × 10^21 FLOPs. The final model described in Section 6.2 uses twice the amount of compute.,The Stack v1.1,"""The base training dataset for the experiments in this paper contains 268 GB of Python,
Java and JavaScript files from The Stack v1.1 (Kocetkov et al., 2022) after removing data from optout requests, near-deduplication, PII-redaction (see Section 4)""",,268 GB,,,,150.0,"Their initial training runs took 3.1 days. The final training run was run for twice as many iterations with ""all other hyper-parameters the same"" and used twice as much compute as this. So likely 6 days or ~150 hours, but they don't explicitly say whether they used the same hardware.",NVIDIA V100,NVIDIA V100,,,,,Likely,"The BigCode project is an open-scientific collaboration working on the responsible development of large language models for code. This tech report describes the progress of the collaboration until December 2022, outlining the current state of the Personally Identifiable Information (PII) redaction pipeline, the experiments conducted to de-risk the model architecture, and the experiments investigating better preprocessing methods for the training data. We train 1.1B parameter models on the Java, JavaScript, and Python subsets of The Stack and evaluate them on the MultiPL-E text-to-code benchmark. We find that more aggressive filtering of near-duplicates can further boost performance and, surprisingly, that selecting files from repositories with 5+ GitHub stars deteriorates performance significantly. Our best model outperforms previous open-source multilingual code generation models (InCoder-6.7B and CodeGen-Multi-2.7B) in both left-to-right generation and infilling on the Java, JavaScript, and Python portions of MultiPL-E, despite being a substantially smaller model. All models are released under an OpenRAIL license at this https URL.",2023-12-05 04:33,Anonymous,,0,"Multinational,United States of America,United States of America,United States of America,United States of America,Multinational,China,United States of America","Hugging Face, ServiceNow, Massachusetts Institute of Technology (MIT), Wellesley College, Saama, EleutherAI, Huawei Noah's Ark Lab, Carnegie Mellon University (CMU)",,,,,,,
Ankh,Language,"Protein generation,Proteins","Ahmed Elnaggar, Hazem Essam, Wafaa Salah-Eldin, Walid Moustafa, Mohamed Elkerdawy, Charlotte Rochereau, Burkhard Rost",SOTA Improvement,"""On average, Ankh improved the PLM SOTA performance by 4.8%""",,https://www.biorxiv.org/content/10.1101/2023.01.16.524265v1,8.00,Ankh: Optimized Protein Language Model Unlocks General-Purpose Modelling,2023-01-18,"Technical University of Munich,Columbia University",Industry - Academia Collaboration (Industry leaning),1150000000.00,See figure 1,,,UniRef50,"""We build upon the same results by pre-training our baseline on UniRef50.""",,"45M proteins and 14B amino acids, per Table 2",68.00,,,,,Google TPU v4,Google TPU v4,,,,,Likely,"""As opposed to scaling-up protein language models (PLMs), we seek improving performance via protein-specific optimization. Although the proportionality between the language model size and the richness of its learned representations is validated, we prioritize accessibility and pursue a path of data-efficient, cost-reduced, and knowledge-guided optimization. Through over twenty experiments ranging from masking, architecture, and pre-training data, we derive insights from protein-specific experimentation into building a model that interprets the language of life, optimally. We present Ankh, the first general-purpose PLM trained on Google’s TPU-v4 surpassing the state-of-the-art performance with fewer parameters (<10% for pre-training, <7% for inference, and <30% for the embedding dimension). We provide a representative range of structure and function benchmarks where Ankh excels. We further provide a protein variant generation analysis on High-N and OneN input data scales where Ankh succeeds in learning protein evolutionary conservation-mutation trends and introducing functional diversity while retaining key structural-functional characteristics. We dedicate our work to promoting accessibility to research innovation via attainable resources.""",2023-11-26 03:25,Anonymous,,,"Germany,United States of America","Technical University of Munich, Columbia University",,,,Ankh,,,
GPT-2+Active-SGD,Language,,"Davood Wadi, Marc Fredette, Sylvain Senecal",,,,https://arxiv.org/abs/2301.10133,0.00,Read the Signs Towards Invariance to Gradient Descent’s Hyperparameter Initialization,2023-01-24,,,124000000.00,,310000000000000000.00,,,,,,200.00,,,,,,,,,,,,,2023-11-23 06:53,Robi Rahman,GPT-2+Active-SGD,,,,,,,,,,
mini-GPT-2+Active-AdamW,Language,,,,,,,,,2023-01-24,,,,,,,,,,,200.00,,,,,,,,,,,Unverified,,2023-12-08 01:01,Robi Rahman,mini-GPT-2+Active-AdamW,1,,,,,,,,,
DDPM-IP (CelebA),Vision,Image generation,"Mang Ning, Enver Sangineto, Angelo Porrello, Simone Calderara, Rita Cucchiara",SOTA Improvement,"""For instance, on CelebA 64×64, we achieve a new state-of-theart FID score of 1.27, while saving 37.5% of the training time""",,https://arxiv.org/abs/2301.11706v3,14.00,Input Perturbation Reduces Exposure Bias in Diffusion Models,2023-01-27,Utrecht University,Academia,295000000.00,"295M for CelebA model, per Table 9",350000000000000000000.00,"""We use Pytorch 1.8 (Paszke et al., 2019) and trained all the models on different NVIDIA Tesla V100s (16G memory). In
more detail, we use 2 GPUs to train the models on CIFAR10 for 2 days, and 4 GPUs to train the models on ImageNet 32×32
for 34 days. For LSUN tower 64×64, CelebA 64×64 and FFHQ 128×128, we used 16 GPUs to train the models for 3 days,
5 days and 4 days, respectively""

5*16 V100-days for CelebA.

5 * 16 * 24 * 3600 * 125 teraflops * 0.4 ~= 3.5e20",CelebA,,203000,,681.00,,,120.0,5 days,NVIDIA V100,NVIDIA V100,,,,,Likely,"Denoising Diffusion Probabilistic Models have shown an impressive generation quality, although their long sampling chain leads to high computational costs. In this paper, we observe that a long sampling chain also leads to an error accumulation phenomenon, which is similar to the exposure bias problem in autoregressive text generation. Specifically, we note that there is a discrepancy between training and testing, since the former is conditioned on the ground truth samples, while the latter is conditioned on the previously generated results. To alleviate this problem, we propose a very simple but effective training regularization, consisting in perturbing the ground truth samples to simulate the inference time prediction errors. We empirically show that, without affecting the recall and precision, the proposed input perturbation leads to a significant improvement in the sample quality while reducing both the training and the inference times. For instance, on CelebA 64×64, we achieve a new state-of-the-art FID score of 1.27, while saving 37.5% of the training time. The code is publicly available at this https URL",2023-12-05 04:33,Anonymous,,,Netherlands,Utrecht University,,,,DDPM-IP (CelebA),,,
UniPi,Multimodal,Video generation,"Yilun Du, Mengjiao Yang, Bo Dai, Hanjun Dai, Ofir Nachum, Joshua B. Tenenbaum, Dale Schuurmans, Pieter Abbeel",,,,https://arxiv.org/abs/2302.00111,36.00,Learning Universal Policies via Text-Guided Video Generation,2023-01-31,"Google DeepMind,Massachusetts Institute of Technology (MIT),UC Berkeley,Georgia Institute of Technology,University of Alberta",Industry - Academia Collaboration,,"Appears to be a composite model, not sure about the total parameter count.",,"256 TPUv4s, no timeframe specified",,"""Our training data consists of an internet-scale pretraining dataset and a smaller real-world
robotic dataset. The pretraining dataset uses the same data as [19], which consists of 14 million
video-text pairs, 60 million image-text pairs, and the publicly available LAION-400M image-text
dataset. The robotic dataset is adopted from the Bridge dataset [29] with 7.2k video-text pairs, where
we use the task IDs as texts. We partition the 7.2k video-text pairs into train (80%) and test (20%)
splits. We pretrain UniPi on the pretraining dataset followed by finetuning on the train split of the
Bridge data. """,,,,,,,,Google TPU v4,Google TPU v4,,,,,Likely,"A goal of artificial intelligence is to construct an agent that can solve a wide variety of tasks. Recent progress in text-guided image synthesis has yielded models with an impressive ability to generate complex novel images, exhibiting combinatorial generalization across domains. Motivated by this success, we investigate whether such tools can be used to construct more general-purpose agents. Specifically, we cast the sequential decision making problem as a text-conditioned video generation problem, where, given a text-encoded specification of a desired goal, a planner synthesizes a set of future frames depicting its planned actions in the future, after which control actions are extracted from the generated video. By leveraging text as the underlying goal specification, we are able to naturally and combinatorially generalize to novel goals. The proposed policy-as-video formulation can further represent environments with different state and action spaces in a unified space of images, which, for example, enables learning and generalization across a variety of robot manipulation tasks. Finally, by leveraging pretrained language embeddings and widely available videos from the internet, the approach enables knowledge transfer through predicting highly realistic video plans for real robots.",2023-12-05 04:33,Anonymous,,0,"Multinational,United States of America,United States of America,United States of America,Canada","Google DeepMind, Massachusetts Institute of Technology (MIT), UC Berkeley, Georgia Institute of Technology, University of Alberta",,,,,,,
CD-GraB (WT103),Language,,"A. Feder Cooper, Wentao Guo, Khiem Pham, Tiancheng Yuan, Charlie F. Ruan, Yucheng Lu, Christopher De Sa",,,,https://arxiv.org/pdf/2302.00845.pdf,2.00,CD-GraB: Coordinating Distributed Example Orders for Provably Accelerated Training,2023-02-02,,Industry,,,,,WikiText-103,,,,30.00,,,,,,,,,,,,,2023-12-05 04:33,Robi Rahman,CD-GraB (WT103),,,,,,,,,,
CD-GraB (WT2),Language,,,,,,,,,2023-02-02,,,,,,,,,,,50.00,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,CD-GraB (WT2),1,,,,,,,,,
Gen-1,Video,Video generation,"Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, Anastasis Germanidis",SOTA Improvement,,,https://arxiv.org/abs/2302.03011,128.00,Structure and Content-Guided Video Synthesis with Diffusion Models,2023-02-06,Runway,Industry,,,,,,,,,,,,,,,,,,,,Unverified,,2023-12-05 04:33,Anonymous,,,United States of America,Runway,,,,Gen-1,,,
BASIC-L + Lion,Vision,Image Classification,"Xiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Yao Liu, Hieu Pham, Xuanyi Dong, Thang Luong, Cho-Jui Hsieh, Yifeng Lu, Quoc V. Le",SOTA Improvement,"""On vision-language contrastive learning, we achieve 88.3% zero-shot and 91.1% fine-tuning accuracy on ImageNet, surpassing the previous best results by 2% and 0.1%, respectively""",,https://arxiv.org/abs/2302.06675v4,98.00,Symbolic Discovery of Optimization Algorithms,2023-02-13,"Google,University of California Los Angeles (UCLA)",Industry - Academia Collaboration (Industry leaning),3070000000.00,parameter count of original BASIC-L,,This model is an optimized version of BASIC-L. We estimated BASIC-L at 8e22 FLOP. Unclear how much optimization compute was required.,,,,,,,,,,,,,,,,Likely,"We present a method to formulate algorithm discovery as program search, and apply it to discover optimization algorithms for deep neural network training. We leverage efficient search techniques to explore an infinite and sparse program space. To bridge the large generalization gap between proxy and target tasks, we also introduce program selection and simplification strategies. Our method discovers a simple and effective optimization algorithm, Lion (EvoLved Sign Momentum). It is more memory-efficient than Adam as it only keeps track of the momentum. Different from adaptive optimizers, its update has the same magnitude for each parameter calculated through the sign operation. We compare Lion with widely used optimizers, such as Adam and Adafactor, for training a variety of models on different tasks. On image classification, Lion boosts the accuracy of ViT by up to 2% on ImageNet and saves up to 5x the pre-training compute on JFT. On vision-language contrastive learning, we achieve 88.3% zero-shot and 91.1% fine-tuning accuracy on ImageNet, surpassing the previous best results by 2% and 0.1%, respectively. On diffusion models, Lion outperforms Adam by achieving a better FID score and reducing the training compute by up to 2.3x. For autoregressive, masked language modeling, and fine-tuning, Lion exhibits a similar or better performance compared to Adam. Our analysis of Lion reveals that its performance gain grows with the training batch size. It also requires a smaller learning rate than Adam due to the larger norm of the update produced by the sign function. Additionally, we examine the limitations of Lion and identify scenarios where its improvements are small or not statistically significant. Lion is also successfully deployed in production systems such as Google search ads CTR model.",2023-12-05 04:33,Anonymous,,,"Multinational,United States of America","Google, University of California Los Angeles (UCLA)",,,,BASIC-L + Lion,,,
Anthropic LM 175B,Language,,"Deep Ganguli, Amanda Askell, Nicholas Schiefer, Thomas I. Liao, Kamilė Lukošiūtė, Anna Chen, Anna Goldie, Azalia Mirhoseini, Catherine Olsson, Danny Hernandez, Dawn Drain, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jackson Kernion, Jamie Kerr, Jared Mueller, Joshua Landau, Kamal Ndousse, Karina Nguyen, Liane Lovitt, Michael Sellitto, Nelson Elhage, Noemi Mercado, Nova DasSarma, Oliver Rausch, Robert Lasenby, Robin Larson, Sam Ringer, Sandipan Kundu, Saurav Kadavath, Scott Johnston, Shauna Kravec, Sheer El Showk, Tamera Lanham, Timothy Telleen-Lawton, Tom Henighan, Tristan Hume, Yuntao Bai, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, Christopher Olah, Jack Clark, Samuel R. Bowman, Jared Kaplan",,,Unreleased,https://arxiv.org/abs/2302.07459,49.00,The Capacity for Moral Self-Correction in Large Language Models,2023-02-15,Anthropic,,175000000000.00,175B,,,,,,,,,,,,,,Reinforcement learning,,,,Confident,"We test the hypothesis that language models trained with reinforcement learning from human feedback (RLHF) have the capability to ""morally self-correct"" -- to avoid producing harmful outputs -- if instructed to do so. We find strong evidence in support of this hypothesis across three different experiments, each of which reveal different facets of moral self-correction. We find that the capability for moral self-correction emerges at 22B model parameters, and typically improves with increasing model size and RLHF training. We believe that at this level of scale, language models obtain two capabilities that they can use for moral self-correction: (1) they can follow instructions and (2) they can learn complex normative concepts of harm like stereotyping, bias, and discrimination. As such, they can follow instructions to avoid certain kinds of morally harmful outputs. We believe our results are cause for cautious optimism regarding the ability to train language models to abide by ethical principles.",2023-12-07 06:12,Anonymous,,0,United States of America,Anthropic,,,,,,,
E-SPA,Language,,"Bobby He, James Martens, Guodong Zhang, Aleksandar Botev, Andrew Brock, Samuel L Smith, Yee Whye Teh",,,,https://arxiv.org/pdf/2302.10322.pdf,7.00,Deep Transformers without Shortcuts: Modifying Self-attention for Faithful Signal Propagation,2023-02-20,,,243000000.00,,,,,,,,,,,,,,,,,,,,,2023-12-05 04:33,Robi Rahman,E-SPA,,,,,,,,,,
Hyena-3-slim,Language,,"Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y. Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, Christopher Ré",,,,https://arxiv.org/pdf/2302.10866,50.00,Hyena Hierarchy: Towards Larger Convolutional Language Models,2023-02-21,,,125000000.00,,,,,,,,,,,,,,,,,,,,,2023-12-05 04:33,Robi Rahman,Hyena-3-slim,,,,,,,,,,
LLaMA-65B,Language,Language modelling,"Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample",Historical significance,Widely-used foundation model that has been adapted for others such as Alpaca.,Fully open-source,https://arxiv.org/abs/2302.13971,2695.00,LLaMA: Open and Efficient Foundation Language Models,2023-02-24,Meta AI,Industry,65200000000.00,"Model card, table 1: https://github.com/facebookresearch/llama/blob/53011c3d7946dadb8274a4c5c7586ab54edf792d/MODEL_CARD.md",5.5e+23,"1.4e12 tokens * 6.52e10 parameters * 6 FLOP/token/parameter = 5.5e23 FLOP

Compared to 2048 A100 GPUs each with 311.84 TFLOPS maximum performance for 21 days, this implies 47% utilization.
https://www.wolframalpha.com/input?i=5.5*10%5E23+FLOP+%2F+%282048+*+311.84+teraFLOPS+*+21+days%29","CCNet, GitHub, Wikipedia, books, arXiv, Stack Exchange","The model was trained using the following source of data: CCNet [67%], C4 [15%], GitHub [4.5%], Wikipedia [4.5%], Books [4.5%], ArXiv [2.5%], Stack Exchange[2%]. The Wikipedia and Books domains include data in the following languages: bg, ca, cs, da, de, en, es, fr, hr, hu, it, nl, pl, pt, ro, ru, sl, sr, sv, uk. See the paper for more details about the training set and corresponding preprocessing.",1050000000000,1.4 trillion tokens * 0.75 words/token = 1.05 trillion words,1.09,130000000000.00,per token,500.0,"""When training a 65B-parameter model, our code processes around 380 tokens/sec/GPU on 2048 A100 GPU with 80GB of RAM. This means that training over our dataset containing 1.4T tokens takes approximately 21 days.""",NVIDIA A100,NVIDIA A100,Supervised,$1179384.75,"1023384 processor-hours on A100 GPUs. May 2023 cost rate is $1.36/GPU-hour on Azure ML cloud. https://azure.microsoft.com/en-us/pricing/details/machine-learning/ 
According to https://www.bls.gov/data/inflation_calculator.htm, $1.18 in May 2023 = $1.00 in January 2020.
$1391674 / 1.18 = $1179385 in 2020 USD.",Industry,Likely,"We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.",2023-12-05 11:25,Anonymous,LLaMA-65B,0,Multinational,Meta AI,,,,LLaMA-65B,2048,0.4746,LLaMA-65B
LLaMA-7B,Language,Language modelling,"Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample","Significant use,Highly cited",,Fully open-source,https://arxiv.org/abs/2302.13971,2695.00,LLaMA: Open and Efficient Foundation Language Models,2023-02-24,Meta AI,,6700000000.00,"6.7B parameters, per table 2: https://arxiv.org/pdf/2302.13971.pdf",2.78e+22,"1T tokens * 6.7B parameters * 6 FLOP/token/parameter = 4e22 FLOP

from paper, Llama-7B took 82,432 GPU hours using A100s

312 trillion * 82,432 * 3600 * 0.3 = 2.78e22 FLOP","CCNet, GitHub, Wikipedia, books, arXiv, Stack Exchange",,750000000000,1 trillion tokens * 0.75 words/token = 750 billion words,1.09,,,,,NVIDIA A100,NVIDIA A100,,,,,Likely,"""We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.""",2023-12-05 04:33,Robi Rahman,LLaMA-7B,,Multinational,Meta AI,,,,LLaMA-7B,,,
CodeGen-Mono 16.1B,Language,"Code generation,Code autocompletion","Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, Caiming Xiong",,"Not as good as code-davinci-001 or code-davinci-002, per Table 1",Fully open-source,https://arxiv.org/abs/2203.13474,266.00,CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis,2023-02-27,Salesforce,Industry,16100000000.00,16.1B parameters,,,"The Pile, Big Query, BigPython","""The family of CODEGEN models is trained sequentially on three datasets: The Pile, BigQuery, and BigPython.""",,,,,,,,Google TPU v4,Google TPU v4,,,,,Likely,"""Program synthesis strives to generate a computer program as a solution to a given problem specification, expressed with input-output examples or natural language descriptions. The prevalence of large language models advances the state-of-the-art for program synthesis, though limited training resources and data impede open access to such models. To democratize this, we train and release a family of large language models up to 16.1B parameters, called CODEGEN, on natural language and programming language data, and open source the training library JAXFORMER. We show the utility of the trained model by demonstrating that it is competitive with the previous state-of-the-art on zero-shot Python code generation on HumanEval. We further investigate the multi-step paradigm for program synthesis, where a single program is factorized into multiple prompts specifying subproblems. To this end, we construct an open benchmark, Multi-Turn Programming Benchmark (MTPB), consisting of 115 diverse problem sets that are factorized into multi-turn prompts. Our analysis on MTPB shows that the same intent provided to CODEGEN in multi-turn fashion significantly improves program synthesis over that provided as a single turn. We make the training library JAXFORMER and model checkpoints available as open source contribution: this https URL.""",2023-12-05 04:33,Anonymous,,,United States of America,Salesforce,,,,,,,
LLaMA-13B,Language,,"Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample",,,,https://arxiv.org/abs/2302.13971,,LLaMA: Open and Efficient Foundation Language Models,2023-02-27,Meta AI,,13000000000.00,13.0B,4.55e+22,"1T tokens * 13B parameters * 6 FLOP/token/parameter = 7.8e22

from paper, Llama-7B took 135,168 GPU hours using A100s

312 trillion * 135,168 * 3600 * 0.3 = 4.55e22 FLOP",,,,,1.09,,,,,NVIDIA A100,NVIDIA A100,,,,,Likely,,2023-12-08 07:16,Robi Rahman,LLaMA-13B,1,Multinational,Meta AI,,,,,,,
LLaMA-30B,Language,,,,,,,,,2023-02-27,,,,,,,,,,,1.09,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,LLaMA-30B,1,,,,,,,,,
LLaMA-33B,Language,,,,,Fully open-source,,,,2023-02-27,,,33000000000.00,,,,,,,,1.09,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,LLaMA-33B,1,,,,,,,,,
Palmyra Large 20B,Language,Language modelling,,,,Permissive license,https://huggingface.co/Writer/palmyra-large,,,2023-03-01,Writer,Industry,20000000000.00,20B,9.6e+22,"""Palmyra-Large is a 20B parameters causal decoder-only model built by Writer and trained on +800B tokens of Palmyra-Index-Data enhanced with curated corpora.""

I'm not sure if the 800B is how many tokens the model was trained on, or the size of the dataset. But the dataset linked on HuggingFace has 1T tokens, so 800B as tokens trained is more likely.

20B*800B*6 = 9.6e22",,"Writer's custom dataset, English text",750000000000,"1 trillion tokens, or 750B words: https://huggingface.co/datasets/Writer/palmyra-data-index",0.80,,,,,,,,,,,Speculative,"Palmyra Large was primarily pre-trained with English text. Note that there is still a trace amount of non-English data present within the training corpus that was accessed through CommonCrawl. A causal language modeling (CLM) objective was utilized during the process of the model's pretraining. Similar to GPT-3, Palmyra Large is a member of the same family of models that only contain a decoder. As a result, it was pre-trained utilizing the objective of self-supervised causal language modeling.",2023-12-07 00:59,Anonymous,,0,,Writer,,,,,,,
Jurassic-2 Jumbo,Language,"Language modelling,Translation",,,,,https://www.ai21.com/blog/introducing-j2,,Announcing Jurassic-2 and Task-Specific APIs,2023-03-01,AI21 Labs,Industry,178000000000.00,Source is https://crfm.stanford.edu/helm/latest/#/leaderboard as viewed on 2023-12-06,,,,,,,,,,,,,,,,,Industry,,"Announcing the launch of Jurassic-2, the latest generation of AI21 Studio’s foundation models, a game-changer in the field of AI, with top-tier quality and new capabilities. And that's not all - we're also releasing our task-specific APIs, with plug-and-play reading and writing capabilities that outperform competitors.",2023-12-09 05:05,Anonymous,,0,Israel,AI21 Labs,,,,,,,
Palmyra Large (20B),,,,,,Permissive license,"https://writer.com/blog/palmyra/
",,"Palmyra LLMs empower secure, enterprise-grade generative AI for business",2023-03-02,Writer,,20000000000.00,20B parameters for Palmyra Large. There is also a 43B version called Palmyra X according to HELM.,9.600000000001e+22,"Assume 1 epoch. For Palmyra Large, C=6DN=6*800B*20B=9.6e22",,,600000000000,"""Palmyra-Large is a 20B parameters causal decoder-only model built by Writer and trained on +800B tokens of Palmyra-Index-Data enhanced with curated corpora.""
https://huggingface.co/Writer/palmyra-large",,,,,,,,,,,Industry,Confident,,2023-12-07 05:36,Anonymous,,0,,Writer,,,,,,,
Flan UL2,Language,Language Generation,,,"""We compare Flan-UL2 20B with other models in the Flan series. We report relative improvements over Flan-T5-XXL. Generally, Flan-UL2 outperforms Flan-T5 XXL on all four setups with an overall decent performance lift of +3.2% relative improvement""",Permissive license,"https://www.yitay.net/blog/flan-ul2-20b, https://arxiv.org/abs/2205.05131",99.00,A New Open Source Flan 20B with UL2,2023-03-03,Google,,19500000000.00,19.5B per https://www.yitay.net/blog/flan-ul2-20b,,,"C4, Flan",UL2 was pre-trained on C4. Flan UL2 is then instruction-tuned using the Flan dataset,,,,,,,,,,,,,,Likely,"Flan-UL2 is an encoder decoder model based on the T5 architecture. It uses the same configuration as the UL2 model released earlier last year. It was fine tuned using the ""Flan"" prompt tuning and dataset collection.",2023-12-14 08:46,Anonymous,,0,Multinational,Google,UL2,,"From paper (https://arxiv.org/pdf/2205.05131.pdf), they say that ""we opt to train UL2 for another 100K steps"" with Flan instruction tuning. Not sure what the batch size is here though. In other parts of the paper they use batch sizes of 128 or 1024.

Either way, small compared to pre-training compute.",,,,
PaLM-E,Robotics,,"Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, Pete Florence",SOTA Improvement,"""Our largest model, PaLM-E-562B with 562B parameters, in addition to being
trained on robotics tasks, is a visual-language generalist
with state-of-the-art performance on OK-VQA, and retains
generalist language capabilities with increasing scale.""",,https://arxiv.org/abs/2303.03378,424.00,PaLM-E: An Embodied Multimodal Language Model,2023-03-06,"Google,TU Berlin",Industry - Academia Collaboration (Industry leaning),562000000000.00,562B,,"Based on Palm-540B and ViT-22B and then trained on robotics data.

",,"""Our three robot environments (Fig. 1) include a Task and Motion Planning (TAMP) domain where a robot has to manipulate (grasp and stack) objects, a table-top pushing environment, and a mobile manipulation domain. In each domain, PaLM-E is trained on expert data from that domain. In many cases, this is a sparse amount of data per task. The TAMP tasks involve large combinatorics over possible plans, and many decision sequences are infeasible. PaLM-E has to generate plans that consist of multiple steps, with complicated decision boundaries. The multi-object tabletop pushing environment is taken from the publicly available Language-Table dataset (Lynch et al., 2022) and is challenging since it includes several objects, large cardinality of language, and complex pushing dynamics""",,,,,,,,,,,,,,Likely,"Large language models excel at a wide range of complex tasks. However, enabling general inference in the real world, e.g., for robotics problems, raises the challenge of grounding. We propose embodied language models to directly incorporate real-world continuous sensor modalities into language models and thereby establish the link between words and percepts. Input to our embodied language model are multi-modal sentences that interleave visual, continuous state estimation, and textual input encodings. We train these encodings end-to-end, in conjunction with a pre-trained large language model, for multiple embodied tasks including sequential robotic manipulation planning, visual question answering, and captioning. Our evaluations show that PaLM-E, a single large embodied multimodal model, can address a variety of embodied reasoning tasks, from a variety of observation modalities, on multiple embodiments, and further, exhibits positive transfer: the model benefits from diverse joint training across internet-scale language, vision, and visual-language domains. Our largest model, PaLM-E-562B with 562B parameters, in addition to being trained on robotics tasks, is a visual-language generalist with state-of-the-art performance on OK-VQA, and retains generalist language capabilities with increasing scale.",2023-12-05 04:33,Anonymous,,0,"Multinational,Germany","Google, TU Berlin",PaLM (540B),,"Based on Palm-540B and ViT 22B. No compute details given.

""We scale PaLM-E up to 562B parameters, integrating the 540B PaLM (Chowdhery et al., 2022) LLM and the 22B Vision Transformer (ViT) (Dehghani et al., 2023) into, to our knowledge, the largest vision-language model currently reported.""",,,,
Claude,Language,Language modelling,,"Historical significance,SOTA improvement",,,https://www.anthropic.com/index/introducing-claude,0.00,Introducing Claude,2023-03-14,Anthropic,Industry,,,,,,,,,,,,,,,,Reinforcement learning,,,,Unverified,"Claude is a next-generation AI assistant based on Anthropic’s research into training helpful, honest, and harmless AI systems. Accessible through chat interface and API in our developer console, Claude is capable of a wide variety of conversational and text processing tasks while maintaining a high degree of reliability and predictability.",2023-11-26 03:25,Anonymous,,,United States of America,Anthropic,,,,Claude,,,
GPT-4,Multimodal,Language modelling,OpenAI,SOTA Improvement,"See the paper, p.1: ""On a suite of traditional NLP benchmarks, GPT-4 outperforms both previous large language models and most state-of-the-art systems (which often have benchmark-specific training or hand-engineering).""",API accessible,https://arxiv.org/abs/2303.08774,1871.00,GPT-4 Technical Report,2023-03-15,OpenAI,Industry,,,2.1e+25,"90% CI: 8.2E+24 to 4.4E+25

NOTE: this is a rough estimate based on public information, much less information than most other systems in the database.

Calculation and confidence intervals here: https://colab.research.google.com/drive/1O99z9b1I5O66bT78r9ScslE_nOj5irN9?usp=sharing",,,,,,,,2280.0,(Speculative) SemiAnalysis conjectures that GPT-4 training took 90-100 days with utilization of 32-36%.,NVIDIA A100 SXM4 40 GB,NVIDIA A100 SXM4 40 GB,Self-supervised learning,,,Industry,Speculative,,2023-12-05 04:33,Robi Rahman,,0,United States of America,OpenAI,,,,GPT-4,25000,0.3400,GPT-4
Falcon-40B,Language,Language modelling,,Historical significance,,Fully open-source,https://www.tii.ae/news/abu-dhabi-based-technology-innovation-institute-introduces-falcon-llm-foundational-large,0.00,Abu Dhabi-based Technology Innovation Institute Introduces Falcon LLM: Foundational Large Language Model (LLM) outperforms GPT-3 with 40 Billion Parameters,2023-03-15,Technology Innovation Institute,Government,40000000000.00,Model comes in 7B and 40B variants.,2.4e+23,C = 6ND = 6 * 40B * 1000B = 2.4e+23 FLOP (assuming one epoch),RefinedWeb,"Falcon-40B was trained on 1,000B tokens of RefinedWeb, a high-quality filtered and deduplicated web dataset which we enhanced with curated corpora. Significant components from our curated copora were inspired by The Pile (Gao et al., 2020).",750000000000,1000B tokens ~= 750B words,,80000000000.00,80B FLOP per token,1440.0,"""Falcon-40B was trained on AWS SageMaker, on 384 A100 40GB GPUs in P4d instances.""
""Training started in December 2022 and took two months.""",NVIDIA A100,NVIDIA A100,,,,Academia,Confident,,2023-11-26 03:25,Anonymous,,,United Arab Emirates,Technology Innovation Institute,,,,Falcon-40B,,,
PanGu-Σ,Language,"Code generation,Language modelling","Xiaozhe Ren, Pingyi Zhou, Xinfan Meng, Xinjing Huang, Yadao Wang, Weichao Wang, Pengfei Li, Xiaoda Zhang, Alexander Podolskiy, Grigory Arshinov, Andrey Bout, Irina Piontkovskaya, Jiansheng Wei, Xin Jiang, Teng Su, Qun Liu, Jun Yao",SOTA Improvement,"""Our experimental findings show that PanGu-{\Sigma} provides state-of-the-art performance in zero-shot learning of various Chinese NLP downstream tasks.""",,https://arxiv.org/abs/2303.10845,22.00,PanGu-Σ: Towards Trillion Parameter Language Model with Sparse Heterogeneous Computing,2023-03-20,Huawei Noah's Ark Lab,Industry,1085000000000.00,"""In this work, we present PanGu-Σ , a large language model with sparse architecture containing 1.085 trillion parameters.""",4.669999999999999e+23,"It has sparse architecture, so we can't use C=6ND.
""We develop PanGu-Σ model under the framework of MindSpore and train it on a cluster with only 512 Ascend 910 AI Accelerators with 329 billion tokens over 100 days.""
100 days * 512 processors * 320 teraFLOPS/processor * 33% utilization = 4.67e+23 FLOP
https://www.wolframalpha.com/input?i=100+days+*+512+*+320+terahertz+*+0.33",,"""329B tokens in more than 40 natural and programming languages""",246750000000,329B tokens ~= 247B words,1.84,,,2400.0,"We develop PanGu-Σ model under the framework of MindSpore 5
and train it on a cluster with only 512 Ascend 910 AI Accelerators [28] with 329 billion tokens over 100 days.",Huawei Ascend 910,Huawei Ascend 910,Self-supervised learning,,,Industry,Confident,"The scaling of large language models has greatly improved natural language understanding, generation, and reasoning. In this work, we develop a system that trained a trillion-parameter language model on a cluster of Ascend 910 AI processors and MindSpore framework, and present the language model with 1.085T parameters named PanGu-{\Sigma}. With parameter inherent from PanGu-{\alpha}, we extend the dense Transformer model to sparse one with Random Routed Experts (RRE), and efficiently train the model over 329B tokens by using Expert Computation and Storage Separation(ECSS). This resulted in a 6.3x increase in training throughput through heterogeneous computing. Our experimental findings show that PanGu-{\Sigma} provides state-of-the-art performance in zero-shot learning of various Chinese NLP downstream tasks. Moreover, it demonstrates strong abilities when fine-tuned in application data of open-domain dialogue, question answering, machine translation and code generation.",2023-12-05 04:33,Anonymous,,,China,Huawei Noah's Ark Lab,,,,PanGu-Σ,,,
Sparse Wide GPT-3 Small,Language,,"Shreyas Saxena, Vithursan Thangarasa, Abhay Gupta, Sean Lie",,,,https://arxiv.org/pdf/2303.11525.pdf,2.00,Sparse Iso-FLOP Transformations for Maximizing Training Efficiency,2023-03-21,,,1300000000.00,,88400000000000000000.00,,,,,,110.00,,,,,,,,,,,,,2023-12-05 04:33,Robi Rahman,Sparse Wide GPT-3 Small,,,,,,,,,,
Vicuna-13B,Language,,,Historical significance,,Fully open-source,https://lmsys.org/blog/2023-03-30-vicuna/,0.00,Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality,2023-03-30,Large Model Systems Organization,Academia,13000000000.00,,,Might be possible to estimate training compute from the training cost. Fine-tuning cost $300.,,"70K conversations from ShareGPT.com, a website where users can share their ChatGPT conversations.",,,,,,,,,,,$259.00,"$300 in 2020, adjusted for inflation using BLS.gov inflation calculator",Academia,Speculative,,2023-11-26 03:25,Anonymous,,,United States of America,Large Model Systems Organization,,,,Vicuna-13B,,,
BloombergGPT,Language,Language modelling,"Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, Gideon Mann",SOTA Improvement,"""We validate BloombergGPT on standard LLM benchmarks, open financial benchmarks, and a suite of internal benchmarks that most accurately reflect our intended usage. Our mixed dataset training leads to a model that outperforms existing models on financial tasks by significant margins without sacrificing performance on general LLM benchmarks.""",,https://arxiv.org/abs/2303.17564,167.00,BloombergGPT: A Large Language Model for Finance,2023-03-30,"Bloomberg,Johns Hopkins University",Industry - Academia Collaboration (Industry leaning),50558868480.00,,2.3599999999999997e+23,"2.36e23 per Table 4

(using our usual hardware method, 512 A100s over 53 days would be 512 * 312 teraFLOP/s * 53 * 24 * 3600 * 0.3 = 2.19e23)",,"""To train BloombergGPT, we construct “FinPile”, a comprehensive dataset consisting of a range of English financial documents including news, filings, press releases, web-scraped financial documents, and social media drawn from the Bloomberg archives. These documents have been acquired through our business process over the past two decades. We augment FinPile with public data widely used to train LLMs. The result is a training corpus that is roughly half domain-specific text and half general-purpose text.""",532000000000,"708.9 billion tokens. At 0.75 English words per token, that's 532B words",0.80,,,1270.0,"""~53 days""",NVIDIA A100,NVIDIA A100,Self-supervised learning,,,,Confident,"The use of NLP in the realm of financial technology is broad and complex, with applications ranging from sentiment analysis and named entity recognition to question answering. Large Language Models (LLMs) have been shown to be effective on a variety of tasks; however, no LLM specialized for the financial domain has been reported in literature. In this work, we present BloombergGPT, a 50 billion parameter language model that is trained on a wide range of financial data. We construct a 363 billion token dataset based on Bloomberg's extensive data sources, perhaps the largest domain-specific dataset yet, augmented with 345 billion tokens from general purpose datasets. We validate BloombergGPT on standard LLM benchmarks, open financial benchmarks, and a suite of internal benchmarks that most accurately reflect our intended usage. Our mixed dataset training leads to a model that outperforms existing models on financial tasks by significant margins without sacrificing performance on general LLM benchmarks. Additionally, we explain our modeling choices, training process, and evaluation methodology. We release Training Chronicles (Appendix C) detailing our experience in training BloombergGPT.",2023-12-05 04:33,Anonymous,,,"United States of America,United States of America","Bloomberg, Johns Hopkins University",,,,BloombergGPT,,,
Pythia-12b,Language,,"Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, Oskar van der Wal",,,,https://arxiv.org/abs/2304.01373,222.00,Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling,2023-04-03,,,12000000000.00,,2.16e+22,,,,,,1.00,,,,,,,,,,,,,2023-12-05 04:33,Robi Rahman,Pythia-12b,,,,,,,,,,
Pythia-2.8b,Language,,,,,,,,,2023-04-03,,,,,,,,,,,1.00,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,Pythia-2.8b,1,,,,,,,,,
Pythia-6.9b,Language,,,,,,,,,2023-04-03,,,,,,,,,,,1.00,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,Pythia-6.9b,1,,,,,,,,,
Pythia-160m,Language,,,,,,,,,2023-04-03,,,,,,,,,,,1.00,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,Pythia-160m,1,,,,,,,,,
Pythia-1b,Language,,,,,,,,,2023-04-03,,,,,,,,,,,1.00,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,Pythia-1b,1,,,,,,,,,
Pythia-1.4b,Language,,,,,,,,,2023-04-03,,,,,,,,,,,1.00,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,Pythia-1.4b,1,,,,,,,,,
Pythia-70m,Language,,,,,,,,,2023-04-03,,,,,,,,,,,1.00,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,Pythia-70m,1,,,,,,,,,
Pythia-410m,Language,,,,,,,,,2023-04-03,,,,,,,,,,,1.00,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,Pythia-410m,1,,,,,,,,,
Cerebras-GPT-13B,Language,Language modelling,"Nolan Dey, Gurpreet Gosal, Zhiming (Charles)Chen, Hemant Khachane, William Marshall, Ribhu Pathria, Marvin Tom, Joel Hestness",,,,https://arxiv.org/abs/2304.03208,33.00,Cerebras-GPT: Open Compute-Optimal Language Models Trained on the Cerebras Wafer-Scale Cluster,2023-04-06,Cerebras Systems,Industry,13000000000.00,13 billion,2.3e+22,"2.3e22, per table 2",The Pile,,278000000000,"371B tokens, or 278B words",0.69,,,,,Cerebras CS-2,Cerebras CS-2,,,,,Confident,"We study recent research advances that improve large language models through efficient pre-training and scaling, and open datasets and tools. We combine these advances to introduce Cerebras-GPT, a family of open compute-optimal language models scaled from 111M to 13B parameters. We train Cerebras-GPT models on the Eleuther Pile dataset following DeepMind Chinchilla scaling rules for efficient pre-training (highest accuracy for a given compute budget). We characterize the predictable power-law scaling and compare Cerebras-GPT with other publicly-available models to show all Cerebras-GPT models have state-of-the-art training efficiency on both pre-training and downstream objectives. We describe our learnings including how Maximal Update Parameterization (μP) can further improve large model scaling, improving accuracy and hyperparameter predictability at scale. We release our pre-trained models and code, making this paper the first open and reproducible work comparing compute-optimal model scaling to models trained on fixed dataset sizes. Cerebras-GPT models are available on HuggingFace: this https URL.",2023-12-05 04:33,Anonymous,,0,Multinational,Cerebras Systems,,,,,,,
Incoder-6.7B,Language,Code generation,"Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Wen-tau Yih, Luke Zettlemoyer, Mike Lewis",SOTA Improvement,"""Zero-shot infilling with bidirectional context substantially outperforms approaches based on left-to-right-only models, and on several tasks
obtains performance comparable to state-of-the-art models fine-tuned on the tasks""",Permissive license,https://arxiv.org/abs/2204.05999,256.00,InCoder: A Generative Model for Code Infilling and Synthesis,2023-04-09,"Facebook AI Research,University of Washington,UC Berkeley,Carnegie Mellon University (CMU),Toyota Technological Institute at Chicago",Industry - Academia Collaboration,6700000000.00,6.7B,3.00001e+21,"per table 5, required 3 zettaflop (3e21) to train.

also, ""INCODER-6.7B was trained on 248 V100 GPUs for 24 days""

hardware method: 125 trillion * 248 * 24 * 24 * 3600 * 0.3 = 2e22. suggests their utilization was quite low, or 24 days was just calendar time.
",,"Code from GitHub and StackOverflow

""To train our models, we collect a corpus of (1) public code with permissive, non-copyleft, opensource licenses from GitHub and GitLab and (2) StackOverflow questions, answers, and comments.
Our primary focus in this paper is on the Python language, but we also include code files from
28 total languages and StackOverflow content from all available languages.""",,"216 GB: ""Our final pre-training corpus contains a total of 159 GB of code, 52 GB of it
in Python, and a total of 57 GB of content from StackOverflow""",1.00,,,576.0,24,NVIDIA V100,NVIDIA V100,,,,,Confident,"Code is seldom written in a single left-to-right pass and is instead repeatedly edited and refined. We introduce InCoder, a unified generative model that can perform program synthesis (via left-to-right generation) as well as editing (via infilling). InCoder is trained to generate code files from a large corpus of permissively licensed code, where regions of code have been randomly masked and moved to the end of each file, allowing code infilling with bidirectional context. Our model is the first generative model that is able to directly perform zero-shot code infilling, which we evaluate on challenging tasks such as type inference, comment generation, and variable re-naming. We find that the ability to condition on bidirectional context substantially improves performance on these tasks, while still performing comparably on standard program synthesis benchmarks in comparison to left-to-right only models pretrained at similar scale. The InCoder models and code are publicly released. this https URL",2023-12-05 04:33,Anonymous,,0,"Multinational,United States of America,United States of America,United States of America,United States of America","Facebook AI Research, University of Washington, UC Berkeley, Carnegie Mellon University (CMU), Toyota Technological Institute at Chicago",,,,,,,
SenseTime SenseNova,Language,Chat,,,,,https://www.sensetime.com/en/news-detail/51166397?categoryId=1072,,"SenseTime Launches “SenseNova” Foundation Model Sets and AI Computing Systems, Advancing AGI Development",2023-04-10,SenseTime,,,"hundreds of billions: ""Natural language serves as a crucial means of communication between humans and machines. “SenseNova” has introduced “SenseChat”, the latest large-scale language model (LLM) developed by SenseTime. As an LLM with hundreds of billions of parameters, SenseChat is trained using a vast amount of data, considering the Chinese context to better understand and process Chinese texts.""",,,,,,,,,,,,,,,,,,Likely,"SenseTime hosted a Tech Day event, sharing their strategic plan for advancing AGI (Artificial General Intelligence) development through the combination of “foundation models + large-scale computing” systems. Under this strategy, SenseTime unveiled the “SenseNova” foundation model set, introducing a variety of foundation models and capabilities in natural language processing, content generation, automated data annotation, and custom model training. At the event, SenseTime not only showcased their large language model’s capabilities, but also demonstrated a series of generative AI models and applications, such as text-to-image creation, 2D/3D digital human generation, and complex scenario/detailed object generation. Additionally, they introduced their AGI research and development platform facilitated by the integration of “foundation models + large-scale computing” systems.",2023-12-07 08:16,Anonymous,,0,Hong Kong,SenseTime,,,,,,,
Helpful Harmless preference model,Language,,"Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, Jared Kaplan",,,,https://arxiv.org/abs/2204.05862,521.00,Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback,2023-04-12,Anthropic,Industry,52000000000.00,,,,,"The 52B preference model was trained on a mixture of helpfulness and harmlessness (red teaming) datasets collected by Anthropic using crowdworkers conversing with language models in a feedback interface.
The helpfulness dataset contains around 44k comparisons and the harmlessness dataset contains around 42k comparisons. The data consists of multi-turn dialogues where crowdworkers choose the more helpful or more harmful response at each turn.
The model was trained using a technique called preference model pretraining on additional datasets before finetuning on Anthropic's human feedback data.",,,,,,,,,,Reinforcement learning,,,Industry,Confident,"We apply preference modeling and reinforcement learning from human feedback (RLHF) to finetune language models to act as helpful and harmless assistants. We find this alignment training improves performance on almost all NLP evaluations, and is fully compatible with training for specialized skills such as python coding and summarization. We explore an iterated online mode of training, where preference models and RL policies are updated on a weekly cadence with fresh human feedback data, efficiently improving our datasets and models. Finally, we investigate the robustness of RLHF training, and identify a roughly linear relation between the RL reward and the square root of the KL divergence between the policy and its initialization. Alongside our main results, we perform peripheral analyses on calibration, competing objectives, and the use of OOD detection, compare our models with human writers, and provide samples from our models using prompts appearing in recent related work.",2023-12-05 04:33,Anonymous,,,United States of America,Anthropic,,,,,,,
Claude 1.3,Language,Chat,,,100k context window may have been SOTA at the time.,,https://twitter.com/AnthropicAI/status/1648353600350060545?lang=en,,,2023-04-18,Anthropic,Industry,,,,,,,,,,,,,,,,,,,Industry,Speculative,,2023-11-21 01:39,Anonymous,,,United States of America,Anthropic,,,,,,,
MOSS-Moon-003,Language,Code generation,,,,Permissive license,https://huggingface.co/fnlp/moss-moon-003-base,,,2023-04-19,Fudan University,,16000000000.00,16B,6.669999999999999e+22,"6.67e22 including pre-training for CodeGen:

""The base language model of MOSS-003, which was initialized with CodeGen and further pre-trained on 100B Chinese tokens and 20B English tokens. The model has seen 700B tokens during pre-training and consumed ~6.67x10^22 FLOPs in total.""
",,,,,,,,,,,,,,,,Likely,"MOSS is an open-sourced plugin-augmented conversational language model. moss-moon models have 16B parameters, allowing users to perform inference on a single A100 GPU or 2 NVIDIA 3090 GPUs with FP16 precision, and on a single NVIDIA 3090 GPU with INT-4/8 precision. The base language model of MOSS was pre-trained on ~700B English, Chinese, and code tokens, including the PILE, BigQuery, BigPython, and our private Chinese corpus. The base model was then fine-tuned on multi-turn plugin-augmented conversational data. Finally, we performed preference-aware training to further improve the model.",2023-12-13 09:16,Anonymous,,0,China,Fudan University,CodeGen-Mono 16.1B,1,"""The base language model of MOSS-003, which was initialized with CodeGen and further pre-trained on 100B Chinese tokens and 20B English tokens. The model has seen 700B tokens during pre-training and consumed ~6.67x10^22 FLOPs in total.""


Using the proportion of tokens for fine-tuning against total tokens, we have:
6.67e22 * 120/(120+700) = 9.7e21.

However, the 6.67e22 might be *just* pre-training (this phrasing isn't clear). so that would be
6.67e22 * (120/700) = 1.14e22

alternatively, 16b * 120b * 6 = 1.15e22

I'll go with 1.14e22 but all these numbers are very close",,,,
WizardLM-7B,Language,Language modelling,"Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, Daxin Jiang",SOTA Improvement,"""Labelers prefer WizardLM outputs over outputs from ChatGPT under complex test instructions. On Evol-Instruct testset, WizardLM performs worse than ChatGPT, with a win
rate 12.8% lower than ChatGPT (28.0% vs. 40.8%). However, in the high-difficulty section
of Evol-Instruct test set (difficulty level ≥ 8), our WizardLM even outperforms ChatGPT,
with a win rate 7.9% larger than ChatGPT (42.9% vs. 35.0%), that is human annotators even
prefer the output of our model than ChatGPT on those hard questions""",Fully open-source,https://arxiv.org/abs/2304.12244,197.00,WizardLM: Empowering Large Language Models to Follow Complex Instructions,2023-04-24,"Microsoft,Peking University",Industry - Academia Collaboration (Industry leaning),6700000000.00,This is Llama-7b's parameter count,4.02e+22,"""We use pre-trained LLaMA 7B [4] to initialize our model. We adopt Adam optimizer as an initial learning rate of 2 ×10−5, a maximum number of tokens 2048, and the batch size is 8 for each GPU. We train our model on 8 V100 GPUs with Deepspeed Zero-3 for 70 hours on 3 epochs""

Llama-7b was ~4e22. 8*70 V100-hours is ~2e20, so fine-tuning was <1% of base training.",,"Fine-tuning dataset is made of LLM-generated instructions: ""In this work, we introduce Evol-Instruct, a novel method using LLMs instead of humans to automatically mass-produce open-domain instructions of various difficulty levels, to improve the performance of LLMs""",,,,,,,,"NVIDIA A100,NVIDIA V100","NVIDIA A100, NVIDIA V100",,,,,Likely,"Training large language models (LLMs) with open-domain instruction following data brings colossal success. However, manually creating such instruction data is very time-consuming and labor-intensive. Moreover, humans may struggle to produce high-complexity instructions. In this paper, we show an avenue for creating large amounts of instruction data with varying levels of complexity using LLM instead of humans. Starting with an initial set of instructions, we use our proposed Evol-Instruct to rewrite them step by step into more complex instructions. Then, we mix all generated instruction data to fine-tune LLaMA. We call the resulting model WizardLM. Human evaluations on a complexity-balanced test bed and Vicuna's testset show that instructions from Evol-Instruct are superior to human-created ones. By analyzing the human evaluation results of the high complexity part, we demonstrate that outputs from our WizardLM are preferred to outputs from OpenAI ChatGPT. In GPT-4 automatic evaluation, WizardLM achieves more than 90\% capacity of ChatGPT on 17 out of 29 skills. Even though WizardLM still lags behind ChatGPT in some aspects, our findings suggest that fine-tuning with AI-evolved instructions is a promising direction for enhancing LLMs. Our code and data are public at this https URL",2023-12-05 04:33,Anonymous,,,"Multinational,China","Microsoft, Peking University",,,,WizardLM-7B,,,
Agile Soccer Robot,Robotics,,"Tuomas Haarnoja, Ben Moran, Guy Lever, Sandy H. Huang, Dhruva Tirumala, Markus Wulfmeier, Jan Humplik, Saran Tunyasuvunakool, Noah Y. Siegel, Roland Hafner, Michael Bloesch, Kristian Hartikainen, Arunkumar Byravan, Leonard Hasenclever, Yuval Tassa, Fereshteh Sadeghi, Nathan Batchelor, Federico Casarini, Stefano Saliceti, Charles Game, Neil Sreendra, Kushal Patel, Marlon Gwira, Andrea Huber, Nicole Hurley, Francesco Nori, Raia Hadsell, Nicolas Heess",SOTA Improvement,"Likely the best bipedal soccer AI, since it's DeepMind, and related work section just discusses results involving specific soccer skills and quadruped robots:

""Whether bipedal or quadrupedal, navigation represents only a fraction of animal and human
capabilities. Motivated by this observation, there is a growing interest in whole body control, i.e.
tasks in which the whole body is used in flexible ways to interact with the environment. Examples
include climbing (Rudin et al., 2022a), getting-up from the ground (Ma et al., 2023), catching objects
(Ma et al., 2023), and mobile manipulation with legs (Cheng et al., 2023). Recently, reinforcement
learning has been applied to learn simple soccer skills, including goalkeeping (Huang et al., 2022),
ball manipulation on diverse terrains (Bohez et al., 2022; Ji et al., 2023), and shooting (Ji et al.,
2022). These works focus on a narrower set of skills than the 1v1 soccer game, and the quadrupedal
platform is inherently more stable and therefore presents an easier learning challenge.""",Unreleased,https://arxiv.org/abs/2304.13653,15.00,Learning Agile Soccer Skills for a Bipedal Robot with Deep Reinforcement Learning,2023-04-26,Google DeepMind,,,,,,,self-play training in simulation,,,,,,240.0,"14+158+68 hours:
""Training the get-up and soccer teachers took 14 and 158 hours (6.5 days), respectively, and distillation and self-play
took 68 hours (see Appendix B for details)""",,,Reinforcement learning,,,,Likely,"We investigate whether Deep Reinforcement Learning (Deep RL) is able to synthesize sophisticated and safe movement skills for a low-cost, miniature humanoid robot that can be composed into complex behavioral strategies in dynamic environments. We used Deep RL to train a humanoid robot with 20 actuated joints to play a simplified one-versus-one (1v1) soccer game. We first trained individual skills in isolation and then composed those skills end-to-end in a self-play setting. The resulting policy exhibits robust and dynamic movement skills such as rapid fall recovery, walking, turning, kicking and more; and transitions between them in a smooth, stable, and efficient manner - well beyond what is intuitively expected from the robot. The agents also developed a basic strategic understanding of the game, and learned, for instance, to anticipate ball movements and to block opponent shots. The full range of behaviors emerged from a small set of simple rewards. Our agents were trained in simulation and transferred to real robots zero-shot. We found that a combination of sufficiently high-frequency control, targeted dynamics randomization, and perturbations during training in simulation enabled good-quality transfer, despite significant unmodeled effects and variations across robot instances. Although the robots are inherently fragile, minor hardware modifications together with basic regularization of the behavior during training led the robots to learn safe and effective movements while still performing in a dynamic and agile way. Indeed, even though the agents were optimized for scoring, in experiments they walked 156% faster, took 63% less time to get up, and kicked 24% faster than a scripted baseline, while efficiently combining the skills to achieve the longer term objectives. Examples of the emergent behaviors and full 1v1 matches are available on the supplementary website.",2023-12-09 07:48,Anonymous,,0,Multinational,Google DeepMind,,,,,,,
OpenLLaMA-13B,Language,,"Xinyang Geng, Hao Liu",,,Permissive license,https://github.com/openlm-research/open_llama,,OpenLLaMA: An Open Reproduction of LLaMA,2023-05-01,OpenLM Research,,13000000000.00,13B,7.8e+22,13b * 1T * 6 = 7.8e22,RedPajama,RedPajama 1T: https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T,750000000000,"1T tokens, or ~750B words",1.00,,,,,Google TPU v4,Google TPU v4,,,resources provided by Google and Stability AI,Industry,Likely,"TL;DR: we are releasing our public preview of OpenLLaMA, a permissively licensed open source reproduction of Meta AI’s LLaMA. We are releasing a series of 3B, 7B and 13B models trained on different data mixtures. Our model weights can serve as the drop in replacement of LLaMA in existing implementations.

In this repo, we present a permissively licensed open source reproduction of Meta AI's LLaMA large language model. We are releasing a series of 3B, 7B and 13B models trained on 1T tokens. We provide PyTorch and JAX weights of pre-trained OpenLLaMA models, as well as evaluation results and comparison against the original LLaMA models. The v2 model is better than the old v1 model trained on a different data mixture.",2023-12-09 07:11,Anonymous,,0,United States of America,OpenLM Research,,,,,,,
Perfusion,Drawing,Text-to-image,"Yoad Tewel, Rinon Gal, Gal Chechik, Yuval Atzmon",,Pareto frontier performance but not SOTA,,https://arxiv.org/abs/2305.01644,28.00,Key-Locked Rank One Editing for Text-to-Image Personalization,2023-05-02,"NVIDIA,Tel Aviv University,Bar-Ilan University",Industry - Academia Collaboration (Industry leaning),,,,,,,,,,,,,,,,,,,Industry,Likely,"Text-to-image models (T2I) offer a new level of flexibility by allowing users to guide the creative process through natural language. However, personalizing these models to align with user-provided visual concepts remains a challenging problem. The task of T2I personalization poses multiple hard challenges, such as maintaining high visual fidelity while allowing creative control, combining multiple personalized concepts in a single image, and keeping a small model size. We present Perfusion, a T2I personalization method that addresses these challenges using dynamic rank-1 updates to the underlying T2I model. Perfusion avoids overfitting by introducing a new mechanism that ""locks"" new concepts' cross-attention Keys to their superordinate category. Additionally, we develop a gated rank-1 approach that enables us to control the influence of a learned concept during inference time and to combine multiple concepts. This allows runtime-efficient balancing of visual-fidelity and textual-alignment with a single 100KB trained model, which is five orders of magnitude smaller than the current state of the art. Moreover, it can span different operating points across the Pareto front without additional training. Finally, we show that Perfusion outperforms strong baselines in both qualitative and quantitative terms. Importantly, key-locking leads to novel results compared to traditional approaches, allowing to portray personalized object interactions in unprecedented ways, even in one-shot settings.",2023-12-05 04:33,Anonymous,,,"United States of America,Israel,Israel","NVIDIA, Tel Aviv University, Bar-Ilan University",,,,,,,
CodeGen2,Language,Code generation,"Erik Nijkamp, Hiroaki Hayashi, Caiming Xiong, Silvio Savarese, Yingbo Zhou",,,Permissive license,https://arxiv.org/abs/2305.02309,28.00,CodeGen2: Lessons for Training LLMs on Programming and Natural Languages,2023-05-03,Salesforce,Industry,16000000000.00,16B for largest CodeGen2 model,,,Stack v1.1,"""We examine our recipe on four model sizes: 1B, 3.7B, 7B, and 16B, and
refer to them as CodeGen2.
1 For training a subset of the Stack v1.1 (Kocetkov et al., 2022), filtered
with a stronger permissive license guideline, is used""",,,,,,,,,,,,,,Likely,"Large language models (LLMs) have demonstrated remarkable abilities in representation learning for program synthesis and understanding tasks. The quality of the learned representations appears to be dictated by the neural scaling laws as a function of the number of model parameters and observations, while imposing upper bounds on the model performance by the amount of available data and compute, which is costly.
In this study, we attempt to render the training of LLMs for program synthesis more efficient by unifying four key components: (1) model architectures, (2) learning methods, (3) infill sampling, and, (4) data distributions. Specifically, for the model architecture, we attempt to unify encoder and decoder-based models into a single prefix-LM. For learning methods, (i) causal language modeling, (ii) span corruption, (iii) infilling are unified into a simple learning algorithm. For infill sampling, we explore the claim of a ""free lunch"" hypothesis. For data distributions, the effect of a mixture distribution and multi-epoch training of programming and natural languages on model performance is explored.
We conduct a comprehensive series of empirical experiments on 1B LLMs, for which failures and successes of this exploration are distilled into five lessons. We will provide a final recipe for training and release CodeGen2 models in size 1B, 3.7B, 7B, and, 16B parameters, along with the training framework as open-source: this https URL.",2023-12-05 04:33,Anonymous,,,United States of America,Salesforce,,,,,,,
MPT-7B,Language,,MosaicML NLP Team,,,Fully open-source,https://www.mosaicml.com/blog/mpt-7b,,"Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable LLMs",2023-05-05,MosaicML,,7000000000.00,,4.2000000000000004e+22,,,,,,1.00,,,,,,,,,,,,,2023-12-12 03:39,Robi Rahman,MPT-7B,,United States of America,MosaicML,,,,,,,
StarCoder,Language,Code generation,"Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, João Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Muñoz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, Harm de Vries",SOTA Improvement,"""We perform the most comprehensive evaluation of Code LLMs to date and show that StarCoderBase outperforms every open Code LLM that supports multiple programming languages and matches or outperforms the OpenAI code-cushman-001 model. Furthermore, StarCoder outperforms every model that is fine-tuned on Python""",Fully open-source,https://arxiv.org/abs/2305.06161,150.00,StarCoder: may the source be with you!,2023-05-09,"Hugging Face,ServiceNow,Northeastern University,Mila- Quebec AI,Carnegie Mellon University (CMU),Johns Hopkins University,Leipzig University,ScaDS.AI,Queen Mary University of London,Roblox,Sea AI Lab,Technion - Israel Institute of Technology,Monash University,CSIRO,Data61,McGill University,Saama,University of British Columbia (UBC),Massachusetts Institute of Technology (MIT),Technical University of Munich,IBM,University of Vermont,UnfoldML,SAP,University of Notre Dame,Columbia University,New York University (NYU),University of Allahabad,Discover Dollar,Toloka,Telefonica,Stanford University,Weizmann Institute of Science,Alan Turing Institute,Wellesley College,EleutherAI,Forschungszentrum Julich",Industry - Academia Collaboration (Industry leaning),15500000000.00,"""We trained a 15.5B parameter model""",1.12e+23,"""We trained our model on a GPU cluster with 512 A100 80 GB GPUs... Based on the total number of GPU hours that training took (320,256) and an average power usage of 280W per GPU... The fine-tuned model adds 3.5% of training time""

320256 * 312 tFLOP/s * 3600 * 1.035 * 0.3 (utilization assumption) = 1.12e23",The Stack,"""StarCoderBase is trained on 1 trillion tokens sourced from The Stack (Kocetkov et al., 2022), a large collection of permissively licensed GitHub repositories with inspection tools and an opt-out process""",,"""StarCoderBase is trained on 1 trillion tokens sourced from The Stack""",1.00,,,,,NVIDIA A100 SXM4 80 GB,NVIDIA A100 SXM4 80 GB,,,,,Likely,"""The BigCode community, an open-scientific collaboration working on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder and StarCoderBase: 15.5B parameter models with 8K context length, infilling capabilities and fast large-batch inference enabled by multi-query attention. StarCoderBase is trained on 1 trillion tokens sourced from The Stack, a large collection of permissively licensed GitHub repositories with inspection tools and an opt-out process. We fine-tuned StarCoderBase on 35B Python tokens, resulting in the creation of StarCoder. We perform the most comprehensive evaluation of Code LLMs to date and show that StarCoderBase outperforms every open Code LLM that supports multiple programming languages and matches or outperforms the OpenAI code-cushman-001 model. Furthermore, StarCoder outperforms every model that is fine-tuned on Python, can be prompted to achieve 40\% pass@1 on HumanEval, and still retains its performance on other programming languages. We take several important steps towards a safe open-access model release, including an improved PII redaction pipeline and a novel attribution tracing tool, and make the StarCoder models publicly available under a more commercially viable version of the Open Responsible AI Model license.""",2023-12-05 04:33,Anonymous,,,"Multinational,United States of America,United States of America,Canada,United States of America,United States of America,Germany,Germany,United Kingdom of Great Britain and Northern Ireland,United States of America,Singapore,Israel,Australia,Australia,Australia,Canada,United States of America,Canada,United States of America,Germany,Multinational,United States of America,Sweden,Multinational,United States of America,United States of America,United States of America,India,India,Multinational,Spain,United States of America,Israel,United Kingdom of Great Britain and Northern Ireland,United States of America,Multinational,Germany","Hugging Face, ServiceNow, Northeastern University, Mila- Quebec AI, Carnegie Mellon University (CMU), Johns Hopkins University, Leipzig University, ScaDS.AI, Queen Mary University of London, Roblox, Sea AI Lab, Technion - Israel Institute of Technology, Monash University, CSIRO, Data61, McGill University, Saama, University of British Columbia (UBC), Massachusetts Institute of Technology (MIT), Technical University of Munich, IBM, University of Vermont, UnfoldML, SAP, University of Notre Dame, Columbia University, New York University (NYU), University of Allahabad, Discover Dollar, Toloka, Telefonica, Stanford University, Weizmann Institute of Science, Alan Turing Institute, Wellesley College, EleutherAI, Forschungszentrum Julich",,,,StarCoder,,,
PaLM 2,Language,Language modelling,"Andrew M. Dai, David R. So, Dmitry Lepikhin, Jonathan H. Clark, Maxim Krikun, Melvin Johnson, Nan Du, Rohan Anil, Siamak Shakeri, Xavier Garcia, Yanping Huang, Yi Tay, Yong Cheng, Yonghui Wu, Yuanzhong Xu, Yujing Zhang, Zachary Nado, Bryan Richter, Alex Polozov, Andrew Nystrom, Fangxiaoyu Feng, Hanzhao Lin, Jacob Austin, Jacob Devlin, Kefan Xiao, Orhan Firat, Parker Riley, Steven Zheng, Yuhuai Wu, Zhongtao Liu, Jiahui Yu, Guy Gur-Ari, Weikang Zhou, Sneha Kudugunta, Sunipa Dev, Frederick Liu, Gustavo Hernandez Abrego, Kelvin Xu, Abe Ittycheriah, Daniel Sohn, John Nham, Le Hou, Siyuan Qiao, Pidong Wang, Zirui Wang, Laurent El Shafey, Hyeontaek Lim, Marcello Maggioni, Michael Isard, Paul Barham, Qiao Zhang, Tao Wang, Yash Katariya, Aurko Roy, Benjamin Lee, Brennan Saeta, Ce Zheng, Hadi Hashemi, Junwhan Ahn, Rajkumar Samuel, Steven Hand, Zhifeng Chen, Kiran Vodrahalli, Aakanksha Chowdhery, Ethan Dyer, Emanuel Taropa, Vlad Feinberg, James Bradbury, Reiner Pope, Wei Li, YaGuang Li, Eric Chu, Jeffrey Hui, Joshua Howland, Vlad Fienber, Aroma Mahendru, Michele Catasta, Vedant Misra, Kevin Robinson, Maysam Moussalem, Sebastian Ruder, Erica Moreira, Eric Ni, Paige Bailey, Lucas Gonzalez, Alexandre Passos, Slav Petrov, Gaurav Mishra, Mark Omernick, Ambrose Slone, Andrea Hu, Colin Cherry, Denny Zhou, Jan Botha, John Wieting, Joshua Maynez, Kathleen Kenealy, Kevin Brooks, Linting Xue, Markus Freitag, Martin Polacek, Pengcheng Yin, Sebastian Gehrmann, Xuezhi Wang, Kathy Meier-Hellstern, Christopher A. Choquette-Choo, Daniel Smilkov, Emily Reif, Alicia Parrish, Alex Castro Ros, Clément Crepy, Dasha Valter, Jeremy Hurwitz, Katherine Lee, Mark Díaz, Marie Pellat, Matthew Jagielski, Renee Shelby, Shachi Dave",SOTA Improvement,,Unreleased,https://ai.google/static/documents/palm2techreport.pdf,367.00,PaLM 2 Technical Report,2023-05-10,Google,Industry,340000000000.00,"Model Architecture: ""PaLM-2 is a new state-of-the-art language model. We have small, medium, and large variants that use stacked layers based on the Transformer architecture, with varying parameters depending on model size. Further details of model size and architecture are withheld from external publication.""
However, the parameter count was leaked to CNBC: https://www.cnbc.com/2023/05/16/googles-palm-2-uses-nearly-five-times-more-text-data-than-predecessor.html",7.34e+24,"Compute Requirements ""Not reported.""
However, it is suggested that C=6ND. Based on 340B parameters and 3.6*10^12 tokens, training compute would be around 7.3*10^24 FLOP.",,"""The PaLM 2 pre-training corpus is composed of a diverse set of sources: web documents, books, code, mathematics, and conversational data. The pre-training corpus is significantly larger than the corpus used to train PaLM (Chowdhery et al., 2022). PaLM 2 is trained on a dataset that includes a higher percentage of non-English data than previous large language models, which is beneficial for multilingual tasks"" (page 9)",2700000000000,"""The pre-training corpus is significantly larger than the corpus used to train PaLM"" so greater than 6e+11. According to the leaked documents viewed by CNBC, the corpus was 4 trillion tokens or around 2.7*10^12 words.",,,,,,Google TPU v4,Google TPU v4,,,PaLM 2 was trained on TPU v4 according to the model card (pages 91-92),Industry,,"We introduce PaLM 2, a new state-of-the-art language model that has better multilingual and reasoning capabilities and is more compute-efficient than its predecessor PaLM (Chowdhery et al., 2022). PaLM 2 is a Transformer-based model trained using a mixture of objectives similar to UL2 (Tay et al., 2023). Through extensive evaluations on English and multilingual language, and reasoning tasks, we demonstrate that PaLM 2 has significantly improved quality on downstream tasks across different model sizes, while simultaneously exhibiting faster and more efficient inference compared to PaLM. This improved efficiency enables broader deployment while also allowing the model to respond faster, for a more natural pace of interaction. PaLM 2 demonstrates robust reasoning capabilities exemplified by large improvements over PaLM on BIG-Bench and other reasoning tasks. PaLM 2 exhibits stable performance on a suite of responsible AI evaluations, and enables inference-time control over toxicity without additional overhead or impact on other capabilities. Overall, PaLM 2 achieves state-of-the-art performance across a diverse set of tasks and capabilities.",2023-12-05 04:33,Robi Rahman,,,Multinational,Google,,,,PaLM 2,,,PaLM 2
OpenCALM,Language,Chat,Ryosuke Ishigami,,,Permissive license,https://huggingface.co/cyberagent/open-calm-7b,,OpenCALM-7B,2023-05-15,CyberAgent,,7000000000.00,7B,,,"Wikipedia (ja), Common Crawl (ja)",,,,,,,,,,,,,,,Likely,"OpenCALM is a suite of decoder-only language models pre-trained on Japanese datasets, developed by CyberAgent, Inc.",2023-12-08 05:44,Anonymous,,0,Japan,CyberAgent,,,"They say ""Library: GPT-NeoX"". could mean it's fine-tuned from GPT-NeoX, or just that it uses the same architecture or something? ",,,,
CodeT5+,Language,"Code generation,Code autocompletion","Yue Wang, Hung Le, Akhilesh Deepak Gotmare, Nghi D.Q. Bui, Junnan Li, Steven C.H. Hoi",SOTA Improvement,"""We extensively evaluate CodeT5+ on over 20 code-related benchmarks in different settings, including zero-shot, finetuning, and instruction-tuning. We observe state-of-the-art (SoTA) model performance on various code-related tasks, such as code generation and completion, math programming, and text-to-code retrieval tasks""",Fully open-source,https://arxiv.org/abs/2305.07922,68.00,CodeT5+: Open Code Large Language Models for Code Understanding and Generation,2023-05-20,Salesforce,Industry,16000000000.00,"""We implemented a family of CodeT5+ models, with model sizes ranging from 220M to 16B""",,,,"""We enlarge the pretraining dataset of CodeSearchNet [Husain et al., 2019] with the recently released GitHub Code dataset""",,"""We use the CodeT5 tokenizer to tokenize the multilingual dataset, resulting in 51.5B tokens""",10.00,,,,,NVIDIA A100,NVIDIA A100,,,,,,"""Large language models (LLMs) pretrained on vast source code have achieved prominent progress in code intelligence. However, existing code LLMs have two main limitations in terms of architecture and pretraining tasks. First, they often adopt a specific architecture (encoder-only or decoder-only) or rely on a unified encoder-decoder network for different downstream tasks. The former paradigm is limited by inflexibility in applications while in the latter, the model is treated as a single system for all tasks, leading to suboptimal performance on a subset of tasks. Secondly, they often employ a limited set of pretraining objectives which might not be relevant to some downstream tasks and hence result in substantial performance degrade. To address these limitations, we propose ``CodeT5+'', a family of encoder-decoder LLMs for code in which component modules can be flexibly combined to suit a wide range of downstream code tasks. Such flexibility is enabled by our proposed mixture of pretraining objectives to mitigate the pretrain-finetune discrepancy. These objectives cover span denoising, contrastive learning, text-code matching, and causal LM pretraining tasks, on both unimodal and bimodal multilingual code corpora. Furthermore, we propose to initialize CodeT5+ with frozen off-the-shelf LLMs without training from scratch to efficiently scale up our models, and explore instruction-tuning to align with natural language instructions. We extensively evaluate CodeT5+ on over 20 code-related benchmarks in different settings, including zero-shot, finetuning, and instruction-tuning. We observe state-of-the-art (SoTA) model performance on various code-related tasks, such as code generation and completion, math programming, and text-to-code retrieval tasks. Particularly, our instruction-tuned CodeT5+ 16B achieves new SoTA results on HumanEval code generation task against other open code LLMs.""",2023-12-05 04:33,Anonymous,,,United States of America,Salesforce,,,,CodeT5+,,,
RWKV-4 14B,Language,Language modelling,"Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, Xuzheng He, Haowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bartlomiej Koptyra, Hayden Lau, Krishna Sri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Xiangru Tang, Bolun Wang, Johan S. Wind, Stansilaw Wozniak, Ruichong Zhang, Zhenyuan Zhang, Qihang Zhao, Peng Zhou, Jian Zhu, Rui-Jie Zhu",,,,https://arxiv.org/abs/2305.13048,39.00,RWKV: Reinventing RNNs for the Transformer Era,2023-05-22,RWKV Foundation,Industry - Academia Collaboration,14000000000.00,14b,2.78e+22,"from HuggingFace page: https://huggingface.co/BlinkDL/rwkv-4-pile-14b

trained for 331B tokens
14 billion * 331 billion * 6 = 2.78e22

paper notes that a forward pass is almost exactly 2x parameters (within 2%): ""Alternative approximations for FLOPs include doubling the parameters which yields similar results within 2% for 14B and a 30% discrepancy for 169M variant."" and that 6*params*tokens is a good approximation because it's not a transformer: ""FLOPs is for a forward pass for one token. It was calculated as 6(V D + 13D2L), which is the
twice (add and multiply) the number of parameters
in linear layers. The backwards pass FLOPs can be
approximated as twice that of the forward pass. So
the total is 6(V D + 13D2L) per token for training
(3x fw FLOPs). It is noteworthy that FLOPs are
independent of the context length, unlike regular
transformers""",The Pile,,,,,27780000000.00,Table 2,,,NVIDIA A100 SXM4 80 GB,NVIDIA A100 SXM4 80 GB,,,,,Likely,"Transformers have revolutionized almost all natural language processing (NLP) tasks but suffer from memory and computational complexity that scales quadratically with sequence length. In contrast, recurrent neural networks (RNNs) exhibit linear scaling in memory and computational requirements but struggle to match the same performance as Transformers due to limitations in parallelization and scalability. We propose a novel model architecture, Receptance Weighted Key Value (RWKV), that combines the efficient parallelizable training of Transformers with the efficient inference of RNNs. Our approach leverages a linear attention mechanism and allows us to formulate the model as either a Transformer or an RNN, which parallelizes computations during training and maintains constant computational and memory complexity during inference, leading to the first non-transformer architecture to be scaled to tens of billions of parameters. Our experiments reveal that RWKV performs on par with similarly sized Transformers, suggesting that future work can leverage this architecture to create more efficient models. This work presents a significant step towards reconciling the trade-offs between computational efficiency and model performance in sequence processing tasks.",2023-12-12 06:27,Anonymous,,0,,RWKV Foundation,,,,,,,
LLaMA-7B (LoRA finetuned),Language,,,,,,,,,2023-05-23,,,,,,,,,,,1.09,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,LLaMA-7B (LoRA finetuned),1,,,,,,,,,
LLaMA-65B (LoRA finetuned),Language,,,,,,,,,2023-05-23,,,,,,,,,,,1.09,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,LLaMA-65B (LoRA finetuned),1,,,,,,,,,
LLaMA-13B (LoRA finetuned),Language,,,,,,,,,2023-05-23,,,,,,,,,,,1.09,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,LLaMA-13B (LoRA finetuned),1,,,,,,,,,
LLaMA-33B (LoRA finetuned),Language,,,,,,,,,2023-05-23,,,,,,,,,,,1.09,,,,,,,,,,,Unverified,,2023-12-08 01:00,Robi Rahman,LLaMA-33B (LoRA finetuned),1,,,,,,,,,
PaLI-X,Multimodal,Image captioning,"Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa, Soravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz, Sebastian Goodman, Xiao Wang, Yi Tay, Siamak Shakeri, Mostafa Dehghani, Daniel Salz, Mario Lucic, Michael Tschannen, Arsha Nagrani, Hexiang Hu, Mandar Joshi, Bo Pang, Ceslee Montgomery, Paulina Pietrzyk, Marvin Ritter, AJ Piergiovanni, Matthias Minderer, Filip Pavetic, Austin Waters, Gang Li, Ibrahim Alabdulmohsin, Lucas Beyer, Julien Amelot, Kenton Lee, Andreas Peter Steiner, Yang Li, Daniel Keysers, Anurag Arnab, Yuanzhong Xu, Keran Rong, Alexander Kolesnikov, Mojtaba Seyedhosseini, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, Radu Soricut",SOTA Improvement,"""PaLI-X advances the state-of-the-art on most vision-and-language benchmarks considered (25+ of them).""",,https://arxiv.org/abs/2305.18565,40.00,PaLI-X: On Scaling up a Multilingual Vision and Language Model,2023-05-29,Google,Industry,55000000000.00,55B (table 1),,,WebLI,"""The main pretraining data for our model is based on WebLI [5], consisting of roughly one billion
images with alt-texts from the web and OCR annotations (using the GCP Vision API), covering over
100 languages. In addition to WebLI ⟨image, text⟩ pairs, we introduce here Episodic WebLI data,
where each episode corresponds to a set of such pairs. We aim to have each episode contain loosely
related images (i.e., they are clustered according to their URL field), so as to encourage attention
among examples in an “episode”. We find this new dataset (with 75M episodes and around 400M
images in total) important for developing the few-shot capabilities of the model.""",1400000000,"1 billion images with alt texts in WebLI, 400m images in Episodic WebLI data",,,,,,,,,,,,Likely,"We present the training recipe and results of scaling up PaLI-X, a multilingual vision and language model, both in terms of size of the components and the breadth of its training task mixture. Our model achieves new levels of performance on a wide-range of varied and complex tasks, including multiple image-based captioning and question-answering tasks, image-based document understanding and few-shot (in-context) learning, as well as object detection, video question answering, and video captioning. PaLI-X advances the state-of-the-art on most vision-and-language benchmarks considered (25+ of them). Finally, we observe emerging capabilities, such as complex counting and multilingual object detection, tasks that are not explicitly in the training mix.",2023-12-05 04:33,Anonymous,,0,Multinational,Google,,,,,,,
Polyglot-Ko-12.8B,Language,,"Hyunwoong Ko, Kichang Yang, Minho Ryu, Taekyoon Choi, Seungmu Yang, Jiwung Hyun, Sungho Park, Kyubyong Park",,,Permissive license,https://arxiv.org/abs/2306.02254; https://huggingface.co/EleutherAI/polyglot-ko-12.8b,2.00,A Technical Report for Polyglot-Ko: Open-Source Large-Scale Korean Language Models,2023-06-04,EleutherAI,,12898631680.00,,1.28e+22,"trained for 167 billion tokens

167b * 12.8b * 6 = 1.28e22",,,96000000000,"863 GB of Korean language data after processing

~111m Korean words per GB, so ~95,793,000,000 or ~96B words

https://docs.google.com/document/d/1G3vvQkn4x_W71MKg0GmHVtzfd9m0y3_Ofcoew0v902Q/edit#heading=h.ieihc08p8dn0",,,,,,NVIDIA A100,NVIDIA A100,,,,Industry,Likely,"Polyglot is a pioneering project aimed at enhancing the non-English language performance of multilingual language models. Despite the availability of various multilingual models such as mBERT (Devlin et al., 2019), XGLM (Lin et al., 2022), and BLOOM (Scao et al., 2022), researchers and developers often resort to building monolingual models in their respective languages due to the dissatisfaction with the current multilingual models non-English language capabilities. Addressing this gap, we seek to develop advanced multilingual language models that offer improved performance in non-English languages. In this paper, we introduce the Polyglot Korean models, which represent a specific focus rather than being multilingual in nature. In collaboration with TUNiB, our team collected 1.2TB of Korean data meticulously curated for our research journey. We made a deliberate decision to prioritize the development of Korean models before venturing into multilingual models. This choice was motivated by multiple factors: firstly, the Korean models facilitated performance comparisons with existing multilingual models; and finally, they catered to the specific needs of Korean companies and researchers. This paper presents our work in developing the Polyglot Korean models, which propose some steps towards addressing the non-English language performance gap in multilingual language models.",2023-12-08 05:52,Anonymous,,0,Multinational,EleutherAI,,,,,,,
RedPajama-INCITE-7B-Base,Language,Chat,,,"one of the fine-tuned versions is Pareto SOTA for open source. wouldn't consider it SOTA overall. ""RedPajama-INCITE-7B-Instruct is the highest scoring open model on HELM benchmarks, making it ideal for a wide range of tasks. It outperforms LLaMA-7B and state-of-the-art open models such as Falcon-7B (Base and Instruct) and MPT-7B (Base and Instruct) on HELM by 2-9 points.""",Permissive license,https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Base,,,2023-06-06,Together,Industry,6900000000.00,6.9b,4.1e+22,"Trained over 1.001 trillion tokens.
6.9b * 1 trillion * 6 = 4.1e22

",RedPajama (1T),https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T,900000000000,"1.2 trillion, or 900b words at 0.75 words/token",0.83,,,,,NVIDIA V100,NVIDIA V100,,,,,Likely,"We trained 3B and 7B models on the Summit supercomputer, in collaboration with AAI CERC lab at Université de Montréal, EleutherAI & LAION for compute time on Summit within the INCITE program award ""Scalable Foundation Models for Transferable Generalist AI”.

Today we are excited to release the v1 versions of the RedPajama-INCITE family of models, including instruct-tuned and chat versions under the Apache 2.0 license.",2023-12-05 09:32,Anonymous,,0,United States of America,Together,,,,,,,
MusicGen,Audio,Audio generation,"Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi, Alexandre Défossez",SOTA Improvement,"""We conduct extensive empirical evaluation, considering both automatic and human
studies, showing the proposed approach is superior to the evaluated baselines on a standard text-to-music benchmark""",,https://arxiv.org/abs/2306.05284,40.00,Simple and Controllable Music Generation,2023-06-08,Meta AI,Industry,,,,,ShutterStock and Pond5 music data collections,"We use 20K hours of licensed music to train MUSICGEN. Specifically, we rely on an internal dataset of 10K high-quality music tracks, and on the ShutterStock and Pond5 music data collections with respectively 25K and 365K instrument-only music tracks. All datasets consist of full-length music sampled at 32 kHz with metadata composed of a textual description and additional information such as the genre, BPM, and tags.",,,,,,,,,,,,,Industry,Unverified,"We tackle the task of conditional music generation. We introduce MusicGen, a single Language Model (LM) that operates over several streams of compressed discrete music representation, i.e., tokens. Unlike prior work, MusicGen is comprised of a single-stage transformer LM together with efficient token interleaving patterns, which eliminates the need for cascading several models, e.g., hierarchically or upsampling. Following this approach, we demonstrate how MusicGen can generate high-quality samples, while being conditioned on textual description or melodic features, allowing better controls over the generated output. We conduct extensive empirical evaluation, considering both automatic and human studies, showing the proposed approach is superior to the evaluated baselines on a standard text-to-music benchmark. Through ablation studies, we shed light over the importance of each of the components comprising MusicGen. Music samples, code, and models are available at this https URL.",2023-12-05 04:33,Anonymous,,,Multinational,Meta AI,,,,MusicGen,,,
Wu Dao Aquila,Language,"Chat,Code generation",,,,Permissive license,https://spectrum.ieee.org/china-chatgpt-wu-dao,,,2023-06-10,Beijing Academy of Artificial Intelligence,,33000000000.00,33B for largest model: https://huggingface.co/BAAI/Aquila-7B,,,,,,,,,,,,NVIDIA A100,NVIDIA A100,,,,,Likely,"Who said all large-language models (LLMs) necessarily need to be large? In China’s case, LLMs are currently downsizing in their size and number of parameters. According to sources, this is because the country is now focusing on enabling Chinese startups and smaller entities to build their own generative AI applications. As part of this downscaling trend, in June the Beijing Academy of Artificial Intelligence (BAAI) introduced Wu Dao 3.0, a series of open-source LLMs.

Based on interviews with high-ranking, anonymous sources involved in the project, IEEE Spectrum can report that Wu Dao 3.0 builds on the academy’s work with Wu Dao 2.0, a sparse, multimodal generative AI model—as has been widely reported about version 2.0—with 1.75 trillion parameters. Although there is no single set of parameters for Wu Dao 3.0 (it’s a range of models with a variety of parameter counts) all are well below the 1.75 trillion high-water mark that version 2.0 set.",2023-12-13 08:01,Anonymous,,0,China,Beijing Academy of Artificial Intelligence,,,,,,,
RoboCat,Robotics,,"Konstantinos Bousmalis, Giulia Vezzani, Dushyant Rao, Coline Devin, Alex X. Lee, Maria Bauza, Todor Davchev, Yuxiang Zhou, Agrim Gupta, Akhil Raju, Antoine Laurens, Claudio Fantacci, Valentin Dalibard, Martina Zambelli, Murilo Martins, Rugile Pevceviciute, Michiel Blokzijl, Misha Denil, Nathan Batchelor, Thomas Lampe, Emilio Parisotto, Konrad Żołna, Scott Reed, Sergio Gómez Colmenarejo, Jon Scholz, Abbas Abdolmaleki, Oliver Groth, Jean-Baptiste Regli, Oleg Sushkov, Tom Rothörl, José Enrique Chen, Yusuf Aytar, Dave Barker, Joy Ortiz, Martin Riedmiller, Jost Tobias Springenberg, Raia Hadsell, Francesco Nori, Nicolas Heess",SOTA Improvement,,,https://arxiv.org/abs/2306.11706,12.00,RoboCat: A Self-Improving Foundation Agent for Robotic Manipulation,2023-06-20,"Google DeepMind,Google",Industry,1180000000.00,"""Most of the experimental results are based on models with a 1.18B-parameter decoder-only transformer (Vaswani et al., 2017) with 24 layers, an embedding size of 2048, and a post-attention feedforward hidden size of 8196."" page 8",,,,"""We use a diverse and large number of datasets for training RoboCat. These include data from agent experience, human demonstrations and self-generated data, on both simulated and real-world robot environments. See Section 3.4 for details on our datasets.""",,,,,,,,,,,,,Industry,Speculative,"The ability to leverage heterogeneous robotic experience from different robots and tasks to quickly master novel skills and embodiments has the potential to transform robot learning. Inspired by recent advances in foundation models for vision and language, we propose a foundation agent for robotic manipulation. This agent, named RoboCat, is a visual goal-conditioned decision transformer capable of consuming multi-embodiment action-labelled visual experience. This data spans a large repertoire of motor control skills from simulated and real robotic arms with varying sets of observations and actions. With RoboCat, we demonstrate the ability to generalise to new tasks and robots, both zero-shot as well as through adaptation using only 100--1000 examples for the target task. We also show how a trained model itself can be used to generate data for subsequent training iterations, thus providing a basic building block for an autonomous improvement loop. We investigate the agent's capabilities, with large-scale evaluations both in simulation and on three different real robot embodiments. We find that as we grow and diversify its training data, RoboCat not only shows signs of cross-task transfer, but also becomes more efficient at adapting to new tasks.",2023-12-05 04:33,Anonymous,,,"Multinational,Multinational","Google DeepMind, Google",,,,RoboCat,,,
MPT-30B,Language,"Language Generation,Code generation",,,,Permissive license,https://huggingface.co/mosaicml/mpt-30b,,,2023-06-22,MosaicML,,30000000000.00,30b,1.8e+23,30b * 1T tokens * 6 = 1.8e23,"mC4, c4, RedPajama, The Stack",among others,3000000000000,"~4T tokens across sources, or 3T words at 0.75 words/token (ignoring the fact that some of the data is code)",0.25,,,,,"NVIDIA A100 SXM4 40 GB,NVIDIA A100 SXM4 80 GB","NVIDIA A100 SXM4 40 GB, NVIDIA A100 SXM4 80 GB",,,,,Likely,"MPT-30B is a decoder-style transformer pretrained from scratch on 1T tokens of English text and code. This model was trained by MosaicML.

MPT-30B is part of the family of Mosaic Pretrained Transformer (MPT) models, which use a modified transformer architecture optimized for efficient training and inference.

MPT-30B comes with special features that differentiate it from other LLMs, including an 8k token context window (which can be further extended via finetuning; see MPT-7B-StoryWriter), support for context-length extrapolation via ALiBi, and efficient inference + training via FlashAttention. It also has strong coding abilities thanks to its pretraining mix. MPT models can also be served efficiently with both standard HuggingFace pipelines and NVIDIA's FasterTransformer. The size of MPT-30B was also specifically chosen to make it easy to deploy on a single GPU—either 1xA100-80GB in 16-bit precision or 1xA100-40GB in 8-bit precision.",2023-12-12 03:49,Anonymous,,0,United States of America,MosaicML,,,,,,,
Inflection-1,Language,Language modelling,,,"""Inflection-1 outperforms models trained with at most the same amount of compute as PaLM-540B on MMLU and the other benchmarks in Table 1.""",Unreleased,https://inflection.ai/assets/Inflection-1.pdf,0.00,Inflection-1 technical memo,2023-06-23,Inflection AI,Industry,,,1.0001e+24,"<= 2.5e24

They define two ""compute classes"", one for models with more compute than PaLM 540B, i.e. GPT-4 and PaLM 2, and one for models with as much compute or less, i.e. GPT-3.5, Chinchilla, LLaMA, and Inflection-1.

PaLM 540B required 2.5e24 FLOP to train (confirmed by Google)",,"""Inflection-1 was trained using thousands of NVIDIA H100 GPUs on a very large dataset.""",,,,,,,,NVIDIA H100 SXM5,NVIDIA H100 SXM5,,,,Industry,Speculative,"Large language models (LLMs) based on the Transformer architecture have been shown to possess a range of advanced capabilities in language generation and understanding. These capabilities have paved the way for deployment of LLMs in products like OpenAI’s Chat-GPT and Google’s Bard. At Inflection AI, our mission is to create personal AIs for everyone, and in May 2023 we released Pi (pi.ai) – an LLM-based personal AI which is designed to be empathetic, useful, and safe. In this work we introduce the foundation model powering Pi, dubbed Inflection-1, and evaluate its performance characteristics across a variety of benchmarks.",2023-11-23 06:53,Anonymous,,0,United States of America,Inflection AI,,,,,,,
ERNIE 3.5,Language,Language modelling,,SOTA Improvement,SOTA scores on AGIEval and MMLU. See article in China Science Daily: https://mp.weixin.qq.com/s/QVdkmofRSTgjQ7UOFX7s1g,,http://research.baidu.com/Blog/index-view?id=185,0.00,Introducing ERNIE 3.5: Baidu’s Knowledge-Enhanced Foundation Model Takes a Giant Leap Forward,2023-06-27,Baidu,Industry,,,,,,,,,,,,,,,,,,,,Unverified,,2023-11-26 03:25,Anonymous,,,China,Baidu,,,,ERNIE 3.5,,,
Stable Diffusion XL,Drawing,Image generation,"Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, Robin Rombach",Significant use,Looks like this is now the main/flagship Stable Diffusion model,,https://arxiv.org/abs/2307.01952,115.00,SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis,2023-07-04,Stability AI,Industry,3400000000.00,"""...result in a model size of 2.6B parameters in the UNet, see Tab. 1. The text encoders have a total size of 817M parameters.""",,,,,,,,,,,,,,,,,Industry,Speculative,"We present SDXL, a latent diffusion model for text-to-image synthesis. Compared to previous versions of Stable Diffusion, SDXL leverages a three times larger UNet backbone: The increase of model parameters is mainly due to more attention blocks and a larger cross-attention context as SDXL uses a second text encoder. We design multiple novel conditioning schemes and train SDXL on multiple aspect ratios. We also introduce a refinement model which is used to improve the visual fidelity of samples generated by SDXL using a post-hoc image-to-image technique. We demonstrate that SDXL shows drastically improved performance compared the previous versions of Stable Diffusion and achieves results competitive with those of black-box state-of-the-art image generators. In the spirit of promoting open research and fostering transparency in large model training and evaluation, we provide access to code and model weights at this https URL",2023-12-05 04:33,Anonymous,,,Multinational,Stability AI,,,,Stable Diffusion XL,,,
InternLM,Language,Language modelling,,SOTA Improvement,"(from Google-translated page) ""In addition to using academic datasets to evaluate InternLM, we also use human examinations to assess its capabilities. InternLM can achieve good scores on examination benchmarks such as MMLU, AGIEval, C-Eval, and GAOKAO-bench that cover different languages and subjects, scoring higher than ChatGPT on multiple benchmarks""",,https://internlm.org/,0.00,,2023-07-06,"Shanghai AI Lab,SenseTime",Academia,100000000000.00,Pre-training a bilingual 100B Foundation model on data with over a trillion tokens,,,,,750000000000,"""Pre-training a bilingual 100B Foundation model on data with over a trillion tokens"" equals approximately 750B words for English, but the tokenizer's conversion ratio may be different for Chinese.",,,,,Training performance for the open-source InternLM-7B: https://github.com/InternLM/InternLM/blob/main/doc/en/train_performance.md,NVIDIA A100 SXM4 80 GB,NVIDIA A100 SXM4 80 GB,,,,,Speculative,"Pre-training a bilingual 100B Foundation model on data with over a trillion tokens, the model exhibits excellent performance in scenarios such as Chinese, English, and coding due to the appropriate data ratio. Based on the foundation model, the application of high-quality human annotated dialogue data combined with RLHF technology enables the InternLM large language model to respond to complex commands during human interaction, while also demonstrating responses in line with human morality and values.",2023-11-26 03:25,Anonymous,,,"China,Hong Kong","Shanghai AI Lab, SenseTime",,,,InternLM,,,
CodeGen2.5,Language,Code generation,"Erik Nijkamp, Hiroaki Hayashi, Yingbo Zhou, Caiming Xiong",,,Permissive license,https://blog.salesforceairesearch.com/codegen25/,,"CodeGen2.5: Small, but mighty",2023-07-06,Salesforce,Industry,7000000000.00,7B,5.899999999999999e+22,"7B parameters, trained on 1.4T tokens

7 billion * 1.4 trillion * 6 = 5.9e22",StarCoderData,,,280 billion tokens,5.00,,,,,,,,,,,Likely,"The family of Salesforce CodeGen models is growing with CodeGen2.5 – a small, but mighty model! While there has been a recent trend of large language models (LLM) of increasing size, we show that a small model can obtain surprisingly good performance, when being trained well.  

The key contributions towards productization of these models are:

Releasing CodeGen2.5 LLM with state-of-the-art on HumanEval for 7B parameters.
CodeGen2.5 with 7B is on par with >15B code-generation models (CodeGen1-16B, CodeGen2-16B, StarCoder-15B), less than half the size.
Featuring robust infill sampling, that is, the model can “read” text of both the left and right hand size of the current position.
Optimized for fast sampling under Flash attention for optimized serving and local deployment on personal machines.
Permissively licensed in Apache 2.0.",2023-11-30 08:17,Anonymous,,0,United States of America,Salesforce,,,,,,,
Claude 2,Language,Language modelling,,Historical significance,,API accessible,https://www.anthropic.com/index/claude-2,0.00,,2023-07-11,Anthropic,Industry,,,3.866e+24,https://colab.research.google.com/drive/1MdPuhS4Emaf23VXYZ-ooExDW-5GXZkw0#scrollTo=Ds0Q5X8aMnOY,,,,,,,,,,,,,,,,Speculative,,2023-11-30 04:03,Anonymous,,,United States of America,Anthropic,,,,Claude 2,,,Claude 2
ChatRhino,Language,Chat,,,,,https://jdcorporateblog.com/jd-com-introduces-chatrhino-empowering-industry-innovations-with-an-advanced-large-language-model/,,JD.com Introduces ChatRhino: Empowering Industry Innovations with an Advanced Large Language Model,2023-07-13,JD.com,,100000000000.00,"""ChatRhino sets a new benchmark as a 100-billion-parameter model"", could be substantially rounded",,,,"Mix of general and supply chain data: ""By combining 70% generalized data with 30% native intelligent supply chain data, JD’s latest AI model offers targeted solutions for real industry challenges across sectors such as retail, logistics, finance, health, and city""",,,,,,,,,,,,,,Likely,"JD.com today unveiled its ChatRhino (Yanxi in Chinese) large language model (LLM) on its 2023 JDDiscovery tech summit, tailored to serve various industries. By combining 70% generalized data with 30% native intelligent supply chain data, JD’s latest AI model offers targeted solutions for real industry challenges across sectors such as retail, logistics, finance, health, and city. Building upon the success of the billion-parameter model K-PLUG launched in 2021 and the 10-billion-parameter model Vega introduced in 2022, JD’s ChatRhino sets a new benchmark as a 100-billion-parameter model.",2023-12-07 08:09,Anonymous,,0,China,JD.com,,,,,,,
RetNet,Language,Language modelling,"Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, Furu Wei",,,,https://arxiv.org/abs/2307.08621,29.00,Retentive Network: A Successor to Transformer for Large Language Models,2023-07-17,"Microsoft Research,Tsinghua University",Industry - Academia Collaboration,6700000000.00,"Table 2

They later mention testing the memory and throughput of a 13B-parameter model, but it doesn't sound like they trained it long enough to test its perplexity.",4.02e+21,C = 6ND = 6 * 6.7 billion * 100 billion,,,75000000000,,,,,,,,,,,,,Likely,"In this work, we propose Retentive Network (RetNet) as a foundation architecture for large language models, simultaneously achieving training parallelism, low-cost inference, and good performance. We theoretically derive the connection between recurrence and attention. Then we propose the retention mechanism for sequence modeling, which supports three computation paradigms, i.e., parallel, recurrent, and chunkwise recurrent. Specifically, the parallel representation allows for training parallelism. The recurrent representation enables low-cost O(1) inference, which improves decoding throughput, latency, and GPU memory without sacrificing performance. The chunkwise recurrent representation facilitates efficient long-sequence modeling with linear complexity, where each chunk is encoded parallelly while recurrently summarizing the chunks. Experimental results on language modeling show that RetNet achieves favorable scaling results, parallel training, low-cost deployment, and efficient inference. The intriguing properties make RetNet a strong successor to Transformer for large language models.",2023-12-05 04:33,Anonymous,,,"United States of America,China","Microsoft Research, Tsinghua University",,,,,,,
Llama 2-70B,Language,Language modelling,"Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez,Robert Stojnic, Sergey Edunov, Thomas Scialom
","Historical significance,Significant use",Model has been open-sourced and frequently downloaded. The paper claims that Llama 2 is the current best open-source chat model as of its release date.,Fully open-source,https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/,1122.00,Llama 2: Open Foundation and Fine-Tuned Chat Models,2023-07-18,Meta AI,Industry,70000000000.00,"Llama has been released in 7B, 13B, and 70B variants.",8.1e+23,"""Pretraining utilized a cumulative 3.3M GPU hours of computation on hardware of type A100-80GB"" of which 1720320 GPU hours were used to train the 70B model.

311.84 BF16 TFLOP/s * 1720320 hours * 0.40 utilization = 7.725e+23 FLOP.

Alternatively: the model was trained for 1 epoch on 2 trillion tokens and has 70B parameters. C = 6ND = 6*70B*2T = 8.4e+23 FLOP.",Llama 2 dataset,"2 trillion tokens of publicly available text, with no text from Meta's products.
""Our training corpus includes a new mix of data from publicly available sources, which does not include data from Meta’s products or services. We made an effort to remove data from certain sites known to contain a high volume of personal information about private individuals. We trained on 2 trillion tokens of data as this
provides a good performance–cost trade-off, up-sampling the most factual sources in an effort to increase knowledge and dampen hallucinations.""",1500000000000,2 trillion tokens ~= 1.5 trillion words,1.00,140000000000.00,Inference compute usage for the 70B model is 140 billion operations per token of input.,4320.0,"Model was trained from January 2023 to July 2023, which is six months.",NVIDIA A100 SXM4 80 GB,NVIDIA A100 SXM4 80 GB,Supervised,$1620000.00,"A100 cost in 2023: $1.10/hour
Training time: 1720320 A100 GPU-hours
Inflation adjustment: $1.000 2020 = $1.145 2023",Industry,Confident,"In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closedsource models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.",2023-12-08 07:18,Anonymous,,0,Multinational,Meta AI,,,,Llama 2,,,Llama 2
Llama 2-34B,Language,,"Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez,Robert Stojnic, Sergey Edunov, Thomas Scialom",,,Unreleased,https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/,983.00,Llama 2: Open Foundation and Fine-Tuned Chat Models,2023-07-18,Meta AI,,34000000000.00,34B,4.08e+23,"All models sizes trained on 2.0T tokens, per table 1
2T * 34b * 6 = 4.08e23

also trained on 1038336 A100-hours, which is 3.5e23 at 30% utilization.",,,,,,,,,,NVIDIA A100 SXM4 80 GB,NVIDIA A100 SXM4 80 GB,,,,,Likely,"In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned
large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.
Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our
models outperform open-source chat models on most benchmarks we tested, and based on
our human evaluations for helpfulness and safety, may be a suitable substitute for closedsource models. We provide a detailed description of our approach to fine-tuning and safety
improvements of Llama 2-Chat in order to enable the community to build on our work and
contribute to the responsible development of LLMs.",2023-12-09 09:12,Anonymous,,1,Multinational,Meta AI,,,,,,,
EXAONE 2.0,Multimodal,Language modelling,,,,,https://aws.amazon.com/solutions/case-studies/lg-ai-research-case-study/,,LG AI Research Develops Foundation Model Using Amazon SageMaker,2023-07-19,LG,Industry,300000000000.00,300 billion,,,,"From KoreaTimes (https://www.koreatimes.co.kr/www/tech/2023/12/129_355258.html)

""EXAONE 2.0 studied about 45 million specialized documents and 350 million images, including patents and papers secured through partnerships.

Considering that much of the existing expertise data is in English, EXAONE 2.0 is developed as a bilingual model that can understand and answer both in Korean and English at the same time. It also learns over four times more data than the previous model.""",,,,,,,,,,,,,,Confident,,2023-12-05 07:37,Anonymous,,0,Korea (Republic of),LG,,,,,,,
Stable Beluga 2,Language,Language Generation,,,"#1 on Open LLM leaderboard (in July, it's much lower now):

""These Stable Beluga results were evaluated by Stability AI researchers and independently reproduced by Hugging Face on July 21st, 2023, and published in their leaderboard.

As of July 27th, 2023, Stable Beluga 2 is the very best model (#1) on the leaderboard, and Stable Beluga 1 is #4""",Permissive license,"https://stability.ai/news/stable-beluga-large-instruction-fine-tuned-models, https://huggingface.co/stabilityai/StableBeluga2",,"Meet Stable Beluga 1 and Stable Beluga 2, Our Large and Mighty Instruction Fine-Tuned Language Models",2023-07-20,Stability AI,,70000000000.00,fine-tuned from Llama 2-70B,,,,,,,,,,,,,,,,,,Likely,"Stability AI and its CarperAI lab proudly announce Stable Beluga 1 and its successor Stable Beluga 2 (formerly codenamed FreeWilly), two powerful new, open access, Large Language Models (LLMs). Both models demonstrate exceptional reasoning ability across varied benchmarks. Stable Beluga 1 leverages the original LLaMA 65B foundation model and was carefully fine-tuned with a new synthetically-generated dataset using Supervised Fine-Tune (SFT) in standard Alpaca format. Similarly, Stable Beluga 2 leverages the LLaMA 2 70B foundation model to achieve industry-leading performance.",2023-12-14 09:21,Anonymous,,0,Multinational,Stability AI,Llama 2-70B,,"Fine-tuned on a 600k dataset. Not sure how many epochs. https://stability.ai/news/stable-beluga-large-instruction-fine-tuned-models

""The training for the Stable Beluga models was directly inspired by the methodology pioneered by Microsoft in its paper: ""Orca: Progressive Learning from Complex Explanation Traces of GPT-4.” While our data generation process is similar, we differ in our data sources. Our variant of the dataset, containing 600,000 data points (roughly 10% of the dataset size the original Orca paper used)...""",,,,
Stable Beluga 1,Language,Language Generation,,,"#4 on Open LLM leaderboard (in July, it's much lower now):

""These Stable Beluga results were evaluated by Stability AI researchers and independently reproduced by Hugging Face on July 21st, 2023, and published in their leaderboard.

As of July 27th, 2023, Stable Beluga 2 is the very best model (#1) on the leaderboard, and Stable Beluga 1 is #4""",Permissive license,https://huggingface.co/stabilityai/StableBeluga1-Delta,,"Meet Stable Beluga 1 and Stable Beluga 2, Our Large and Mighty Instruction Fine-Tuned Language Models",2023-07-21,Stability AI,,65200000000.00,65.2B,,,,,,,,,,,,,,,,,,Likely,"Stability AI and its CarperAI lab proudly announce Stable Beluga 1 and its successor Stable Beluga 2 (formerly codenamed FreeWilly), two powerful new, open access, Large Language Models (LLMs). Both models demonstrate exceptional reasoning ability across varied benchmarks. Stable Beluga 1 leverages the original LLaMA 65B foundation model and was carefully fine-tuned with a new synthetically-generated dataset using Supervised Fine-Tune (SFT) in standard Alpaca format. Similarly, Stable Beluga 2 leverages the LLaMA 2 70B foundation model to achieve industry-leading performance.",2023-12-14 09:17,Anonymous,,0,Multinational,Stability AI,LLaMA-65B,,"Fine-tuned on a 600k dataset. Not sure how many epochs. https://stability.ai/news/stable-beluga-large-instruction-fine-tuned-models

""The training for the Stable Beluga models was directly inspired by the methodology pioneered by Microsoft in its paper: ""Orca: Progressive Learning from Complex Explanation Traces of GPT-4.” While our data generation process is similar, we differ in our data sources. Our variant of the dataset, containing 600,000 data points (roughly 10% of the dataset size the original Orca paper used)...""",,,,
bilingual-gpt-neox-4b,Language,Language Generation,"Tianyu Zhao, Toshiaki Wakatsuki, Akio Kaga, Koh Mitsuda, Kei Sawada",,,Permissive license,https://huggingface.co/rinna/bilingual-gpt-neox-4b,,,2023-07-31,rinna,,3800000000.00,3.8 billion,1.2e+22,3.8 billion params * 524b tokens * 6 = 1.2e22,"Japanese CC-100, Japanese C4, The Pile, RedPajama",,,,,,,,,,,,,,,Likely,This repository provides an English-Japanese bilingual GPT-NeoX model of 3.8 billion parameters.,2023-12-12 05:18,Anonymous,,0,Japan,rinna,,,,,,,
JIANG,Language,Language modelling,"Qinhua Duan, Wenchao Gu, Yujia Chen, Wenxin Mao, Zewen Tian, Hui Cao",,,Permissive license,https://arxiv.org/abs/2308.00624,0.00,JIANG: Chinese Open Foundation Language Model,2023-08-01,,Industry,,They show a chart with different 400M models they trained to refine the design. Main model probably has more but they don't specify.,4.03e+22,"""The training was conducted using 96 A100 80G GPUs, and the entire process took approximately 52 days.""

312 teraflop/s * 96 * 52 * 24 * 3600 * 0.3 = 4e22",,"""The model is trained on a large quantity of textual data including both English and Chinese and use a standard optimizer.""

Mostly from Chinese internet, and The Pile and GitHub.",467000000000,"467B tokens (inferred from Table 1).

It's a mix of Chinese and English text, I'll use our standard 1:1 token:words ratio for Chinese.",,,,1200.0,52 days,NVIDIA A100 SXM4 80 GB,NVIDIA A100 SXM4 80 GB,,,,,Likely,"With the advancements in large language model technology, it has showcased capabilities that come close to those of human beings across various tasks. This achievement has garnered significant interest from companies and scientific research institutions, leading to substantial investments in the research and development of these models. While numerous large models have emerged during this period, the majority of them have been trained primarily on English data. Although they exhibit decent performance in other languages, such as Chinese, their potential remains limited due to factors like vocabulary design and training corpus. Consequently, their ability to fully express their capabilities in Chinese falls short. To address this issue, we introduce the model named JIANG (Chinese pinyin of ginger) specifically designed for the Chinese language. We have gathered a substantial amount of Chinese corpus to train the model and have also optimized its structure. The extensive experimental results demonstrate the excellent performance of our model.",2023-11-29 07:09,Anonymous,,0,,,,,,,,,
StableLM-Base-Alpha-7B,Language,Language modelling,,,,Permissive license,https://huggingface.co/stabilityai/stablelm-base-alpha-7b-v2,,,2023-08-05,Stability AI,Industry,6890209280.00,,4.5e+22,"""StableLM-Base-Alpha-7B-v2 is pre-trained using a multi-stage context length extension schedule following similar work (Nijkamp et al. 2023); first pre-training at a context length of 2048 for 1 trillion tokens, then fine-tuning at a context length of 4096 for another 100B tokens""

6890209280 params * 1.1 trillion tokens * 6 = 4.5e22

alternatively: ""StableLM-Base-Alpha-7B-v2 was trained on the Stability AI cluster - occupying 384 NVIDIA A100 40GB GPUs across AWS P4d instances. Training took approximately 16.33 days to complete across both stages.""

312 teraflops * 384 * 16.33 * 24 * 3600 * 0.3 = 5.07e22","Falcon RefinedWeb, RedPajama-Data, The Pile","""The first pre-training stage relies on 1 trillion tokens sourced from a mix of the public Falcon RefinedWeb extract (Penedo et al., 2023), RedPajama-Data (Together Computer 2023, The Pile (Gao et al., 2020), and internal datasets with web text sampled at a rate of 71%.

In the second stage, we include the StarCoder (Li et al., 2023) dataset and down sample web text to 55% while increasing sampling proportions of naturally long text examples in the aforementioned sources.""",750000000000,1 trillion tokens,1.00,,,392.0,16.33 days,NVIDIA A100 SXM4 40 GB,NVIDIA A100 SXM4 40 GB,,,,,Likely,"StableLM-Base-Alpha-7B-v2 is a 7 billion parameter decoder-only language model pre-trained on diverse English datasets. This model is the successor to the first StableLM-Base-Alpha-7B model, addressing previous shortcomings through the use of improved data sources and mixture ratios.",2023-12-01 06:21,Anonymous,,0,Multinational,Stability AI,,,,,,,
CALM,Robotics,,"Chen Tessler, Yoni Kasten, Yunrong Guo, Shie Mannor, Gal Chechik, Xue Bin Peng",,,,https://research.nvidia.com/labs/par/calm/,9.00,CALM: Conditional Adversarial Latent Models for Directable Virtual Characters,2023-08-06,"NVIDIA,Technion - Israel Institute of Technology",Industry - Academia Collaboration (Industry Leaning),,,,The total pre-training of the networks involved 5 billion environment steps. The low-level policy operates at 30Hz while the high-level policy operates at 6Hz.,,"160 diverse motion clips totaling 30 minutes, from a motion capture dataset. These include motions like walking, running, sword strikes, etc.",,,,,,,,NVIDIA A100,NVIDIA A100,Reinforcement learning,,,Industry,Speculative,"In this work, we present Conditional Adversarial Latent Models (CALM),
an approach for generating diverse and directable behaviors for user-controlled
interactive virtual characters. Using imitation learning, CALM learns a representation of movement that captures the complexity and diversity of human
motion, and enables direct control over character movements. The approach
jointly learns a control policy and a motion encoder that reconstructs key
characteristics of a given motion without merely replicating it. The results
show that CALM learns a semantic motion representation, enabling control
over the generated motions and style-conditioning for higher-level task training. Once trained, the character can be controlled using intuitive interfaces,
akin to those found in video games.",2023-11-21 01:39,Anonymous,,,"United States of America,Israel","NVIDIA, Technion - Israel Institute of Technology",,,,,,,
Baichuan2-53B,Language,,,,,,https://technode.com/2023/08/09/chinese-ai-startup-baichuan-rolls-out-third-llm-in-four-months/,,Chinese AI startup Baichuan rolls out third LLM in four months,2023-08-09,Baichuan,,,,,,,,,,,,,,,,,,,,,Likely,"On Tuesday, four-month-old AI startup Baichuan Intelligent Technology unveiled its first closed-source model equipped with 53 billion parameters. Following the Chinese company’s rapid release of two open-source large language models since its founding in April, the new model demonstrates the firm’s fast pace in delivering pre-trained models for larger parameters. The freshly introduced model, Baichuan-53B, is mainly for corporate clients and focused on text generation. A ChatGPT-like chat service built on the model entered internal testing on the same day the model was launched, its official website shows, with plans for the firm to publicly launch APIs and associated components next month.",2023-12-07 07:21,Anonymous,,0,China,Baichuan,,,,,,,
Japanese StableLM Base Alpha 7B,Language,,"Meng Lee, Fujiki Nakamura, Makoto Shing, Paul McCann, Takuya Akiba, Naoki Orii",,"best open-source Japanese LM: ""It stands as the top-performing publicly available Japanese language model, according to a benchmark suite against four sets of other Japanese LMs.""",Permissive license,https://stability.ai/news/stability-ai-new-jplm-japanese-language-model-stablelm,,"Japanese StableLM, Marking Entry into International Language Model  Market",2023-08-10,Stability AI,,7000000000.00,7B,3.15e+22,"7b params, 750b tokens
7b * 750b * 6 = 3.15e22",,Japanese language sources + RedPajama,,,,,,,,,,,,,,Likely,"Japanese StableLM is a 7 billion-parameter general-purpose language model. It stands as the top-performing publicly available Japanese language model, according to a benchmark suite against four sets of other Japanese LMs.

Japanese StableLM Base Alpha 7B will be released under the commercially available Apache License 2.0. Japanese StableLM Instruct Alpha 7B is a model created for research purposes and is released exclusively for research use. For details, please refer to the Hugging Face Hub page.",2023-12-12 05:31,Anonymous,,0,Multinational,Stability AI,,,,,,,
Platypus-70B,Language,Language Generation,"Ariel N. Lee, Cole J. Hunter, Nataniel Ruiz",,"SOTA for open-source but not in general (per Table 3)

""As per the Hugging Face Open LLM Leaderboard data dated 8/10/23 (Table 3), our Platypus2-70Binstruct variant has outperformed its competitors, securing the top position with an average score of 73.13""",Permissive license,https://arxiv.org/abs/2308.07317,14.00,"Platypus: Quick, Cheap, and Powerful Refinement of LLMs",2023-08-14,Boston University,Academia,70000000000.00,fine-tuned from LLaMa 2-70B,,"""For example, our 13B model was fine-tuned using 1 A100 80GB for 5 hours and our 70B model using 4 A100s 80GB for 22 hours""

4*22 A100-hours is roughly 1e20, or 2-3 OOM less compute than required to train the Llama-2 70B.",,"Platypus is fine-tuned Llama 2, so starts with whatever dataset was used to train Llama 2. Then they fine-tuned using Open-Platypus:

""Open-Platypus, a small-scale dataset that consists of a curated sub-selection of public text datasets. The dataset is focused on improving LLMs’ STEM and logic knowledge, and is made up of 11 open-source datasets. It is comprised mainly of human-designed questions,
with only 10% of questions generated by an LLM. """,,,,,,,,NVIDIA A100,NVIDIA A100,,,,,Likely,"We present Platypus, a family of fine-tuned and merged Large Language Models (LLMs) that achieves the strongest performance and currently stands at first place in HuggingFace's Open LLM Leaderboard as of the release date of this work. In this work we describe (1) our curated dataset Open-Platypus, that is a subset of other open datasets and which we release to the public (2) our process of fine-tuning and merging LoRA modules in order to conserve the strong prior of pretrained LLMs, while bringing specific domain knowledge to the surface (3) our efforts in checking for test data leaks and contamination in the training data, which can inform future research. Specifically, the Platypus family achieves strong performance in quantitative LLM metrics across model sizes, topping the global Open LLM leaderboard while using just a fraction of the fine-tuning data and overall compute that are required for other state-of-the-art fine-tuned LLMs. In particular, a 13B Platypus model can be trained on a single A100 GPU using 25k questions in 5 hours. This is a testament of the quality of our Open-Platypus dataset, and opens opportunities for more improvements in the field. Project page: this https URL",2023-12-05 04:33,Anonymous,,,United States of America,Boston University,Llama 2-70B,39540000000000000000,"22 hours on 4 A100 GPUs.

If FP16, this was around 312 TFLOPS * 88 hours * 40% = 3.954*10^19 FLOP

It would be double this if they used INT8.",,,,
Code Llama-34B,Language,Code generation,"Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Ellen Tan, Yossef (Yossi) Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Defossez, Jade Copet, Faisal Azhar, Hugo Touvron, Gabriel Synnaeve, Louis Martin, Nicolas Usunier, Thomas Scialom",,"SOTA for open models: ""Moreover, our largest model,
with its 34B parameters, is significantly larger than previous open-source models – GPT-NeoX-20B (Black
et al., 2022) and StarCoder with 15.5B parameters – which allows it to achieve state-of-the-art performances
on HumanEval, MBPP and MultiPL-E among open-source models""",Permissive license,https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/,96.00,Code Llama: Open Foundation Models for Code,2023-08-14,Meta AI,Industry,34000000000.00,34B,5.3e+23,"1.22e23 finetune compute, or ~5.3e23 including Llama-2 34B base compute
",,"""As shown in Table 1, Code Llama is trained predominantly on a near-deduplicated dataset of
publicly available code. We also source 8% of our samples data from natural language datasets related to
code. This dataset contains many discussions about code and code snippets included in natural language
questions or answers. To help the model retain natural language understanding skills, we also sample a small
proportion of our batches from a natural language dataset""",,,,,,,,NVIDIA A100 SXM4 80 GB,NVIDIA A100 SXM4 80 GB,,,,,Likely,"We release Code Llama, a family of large language models for code based on Llama 2 providing state-of-the-art performance among open models, infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks. We provide multiple flavors to cover a wide range of applications: foundation models (Code Llama), Python specializations (Code Llama - Python), and instruction-following models (Code Llama - Instruct) with 7B, 13B and 34B parameters each. All models are trained on sequences of 16k tokens and show improvements on inputs with up to 100k tokens. 7B and 13B Code Llama and Code Llama - Instruct variants support infilling based on surrounding content. Code Llama reaches state-of-the-art performance among open models on several code benchmarks, with scores of up to 53% and 55% on HumanEval and MBPP, respectively. Notably, Code Llama - Python 7B outperforms Llama 2 70B on HumanEval and MBPP, and all our models outperform every other publicly available model on MultiPL-E. We release Code Llama under a permissive license that allows for both research and commercial use.",2023-12-09 09:16,Anonymous,,0,Multinational,Meta AI,Llama 2-34B,1,"Training the nine Code Llama models took 400k A100-hours across all the models, per model card. It's nine models because there are three base models at 7B, 13B, 34B, and then Instruct and Python models across all three sizes. I'll calculate for Code Llama Python-34B since it's the most trained.

Code Llama-base is fine-tuned from Llama 2 with 500B tokens: ""We train Code Llama on 500B tokens during the initial phase, starting from the 7B, 13B, and 34B versions of Llama 2""

Code Llama-Python required an additional 100B tokens in fine-tuning: ""We train Code Llama on 500B additional tokens and Code Llama - Python further on 100B tokens.""
Code Llama-Instruct is fine-tuned on 5B tokens: ""For Code Llama - Instruct, we train with a batch size of 524,288 tokens and on approx. 5B tokens in total.""

34B * (500B+100B) * 6 = 1.22e23

Using the hardware method, we get 400,000 * 3600 * 312 trillion * 0.3 = 1.32e23 for the whole family. Using 34/(34+13+7) as the proportion of compute used for the 34B model, we get 0.63 * 1.32e23 = 8.5e22. (this is rounding down instruct tuning to 0).

Token*params estimate is probably better; seems like evidence of utilization > 0.3.",,,,
DeciCoder,Language,Code generation,,,,Permissive license,https://huggingface.co/Deci/DeciCoder-1b,,,2023-08-15,Deci AI,,1100000000.00,"1.1B, per model card https://huggingface.co/Deci/DeciCoder-1b",2.9e+21,446b * 1.1b * 6 = 2.9e21,StarCoder,"""DeciCoder was trained on StarCoder Training Dataset, filtered for Python, Java, and Javascript code""",,,,,,,,,,,,,,Likely,"DeciCoder 1B is a 1 billion parameter decoder-only code completion model trained on the Python, Java, and Javascript subsets of Starcoder Training Dataset. The model uses Grouped Query Attention and has a context window of 2048 tokens. It was trained using a Fill-in-the-Middle training objective. The model's architecture was generated by Deci's proprietary Neural Architecture Search-based technology, AutoNAC.",2023-12-12 06:12,Anonymous,,0,Israel,Deci AI,,,,,,,
Dou Bao,Language,Chat,,,,,https://pandaily.com/bytedance-launches-its-first-large-scale-ai-conversation-product-dou-bao/,,,2023-08-18,ByteDance,,,,,,,,,,,,,,,,,,,,,Confident,"The first AI conversational app “Dou Bao” and its web version have recently been launched, with the download channel for Android already open. The “Dou Bao” is the internal codename “Grace” AI project by ByteDance, and currently has functions such as text-based conversation and image-based conversation.",2023-12-08 08:23,Anonymous,,0,China,ByteDance,,,,,,,
IDEFICS,Multimodal,Language modelling,"Hugo Laurencon, Daniel van Strien, Stas Bekman, Leo Tronchon, Lucile Saulnier, Thomas Wang, Siddharth Karamcheti, Amanpreet Singh, Giada Pistilli, Yacine Jernite, Victor Sanh",,,,https://huggingface.co/blog/idefics,0.00,Introducing IDEFICS: An Open Reproduction of State-of-the-Art Visual Language Model,2023-08-22,Hugging Face,Industry,80000000000.00,IDEFICS... comes in two variants—the base version and the instructed version. Each variant is available at the 9 billion and 80 billion parameter sizes.,,,"Wikipedia, Public Multimodal Dataset, LAION, OBELICS","IDEFICS was trained on a mixture of openly available datasets: Wikipedia, Public Multimodal Dataset, and LAION, as well as a new 115B token dataset called OBELICS that we created. OBELICS consists of 141 million interleaved image-text documents scraped from the web and contains 353 million images.",,,,,,,,,,,,,Industry,Speculative,"We are excited to release IDEFICS (Image-aware Decoder Enhanced à la Flamingo with Interleaved Cross-attentionS), an open-access visual language model. IDEFICS is based on Flamingo, a state-of-the-art visual language model initially developed by DeepMind, which has not been released publicly. Similarly to GPT-4, the model accepts arbitrary sequences of image and text inputs and produces text outputs.",2023-11-21 01:39,Anonymous,,,Multinational,Hugging Face,,,,,,,
PULI GPTrio,Language,Chat,"Zijian Győző Yang, László János Laki, Tamás Váradi & Gábor Prószéky ",,,Permissive license,https://link.springer.com/chapter/10.1007/978-3-031-40498-6_9; https://huggingface.co/NYTK/PULI-GPTrio,,Mono- and Multilingual GPT-3 Models for Hungarian,2023-08-23,Hungarian Research Centre for Linguistics,,6700000000.00,6.7B,5.8e+21,"8 A100s for three months

8 * 312 trillion * 24 * 3600 * 90 * 0.3 (utilization assumption) = 5.8e21",,"Mix of Hungarian, English, and Chinese text",214000000000,"adding up column in Table 2.

might be slightly off because it's counting non-Chinese tokens in the Chinese data, rather than non-Chinese words, but close.",,,,2200.0,3 months,NVIDIA A100,NVIDIA A100,,,,,Likely,"In recent years, the growth in size of Transformer-based language models has accelerated significantly. Global technology companies are training larger and larger models that require enormous resources and training data. With these experiments, they aim to demonstrate that sufficiently large models with abundant training data can solve any natural language processing task even without fine-tuning. It may not be feasible to compete directly in this race, but there is an opportunity to conduct experiments in the direction of larger models in their shadow. Our aim is to train large language models for Hungarian. According to the knowledge transfer researches, a language model can adapt valuable knowledge from other languages. Furthermore, in order for the model to be able to solve translation tasks, it also needs multilingual knowledge. In our research, we trained a Hungarian monolingual and a Hungarian-English-Chinese trilingual 6.7 billion parameter GPT language model with more than 1TB text data. In our experiments, we also fine-tuned our model with the prompts provided by the Stanford Alpaca dataset. Thus, employing this methodology, an instruct GPT was built, which, as far as we know, is the first multilingual large language model in this region that can follow instructions.",2023-12-08 06:38,Anonymous,,0,Hungary,Hungarian Research Centre for Linguistics,,,,,,,
Jais,Language,Language modelling,"Neha Sengupta, Sunil Kumar Sahu, Bokang Jia, Satheesh Katipomu, Haonan Li, Fajri Koto, Osama Mohammed Afzal, Samta Kamboj, Onkar Pandit, Rahul Pal, Lalit Pradhan, Zain Muhammad Mujahid, Massa Baali, Alham Fikri Aji, Zhengzhong Liu, Andy Hock, Andrew Feldman, Jonathan Lee, Andrew Jackson, Preslav Nakov, Timothy Baldwin, Eric Xing",SOTA Improvement,SOTA at Arabic language tasks.,,https://inceptioniai.org/jais/docs/Technicalpaper.pdf,7.00,Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models,2023-08-29,"Cerebras Systems,Mohamed bin Zayed University of Artificial Intelligence,Inception",Industry - Academia Collaboration (Industry leaning),13000000000.00,"""With 13 billion parameters, they demonstrate better knowledge and reasoning capabilities in Arabic""",3.08e+22,C = 6ND = 6 * 13 billion params * 395 billion tokens = 3.081e+22 FLOP,"Abu El-Khair, Aranews, ArabicText 2022, C4 Arabic, Arabic Wikipedia, ArabicNews 2020, Maktabah, United Nations Parallel Corpus, The Pile, Books3, arXiv, PubMed Central, OpenWebText2, English Wikipedia, FreeLaw, PubMed Abstracts, DeepMind Mathematics, Project Gutenberg, BookCorpus2, EuroParl, PhilPapers, YouTube Subtitles, NIH Grant Abstracts, Enron Emails, GitHub","It was pretrained on 395 billion tokens, including 116 billion Arabic tokens, 232 billion English tokens, and 46 billion tokens of code.
The Arabic data consists of 72 billion tokens, which was augmented by 18 billion tokens of translated English text and then upsampled 1.6 times to reach 116 billion tokens.
The English data is sampled from the Pile dataset and consists of 232 billion tokens.
The code data consists of 46 billion tokens sampled from GitHub.",300000000000,395B tokens ~= 300B words,,26000000000.00,26 billion FLOP per token,600.0,2023 June 25 to July 18 = 25 days = 600 hours,,,,,,Industry,Confident,"We introduce Jais and Jais-chat, new state-of-the-art Arabic-centric foundation and instruction-tuned open generative large language models (LLMs). The models are based on the GPT-3 decoder-only architecture
and are pretrained on a mixture of Arabic and English texts, including source code in various programming languages. With 13 billion parameters, they demonstrate better knowledge and reasoning capabilities in Arabic than any existing open Arabic and multilingual models by a sizable margin, based on extensive evaluation. Moreover, the models are competitive in English compared to English-centric open models of similar size, despite being trained on much less English data. We provide a detailed description of the training, the tuning, the safety alignment, and the evaluation of the models. We release two open versions of the model —the foundation Jais model, and an instruction-tuned Jais-chat variant— with the aim of promoting research on Arabic LLMs.",2023-12-05 04:33,Anonymous,,,"Multinational,United Arab Emirates,United States of America","Cerebras Systems, Mohamed bin Zayed University of Artificial Intelligence, Inception",,,,Jais,,,
Swift,Robotics,Helicopter driving,"Elia Kaufmann, Leonard Bauersfeld, Antonio Loquercio, Matthias Müller, Vladlen Koltun, Davide Scaramuzza ",SOTA Improvement,"""Our work marks the first time, to our knowledge, that an autonomous mobile robot achieved world-champion-level performance in a real-world competitive sport.""",,https://www.nature.com/articles/s41586-023-06419-4,1.00,Champion-level drone racing using deep reinforcement learning,2023-08-30,Intel Labs,Industry - Academia Collaboration (Industry leaning),21124.00,"The control network is an MLP with input dimension 31, two hidden layers of size 128, and an output of dimension 4.
(31+1)*128+(128+1)*128+(128+1)*4 = 21124",53370000000000000.00,"Policies are trained for a total of 1 × 108 environment interactions, which takes 50 min on a workstation (i9 12900K, RTX 3090, 32 GB RAM DDR5). Fine-tuning is performed for 2 × 107 environment interactions.

35.58 TFLOPS * 50 min * 60 s/min * 0.50 utilization = 5.337*10^16 FLOP",,,,,,,,0.8,"50 minutes (training details, page 8)",NVIDIA GeForce RTX 3090,NVIDIA GeForce RTX 3090,Reinforcement learning,,,Industry,Likely,"First-person view (FPV) drone racing is a televised sport in which professional competitors pilot high-speed aircraft through a 3D circuit. Each pilot sees the environment from the perspective of their drone by means of video streamed from an onboard camera. Reaching the level of professional pilots with an autonomous drone is challenging because the robot needs to fly at its physical limits while estimating its speed and location in the circuit exclusively from onboard sensors1. Here we introduce Swift, an autonomous system that can race physical vehicles at the level of the human world champions. The system combines deep reinforcement learning (RL) in simulation with data collected in the physical world. Swift competed against three human champions, including the world champions of two international leagues, in real-world head-to-head races. Swift won several races against each of the human champions and demonstrated the fastest recorded race time. This work represents a milestone for mobile robotics and machine intelligence2, which may inspire the deployment of hybrid learning-based solutions in other physical systems.",2023-11-26 03:25,Anonymous,,,Multinational,Intel Labs,,,,Swift,,,
360 Smart Brain,Multimodal,"Language Generation,Chat,Image generation",,,,,https://www.hayo.com/article/64f68f1d8578eea6c7ec663f,,The 360 ​​Brain Model is now open to the public,2023-09-04,360 Security Technology,,,"""hundreds of billions"" https://www.hayo.com/article/650babb37c769bcba319ed83",,"hundreds of billions of parameters, trained on over 1T tokens, per this: https://www.hayo.com/article/650babb37c769bcba319ed83

vague report from a Google-translated article, though",,,,,,,,,,,,,,,,Likely,"According to news on September 5, the 360 ​​Smart Brain large model will be open to the public from now on and will be fully accessible to the 360 ​​“Family Bucket”.

360 Zhi Nao will be open to the public on five major platforms. Users can download the “360 Zhi Nao” App through the 360 ​​Zhi Nao official website and major application stores.",2023-12-08 03:09,Anonymous,,0,China,360 Security Technology,,,,,,,
Falcon 180B,Language,Language modelling,,SOTA Improvement,"""It's currently at the top of the Hugging Face Leaderboard for pre-trained Open Large Language Models and is available for both research and commercial use.""

""This model performs exceptionally well in various tasks like reasoning, coding, proficiency, and knowledge tests, even beating competitors like Meta's LLaMA 2.""",Permissive license,https://falconllm.tii.ae/falcon-180b.html,0.00,Falcon LLM - Falcon 180B,2023-09-06,Technology Innovation Institute,Government,180000000000.00,"""Falcon 180B is a super-powerful language model with 180 billion parameters""",3.78e+24,C = 6ND = 6 FLOP/token/parameter * 3.5 trillion tokens * 180 billion parameters = 3.78*10^24 FLOP,,,2625000000000,3.5 trillion tokens * (~3 words per 4 tokens) ~= 2.625 trillion words,,360000000000.00,C_inference = 2 FLOP / token / param * N => 360B FLOP per token,4320.0,"Stanford CRFM foundation model ecosystem graph data page says 9 months: https://crfm.stanford.edu/ecosystem-graphs/index.html?asset=Falcon-180B

However, I think 6 months is more realistic, because this is the release gap between this and Falcon 40B.",NVIDIA A100 SXM4 40 GB,NVIDIA A100 SXM4 40 GB,,,"From Hugging Face:
""Falcon-180B was trained on up to 4,096 A100 40GB GPUs, using a 3D parallelism strategy (TP=8, PP=8, DP=64) combined with ZeRO.""
""Falcon-180B was trained on AWS SageMaker, on up to 4,096 A100 40GB GPUs in P4d instances.""
https://huggingface.co/tiiuae/falcon-180B

Utilization must have been at least 12.5%, and they probably did not use the whole 4096 GPU cluster for 9 months, so it was probably higher. Lower bound estimate:
https://www.wolframalpha.com/input?i=%286+FLOP+*+3.5+trillion+*+180+billion%29+%2F+%284096*312+teraFLOPS+*+9+months%29",,Likely,"Falcon 180B is a super-powerful language model with 180 billion parameters, trained on 3.5 trillion tokens. It's currently at the top of the Hugging Face Leaderboard for pre-trained Open Large Language Models and is available for both research and commercial use.

This model performs exceptionally well in various tasks like reasoning, coding, proficiency, and knowledge tests, even beating competitors like Meta's LLaMA 2.

Among closed source models, it ranks just behind OpenAI's GPT 4, and performs on par with Google's PaLM 2 Large, which powers Bard, despite being half the size of the model.",2023-12-01 04:22,Anonymous,,,United Arab Emirates,Technology Innovation Institute,,,,Falcon 180B,4096,0.1876,Falcon 180B
Baichuan2-13B,Language,Chat,,,,Permissive license,https://huggingface.co/baichuan-inc/Baichuan2-13B-Base,,,2023-09-06,Baichuan,,13000000000.00,,2.03e+23,"They describe the training set as 2.6T tokens, but the checkpoint graph makes it clear that's also the number of tokens the model was trained on.

13b * 2.6t * 6 = 2.03e23",,"2.6 trillion tokens, bilingual",2300000000000,"2.6T tokens, or ~2.3T words assuming that the dataset is roughly even English (0.75 words/token) and Chinese (1 word/token)",1.00,,,,,,,,,,,Likely,,2023-12-14 05:36,Anonymous,,0,China,Baichuan,,,,,,,
Persimmon-8B,Language,Language modelling,"Erich Elsen, Augustus Odena, Maxwell Nye, Sağnak Taşırlar, Tri Dao, Curtis Hawthorne, Deepak Moparthi, Arushi Somani",,,,https://www.adept.ai/blog/persimmon-8b,0.00,Releasing Persimmon-8B,2023-09-07,,,9300000000.00,"""The checkpoint we are releasing has approximately 9.3B parameters. In order to make pipelining during training more efficient, we chose to decouple the input and output embeddings. Doing this does not increase the capacity of the model–it is purely a systems optimization to avoid all-reducing the gradients for the (very large) embeddings across potentially slow communication links. In terms of inference cost, the model is equivalent to an 8B parameter model with coupled input/output embeddings.""",,,,"We train the model from start to finish with a sequence length of 16K on 737B tokens uniformly sampled from a much larger dataset, which is a mix of text (~75%) and code (~25%).",552750000000,737B tokens = 552750M words,,16000000000.00,"The checkpoint we are releasing has approximately 9.3B parameters. In order to make pipelining during training more efficient, we chose to decouple the input and output embeddings. Doing this does not increase the capacity of the model–it is purely a systems optimization to avoid all-reducing the gradients for the (very large) embeddings across potentially slow communication links. In terms of inference cost, the model is equivalent to an 8B parameter model with coupled input/output embeddings.

8B * 2 FLOP/token/parameter = 16B FLOP/token inference compute",,,,,,,,Industry,Likely,,2023-10-31 05:12,Anonymous,,,,,,,,,,,
FLM-101B,Language,,"Xiang Li, Yiqun Yao, Xin Jiang, Xuezhi Fang, Xuying Meng, Siqi Fan, Peng Han, Jing Li, Li Du, Bowen Qin, Zheng Zhang, Aixin Sun, Yequan Wang",,,,https://arxiv.org/abs/2309.03852,3.00,FLM-101B: An Open LLM and How to Train It with $100K Budget,2023-09-07,"Chinese Academy of Sciences,Harbin Institute of Technology,Nanyang Technological University,Beijing Academy of Artificial Intelligence",Academia,101000000000.00,,5.720000000000001e+22,"192 GPUs * 160 TFLOP/s per GPU (reported, adjusted for utilization) * 21.54 days * 24 * 3600 = 5.72e22",,"""By design, FLM-101B is an English-Chinese bilingual model pre-trained with causal language modeling. It mixes English and Chinese corpora at a ratio of approximately 53.5% : 46.5% for language modeling.""",350000000000,"Trained with 311.54B tokens. The dataset is approximately 50/50 English/Chinese: ""It mixes English and Chinese corpora at a ratio of approximately 53.5% : 46.5% for language modeling"". We assume 1 Chinese word per token and 0.75 English words per token (0.875 on average). 311B/0.875 ~= 350B.",,,,517.0,"""Under this growth schedule, the total time cost for our 101B model is 21.54 days""",NVIDIA A800,NVIDIA A800,,$84000.00,Authors report $100k. Adjusted for inflation.,,Likely,"Large language models (LLMs) have achieved remarkable success in NLP and multimodal tasks, among others. Despite these successes, two main challenges remain in developing LLMs: (i) high computational cost, and (ii) fair and objective evaluations. In this paper, we report a solution to significantly reduce LLM training cost through a growth strategy. We demonstrate that a 101B-parameter LLM with 0.31T tokens can be trained with a budget of 100K US dollars. Inspired by IQ tests, we also consolidate an additional range of evaluations on top of existing evaluations that focus on knowledge-oriented abilities. These IQ evaluations include symbolic mapping, rule understanding, pattern mining, and anti-interference. Such evaluations minimize the potential impact of memorization. Experimental results show that our model, named FLM-101B, trained with a budget of 100K US dollars, achieves performance comparable to powerful and well-known models, e.g., GPT-3 and GLM-130B, especially on the additional range of IQ evaluations.",2023-11-23 06:53,Anonymous,,,"China,China,Singapore,China","Chinese Academy of Sciences, Harbin Institute of Technology, Nanyang Technological University, Beijing Academy of Artificial Intelligence",,,,,,,
XGen-7B,Language,Language Generation,"Erik Nijkamp, Tian Xie, Hiroaki Hayashi, Bo Pang, Congying Xia, Chen Xing, Jesse Vig, Semih Yavuz, Philippe Laban, Ben Krause, Senthil Purushwalkam, Tong Niu, Wojciech Kryscinski, Lidiya Murakhovs’ka, Prafulla Kumar Choubey, Alex Fabbri, Ye Liu, Rui Meng, Lifu Tu, Meghana Bhat, Chien-Sheng Wu, Silvio Savarese, Yingbo Zhou, Shafiq Joty, Caiming Xiong",SOTA Improvement,"""Our evaluation on standard benchmarks shows that XGen-7B models
achieve comparable or better results when compared with state-of-the-art opensource LLMs""",Permissive license,https://arxiv.org/abs/2309.03450,3.00,XGen-7B Technical Report,2023-09-07,Salesforce,Industry,6700000000.00,"I think this suggests the same number of params as Llama-7b. In any case ~7B. 
""The model architecture follows LLaMA with exact numerical compatibility to ease adoption in third-party frameworks. The hyperparameters closely follow LLaMA-7B [34] with the following alterations""",8.02e+22,"270,336 TPUv4-hours per the carbon emissions section. This is probably the total for the whole paper? They released two versions of the base model, one of which is just trained further than the other, and they have two fine-tuned versions. If fine-tuning compute is minor, then this total is close to the largest training compute for any single model in the paper. 

270336 * 275 trillion * 3600 * 0.3 = 8.02e22 FLOP",,"""Our pre-training dataset is a mixture of data from several public sources, reported in Table 2. We employ a two-stage training strategy, where each stage uses a different data mixture, as shown in
Table 3.
Natural language data for stage 1. Natural language data is a mixture of publicly available data.
We made an effort to improve safety and diversity of the data.
Code data for stage 1. We use the GitHub subset from the recently released RedPajama dataset [9].
We also added Apex code data to enhance our model’s proficiency in Apex code generation. Apex is
a widely used object-oriented programming language in Salesforce products.
BigCode Starcoder data for stage 2. We use all the 86 programming languages from the Starcoder [15] data, preserving the original weight of each. Subsequently, we further filter the data
according to a stronger permissive license guideline.""",1113000000000,1484B tokens per Table 2. 1113B words at 0.75 words/token,1.00,,,,,Google TPU v4,Google TPU v4,,,,,Likely,"Large Language Models (LLMs) have become ubiquitous across various domains,
transforming the way we interact with information and conduct research. However,
most high-performing LLMs remain confined behind proprietary walls, hindering
scientific progress. Most open-source LLMs, on the other hand, are limited in their
ability to support longer sequence lengths, which is a key requirement for many
tasks that require inference over an input context. To address this, we have trained
XGen-7B, a series of 7B parameter models on up to 8K sequence length for up
to 1.5T tokens. We have also finetuned the XGen-7B models on public-domain
instructional data, creating their instruction-tuned counterparts (XGen-7B-Inst).
We open-source our models for both research advancements and commercial
applications. Our evaluation on standard benchmarks shows that XGen-7B models
achieve comparable or better results when compared with state-of-the-art opensource LLMs. Our targeted evaluation on long sequence modeling tasks shows the
benefits of our 8K-sequence models over 2K-sequence open-source LLMs.",2023-11-26 03:25,Anonymous,,,United States of America,Salesforce,,,,XGen-7B,,,
Phi-1.5,Language,Language Generation,"Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, Yin Tat Lee",,,Permissive license,"https://arxiv.org/abs/2309.05463, https://huggingface.co/microsoft/phi-1_5",18.00,Textbooks Are All You Need II: phi-1.5 technical report,2023-09-11,Microsoft,,1300000000.00,,1.17e+21,"150B training tokens

150B*1.3B*6 = 1.17e21

also, took 1.5k GPU-hours with A100s, per Table 1

1500 * 312 trillion * 3600 * 0.3 = 5.05e20
",,"Synthetic ""textbook"" data: 

""Our training data for phi-1.5 is a combination of phi-1’s training data (7B tokens) and newly created
synthetic, “textbook-like” data (roughly 20B tokens) for the purpose of teaching common sense reasoning
and general knowledge of the world (science, daily activities, theory of mind, etc.)""",22500000000,"30B tokens, or ~22.5B words",5.00,,,192.0,,NVIDIA A100 SXM4 40 GB,NVIDIA A100 SXM4 40 GB,,,,,Likely,"We continue the investigation into the power of smaller Transformer-based language models as initiated by \textbf{TinyStories} -- a 10 million parameter model that can produce coherent English -- and the follow-up work on \textbf{phi-1}, a 1.3 billion parameter model with Python coding performance close to the state-of-the-art. The latter work proposed to use existing Large Language Models (LLMs) to generate ``textbook quality"" data as a way to enhance the learning process compared to traditional web data. We follow the ``Textbooks Are All You Need"" approach, focusing this time on common sense reasoning in natural language, and create a new 1.3 billion parameter model named \textbf{phi-1.5}, with performance on natural language tasks comparable to models 5x larger, and surpassing most non-frontier LLMs on more complex reasoning tasks such as grade-school mathematics and basic coding. More generally, \textbf{phi-1.5} exhibits many of the traits of much larger LLMs, both good -- such as the ability to ``think step by step"" or perform some rudimentary in-context learning -- and bad, including hallucinations and the potential for toxic and biased generations -- encouragingly though, we are seeing improvement on that front thanks to the absence of web data. We open-source \textbf{phi-1.5} to promote further research on these urgent topics.",2023-12-14 07:55,Anonymous,,0,Multinational,Microsoft,,,,,,,
Robot Parkour,Robotics,,"Ziwen Zhuang, Zipeng Fu, Jianren Wang, Christopher Atkeson, Soeren Schwertfeger, Chelsea Finn, Hang Zhao",SOTA Improvement,,,https://arxiv.org/abs/2309.05665,9.00,Robot Parkour Learning,2023-09-12,"Shanghai Qi Zhi institute,Stanford University,Carnegie Mellon University (CMU),Tsinghua University",Industry - Academia Collaboration,500000.00,"Parkour policy details on page 8, table 11.",,"The paper provides some details on the training time and hardware used:

Each specialized skill policy (climbing, leaping, etc) was pre-trained with soft dynamics constraints for 12 hours using 1 Nvidia RTX 3090 GPU.
The skills were then fine-tuned with hard dynamics constraints for 6 hours each.
The final parkour policy distillation process used 4 computers with 1 RTX 3090 GPU each, training for an unspecified amount of time.
So the total training time was at least 12 + 6 x 5 = 42 hours for the initial skills, plus an additional unknown time for the distillation.

The hardware used was high-end Nvidia RTX 3090 GPUs, which at the time of paper writing would have been top of the line GPUs. Multiple GPUs were used in parallel during the distillation stage.",,"Isaac Gym simulated proprioceptive data, images, and actions",,,,,,,,NVIDIA GeForce RTX 3090,NVIDIA GeForce RTX 3090,,,,,Likely,"Parkour is a grand challenge for legged locomotion that requires robots to overcome various obstacles rapidly in complex environments. Existing methods can generate either diverse but blind locomotion skills or vision-based but specialized skills by using reference animal data or complex rewards. However, autonomous parkour requires robots to learn generalizable skills that are both vision-based and diverse to perceive and react to various scenarios. In this work, we propose a system for learning a single end-to-end vision-based parkour policy of diverse parkour skills using a simple reward without any reference motion data. We develop a reinforcement learning method inspired by direct collocation to generate parkour skills, including climbing over high obstacles, leaping over large gaps, crawling beneath low barriers, squeezing through thin slits, and running. We distill these skills into a single vision-based parkour policy and transfer it to a quadrupedal robot using its egocentric depth camera. We demonstrate that our system can empower two different low-cost robots to autonomously select and execute appropriate parkour skills to traverse challenging real-world environments.",2023-12-05 04:33,Anonymous,,,"China,United States of America,United States of America,China","Shanghai Qi Zhi institute, Stanford University, Carnegie Mellon University (CMU), Tsinghua University",,,,Robot Parkour,,,
BTLM-3B,Language,"Language Generation,Code generation","Nolan Dey, Daria Soboleva, Faisal Al-Khateeb, Bowen Yang, Ribhu Pathria, Hemant Khachane, Shaheer Muhammad, Zhiming (Charles)Chen, Robert Myers, Jacob Robert Steeves, Natalia Vassilieva, Marvin Tom, Joel Hestness",,"SOTA for its parameter class: ""BTLM-3B-8K achieves state-of-the-art performance among 3B parameter models""",Permissive license,https://arxiv.org/abs/2309.11568,,BTLM-3B-8K: 7B Parameter Performance in a 3B Parameter Model,2023-09-20,Cerebras Systems,,2600000000.00,"2.6B, per paper",9.8e+21,2.6b params * 627b tokens * 6 = 9.8e21,SlimPajama,"""To bolster BTLM’s performance, we create a high quality
627B token dataset called SlimPajama ((Soboleva et al., 2023)). Starting from the 1.21T token RedPajama dataset Computer (2023), we apply filtering and deduplication to improve data quality. First, we
remove documents containing fewer than 200 characters, as we find these typically contain only metadata. Next, we perform global deduplication using MinHashLSH (Leskovec et al., 2014) to extensively
remove documents with significant overlapping text.""",470000000000,"627B tokens, equivalent to 470B english words",1.00,,,,,Cerebras CS-2,Cerebras CS-2,,,,,Likely,"We introduce the Bittensor Language Model, called ""BTLM-3B-8K"", a new state-of-the-art 3 billion parameter open-source language model. BTLM-3B-8K was trained on 627B tokens from the SlimPajama dataset with a mixture of 2,048 and 8,192 context lengths. BTLM-3B-8K outperforms all existing 3B parameter models by 2-5.5% across downstream tasks. BTLM-3B-8K is even competitive with some 7B parameter models. Additionally, BTLM-3B-8K provides excellent long context performance, outperforming MPT-7B-8K and XGen-7B-8K on tasks up to 8,192 context length. We trained the model on a cleaned and deduplicated SlimPajama dataset; aggressively tuned the \textmu P hyperparameters and schedule; used ALiBi position embeddings; and adopted the SwiGLU nonlinearity.
On Hugging Face, the most popular models have 7B parameters, indicating that users prefer the quality-size ratio of 7B models. Compacting the 7B parameter model to one with 3B parameters, with little performance impact, is an important milestone. BTLM-3B-8K needs only 3GB of memory with 4-bit precision and takes 2.5x less inference compute than 7B models, helping to open up access to a powerful language model on mobile and edge devices. BTLM-3B-8K is available under an Apache 2.0 license on Hugging Face: this https URL.",2023-12-12 04:56,Anonymous,,0,Multinational,Cerebras Systems,,,,,,,
GPT-4V,Multimodal,Language model,,Significant use,Incorporated into ChatGPT,,https://cdn.openai.com/papers/GPTV_System_Card.pdf,0.00,GPT-4V(ision) system card,2023-09-25,OpenAI,Industry,,,,,,,,,,,,,,,,Reinforcement learning,,,Industry,Speculative,"GPT-4 with vision (GPT-4V) enables users to instruct GPT-4 to analyze image inputs provided by the user, and is the latest capability we are making broadly available. Incorporating additional modalities (such as image inputs) into large language models (LLMs) is viewed by some as a key frontier in artificial intelligence research and development. Multimodal LLMs offer the possibility of expanding the impact of language-only systems with novel interfaces and capabilities, enabling them to solve new tasks and provide novel experiences for their users. In this system card, we analyze the safety properties of GPT-4V. Our work on safety for GPT-4V builds on the work done for GPT-4 and here we dive deeper into the evaluations, preparation, and mitigation work done specifically for image inputs.",2023-11-26 03:25,Anonymous,,,United States of America,OpenAI,,,,GPT-4V,,,
Show-1,Text-to-Video,,"David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, Mike Zheng Shou",SOTA Improvement,"""Our approach achieves state-of-the-art performance on standard benchmarks including UCF-101 and MSR-VTT.""",,https://arxiv.org/abs/2309.15818,,Show-1: Marrying Pixel and Latent Diffusion Models for Text-to-Video Generation,2023-09-27,National University of Singapore,Academia,,,,,WebVid-10M,"""WebVid-10M is a large-scale dataset of short videos with textual descriptions sourced from stock footage sites. The videos are diverse and rich in their content. 10.7M video-caption pairs. 52K total video hours.""",,,,,,,,NVIDIA A100,NVIDIA A100,,,,,,"Significant advancements have been achieved in the realm of large-scale pre-trained text-to-video Diffusion Models (VDMs). However, previous methods either rely solely on pixel-based VDMs, which come with high computational costs, or on latent-based VDMs, which often struggle with precise text-video alignment. In this paper, we are the first to propose a hybrid model, dubbed as Show-1, which marries pixel-based and latent-based VDMs for text-to-video generation. Our model first uses pixel-based VDMs to produce a low-resolution video of strong text-video correlation. After that, we propose a novel expert translation method that employs the latent-based VDMs to further upsample the low-resolution video to high resolution. Compared to latent VDMs, Show-1 can produce high-quality videos of precise text-video alignment; Compared to pixel VDMs, Show-1 is much more efficient (GPU memory usage during inference is 15G vs 72G). We also validate our model on standard video generation benchmarks. Our code and model weights are publicly available at this https URL.",2023-11-26 03:25,Anonymous,,,Singapore,National University of Singapore,,,,Show-1,,,
Qwen-14B,Language,,"Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, Tianhang Zhu",,,Permissive license,https://arxiv.org/abs/2309.16609,28.00,Qwen Technical Report,2023-09-28,Alibaba,,14000000000.00,14B,2.5e+23,"3T tokens per Table 1

14b*3T*6 = 2.5e23",,"""Our dataset is designed to meet these requirements and includes public
web documents, encyclopedia, books, codes, etc. Additionally, our dataset is multilingual, with a
significant portion of the data being in English and Chinese.""",,,,,,,,,,,,,,Likely,"Large language models (LLMs) have revolutionized the field of artificial intelligence, enabling natural language processing tasks that were previously thought to be exclusive to humans. In this work, we introduce Qwen, the first installment of our large language model series. Qwen is a comprehensive language model series that encompasses distinct models with varying parameter counts. It includes Qwen, the base pretrained language models, and Qwen-Chat, the chat models finetuned with human alignment techniques. The base language models consistently demonstrate superior performance across a multitude of downstream tasks, and the chat models, particularly those trained using Reinforcement Learning from Human Feedback (RLHF), are highly competitive. The chat models possess advanced tool-use and planning capabilities for creating agent applications, showcasing impressive performance even when compared to bigger models on complex tasks like utilizing a code interpreter. Furthermore, we have developed coding-specialized models, Code-Qwen and Code-Qwen-Chat, as well as mathematics-focused models, Math-Qwen-Chat, which are built upon base language models. These models demonstrate significantly improved performance in comparison with open-source models, and slightly fall behind the proprietary models.",2023-12-12 08:11,Anonymous,,1,China,Alibaba,,,,,,,
Amazon Titan,Language,,,,,API accessible,https://aws.amazon.com/bedrock/titan/,,,2023-09-28,Amazon,,,,,trained using NVIDIA NeMo: https://blogs.nvidia.com/blog/nemo-amazon-titan/,,,,,,,,,,,,,,,Industry,Likely,,2023-12-09 07:26,Anonymous,,0,Multinational,Amazon,,,,,,,
StableLM-3B-4E1T,Language,Language Generation,"Jonathan Tow, Marco Bellagente, Dakota Mahan, Carlos Riquelme Ruiz",,,,https://stability.wandb.io/stability-llm/stable-lm/reports/StableLM-3B-4E1T--VmlldzoyMjU4?accessToken=u3zujipenkx5g7rtcj9qojjgxpconyjktjkli2po09nffrffdhhchq045vp0wyfo,,,2023-09-29,Stability AI,Industry,2795443200.00,,6.21e+22,"""StableLM-3B-4E1T was trained on the Stability AI cluster across 256 NVIDIA A100 40GB GPUs (AWS P4d instances). Training began on August 23, 2023, and took approximately 30 days to complete.""

256 * 30 * 24* 3600 * 312 trillion * 0.3 utilization (assumption) = 6.21e22",,"""The dataset is comprised of a filtered mixture of open-source large-scale datasets available on the HuggingFace Hub: Falcon RefinedWeb extract (Penedo et al., 2023), RedPajama-Data (Together Computer, 2023) and The Pile (Gao et al., 2020), both without the Books3 subset, and StarCoder (Li et al., 2023). The complete list is provided in Table 1.""",750000000000,Trained on 1T tokens (~750B words),4.00,,,720.0,approximately 30 days,NVIDIA A100,NVIDIA A100,,,,,Likely,"StableLM-3B-4E1T is a 3 billion (3B) parameter language model pre-trained under the multi-epoch regime to study the impact of repeated tokens on downstream performance. Given prior success in this area (Taylor et al., 2022 and Tay et al., 2023), we train on 1 trillion (1T) tokens for 4 epochs following the observations of Muennighoff et al. (2023) in ""Scaling Data-Constrained Language Models"" in which they find ""training with up to 4 epochs of repeated data yields negligible changes to loss compared to having unique data."" Further inspiration for the token count is taken from ""Go smol or go home"" (De Vries, 2023), which suggests a 2.96B model trained for 2.85 trillion tokens achieves a similar loss to a Chinchilla compute-optimal 9.87B language model.",2023-11-23 06:53,Anonymous,,,Multinational,Stability AI,,,,,,,
CTM (CIFAR-10),Drawing,Image generation,"Dongjun Kim, Chieh-Hsin Lai, Wei-Hsiang Liao, Naoki Murata, Yuhta Takida, Toshimitsu Uesaka, Yutong He, Yuki Mitsufuji, Stefano Ermon",SOTA Improvement,"""CTM... achieves new state-of-the-art FIDs for single-step diffusion model sampling on CIFAR-10 (FID 1.73)""",,https://arxiv.org/abs/2310.02279v1,2.00,Consistency Trajectory Models: Learning Probability Flow ODE Trajectory of Diffusion,2023-10-01,"Stanford University,Sony",,,,,,CIFAR-10,,,,,,,,,NVIDIA V100,NVIDIA V100,,,,,Likely,"Consistency Models (CM) (Song et al., 2023) accelerate score-based diffusion model sampling at the cost of sample quality but lack a natural way to trade-off quality for speed. To address this limitation, we propose Consistency Trajectory Model (CTM), a generalization encompassing CM and score-based models as special cases. CTM trains a single neural network that can -- in a single forward pass -- output scores (i.e., gradients of log-density) and enables unrestricted traversal between any initial and final time along the Probability Flow Ordinary Differential Equation (ODE) in a diffusion process. CTM enables the efficient combination of adversarial training and denoising score matching loss to enhance performance and achieves new state-of-the-art FIDs for single-step diffusion model sampling on CIFAR-10 (FID 1.73) and ImageNet at 64X64 resolution (FID 2.06). CTM also enables a new family of sampling schemes, both deterministic and stochastic, involving long jumps along the ODE solution trajectories. It consistently improves sample quality as computational budgets increase, avoiding the degradation seen in CM. Furthermore, CTM's access to the score accommodates all diffusion model inference techniques, including exact likelihood computation.",2023-12-07 06:27,Anonymous,,0,"United States of America,Japan","Stanford University, Sony",,,,,,,
FinGPT-13B,Language,,"Neng Wang, Hongyang Yang, Christina Dan Wang",SOTA Improvement,SOTA for financial sentiment analysis,Permissive license,https://arxiv.org/abs/2310.04793; https://github.com/AI4Finance-Foundation/FinGPT,1.00,FinGPT: Instruction Tuning Benchmark for Open-Source Large Language Models in Financial Datasets,2023-10-07,"University of California Los Angeles (UCLA),Columbia University,New York University (NYU)",,13000000000.00,,,,,Financial sentiment data (for fine-tuning): https://huggingface.co/datasets/FinGPT/fingpt-sentiment-train,,,,,,,,,,Supervised,,,,Likely,"In the swiftly expanding domain of Natural Language Processing (NLP), the potential of GPT-based models for the financial sector is increasingly evident. However, the integration of these models with financial datasets presents challenges, notably in determining their adeptness and relevance. This paper introduces a distinctive approach anchored in the Instruction Tuning paradigm for open-source large language models, specifically adapted for financial contexts. Through this methodology, we capitalize on the interoperability of open-source models, ensuring a seamless and transparent integration. We begin by explaining the Instruction Tuning paradigm, highlighting its effectiveness for immediate integration. The paper presents a benchmarking scheme designed for end-to-end training and testing, employing a cost-effective progression. Firstly, we assess basic competencies and fundamental tasks, such as Named Entity Recognition (NER) and sentiment analysis to enhance specialization. Next, we delve into a comprehensive model, executing multi-task operations by amalgamating all instructional tunings to examine versatility. Finally, we explore the zero-shot capabilities by earmarking unseen tasks and incorporating novel datasets to understand adaptability in uncharted terrains. Such a paradigm fortifies the principles of openness and reproducibility, laying a robust foundation for future investigations in open-source financial large language models (FinLLMs).",2023-12-08 07:13,Anonymous,,0,"United States of America,United States of America,United States of America","University of California Los Angeles (UCLA), Columbia University, New York University (NYU)",Llama 2-70B,2200000000000000000,"fine-tuned Llama 2 13B

RTX 3090 for 17 hours, at a cost of $17

35.5 trillion flops * 17 * 3600 = 2.2e18",,,,
Mistral 7B,Language,"Code generation,Language Generation","Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El Sayed",,,,https://arxiv.org/abs/2310.06825,12.00,Mistral 7B,2023-10-10,Mistral AI,Industry,7000000000.00,,,,,,,,,,,,,,,,,,Industry,Speculative,"We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses the Llama 2 13B -- Chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license.",2023-12-05 04:33,Anonymous,,,France,Mistral AI,,,,,,,
CodeFuse-13B,Language,Code generation,"Peng Di, Jianguo Li, Hang Yu, Wei Jiang, Wenting Cai, Yang Cao, Chaoyu Chen, Dajun Chen, Hongwei Chen, Liang Chen, Gang Fan, Jie Gong, Zi Gong, Wen Hu, Tingting Guo, Zhichao Lei, Ting Li, Zheng Li, Ming Liang, Cong Liao, Bingchang Liu, Jiachen Liu, Zhiwei Liu, Shaojun Lu, Min Shen, Guangpei Wang, Huan Wang, Zhi Wang, Zhaogui Xu, Jiawei Yang, Qing Ye, Gehao Zhang, Yu Zhang, Zelin Zhao, Xunjin Zheng, Hailian Zhou, Lifu Zhu, Xianying Zhu",,"""The results demonstrate that CodeFuse-13B achieves a HumanEval pass@1 score of 37.10%, positioning it as one of the top multi-lingual code LLMs with similar parameter sizes""",Permissive license,https://arxiv.org/abs/2310.06266,2.00,CodeFuse-13B: A Pretrained Multi-lingual Code Large Language Model,2023-10-10,Ant Group,,13000000000.00,,3.3e+23,"""CodeFuse-13B was trained using 512 Nvidia A100 GPU cards, with
a Hardware FLOPs Utilization (HFU) of approximately 60%. The
training process took approximately 40 days to complete""

512 * 312 trillion * 40 * 24 * 3600 * 0.6 = 3.3e23

Using params*tokens, we have 13 billion * 1 trillion * 6 = 7.8e22. might be a sign of multiple epochs? 1T is the size of the dataset; they don't clearly state the number of training tokens","The Stack, GitHub","80% code, 10% English, 10% Chinese: ""The pre-training data for CodeFuse consists
of 196TB of code, 1.75TB of Chinese raw data, and 1.7TB of English raw data, totaling 200TB, that are tokenized into 800 billion
tokens of code, 100 billion tokens of Chinese corpus, and 100 billion
tokens of English corpus (see Section 3.1).""",1000000000000,"1T tokens, mostly code but some Chinese/English",,,,960.0,~40 days,NVIDIA A100 SXM4 80 GB,NVIDIA A100 SXM4 80 GB,,,,,Likely,"Code Large Language Models (Code LLMs) have gained significant attention in the industry due to their wide applications in the full lifecycle of software engineering. However, the effectiveness of existing models in understanding non-English inputs for multi-lingual code-related tasks is still far from well studied. This paper introduces CodeFuse-13B, an open-sourced pre-trained code LLM. It is specifically designed for code-related tasks with both English and Chinese prompts and supports over 40 programming languages. CodeFuse achieves its effectiveness by utilizing a high quality pre-training dataset that is carefully filtered by program analyzers and optimized during the training process. Extensive experiments are conducted using real-world usage scenarios, the industry-standard benchmark HumanEval-x, and the specially designed CodeFuseEval for Chinese prompts. To assess the effectiveness of CodeFuse, we actively collected valuable human feedback from the AntGroup's software development process where CodeFuse has been successfully deployed. The results demonstrate that CodeFuse-13B achieves a HumanEval pass@1 score of 37.10%, positioning it as one of the top multi-lingual code LLMs with similar parameter sizes. In practical scenarios, such as code generation, code translation, code comments, and testcase generation, CodeFuse performs better than other models when confronted with Chinese prompts.",2023-12-13 06:08,Anonymous,,0,China,Ant Group,,,,,,,
Ferret (13B),Multimodal,"Object recognition,Language modelling","Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang, Yinfei Yang",SOTA Improvement,"""To evaluate this new capability, we introduce Ferret-Bench, covering three new types of tasks: Referring Description, Referring Reasoning, and Grounding in Conversation. We benchmark existing MLLMs and observe that Ferret can outperform the best of them by 20.4% on average.""",,https://arxiv.org/abs/2310.07704,13.00,Ferret: Refer and Ground Anything Anywhere at Any Granularity,2023-10-11,"Columbia University,Apple",Industry - Academia Collaboration,13000000000.00,13B,,"Fine-tuned from Vicuna-13B, which we don't have an estimate for. Finetuning cost is ~4e19.

""Training Details. We initialize the image encoder with CLIP-ViT-L/14@336p, the LLM with Vicuna, and the projection layer with LLaVA’s first-stage weights, leaving the visual sampler randomly initialized. After the initialization, Ferret is trained on the aforementioned GRIT data for three epochs, optimized by Loshchilov & Hutter (2017) with a learning rate of 2e − 5 and a batch size of 128. The training takes ∼5/2.5 days on 8 A100 GPU for a Ferret-13B/7B.""

5 * 24 * 3600 * 0.3 utilization (assumption) * 312 TFLOP/s = 4.04e19",GRIT,"""In order to make the refer-and-ground capability in Ferret open-vocabulary, instruction-following, and robust, we collect GRIT, a Ground-and-Refer Instruction-Tuning dataset with 1.1M samples. GRIT contains multiple levels of spatial knowledge, covering objects, relationships, region descriptions, and complex reasoning. It includes both text-in location-out (grounding) and location-in textout (referring) data, as well as data that mixes location and text in both input and output""",,,3.00,,,120.0,"""The training takes ∼5/2.5 days on 8 A100 GPU for a Ferret-13B/7B.""",NVIDIA A100,NVIDIA A100,,,,,Likely,"We introduce Ferret, a new Multimodal Large Language Model (MLLM) capable of understanding spatial referring of any shape or granularity within an image and accurately grounding open-vocabulary descriptions. To unify referring and grounding in the LLM paradigm, Ferret employs a novel and powerful hybrid region representation that integrates discrete coordinates and continuous features jointly to represent a region in the image. To extract the continuous features of versatile regions, we propose a spatial-aware visual sampler, adept at handling varying sparsity across different shapes. Consequently, Ferret can accept diverse region inputs, such as points, bounding boxes, and free-form shapes. To bolster the desired capability of Ferret, we curate GRIT, a comprehensive refer-and-ground instruction tuning dataset including 1.1M samples that contain rich hierarchical spatial knowledge, with 95K hard negative data to promote model robustness. The resulting model not only achieves superior performance in classical referring and grounding tasks, but also greatly outperforms existing MLLMs in region-based and localization-demanded multimodal chatting. Our evaluations also reveal a significantly improved capability of describing image details and a remarkable alleviation in object hallucination.",2023-12-09 03:24,Anonymous,,,"United States of America,United States of America","Columbia University, Apple",Vicuna-13B,40400000000000000000,"""The training takes ~5 days on 8 A100 GPU for a Ferret-13B""

5 * 24 * 3600 * 0.3 utilization (assumption) * 312 TFLOP/s = 4.04e19",Ferret (13B),8,,
Jiutian,Language,,,,,,https://www.globaltimes.cn/page/202310/1299716.shtml,,,2023-10-12,China Mobile,,,,,,"""Designed to enhance efficiency, the model has trained over 2 trillion tokens""",,,,,,,,,,,,,,,Likely,"China Mobile, the largest telecom operator in the world by subscribers, unveiled its ""Jiutian"" artificial intelligence (AI) large-scale model on Thursday, which has reportedly won support from large enterprises including China Ocean Shipping (Group) Co and China Railway Construction Co.",2023-12-14 06:29,Anonymous,,0,China,China Mobile,,,,,,,
Wu Dao Aquila2 34B,Language,Chat,,,,Permissive license,https://github.com/FlagAI-Open/Aquila2,,,2023-10-13,Beijing Academy of Artificial Intelligence,,34000000000.00,"34B

There's also a 70B ""experimental"" version: https://github.com/FlagAI-Open/Aquila2",,,,,,,,,,,,,,,,,,Likely,"We announce that our Aquila2 series is now open source, comprising Aquila2 (the base language models: Aquila2-7B, Aquila2-34B and Aquila2-70B-Expr) and AquilaChat2 (the chat models, namely AquilaChat2-7B, AquilaChat2-34B and AquilaChat2-70B-Expr, as well as the long-text chat models, namely AquilaChat2-7B-16k and AquilaChat2-34B-16k). You can find the links in the following table. Kindly click on them to access the model cards.",2023-12-13 08:14,Anonymous,,0,China,Beijing Academy of Artificial Intelligence,,,,,,,
ERNIE 4.0,Multimodal,Chat,,,Likely SOTA for Mandarin? But very little info available.,,https://www.prnewswire.com/news-releases/baidu-launches-ernie-4-0-foundation-model-leading-a-new-wave-of-ai-native-applications-301958681.html,,"Baidu Launches ERNIE 4.0 Foundation Model, Leading a New Wave of AI-Native Applications",2023-10-17,Baidu,,,,,,,,,,,,,,,,,,,,,Confident,"Baidu, Inc. (NASDAQ: BIDU and HKEX: 9888), a leading AI company with strong Internet foundation, today hosted its annual flagship technology conference Baidu World 2023 in Beijing, marking the conference's return to an offline format after four years. With the theme ""Prompt the World,"" this year's Baidu World conference saw Baidu launch ERNIE 4.0, Baidu's next-generation and most powerful foundation model offering drastically enhanced core AI capabilities. Baidu also showcased some of its most popular applications, solutions, and products re-built around the company's state-of-the-art generative AI.                                                                               

Robin Li, Co-founder, Chairman and CEO of Baidu, announced ERNIE 4.0 at Baidu World 2023
""ERNIE 4.0 has achieved a full upgrade with drastically improved performance in understanding, generation, reasoning, and memory,"" Robin Li, Co-founder, Chairman and CEO of Baidu, said at the event. ""These four core capabilities form the foundation of AI-native applications and have now unleashed unlimited opportunities for new innovations.""

",2023-12-07 05:41,Anonymous,,0,China,Baidu,,,,,,,
DALL·E 3,Drawing,Image generation,"James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, Wesam Manassra, Prafulla Dhariwal, Casey Chu, Yunxin Jiao, Aditya Ramesh",SOTA Improvement,,,https://cdn.openai.com/papers/dall-e-3.pdf,24.00,Improving Image Generation with Better Captions,2023-10-19,OpenAI,Industry,,,,,,,,,,,,,,,,,,,Industry,Speculative,"We show that prompt following abilities of text-to-image models can be substantially improved by training on highly descriptive generated image captions.
Existing text-to-image models struggle to follow detailed image descriptions and
often ignore words or confuse the meaning of prompts. We hypothesize that this
issue stems from noisy and inaccurate image captions in the training dataset. We
address this by training a bespoke image captioner and use it to recaption the
training dataset. We then train several text-to-image models and find that training
on these synthetic captions reliably improves prompt following ability. Finally, we
use these findings to build DALL-E 3: a new text-to-image generation system, and
benchmark its performance on an evaluation designed to measure prompt following, coherence, and aesthetics, finding that it compares favorably to competitors. We publish samples and code for these evaluations so that future research can continue optimizing this important aspect of text-to-image systems.
",2023-12-05 04:33,Anonymous,,,United States of America,OpenAI,,,,DALL·E 3,,,
Spark 3.0,Language,"Code generation,Language Generation",,,"""One such claim comes from Chinese company iFlytek, which asserts that its latest large language model, Spark 3.0, surpasses OpenAI’s GPT-3.5 in Chinese language tasks while demonstrating comparable performance in English contexts""",,https://www.gizmochina.com/2023/10/26/iflytek-spark-3-0-vs-openai-gpt/,,,2023-10-24,iFlytek,,,,,"""The company said that Spark 3.0 has been trained on a dataset of 1.2 trillion words and code, and that it can perform a variety of tasks, including text generation, language understanding, knowledge answering, logical reasoning, mathematical computation, code generation, and multimodal interaction."" https://www.gizmochina.com/2023/10/26/iflytek-spark-3-0-vs-openai-gpt/",,,,,,,,,,,,,,,,Likely,,2023-12-14 06:53,Anonymous,,0,China,iFlytek,,,,,,,
CODEFUSION (Python),Language,Code generation,"Mukul Singh, José Cambronero, Sumit Gulwani, Vu Le, Carina Negreanu, Gust Verbruggen",SOTA Improvement,"See Table 1, SOTA in Python code generation",,https://arxiv.org/abs/2310.17680,0.00,CODEFUSION: A Pre-trained Diffusion Model for Code Generation,2023-10-26,"Microsoft,Microsoft Research",Industry,75000000.00,Table 1,7920000000000000000.00,"V100 performance: 125 teraFLOPS according to https://www.nvidia.com/en-us/data-center/v100/

11 hours * 4 GPUs * 125 teraFLOPS/GPU * 0.40 utilization = 7.92e18 FLOP",,,4390400,"Section A3, Table 5: for python, 56k samples with an average length of 78.4 tokens",,,,11.0,"""The system used to run the experiments uses an Intel Core i7 processor (base at 1.8 GHz) along with 4 V100 GPU units, a 64-bit operating system, and 56 GB RAM. CODEFUSION took 8 hours to pre-train and 3 hours to fine-tune on average for each dataset.""",NVIDIA Tesla V100 SXM2 32 GB,NVIDIA Tesla V100 SXM2 32 GB,Self-supervised learning,,,Industry,Confident,"Imagine a developer who can only change their last line of code, how often would they have to start writing a function from scratch before it is correct? Auto-regressive models for code generation from natural language have a similar limitation: they do not easily allow reconsidering earlier tokens generated. We introduce CodeFusion, a pre-trained diffusion code generation model that addresses this limitation by iteratively denoising a complete program conditioned on the encoded natural language. We evaluate CodeFusion on the task of natural language to code generation for Bash, Python, and Microsoft Excel conditional formatting (CF) rules. Experiments show that CodeFusion (75M parameters) performs on par with state-of-the-art auto-regressive systems (350M-175B parameters) in top-1 accuracy and outperforms them in top-3 and top-5 accuracy due to its better balance in diversity versus quality.",2023-11-26 03:25,Anonymous,,,"Multinational,United States of America","Microsoft, Microsoft Research",,,,CODEFUSION (Python),,,
ChatGLM3,Multimodal,"Chat,Visual question answering",,SOTA Improvement,"Aiming at GPT-4V, ChatGLM3 has implemented iterative upgrades of several new functions this time, including:

CogVLM with multi-modal understanding capabilities, looks at image semantics, and achieved SOTA on more than 10 international standard image and text evaluation data sets;",,https://www.zhipuai.cn/en/news/76,,Zhipu AI launches third-generation base model,2023-10-27,Zhipu AI,,130000000000.00,"Highly speculative. The ChatGLM website https://chatglm.cn/ states that the model has hundreds of billions of parameters, so at least 100e9. It also states that the new model is based on ChatGLM2 and the GLM architectures. There is a previous GLM 130B model, so this may be the most likely size.",1.09200000000001e+24,"Highly speculative.
Assume 1 epoch on 1.4T tokens.
6 FLOP/token/param * 1.4T tokens * 130B params
https://www.wolframalpha.com/input?i=6*130+billion*1.4+trillion",,ChatGLM2 corpus pretraining plus human preference alignment training,1050000000000000,"The ChatGLM website states that the latest ChatGLM service is based on (and upgraded from) ChatGLM2, which was trained on 1.4T tokens. Assume that ChatGLM3 is trained on at least the same number of tokens.
Sources:
https://chatglm.cn/
https://github.com/THUDM/ChatGLM2-6B/blob/main/README_EN.md
https://www.zhipuai.cn/en/news/76",,,,,,,,,,,Industry,Speculative,"On October 27, 2023, at the 2023 China Computer Conference (CNCC), Zhipu AI launched the fully self-developed third-generation large base model ChatGLM3 and related series of products.",2023-12-07 08:12,Anonymous,,0,China,Zhipu AI,,,,,,,
Skywork-13B,Language,Language modelling,"Tianwen Wei, Liang Zhao, Lichang Zhang, Bo Zhu, Lijie Wang, Haihua Yang, Biye Li, Cheng Cheng, Weiwei Lü, Rui Hu, Chenxia Li, Liu Yang, Xilin Luo, Xuejie Wu, Lunan Liu, Wenjun Cheng, Peng Cheng, Jianhao Zhang, Xiaoyu Zhang, Lei Lin, Xiaokun Wang, Yutuan Ma, Chuanhai Dong, Yanqi Sun, Yifu Chen, Yongyi Peng, Xiaojuan Liang, Shuicheng Yan, Han Fang, Yahui Zhou",SOTA Improvement,"""We show that our model not only excels on popular benchmarks, but also achieves state of the art performance in Chinese language modeling on diverse domains""",Permissive license,https://arxiv.org/abs/2310.19341,6.00,Skywork: A More Open Bilingual Foundation Model,2023-10-30,Kunlun Inc.,Industry,13000000000.00,13B,3.04e+23,"""Our Skywork-13B is trained on a cluster of 64 NVIDIA-HGX-A800 nodes, a total of 512 A800-80G SXM GPUs... The training process of Skywork-13B spanned a total of 39 days.""

They note that ""we achieved a token throughput of 1873 per GPU per second and a model flops utilization (MFU) of 56.5%... "". 

I think ""MFU"" was coined in the Palm paper (https://arxiv.org/pdf/2204.02311.pdf) and only counts operations used to train the model, not all operations observed on the hardware. So it might be lower than traditional hardware utilization? Palm had lower MFU than hardware flops utilization.

Using the 56.5% number, we have 512 * 312 trillion * 39 * 24 * 3600 * 0.565 = 3.04e23",SkyPile,"""In order to train Skywork-13B, we build SkyPile, a vast, high quality corpus comprising more than 6 trillion tokens. A segment of the corpus, comprising over 150 billion tokens of web text, has been open sourced to facilitate research and training on Chinese LLMs""",5100000000000,"6 trillion tokens.

Roughly half English and half Chinese: (https://huggingface.co/Skywork/Skywork-13B-base).

Using 0.85 words per token (roughly the average of the ratios 0.75 and 1 we use for English and Chinese), that's 5.1 trillion words.

",0.50,,,940.0,39 days,NVIDIA A800,NVIDIA A800,,,,,Likely,"In this technical report, we present Skywork-13B, a family of large language models (LLMs) trained on a corpus of over 3.2 trillion tokens drawn from both English and Chinese texts. This bilingual foundation model is the most extensively trained and openly published LLMs of comparable size to date. We introduce a two-stage training methodology using a segmented corpus, targeting general purpose training and then domain-specific enhancement training, respectively. We show that our model not only excels on popular benchmarks, but also achieves state of the art performance in Chinese language modeling on diverse domains. Furthermore, we propose a novel leakage detection method, demonstrating that test data contamination is a pressing issue warranting further investigation by the LLM community. To spur future research, we release Skywork-13B along with checkpoints obtained during intermediate stages of the training process. We are also releasing part of our SkyPile corpus, a collection of over 150 billion tokens of web text, which is the largest high quality open Chinese pre-training corpus to date. We hope Skywork-13B and our open corpus will serve as a valuable open-source resource to democratize access to high-quality LLMs.",2023-12-05 04:33,Anonymous,,0,China,Kunlun Inc.,,,,,,,
BlueLM 13B,,,,,,Fully open-source,,,,2023-10-31,vivo AI lab,,7000000000000.00,"""BlueLM is a large-scale pre-trained language model independently developed by vivo AI Global Research Institute. This release includes 7B base (base) model and 7B conversation (chat) model. At the same time, we have open sourced the long text base (base) model that supports 32K and conversation (chat) model."" from GitHub https://github.com/vivo-ai-lab/BlueLM

",1.0920000000001e+23,"C = 6DN = 6 * 2.6T * 7B = 1.092*10^23 FLOP
https://www.wolframalpha.com/input?i=6*7+billion+*+2.6+trillion
(assuming 1 epoch)",,,1950000000000000,"""Larger amounts of high-quality data : high-quality corpus for training, reaching a scale of 2.6 trillion tokens. The corpus includes Chinese, English and a small amount of Japanese and Korean data"" from GitHub",,,,,,,,,,,,Speculative,,2023-12-07 05:29,Anonymous,,0,China,vivo AI lab,,,,,,,
Tongyi Qianwen (Qwen) 2.0,Language,Chat,,,,API accessible,https://www.alibabacloud.com/blog/alibaba-cloud-launches-tongyi-qianwen-2-0-and-industry-specific-models-to-support-customers-reap-benefits-of-generative-ai_600526,,Alibaba Cloud Launches Tongyi Qianwen 2.0 and Industry-specific Models to Support Customers Reap Benefits of Generative AI,2023-10-31,Alibaba,,,"""Tongyi Qianwen 2.0, a generic LLM with a few hundreds of billions of parameters""",,,,,,,,,,,,,,,,,,Likely,"Alibaba Cloud, the digital technology and intelligence backbone of Alibaba Group, today announced the launch of Tongyi Qianwen 2.0, its latest large language model (LLM), along with new industry-specific models at its annual flagship tech event Apsara Conference. This release signifies another significant progress in Alibaba Cloud's pursuit of cutting-edge AI innovation and its ongoing commitment to fuel digital transformation in businesses.",2023-12-12 08:12,Anonymous,,0,China,Alibaba,,,,,,,
Nanbeige-16B,Language,Chat,,,"a little worse than Qwen on most metrics, and well short of GPT-4, ofc",Permissive license,https://github.com/Nanbeige/Nanbeige/blob/main/README_EN.md,,,2023-11-01,Nanbeige LLM Lab,,16000000000.00,16 billion,2.4e+23,"""It uses 2.5T Tokens for pre-training"". I think that's the number of tokens the model was trained on, not the dataset size, but I'm not sure.

16 billion * 2.5 trillion * 6 = 2.4e23",,"""The training data includes a large amount of high-quality internet corpus, various books, code, etc""",,,,,,,,,,,,,,Likely,"Nanbeige-16B is a 16 billion parameter language model developed by Nanbeige LLM Lab. It uses 2.5T Tokens for pre-training. The training data includes a large amount of high-quality internet corpus, various books, code, etc. It has achieved good results on various authoritative evaluation data sets. This release includes the Base, Chat, Base-32k and Chat-32k.",2023-12-13 08:56,Anonymous,,0,China,Nanbeige LLM Lab,,,,,,,
LingoWhale-8B,Language,,,,competitive with Qwen-7B on C-Eval,Permissive license,https://github.com/DeepLangAI/LingoWhale-8B/blob/main/README_EN.md,,,2023-11-01,DeepLang AI,,8000000000.00,,,,,"""pre-trained on a large volume of high-quality bilingual data"" Chinese + English",,,,,,,,,,,,,,Likely,"LingoWhale-8B is the first open-source model in the LingoWhale series introduced by DeepLangAI. It's a bilingual (Chinese-English) large language model.

LingoWhale-8B has been pre-trained on a large volume of high-quality bilingual data and exhibits powerful capabilities as a foundation model. It has achieved leading results on multiple public benchmarks. During its pre-training phase, the model was trained with a context window of 8K, allowing it to comprehend and generate longer sequences.

LingoWhale-8B is fully open for academic research. Users can apply for commercial use by email, and once granted official commercial permission, they can use it for free.",2023-12-13 09:28,Anonymous,,0,China,DeepLang AI,,,,,,,
DeepSeek Coder 33B,Language,Code generation,,,"SOTA among open-source: ""For coding capabilities, DeepSeek Coder achieves state-of-the-art performance among open-source code models on multiple programming languages and various benchmarks.""",Permissive license,https://github.com/deepseek-ai/DeepSeek-Coder,,,2023-11-02,DeepSeek,,33000000000.00,33B,3.96e+23,"""Step 1: Initially pre-trained with a dataset consisting of 87% code, 10% code-related language (Github Markdown and StackExchange), and 3% non-code-related Chinese language. Models are pre-trained using 1.8T tokens and a 4K window size in this step.
Step 2: Further Pre-training using an extended 16K window size on an additional 200B tokens, resulting in foundational models (DeepSeek-Coder-Base).
Step 3: Instruction Fine-tuning on 2B tokens of instruction data, resulting in instruction-tuned models (DeepSeek-Coder-Instruct).""

This means it was trained on 2T tokens. 2T * 33B * 6 = 3.96e23",,"""Trained from scratch on 2T tokens, including 87% code and 13% linguistic data in both English and Chinese languages.""",,,,,,,,,,,,,,Likely,"DeepSeek Coder is composed of a series of code language models, each trained from scratch on 2T tokens, with a composition of 87% code and 13% natural language in both English and Chinese. We provide various sizes of the code model, ranging from 1B to 33B versions. Each model is pre-trained on project-level code corpus by employing a window size of 16K and an extra fill-in-the-blank task, to support project-level code completion and infilling. For coding capabilities, DeepSeek Coder achieves state-of-the-art performance among open-source code models on multiple programming languages and various benchmarks.",2023-12-08 08:37,Anonymous,,0,China,DeepSeek,,,,,,,
Yi-34B,Language,Chat,,Significant use,"2nd most popular model on HuggingFace: https://decrypt.co/206195/new-open-source-ai-model-from-china-boasts-twice-the-capacity-of-chatgpt

also maybe the best open-source model, does better than Llama 2-70B on several benchmarks",Permissive license,https://github.com/01-ai/Yi,,,2023-11-02,01.AI,,34000000000.00,34b,6.1e+23,"""The dataset we use contains Chinese & English only. We used approximately 3T tokens"" sounds like this means it was trained on 3T tokens, not necessarily that the dataset contains 3T tokens?

If so, 34b * 3T * 6 = 6.1e23",,Chinese and English dataset,,,,,,,,,,,,,,Speculative,The Yi series models are large language models trained from scratch by developers at 01.AI.,2023-12-12 07:27,Anonymous,,0,China,01.AI,,,,,,,
BLUUMI,Language,Language modelling,"Risto Luukkonen, Ville Komulainen, Jouni Luoma, Anni Eskelinen, Jenna Kanerva, Hanna-Mari Kupari, Filip Ginter, Veronika Laippala, Niklas Muennighoff, Aleksandra Piktus, Thomas Wang, Nouamane Tazi, Teven Le Scao, Thomas Wolf, Osma Suominen, Samuli Sairanen, Mikko Merioksa, Jyrki Heinonen, Aija Vahtola, Samuel Antao, Sampo Pyysalo",SOTA Improvement,"SOTA for Finnish: ""Our best monolingual model outperforms this result by over
10% points and the BLUUMI model by over 20% points, representing a substantial advance in the
state of the art in the capability of generative models trained for Finnish.""",,https://arxiv.org/abs/2311.05640,1.00,FinGPT: Large Generative Models for a Small Language,2023-11-03,"University of Turku,Hugging Face",Academia,176000000000.00,176 billion,,,,Finnish data from several sources,38000000000,38B tokens,,,,,,AMD Instinct MI250X,AMD Instinct MI250X,,,,,Likely,"Large language models (LLMs) excel in many tasks in NLP and beyond, but most open models have very limited coverage of smaller languages and LLM work tends to focus on languages where nearly unlimited data is available for pretraining. In this work, we study the challenges of creating LLMs for Finnish, a language spoken by less than 0.1% of the world population. We compile an extensive dataset of Finnish combining web crawls, news, social media and eBooks. We pursue two approaches to pretrain models: 1) we train seven monolingual models from scratch (186M to 13B parameters) dubbed FinGPT, 2) we continue the pretraining of the multilingual BLOOM model on a mix of its original training data and Finnish, resulting in a 176 billion parameter model we call BLUUMI. For model evaluation, we introduce FIN-bench, a version of BIG-bench with Finnish tasks. We also assess other model qualities such as toxicity and bias. Our models and tools are openly available at this https URL.",2023-12-05 04:33,Anonymous,,0,"Finland,Multinational","University of Turku, Hugging Face",BLOOM,,"They ""continued pretraining"" of BLOOM on Finnish data. Don't think they specify the number of tokens they trained BLOOM/BLUUMI on; for their smaller models it was 300b.",,,,
Grok-1,Language,"Language modelling,Chat",,SOTA Improvement,"""On these benchmarks, Grok-1 displayed strong results, surpassing all other models in its compute class, including ChatGPT-3.5 and Inflection-1""",,https://x.ai/model-card/,,Announcing Grok,2023-11-04,xAI,Industry,,"Grok-0, xAI's previous model, is 33B parameters. Not clear whether Grok-1 has the same number.",,"""On these benchmarks, Grok-1 displayed strong results, surpassing all other models in its compute class, including ChatGPT-3.5 and Inflection-1. It is only surpassed by models that were trained with a significantly larger amount of training data and compute resources like GPT-4""

Per table, Grok-1 is surpassed by Palm 2, Claude 2, GPT-4, so it required less compute than these three models. Palm 2 was trained on 7e24 FLOP.

GPT-3.5 is ~2.6e24. Inflection-1's compute is not public/known by us but Inflection says Inflection-1 compute was <= Palm-540B's (which was ~2.5e24). ",,,,,,,,,,,,,,,,Likely,"Grok is an AI modeled after the Hitchhiker’s Guide to the Galaxy, so intended to answer almost anything and, far harder, even suggest what questions to ask!

Grok is designed to answer questions with a bit of wit and has a rebellious streak, so please don’t use it if you hate humor!

A unique and fundamental advantage of Grok is that it has real-time knowledge of the world via the 𝕏 platform. It will also answer spicy questions that are rejected by most other AI systems.

Grok is still a very early beta product – the best we could do with 2 months of training – so expect it to improve rapidly with each passing week with your help.",2023-12-08 06:45,Anonymous,,,United States of America,xAI,,,,Grok-1,,,
Grok-0,Language,,,,,,https://x.ai/,,Announcing Grok,2023-11-04,xAI,,33000000000.00,33 billion,,"Half of Llama 2-70B? (which we estimated at 8e23) ""This early model approaches LLaMA 2 (70B) capabilities on standard LM benchmarks but uses only half of its training resources""",,,,,,,,,,,,,,,,Likely,"""The engine powering Grok is Grok-1, our frontier LLM, which we developed over the last four months. Grok-1 has gone through many iterations over this span of time.

After announcing xAI, we trained a prototype LLM (Grok-0) with 33 billion parameters. This early model approaches LLaMA 2 (70B) capabilities on standard LM benchmarks but uses only half of its training resources.""",2023-12-08 06:46,Anonymous,,0,United States of America,xAI,,,,,,,
XVERSE-13B-2,Language,Language Generation,,,,Permissive license,https://huggingface.co/xverse/XVERSE-13B,,,2023-11-06,Shenzhen Yuanxiang Technology,Industry,13000000000.00,13B,,"Not enough info, eg number of epochs",,"""The model has been thoroughly trained on a diversified and high-quality dataset consisting of 3.2 trillion of tokens, including more than 40 languages such as Chinese, English, Russian, and Spanish. The sampling ratio of different types of data is finely set, which makes the performance of Chinese and English excellent, and also takes into account the effect of other languages.""",2800000000000,"Multilingual, 3.2 trillion tokens. Likely majority Chinese and English, so I'll assume .87 words per token, or ~2.8 trillion words",,,,,,,,,,,,Likely,"XVERSE-13B is a multilingual large language model, independently developed by Shenzhen Yuanxiang Technology. Its key features are as follows:

Model Structure: XVERSE-13B uses the mainstream Decoder-only Transformer network structure, supports 8k context length, the longest one among models of the same size, which can meet the need of longer multi-round dialogues, knowledge question-answering, and summarization. This makes the model more versatile in application scenarios.
Training Data: The model has been thoroughly trained on a diversified and high-quality dataset consisting of 3.2 trillion of tokens, including more than 40 languages such as Chinese, English, Russian, and Spanish. The sampling ratio of different types of data is finely set, which makes the performance of Chinese and English excellent, and also takes into account the effect of other languages.
Tokenization: Based on the BPE (Byte-Pair Encoding) algorithm, a tokenizer with a vocabulary size of 100,534 has been trained using hundreds of gigabytes of language data. This tokenizer is capable of supporting multilingual without the need for additional vocabulary expansion.
Training Framework: Several key technologies have also been independently developed, including efficient operators, memory optimization, parallel scheduling strategies, overlap of data-computation-communication, and synergy between platforms and frameworks. These advancements enhance training efficiency and model stability. With these technologies, the peak computational power utilization rate on a thousand-card cluster can reach 58.5%, ranking at the forefront of the industry.",2023-12-01 03:45,Anonymous,,0,China,Shenzhen Yuanxiang Technology,,,,,,,
Whisper v3,Audio,Audio Speech Recognition,,,"seems not SOTA: ""Our studies show that... accuracy on speech recognition and translation is near the state-of-the-art level""",Permissive license,https://huggingface.co/openai/whisper-large-v3,,,2023-11-06,OpenAI,,1550000000.00,,,"Could derive this in terms of Whisper v1, which according to the paper was trained for 680k hours for between 2-3 epochs. Whisper v3 was trained on 5 million hours for 2 epochs, or ~5-7x as much data, and has the same architecture. We have an estimate of 4.65e22 for Whisper 1 but the data source doesn't work for me.",,"""The Whisper large-v3 model is trained on 1 million hours of weakly labeled audio and 4 million hours of pseudolabeled audio collected using Whisper large-v2""",60000000000,"English audio is roughly 228 wpm: https://docs.google.com/document/d/1G3vvQkn4x_W71MKg0GmHVtzfd9m0y3_Ofcoew0v902Q/edit#heading=h.sxcem9l5k3ce

The dataset is multilingual and other languages seem to have lower wpms. So using 200 wpm, we have

200*60*5 million hours = 60,000,000,000 (60B) words",2.00,,,,,,,Supervised,,,,Likely,,2023-12-12 07:43,Anonymous,,0,United States of America,OpenAI,,,,,,,
Lyria,Audio,Audio generation,"Kazuya Kawakami, David Ding, Björn Winckler, Cătălina Cangea, Tobenna Peter Igwe, Will Grathwohl, Yan Wu, Yury Sulsky, Jacob Kelly, Charlie Nash, Conor Durkan, Yaroslav Ganin, Tom Eccles, Zach Eaton-Rosen, Jakob Bauer, Mikita Sazanovich, Morgane Rivière, Evgeny Gladchenko, Mikołaj Bińkowski, Ali Razavi, Jeff Donahue, Benigno Uria, Sander Dieleman, Sherjil Ozair, John Schultz, Ankush Gupta, Junlin Zhang, Drew Jaegle, Aäron van den Oord.",,,,https://deepmind.google/discover/blog/transforming-the-future-of-music-creation/,,Transforming the future of music creation,2023-11-16,Google DeepMind,,,,,,,,,,,,,,,,,,,,,Confident,"Music contains huge amounts of information — consider every beat, note, and vocal harmony in every second. When generating long sequences of sound, it’s difficult for AI models to maintain musical continuity across phrases, verses, or extended passages. Since music often includes multiple voices and instruments at the same time, it's much harder to create than speech.

Built by Google DeepMind, the Lyria model excels at generating high-quality music with instrumentals and vocals, performing transformation and continuation tasks, and giving users more nuanced control of the output’s style and performance.",2023-12-07 05:55,Anonymous,,0,Multinational,Google DeepMind,,,,,,,
Claude 2.1,Language,Chat,,Significant use,,API accessible,https://www.anthropic.com/index/claude-2-1,0.00,Introducing Claude 2.1,2023-11-21,Anthropic,Industry,,,,,,,,,,,,,,,,Reinforcement learning,,,Industry,Confident,"Our latest model, Claude 2.1, is now available over API in our Console and is powering our claude.ai chat experience. Claude 2.1 delivers advancements in key capabilities for enterprises—including an industry-leading 200K token context window, significant reductions in rates of model hallucination, system prompts and our new beta feature: tool use.",2023-11-26 03:25,Anonymous,,0,United States of America,Anthropic,Claude 2,,,Claude 2.1,,,
Inflection-2,Language,Language modelling,,Significant use,"Inflection-2 either already powers Pi or soon will: https://inflection.ai/inflection-2

Inflection has claimed that Pi has >1m users: https://x.com/inflectionAI/status/1699100179390210091?s=20",API accessible,https://inflection.ai/inflection-2,,Inflection-2: The Next Step Up,2023-11-22,Inflection AI,Industry,,,1.001e+25,"""Inflection-2 was trained on 5,000 NVIDIA H100 GPUs in fp8 mixed precision for ~10²⁵ FLOPs""

(the second 1 is there because of airtable funkiness, not a real sig fig)",,,,,,,,,,NVIDIA H100 SXM5,NVIDIA H100 SXM5,,,,Industry,Likely,"Today we are proud to announce that we have completed training of Inflection-2, the best model in the world for its compute class and the second most capable LLM in the world today. Our mission at Inflection is to create a personal AI for everyone. Just a few months ago, we announced Inflection-1 — a best-in-class language model that currently powers Pi. Our new model, Inflection-2, is substantially more capable than Inflection-1, demonstrating much improved factual knowledge, better stylistic control, and dramatically improved reasoning.",2023-12-12 03:37,Anonymous,,0,United States of America,Inflection AI,,,,,,,
Yuan 2.0,Language,,"Shaohua Wu, Xudong Zhao, Shenling Wang, Jiangang Luo, Lingjun Li, Xi Chen, Bing Zhao, Wei Wang, Tong Yu, Rongguo Zhang, Jiahua Zhang, Chao Wang",,,Permissive license,https://arxiv.org/abs/2311.15786v1,,YUAN 2.0: A Large Language Model with Localized Filtering-based Attention,2023-11-27,Inspur,Industry,102600000000.00,102.6 billion,,"Trained on 288B tokens

6*102.6b*288b = 1.78e23",,"""The pretraining corpus includes a mix of books, codes, and encyclopedia in both Chinese and English (Table
2)""

with synthetic code data:
""Code (CN). Considering the diversity of programming tasks, we also build a synthesized instruction dataset
with 4 million code samples in Chinese. To cover the concepts involved in programming tasks as many as
possible, we collect 15,000 words of programming, computer science, mathematics, and other relevant
topics from the Sogou input dictionary. Two topic words are randomly selected as the seeds for a wellcrafted prompt in each time. Then the prompt will be fed to GPT-3.5 to generate a programming task and
corresponding Python solution. ""

and translated open-source fine-tuning instruction data:
""We construct a fine-tuning dataset focused on code, math and chat tasks.
Code Instruction dataset. We collect some open-source code instruction datasets, including CodeAlpaca20k [28], Evol-Instruct-Code-80k[38], CodeFuse-CodeExercise-Python-27k and CodeFuse-Evolinstruction-66k [39]. We translate the English code instruction into Chinese with GPT-3.5""",,,,,,,,,,,,,,Likely,"In this work, the Localized Filtering-based Attention (LFA) is introduced to incorporate prior knowledge of local dependencies of natural language into Attention. Based on LFA, we develop and release Yuan 2.0, a large language model with parameters ranging from 2.1 billion to 102.6 billion. A data filtering and generation method is presented to build pretraining and fine-tuning dataset in high quality. A distributed training method with non-uniform pipeline parallel, data parallel, and optimizer parallel is proposed, which greatly reduces the bandwidth requirements of intra-node communication, and achieves good performance in large-scale distributed training. Yuan 2.0 models display impressive ability in code generation, math problem-solving, and chat compared with existing models. The latest version of YUAN 2.0, including model weights and source code, is accessible at Github.",2023-12-02 07:29,Anonymous,,0,China,Inspur,,,,,,,
Qwen-72B,Language,"Chat,Code generation","Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, Tianhang Zhu",SOTA Improvement,"SOTA on several Chinese benchmarks, with highest average rating overall for Chinese benchmarks:

https://opencompass.org.cn/leaderboard-llm",Permissive license,https://huggingface.co/Qwen/Qwen-72B,,,2023-11-30,Alibaba,,72000000000.00,72B,1.3e+24,"72 billion params, 3 trillion tokens
72b * 3T * 6 = 1.3e24",,"""It is pretrained on over 3 trillion tokens, including Chinese, English, multilingual texts, code, and mathematics, covering general and professional fields""",,,,,,,,,,,,,,Likely,"Qwen-72B is the 72B-parameter version of the large language model series, Qwen (abbr. Tongyi Qianwen), proposed by Alibaba Cloud. Qwen-72B is a Transformer-based large language model, which is pretrained on a large volume of data, including web texts, books, codes, etc. Additionally, based on the pretrained Qwen-72B, we release Qwen-72B-Chat, a large-model-based AI assistant, which is trained with alignment techniques.",2023-12-12 08:20,Anonymous,,0,China,Alibaba,,,,,,,
Gemini Ultra,Multimodal,"Language modelling,Visual question answering,Chat,Translation",Gemini Team,SOTA Improvement,""" Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined.""",Unreleased,https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,0.00,Gemini: A Family of Highly Capable Multimodal Models,2023-12-06,Google DeepMind,,,,9.0000000001e+25,"This number is an estimate based on limited evidence. In particular, we combine information about the performance of Gemini Ultra on various benchmarks compared to other models, and guesstimates about the hardware setup used for training to arrive at our estimate. Our reasoning and calculations are detailed in this Colab notebook.
https://colab.research.google.com/drive/1sfG91UfiYpEYnj_xB5YRy07T5dv-9O_c",,,,,,,,2400.0,"Dylan Patel, author of SemiAnalysis, speculates that the training duration of Gemini may have been 100 days.",Google TPU v4,Google TPU v4,,,,Industry,Speculative,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",2023-12-13 07:45,Anonymous,,0,Multinational,Google DeepMind,,,,,,,
Phi-2,Language,Language Generation,"Marah Abdin, Jyoti Aneja, Sebastien Bubeck, Caio César Teodoro Mendes, Weizhu Chen, Allie Del Giorno, Ronen Eldan, Sivakanth Gopi, Suriya Gunasekar, Mojan Javaheripi, Piero Kauffmann, Yin Tat Lee, Yuanzhi Li, Anh Nguyen, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Michael Santacroce, Harkirat Singh Behl, Adam Taumann Kalai, Xin Wang, Rachel Ward, Philipp Witte, Cyril Zhang, Yi Zhang",,"""With only 2.7 billion parameters, Phi-2 surpasses the performance of Mistral and Llama-2 models at 7B and 13B parameters on various aggregated benchmarks. Notably, it achieves better performance compared to 25x larger Llama-2-70B model on muti-step reasoning tasks, i.e., coding and math. Furthermore, Phi-2 matches or outperforms the recently-announced Google Gemini Nano 2, despite being smaller in size.""",API accessible,https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/,,Phi-2: The surprising power of small language models,2023-12-12,Microsoft,,2700000000.00,2.7B,2.27e+22,"2.7B params, trained on 1.4T tokens

2.7 billion * 1.4 trillion * 6 = 2.27e22

96*14 A100-days

14 * 96 * 312 trillion * 24 * 3600 * 0.3 = 1.09e22",,"""Our training data mixture contains synthetic datasets specifically created to teach the model common sense reasoning and general knowledge, including science, daily activities, and theory of mind, among others. We further augment our training corpus with carefully selected web data that is filtered based on educational value and content quality. Secondly, we use innovative techniques to scale up, starting from our 1.3 billion parameter model, Phi-1.5, and embedding its knowledge within the 2.7 billion parameter Phi-2. This scaled knowledge transfer not only accelerates training convergence but shows clear boost in Phi-2 benchmark scores.""",,,,,,336.0,14 days,NVIDIA A100,NVIDIA A100,,,,,Likely,"Over the past few months, our Machine Learning Foundations team at Microsoft Research has released a suite of small language models (SLMs) called “Phi” that achieve remarkable performance on a variety of benchmarks. Our first model, the 1.3 billion parameter Phi-1(opens in new tab), achieved state-of-the-art performance on Python coding among existing SLMs (specifically on the HumanEval and MBPP benchmarks). We then extended our focus to common sense reasoning and language understanding and created a new 1.3 billion parameter model named Phi-1.5(opens in new tab), with performance comparable to models 5x larger.

We are now releasing Phi-2(opens in new tab), a 2.7 billion-parameter language model that demonstrates outstanding reasoning and language understanding capabilities, showcasing state-of-the-art performance among base language models with less than 13 billion parameters. On complex benchmarks Phi-2 matches or outperforms models up to 25x larger, thanks to new innovations in model scaling and training data curation.",2023-12-14 08:05,Anonymous,,0,Multinational,Microsoft,,,,,,,
Gen-2,Text-to-Video,Video generation,Gen-2 authors,SOTA Improvement,"SOTA improvement over Stable Diffusion and Text2Live, paper forthcoming",,https://research.runwayml.com/gen2,0.00,,2023-12-31,Runway,Industry,,,,,,,,,,,,,,,,,,,,Unverified,,2023-11-26 03:25,Anonymous,,1,United States of America,Runway,,,,Gen-2,,,
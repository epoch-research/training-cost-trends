Model,Domain,Task,Organization,Authors,Publication date,Reference,Link,Citations,Notability criteria,Notability criteria notes,Parameters,Parameters notes,Training compute (FLOP),Training compute notes,Training dataset,Training dataset notes,Training dataset size (datapoints),Dataset size notes,Training time (hours),Training time notes,Training hardware,Approach,Confidence,Abstract,Epochs,Benchmark data,Model accessibility,Country (of organization),Base model,Finetune compute (FLOP),Finetune compute notes,Hardware quantity,Hardware utilization (MFU),Last modified,Training cloud compute vendor,Training data center,Archived links,Batch size,Batch size notes,Organization categorization,Foundation model,Training compute lower bound,Training compute upper bound,Training chip-hours,Training code accessibility,Accessibility notes,Organization categorization (from Organization),Training compute cost (2023 USD),Utilization notes,Numerical format,Training power draw (W),Training compute estimation method,Hugging Face developer id,Post-training compute (FLOP),Post-training compute notes,Compute rank when published
Magistral Small,Language,"Language modeling/generation,Question answering,Quantitative reasoning,Code generation,Translation",Mistral AI,"Abhinav Rastogi, Albert Q. Jiang, Andy Lo, Gabrielle Berrada, Guillaume Lample, Jason Rute, Joep Barmentlo, Karmesh Yadav, Kartik Khandelwal, Khyathi Raghavi Chandu, LÃ©onard Blier, Lucile Saulnier, Matthieu Dinot, Maxime Darrin, Neha Gupta, Roman Soletskyi, Sagar Vaze, Teven Le
Scao, Yihan Wang",2025-06-10,"Announcing Magistral â€” the first reasoning model by Mistral AI â€” excelling in domain-specific, transparent, and multilingual reasoning.",https://mistral.ai/news/magistral,,,,24000000000.0,"24B

""We finetuned Mistral Small 3 Instruct (a 24-billion parameter model) for 4 epochs""",,,,,,,,,,,Confident,"We introduce Magistral, Mistralâ€™s first reasoning model and our own scalable reinforcement learning (RL) pipeline. Instead of relying on existing implementations and RL traces distilled from prior models, we follow a ground up approach, relying solely on our own models and infrastructure. Notably, we demonstrate a stack that enabled us to explore the limits of pure RL training of LLMs, present a simple method to force the reasoning language of the model, and show that RL on text data alone maintains most of the initial checkpointâ€™s capabilities. We find that RL on text maintains or improves multimodal understanding, instruction following and function calling. We present Magistral Medium, trained for reasoning on top of Mistral Medium 3 with RL alone, and we open-source Magistral Small (Apache 2.0) which further includes cold-start data from Magistral Medium.",4.0,,Open weights (unrestricted),France,Mistral Small 3.1,,,,,2025-06-10 16:37,,,,,,Industry,,,,,Unreleased,"Apache 2.0
https://huggingface.co/mistralai/Magistral-Small-2506",Industry,,,,,,mistralai,,,
Boltz-2,Biology,"Protein interaction prediction,Protein-ligand binding affinity prediction","Massachusetts Institute of Technology (MIT),Recursion Pharmaceuticals,ETH Zurich,Valence Labs","Saro Passaro, Gabriele Corso, Jeremy Wohlwend, Mateo Reveiz, Stephan Thaler, Vignesh Ram Somnath, Noah Getz, Tally Portnoi, Julien Roy, Hannes Stark, David Kwabi-Addo, Dominique Beaini, Tommi Jaakkola, Regina Barzilay",2025-06-06,Boltz-2: Towards Accurate and Efficient Binding Affinity Prediction,http://jeremywohlwend.com/assets/boltz2.pdf,0.0,,,,,,,"PDB (Protein Data Bank),MISATO,ATLAS,mdCATH,PubChem,ChEMBL,BindingDB,Ligand Discovery at CeMM,MIDAS,Rfam 14,JASPAR 2024,R-SIM,VDJdb, AlphaFold database (AFDB),Immune Epitope Database (IEDB)",Mix of curated structural and binding affinity data from a variety of sources.,,Wide range of different datasets and data types - I'm unsure how one would go about aggregating these.,,"""In our specific case, this condition was met after approximately 400,000 molecules were sampled (corresponding to roughly 16 hours of runtime), totaling nearly 1,000 GPU-hours of computation for the generative screen.""","NVIDIA A100,NVIDIA H100 NVL",,Unknown,"Boltz-2 is a new biomolecular foundation model that goes beyond AlphaFold3 and Boltz-1 by jointly modeling complex structures and binding affinities, a critical component towards accurate molecular design. Boltz-2 is the first deep learning model to approach the accuracy of physics-based free-energy perturbation (FEP) methods, while running 1000x faster â€” making accurate in silico screening practical for early-stage drug discovery.",,,Open weights (unrestricted),"United States of America,United States of America,Switzerland,Canada",,,,188.0,,2025-06-19 12:41,,,,,,"Academia,Industry,Academia,Industry",,,,,,,"Academia,Industry,Academia,Industry",,,,,,,,,
Claude Opus 4,"Language,Multimodal,Vision","Code generation,Language modeling/generation,Quantitative reasoning,Search,Visual question answering,Translation,Image captioning,Instruction interpretation,Mathematical reasoning,Visual puzzles,Code autocompletion,Chat,Character recognition,Language modeling,Language generation,Text autocompletion,Retrieval-augmented generation,System control",Anthropic,,2025-05-22,"Hybrid reasoning model that pushes the frontier for coding and AI agents, featuring a 200K context window","https://www.anthropic.com/claude/opus

https://www.anthropic.com/news/claude-4",,Training cost,,,,,,Unspecified unreleased,"Knowledge cutoff date is March 1, 2025, according to https://support.anthropic.com/en/articles/8114494-how-up-to-date-is-claude-s-training-data.",,,,,,,Unknown,"Claude Opus 4 is our most powerful model yet and the best coding model in the world, leading on SWE-bench (72.5%) and Terminal-bench (43.2%). It delivers sustained performance on long-running tasks that require focused effort and thousands of steps, with the ability to work continuously for several hoursâ€”dramatically outperforming all Sonnet models and significantly expanding what AI agents can accomplish.",,,API access,United States of America,,,,,,2025-06-12 14:51,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
Claude Sonnet 4,"Language,Multimodal,Vision","Code generation,Language modeling/generation,Quantitative reasoning,Search,Visual question answering,Translation,Image captioning,Instruction interpretation,Mathematical reasoning,Visual puzzles,Code autocompletion,Chat,Character recognition,Language modeling,Language generation,Text autocompletion,Retrieval-augmented generation,System control",Anthropic,,2025-05-22,"Hybrid reasoning model with superior intelligence for high-volume use cases, and 200K context window","https://www.anthropic.com/claude/sonnet

https://www.anthropic.com/news/claude-4",,Training cost,,,,,,Unspecified unreleased,"Knowledge cutoff date is March 1, 2025, according to https://support.anthropic.com/en/articles/8114494-how-up-to-date-is-claude-s-training-data.",,,,,,,Unknown,"Claude Sonnet 4 can understand nuanced instructions and context, recognize and correct its own mistakes, and create sophisticated analysis and insights from complex data. Combined with superior coding, vision, and writing skills, you can use Claude Sonnet 4 for a variety of use cases.

Claude Sonnet 4 can produce near-instant responses or extended, step-by-step thinking that is made visible to the user. API users also have fine-grained control over how long the model thinks for.",,,API access,United States of America,,,,,,2025-06-12 15:29,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
Veo 3,"Video,Vision","Video generation,Image-to-video,Text-to-video",Google DeepMind,"Abhishek Sharma, Alina Kuznetsova, Ali Razavi, Aleksander Holynski, Alina Kuznetsova, Ankush Gupta, Austin Waters, Ben Poole, Daniel Tanis, Derek Gasaway, Dumitru Erhan, Enric Corona, Frank Belletti, Gabe Barth-Maron, Hakan Erdogan, Henna Nandwani, Hernan Moraldo, Ilya Figotin, Igor Saprykin, Jason Baldridge, Jeff Donahue, Jimmy Shi, Kurtis David, Mai Gimenez, Medhini Narasimhan, Miaosen Wang, Mingda Zhang, Mohammad Babaeizadeh, Mukul Bhutani, Nikhil Khadke, Nilpa Jha, Pieter-Jan Kindermans, Poorva Rane, Rachel Hornung, Ricky Wong, Ruben Villegas, Ruiqi Gao, Ryan Poplin, Salah Zaiem, Sayna Ebrahimi, Scott Wisdom, Shlomi Fruchter, Sophia Sanchez, Vikas Verma, Viral Carpenter, Xinchen Yan, Xinyu Wang, Yiwen Luo, Zhichao Yin, Zu Kim",2025-05-21,Our state-of-the-art video generation model,https://deepmind.google/models/veo/,,SOTA improvement,https://deepmind.google/models/veo/evals/,,,,,Unspecified unreleased,,,,,,,,Unknown,"Re-designed for greater realism
Greater realism and fidelity, including 4k output and Veo 3â€™s real world physics and audio

Follows prompts like never before
Improved prompt adherence, meaning more accurate responses to your instructions.

Improved creative control
New capabilities to achieve new levels of control, consistency, and creativity.",,,API access,"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France",,,,,,2025-05-27 10:02,,,,,,Industry,,,,,Unreleased,https://cloud.google.com/vertex-ai/generative-ai/docs/models/veo/3-0-generate-preview,Industry,,,,,,,,,
Lyria RealTime,Audio,Audio generation,Google DeepMind,"Adam Roberts, Antoine Caillon, Cassie Tarakajian, Chris Donahue, Ian Simon, Jesse Engel, Noah Constant, Alberto Lalama, Andrea Agostinelli, Anna Huang, Ashu Desai, Brian McWilliams, David Tian, DY Kim, Ethan Manilow, George Brower, Hema Manickavasagam, Ilaria Manco, Julian Salazar, Kehang Han, Mauricio Zuluaga, Michael Dooley, Matej Kastelic, Monica Dinculescu, Ondrej Skopek, Pen Li, Reed Enger, Timo Denk, Yotam Mann and Zalan Borsos",2025-05-20,Interactive real-time music creation model,https://deepmind.google/models/lyria/realtime/,,,,,,,,Unspecified unreleased,,,,,,,,Unknown,"Advanced controls

Blend multiple text prompts to mix genres, change instruments, and alter the mood of the music. You can also directly control the key, tempo, density, and brightness.

Production-quality sound

Generate professional-grade 48kHz stereo audio, perfect for performance or as a source of inspiration for production.

Musical surprises

Forge unlikely connections between existing sounds and genres to explore diverse new ideas.",,,API access,"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France",,,,,,2025-06-16 15:23,,,,,,Industry,,,,,Unreleased,API: https://ai.google.dev/gemini-api/docs/music-generation,Industry,,,,,,,,,
Marin 8B,Language,"Language modeling/generation,Question answering,Code generation,Quantitative reasoning",Marin,,2025-05-19,Marin 8B Retrospective,https://marin.readthedocs.io/en/latest/reports/marin-8b-retro/,,,,8000000000.0,"8B

Architecture Details
Architecture: Llama 3 8B
Hidden size: 4096
Feedforward size: 14336
Number of layers: 32
Number of attention heads: 32
Number of KV heads: 8",6.12e+23,6 FLOP / parameter / token * 8 *10^9 parameters * 12.75 * 10^12 tokens = 6.12e+23 FLOP,"DCLM-baseline,StarCoder,Proofpile 2,Nemotron CC","""Retrospectively, we can partition the 8B run into several distinct phasesâ€”each nicknamed after an animal:

Kestrel (DCLM WSD-S Phase): In the first phase, we used the ""DCLM mix"" and WSD-S for about 2.7T tokens. We used 2x TPU v5e-256 coordinated with multislice for this. (0â†’2.7T tokens)
Ocelot (DCLM WSD Phase): We were given access to a v4-2048 slice and moved to that. To better utilize the hardware, we increased our batch size 50%. We also switched from WSD-S to WSD. We kept the learning rate high through 3.78T tokens.
Jellyfish (First Cooldown): It was time to cooldown as we were starting to run low on DCLM. Following recent work on midtraining (e.g. Olmo 2), we decided to fold in higher quality data during cooldown. (3.78Tâ†’4.78T tokens)
Phoenix (Reheated): We had more time for training, so we rapidly rewarmed the model and transitioned our mixture to Nemotron-CC (plus StarCoder Data). (4.78Tâ†’11.1T tokens)
Starling (Second Cooldown): Now we were running low on time, so we started another cooldown. We followed a similar process to the first cooldown, but added a few new datasets that we had created and also some that had dropped since our previous attempt. (11.1Tâ†’12.75T tokens)""",12750000000000.0,12.75T tokens,,,"Google TPU v5e,Google TPU v4",,Confident,"The ""Tootsie Roll"" Process
A core premise of the Marin 8B run was that we didn't fully know the best recipeâ€” so we just started training with what we had, and planned to adapt along the way. Internally, we referred to this as the ""Tootsie"" process, a reference to Tootsie Rolls, which use a ""graining"" process where each day's batch contains a bit of the previous day's, seeding crystallization or something. (We are not food scientists.) This is admittedly a bit of a strained metaphor, but the idea was that we'd keep folding in new data, training techniques, and whatever else as the training process went on. (As it would turn out, dear reader, we would often change more than the data...)

Model Basics
Model Size
We decided to build a roughly 7-8 billion parameter model mostly out of pragmatism: we initially only had reserved capacity to train a model of that size for long enough.

Architecture
We settled on the Llama architecture for the usual reasons: it has been shown to work well, easier to plug into existing inference stacks, no one ever got fired for buying IBM, etc.

We used the same settings as Llama 3.1 8B.",,,Open weights (unrestricted),United States of America,,,,,,2025-05-30 14:51,Google Cloud,,,,,Research collective,,,,,Open source,"Apache 2.0

https://huggingface.co/marin-community/marin-8b-base",Research collective,,,,,Operation counting,marin-community,,,
Pangu Ultra MoE,Language,"Language modeling/generation,Question answering",Huawei,"Yehui Tang, Yichun Yin, Yaoyuan Wang, Hang Zhou, Yu Pan, Wei Guo, Ziyang Zhang, Miao Rang, Fangcheng Liu, Naifu Zhang, Binghan Li, Yonghan Dong, Xiaojun Meng, Yasheng Wang, Dong Li, Yin Li, Dandan Tu, Can Chen, Youliang Yan, Fisher Yu, Ruiming Tang, Yunhe Wang, Botian Huang, Bo Wang, Boxiao Liu, Changzheng Zhang, Da Kuang, Fei Liu, Gang Huang, Jiansheng Wei, Jiarui Qin, Jie Ran, Jinpeng Li, Jun Zhao, Liang Dai, Lin Li, Liqun Deng, Peifeng Qin, Pengyuan Zeng, Qiang Gu, Shaohua Tang, Shengjun Cheng, Tao Gao, Tao Yu, Tianshu Li, Tianyu Bi, Wei He, Weikai Mao, Wenyong Huang, Wulong Liu, Xiabing Li, Xianzhi Yu, Xueyu Wu, Xu He, Yangkai Du, Yan Xu, Ye Tian, Yimeng Wu, Yongbing Huang, Yong Tian, Yong Zhu, Yue Li, Yufei Wang, Yuhang Gai, Yujun Li, Yu Luo, Yunsheng Ni, Yusen Sun, Zelin Chen, Zhe Liu, Zhicheng Liu, Zhipeng Tu, Zilin Ding, Zongyuan Zhan",2025-05-07,Pangu Ultra MoE: How to Train Your Big MoE on Ascend NPUs,https://arxiv.org/abs/2505.04519,,,,718000000000.0,"a sparse LLM with 718 billion parameters

39B activated parameters",3.0888e+24,"Speculatively assuming they trained MoE model on the same amount of tokens as Pangu Ultra (13,2 T): 

6 FLOP / parameter / token * 39 * 10^9 activated parameters * 13,2 * 10^12 tokens = 3.0888e+24 FLOP",,,,,,,Huawei Ascend 910B,,Speculative,"Sparse large language models (LLMs) with Mixture of Experts (MoE) and close to a trillion parameters are dominating the realm of most capable language models. However, the massive model scale poses significant challenges for the underlying software and hardware systems. In this paper, we aim to uncover a recipe to harness such scale on Ascend NPUs. The key goals are better usage of the computing resources under the dynamic sparse model structures and materializing the expected performance gain on the actual hardware. To select model configurations suitable for Ascend NPUs without repeatedly running the expensive experiments, we leverage simulation to compare the trade-off of various model hyperparameters. This study led to Pangu Ultra MoE, a sparse LLM with 718 billion parameters, and we conducted experiments on the model to verify the simulation results. On the system side, we dig into Expert Parallelism to optimize the communication between NPU devices to reduce the synchronization overhead. We also optimize the memory efficiency within the devices to further reduce the parameter and activation management overhead. In the end, we achieve an MFU of 30.0% when training Pangu Ultra MoE, with performance comparable to that of DeepSeek R1, on 6K Ascend NPUs, and demonstrate that the Ascend system is capable of harnessing all the training stages of the state-of-the-art language models. Extensive experiments indicate that our recipe can lead to efficient training of large-scale sparse language models with MoE. We also study the behaviors of such models for future reference.",,,Hosted access (no API),China,,,,6000.0,0.3,2025-05-30 14:51,,,,,,Industry,,,,,Unreleased,,Industry,,,,4703802.674709778,Operation counting,,,,
Apriel Nemotron 15B,Language,"Language modeling/generation,Question answering,Quantitative reasoning,Chat","NVIDIA,ServiceNow",,2025-05-06,"New Apriel Nemotron 15B reasoning model delivers lower latency, lower inference costs, and faster agentic AIâ€”purpose built for performance, cost, and scale",https://www.servicenow.com/company/media/press-room/nvidia-enterprise-ai-agents.html,,,,15000000000.0,15B,9.00001e+21,"6 FLOP / parameter / token * 15 * 10^9 parameters * 100 * 10^9 tokens = 9e+21 FLOP

[1 epoch assumed] -> ""Likely"" confidence",,,100000000000.0,"1. Mid training / Continual Preâ€‘training In this stage, the model is trained on 100+â€¯billion tokens of carefully curated examples drawn from mathematical reasoning, coding challenges, scientific discourse and logical puzzles.

2. Supervised Fineâ€‘Tuning (SFT) Next, we SFT the model using 200,000 highâ€‘quality demonstrations that cover mathematical and scientific problemâ€‘solving, coding tasks, generic instructionâ€‘following scenarios, API/function invocation use cases etc.

3. RLHF",,,,,Likely,"Apriel-Nemotron-15b-Thinker is a 15â€¯billionâ€‘parameter reasoning model in ServiceNowâ€™s Apriel SLM series which achieves competitive performance against similarly sized state-of-the-art models like o1â€‘mini, QWQâ€‘32b, and EXAONEâ€‘Deepâ€‘32b, all while maintaining only half the memory footprint of those alternatives. It builds upon the Aprielâ€‘15bâ€‘base checkpoint through a threeâ€‘stage training pipeline (CPT, SFT and GRPO).

Highlights

Half the size of SOTA models like QWQ-32b and EXAONE-32b and hence memory efficient.
It consumes 40% less tokens compared to QWQ-32b, making it super efficient in production. ðŸš€ðŸš€ðŸš€
On par or outperforms on tasks like - MBPP, BFCL, Enterprise RAG, MT Bench, MixEval, IFEval and Multi-Challenge making it great for Agentic / Enterprise tasks.
Competitive performance on academic benchmarks like AIME-24 AIME-25, AMC-23, MATH-500 and GPQA considering model size.",,,Open weights (unrestricted),"United States of America,United States of America",,,,,,2025-05-22 13:15,,,,,,"Industry,Industry",,,,,Unreleased,"MIT license
https://huggingface.co/ServiceNow-AI/Apriel-Nemotron-15b-Thinker","Industry,Industry",,,,,,ServiceNow-AI,,,
Typhoon 2.1 Gemma 4B,Language,"Language modeling/generation,Question answering,Quantitative reasoning,Code generation,Translation",Typhoon / SCB 10X,,2025-05-05,Typhoon 2.1 Gemma Release: Big Performance in Small Sizes,https://opentyphoon.ai/blog/en/typhoon2-1-gemma-release,,,,4000000000.0,"4B
Model type: A 4B instruct decoder-only model based on Gemma3 architecture.",,,Unspecified unreleased,,,,,,,,Confident,"Typhoon2.1-Gemma3-4B is a instruct Thai ðŸ‡¹ðŸ‡­ large language model with 4 billion parameters, a 128K context length, and function-calling capabilities. It is based on Gemma3 4B.

To build Typhoon 2.1 (Gemma3-based), we introduced a new approach that combines fine-tuning, model merging techniques from Typhoon 2 R1, and reinforcement learning (RL) fine-tuningâ€”details to be shared in an upcoming paper.

We began by using supervised fine-tuning (SFT) and merging to align the model with Thai-specific preferences, applying a curated subset of post-training recipes from Typhoon 2. This made the model more controllable and better suited to Thai use cases. Once we achieved strong instruction-following in Thai, we applied RL fine-tuning to correct merging artifacts and train the model to perform controllable long-thought processes.",,,Open weights (restricted use),Thailand,Gemma 3 4B,,,,,2025-05-06 11:50,,,,,,Industry,,,,,Unreleased,"Gemma license
https://huggingface.co/scb10x/typhoon2.1-gemma3-4b",Industry,,,,,,scb10x,,,
Typhoon 2.1 Gemma 12B,Language,"Language modeling/generation,Question answering,Quantitative reasoning,Code generation,Translation",Typhoon / SCB 10X,,2025-05-05,Typhoon 2.1 Gemma Release: Big Performance in Small Sizes,https://opentyphoon.ai/blog/en/typhoon2-1-gemma-release,,,,12000000000.0,12B,,,Unspecified unreleased,,,,,,,,Confident,"Typhoon2.1-Gemma3-12B is a instruct Thai ðŸ‡¹ðŸ‡­ large language model with 12 billion parameters, a 128K context length, and function-calling capabilities. It is based on Gemma3 12B.

To build Typhoon 2.1 (Gemma3-based), we introduced a new approach that combines fine-tuning, model merging techniques from Typhoon 2 R1, and reinforcement learning (RL) fine-tuningâ€”details to be shared in an upcoming paper.

We began by using supervised fine-tuning (SFT) and merging to align the model with Thai-specific preferences, applying a curated subset of post-training recipes from Typhoon 2. This made the model more controllable and better suited to Thai use cases. Once we achieved strong instruction-following in Thai, we applied RL fine-tuning to correct merging artifacts and train the model to perform controllable long-thought processes.",,,Open weights (restricted use),Thailand,Gemma 3 12B,,,,,2025-05-06 11:50,,,,,,Industry,,,,,Unreleased,"gemma license
https://huggingface.co/scb10x/typhoon2.1-gemma3-12b",Industry,,,,,,scb10x,,,
Phi-4-Reasoning,Language,"Language modeling/generation,Quantitative reasoning,Question answering,Mathematical reasoning,Code generation",Microsoft,"Marah Abdin, Sahaj Agarwal, Ahmed Awadallah, Vidhisha Balachandran, Harkirat Behl, Lingjiao Chen, Gustavo de Rosa, Suriya Gunasekar, Mojan Javaheripi, Neel Joshi, Piero Kauffmann, Yash Lara, Caio CÃ©sar Teodoro Mendes, Arindam Mitra, Besmira Nushi, Dimitris Papailiopoulos, Olli Saarikivi, Shital Shah, Vaishnavi Shrivastava, Vibhav Vineet, Yue Wu, Safoora Yousefi, Guoqing Zheng",2025-04-30,Phi-4-reasoning Technical Report,https://arxiv.org/abs/2504.21318,,,,14000000000.0,14B,9.3368077e+23,9.3202015e+23 FLOP [base model compute] + 1.6606191e+21 FLOP [finetune compute] = 9.3368077e+23 FLOP,Unspecified unreleased,"Knowledge cutoff date is March 2025, according to https://huggingface.co/microsoft/Phi-4-reasoning.",8300000000.0,"""Our SFT data comprises over 1.4 million prompt-response pairs, totaling 8.3 billion unique tokens of reasoning domains such as math and coding, and alignment data for safety and Responsible AI. Training is run over roughly 16K steps, with a global batch size of 32 and a context length of 32K tokens. ""

""The final model was trained for 16B tokens using this mixture.""

from HF: 
Training data	16B tokens, ~8.3B unique tokens

-> ~1.93 epochs",60.0,"""GPUs	32 H100-80G
Training time	2.5 days""

2.5*24 = 60 (hours)",NVIDIA H100 SXM5 80GB,,Confident,"We introduce Phi-4-reasoning, a 14-billion parameter reasoning model that achieves strong performance on complex reasoning tasks. Trained via supervised fine-tuning of Phi-4 on carefully curated set of ""teachable"" prompts-selected for the right level of complexity and diversity-and reasoning demonstrations generated using o3-mini, Phi-4-reasoning generates detailed reasoning chains that effectively leverage inference-time compute. We further develop Phi-4-reasoning-plus, a variant enhanced through a short phase of outcome-based reinforcement learning that offers higher performance by generating longer reasoning traces. Across a wide range of reasoning tasks, both models outperform significantly larger open-weight models such as DeepSeek-R1-Distill-Llama-70B model and approach the performance levels of full DeepSeek-R1 model. Our comprehensive evaluations span benchmarks in math and scientific reasoning, coding, algorithmic problem solving, planning, and spatial understanding. Interestingly, we observe a non-trivial transfer of improvements to general-purpose benchmarks as well. In this report, we provide insights into our training data, our training methodologies, and our evaluations. We show that the benefit of careful data curation for supervised fine-tuning (SFT) extends to reasoning language models, and can be further amplified by reinforcement learning (RL). Finally, our evaluation points to opportunities for improving how we assess the performance and robustness of reasoning models.",1.93,,Open weights (unrestricted),"United States of America,Multinational,India,Belgium",Phi-4,1,"6 FLOP / token / parameter * 14 * 10^9 parameters * 16 * 10^9 tokens = 1.344e+21 FLOP

989500000000000 FLOP / sec / GPU [bf assumed] * 32 GPUs * 60 hours [see training time notes] * 3600 sec / hour * 0.3 [assumed utilization] = 2.0518272e+21 FLOP

sqrt(1.344e+21*2.0518272e+21) = 1.6606191e+21 FLOP",32.0,,2025-05-16 10:28,,,,,,Industry,,,,,Unreleased,"https://huggingface.co/microsoft/Phi-4-reasoning

MIT license",Industry,,,,43909.002544977615,"Operation counting,Hardware",microsoft,,,
Phi-4-Reasoning-plus,Language,"Language modeling/generation,Quantitative reasoning,Question answering,Mathematical reasoning,Code generation",Microsoft,"Marah Abdin, Sahaj Agarwal, Ahmed Awadallah, Vidhisha Balachandran, Harkirat Behl, Lingjiao Chen, Gustavo de Rosa, Suriya Gunasekar, Mojan Javaheripi, Neel Joshi, Piero Kauffmann, Yash Lara, Caio CÃ©sar Teodoro Mendes, Arindam Mitra, Besmira Nushi, Dimitris Papailiopoulos, Olli Saarikivi, Shital Shah, Vaishnavi Shrivastava, Vibhav Vineet, Yue Wu, Safoora Yousefi, Guoqing Zheng",2025-04-30,Phi-4-reasoning Technical Report,https://arxiv.org/abs/2504.21318,,,,14000000000.0,14B,,,Unspecified unreleased,"Knowledge cutoff date is March 2025, according to https://huggingface.co/microsoft/Phi-4-reasoning-plus.",,"""Phi-4-reasoning-plus is further trained with Reinforcement Learning on a small set of âˆ¼6K high-quality math-focused problems with verifiable solutions.""

""the model trained for 90 steps, over only âˆ¼ 6k examples (and 8 trajectories of responses per example).""",,,NVIDIA H100 SXM5 80GB,,Confident,"We introduce Phi-4-reasoning, a 14-billion parameter reasoning model that achieves strong performance on complex reasoning tasks. Trained via supervised fine-tuning of Phi-4 on carefully curated set of ""teachable"" prompts-selected for the right level of complexity and diversity-and reasoning demonstrations generated using o3-mini, Phi-4-reasoning generates detailed reasoning chains that effectively leverage inference-time compute. We further develop Phi-4-reasoning-plus, a variant enhanced through a short phase of outcome-based reinforcement learning that offers higher performance by generating longer reasoning traces. Across a wide range of reasoning tasks, both models outperform significantly larger open-weight models such as DeepSeek-R1-Distill-Llama-70B model and approach the performance levels of full DeepSeek-R1 model. Our comprehensive evaluations span benchmarks in math and scientific reasoning, coding, algorithmic problem solving, planning, and spatial understanding. Interestingly, we observe a non-trivial transfer of improvements to general-purpose benchmarks as well. In this report, we provide insights into our training data, our training methodologies, and our evaluations. We show that the benefit of careful data curation for supervised fine-tuning (SFT) extends to reasoning language models, and can be further amplified by reinforcement learning (RL). Finally, our evaluation points to opportunities for improving how we assess the performance and robustness of reasoning models.",,,Open weights (unrestricted),"United States of America,Multinational,India,Belgium",Phi-4-Reasoning,,,32.0,,2025-05-12 23:54,,,,,,Industry,,,,,Unreleased,"https://huggingface.co/microsoft/Phi-4-reasoning-plus

MIT license",Industry,,,,43909.002544977615,Operation counting,microsoft,,,
Qwen3-235B-A22B,Language,"Language modeling/generation,Question answering,Mathematical reasoning,Quantitative reasoning,Code generation,Translation",Alibaba,,2025-04-29,"Qwen3: Think Deeper, Act Faster",https://qwenlm.github.io/blog/qwen3/,,,,235000000000.0,"235 billion total parameters and 22 billion activated parameters

Number of Layers: 94
Number of Attention Heads (GQA): 64 for Q and 4 for KV
Number of Experts: 128
Number of Activated Experts: 8
Context Length: 32,768 natively and 131,072 tokens with YaRN.",4.752e+24,6 FLOP / parameter / token * 22*10^9 active parameters * 36000000000000 tokens = 4.752e+24 FLOP,Unspecified unreleased,"""While Qwen2.5 was pre-trained on 18 trillion tokens, Qwen3 uses nearly twice that amount, with approximately 36 trillion tokens covering 119 languages and dialects. To build this large dataset, we collected data not only from the web but also from PDF-like documents. We used Qwen2.5-VL to extract text from these documents and Qwen2.5 to improve the quality of the extracted content. To increase the amount of math and code data, we used Qwen2.5-Math and Qwen2.5-Coder to generate synthetic data. This includes textbooks, question-answer pairs, and code snippets.

The pre-training process consists of three stages. In the first stage (S1), the model was pretrained on over 30 trillion tokens with a context length of 4K tokens. This stage provided the model with basic language skills and general knowledge. In the second stage (S2), we improved the dataset by increasing the proportion of knowledge-intensive data, such as STEM, coding, and reasoning tasks. The model was then pretrained on an additional 5 trillion tokens. In the final stage, we used high-quality long-context data to extend the context length to 32K tokens. """,36000000000000.0,36T,,,,,Likely,"Today, we are excited to announce the release of Qwen3, the latest addition to the Qwen family of large language models. Our flagship model, Qwen3-235B-A22B, achieves competitive results in benchmark evaluations of coding, math, general capabilities, etc., when compared to other top-tier models such as DeepSeek-R1, o1, o3-mini, Grok-3, and Gemini-2.5-Pro. Additionally, the small MoE model, Qwen3-30B-A3B, outcompetes QwQ-32B with 10 times of activated parameters, and even a tiny model like Qwen3-4B can rival the performance of Qwen2.5-72B-Instruct.",1.0,,Open weights (unrestricted),China,,,,,,2025-06-03 06:57,,,,,,Industry,,,,,Unreleased,"Apache 2.0

https://huggingface.co/Qwen/Qwen3-235B-A22B",Industry,,,,,Operation counting,Qwen,,,
Qwen3-30B-A3B,Language,"Language modeling/generation,Question answering,Mathematical reasoning,Quantitative reasoning,Code generation,Translation",Alibaba,,2025-04-29,"Qwen3: Think Deeper, Act Faster",https://qwenlm.github.io/blog/qwen3/,,,,30000000000.0,"30 billion total parameters and 3 billion activated parameters

Number of Layers: 48
Number of Attention Heads (GQA): 32 for Q and 4 for KV
Number of Experts: 128
Number of Activated Experts: 8
Context Length: 32,768",6.48e+23,6 FLOP / parameter / token * 3*10^9 active parameters * 36000000000000 tokens = 6.48e+23 FLOP,Unspecified unreleased,"""While Qwen2.5 was pre-trained on 18 trillion tokens, Qwen3 uses nearly twice that amount, with approximately 36 trillion tokens covering 119 languages and dialects. To build this large dataset, we collected data not only from the web but also from PDF-like documents. We used Qwen2.5-VL to extract text from these documents and Qwen2.5 to improve the quality of the extracted content. To increase the amount of math and code data, we used Qwen2.5-Math and Qwen2.5-Coder to generate synthetic data. This includes textbooks, question-answer pairs, and code snippets.

The pre-training process consists of three stages. In the first stage (S1), the model was pretrained on over 30 trillion tokens with a context length of 4K tokens. This stage provided the model with basic language skills and general knowledge. In the second stage (S2), we improved the dataset by increasing the proportion of knowledge-intensive data, such as STEM, coding, and reasoning tasks. The model was then pretrained on an additional 5 trillion tokens. In the final stage, we used high-quality long-context data to extend the context length to 32K tokens. """,36000000000000.0,36T,,,,,Likely,"Today, we are excited to announce the release of Qwen3, the latest addition to the Qwen family of large language models. Our flagship model, Qwen3-235B-A22B, achieves competitive results in benchmark evaluations of coding, math, general capabilities, etc., when compared to other top-tier models such as DeepSeek-R1, o1, o3-mini, Grok-3, and Gemini-2.5-Pro. Additionally, the small MoE model, Qwen3-30B-A3B, outcompetes QwQ-32B with 10 times of activated parameters, and even a tiny model like Qwen3-4B can rival the performance of Qwen2.5-72B-Instruct.",1.0,,Open weights (unrestricted),China,,,,,,2025-05-29 15:16,,,,,,Industry,,,,,Unreleased,"Apache 2.0

https://huggingface.co/Qwen/Qwen3-30B-A3B",Industry,,,,,Operation counting,Qwen,,,
Qwen3-32B,Language,"Language modeling/generation,Question answering,Mathematical reasoning,Quantitative reasoning,Code generation,Translation",Alibaba,,2025-04-29,"Qwen3: Think Deeper, Act Faster",https://qwenlm.github.io/blog/qwen3/,,,,32800000000.0,"Number of Parameters: 32.8B
Number of Paramaters (Non-Embedding): 31.2B
Number of Layers: 64
Number of Attention Heads (GQA): 64 for Q and 8 for KV
Context Length: 32,768 natively and 131,072 tokens with YaRN.",7.084799999999999e+24,6 FLOP / parameter / token * 36 * 10^12 tokens * 32.8  * 10^9 parameters = 7.0848e+24 FLOP,Unspecified unreleased,"""While Qwen2.5 was pre-trained on 18 trillion tokens, Qwen3 uses nearly twice that amount, with approximately 36 trillion tokens covering 119 languages and dialects. To build this large dataset, we collected data not only from the web but also from PDF-like documents. We used Qwen2.5-VL to extract text from these documents and Qwen2.5 to improve the quality of the extracted content. To increase the amount of math and code data, we used Qwen2.5-Math and Qwen2.5-Coder to generate synthetic data. This includes textbooks, question-answer pairs, and code snippets.

The pre-training process consists of three stages. In the first stage (S1), the model was pretrained on over 30 trillion tokens with a context length of 4K tokens. This stage provided the model with basic language skills and general knowledge. In the second stage (S2), we improved the dataset by increasing the proportion of knowledge-intensive data, such as STEM, coding, and reasoning tasks. The model was then pretrained on an additional 5 trillion tokens. In the final stage, we used high-quality long-context data to extend the context length to 32K tokens. """,36000000000000.0,36T,,,,,Confident,"Qwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models. Built upon extensive training, Qwen3 delivers groundbreaking advancements in reasoning, instruction-following, agent capabilities, and multilingual support, with the following key features:

Uniquely support of seamless switching between thinking mode (for complex logical reasoning, math, and coding) and non-thinking mode (for efficient, general-purpose dialogue) within single model, ensuring optimal performance across various scenarios.
Significantly enhancement in its reasoning capabilities, surpassing previous QwQ (in thinking mode) and Qwen2.5 instruct models (in non-thinking mode) on mathematics, code generation, and commonsense logical reasoning.
Superior human preference alignment, excelling in creative writing, role-playing, multi-turn dialogues, and instruction following, to deliver a more natural, engaging, and immersive conversational experience.
Expertise in agent capabilities, enabling precise integration with external tools in both thinking and unthinking modes and achieving leading performance among open-source models in complex agent-based tasks.
Support of 100+ languages and dialects with strong capabilities for multilingual instruction following and translation.",1.0,,Open weights (unrestricted),China,,,,,,2025-05-29 15:08,,,,,,Industry,,,,,Unreleased,"Apache 2.0

https://huggingface.co/Qwen/Qwen3-32B-Base",Industry,,,,,Operation counting,Qwen,,,
Qwen3-14B,Language,"Language modeling/generation,Question answering,Mathematical reasoning,Quantitative reasoning,Code generation,Translation",Alibaba,,2025-04-29,"Qwen3: Think Deeper, Act Faster",https://qwenlm.github.io/blog/qwen3/,,,,14800000000.0,"Number of Parameters: 14.8B
Number of Paramaters (Non-Embedding): 13.2B
Number of Layers: 40
Number of Attention Heads (GQA): 40 for Q and 8 for KV
Context Length: 32,768 natively and 131,072 tokens with YaRN.",3.1968e+24,6 FLOP / parameter / token * 36 * 10^12 tokens * 14.8  * 10^9 parameters = 3.1968e+24 FLOP,Unspecified unreleased,"""While Qwen2.5 was pre-trained on 18 trillion tokens, Qwen3 uses nearly twice that amount, with approximately 36 trillion tokens covering 119 languages and dialects. To build this large dataset, we collected data not only from the web but also from PDF-like documents. We used Qwen2.5-VL to extract text from these documents and Qwen2.5 to improve the quality of the extracted content. To increase the amount of math and code data, we used Qwen2.5-Math and Qwen2.5-Coder to generate synthetic data. This includes textbooks, question-answer pairs, and code snippets.

The pre-training process consists of three stages. In the first stage (S1), the model was pretrained on over 30 trillion tokens with a context length of 4K tokens. This stage provided the model with basic language skills and general knowledge. In the second stage (S2), we improved the dataset by increasing the proportion of knowledge-intensive data, such as STEM, coding, and reasoning tasks. The model was then pretrained on an additional 5 trillion tokens. In the final stage, we used high-quality long-context data to extend the context length to 32K tokens. """,36000000000000.0,36T,,,,,Confident,"Qwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models. Built upon extensive training, Qwen3 delivers groundbreaking advancements in reasoning, instruction-following, agent capabilities, and multilingual support, with the following key features:

Uniquely support of seamless switching between thinking mode (for complex logical reasoning, math, and coding) and non-thinking mode (for efficient, general-purpose dialogue) within single model, ensuring optimal performance across various scenarios.
Significantly enhancement in its reasoning capabilities, surpassing previous QwQ (in thinking mode) and Qwen2.5 instruct models (in non-thinking mode) on mathematics, code generation, and commonsense logical reasoning.
Superior human preference alignment, excelling in creative writing, role-playing, multi-turn dialogues, and instruction following, to deliver a more natural, engaging, and immersive conversational experience.
Expertise in agent capabilities, enabling precise integration with external tools in both thinking and unthinking modes and achieving leading performance among open-source models in complex agent-based tasks.
Support of 100+ languages and dialects with strong capabilities for multilingual instruction following and translation.",1.0,,Open weights (unrestricted),China,,,,,,2025-05-29 15:06,,,,,,Industry,,,,,Unreleased,"Apache 2.0

https://huggingface.co/Qwen/Qwen3-14B-Base",Industry,,,,,Operation counting,Qwen,,,
Qwen3-8B,Language,"Language modeling/generation,Question answering,Mathematical reasoning,Quantitative reasoning,Code generation,Translation",Alibaba,,2025-04-29,"Qwen3: Think Deeper, Act Faster",https://qwenlm.github.io/blog/qwen3/,,,,8200000000.0,"Number of Parameters: 8.2B
Number of Paramaters (Non-Embedding): 6.95B
Number of Layers: 36
Number of Attention Heads (GQA): 32 for Q and 8 for KV
Context Length: 32,768 natively and 131,072 tokens with YaRN.",1.7711999999999997e+24,6 FLOP / parameter / token * 36 * 10^12 tokens * 8.2  * 10^9 parameters = 1.7712e+24 FLOP,Unspecified unreleased,"""While Qwen2.5 was pre-trained on 18 trillion tokens, Qwen3 uses nearly twice that amount, with approximately 36 trillion tokens covering 119 languages and dialects. To build this large dataset, we collected data not only from the web but also from PDF-like documents. We used Qwen2.5-VL to extract text from these documents and Qwen2.5 to improve the quality of the extracted content. To increase the amount of math and code data, we used Qwen2.5-Math and Qwen2.5-Coder to generate synthetic data. This includes textbooks, question-answer pairs, and code snippets.

The pre-training process consists of three stages. In the first stage (S1), the model was pretrained on over 30 trillion tokens with a context length of 4K tokens. This stage provided the model with basic language skills and general knowledge. In the second stage (S2), we improved the dataset by increasing the proportion of knowledge-intensive data, such as STEM, coding, and reasoning tasks. The model was then pretrained on an additional 5 trillion tokens. In the final stage, we used high-quality long-context data to extend the context length to 32K tokens. """,36000000000000.0,36T,,,,,Confident,"Qwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models. Built upon extensive training, Qwen3 delivers groundbreaking advancements in reasoning, instruction-following, agent capabilities, and multilingual support, with the following key features:

Uniquely support of seamless switching between thinking mode (for complex logical reasoning, math, and coding) and non-thinking mode (for efficient, general-purpose dialogue) within single model, ensuring optimal performance across various scenarios.
Significantly enhancement in its reasoning capabilities, surpassing previous QwQ (in thinking mode) and Qwen2.5 instruct models (in non-thinking mode) on mathematics, code generation, and commonsense logical reasoning.
Superior human preference alignment, excelling in creative writing, role-playing, multi-turn dialogues, and instruction following, to deliver a more natural, engaging, and immersive conversational experience.
Expertise in agent capabilities, enabling precise integration with external tools in both thinking and unthinking modes and achieving leading performance among open-source models in complex agent-based tasks.
Support of 100+ languages and dialects with strong capabilities for multilingual instruction following and translation.",1.0,,Open weights (unrestricted),China,,,,,,2025-05-29 15:08,,,,,,Industry,,,,,Unreleased,"Apache 2.0

https://huggingface.co/Qwen/Qwen3-8B-Base",Industry,,,,,Operation counting,Qwen,,,
Qwen3-4B,Language,"Language modeling/generation,Question answering,Mathematical reasoning,Quantitative reasoning,Code generation,Translation",Alibaba,,2025-04-29,"Qwen3: Think Deeper, Act Faster",https://qwenlm.github.io/blog/qwen3/,,,,4000000000.0,"Number of Parameters: 4.0B
Number of Paramaters (Non-Embedding): 3.6B
Number of Layers: 36
Number of Attention Heads (GQA): 32 for Q and 8 for KV
Context Length: 32,768 natively and 131,072 tokens with YaRN.",8.64e+23,6 FLOP / parameter / token * 36 * 10^12 tokens * 4  * 10^9 parameters = 8.64e+23 FLOP,Unspecified unreleased,"""While Qwen2.5 was pre-trained on 18 trillion tokens, Qwen3 uses nearly twice that amount, with approximately 36 trillion tokens covering 119 languages and dialects. To build this large dataset, we collected data not only from the web but also from PDF-like documents. We used Qwen2.5-VL to extract text from these documents and Qwen2.5 to improve the quality of the extracted content. To increase the amount of math and code data, we used Qwen2.5-Math and Qwen2.5-Coder to generate synthetic data. This includes textbooks, question-answer pairs, and code snippets.

The pre-training process consists of three stages. In the first stage (S1), the model was pretrained on over 30 trillion tokens with a context length of 4K tokens. This stage provided the model with basic language skills and general knowledge. In the second stage (S2), we improved the dataset by increasing the proportion of knowledge-intensive data, such as STEM, coding, and reasoning tasks. The model was then pretrained on an additional 5 trillion tokens. In the final stage, we used high-quality long-context data to extend the context length to 32K tokens. """,36000000000000.0,36T,,,,,Confident,"Qwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models. Built upon extensive training, Qwen3 delivers groundbreaking advancements in reasoning, instruction-following, agent capabilities, and multilingual support, with the following key features:

Uniquely support of seamless switching between thinking mode (for complex logical reasoning, math, and coding) and non-thinking mode (for efficient, general-purpose dialogue) within single model, ensuring optimal performance across various scenarios.
Significantly enhancement in its reasoning capabilities, surpassing previous QwQ (in thinking mode) and Qwen2.5 instruct models (in non-thinking mode) on mathematics, code generation, and commonsense logical reasoning.
Superior human preference alignment, excelling in creative writing, role-playing, multi-turn dialogues, and instruction following, to deliver a more natural, engaging, and immersive conversational experience.
Expertise in agent capabilities, enabling precise integration with external tools in both thinking and unthinking modes and achieving leading performance among open-source models in complex agent-based tasks.
Support of 100+ languages and dialects with strong capabilities for multilingual instruction following and translation.",1.0,,Open weights (unrestricted),China,,,,,,2025-05-16 10:30,,,,,,Industry,,,,,Unreleased,"Apache 2.0

https://huggingface.co/Qwen/Qwen3-4B-Base",Industry,,,,,Operation counting,Qwen,,,
Qwen3-1.7B,Language,"Language modeling/generation,Question answering,Mathematical reasoning,Quantitative reasoning,Code generation,Translation",Alibaba,,2025-04-29,"Qwen3: Think Deeper, Act Faster",https://qwenlm.github.io/blog/qwen3/,,,,1700000000.0,"Number of Parameters: 1.7B
Number of Paramaters (Non-Embedding): 1.4B
Number of Layers: 28
Number of Attention Heads (GQA): 16 for Q and 8 for KV
Context Length: 32,768
",3.672e+23,6 FLOP / parameter / token * 36 * 10^12 tokens * 1.7  * 10^9 parameters = 3.672e+23 FLOP,Unspecified unreleased,"""While Qwen2.5 was pre-trained on 18 trillion tokens, Qwen3 uses nearly twice that amount, with approximately 36 trillion tokens covering 119 languages and dialects. To build this large dataset, we collected data not only from the web but also from PDF-like documents. We used Qwen2.5-VL to extract text from these documents and Qwen2.5 to improve the quality of the extracted content. To increase the amount of math and code data, we used Qwen2.5-Math and Qwen2.5-Coder to generate synthetic data. This includes textbooks, question-answer pairs, and code snippets.

The pre-training process consists of three stages. In the first stage (S1), the model was pretrained on over 30 trillion tokens with a context length of 4K tokens. This stage provided the model with basic language skills and general knowledge. In the second stage (S2), we improved the dataset by increasing the proportion of knowledge-intensive data, such as STEM, coding, and reasoning tasks. The model was then pretrained on an additional 5 trillion tokens. In the final stage, we used high-quality long-context data to extend the context length to 32K tokens. """,36000000000000.0,36T,,,,,Confident,"Qwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models. Built upon extensive training, Qwen3 delivers groundbreaking advancements in reasoning, instruction-following, agent capabilities, and multilingual support, with the following key features:

Uniquely support of seamless switching between thinking mode (for complex logical reasoning, math, and coding) and non-thinking mode (for efficient, general-purpose dialogue) within single model, ensuring optimal performance across various scenarios.
Significantly enhancement in its reasoning capabilities, surpassing previous QwQ (in thinking mode) and Qwen2.5 instruct models (in non-thinking mode) on mathematics, code generation, and commonsense logical reasoning.
Superior human preference alignment, excelling in creative writing, role-playing, multi-turn dialogues, and instruction following, to deliver a more natural, engaging, and immersive conversational experience.
Expertise in agent capabilities, enabling precise integration with external tools in both thinking and unthinking modes and achieving leading performance among open-source models in complex agent-based tasks.
Support of 100+ languages and dialects with strong capabilities for multilingual instruction following and translation.",1.0,,Open weights (unrestricted),China,,,,,,2025-05-16 10:30,,,,,,Industry,,,,,Unreleased,"Apache 2.0

https://huggingface.co/Qwen/Qwen3-1.7B-Base",Industry,,,,,Operation counting,Qwen,,,
Qwen3-0.6B,Language,"Language modeling/generation,Question answering,Mathematical reasoning,Quantitative reasoning,Code generation,Translation",Alibaba,,2025-04-29,"Qwen3: Think Deeper, Act Faster",https://qwenlm.github.io/blog/qwen3/,,,,600000000.0,"Number of Parameters: 0.6B
Number of Paramaters (Non-Embedding): 0.44B
Number of Layers: 28
Number of Attention Heads (GQA): 16 for Q and 8 for KV
Context Length: 32,768",1.296e+23,6 FLOP / parameter / token * 36 * 10^12 tokens * 0.6 * 10^9 parameters = 1.296e+23 FLOP,Unspecified unreleased,"""While Qwen2.5 was pre-trained on 18 trillion tokens, Qwen3 uses nearly twice that amount, with approximately 36 trillion tokens covering 119 languages and dialects. To build this large dataset, we collected data not only from the web but also from PDF-like documents. We used Qwen2.5-VL to extract text from these documents and Qwen2.5 to improve the quality of the extracted content. To increase the amount of math and code data, we used Qwen2.5-Math and Qwen2.5-Coder to generate synthetic data. This includes textbooks, question-answer pairs, and code snippets.

The pre-training process consists of three stages. In the first stage (S1), the model was pretrained on over 30 trillion tokens with a context length of 4K tokens. This stage provided the model with basic language skills and general knowledge. In the second stage (S2), we improved the dataset by increasing the proportion of knowledge-intensive data, such as STEM, coding, and reasoning tasks. The model was then pretrained on an additional 5 trillion tokens. In the final stage, we used high-quality long-context data to extend the context length to 32K tokens. """,36000000000000.0,36T,,,,,Confident,"Qwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models. Built upon extensive training, Qwen3 delivers groundbreaking advancements in reasoning, instruction-following, agent capabilities, and multilingual support, with the following key features:

Uniquely support of seamless switching between thinking mode (for complex logical reasoning, math, and coding) and non-thinking mode (for efficient, general-purpose dialogue) within single model, ensuring optimal performance across various scenarios.
Significantly enhancement in its reasoning capabilities, surpassing previous QwQ (in thinking mode) and Qwen2.5 instruct models (in non-thinking mode) on mathematics, code generation, and commonsense logical reasoning.
Superior human preference alignment, excelling in creative writing, role-playing, multi-turn dialogues, and instruction following, to deliver a more natural, engaging, and immersive conversational experience.
Expertise in agent capabilities, enabling precise integration with external tools in both thinking and unthinking modes and achieving leading performance among open-source models in complex agent-based tasks.
Support of 100+ languages and dialects with strong capabilities for multilingual instruction following and translation.",1.0,,Open weights (unrestricted),China,,,,,,2025-05-16 10:30,,,,,,Industry,,,,,Unreleased,"Apache 2.0

https://huggingface.co/Qwen/Qwen3-0.6B-Base",Industry,,,,,Operation counting,Qwen,,,
Pleias-RAG-350m,Language,"Retrieval-augmented generation,Language modeling/generation,Question answering,Search,Text summarization,Translation",PleIAs,"Pierre-Carl Langlais, Pavel Chizhov, Mattia Nee, Carlos Rosas Hinostroza, Matthieu Delsart, IrÃ¨ne Girard, Othman Hicheur, Anastasia Stasenko, Ivan P. Yamshchikov",2025-04-25,Even Small Reasoners Should Quote Their Sources: Introducing the Pleias-RAG Model Family,https://arxiv.org/abs/2504.18225,,,,350000000.0,"350M
",2.7186806e+21,base model compute 2.6788982e+21 FLOP + finetune compute  3.9782379e+19 FLOP = 2.7186806e+21 FLOP,,"""In total, our mid-training dataset includes 3,126,691 RAG examples, making about 9,471,995,091
tokens (hence roughly 3,000 tokens per example on average).""",9471995091.0,,,,NVIDIA H100 SXM5 80GB,,Confident,"We introduce a new generation of small reasoning models for RAG, search, and source summarization. Pleias-RAG-350m and Pleias-RAG-1B are mid-trained on a large synthetic dataset emulating the retrieval of a wide variety of multilingual open sources from the Common Corpus. They provide native support for citation and grounding with literal quotes and reintegrate multiple features associated with RAG workflows, such as query routing, query reformulation, and source reranking. Pleias-RAG-350m and Pleias-RAG-1B outperform SLMs below 4 billion parameters on standardized RAG benchmarks (HotPotQA, 2wiki) and are competitive with popular larger models, including Qwen-2.5-7B, Llama-3.1-8B, and Gemma-3-4B. They are the only SLMs to date maintaining consistent RAG performance across leading European languages and ensuring systematic reference grounding for statements. Due to their size and ease of deployment on constrained infrastructure and higher factuality by design, the models unlock a range of new use cases for generative AI.",2.0,,Open weights (unrestricted),France,Pleias 1.0 350m,39782379000000000000,6 FLOP / parameter / token * 350 * 10^6 parameters * 9471995091 tokens * 2 epochs = 3.9782379e+19 FLOP,16.0,,2025-05-12 13:53,,,,,,Industry,,,,,Unreleased,"Apache 2.0

https://huggingface.co/PleIAs/Pleias-RAG-350M",Industry,,,,21956.94597329117,Operation counting,PleIAs,,,
Pleias-RAG-1B,Language,"Retrieval-augmented generation,Language modeling/generation,Question answering,Search,Text summarization,Translation",PleIAs,"Pierre-Carl Langlais, Pavel Chizhov, Mattia Nee, Carlos Rosas Hinostroza, Matthieu Delsart, IrÃ¨ne Girard, Othman Hicheur, Anastasia Stasenko, Ivan P. Yamshchikov",2025-04-25,Even Small Reasoners Should Quote Their Sources: Introducing the Pleias-RAG Model Family,https://arxiv.org/abs/2504.18225,,,,1200000000.0,1.2B,2.9907184e+22,base model compute 2.9770787e+22 FLOP + finetune compute 1.3639673e+20 FLOP = 2.9907184e+22 FLOP,,"""In total, our mid-training dataset includes 3,126,691 RAG examples, making about 9,471,995,091
tokens (hence roughly 3,000 tokens per example on average).""",9471995091.0,,,,NVIDIA H100 SXM5 80GB,,Confident,"We introduce a new generation of small reasoning models for RAG, search, and source summarization. Pleias-RAG-350m and Pleias-RAG-1B are mid-trained on a large synthetic dataset emulating the retrieval of a wide variety of multilingual open sources from the Common Corpus. They provide native support for citation and grounding with literal quotes and reintegrate multiple features associated with RAG workflows, such as query routing, query reformulation, and source reranking. Pleias-RAG-350m and Pleias-RAG-1B outperform SLMs below 4 billion parameters on standardized RAG benchmarks (HotPotQA, 2wiki) and are competitive with popular larger models, including Qwen-2.5-7B, Llama-3.1-8B, and Gemma-3-4B. They are the only SLMs to date maintaining consistent RAG performance across leading European languages and ensuring systematic reference grounding for statements. Due to their size and ease of deployment on constrained infrastructure and higher factuality by design, the models unlock a range of new use cases for generative AI.",2.0,,Open weights (unrestricted),France,Pleias 1.0 1.2B,136396730000000000000,6 FLOP / parameter / token * 1.2 * 10^9 parameters * 9471995091 tokens * 2 epochs = 1.3639673e+20 FLOP,16.0,,2025-05-12 13:53,,,,,,Industry,,,,,Unreleased,"Apache 2.0

https://huggingface.co/PleIAs/Pleias-RAG-1B",Industry,,,,21956.94597329117,Operation counting,PleIAs,,,
HiDream-I1,Image generation,"Image generation,Text-to-image",HiDream,,2025-04-25,HiDream-I1 is a new open-source image generative foundation model with 17B parameters that achieves state-of-the-art image generation quality within seconds.,https://huggingface.co/HiDream-ai/HiDream-I1-Full,,,,18000000000.0,18B,,,Unspecified unreleased,,,,,,,,Confident,"Key Features
âœ¨ Superior Image Quality - Produces exceptional results across multiple styles including photorealistic, cartoon, artistic, and more. Achieves state-of-the-art HPS v2.1 score, which aligns with human preferences.
ðŸŽ¯ Best-in-Class Prompt Following - Achieves industry-leading scores on GenEval and DPG benchmarks, outperforming all other open-source models.
ðŸ”“ Open Source - Released under the MIT license to foster scientific advancement and enable creative innovation.
ðŸ’¼ Commercial-Friendly - Generated images can be freely used for personal projects, scientific research, and commercial applications.",,,Open weights (unrestricted),China,,,,,,2025-05-22 10:54,,,,,,Industry,,,,,,"MIT license

https://huggingface.co/HiDream-ai/HiDream-I1-Full",Industry,,,,,,HiDream-ai,,,
Ï€0.5 (pi-0.5),"Robotics,Vision",Robotic manipulation,Physical Intelligence,"Kevin Black, Noah Brown, James Darpinian, Karan Dhabalia, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Manuel Y. Galliker, Dibya Ghosh, Lachy Groom, Karol Hausman, Brian Ichter, Szymon Jakubczak, Tim Jones, Liyiming Ke, Devin LeBlanc, Sergey Levine, Adrian Li-Bell, Mohith Mothukuri, Suraj Nair, Karl Pertsch, Allen Z. Ren, Lucy Xiaoyang Shi, Laura Smith, Jost Tobias Springenberg, Kyle Stachowicz, James Tanner, Quan Vuong, Homer Walke, Anna Walling, Haohuan Wang, Lili Yu, Ury Zhilinsky",2025-04-22,$Ï€_{0.5}$: a Vision-Language-Action Model with Open-World Generalization,https://arxiv.org/abs/2504.16054,21.0,,out-performs Ï€0,3300000000.0,"Fig. 3: Model overview
pre-training:
- PaliGemma (3B): SigLIP (400M) + Gemma (2.6B)
- FAST tokenizer
post-training:
- pretrained VLM (3B)
- action expert (300M)
inference: same as post-training

this is in line with Ï€0's 3.3B parameters
",,"pre-training: ""After pre-training the model with discrete tokens for 280k gradient steps...""
post-training: ""We optimize the objective in Equation (1), with Î± = 10.0 for 80k additional steps.""
","Open X-Embodiment,COCO,Cambrian-7M,PixMo,VQAv2,Unspecified unreleased","pre-training:
- mobile manipulator (MM)
- multi-environment (ME)
- cross-embodiment (CE): ""OXE"" -> Open X-Embodiment
- high-level subtask prediction (HL)
- multi-modal web data (WD): CapsFusion, COCO, Cambrian-7M, PixMo, VQAv2

post-training:
""The post-training action dataset consists of the MM and ME robot data, filtered down to successful episodes that are below a fixed length threshold. We include web data (WD) to preserve the modelâ€™s semantic and visual capabilities, and the slice of HL data corresponding to the multi-environment datasets. Additionally, to improve the modelâ€™s ability to predict appropriate high-level subtasks, we collect verbal instruction demonstrations (VI), which are constructed by expert users providing â€œlanguage demonstrations,â€ selecting appropriate sub-task commands to command the robot to perform mobile manipulation tasks step by step. These examples are collected by â€œteleoperatingâ€ the robot in real time with language to perform tasks with the learned low level policy, essentially providing demonstrations of good high-level subtask outputs for a trained policy.""",,,,,,,Confident,"In order for robots to be useful, they must perform practically relevant tasks in the real world, outside of the lab. While vision-language-action (VLA) models have demonstrated impressive results for end-to-end robot control, it remains an open question how far such models can generalize in the wild. We describe $\pi_{0.5}$, a new model based on $\pi_{0}$ that uses co-training on heterogeneous tasks to enable broad generalization. $\pi_{0.5}$\ uses data from multiple robots, high-level semantic prediction, web data, and other sources to enable broadly generalizable real-world robotic manipulation. Our system uses a combination of co-training and hybrid multi-modal examples that combine image observations, language commands, object detections, semantic subtask prediction, and low-level actions. Our experiments show that this kind of knowledge transfer is essential for effective generalization, and we demonstrate for the first time that an end-to-end learning-enabled robotic system can perform long-horizon and dexterous manipulation skills, such as cleaning a kitchen or bedroom, in entirely new homes.",,,Unreleased,United States of America,PaliGemma,,,,,2025-05-30 12:28,,,,,,Industry,,,,,,,Industry,,,,,,,,,
Eagle 2.5,"Vision,Robotics,Language",,"NVIDIA,Nanjing University,Hong Kong Polytechnic University,Rutgers University","Guo Chen, Zhiqi Li, Shihao Wang, Jindong Jiang, Yicheng Liu, Lidong Lu, De-An Huang, Wonmin Byeon, Matthieu Le, Tuomas Rintamaki, Tyler Poon, Max Ehrlich, Tuomas Rintamaki, Tyler Poon, Tong Lu, Limin Wang, Bryan Catanzaro, Jan Kautz, Andrew Tao, Zhiding Yu, Guilin Liu",2025-04-21,Eagle 2.5: Boosting Long-Context Post-Training for Frontier Vision-Language Models,"https://arxiv.org/abs/2504.15271
https://nvlabs.github.io/EAGLE/
https://github.com/NVlabs/EAGLE",,,NVIDIA may have been selective in choosing rival models,8000000000.0,"Table 8. ""Full Model, 8B""
Could be Qwen2.5-7B + MLP Connector (40M)

https://nvlabs.github.io/EAGLE/
as of 2025-05-29: ""Eagle-2.5 Weights (Coming Soon)"" ",,,,"Table 8: The proposed progressive training settings.
Eagle2.5-Stage-1: ALLaVA (1.2M samples)
Eagle2.5-Stage-1.5: Rich Diverse Data (21.6M samples)
Eagle2.5-Stage-2: Short + Long Data (9.2M = 4.6M + 4.6M samples)
Eagle2.5-Stage-3: Short + Long Data (9.2M = 4.6M + 4.6M samples)
Eagle2.5-Stage-4: Short + Long Data (9.2M = 4.6M + 4.6M samples)",,,,,,,Confident,"We introduce Eagle 2.5, a family of frontier vision-language models (VLMs) for long-context multimodal learning. Our work addresses the challenges in long video comprehension and high-resolution image understanding, introducing a generalist framework for both tasks. The proposed training framework incorporates Automatic Degrade Sampling and Image Area Preservation, two techniques that preserve contextual integrity and visual details. The framework also includes numerous efficiency optimizations in the pipeline for long-context data training. Finally, we propose Eagle-Video-110K, a novel dataset that integrates both story-level and clip-level annotations, facilitating long-video understanding. Eagle 2.5 demonstrates substantial improvements on long-context multimodal benchmarks, providing a robust solution to the limitations of existing VLMs. Notably, our best model Eagle 2.5-8B achieves 72.4% on Video-MME with 512 input frames, matching the results of top-tier commercial model such as GPT-4o and large-scale open-source models like Qwen2.5-VL-72B and InternVL2.5-78B.",,,,"United States of America,China,Hong Kong,China,United States of America",Qwen2.5-7B,,,,,2025-05-30 12:29,,,,,,"Industry,Academia,Academia,Academia",,,,,,,"Industry,Academia,Academia,Academia",,,,,,,,,
Gemini 2.5 Flash,"Language,Multimodal,Vision,Speech,Video","Language modeling/generation,Question answering,Code generation,Quantitative reasoning,Visual question answering,Translation,Image captioning,Speech recognition,Video description,Search,Text summarization,Chat",Google DeepMind,,2025-04-17,Our powerful and most efficient workhorse model designed for speed and low-cost.,https://deepmind.google/models/gemini/flash/,,,,,,,,Unspecified unreleased,Knowledge cutoff January 2025,,,,,,,Unknown,"Speed and value at scale
Ideal for tasks like summarization, chat applications, data extraction, and captioning.
Thinking budget: Control how much 2.5 Flash reasons to balance latency and cost.
Natively multimodal: Understands input across text, audio, images and video.
Long context: Explore vast datasets with a 1-million token context window.
Adaptive and budgeted thinking
Adaptive controls and adjustable thinking budgets allow you to balance performance and cost.
Calibrated: The model explores diverse thinking strategies, leading to more accurate and relevant outputs.
Controllable: Developers have fine-grained control over the model's thinking process, allowing them to manage resource usage.
Adaptive: When no thinking budget is set, the model assesses the complexity of a task and calibrates the amount of thinking accordingly.",,,API access,"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France",,,,,,2025-06-12 15:33,,,,,,Industry,,,,,Unreleased,"Availability	
Google AI Studio
Gemini API
Gemini App",Industry,,,,,,,,,
o4-mini,"Multimodal,Language,Vision","Language modeling/generation,Search,Question answering,Quantitative reasoning,Chat,Translation,Code generation,Visual question answering,Instruction interpretation,Visual puzzles",OpenAI,,2025-04-16,Introducing OpenAI o3 and o4-mini: Our smartest and most capable models to date with full tool access,https://openai.com/index/introducing-o3-and-o4-mini/,,,,,,,,Unspecified unreleased,"""The OpenAI o-series models are trained with large-scale reinforcement learning on chains of thought.""

""OpenAI o3 and o4-mini were trained on diverse datasets,
including information that is publicly available on the internet, information that we partner with third parties to access, and information that our users or human trainers and researchers provide or generate. Our data processing pipeline includes rigorous filtering to maintain data quality and mitigate potential risks. We use advanced data filtering processes to reduce personal information
from training data. We also employ a combination of our Moderation API and safety classifiers to help prevent the use of harmful or sensitive content, including explicit materials such as sexual content involving a minor.""
This has a knowledge cutoff date of May 31, 2024, according to https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models?tabs=global-standard%2Cstandard-chat-completions#o-series-models.",,,,,,,Unknown,"Today, weâ€™re releasing OpenAI o3 and o4-mini, the latest in our o-series of models trained to think for longer before responding. These are the smartest models weâ€™ve released to date, representing a step change in ChatGPT's capabilities for everyone from curious users to advanced researchers. For the first time, our reasoning models can agentically use and combine every tool within ChatGPTâ€”this includes searching the web, analyzing uploaded files and other data with Python, reasoning deeply about visual inputs, and even generating images. Critically, these models are trained to reason about when and how to use tools to produce detailed and thoughtful answers in the right output formats, typically in under a minute, to solve more complex problems. This allows them to tackle multi-faceted questions more effectively, a step toward a more agentic ChatGPT that can independently execute tasks on your behalf. The combined power of state-of-the-art reasoning with full tool access translates into significantly stronger performance across academic benchmarks and real-world tasks, setting a new standard in both intelligence and usefulness.
<..>
OpenAI o4-mini is a smaller model optimized for fast, cost-efficient reasoningâ€”it achieves remarkable performance for its size and cost, particularly in math, coding, and visual tasks. It is the best-performing benchmarked model on AIME 2024 and 2025. Although access to a computer meaningfully reduces the difficulty of the AIME exam, we also found it notable that o4-mini achieves 99.5% pass@1 (100% consensus@8) on AIME 2025 when given access to a Python interpreter. While these results should not be compared to the performance of models without tool access, they are one example of how effectively o4-mini leverages available tools; o3 shows similar improvements on AIME 2025 from tool use (98.4% pass@1, 100% consensus@8).",,,API access,United States of America,,,,,,2025-06-06 13:58,,,,,,Industry,,,,,Unreleased,"""Both o3 and o4-mini are also available to developers today via the Chat Completions API and Responses API (some developers will need to verify their organizationsâ (opens in a new window) to access these models)""",Industry,,,,,,,,,
Nova-3,Speech,Speech recognition,Deepgram,,2025-04-15,Introducing Nova-3: Setting a New Standard for AI-Driven Speech-to-Text,https://deepgram.com/learn/introducing-nova-3-speech-to-text-api,,,,,,,,Unspecified unreleased,,,,,,,,Unknown,"Nova-3 advances Deepgram's industry-leading accuracy, extending its capabilities to a broader range of real-world enterprise use cases and challenging audio conditions.

Nova-3 is the first voice AI model to offer real-time multilingual transcription. 

Our latest model is also the first to provide users with demonstrably effective and highly accurate self-serve customizationâ€”enabling instant vocabulary adaptation without model retraining.

Superior accuracy: Nova-3 delivers industry-leading performance with a 54.3% reduction in word error rate (WER) for streaming and 47.4% for batch processing compared to competitors.

Preferred for multilingual support: Deepgram was preferred over Whisper on 7 out of 7 languages testedâ€”reaching as high as an 8-to-1 preference on certain languages.",,,API access,United States of America,,,,,,2025-05-12 15:06,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
Kling 2.0 Video Generation,"Video,Vision","Video generation,Image-to-video,Video-to-video,Text-to-video",Kuaishou Technology,,2025-04-15,"Kling AI Advances to the 2.0 Era, Empowering Everyone to Tell Great Stories with AI",https://ir.kuaishou.com/news-releases/news-release-details/kling-ai-advances-20-era-empowering-everyone-tell-great-stories,,,,,,,,Unspecified unreleased,,,,,,,,Unknown,"As the worldâ€™s first user-accessible DiT video generation model, in the 10 months since its initial launch in June of last year, its global user base has surpassed 22 million. On March 27, Artificial Analysis, a globally renowned AI benchmarking organization, released the latest global rankings for video generation large models. Kuaishou Kling 1.6 Pro (high-quality mode) topped the Image to Video category with an Arena ELO benchmark score of 1,000, while Google Veo 2 and Pika Art ranked second and third, respectively.

In this 2.0 model iteration, Kling AI officially introduces multi-modal visual language (MVL), a new interactive concept for AI video generation. This feature allows users to integrate multimodal inputs, such as image references and video clips, enabling them to convey complex creative ideas effectively and directly to AI, covering aspects such as identity, appearance, style, scenarios, actions, expressions, camera movements, and other elements.",,,API access,China,,,,,,2025-05-19 12:02,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
Kolors 2.0 Image Generation,Image generation,"Image generation,Text-to-image",Kuaishou Technology,,2025-04-15,"Kling AI Advances to the 2.0 Era, Empowering Everyone to Tell Great Stories with AI",https://ir.kuaishou.com/news-releases/news-release-details/kling-ai-advances-20-era-empowering-everyone-tell-great-stories,,,,,,,,Unspecified unreleased,,,,,,,,Unknown,"In the realm of large image generation models, Kuaishou Kolors leads the industry with several core advantages, such as powerful complex semantic understanding, movie-level visual quality, and controllable stylized generation under multiple conditions. In a number of internal team win-loss reviews, it maintains a significant advantage over industry-leading image models such as Midjourney V7, FLUX 1.1 Pro, and Reve.",,,API access,China,,,,,,2025-05-14 11:13,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
GPT-4.1,"Multimodal,Language,Vision,Video","Language modeling/generation,Code generation,Question answering,Quantitative reasoning,Instruction interpretation,System control,Visual question answering,Video description",OpenAI,"Research leads
Ananya Kumar, Jiahui Yu, John Hallman, Michelle Pokrass

Research core contributors
Adam Goucher, Adi Ganesh, Bowen Cheng, Brandon McKinzie, Brian Zhang, Chris Koch, Colin Wei, David Medina, Edmund Wong, Erin Kavanaugh, Florent Bekerman, Haitang Hu, Hongyu Ren, Ishaan Singal, Jamie Kiros, Jason Ai, Ji Lin, Jonathan Chien, Josh McGrath, Julian Lee, Julie Wang, Kevin Lu, Kristian Georgiev, Kyle Luther, Li Jing, Max Schwarzer, Miguel Castro, Nitish Keskar, Rapha Gontijo Lopes, Shengjia Zhao, Sully Chen, Suvansh Sanjeev, Taylor Gordon, Ted Sanders, Wenda Zhou, Yang Song, Yujia Xie, Yujia Jin, Zhishuai Zhang",2025-04-14,"Introducing GPT-4.1 in the API: A new series of GPT models featuring major improvements on coding, instruction following, and long contextâ€”plus our first-ever nano model.",https://openai.com/index/gpt-4-1/,,,,,,,,Unspecified unreleased,"This has a knowledge cutoff date of May 31, 2024, according to https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models?tabs=global-standard%2Cstandard-chat-completions#o-series-models.",,,,,,,Unknown,"Today, weâ€™re launching three new models in the API: GPTâ€‘4.1, GPTâ€‘4.1 mini, and GPTâ€‘4.1 nano. These models outperform GPTâ€‘4o and GPTâ€‘4o mini across the board, with major gains in coding and instruction following. They also have larger context windowsâ€”supporting up to 1 million tokens of contextâ€”and are able to better use that context with improved long-context comprehension. They feature a refreshed knowledge cutoff of June 2024.

GPTâ€‘4.1 excels at the following industry standard measures: 

Coding: GPTâ€‘4.1 scores 54.6% on SWE-bench Verified, improving by 21.4%abs over GPTâ€‘4o and 26.6%abs over GPTâ€‘4.5â€”making it a leading model for coding.
Instruction following: On Scaleâ€™s MultiChallengeâ (opens in a new window) benchmark, a measure of instruction following ability, GPTâ€‘4.1 scores 38.3%, a 10.5%abs increase over GPTâ€‘4o.
Long context: On Video-MMEâ (opens in a new window), a benchmark for multimodal long context understanding, GPTâ€‘4.1 sets a new state-of-the-art resultâ€”scoring 72.0% on the long, no subtitles category, a 6.7%abs improvement over GPTâ€‘4o.",,,API access,United States of America,,,,,,2025-05-30 14:50,,,,,,Industry,,,,,Unreleased,"""GPTâ€‘4.1 will only be available via the API""",Industry,,,,,,,,,
GPT-4.1 mini,"Multimodal,Language,Vision,Video","Language modeling/generation,Code generation,Question answering,Quantitative reasoning,Instruction interpretation,System control,Visual question answering,Video description",OpenAI,"Research leads
Ananya Kumar, Jiahui Yu, John Hallman, Michelle Pokrass

Research core contributors
Adam Goucher, Adi Ganesh, Bowen Cheng, Brandon McKinzie, Brian Zhang, Chris Koch, Colin Wei, David Medina, Edmund Wong, Erin Kavanaugh, Florent Bekerman, Haitang Hu, Hongyu Ren, Ishaan Singal, Jamie Kiros, Jason Ai, Ji Lin, Jonathan Chien, Josh McGrath, Julian Lee, Julie Wang, Kevin Lu, Kristian Georgiev, Kyle Luther, Li Jing, Max Schwarzer, Miguel Castro, Nitish Keskar, Rapha Gontijo Lopes, Shengjia Zhao, Sully Chen, Suvansh Sanjeev, Taylor Gordon, Ted Sanders, Wenda Zhou, Yang Song, Yujia Xie, Yujia Jin, Zhishuai Zhang",2025-04-14,"Introducing GPT-4.1 in the API: A new series of GPT models featuring major improvements on coding, instruction following, and long contextâ€”plus our first-ever nano model.",https://openai.com/index/gpt-4-1/,,,,,,,,Unspecified unreleased,"This has a knowledge cutoff date of May 31, 2024, according to https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models?tabs=global-standard%2Cstandard-chat-completions#o-series-models.",,,,,,,Unknown,"GPTâ€‘4.1 mini is a significant leap in small model performance, even beating GPTâ€‘4o in many benchmarks. It matches or exceeds GPTâ€‘4o in intelligence evals while reducing latency by nearly half and reducing cost by 83%. ",,,API access,United States of America,,,,,,2025-05-30 14:50,,,,,,Industry,,,,,Unreleased,"""GPTâ€‘4.1 will only be available via the API""",Industry,,,,,,,,,
GPT-4.1 nano,"Multimodal,Language,Vision,Video","Language modeling/generation,Code generation,Question answering,Quantitative reasoning,Instruction interpretation,System control,Visual question answering,Video description",OpenAI,"Research leads
Ananya Kumar, Jiahui Yu, John Hallman, Michelle Pokrass

Research core contributors
Adam Goucher, Adi Ganesh, Bowen Cheng, Brandon McKinzie, Brian Zhang, Chris Koch, Colin Wei, David Medina, Edmund Wong, Erin Kavanaugh, Florent Bekerman, Haitang Hu, Hongyu Ren, Ishaan Singal, Jamie Kiros, Jason Ai, Ji Lin, Jonathan Chien, Josh McGrath, Julian Lee, Julie Wang, Kevin Lu, Kristian Georgiev, Kyle Luther, Li Jing, Max Schwarzer, Miguel Castro, Nitish Keskar, Rapha Gontijo Lopes, Shengjia Zhao, Sully Chen, Suvansh Sanjeev, Taylor Gordon, Ted Sanders, Wenda Zhou, Yang Song, Yujia Xie, Yujia Jin, Zhishuai Zhang",2025-04-14,"Introducing GPT-4.1 in the API: A new series of GPT models featuring major improvements on coding, instruction following, and long contextâ€”plus our first-ever nano model.",https://openai.com/index/gpt-4-1/,,,,,,,,Unspecified unreleased,"This has a knowledge cutoff date of May 31, 2024, according to https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models?tabs=global-standard%2Cstandard-chat-completions#o-series-models.",,,,,,,Unknown,"For tasks that demand low latency, GPTâ€‘4.1 nano is our fastest and cheapest model available. It delivers exceptional performance at a small size with its 1 million token context window, and scores 80.1% on MMLU, 50.3% on GPQA, and 9.8% on Aider polyglot codingâ€”even higher than GPTâ€‘4o mini. Itâ€™s ideal for tasks like classification or autocompletion.",,,API access,United States of America,,,,,,2025-05-30 14:50,,,,,,Industry,,,,,Unreleased,"""GPTâ€‘4.1 will only be available via the API""",Industry,,,,,,,,,
Nemotron-H 8B,Language,"Language modeling/generation,Question answering,Translation,Quantitative reasoning,Code generation",NVIDIA,"NVIDIA: Aaron Blakeman, Aarti Basant, Abhinav Khattar, Adithya Renduchintala, Akhiad Bercovich, Aleksander Ficek, Alexis Bjorlin, Ali Taghibakhshi, Amala Sanjay Deshmukh, Ameya Sunil Mahabaleshwarkar, Andrew Tao, Anna Shors, Ashwath Aithal, Ashwin Poojary, Ayush Dattagupta, Balaram Buddharaju, Bobby Chen, Boris Ginsburg, Boxin Wang, Brandon Norick, Brian Butterfield, Bryan Catanzaro, Carlo del Mundo, Chengyu Dong, Christine Harvey, Christopher Parisien, Dan Su, Daniel Korzekwa, Danny Yin, Daria Gitman, David Mosallanezhad, Deepak Narayanan, Denys Fridman, Dima Rekesh, Ding Ma, Dmytro Pykhtar, Dong Ahn, Duncan Riach, Dusan Stosic, Eileen Long, Elad Segal, Ellie Evans, Eric Chung, Erick Galinkin, Evelina Bakhturina, Ewa Dobrowolska, Fei Jia, Fuxiao Liu, Gargi Prasad, Gerald Shen, Guilin Liu, Guo Chen, Haifeng Qian, Helen Ngo, Hongbin Liu, Hui Li, Igor Gitman, Ilia Karmanov, Ivan Moshkov, Izik Golan, Jan Kautz, Jane Polak Scowcroft, Jared Casper, Jarno Seppanen, Jason Lu, Jason Sewall, Jiaqi Zeng, Jiaxuan You, Jimmy Zhang, Jing Zhang, Jining Huang, Jinze Xue, Jocelyn Huang, Joey Conway, John Kamalu, Jon Barker, Jonathan Cohen, Joseph Jennings, Jupinder Parmar, Karan Sapra, Kari Briski, Kateryna Chumachenko, Katherine Luna, Keshav Santhanam, Kezhi Kong, Kirthi Sivamani, Krzysztof Pawelec, Kumar Anik, Kunlun Li, Lawrence McAfee, Leon Derczynski, Lindsey Pavao, Luis Vega, Lukas Voegtle, Maciej Bala, Maer Rodrigues de Melo, Makesh Narsimhan Sreedhar, Marcin Chochowski, Markus Kliegl et al. (100 additional authors not shown)",2025-04-14,Nemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer Models,https://arxiv.org/abs/2504.03624,,,,8000000000.0,"Model Architecture
Architecture Type: Hybrid Mamba-Transformer
Network Architecture: Nemotron-H
This model has 8B model parameters.",7.2e+23,6 FLOP / parameter / token * 8 * 10^9 parameters * 15 * 10^12 tokens = 7.2e+23 FLOP,"Common Crawl,Unspecified unreleased","""The training corpus for Nemotron-H-8B-Base-8K consists of English and multilingual text (German, Spanish, French, Italian, Korean, Portuguese, Russian, Japanese, Chinese and English), as well as code. Our sources cover a variety of document types such as: webpages, dialogue, articles, and other written materials. This model was also improved using synthetic data from Qwen (Built with Qwen). The corpus spans domains including legal, math, science, finance, and more. We also include a small portion of question-answering, and alignment style data to improve model accuracy.""",15000000000000.0,"""We trained Nemotron-H-8B-Base on a token horizon of 15 trillion tokens""",,,,,Confident,"As inference-time scaling becomes critical for enhanced reasoning capabilities, it is increasingly becoming important to build models that are efficient to infer. We introduce Nemotron-H, a family of 8B and 56B/47B hybrid Mamba-Transformer models designed to reduce inference cost for a given accuracy level. To achieve this goal, we replace the majority of self-attention layers in the common Transformer model architecture with Mamba layers that perform constant computation and require constant memory per generated token. We show that Nemotron-H models offer either better or on-par accuracy compared to other similarly-sized state-of-the-art open-sourced Transformer models (e.g., Qwen-2.5-7B/72B and Llama-3.1-8B/70B), while being up to 3Ã— faster at inference. To further increase inference speed and reduce the memory required at inference time, we created Nemotron-H-47B-Base from the 56B model using a new compression via pruning and distillation technique called MiniPuzzle. Nemotron-H-47B-Base achieves similar accuracy to the 56B model, but is 20% faster to infer. In addition, we introduce an FP8-based training recipe and show that it can achieve on par results with BF16-based training. This recipe is used to train the 56B model. We are releasing Nemotron-H base model checkpoints with support in Hugging Face and NeMo.",1.0,,Open weights (non-commercial),United States of America,,,,,,2025-05-16 10:30,,,,6291456.0,"""We used a sequence length of 8192 and global batch size of 768 (6291456 tokens per batch).""",Industry,,,,,Unreleased,"https://huggingface.co/nvidia/Nemotron-H-8B-Base-8K
nvidia-internal-scientific-research-and-development-model-license",Industry,,,FP8,,Operation counting,nvidia,,,
Nemotron-H 47B,Language,"Language modeling/generation,Question answering,Translation,Quantitative reasoning,Code generation,Neural Architecture Search - NAS",NVIDIA,"NVIDIA: Aaron Blakeman, Aarti Basant, Abhinav Khattar, Adithya Renduchintala, Akhiad Bercovich, Aleksander Ficek, Alexis Bjorlin, Ali Taghibakhshi, Amala Sanjay Deshmukh, Ameya Sunil Mahabaleshwarkar, Andrew Tao, Anna Shors, Ashwath Aithal, Ashwin Poojary, Ayush Dattagupta, Balaram Buddharaju, Bobby Chen, Boris Ginsburg, Boxin Wang, Brandon Norick, Brian Butterfield, Bryan Catanzaro, Carlo del Mundo, Chengyu Dong, Christine Harvey, Christopher Parisien, Dan Su, Daniel Korzekwa, Danny Yin, Daria Gitman, David Mosallanezhad, Deepak Narayanan, Denys Fridman, Dima Rekesh, Ding Ma, Dmytro Pykhtar, Dong Ahn, Duncan Riach, Dusan Stosic, Eileen Long, Elad Segal, Ellie Evans, Eric Chung, Erick Galinkin, Evelina Bakhturina, Ewa Dobrowolska, Fei Jia, Fuxiao Liu, Gargi Prasad, Gerald Shen, Guilin Liu, Guo Chen, Haifeng Qian, Helen Ngo, Hongbin Liu, Hui Li, Igor Gitman, Ilia Karmanov, Ivan Moshkov, Izik Golan, Jan Kautz, Jane Polak Scowcroft, Jared Casper, Jarno Seppanen, Jason Lu, Jason Sewall, Jiaqi Zeng, Jiaxuan You, Jimmy Zhang, Jing Zhang, Jining Huang, Jinze Xue, Jocelyn Huang, Joey Conway, John Kamalu, Jon Barker, Jonathan Cohen, Joseph Jennings, Jupinder Parmar, Karan Sapra, Kari Briski, Kateryna Chumachenko, Katherine Luna, Keshav Santhanam, Kezhi Kong, Kirthi Sivamani, Krzysztof Pawelec, Kumar Anik, Kunlun Li, Lawrence McAfee, Leon Derczynski, Lindsey Pavao, Luis Vega, Lukas Voegtle, Maciej Bala, Maer Rodrigues de Melo, Makesh Narsimhan Sreedhar, Marcin Chochowski, Markus Kliegl et al. (100 additional authors not shown)",2025-04-14,Nemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer Models,https://arxiv.org/abs/2504.03624,,,,47000000000.0,"""We use MiniPuzzle to distill Nemotron-H-56B-Base to Nemotron-H-47B-Base, using only 63 billion training tokens and FP8 training""",,,Unspecified unreleased,"""The training corpus for Nemotron-H-47B-Base-8K consists of English and multilingual text (German, Spanish, French, Italian, Korean, Portuguese, Russian, Japanese, Chinese and English), as well as code. Our sources cover a variety of document types such as: webpages, dialogue, articles, and other written materials. This model was also improved using synthetic data from Qwen (Built with Qwen). The corpus spans domains including legal, math, science, finance, and more. We also include a small portion of question-answering, and alignment style data to improve model accuracies.""",63000000000.0,"""We use MiniPuzzle to distill Nemotron-H-56B-Base to Nemotron-H-47B-Base,
using only 63 billion training tokens and FP8 training""",,,,,Confident,"As inference-time scaling becomes critical for enhanced reasoning capabilities, it is increasingly becoming important to build models that are efficient to infer. We introduce Nemotron-H, a family of 8B and 56B/47B hybrid Mamba-Transformer models designed to reduce inference cost for a given accuracy level. To achieve this goal, we replace the majority of self-attention layers in the common Transformer model architecture with Mamba layers that perform constant computation and require constant memory per generated token. We show that Nemotron-H models offer either better or on-par accuracy compared to other similarly-sized state-of-the-art open-sourced Transformer models (e.g., Qwen-2.5-7B/72B and Llama-3.1-8B/70B), while being up to 3Ã— faster at inference. To further increase inference speed and reduce the memory required at inference time, we created Nemotron-H-47B-Base from the 56B model using a new compression via pruning and distillation technique called MiniPuzzle. Nemotron-H-47B-Base achieves similar accuracy to the 56B model, but is 20% faster to infer. In addition, we introduce an FP8-based training recipe and show that it can achieve on par results with BF16-based training. This recipe is used to train the 56B model. We are releasing Nemotron-H base model checkpoints with support in Hugging Face and NeMo.",1.0,,Open weights (non-commercial),United States of America,Nemotron-H 56B,1,6 FLOP / parameter / token * 47*10^9 parameters * 63*10^9 tokens = 1.7766e+22 FLOP,,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,"https://huggingface.co/nvidia/Nemotron-H-47B-Base-8K
nvidia-internal-scientific-research-and-development-model-license",Industry,,,FP8,,Operation counting,nvidia,,,
Nemotron-H 56B,Language,"Language modeling/generation,Question answering,Translation,Quantitative reasoning,Code generation",NVIDIA,"NVIDIA: Aaron Blakeman, Aarti Basant, Abhinav Khattar, Adithya Renduchintala, Akhiad Bercovich, Aleksander Ficek, Alexis Bjorlin, Ali Taghibakhshi, Amala Sanjay Deshmukh, Ameya Sunil Mahabaleshwarkar, Andrew Tao, Anna Shors, Ashwath Aithal, Ashwin Poojary, Ayush Dattagupta, Balaram Buddharaju, Bobby Chen, Boris Ginsburg, Boxin Wang, Brandon Norick, Brian Butterfield, Bryan Catanzaro, Carlo del Mundo, Chengyu Dong, Christine Harvey, Christopher Parisien, Dan Su, Daniel Korzekwa, Danny Yin, Daria Gitman, David Mosallanezhad, Deepak Narayanan, Denys Fridman, Dima Rekesh, Ding Ma, Dmytro Pykhtar, Dong Ahn, Duncan Riach, Dusan Stosic, Eileen Long, Elad Segal, Ellie Evans, Eric Chung, Erick Galinkin, Evelina Bakhturina, Ewa Dobrowolska, Fei Jia, Fuxiao Liu, Gargi Prasad, Gerald Shen, Guilin Liu, Guo Chen, Haifeng Qian, Helen Ngo, Hongbin Liu, Hui Li, Igor Gitman, Ilia Karmanov, Ivan Moshkov, Izik Golan, Jan Kautz, Jane Polak Scowcroft, Jared Casper, Jarno Seppanen, Jason Lu, Jason Sewall, Jiaqi Zeng, Jiaxuan You, Jimmy Zhang, Jing Zhang, Jining Huang, Jinze Xue, Jocelyn Huang, Joey Conway, John Kamalu, Jon Barker, Jonathan Cohen, Joseph Jennings, Jupinder Parmar, Karan Sapra, Kari Briski, Kateryna Chumachenko, Katherine Luna, Keshav Santhanam, Kezhi Kong, Kirthi Sivamani, Krzysztof Pawelec, Kumar Anik, Kunlun Li, Lawrence McAfee, Leon Derczynski, Lindsey Pavao, Luis Vega, Lukas Voegtle, Maciej Bala, Maer Rodrigues de Melo, Makesh Narsimhan Sreedhar, Marcin Chochowski, Markus Kliegl et al. (100 additional authors not shown)",2025-04-14,Nemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer Models,https://arxiv.org/abs/2504.03624,,,,56000000000.0,"Model Architecture
Architecture Type: Hybrid Mamba-Transformer
Network Architecture: Nemotron-H
This model has 56B model parameters.",6.719999999999999e+24,6 FLOP / parameter / token * 56 * 10^9 parameters * 20 * 10^12 tokens = 6.72e+24 FLOP,"Common Crawl,Unspecified unreleased",,20000000000000.0,"""We trained Nemotron-H-8B-Base on a token horizon of 15 trillion tokens and Nemotron-H-56B-Base on a token horizon of 20 trillion tokens. We used a sequence length of 8192 and global batch size of
768 (6291456 tokens per batch).""",,,NVIDIA H100 SXM5 80GB,,Confident,"As inference-time scaling becomes critical for enhanced reasoning capabilities, it is increasingly becoming important to build models that are efficient to infer. We introduce Nemotron-H, a family of 8B and 56B/47B hybrid Mamba-Transformer models designed to reduce inference cost for a given accuracy level. To achieve this goal, we replace the majority of self-attention layers in the common Transformer model architecture with Mamba layers that perform constant computation and require constant memory per generated token. We show that Nemotron-H models offer either better or on-par accuracy compared to other similarly-sized state-of-the-art open-sourced Transformer models (e.g., Qwen-2.5-7B/72B and Llama-3.1-8B/70B), while being up to 3Ã— faster at inference. To further increase inference speed and reduce the memory required at inference time, we created Nemotron-H-47B-Base from the 56B model using a new compression via pruning and distillation technique called MiniPuzzle. Nemotron-H-47B-Base achieves similar accuracy to the 56B model, but is 20% faster to infer. In addition, we introduce an FP8-based training recipe and show that it can achieve on par results with BF16-based training. This recipe is used to train the 56B model. We are releasing Nemotron-H base model checkpoints with support in Hugging Face and NeMo.",1.0,,Open weights (non-commercial),United States of America,,,,6144.0,,2025-05-16 10:30,,,,6291456.0,"""We used a sequence length of 8192 and global batch size of 768 (6291456 tokens per batch).""",Industry,,,,,Unreleased,"https://huggingface.co/nvidia/Nemotron-H-56B-Base-8K
nvidia-internal-scientific-research-and-development-model-license",Industry,,,FP8,8433532.904958995,Operation counting,nvidia,,,
DolphinGemma,Audio,"Audio generation,Speech recognition,Audio classification","Google DeepMind,Georgia Institute of Technology,Wild Dolphin Project","Denise Herzing, Thad Starner",2025-04-14,DolphinGemma: How Google AI is helping decode dolphin communication,https://blog.google/technology/ai/dolphingemma/,,,,400000000.0,400M,,,Unspecified unreleased,"""Trained extensively on WDPâ€™s acoustic database of wild Atlantic spotted dolphins""",,,,,,,Confident,"Enter DolphinGemma. Developed by Google, this AI model makes use of specific Google audio technologies: the SoundStream tokenizer efficiently represents dolphin sounds, which are then processed by a model architecture suited for complex sequences. This ~400M parameter model is optimally-sized to run directly on the Pixel phones WDP uses in the field.

This model builds upon insights from Gemma, Googleâ€™s collection of lightweight, state-of-the-art open models that are built from the same research and technology that powers our Gemini models. Trained extensively on WDPâ€™s acoustic database of wild Atlantic spotted dolphins, DolphinGemma functions as an audio-in, audio-out model, processes sequences of natural dolphin sounds to identify patterns, structure and ultimately predict the likely subsequent sounds in a sequence, much like how large language models for human language predict the next word or token in a sentence.

WDP is beginning to deploy DolphinGemma this field season with immediate potential benefits. By identifying recurring sound patterns, clusters and reliable sequences, the model can help researchers uncover hidden structures and potential meanings within the dolphins' natural communication â€” a task previously requiring immense human effort. Eventually, these patterns, augmented with synthetic sounds created by the researchers to refer to objects with which the dolphins like to play, may establish a shared vocabulary with the dolphins for interactive communication.",,,Unreleased,"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France,United States of America,United States of America",,,,,,2025-05-01 10:42,,,,,,"Industry,Academia,Research collective",,,,,Unreleased,"""Recognizing the value of collaboration in scientific discovery, weâ€™re planning to share DolphinGemma as an open model this summer. "" [2025]","Industry,Academia,Research collective",,,,,,,,,
GLM-Z1-Rumination-32B-0414,Language,"Language modeling/generation,Question answering,Quantitative reasoning,Code generation",Tsinghua University,,2025-04-14,GLM-4-Z1-Rumination-32B-0414,https://huggingface.co/THUDM/GLM-Z1-Rumination-32B-0414,,,,32000000000.0,32B,2.88e+24,6 FLOP / parameter / token * 32 * 10^9 parameters * 15 * 10^12 tokens = 2.88e+24 FLOP,Unspecified unreleased,"""GLM-4-32B-Base-0414 was pre-trained on 15T of high-quality data, including substantial reasoning-type synthetic data. This lays the foundation for subsequent reinforcement learning extensions. In the post-training stage, we employed human preference alignment for dialogue scenarios. ""

""Z1-Rumination is trained through scaling end-to-end reinforcement learning with responses graded by the ground truth answers or rubrics and can make use of search tools during its deep thinking process to handle complex tasks.""",15000000000000.0,15T (base model) + RL,,,,,Confident,"The GLM family welcomes a new generation of open-source models, the GLM-4-32B-0414 series, featuring 32 billion parameters. Its performance is comparable to OpenAI's GPT series and DeepSeek's V3/R1 series, and it supports very user-friendly local deployment features. GLM-4-32B-Base-0414 was pre-trained on 15T of high-quality data, including a large amount of reasoning-type synthetic data, laying the foundation for subsequent reinforcement learning extensions. In the post-training stage, in addition to human preference alignment for dialogue scenarios, we also enhanced the model's performance in instruction following, engineering code, and function calling using techniques such as rejection sampling and reinforcement learning, strengthening the atomic capabilities required for agent tasks. GLM-4-32B-0414 achieves good results in areas such as engineering code, Artifact generation, function calling, search-based Q&A, and report generation. Some benchmarks even rival larger models like GPT-4o and DeepSeek-V3-0324 (671B).

GLM-Z1-Rumination-32B-0414 is a deep reasoning model with rumination capabilities (benchmarked against OpenAI's Deep Research). Unlike typical deep thinking models, the rumination model employs longer periods of deep thought to solve more open-ended and complex problems (e.g., writing a comparative analysis of AI development in two cities and their future development plans). The rumination model integrates search tools during its deep thinking process to handle complex tasks and is trained by utilizing multiple rule-based rewards to guide and extend end-to-end reinforcement learning. Z1-Rumination shows significant improvements in research-style writing and complex retrieval tasks.",,,Open weights (unrestricted),China,,,,,,2025-05-16 10:30,,,,,,Academia,,,,,Unreleased,"MIT license

https://huggingface.co/THUDM/GLM-Z1-Rumination-32B-0414",Academia,,,,,Operation counting,THUDM,,,
GLM-4-9B-0414,Language,"Language modeling/generation,Question answering,Quantitative reasoning,Code generation",Tsinghua University,,2025-04-14,GLM-4-9B-0414,https://huggingface.co/THUDM/GLM-4-9B-0414,,,,9000000000.0,9B,8.1e+23,"Assuming it was trained on the same 15T dataset as 32B model:

6 FLOP / parameter / token * 9 * 10^9 parameters * 15 * 10^12 tokens = 8.1e+23 FLOP 

""Likely"" confidence due to the uncertain dataset size",Unspecified unreleased,,,,,,,,Likely,"Finally, GLM-Z1-9B-0414 is a surprise. We employed all the aforementioned techniques to train a small model (9B). GLM-Z1-9B-0414 exhibits excellent capabilities in mathematical reasoning and general tasks. Its overall performance is top-ranked among all open-source models of the same size. Especially in resource-constrained scenarios, this model achieves an excellent balance between efficiency and effectiveness, providing a powerful option for users seeking lightweight deployment.",,,Open weights (unrestricted),China,,,,,,2025-05-16 10:30,,,,,,Academia,,,,,Unreleased,"MIT license

https://huggingface.co/THUDM/GLM-4-9B-0414",Academia,,,,,Operation counting,THUDM,,,
SenseNova V6,"Multimodal,Language,Video,Vision","Language modeling/generation,Quantitative reasoning,Question answering,Visual question answering,Video description,Character recognition,Code generation,Chat",SenseTime,PRNewswire,2025-04-12,SenseTime's SenseNova V6: China's Most Advanced Multimodal Model with the Lowest Cost in the Industry,"https://www.prnewswire.com/apac/news-releases/sensetimes-sensenova-v6-chinas-most-advanced-multimodal-model-with-the-lowest-cost-in-the-industry-302426998.html
https://platform.sensenova.cn/technology/fusion/Pro/",,,,600000000000.0,"""Mixture of Experts (MoE)-based multimodal general foundation model with over 600 billion parameters,""",,,,,,,,,,,Confident,"HONG KONG, April 12, 2025 /PRNewswire/ -- SenseTime launched its newly upgraded large model series, SenseNova V6, at its Tech Day event held in several locations, including Shanghai and Shenzhen. Leveraging advances in the training of multimodal long chain-of-thought (CoT), global memory, and reinforcement learning, the model delivers industry-leading multimodal reasoning capabilities while setting a new benchmark for cost efficiency.

The capabilities of the SenseNova V6 model have been greatly enhanced, with strong advantages in long CoT, reasoning, mathematical capabilities, and global memory. Its multimodal reasoning capabilities ranked first in China when benchmarked against GPT-o1, while its data analysis performance outpaced GPT-4o. It also combines high performance with cost efficiency. Its multimodal training efficiency is aligned with that of language models, providing the lowest training costs in the industry. Its reasoning costs are also the lowest in the industry. The new lightweight full-modal interactive model, SenseNova V6 Omni, delivers the most advanced multimodal interactive capabilities in China. It is China's first large model that supports in-depth analysis of 10-minute mid-to-long form videos, benchmarked against Gemini 2.5 Turbo to be among the strongest in its class",,,API access,"Hong Kong,China",,,,,,2025-06-11 17:00,,,,,,Industry,,,,,Unreleased,"""The SenseChat app is available for preview and SenseNova V6 is now available for trial via the SenseChat web platform at https://chat.sensetime.com/wb/chat.""

API: https://platform.sensenova.cn/technology/fusion/Pro/",Industry,,,,,,,,,
Seaweed-7B,Video,"Video generation,Text-to-video,Image-to-video,Audio generation",ByteDance,"Team Seawead, Ceyuan Yang, Zhijie Lin, Yang Zhao, Shanchuan Lin, Zhibei Ma, Haoyuan Guo, Hao Chen, Lu Qi, Sen Wang, Feng Cheng, Feilong Zuo Xuejiao Zeng, Ziyan Yang, Fangyuan Kong, Zhiwu Qing, Fei Xiao, Meng Wei, Tuyen Hoang, Siyu Zhang, Peihao Zhu, Qi Zhao, Jiangqiao Yan, Liangke Gui, Sheng Bi, Jiashi Li, Yuxi Ren, Rui Wang, Huixia Li, Xuefeng Xiao, Shu Liu, Feng Ling, Heng Zhang, Houmin Wei, Huafeng Kuang, Jerry Duncan, Junda Zhang, Junru Zheng, Li Sun, Manlin Zhang, Renfei Sun, Xiaobin Zhuang, Xiaojie Li, Xin Xia, Xuyan Chi, Yanghua Peng, Yuping Wang, Yuxuan Wang, Zhongkai Zhao, Zhuo Chen, Zuquan Song, Zhenheng Yang, Jiashi Feng, Jianchao Yang, Lu Jiang",2025-04-11,"Seaweed-7B: Cost-Effective Training of Video Generation Foundation Model
",https://arxiv.org/abs/2504.08685,0.0,,,7000000000.0,,9.0007697e+23,"989400000000000 FLOP / GPU / sec [H100 reported, bf16 assumed] * 665000 GPU-hours * 3600 sec / hour * 0.38 [reported utilization] = 9.0007697e+23 FLOP",,,,,,"We train the model from scratch using 665,000 H100 GPU hours, equivalent to 27.7 days of training on 1,000 H100 GPUs.",NVIDIA H100 SXM5 80GB,,Confident,"This technical report presents a cost-efficient strategy for training a video generation foundation model. We present a mid-sized research model with approximately 7 billion parameters (7B) called Seaweed-7B trained from scratch using 665,000 H100 GPU hours. Despite being trained with moderate computational resources, Seaweed-7B demonstrates highly competitive performance compared to contemporary video generation models of much larger size. Design choices are especially crucial in a resource-constrained setting. This technical report highlights the key design decisions that enhance the performance of the medium-sized diffusion model. Empirically, we make two observations: (1) Seaweed-7B achieves performance comparable to, or even surpasses, larger models trained on substantially greater GPU resources, and (2) our model, which exhibits strong generalization ability, can be effectively adapted across a wide range of downstream applications either by lightweight fine-tuning or continue training. See the project page at this https URL",,,Unreleased,China,,,,,0.38,2025-06-12 14:05,,,,,,Industry,,,,665000.0,Unreleased,,Industry,,,,,Hardware,,,,
Pangu Ultra,Language,"Code generation,Language modeling/generation",Huawei,"Yichun Yin, Wenyong Huang, Kaikai Song, Yehui Tang, Xueyu Wu, Wei Guo, Peng Guo, Yaoyuan Wang, Xiaojun Meng, Yasheng Wang, Dong Li, Can Chen, Dandan Tu, Yin Li, Fisher Yu, Ruiming Tang, Yunhe Wang, Baojun Wang, Bin Wang, Bo Wang, Boxiao Liu, Changzheng Zhang, Duyu Tang, Fei Mi, Hui Jin, Jiansheng Wei, Jiarui Qin, Jinpeng Li, Jun Zhao, Liqun Deng, Lin Li, Minghui Xu, Naifu Zhang, Nianzu Zheng, Qiang Li, Rongju Ruan, Shengjun Cheng, Tianyu Guo, Wei He, Wei Li, Weiwen Liu, Wulong Liu, Xinyi Dai, Yonghan Dong, Yu Pan, Yue Li, Yufei Wang, Yujun Li, Yunsheng Ni, Zhe Liu, Zhenhe Zhang, Zhicheng Liu",2025-04-10,"Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs
",https://arxiv.org/abs/2504.07866,0.0,SOTA improvement,"A closer examination reveals that Pangu Ultra excels on Chinese benchmarks, surpassing both Qwen 2.5 72B and DeepSeek V3, the current best-performing Chinese model. In addition, when compared to Llama 3.1 405B, Pangu Ultra achieves better scores on most of the challenging benchmarks, while utilizing only about 29% of the training FLOPs required by Llama 405B. These results suggest the effectiveness of our model architecture and the high quality of our training data.",135000000000.0,,1.0692e+25,"When compared to Llama 3.1 405B, Pangu Ultra achieves better scores on most of the challenging benchmarks, while utilizing only about 29% of the training FLOPs required by Llama 405B.

6*135000000000*13200000000000=1.069200e+25",,,13200000000000.0, 13.2 trillion tokens,,,Huawei Ascend 910B,,Confident,"We present Pangu Ultra, a Large Language Model (LLM) with 135 billion parameters and dense Transformer modules trained on Ascend Neural Processing Units (NPUs). Although the field of LLM has been witnessing unprecedented advances in pushing the scale and capability of LLM in recent years, training such a large-scale model still involves significant optimization and system challenges. To stabilize the training process, we propose depth-scaled sandwich normalization, which effectively eliminates loss spikes during the training process of deep models. We pre-train our model on 13.2 trillion diverse and high-quality tokens and further enhance its reasoning capabilities during post-training. To perform such large-scale training efficiently, we utilize 8,192 Ascend NPUs with a series of system optimizations. Evaluations on multiple diverse benchmarks indicate that Pangu Ultra significantly advances the state-of-the-art capabilities of dense LLMs such as Llama 405B and Mistral Large 2, and even achieves competitive results with DeepSeek-R1, whose sparse model structure contains much more parameters. Our exploration demonstrates that Ascend NPUs are capable of efficiently and effectively training dense models with more than 100 billion parameters. Our model and system will be available for our commercial customers.",,,Hosted access (no API),China,,,,8192.0,,2025-05-22 16:21,,,,,,Industry,,,,,Unreleased,"""Our model and system will be available for our commercial customers""",Industry,," Overall, we achieve over 52% Model FLOPs Utilization (MFU) when training
Pangu Ultra on 8,192 Ascend NPUs.",,6426121.277196856,,,,,19
AMIE (Articulate Medical Intelligence Explorer),"Medicine,Language","Medical diagnosis,Language modeling/generation,Question answering,Chat","Google DeepMind,Google Research","Tao Tu, Mike Schaekermann, Anil Palepu, Khaled Saab, Jan Freyberg, Ryutaro Tanno, Amy Wang, Brenna Li, Mohamed Amin, Yong Cheng, Elahe Vedadi, Nenad Tomasev, Shekoofeh Azizi, Karan Singhal, Le Hou, Albert Webson, Kavita Kulkarni, S. Sara Mahdavi, Christopher Semturs, Juraj Gottweis, Joelle Barral, Katherine Chou, Greg S. Corrado, Yossi Matias, Alan Karthikesalingam, Vivek Natarajan",2025-04-09,Towards conversational diagnostic artificial intelligence,"https://www.nature.com/articles/s41586-025-08866-7?linkId=13898052
https://www.nature.com/articles/s41586-025-08869-4?linkId=13898054",,,,340000000000.0,Supposedly same amount of parameters as Palm 2 Large,7.34e+24,"Assuming negligible fine-tune compute
base model (Palm 2): 7.34e+24 FLOP
","MultiMedQA,MIMIC-III Critical Care Database","""AMIE was fine-tuned with long context length on a task mixture consisting
of medical question answering (multiple-choice and long-form questions), medical dialogue generation and electronic health record (EHR) note summarization. The datasets used included the training splits of MultiMedQA (MedQA, MedMCQA, HealthSearchQA, LiveQA and MedicationQA)10, a proprietary dataset of medical conversations, and expert handcrafted EHR note summaries from MIMIC-III. The capability to process long context input enables AMIE to handle tasks that require long-range reasoning and comprehension. From MedQA (multiple-choice) we used US Medical Licensing Examination (USMLE) multiple-choice style open domain questions with
four or five possible answers. A set of 11,450 questions were used
for training and 1,273 questions reserved for testing. We curated 191
MedQA questions from the training set where clinical experts crafted
step-by-step reasoning leading to the correct answer. From the Health-
SearchQA, LiveQA and MedicationQA datasets we used expert-crafted
long-form responses to 64 questions. The medical conversations were
from a dataset of 218 dialogues between a clinician and patient that are
associated with a corresponding ground truth diagnosis. The dataset
features respiratory (n = 168), musculoskeletal (n = 40), cardiology
(n = 5), gastroenterology (n = 4) and dermatology (n = 1) dialogues. In
total, 102 unique diagnoses are represented with the mean number of
turns 98 (25th percentile = 85.0, 75th percentile = 113.0).
No NEJM case report data was used in the tuning process and we
perform contamination experiments to assess whether performance
was impacted by case text that may have been in the pretraining corpus.""",,,,,,,Likely,"At the heart of medicine lies physicianâ€“patient dialogue, where skillful history-taking enables effective diagnosis, management and enduring trust1,2. Artificial intelligence (AI) systems capable of diagnostic dialogue could increase accessibility and quality of care. However, approximating cliniciansâ€™ expertise is an outstanding challenge. Here we introduce AMIE (Articulate Medical Intelligence Explorer), a large language model (LLM)-based AI system optimized for diagnostic dialogue. AMIE uses a self-play-based3 simulated environment with automated feedback for scaling learning across disease conditions, specialties and contexts. We designed a framework for evaluating clinically meaningful axes of performance, including history-taking, diagnostic accuracy, management, communication skills and empathy. We compared AMIEâ€™s performance to that of primary care physicians in a randomized, double-blind crossover study of text-based consultations with validated patient-actors similar to objective structured clinical examination4,5. The study included 159 case scenarios from providers in Canada, the United Kingdom and India, 20 primary care physicians compared to AMIE, and evaluations by specialist physicians and patient-actors. AMIE demonstrated greater diagnostic accuracy and superior performance on 30 out of 32 axes according to the specialist physicians and 25 out of 26 axes according to the patient-actors. Our research has several limitations and should be interpreted with caution. Clinicians used synchronous text chat, which permits large-scale LLMâ€“patient interactions, but this is unfamiliar in clinical practice. While further research is required before AMIE could be translated to real-world settings, the results represent a milestone towards conversational diagnostic AI.",,,Unreleased,"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France,Multinational,United States of America,Canada,Switzerland",PaLM 2,,,,,2025-05-16 10:30,,,,,,"Industry,Industry",,,,,Unreleased,"""AMIE is an LLM-based research AI system for diagnostic dialogue. We
are not making the model code and weights open source owing to the
safety implications of unmonitored use of such a system in medical
settings. In the interest of responsible innovation, we will be working
with research partners, regulators and providers to validate and explore
safe onward uses of AMIE. For reproducibility, we have documented
technical deep learning methods while keeping the paper accessible to
a clinical and general scientific audience. Our work builds on PaLM 2,
for which technical details have been described extensively in the
technical report""","Industry,Industry",,,,,Comparison with other models,,,,
SHIFT-SUV,3D modeling,Aerodynamics simulations,"Luminary Cloud,NVIDIA,Honda",,2025-04-09,Luminary Cloud Unveils First Physics AI Open-Source Automotive Foundation Model for SUV Aerodynamics in Collaboration with Honda and NVIDIA,https://www.luminarycloud.com/resources/luminary-cloud-unveils-first-physics-ai-open-source-automotive-foundation-model-for-suv-aerodynamics-in-collaboration-with-honda-and-nvidia/,,,,,,,,SHIFT-SUV,,,"""At launch, the SHIFT-SUV model is trained on approximately one thousand simulations, with a goal of advancing to 25,000 by the end of the year.""",,,,,Unknown,"Our first physics foundation model, SHIFT-SUV, is a massive step forward in that directionâ€”purpose-built for high-fidelity aerodynamic inference, without requiring CFD expertise or meshing. It was developed in collaboration with Honda and NVIDIA, combining deep automotive domain expertise with cutting-edge AI and GPU computing.

SHIFT-SUV is based on thousands of parametrically morphed variants of the AeroSUV platformâ€”an open-access, SUV reference model developed by FKFS (Forschungsinstitut fÃ¼r Kraftfahrwesen und Fahrzeugmotoren Stuttgart) as an extension of the DrivAer concept.

We're releasing both the dataset and the SHIFT-SUV pretrained model to the public as open source, and we're eager to collaborate with the community. Whether you're fine-tuning SHIFT for proprietary geometries, contributing new data, or exploring novel model architectures, we want to work with you - contact us via the form at shift.luminarycloud.com. You can also download a sample dataset (99 simulations, volume fields omitted) from Hugging Face as a starting point.",,,Open weights (non-commercial),"United States of America,United States of America,Japan",,,,,,2025-05-12 12:18,,,,,,"Industry,Industry,Industry",,,,,Unreleased,"The model is described as open source for non-commercial purposes but it requires filling a form to access it

https://shift.luminarycloud.com/","Industry,Industry,Industry",,,,,,,,,
Gen-4 Turbo,"Video,Vision","Video generation,Image-to-video",Runway,,2025-04-09,"Introducing Runway Gen-4
Our next-generation series of AI models for media generation and world consistency.",https://help.runwayml.com/hc/en-us/articles/37327109429011-Creating-with-Gen-4-Video,,,,,,,,Unspecified unreleased,,,,,,,,Unknown,"Gen-4 Turbo is now available on the Runway API, allowing developers to easily integrate our most powerful and efficient video model directly into their apps and products.

Gen-4 Turbo offers the same price, scale and reliability of Gen-3 Alpha Turbo but with the state-of-the-art video generation capabilities of Gen-4.",,,API access,United States of America,,,,,,2025-05-29 14:58,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
TxGemma 27B,"Language,Biology","Protein or nucleotide language model (pLM/nLM),Protein property prediction,Small molecule property prediction,Chat,Question answering,Protein question answering,Protein function prediction,Language modeling/generation","Google DeepMind,Google Research","Eric Wang, Samuel Schmidgall, Paul F. Jaeger, Fan Zhang, Rory Pilgrim, Yossi Matias, Joelle Barral, David Fleet, Shekoofeh Azizi",2025-04-08,TxGemma: Efficient and Agentic LLMs for Therapeutics,https://arxiv.org/abs/2504.06196,,,,27000000000.0,27B,2.116854e+24,base model compute 2.106e+24 FLOP + finetune compute 1.0854e+22 FLOP = 2.116854e+24 FLOP,Therapeutic Data Commons (TDC),"""a diverse set of instruction-tuning datasets, curated from the Therapeutics Data Commons (TDC)""",5600000000.0,"""This encompassed all approximately 7 million training examples, comprising 3.3 million from regression/generation and 3.7 million from binary classification tasks.  Fine-tuning proceeded for 67B tokens (12 epochs) using 256 TPUv4 chips""",,,Google TPU v4,,Confident,"Therapeutic development is a costly and high-risk endeavor that is often plagued by high failure rates. To address this, we introduce TxGemma, a suite of efficient, generalist large language models (LLMs) capable of therapeutic property prediction as well as interactive reasoning and explainability. Unlike task-specific models, TxGemma synthesizes information from diverse sources, enabling broad application across the therapeutic development pipeline. The suite includes 2B, 9B, and 27B parameter models, fine-tuned from Gemma-2 on a comprehensive dataset of small molecules, proteins, nucleic acids, diseases, and cell lines. Across 66 therapeutic development tasks, TxGemma achieved superior or comparable performance to the state-of-the-art generalist model on 64 (superior on 45), and against state-of-the-art specialist models on 50 (superior on 26). Fine-tuning TxGemma models on therapeutic downstream tasks, such as clinical trial adverse event prediction, requires less training data than fine-tuning base LLMs, making TxGemma suitable for data-limited applications. Beyond these predictive capabilities, TxGemma features conversational models that bridge the gap between general LLMs and specialized property predictors. These allow scientists to interact in natural language, provide mechanistic reasoning for predictions based on molecular structure, and engage in scientific discussions. Building on this, we further introduce Agentic-Tx, a generalist therapeutic agentic system powered by Gemini 2.5 that reasons, acts, manages diverse workflows, and acquires external domain knowledge. Agentic-Tx surpasses prior leading models on the Humanity's Last Exam benchmark (Chemistry & Biology) with 52.3% relative improvement over o3-mini (high) and 26.7% over o3-mini (high) on GPQA (Chemistry) and excels with improvements of 6.3% (ChemBench-Preference) and 2.4% (ChemBench-Mini) over o3-mini (high).",12.0,,Open weights (restricted use),"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France,Multinational,United States of America,Canada,Switzerland",Gemma 2 27B,1,6 FLOP / parameter / token * 27 *10^9 parameters * 67*10^9 tokens = 1.0854e+22 FLOP,256.0,,2025-05-23 16:39,,,,,,"Industry,Industry",,,,,Unreleased,"Health AI Developer Foundations Terms of Use: not for clinical use
https://huggingface.co/google/txgemma-27b-predict

no training code here
https://github.com/google-gemini/gemma-cookbook/tree/main/TxGemma","Industry,Industry",,,,85350.72454206382,Operation counting,google,,,
TxGemma 9B,"Language,Biology","Protein or nucleotide language model (pLM/nLM),Protein property prediction,Small molecule property prediction,Chat,Question answering,Protein question answering,Protein function prediction,Language modeling/generation","Google DeepMind,Google Research","Eric Wang, Samuel Schmidgall, Paul F. Jaeger, Fan Zhang, Rory Pilgrim, Yossi Matias, Joelle Barral, David Fleet, Shekoofeh Azizi",2025-04-08,TxGemma: Efficient and Agentic LLMs for Therapeutics,https://arxiv.org/abs/2504.06196,,,,9000000000.0,9B,4.35618e+23,base model compute 4.32e+23 FLOP + finetune compute 3.618e+21 FLOP = 4.35618e+23 FLOP,Therapeutic Data Commons (TDC),"""a diverse set of instruction-tuning datasets, curated from the Therapeutics Data Commons (TDC)""",5600000000.0,"""This encompassed all approximately 7 million training examples, comprising 3.3 million from regression/generation and 3.7 million from binary classification tasks.  Fine-tuning proceeded for 67B tokens (12 epochs) using 256 TPUv4 chips""",,,Google TPU v4,,Confident,"Therapeutic development is a costly and high-risk endeavor that is often plagued by high failure rates. To address this, we introduce TxGemma, a suite of efficient, generalist large language models (LLMs) capable of therapeutic property prediction as well as interactive reasoning and explainability. Unlike task-specific models, TxGemma synthesizes information from diverse sources, enabling broad application across the therapeutic development pipeline. The suite includes 2B, 9B, and 27B parameter models, fine-tuned from Gemma-2 on a comprehensive dataset of small molecules, proteins, nucleic acids, diseases, and cell lines. Across 66 therapeutic development tasks, TxGemma achieved superior or comparable performance to the state-of-the-art generalist model on 64 (superior on 45), and against state-of-the-art specialist models on 50 (superior on 26). Fine-tuning TxGemma models on therapeutic downstream tasks, such as clinical trial adverse event prediction, requires less training data than fine-tuning base LLMs, making TxGemma suitable for data-limited applications. Beyond these predictive capabilities, TxGemma features conversational models that bridge the gap between general LLMs and specialized property predictors. These allow scientists to interact in natural language, provide mechanistic reasoning for predictions based on molecular structure, and engage in scientific discussions. Building on this, we further introduce Agentic-Tx, a generalist therapeutic agentic system powered by Gemini 2.5 that reasons, acts, manages diverse workflows, and acquires external domain knowledge. Agentic-Tx surpasses prior leading models on the Humanity's Last Exam benchmark (Chemistry & Biology) with 52.3% relative improvement over o3-mini (high) and 26.7% over o3-mini (high) on GPQA (Chemistry) and excels with improvements of 6.3% (ChemBench-Preference) and 2.4% (ChemBench-Mini) over o3-mini (high).",12.0,,Open weights (restricted use),"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France,Multinational,United States of America,Canada,Switzerland",Gemma 2 9B,3,6 FLOP / parameter / token * 9*10^9 parameters * 67*10^9 tokens = 3.618e+21 FLOP,256.0,,2025-05-23 16:38,,,,,,"Industry,Industry",,,,,Unreleased,"https://huggingface.co/google/txgemma-9b-predict
Health AI Developer Foundations Terms of Use: not for clinical use

no training code here
https://github.com/google-gemini/gemma-cookbook/tree/main/TxGemma","Industry,Industry",,,,85350.72454206382,Operation counting,google,,,
TxGemma 2B,"Language,Biology","Protein or nucleotide language model (pLM/nLM),Protein property prediction,Small molecule property prediction,Question answering,Protein question answering,Protein function prediction,Language modeling/generation","Google DeepMind,Google Research","Eric Wang, Samuel Schmidgall, Paul F. Jaeger, Fan Zhang, Rory Pilgrim, Yossi Matias, Joelle Barral, David Fleet, Shekoofeh Azizi",2025-04-08,TxGemma: Efficient and Agentic LLMs for Therapeutics,https://arxiv.org/abs/2504.06196,,,,2600000000.0,2.6B,3.2245199999999995e+22,base model compute 3.12e+22 FLOP + finetune compute 1.0452e+21 FLOP = 3.22452e+22 FLOP,Therapeutic Data Commons (TDC),"""a diverse set of instruction-tuning datasets, curated from the Therapeutics Data Commons (TDC)""",5600000000.0,"""This encompassed all approximately 7 million training examples, comprising 3.3 million from regression/generation and 3.7 million from binary classification tasks.  Fine-tuning proceeded for 67B tokens (12 epochs) using 256 TPUv4 chips""

67B/12 = 5.6B tokens per epoch (~800 tokens per training example)",,,Google TPU v4,,Confident,"Therapeutic development is a costly and high-risk endeavor that is often plagued by high failure rates. To address this, we introduce TxGemma, a suite of efficient, generalist large language models (LLMs) capable of therapeutic property prediction as well as interactive reasoning and explainability. Unlike task-specific models, TxGemma synthesizes information from diverse sources, enabling broad application across the therapeutic development pipeline. The suite includes 2B, 9B, and 27B parameter models, fine-tuned from Gemma-2 on a comprehensive dataset of small molecules, proteins, nucleic acids, diseases, and cell lines. Across 66 therapeutic development tasks, TxGemma achieved superior or comparable performance to the state-of-the-art generalist model on 64 (superior on 45), and against state-of-the-art specialist models on 50 (superior on 26). Fine-tuning TxGemma models on therapeutic downstream tasks, such as clinical trial adverse event prediction, requires less training data than fine-tuning base LLMs, making TxGemma suitable for data-limited applications. Beyond these predictive capabilities, TxGemma features conversational models that bridge the gap between general LLMs and specialized property predictors. These allow scientists to interact in natural language, provide mechanistic reasoning for predictions based on molecular structure, and engage in scientific discussions. Building on this, we further introduce Agentic-Tx, a generalist therapeutic agentic system powered by Gemini 2.5 that reasons, acts, manages diverse workflows, and acquires external domain knowledge. Agentic-Tx surpasses prior leading models on the Humanity's Last Exam benchmark (Chemistry & Biology) with 52.3% relative improvement over o3-mini (high) and 26.7% over o3-mini (high) on GPQA (Chemistry) and excels with improvements of 6.3% (ChemBench-Preference) and 2.4% (ChemBench-Mini) over o3-mini (high).",12.0,,Open weights (restricted use),"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France,Multinational,United States of America,Canada,Switzerland",Gemma 2 2B,1,6 FLOP / parameter / token * 2.6*10^9 parameters * 67*10^9 tokens = 1.0452e+21 FLOP,256.0,,2025-05-01 10:42,,,,,,"Industry,Industry",,,,,,"https://huggingface.co/google/txgemma-2b-predict
Health AI Developer Foundations Terms of Use: not for clinical use","Industry,Industry",,,,85350.72454206382,Operation counting,google,,,
Amazon Nova Reel,"Video,Vision","Video generation,Text-to-video,Image-to-video",Amazon,,2025-04-07,Amazon Nova Reel is a video generation model that supports the generation of short videos from input text and images. Amazon Nova Reel provides camera motion controls using natural language inputs.,https://docs.aws.amazon.com/ai/responsible-ai/nova-reel/overview.html,,,,,,,,Unspecified unreleased,,,,,,,,Unknown,"Amazon Nova Reel is a proprietary multimodal foundation model (FM) designed for enterprise use cases. Amazon Nova Reel generates a novel video from a descriptive natural language text string and an optional reference image (together, the â€œpromptâ€). Customers can use Amazon Nova Reel to create content within advertising, branding, product design, and social media workflows.
This AI Service Card applies to the use of Amazon Nova Reel via Amazon Bedrock Console and Amazon Bedrock API. Typically, customers use the Console to develop and test applications,
and the API for production loads at scale. Each Nova model is a managed subservice of Amazon Bedrock; customers can focus on executing prompts without having to provision or manage any
infrastructure such as instance types, network topology, and endpoints.
An Amazon Nova Reel <text prompt, <optional image prompts>, generated video> triple is said to be ""effective"" if a skilled human evaluator decides that the generated video: 1/ has the content
requested by the input prompts (the combination of text and optional image prompts); 2/ makes reasonable assumptions about elements not specified in the input prompts (for example, if asked
for a video of a kitchen, a refrigerator and microwave are present and not a couch or a tiger); 3/ is free from defects or image composition errors (for example, human body parts are attached in the correct places and objects are not warped); and 4/ is consistent with the standards of safety, fairness, and other properties valued by the evaluator. Otherwise, a triple is said to be ""ineffective.""
A customer's workflow must decide if a generated video is effective using human judgment, whether human judgement is applied on a case-by-case basis (as happens when the Console is
used as a productivity tool by itself) or is applied via the customer's choice of an acceptable score on an automated test",,,API access,United States of America,,,,,,2025-05-30 14:48,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
Llama 4 Scout,"Multimodal,Language,Vision","Chat,Code generation,Visual question answering,Language modeling/generation,Question answering",Meta AI,,2025-04-05,The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation,https://ai.meta.com/blog/llama-4-multimodal-intelligence/,,,,109000000000.0,"""Our smaller model, Llama 4 Scout, is a general purpose model with 17 billion active parameters, 16 experts, and 109 billion total parameters that delivers state-of-the-art performance for its class.""",4.08e+24,"40T training tokens per model card:

https://github.com/meta-llama/llama-models/blob/main/models/llama4/MODEL_CARD.md  

Estimating training compute from parameters and tokens:
6 FLOP per token per parameter * 17B active parameters * 40T tokens = 4.08e24 FLOP
(Implying mean throughput was 227 TFLOPS/GPU, or 11.5% MFU in FP8)


The model card also states that Llama 4 Scout used 5.0M H100-hours.
The blog post gives a figure of 390 TFLOPS/GPU, but this may have been the utilization rate for Behemoth, or all of the models together. Using this utilization, we have:
Compute = 390 TFLOP/s * 5 million hours = 7.02e24 FLOP
(This value is higher than the compute implied by parameters and tokens, and suggests utilization may have been lower for Scout than for Behemoth.)",,"Knowledge cutoff date is August 2024, according to https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E.",30000000000000.0,"""The overall data mixture for training consisted of more than 30 trillion tokens, which is more than double the Llama 3 pre-training mixture and includes diverse text, image, and video datasets.""",,,,,Likely,"Weâ€™re sharing the first models in the Llama 4 herd, which will enable people to build more personalized multimodal experiences.
Llama 4 Scout, a 17 billion active parameter model with 16 experts, is the best multimodal model in the world in its class and is more powerful than all previous generation Llama models, while fitting in a single NVIDIA H100 GPU. Additionally, Llama 4 Scout offers an industry-leading context window of 10M and delivers better results than Gemma 3, Gemini 2.0 Flash-Lite, and Mistral 3.1 across a broad range of widely reported benchmarks.
Llama 4 Maverick, a 17 billion active parameter model with 128 experts, is the best multimodal model in its class, beating GPT-4o and Gemini 2.0 Flash across a broad range of widely reported benchmarks, while achieving comparable results to the new DeepSeek v3 on reasoning and codingâ€”at less than half the active parameters. Llama 4 Maverick offers a best-in-class performance to cost ratio with an experimental chat version scoring ELO of 1417 on LMArena.
These models are our best yet thanks to distillation from Llama 4 Behemoth, a 288 billion active parameter model with 16 experts that is our most powerful yet and among the worldâ€™s smartest LLMs. Llama 4 Behemoth outperforms GPT-4.5, Claude Sonnet 3.7, and Gemini 2.0 Pro on several STEM benchmarks. Llama 4 Behemoth is still training, and weâ€™re excited to share more details about it even while itâ€™s still in flight.
Download the Llama 4 Scout and Llama 4 Maverick models today on llama.com and Hugging Face. Try Meta AI built with Llama 4 in WhatsApp, Messenger, Instagram Direct, and on the web.",,,Open weights (restricted use),United States of America,,,,,,2025-05-29 15:19,,,,,,Industry,,,,,Unreleased,"Llama 4 license (branding requirements, size cap 700M MAU)
https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E

no training code here 
https://github.com/meta-llama/llama-models/tree/main/models/llama4",Industry,,,,,,meta-llama,,,
Llama 4 Maverick,"Multimodal,Language,Vision","Chat,Code generation,Visual question answering,Language modeling/generation,Question answering",Meta AI,,2025-04-05,The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation,https://ai.meta.com/blog/llama-4-multimodal-intelligence/,,,,400000000000.0,"""Llama 4 Maverick models have 17B active parameters and 400B total parameters.""

https://ai.meta.com/blog/llama-4-multimodal-intelligence/",2.244000000001e+24,"22T training tokens per model card:

https://github.com/meta-llama/llama-models/blob/main/models/llama4/MODEL_CARD.md   

Maverick was trained using co-distillation from Llama 4 Behemoth. It isn't 100% clear that all 22T tokens used distillation, but we assume this for the time being.

Estimating training compute from parameters and tokens:
Compute = 6 FLOP per token per parameter * 17B active parameters * 22T tokens = 2.244e24 FLOP
(Implying mean throughput was 262 TFLOPS/GPU, or 13.2% MFU in FP8)


The model card also states that Llama 4 Maverick used 2.38M H100-hours.
The blog post gives a figure of 390 TFLOPS/GPU, but this may have been the utilization rate for Behemoth, or all of the models together. Using this utilization, we have:
Compute = 390 TFLOP/s * 2.38 million hours = 3.342e24 FLOP
(This value is higher than the compute implied by parameters and tokens, and suggests utilization may have been lower for Maverick than for Behemoth.)",,"Knowledge cutoff date is August 2024, according to https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E.",30000000000000.0,"""The overall data mixture for training consisted of more than 30 trillion tokens, which is more than double the Llama 3 pre-training mixture and includes diverse text, image, and video datasets.""",,,,,Likely,"Weâ€™re sharing the first models in the Llama 4 herd, which will enable people to build more personalized multimodal experiences.
Llama 4 Scout, a 17 billion active parameter model with 16 experts, is the best multimodal model in the world in its class and is more powerful than all previous generation Llama models, while fitting in a single NVIDIA H100 GPU. Additionally, Llama 4 Scout offers an industry-leading context window of 10M and delivers better results than Gemma 3, Gemini 2.0 Flash-Lite, and Mistral 3.1 across a broad range of widely reported benchmarks.
Llama 4 Maverick, a 17 billion active parameter model with 128 experts, is the best multimodal model in its class, beating GPT-4o and Gemini 2.0 Flash across a broad range of widely reported benchmarks, while achieving comparable results to the new DeepSeek v3 on reasoning and codingâ€”at less than half the active parameters. Llama 4 Maverick offers a best-in-class performance to cost ratio with an experimental chat version scoring ELO of 1417 on LMArena.
These models are our best yet thanks to distillation from Llama 4 Behemoth, a 288 billion active parameter model with 16 experts that is our most powerful yet and among the worldâ€™s smartest LLMs. Llama 4 Behemoth outperforms GPT-4.5, Claude Sonnet 3.7, and Gemini 2.0 Pro on several STEM benchmarks. Llama 4 Behemoth is still training, and weâ€™re excited to share more details about it even while itâ€™s still in flight.
Download the Llama 4 Scout and Llama 4 Maverick models today on llama.com and Hugging Face. Try Meta AI built with Llama 4 in WhatsApp, Messenger, Instagram Direct, and on the web.",,,Open weights (restricted use),United States of America,,,,,,2025-06-06 16:27,,,,,,Industry,,,,,Unreleased,"Llama 4 license (branding requirements, size cap 700M MAU)
https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E-Original

no training code here 
https://github.com/meta-llama/llama-models/tree/main/models/llama4",Industry,,,,,,meta-llama,,,
Llama 4 Behemoth (preview),"Multimodal,Language,Vision","Chat,Code generation,Visual question answering,Translation,Language modeling/generation,Quantitative reasoning,Question answering",Meta AI,,2025-04-05,The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation,https://ai.meta.com/blog/llama-4-multimodal-intelligence/,,Training cost,,2000000000000.0,"""Llama 4 Behemoth, a 288 billion active parameter model with 16 experts that is our most powerful yet and among the worldâ€™s smartest LLMs.""",5.18400000000001e+25,"Behemoth's training dataset is at least 30T tokens:
https://ai.meta.com/blog/llama-4-multimodal-intelligence/ 

6 FLOP / parameter / token * 288 * 10^9 activated parameters * 30 * 10^12 tokens = 5.184e+25 FLOP",,,30000000000000.0,"""The overall data mixture for training consisted of more than 30 trillion tokens, which is more than double the Llama 3 pre-training mixture and includes diverse text, image, and video datasets.""",,"Based on the model cards for Llama 4 Scout and Maverick, they seem to be using H100-80GB GPUs, despite the article saying that 390 TFLOPS/GPU was a high MFU (it is high throughput, but <20% MFU in FP8).",,,Likely,"Weâ€™re sharing the first models in the Llama 4 herd, which will enable people to build more personalized multimodal experiences.
...
Llama 4 Behemoth, a 288 billion active parameter model with 16 experts that is our most powerful yet and among the worldâ€™s smartest LLMs. Llama 4 Behemoth outperforms GPT-4.5, Claude Sonnet 3.7, and Gemini 2.0 Pro on several STEM benchmarks. Llama 4 Behemoth is still training, and weâ€™re excited to share more details about it even while itâ€™s still in flight.
Download the Llama 4 Scout and Llama 4 Maverick models today on llama.com and Hugging Face. Try Meta AI built with Llama 4 in WhatsApp, Messenger, Instagram Direct, and on the web.",,,Unreleased,United States of America,,,,32000.0,,2025-05-23 09:57,,,,,,Industry,,,,,Unreleased,"""While weâ€™re not yet releasing Llama 4 Behemoth as it is still training""",Industry,,,,,Operation counting,,,,3
Amazon Nova Act,Multimodal,System control,Amazon,Amazon AGI,2025-03-31,Introducing Amazon Nova Act,https://labs.amazon.science/blog/nova-act,,,,,,,,,,,,,,,,Unknown,"Nova Act is an early research preview of an SDK + model for building agents designed to reliably take actions in web browsers. Building with the SDK enables developers to break down complex workflows into smaller, reliable, commands, add more detail where needed, call APIs, and intersperse direct browser manipulation. Developers can interleave Python code, whether it be tests, breakpoints, asserts, or threadpooling for parallelization. Read more about the announcement: https://labs.amazon.science/blog/nova-act.",,,API access,United States of America,,,,,,2025-06-09 15:58,,,,,,Industry,,,,,Unreleased,"Apache 2.0 for inference code:
https://github.com/aws/nova-act",Industry,,,,,,,,,
Gen-4,"Video,Vision","Video generation,Image-to-video,Text-to-video",Runway,,2025-03-31,"Introducing Runway Gen-4
Our next-generation series of AI models for media generation and world consistency.","https://runwayml.com/research/introducing-runway-gen-4

https://help.runwayml.com/hc/en-us/articles/37327109429011-Creating-with-Gen-4-Video",,,,,,,,Unspecified unreleased,,,,,,,,Unknown,"Gen-4 sets a new standard for video generation and is a marked improvement over Gen-3 Alpha. It excels in its ability to generate highly dynamic videos with realistic motion as well as subject, object and style consistency with superior prompt adherence and best-in-class world understanding. 
Using visual references, combined with instructions, Gen-4 allows you to create new images and videos with consistent styles, subjects, locations and more. Allowing for continuity and control within your stories. ",,,API access,United States of America,,,,,,2025-05-29 14:58,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
QVQ-Max,"Language,Vision,Multimodal","Visual question answering,Video description,Language modeling/generation,Translation,Question answering,Character recognition,Quantitative reasoning",Alibaba,"Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, Junyang Lin",2025-03-28,QVQ-Max: Think with Evidence,https://qwenlm.github.io/blog/qvq-max-preview/,,,,,,,,Unspecified unreleased,,,,,,,,Confident,"Last December, we launched QVQ-72B-Preview as an exploratory model, but it had many issues. Today, we are officially releasing the first version of QVQ-Max, our visual reasoning model. This model can not only â€œunderstandâ€ the content in images and videos but also analyze and reason with this information to provide solutions. From math problems to everyday questions, from programming code to artistic creation, QVQ-Max has demonstrated impressive capabilities. Though this is just our first version, its potential is already eye-catching.",,,,China,,,,,,2025-05-31 22:39,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
Lumina-Image-2.0,Image generation,"Image generation,Text-to-image","Shanghai AI Lab,University of Sydney,Chinese University of Hong Kong (CUHK),Shanghai Jiao Tong University,Krea AI","Qi Qin, Le Zhuo, Yi Xin, Ruoyi Du, Zhen Li, Bin Fu, Yiting Lu, Jiakang Yuan, Xinyue Li, Dongyang Liu, Xiangyang Zhu, Manyuan Zhang, Will Beddow, Erwann Millon, Victor Perez, Wenhai Wang, Conghui He, Bo Zhang, Xiaohong Liu, Hongsheng Li, Yu Qiao, Chang Xu, Peng Gao",2025-03-27,Lumina-Image 2.0: A Unified and Efficient Image Generative Framework,https://arxiv.org/abs/2503.21758,,,,2600000000.0,2.6B,4.7794406e+21,"312000000000000 FLOP / GPU / sec [A100 reported, bf16 assumed] * 14184 GPU-hours [see training time notes] * 3600 sec / hour * 0.3 [assumed utilization] = 4.7794406e+21 FLOP",,,,"""we constructed a dataset combining both real and
synthetic data, and performed data filtering based on the techniques outlined in [15, 22, 58], resulting in total 110M
samples."" ",,"(Table 3) GPU-days:
191+176+224 = 591 GPU-days = 14184 GPU-hours",NVIDIA A100,,Confident,"We introduce Lumina-Image 2.0, an advanced text-to-image generation framework that achieves significant progress compared to previous work, Lumina-Next. Lumina-Image 2.0 is built upon two key principles: (1) Unification - it adopts a unified architecture (Unified Next-DiT) that treats text and image tokens as a joint sequence, enabling natural cross-modal interactions and allowing seamless task expansion. Besides, since high-quality captioners can provide semantically well-aligned text-image training pairs, we introduce a unified captioning system, Unified Captioner (UniCap), specifically designed for T2I generation tasks. UniCap excels at generating comprehensive and accurate captions, accelerating convergence and enhancing prompt adherence. (2) Efficiency - to improve the efficiency of our proposed model, we develop multi-stage progressive training strategies and introduce inference acceleration techniques without compromising image quality. Extensive evaluations on academic benchmarks and public text-to-image arenas show that Lumina-Image 2.0 delivers strong performances even with only 2.6B parameters, highlighting its scalability and design efficiency. We have released our training details, code, and models at this https URL.",,,Open weights (unrestricted),"China,Australia,Hong Kong,China,China,United States of America",,,,,,2025-06-06 14:54,,,,,,"Academia,Academia,Academia,Academia,Industry",,,,14184.0,Unreleased,"Apache 2.0 for weights
https://huggingface.co/Alpha-VLLM/Lumina-Image-2.0

Apache 2.0
https://github.com/Alpha-VLLM/Lumina-Image-2.0","Academia,Academia,Academia,Academia,Industry",,,,,Hardware,Alpha-VLLM,,,
GAIA-2,"Video,Vision,Multimodal,Language","Self-driving car,Video generation,Instruction interpretation",Wayve,"Lloyd Russell, Anthony Hu, Lorenzo Bertoni, George Fedoseev, Jamie Shotton, Elahe Arani, Gianluca Corrado",2025-03-26,GAIA-2: A Controllable Multi-View Generative World Model for Autonomous Driving,https://arxiv.org/abs/2503.20523v1,,,,8685000000.0,"video tokenizer: 285M parameters
workd model:  8.4B parameters

total: 8.685B parameters",7.5609676e+22,"""The video tokenizer was trained for 300,000 steps with a batch size of 128 using 128 H100 GPUs. Input sequences consisted of Tv = 24 video frames sampled at their native capture frequencies (20, 25, or 30 Hz). Random spatial crops of size 448 Ã— 960 were extracted from the frames. For each training sample, a camera view was randomly selected from the available N = 5
perspectives.""

""The latent world model was trained for 460,000 steps with a batch size of 256 on 256 H100 GPUs. Inputs consisted of 48 video frames at native capture frequencies (20, 25, or 30 Hz), spatial resolution 448 Ã— 960 and across N = 5 cameras.  After encoding these videos to the latent space, this corresponds to T Ã— N Ã— H Ã— W = 6 Ã— 5 Ã— 14 Ã— 30 = 12,600 input tokens.""

video tokenizer training compute estimate:
6 FLOP / token / parameter * 285*10^6 parameters * 300000 steps * 128 samples per step * 12600 tokens per sample = 8.273664e+20 FLOP


world model training compute estimation:

6 FLOP / token / parameter * 8.4 * 10^9 parameters * 460000 steps * 256 samples per step * 12600 tokens per sample = 7.478231e+22 FLOP

Total: 8.273664e+20 FLOP + 7.478231e+22 FLOP = 7.5609676e+22 FLOP",Unspecified unreleased,,,"""The dataset comprises approximately 25 million video sequences, each spanning 2 seconds, collected between 2019 and 2024. Recordings were obtained across three countriesâ€”the United Kingdom, the United States, and Germanyâ€”to ensure coverage of geographically and environmentally diverse driving conditions.
""",,"""The video tokenizer was trained for 300,000 steps with a batch size of 128 using 128 H100 GPUs""

""The latent world model was trained for 460,000 steps with a batch size of 256 on 256 H100 GPUs""",NVIDIA H100 SXM5 80GB,,Confident,"Generative models offer a scalable and flexible paradigm for simulating complex environments, yet current approaches fall short in addressing the domain-specific requirements of autonomous driving - such as multi-agent interactions, fine-grained control, and multi-camera consistency. We introduce GAIA-2, Generative AI for Autonomy, a latent diffusion world model that unifies these capabilities within a single generative framework. GAIA-2 supports controllable video generation conditioned on a rich set of structured inputs: ego-vehicle dynamics, agent configurations, environmental factors, and road semantics. It generates high-resolution, spatiotemporally consistent multi-camera videos across geographically diverse driving environments (UK, US, Germany). The model integrates both structured conditioning and external latent embeddings (e.g., from a proprietary driving model) to facilitate flexible and semantically grounded scene synthesis. Through this integration, GAIA-2 enables scalable simulation of both common and rare driving scenarios, advancing the use of generative world models as a core tool in the development of autonomous systems. Videos are available at this https URL.",,,Unreleased,United Kingdom of Great Britain and Northern Ireland,,,,,,2025-06-12 12:16,,,,,,Industry,,,,,Unreleased,,Industry,,,,,Operation counting,,,,
Gemini 2.5 Pro,"Language,Vision,Video,Multimodal,Speech","Language modeling/generation,Question answering,Code generation,Quantitative reasoning,Visual question answering,Translation,Image captioning,Video description,Speech recognition",Google DeepMind,,2025-03-25,Gemini 2.5: Our most intelligent AI model,https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/#gemini-2-5-thinking,,,,,,,,Unspecified unreleased,Knowledge cutoff	January 2025,,,,,,,Unknown,"Gemini 2.5 Pro Experimental is our most advanced model for complex tasks. It tops the LMArena leaderboard â€” which measures human preferences â€” by a significant margin, indicating a highly capable model equipped with high-quality style. 2.5 Pro also shows strong reasoning and code capabilities, leading on common coding, math and science benchmarks.

Gemini 2.5 Pro is available now in Google AI Studio and in the Gemini app for Gemini Advanced users, and will be coming to Vertex AI soon. Weâ€™ll also introduce pricing in the coming weeks, enabling people to use 2.5 Pro with higher rate limits for scaled production use.",,,API access,"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France",,,,,,2025-06-18 07:30,,,,,,Industry,,,,,Unreleased,"Availability	
Google AI Studio
Gemini API
Gemini App",Industry,,,,,,,,,
4o Image Generation,Image generation,"Image generation,Text-to-image",OpenAI,"Leadership
Gabriel Goh: Image Generation
Jackie Shannon: ChatGPT Product
Mengchao Zhong, Wayne Chang: ChatGPT Engineering
Rohan Sahai: Sora Product and Engineering
Brendan Quinn, Tomer Kaftan: Inference
Prafulla Dhariwal: Multimodal Organization

Research

Foundational Research
Allan Jabri, David Medina, Gabriel Goh, Kenji Hata, Lu Liu, Prafulla Dhariwal

Core Research
Aditya Ramesh, Alex Nichol, Casey Chu, Cheng Lu, Dian Ang Yap, Heewoo Jun, James Betker, Jianfeng Wang, Long Ouyang, Li Jing, Wesam Manassra

Research Contributors
Aiden Low, Brandon McKinzie, Charlie Nash, Huiwen Chang, Ishaan Gulrajani, Jamie Kiros, Ji Lin, Kshitij Gupta, Yang Song

Model Behavior
Laurentia Romaniuk

Multimodal Organization
Andrew Gibiansky, Yang Lu",2025-03-25,"Introducing 4o Image Generation
Unlocking useful and valuable image generation with a natively multimodal model capable of precise, accurate, photorealistic outputs.",https://openai.com/index/introducing-4o-image-generation/,,,,,,,,Unspecified unreleased,"""We trained our models on the joint distribution of online images and text, learning not just how images relate to language, but how they relate to each other.""
All GPT-4o models with image capabilities have a knowledge cutoff date of October 2023 - which I assume means October 1, 2023 - according to https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models?tabs=global-standard%2Cstandard-chat-completions#gpt-4o-and-gpt-4-turbo.",,,,,,,Unknown,"GPTâ€‘4o image generation excels at accurately rendering text, precisely following prompts, and leveraging 4oâ€™s inherent knowledge base and chat contextâ€”including transforming uploaded images or using them as visual inspiration. These capabilities make it easier to create exactly the image you envision, helping you communicate more effectively through visuals and advancing image generation into a practical tool with precision and power.",,,Hosted access (no API),United States of America,GPT-4o,,,,,2025-05-30 14:48,,,,,,Industry,,,,,Unreleased,"""4o image generation rolls out starting today to Plus, Pro, Team, and Free users as the default image generator in ChatGPT, with access coming soon to Enterprise and Edu. Itâ€™s also available to use in Sora. For those who hold a special place in their hearts for DALLÂ·E, it can still be accessed through a dedicated DALLÂ·E GPT.

Developers will soon be able to generate images with GPTâ€‘4o via the API, with access rolling out in the next few weeks.""",Industry,,,,,,,,,
Llama Nemotron Nano 8B,Language,"Language modeling/generation,Question answering,Quantitative reasoning,Code generation,Neural Architecture Search - NAS",NVIDIA,"Akhiad Bercovich, Itay Levy, Izik Golan, Mohammad Dabbah, Ran El-Yaniv, Omri Puny, Ido Galil, Zach Moshe, Tomer Ronen, Najeeb Nabwani, Ido Shahaf, Oren Tropp, Ehud Karpas, Ran Zilberstein, Jiaqi Zeng, Soumye Singhal, Alexander Bukharin, Yian Zhang, Tugrul Konuk, Gerald Shen, Ameya Sunil Mahabaleshwarkar, Bilal Kartal, Yoshi Suhara, Olivier Delalleau, Zijia Chen, Zhilin Wang, David Mosallanezhad, Adi Renduchintala, Haifeng Qian, Dima Rekesh, Fei Jia, Somshubra Majumdar, Vahid Noroozi, Wasi Uddin Ahmad, Sean Narenthiran, Aleksander Ficek, Mehrzad Samadi, Jocelyn Huang, Siddhartha Jain, Igor Gitman, Ivan Moshkov, Wei Du, Shubham Toshniwal, George Armstrong, Branislav Kisacanin, Matvei Novikov, Daria Gitman, Evelina Bakhturina, Jane Polak Scowcroft, John Kamalu, Dan Su, Kezhi Kong, Markus Kliegl, Rabeeh Karimi, Ying Lin, Sanjeev Satheesh, Jupinder Parmar, Pritam Gundecha, Brandon Norick, Joseph Jennings, Shrimai Prabhumoye, Syeda Nahida Akter, Mostofa Patwary, Abhinav Khattar, Deepak Narayanan, Roger Waleffe, Jimmy Zhang, Bor-Yiing Su, Guyue Huang, Terry Kong, Parth Chadha, Sahil Jain, Christine Harvey, Elad Segal, Jining Huang, Sergey Kashirsky, Robert McQueen, Izzy Putterman, George Lam, Arun Venkatesan, Sherry Wu, Vinh Nguyen, Manoj Kilaru, Andrew Wang, Anna Warno, Abhilash Somasamudramath, Sandip Bhaskar, Maka Dong, Nave Assaf, Shahar Mor, Omer Ullman Argov, Scot Junkin, Oleksandr Romanenko, Pedro Larroy, Monika Katariya, Marco Rovinelli, Viji Balas, Nicholas Edelman, Anahita Bhiwandiwalla, Muthu Subramaniam et al. (32 additional authors not shown)",2025-03-18,Nano is 8B fine-tuned from Llama 3.1 8B for highest accuracy on PC and edge.,https://arxiv.org/abs/2505.00949,,,,8000000000.0,Dense decoder-only Transformer model,,,Llama Nemotron Post Training Dataset,"Data Collection for Training Datasets: Hybrid: Automated, Human, Synthetic",450000000000.0,"SFT:
""LN-Nano differently from other models below, undergoes a three-stage SFT pipeline with a global batch size of 256 using sequence packing with effective sequence length of 32k tokens. In the first stage, the model is fine-tuned exclusively on reasoning data from code, math, and science domains (Section 3.1) with a learning rate of 1eâˆ’4 for four epochs.""

for Super model (from the blog) ""60B tokens of synthetic data (representing 4M of the 30M generated samples)"" -> the entire dataset is ~450B tokens ",,,,,Likely,"Llama-3.1-Nemotron-Nano-8B-v1 is a large language model (LLM) which is a derivative of Meta Llama-3.1-8B-Instruct (AKA the reference model). It is a reasoning model that is post trained for reasoning, human chat preferences, and tasks, such as RAG and tool calling.

Llama-3.1-Nemotron-Nano-8B-v1 is a model which offers a great tradeoff between model accuracy and efficiency. It is created from Llama 3.1 8B Instruct and offers improvements in model accuracy. The model fits on a single RTX GPU and can be used locally. The model supports a context length of 128K.

This model underwent a multi-phase post-training process to enhance both its reasoning and non-reasoning capabilities. This includes a supervised fine-tuning stage for Math, Code, Reasoning, and Tool Calling as well as multiple reinforcement learning (RL) stages using REINFORCE (RLOO) and Online Reward-aware Preference Optimization (RPO) algorithms for both chat and instruction-following. The final model checkpoint is obtained after merging the final SFT and Online RPO checkpoints.",4.0,,Open weights (restricted use),United States of America,Llama 3.1-8B,8,6 FLOP / token / parameter * 8 * 10^9 parameters * 450 * 10^9 tokens * 4 epochs = 8.64e+22 FLOP,,,2025-05-05 14:54,,,,,,Industry,,,,,Unreleased,"GOVERNING TERMS: Your use of this model is governed by the NVIDIA Open Model License. Additional Information: Llama 3.1 Community License Agreement. Built with Llama.

""Models are commercially usable""

https://huggingface.co/nvidia/Llama-3.1-Nemotron-Nano-8B-v1",Industry,,,,,Operation counting,nvidia,,,
Llama Nemotron Ultra 253B,Language,"Language modeling/generation,Question answering,Quantitative reasoning,Code generation,Neural Architecture Search - NAS",NVIDIA,"Akhiad Bercovich, Itay Levy, Izik Golan, Mohammad Dabbah, Ran El-Yaniv, Omri Puny, Ido Galil, Zach Moshe, Tomer Ronen, Najeeb Nabwani, Ido Shahaf, Oren Tropp, Ehud Karpas, Ran Zilberstein, Jiaqi Zeng, Soumye Singhal, Alexander Bukharin, Yian Zhang, Tugrul Konuk, Gerald Shen, Ameya Sunil Mahabaleshwarkar, Bilal Kartal, Yoshi Suhara, Olivier Delalleau, Zijia Chen, Zhilin Wang, David Mosallanezhad, Adi Renduchintala, Haifeng Qian, Dima Rekesh, Fei Jia, Somshubra Majumdar, Vahid Noroozi, Wasi Uddin Ahmad, Sean Narenthiran, Aleksander Ficek, Mehrzad Samadi, Jocelyn Huang, Siddhartha Jain, Igor Gitman, Ivan Moshkov, Wei Du, Shubham Toshniwal, George Armstrong, Branislav Kisacanin, Matvei Novikov, Daria Gitman, Evelina Bakhturina, Jane Polak Scowcroft, John Kamalu, Dan Su, Kezhi Kong, Markus Kliegl, Rabeeh Karimi, Ying Lin, Sanjeev Satheesh, Jupinder Parmar, Pritam Gundecha, Brandon Norick, Joseph Jennings, Shrimai Prabhumoye, Syeda Nahida Akter, Mostofa Patwary, Abhinav Khattar, Deepak Narayanan, Roger Waleffe, Jimmy Zhang, Bor-Yiing Su, Guyue Huang, Terry Kong, Parth Chadha, Sahil Jain, Christine Harvey, Elad Segal, Jining Huang, Sergey Kashirsky, Robert McQueen, Izzy Putterman, George Lam, Arun Venkatesan, Sherry Wu, Vinh Nguyen, Manoj Kilaru, Andrew Wang, Anna Warno, Abhilash Somasamudramath, Sandip Bhaskar, Maka Dong, Nave Assaf, Shahar Mor, Omer Ullman Argov, Scot Junkin, Oleksandr Romanenko, Pedro Larroy, Monika Katariya, Marco Rovinelli, Viji Balas, Nicholas Edelman, Anahita Bhiwandiwalla, Muthu Subramaniam et al. (32 additional authors not shown)",2025-03-18,Ultra is 253B distilled from Llama 3.1 405B for maximum agentic accuracy on multi-GPU data center servers.,https://arxiv.org/abs/2505.00949,,,,253000000000.0,"253B
""Dense decoder-only Transformer model Network Architecture: Llama-3.1-405B-Instruct, customized through Neural Architecture Search (NAS)

**This model was developed based on Llama-3.1-405B-Instruct
** This model has 253B model parameters.""",3.911001e+25,"Total training compute: 3.8e+25 FLOP (base model) + 1.11e+24 FLOP (fine-tuning) = 3.9e25 FLOP
See calculation in the finetune compute notes.","Unspecified unreleased,Llama Nemotron Post Training Dataset",,603000000000.0,"KD + Continued Training: 
""LN-Ultra is first trained with knowledge distillation for 65B tokens using the same distillation dataset, followed by 88B tokens of continued training on the Nemotron-H phase 4 pretraining dataset (NVIDIA et al., 2025)."" (from the paper)

Reasoning training data (SFT): 
for Super model (from the blog) ""60B tokens of synthetic data (representing 4M of the 30M generated samples)"" -> the entire dataset is ~450B tokens (Ultra model is likely to be trained on the entire dataset (""Likely"" confidence}

65b+88b+450b = 603b tokens

RL for Scientific Reasoning:140k h100 hours (240k samples)

RL for instruction following: 30k prompts

RL for chat: 50k prompts",,,,,Likely,"Llama-3.1-Nemotron-Ultra-253B-v1 is a large language model (LLM) which is a derivative of Meta Llama-3.1-405B-Instruct (AKA the reference model). It is a reasoning model that is post trained for reasoning, human chat preferences, and tasks, such as RAG and tool calling. The model supports a context length of 128K tokens. This model fits on a single 8xH100 node for inference.

Llama-3.1-Nemotron-Ultra-253B-v1 is a model which offers a great tradeoff between model accuracy and efficiency. Efficiency (throughput) directly translates to savings. Using a novel Neural Architecture Search (NAS) approach, we greatly reduce the modelâ€™s memory footprint, enabling larger workloads, as well as reducing the number of GPUs required to run the model in a data center environment. This NAS approach enables the selection of a desired point in the accuracy-efficiency tradeoff. Furthermore, by using a novel method to vertically compress the model (see details here), it also offers a significant improvement in latency.

The model underwent a multi-phase post-training process to enhance both its reasoning and non-reasoning capabilities. This includes a supervised fine-tuning stage for Math, Code, Reasoning, Chat, and Tool Calling as well as multiple reinforcement learning (RL) stages using Group Relative Policy Optimization (GRPO) algorithms for reasoning, chat, and instruction-following.",,,Open weights (restricted use),United States of America,Llama 3.1-405B,1,"Knowledge Distillation + Continued pre-training + SFT: 

6 FLOP / parameter / token * 253000000000 parameters * 6033000000000 tokens [see dataset size notes] = 9.15354e+23 FLOP

RL: ""the whole training takes approximately 140k H100 hours""

989400000000000 FLOP / sec / GPU [bf16] * 140000 GPU-hours * 3600 sec / hour * 0.4 [assumed utilization] = 1.9946304e+23 FLOP

Total: 9.15354e+23 FLOP + 1.9946304e+23 FLOP = 1.114817e+24 FLOP
",,,2025-06-17 13:12,,,,,,Industry,,,,,Unreleased,"GOVERNING TERMS: Your use of this model is governed by the NVIDIA Open Model License. Additional Information: Llama 3.3 Community License Agreement. Built with Llama.

https://huggingface.co/nvidia/Llama-3_1-Nemotron-Ultra-253B-v1",Industry,,,,,Operation counting,nvidia,,,
Llama Nemotron Super 49B,Language,"Language modeling/generation,Question answering,Quantitative reasoning,Code generation,Neural Architecture Search - NAS",NVIDIA,"Akhiad Bercovich, Itay Levy, Izik Golan, Mohammad Dabbah, Ran El-Yaniv, Omri Puny, Ido Galil, Zach Moshe, Tomer Ronen, Najeeb Nabwani, Ido Shahaf, Oren Tropp, Ehud Karpas, Ran Zilberstein, Jiaqi Zeng, Soumye Singhal, Alexander Bukharin, Yian Zhang, Tugrul Konuk, Gerald Shen, Ameya Sunil Mahabaleshwarkar, Bilal Kartal, Yoshi Suhara, Olivier Delalleau, Zijia Chen, Zhilin Wang, David Mosallanezhad, Adi Renduchintala, Haifeng Qian, Dima Rekesh, Fei Jia, Somshubra Majumdar, Vahid Noroozi, Wasi Uddin Ahmad, Sean Narenthiran, Aleksander Ficek, Mehrzad Samadi, Jocelyn Huang, Siddhartha Jain, Igor Gitman, Ivan Moshkov, Wei Du, Shubham Toshniwal, George Armstrong, Branislav Kisacanin, Matvei Novikov, Daria Gitman, Evelina Bakhturina, Jane Polak Scowcroft, John Kamalu, Dan Su, Kezhi Kong, Markus Kliegl, Rabeeh Karimi, Ying Lin, Sanjeev Satheesh, Jupinder Parmar, Pritam Gundecha, Brandon Norick, Joseph Jennings, Shrimai Prabhumoye, Syeda Nahida Akter, Mostofa Patwary, Abhinav Khattar, Deepak Narayanan, Roger Waleffe, Jimmy Zhang, Bor-Yiing Su, Guyue Huang, Terry Kong, Parth Chadha, Sahil Jain, Christine Harvey, Elad Segal, Jining Huang, Sergey Kashirsky, Robert McQueen, Izzy Putterman, George Lam, Arun Venkatesan, Sherry Wu, Vinh Nguyen, Manoj Kilaru, Andrew Wang, Anna Warno, Abhilash Somasamudramath, Sandip Bhaskar, Maka Dong, Nave Assaf, Shahar Mor, Omer Ullman Argov, Scot Junkin, Oleksandr Romanenko, Pedro Larroy, Monika Katariya, Marco Rovinelli, Viji Balas, Nicholas Edelman, Anahita Bhiwandiwalla, Muthu Subramaniam et al. (32 additional authors not shown)",2025-03-18,Super is 49B distilled from Llama 3.3 70B for best accuracy with highest throughput on a data center GPU.,https://arxiv.org/abs/2505.00949,,,,49000000000.0,"The model is a derivative of Metaâ€™s Llama-3.3-70B-Instruct, using Neural Architecture Search (NAS). The NAS algorithm results in non-standard and non-repetitive blocks. This includes the following: Skip attention: In some blocks, the attention is skipped entirely, or replaced with a single linear layer. Variable FFN: The expansion/compression ratio in the FFN layer is different between blocks.

We utilize a block-wise distillation of the reference model, where for each block we create multiple variants providing different tradeoffs of quality vs. computational complexity, discussed in more depth below. We then search over the blocks to create a model which meets the required throughput and memory (optimized for a single H100-80GB GPU) while minimizing the quality degradation.",,,"FineWeb,Buzz-V1.2,Dolma,Llama Nemotron Post Training Dataset",,100000000000.0,"""The model then undergoes knowledge distillation (KD), with a focus on English single and multi-turn chat use-cases. The KD step included 40 billion tokens consisting of a mixture of 3 datasets - FineWeb, Buzz-V1.2 and Dolma.""

KD: ""LN-Super is trained for 40B tokens using a knowledge distillation objective over the Distillation Mix dataset introduced by Bercovich et al. (2024)."" (from the paper)

SFT: 60B tokens of NVIDIA generated synthetic data
(from the blog https://developer.nvidia.com/blog/build-enterprise-ai-agents-with-advanced-open-nvidia-llama-nemotron-reasoning-models/)

RL for Chat: 80K promts",,,,,Likely,"Llama-3.3-Nemotron-Super-49B-v1 is a large language model (LLM) which is a derivative of Meta Llama-3.3-70B-Instruct (AKA the reference model). It is a reasoning model that is post trained for reasoning, human chat preferences, and tasks, such as RAG and tool calling. The model supports a context length of 128K tokens.

Llama-3.3-Nemotron-Super-49B-v1 is a model which offers a great tradeoff between model accuracy and efficiency. Efficiency (throughput) directly translates to savings. Using a novel Neural Architecture Search (NAS) approach, we greatly reduce the modelâ€™s memory footprint, enabling larger workloads, as well as fitting the model on a single GPU at high workloads (H200). This NAS approach enables the selection of a desired point in the accuracy-efficiency tradeoff.

The model underwent a multi-phase post-training process to enhance both its reasoning and non-reasoning capabilities. This includes a supervised fine-tuning stage for Math, Code, Reasoning, and Tool Calling as well as multiple reinforcement learning (RL) stages using REINFORCE (RLOO) and Online Reward-aware Preference Optimization (RPO) algorithms for both chat and instruction-following. The final model checkpoint is obtained after merging the final SFT and Online RPO checkpoints.",,,Open weights (restricted use),United States of America,Llama 3.3 70B,2,"Knowledge Distillation + SFT:
6 FLOP / parameter / token * 49000000000 parameters * 100000000000 tokens = 2.94e+22 FLOP

RL compute unknown, likely to be negligible",,,2025-06-17 13:12,,,,,,Industry,,,,,Unreleased,"GOVERNING TERMS: Your use of this model is governed by the NVIDIA Open Model License. Additional Information: Llama 3.3 Community License Agreement. Built with Llama.

https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1",Industry,,,,,Operation counting,nvidia,,,
GR00T N1,"Robotics,Vision,Language",Robotic manipulation,NVIDIA,"Johan Bjorck, Fernando CastaÃ±eda, Nikita Cherniadev, Xingye Da, Runyu Ding, Linxi ""Jim"" Fan, Yu Fang, Dieter Fox, Fengyuan Hu, Spencer Huang, Joel Jang, Zhenyu Jiang, Jan Kautz, Kaushil Kundalia, Lawrence Lao, Zhiqi Li, Zongyu Lin, Kevin Lin, Guilin Liu, Edith Llontop, Loic Magne, Ajay Mandlekar, Avnish Narayan, Soroush Nasiriany, Scott Reed, You Liang Tan, Guanzhi Wang, Zu Wang, Jing Wang, Qi Wang, Jiannan Xiang, Yuqi Xie, Yinzhen Xu, Zhenjia Xu, Seonghyeon Ye, Zhiding Yu, Ao Zhang, Hao Zhang, Yizhou Zhao, Ruijie Zheng, Yuke Zhu",2025-03-18,GR00T N1: An Open Foundation Model for Generalist Humanoid Robots,"https://arxiv.org/abs/2503.14734v2
https://arxiv.org/abs/2503.14734v1
https://github.com/NVIDIA/Isaac-GR00T
https://nvidianews.nvidia.com/news/nvidia-isaac-gr00t-n1-open-humanoid-robot-foundation-model-simulation-frameworks
https://developer.nvidia.com/blog/accelerate-generalist-humanoid-robot-development-with-nvidia-isaac-gr00t-n1/
https://huggingface.co/nvidia/GR00T-N1-2B
https://huggingface.co/datasets/nvidia/PhysicalAI-Robotics-GR00T-X-Embodiment-Sim

",27.0,,Top10 recent paper from Sebastian Sartor 2025-05-14,2190000000.0,"""Specifically, our publicly released GR00T-N1-2B model has 2.2B parameters in total, with 1.34B in the VLM.""

https://huggingface.co/nvidia/GR00T-N1-2B
2.19e9 parameters, bf16, 4.38 GB",7.12368e+22,"""GR00T-N1-2B used roughly 50,000 H100 GPU hours for pretraining""
Assume H100 SXM5 variant, BF16, 0.4 utilization (NVIDIA in-house)
H100 SXM5 BF16 performance = 989400000000000 FLOP/s = 9.894e14 FLOP/s
(0.4 * 9.894e14 FLOP/s/GPU) * (3600 s/1 hr) * 50e3 GPU*hr = 7.12368e+22 FLOP",GR00T N1 [pre-train],"https://huggingface.co/datasets/nvidia/PhysicalAI-Robotics-GR00T-X-Embodiment-Sim
",592900000.0,"Table 7: Pre-training Dataset Statistics
Dataset | Length (Frames) | Duration (hr)
-------------------------------------------
Total robot data         | 262.3M | 3,288.8
Total human data       | 181.3M | 2,517.0
Total simulation data | 125.5M  | 1,742.6
Total neural data        | 23.8M   | 827.3
Total                           | 592.9M | 8,375.7

592.9e6 total frames",48.8,"""GR00T-N1-2B used roughly 50,000 H100 GPU hours for pretraining""
""We use up to 1024 GPUs for a single model.""

50e3 GPU*hr / 1024 GPU = 48.8 hr ~= 2.03 day

Table 6: Training hyperparameters.
Hyperparameter | Pre-training Value
---------------------------------------
Batch size | 16,384
Gradient steps | 200,000
Backboneâ€™s vision encoder | unfrozen
Backboneâ€™s text tokenizer | frozen
DiT | unfrozen

200e3 batches-seen * 16384 examples/batch = 3.2768e9 examples-seen
592.9e6 examples",NVIDIA H100 SXM5 80GB,,Confident,"General-purpose robots need a versatile body and an intelligent mind. Recent advancements in humanoid robots have shown great promise as a hardware platform for building generalist autonomy in the human world. A robot foundation model, trained on massive and diverse data sources, is essential for enabling the robots to reason about novel situations, robustly handle real-world variability, and rapidly learn new tasks. To this end, we introduce GR00T N1, an open foundation model for humanoid robots. GR00T N1 is a Vision-Language-Action (VLA) model with a dual-system architecture. The vision-language module (System 2) interprets the environment through vision and language instructions. The subsequent diffusion transformer module (System 1) generates fluid motor actions in real time. Both modules are tightly coupled and jointly trained end-to-end. We train GR00T N1 with a heterogeneous mixture of real-robot trajectories, human videos, and synthetically generated datasets. We show that our generalist robot model GR00T N1 outperforms the state-of-the-art imitation learning baselines on standard simulation benchmarks across multiple robot embodiments. Furthermore, we deploy our model on the Fourier GR-1 humanoid robot for language-conditioned bimanual manipulation tasks, achieving strong performance with high data efficiency.",,,Open weights (non-commercial),United States of America,Eagle 2,,see post-train dataset,1024.0,,2025-06-11 21:40,,,,16384.0,"Table 6: Training hyperparameters.
Hyperparameter | Pre-training Value
---------------------------------------
Batch size | 16,384",Industry,,,,,,"https://huggingface.co/nvidia/GR00T-N1-2B/blob/main/LICENSE
NVIDIA License
""3.3 Use Limitation. The Work and any derivative works thereof only may be used or intended for use non-commercially. Notwithstanding the foregoing, NVIDIA Corporation and its affiliates may use the Work and any derivative works commercially. As used herein, â€œnon-commerciallyâ€ means for research or evaluation purposes only.""

https://github.com/NVIDIA/Isaac-GR00T/blob/main/LICENSE
Apache License 2.0
This more permissive license only appears to apply to code used to conduct loading/inference/fine-tuning - not to the weights themselves.



",Industry,,,BF16,1406434.214264239,,,,,
Mistral Small 3.1,Language,"Language modeling/generation,Question answering,Quantitative reasoning,Code generation,Translation,Visual question answering",Mistral AI,,2025-03-17,Mistral Small 3.1: the best model in its weight class.,https://mistral.ai/news/mistral-small-3-1,,,,24000000000.0,24B,,At least 1.152e+24 FLOP (base model Mistral Small 3 training compute),Unspecified unreleased,,,,,,,,Confident,"Building on Mistral Small 3, this new model comes with improved text performance, multimodal understanding, and an expanded context window of up to 128k tokens. The model outperforms comparable models like Gemma 3 and GPT-4o Mini, while delivering inference speeds of 150 tokens per second.
Lightweight: Mistral Small 3.1 can run on a single RTX 4090 or a Mac with 32GB RAM. This makes it a great fit for on-device use cases.

Fast-response conversational assistance: Ideal for virtual assistants and other applications where quick, accurate responses are essential. 

Low-latency function calling: Capable of rapid function execution within automated or agentic workflows

Fine-tuning for specialized domains: Mistral Small 3.1 can be fine-tuned to specialize in specific domains, creating accurate subject matter experts. This is particularly useful in fields like legal advice, medical diagnostics, and technical support.

Foundation for advanced reasoning: We continue to be impressed by how the community builds on top of open Mistral models. Just in the last few weeks, we have seen several excellent reasoning models built on Mistral Small 3, such as the DeepHermes 24B by Nous Research. To that end, we are releasing both base and instruct checkpoints for Mistral Small 3.1 to enable further downstream customization of the model.

Mistral Small 3.1 can be used across various enterprise and consumer applications that require multimodal understanding, such as document verification, diagnostics, on-device image processing, visual inspection for quality checks, object detection in security systems, image-based customer support, and general purpose assistance.",,,Open weights (unrestricted),France,Mistral Small 3,,,,,2025-05-01 10:42,,,,,,Industry,,1.152e+24,,,Unreleased,"Apache 2.0

https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Base-2503",Industry,,,,,,mistralai,,,
EXAONE Deep 32B,Language,"Language modeling/generation,Question answering,Quantitative reasoning,Code generation",LG AI Research,"LG AI Research, Kyunghoon Bae, Eunbi Choi, Kibong Choi, Stanley Jungkyu Choi, Yemuk Choi, Seokhee Hong, Junwon Hwang, Hyojin Jeon, Kijeong Jeon, Gerrard Jeongwon Jo, Hyunjik Jo, Jiyeon Jung, Hyosang Kim, Joonkee Kim, Seonghwan Kim, Soyeon Kim, Sunkyoung Kim, Yireun Kim, Yongil Kim, Youchul Kim, Edward Hwayoung Lee, Haeju Lee, Honglak Lee, Jinsik Lee, Kyungmin Lee, Sangha Park, Yongmin Park, Sihoon Yang, Heuiyeen Yeen, Sihyuk Yi, Hyeongu Yun",2025-03-16,EXAONE Deep: LLMs with Enhanced Reasoning Performance,https://arxiv.org/abs/2503.12524,,Training cost,"512 H100 for 3 months

Math â€“ EXAONE Deep 32B Outperforms Competitors in High-Difficulty Math Benchmarks Even at Just 5% of Their Size

MMLU â€“ EXAONE Deep 32B Achieves 83.0 score, Proving the Best Performance Among Domestic Models",32000000000.0,32B,1.26e+24,"1.25 Ã— 10^24 (base model reported training compute) + 7.04 Ã— 10^21 (finetune compute) = 1.26 Ã— 10^24 FLOP

Table 1",Unspecified unreleased,,12000000000.0,"""To enhance the reasoning capabilities of language models, we have utilized 1.6M instances for SFT and 20K instances of preference data for DPO. The SFT dataset contains approximately 12B tokens""",2160.0,512 H100 GPUs were used for three months,NVIDIA H100 SXM5 80GB,,Confident,"We present EXAONE Deep series, which exhibits superior capabilities in various reasoning tasks, including math and coding benchmarks. We train our models mainly on the reasoning-specialized dataset that incorporates long streams of thought processes. Evaluation results show that our smaller models, EXAONE Deep 2.4B and 7.8B, outperform other models of comparable size, while the largest model, EXAONE Deep 32B, demonstrates competitive performance against leading open-weight models. All EXAONE Deep models are openly available for research purposes and can be downloaded from this https URL",,,Open weights (non-commercial),Korea (Republic of),EXAONE 3.5 32B,7,"Table 1 (reported): 7.04 Ã— 10^21 FLOP

6ND = 6*32B parameters * 12B tokens = 2.304e+21 FLOP",512.0,0.3156,2025-05-16 10:30,Google Cloud,,,,,Industry,,,,,Unreleased,"https://huggingface.co/LGAI-EXAONE/EXAONE-Deep-32B

Exaone License",Industry,,"512 H100 GPUs used for 3 months (pre-traning and fine-tuning). Model was trained in 16-bit precision.
https://www.wolframalpha.com/input?i=1.26e24+FLOP+%2F+%28512+*+989+TFLOPS+*+3+months%29",BF16,703248.4282353076,"Reported,Operation counting,Hardware",LGAI-EXAONE,,,
EXAONE Deep 7.8B,Language,"Language modeling/generation,Question answering,Quantitative reasoning,Code generation",LG AI Research,"LG AI Research, Kyunghoon Bae, Eunbi Choi, Kibong Choi, Stanley Jungkyu Choi, Yemuk Choi, Seokhee Hong, Junwon Hwang, Hyojin Jeon, Kijeong Jeon, Gerrard Jeongwon Jo, Hyunjik Jo, Jiyeon Jung, Hyosang Kim, Joonkee Kim, Seonghwan Kim, Soyeon Kim, Sunkyoung Kim, Yireun Kim, Yongil Kim, Youchul Kim, Edward Hwayoung Lee, Haeju Lee, Honglak Lee, Jinsik Lee, Kyungmin Lee, Sangha Park, Yongmin Park, Sihoon Yang, Heuiyeen Yeen, Sihyuk Yi, Hyeongu Yun",2025-03-16,EXAONE Deep: LLMs with Enhanced Reasoning Performance,https://arxiv.org/abs/2503.12524,,,,7800000000.0,7.8B,4.23e+23,"4.21 Ã— 10^23 (base model reported training compute) + 1.71 Ã— 10^21 (finetune compute) = 4.23 Ã— 10^23 FLOP

Table 1",Unspecified unreleased,,12000000000.0,"""To enhance the reasoning capabilities of language models, we have utilized 1.6M instances for SFT and 20K instances of preference data for DPO. The SFT dataset contains approximately 12B tokens""",,,NVIDIA H100 SXM5 80GB,,Confident,"We present EXAONE Deep series, which exhibits superior capabilities in various reasoning tasks, including math and coding benchmarks. We train our models mainly on the reasoning-specialized dataset that incorporates long streams of thought processes. Evaluation results show that our smaller models, EXAONE Deep 2.4B and 7.8B, outperform other models of comparable size, while the largest model, EXAONE Deep 32B, demonstrates competitive performance against leading open-weight models. All EXAONE Deep models are openly available for research purposes and can be downloaded from this https URL",,,Open weights (non-commercial),Korea (Republic of),EXAONE 3.5 7.8B,1,"Table 1 (reported): 1.71 Ã— 10^21 FLOP

6ND = 6*7.8B parameters * 12B tokens = 5.616e+20 FLOP",,,2025-05-16 10:30,Google Cloud,,,,,Industry,,,,,Unreleased,"https://huggingface.co/LGAI-EXAONE/EXAONE-Deep-7.8B

Exaone License",Industry,,,,,"Reported,Operation counting",LGAI-EXAONE,,,
EXAONE Deep 2.4B,Language,"Language modeling/generation,Question answering,Quantitative reasoning,Code generation",LG AI Research,"LG AI Research, Kyunghoon Bae, Eunbi Choi, Kibong Choi, Stanley Jungkyu Choi, Yemuk Choi, Seokhee Hong, Junwon Hwang, Hyojin Jeon, Kijeong Jeon, Gerrard Jeongwon Jo, Hyunjik Jo, Jiyeon Jung, Hyosang Kim, Joonkee Kim, Seonghwan Kim, Soyeon Kim, Sunkyoung Kim, Yireun Kim, Yongil Kim, Youchul Kim, Edward Hwayoung Lee, Haeju Lee, Honglak Lee, Jinsik Lee, Kyungmin Lee, Sangha Park, Yongmin Park, Sihoon Yang, Heuiyeen Yeen, Sihyuk Yi, Hyeongu Yun",2025-03-16,EXAONE Deep: LLMs with Enhanced Reasoning Performance,https://arxiv.org/abs/2503.12524,,,,2400000000.0,2.4B,9.41e+22,"9.36 Ã— 10^22 (base model reported training compute) + 5.27 Ã— 10^20 (finetune compute) = 9.41 Ã— 10^22 FLOP

Table 1",Unspecified unreleased,,12000000000.0,"""To enhance the reasoning capabilities of language models, we have utilized 1.6M instances for SFT and 20K instances of preference data for DPO. The SFT dataset contains approximately 12B tokens""",,,NVIDIA H100 SXM5 80GB,,Confident,"We present EXAONE Deep series, which exhibits superior capabilities in various reasoning tasks, including math and coding benchmarks. We train our models mainly on the reasoning-specialized dataset that incorporates long streams of thought processes. Evaluation results show that our smaller models, EXAONE Deep 2.4B and 7.8B, outperform other models of comparable size, while the largest model, EXAONE Deep 32B, demonstrates competitive performance against leading open-weight models. All EXAONE Deep models are openly available for research purposes and can be downloaded from this https URL",,,Open weights (non-commercial),Korea (Republic of),EXAONE 3.5 2.4B,527000000000000000000,"Table 1 (reported): 5.27 Ã— 10^20 FLOP

6ND = 6*2.4B parameters * 12B tokens = 1.728e+20 FLOP",,,2025-05-01 10:42,Google Cloud,,,,,Industry,,,,,Unreleased,"https://huggingface.co/LGAI-EXAONE/EXAONE-Deep-2.4B

Exaone License",Industry,,,,,"Reported,Operation counting",LGAI-EXAONE,,,
ERNIE 4.5 (æ–‡å¿ƒå¤§æ¨¡åž‹4.5),"Multimodal,Language,Vision","Language modeling/generation,Visual question answering,Video description,Speech recognition,Quantitative reasoning,Code generation,Translation",Baidu,,2025-03-16,"Baidu's ERNIE 4.5 & X1: Features, Access, DeepSeek Comparison
",https://www.datacamp.com/blog/ernie-4-5-x1,,SOTA improvement,"ERNIE 4.5 surpassed GPT-4o in six out of seven evaluated benchmarks:

CCBench: Evaluates common-sense reasoning across text and images. ERNIE 4.5 scored approximately 81, slightly outperforming GPT-4oâ€™s ~79.
OCRBench: Assesses optical character recognition capabilities, focusing on text extraction from images. ERNIE 4.5 achieved around 88, surpassing GPT-4oâ€™s ~81.
ChartQA: Tests understanding of data presented in charts. ERNIE 4.5 scored ~82, marginally ahead of GPT-4oâ€™s ~81.
MMMU: Measures multimodal reasoning across various topics. Here, GPT-4o led with ~70, while ERNIE 4.5 scored ~64, indicating an area for improvement.
MathVista: Evaluates mathematical reasoning in visual contexts. ERNIE 4.5 scored ~69, outperforming GPT-4oâ€™s ~61.
DocVQA: Assesses the ability to answer questions based on document visuals. ERNIE 4.5 excelled with a score of ~91, compared to GPT-4oâ€™s ~85.
MVBench: Focuses on temporal understanding in dynamic video tasks, requiring reasoning over sequences of frames. ERNIE 4.5 scored ~72, significantly outperforming GPT-4oâ€™s ~63.
Text-only benchmarks
On text-only tasks, ERNIE 4.5 achieved an average score of 79.6, slightly ahead of GPT-4.5â€™s average of 79.14, and also surpassing DeepSeek-V3 (~77).",,,,,,,,,,,,,Unknown,,,,API access,China,,,,,,2025-05-28 17:30,,,,,,Industry,,,,,Unreleased,"""The language model is now accessible via APIs on Baidu AI Cloudâ€™s Qianfan platform, making it available for enterprise users and developers.

Also, Baiduâ€™s conversational AI platform, ERNIE Bot, is now freely accessible to all users, ahead of schedule.""",Industry,,,,,,,,,
EXAONE 3.5-R 2.4B,Language,"Language modeling/generation,Question answering,Translation",LG AI Research,,2025-03-14,,,,,,2400000000.0,2.4B,9.504e+22,9.36 Ã— 10^22 (base model reported training compute) + 1.44 Ã— 10^21 (finetune compute) = 9.504e+22 FLOP,,,,,,,,,Confident,,,,Unreleased,Korea (Republic of),EXAONE 3.5 2.4B,1,1.44e21,,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,,Industry,,,,,Reported,,,,
EXAONE 3.5-R 32B,Language,"Language modeling/generation,Question answering,Translation",LG AI Research,,2025-03-14,,,,,,32000000000.0,32B,1.2692e+24,1.25 Ã— 10^24 (base model reported training compute) + 1.92 Ã— 10^22 (finetune compute) = 1.2692e+24 FLOP,,,,,,,,,Confident,,,,Unreleased,Korea (Republic of),EXAONE 3.5 32B,1,1.92e22,,,2025-05-16 10:30,,,,,,Industry,,,,,Unreleased,,Industry,,,,,Reported,,,,
EXAONE 3.5-R 7.8B,Language,"Language modeling/generation,Question answering,Translation",LG AI Research,,2025-03-14,,,,,,7800000000.0,7.8B,4.2568e+23,4.21 Ã— 10^23 (base model reported training compute) + 4.68 Ã— 10^21 (finetune compute) = 4.2568e+23 FLOP,,,,,,,,,Confident,,,,Unreleased,Korea (Republic of),EXAONE 3.5 7.8B,4,4.68e21,,,2025-05-16 10:30,,,,,,Industry,,,,,Unreleased,,Industry,,,,,Reported,,,,
Meissonic,Image generation,"Text-to-image,Image generation","National University of Singapore,Skywork AI,Hong Kong University of Science and Technology (HKUST),University of California (UC) Berkeley,Zhejiang University (ZJU)","Jinbin Bai, Tian Ye, Wei Chow, Enxin Song, Xiangtai Li, Zhen Dong, Lei Zhu, Shuicheng Yan",2025-03-13,Meissonic: Revitalizing Masked Generative Transformers for Efficient High-Resolution Text-to-Image Synthesis,https://arxiv.org/abs/2410.08261,,,,1000000000.0,1B (Table 1),1.2292301e+21,"Table 1: equivalent to 19 days on 8 A100 GPUs using fp16 precision (same as 48 H100 GPU days)

312*10^12 FLOP / sec / GPU * 8 GPUs * 19 days * 24 hours / day * 3600 sec / hour * 0.3 [assumed utilization] = 1.2292301e+21 FLOP",Unspecified unreleased,""". We curated a high-quality internal dataset with accurate captions, which, combined with our training strategy, significantly improved the generative capabilities of the base model.""",210000000.0,"[IMAGES] 

210M of training images (table 1)

First, we train Meissonic-256 with a batch size of 2,048 for 100,000 steps. Second, we continue training Meissonic-512 with a batch size of 512 for an additional 100,000 steps. Third, we continue training Meissonic with a batch size of 256 for 42,000 steps with a resolution of 1024 Ã— 1024.",,"""48 H100 days"" (Table 1)

48*24 = 1152 GPU-hours",NVIDIA H100 SXM5 80GB,,Confident,"We present Meissonic, which elevates non-autoregressive masked image modeling (MIM) text-to-image to a level comparable with state-of-the-art diffusion models like SDXL. By incorporating a comprehensive suite of architectural innovations, advanced positional encoding strategies, and optimized sampling conditions, Meissonic substantially improves MIM's performance and efficiency. Additionally, we leverage high-quality training data, integrate micro-conditions informed by human preference scores, and employ feature compression layers to further enhance image fidelity and resolution. Our model not only matches but often exceeds the performance of existing models like SDXL in generating high-quality, high-resolution images. Extensive experiments validate Meissonic's capabilities, demonstrating its potential as a new standard in text-to-image synthesis. We release a model checkpoint capable of producing  resolution images.",,,Open weights (unrestricted),"Singapore,Singapore,Hong Kong,China,United States of America,China",CLIP ViT-H/14 - LAION-2B,,,,,2025-05-14 14:05,,,,,,"Academia,Industry,Academia,Academia,Academia",,,,1152.0,Open source,"Apache 2.0

https://huggingface.co/MeissonFlow/Meissonic

https://github.com/viiika/Meissonic","Academia,Industry,Academia,Academia,Academia",,,BF16,,Hardware,MeissonFlow,,,
Gemma 3 27B,"Language,Vision,Multimodal","Language modeling/generation,Question answering,Translation,Chat,Quantitative reasoning,Visual question answering,Code generation",Google DeepMind,"Core contributors: Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre RamÃ©, Morgane RiviÃ¨re, Louis Rouillard, Thomas Mesnard, Geoffrey Cideron, Jean-bastien Grill, Sabela Ramos, Edouard Yvinec, Michelle Casbon, Etienne Pot, Ivo Penchev, GaÃ«l Liu, Francesco Visin, Kathleen Kenealy, Lucas Beyer, Xiaohai Zhai, Anton Tsitsulin, Robert Busa-Fekete, Alex Feng, Noveen Sachdeva, Benjamin Coleman, Yi Gao, Basil Mustafa, Iain Barr, Emilio Parisotto, David Tian, Matan Eyal, Colin Cherry, Jan-Thorsten Peter, Danila Sinopalnikov, Surya Bhupatiraju, Rishabh Agarwal, Mehran Kazemi, Dan Malkin, David Vilar, Idan Brusilovsky, Jiaming Luo, Andreas Steiner",2025-03-12,"Gemma 3 Technical Report
",https://storage.googleapis.com/deepmind-media/gemma/Gemma3Report.pdf,,,,27000000000.0,"Vision Encoder: 417M
Embedding Parameters:  1,416M
Non-embedding Parameters:  25,600M
",2.268e+24,6ND =  6 * 27B parameters * 14T training tokens = 2.268 Ã— 10^24 FLOP,Unspecified unreleased,,14000000000000.0,14T,,,Google TPU v5p,,Confident,"We introduce Gemma 3, a multimodal addition to the Gemma family of lightweight open models, ranging in scale from 1 to 27 billion parameters. This version introduces vision understanding abilities, a wider coverage of languages and longer context â€“ at least 128K tokens. We also change the architecture of the model to reduce the KV-cache memory that tends to explode with long context. This is achieved by increasing the ratio of local to global attention layers, and keeping the span on local attention short.
The Gemma 3 models are trained with distillation and achieve superior performance to Gemma 2 for both pre-trained and instruction finetuned versions. In particular, our novel post-training recipe significantly improves the math, chat, instruction-following and multilingual abilities, making Gemma3-4B-IT competitive with Gemma2-27B-IT and Gemma3-27B-IT comparable to Gemini-1.5-Pro across benchmarks. We release all our models to the community.",,,Open weights (restricted use),"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France",SigLIP 400M,,,6144.0,,2025-05-29 15:11,,,,,,Industry,,,,,Unreleased,"https://huggingface.co/google/gemma-3-27b-it
Gemma License",Industry,,,,,Operation counting,google,,,
Gemma 3 12B,"Language,Vision,Multimodal","Language modeling/generation,Question answering,Translation,Chat,Quantitative reasoning,Visual question answering,Code generation",Google DeepMind,"Core contributors: Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre RamÃ©, Morgane RiviÃ¨re, Louis Rouillard, Thomas Mesnard, Geoffrey Cideron, Jean-bastien Grill, Sabela Ramos, Edouard Yvinec, Michelle Casbon, Etienne Pot, Ivo Penchev, GaÃ«l Liu, Francesco Visin, Kathleen Kenealy, Lucas Beyer, Xiaohai Zhai, Anton Tsitsulin, Robert Busa-Fekete, Alex Feng, Noveen Sachdeva, Benjamin Coleman, Yi Gao, Basil Mustafa, Iain Barr, Emilio Parisotto, David Tian, Matan Eyal, Colin Cherry, Jan-Thorsten Peter, Danila Sinopalnikov, Surya Bhupatiraju, Rishabh Agarwal, Mehran Kazemi, Dan Malkin, David Vilar, Idan Brusilovsky, Jiaming Luo, Andreas Steiner",2025-03-12,"Gemma 3 Technical Report
",https://storage.googleapis.com/deepmind-media/gemma/Gemma3Report.pdf,,,,12000000000.0,"Vision Encoder: 417M
Embedding Parameters: 1,012M
Non-embedding Parameters: 10,759M",8.64e+23,6ND = 6 * 12B parameters * 12T training tokens = 8.64 Ã— 10^23 FLOP,Unspecified unreleased,,12000000000000.0,12T,,,Google TPU v4,,Confident,"We introduce Gemma 3, a multimodal addition to the Gemma family of lightweight open models, ranging in scale from 1 to 27 billion parameters. This version introduces vision understanding abilities, a wider coverage of languages and longer context â€“ at least 128K tokens. We also change the architecture of the model to reduce the KV-cache memory that tends to explode with long context. This is achieved by increasing the ratio of local to global attention layers, and keeping the span on local attention short.
The Gemma 3 models are trained with distillation and achieve superior performance to Gemma 2 for both pre-trained and instruction finetuned versions. In particular, our novel post-training recipe significantly improves the math, chat, instruction-following and multilingual abilities, making Gemma3-4B-IT competitive with Gemma2-27B-IT and Gemma3-27B-IT comparable to Gemini-1.5-Pro across benchmarks. We release all our models to the community.",,,Open weights (restricted use),"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France",SigLIP 400M,,,6144.0,,2025-05-16 10:30,,,,,,Industry,,,,,Unreleased,"https://huggingface.co/google/gemma-3-12b-it
Gemma License",Industry,,,,2049649.4174839524,Operation counting,google,,,
Gemma 3 4B,"Language,Vision,Multimodal","Language modeling/generation,Question answering,Translation,Chat,Quantitative reasoning,Visual question answering,Code generation",Google DeepMind,"Core contributors: Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre RamÃ©, Morgane RiviÃ¨re, Louis Rouillard, Thomas Mesnard, Geoffrey Cideron, Jean-bastien Grill, Sabela Ramos, Edouard Yvinec, Michelle Casbon, Etienne Pot, Ivo Penchev, GaÃ«l Liu, Francesco Visin, Kathleen Kenealy, Lucas Beyer, Xiaohai Zhai, Anton Tsitsulin, Robert Busa-Fekete, Alex Feng, Noveen Sachdeva, Benjamin Coleman, Yi Gao, Basil Mustafa, Iain Barr, Emilio Parisotto, David Tian, Matan Eyal, Colin Cherry, Jan-Thorsten Peter, Danila Sinopalnikov, Surya Bhupatiraju, Rishabh Agarwal, Mehran Kazemi, Dan Malkin, David Vilar, Idan Brusilovsky, Jiaming Luo, Andreas Steiner",2025-03-12,"Gemma 3 Technical Report
",https://storage.googleapis.com/deepmind-media/gemma/Gemma3Report.pdf,,,,4000000000.0,"Vision Encoder: 417M
Embedding Parameters: 675M
Non-embedding Parameters: 3,209M",9.6e+22,6ND = 6 * 4B parameters * 4T training tokens = 9.6 Ã— 10^22 FLOP,Unspecified unreleased,,4000000000000.0,4T,,,Google TPU v5e,,Confident,"We introduce Gemma 3, a multimodal addition to the Gemma family of lightweight open models, ranging in scale from 1 to 27 billion parameters. This version introduces vision understanding abilities, a wider coverage of languages and longer context â€“ at least 128K tokens. We also change the architecture of the model to reduce the KV-cache memory that tends to explode with long context. This is achieved by increasing the ratio of local to global attention layers, and keeping the span on local attention short.
The Gemma 3 models are trained with distillation and achieve superior performance to Gemma 2 for both pre-trained and instruction finetuned versions. In particular, our novel post-training recipe significantly improves the math, chat, instruction-following and multilingual abilities, making Gemma3-4B-IT competitive with Gemma2-27B-IT and Gemma3-27B-IT comparable to Gemini-1.5-Pro across benchmarks. We release all our models to the community.",,,Open weights (restricted use),"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France",SigLIP 400M,,,2048.0,,2025-05-30 14:46,,,,,,Industry,,,,,Unreleased,"https://huggingface.co/google/gemma-3-4b-it
Gemma License",Industry,,,,562648.8597014771,Operation counting,google,,,
Gemma 3 1B,Language,"Language modeling/generation,Question answering,Translation,Chat,Quantitative reasoning,Code generation",Google DeepMind,"Core contributors: Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre RamÃ©, Morgane RiviÃ¨re, Louis Rouillard, Thomas Mesnard, Geoffrey Cideron, Jean-bastien Grill, Sabela Ramos, Edouard Yvinec, Michelle Casbon, Etienne Pot, Ivo Penchev, GaÃ«l Liu, Francesco Visin, Kathleen Kenealy, Lucas Beyer, Xiaohai Zhai, Anton Tsitsulin, Robert Busa-Fekete, Alex Feng, Noveen Sachdeva, Benjamin Coleman, Yi Gao, Basil Mustafa, Iain Barr, Emilio Parisotto, David Tian, Matan Eyal, Colin Cherry, Jan-Thorsten Peter, Danila Sinopalnikov, Surya Bhupatiraju, Rishabh Agarwal, Mehran Kazemi, Dan Malkin, David Vilar, Idan Brusilovsky, Jiaming Luo, Andreas Steiner",2025-03-12,"Gemma 3 Technical Report
",https://storage.googleapis.com/deepmind-media/gemma/Gemma3Report.pdf,,,,1000000000.0,"Embedding Parameters: 302M
Non-embedding Parameters: 698M",1.2e+22,6ND = 6 * 1B parameters * 2T training tokens = 1.2 Ã— 10^22 FLOP,Unspecified unreleased,,2000000000000.0,2T,,,Google TPU v5e,,Confident,"We introduce Gemma 3, a multimodal addition to the Gemma family of lightweight open models, ranging in scale from 1 to 27 billion parameters. This version introduces vision understanding abilities, a wider coverage of languages and longer context â€“ at least 128K tokens. We also change the architecture of the model to reduce the KV-cache memory that tends to explode with long context. This is achieved by increasing the ratio of local to global attention layers, and keeping the span on local attention short.
The Gemma 3 models are trained with distillation and achieve superior performance to Gemma 2 for both pre-trained and instruction finetuned versions. In particular, our novel post-training recipe significantly improves the math, chat, instruction-following and multilingual abilities, making Gemma3-4B-IT competitive with Gemma2-27B-IT and Gemma3-27B-IT comparable to Gemini-1.5-Pro across benchmarks. We release all our models to the community.",,,Open weights (restricted use),"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France",SigLIP 400M,,,512.0,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,"https://huggingface.co/google/gemma-3-1b-it
Gemma License",Industry,,,,140662.21492536928,Operation counting,google,,,
Gemini Robotics,"Robotics,Vision,Speech","Instruction interpretation,Robotic manipulation,Speech recognition,Object recognition,Object detection",Google DeepMind,,2025-03-12,Gemini Robotics brings AI into the physical world,https://storage.googleapis.com/deepmind-media/gemini-robotics/gemini_robotics_report.pdf,,,,,,,,Unspecified unreleased,"""We collected a large-scale teleoperated robot action dataset on a fleet of ALOHA 2 robots (Team et al., 2024; Zhao et al., 2025) over 12 months, which consists of thousands of hours of real-world expert robot demonstrations. This dataset contains thousands of diverse tasks, covering scenarios with varied manipulation skills, objects, task difficulties, episode horizons, and dexterity requirements.
The training data further includes non-action data such as web documents, code, multi-modal content (image, audio, video), and embodied reasoning and visual question answering data.""",,,,,"Google TPU v4,Google TPU v5e",,Unknown,"Gemini Robotics, an advanced vision-language-action (VLA) model that was built on Gemini 2.0 with the addition of physical actions as a new output modality for the purpose of directly controlling robots. 

Gemini Robotics is a state-of-the-art vision-language-action model enabling general-purpose robotic manipulation on different tasks, scenes,
and across multiple robots.

Input(s) The models take text (e.g., a question or prompt or numerical coordinates) and images (e.g., robotâ€™s scene or environment) as input.

Output(s) Gemini Robotics generates text about robot actions
in response to the input.",,,Unreleased,"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France",Gemini Robotics-ER,,,,,2025-05-21 11:39,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
Gemini Robotics-ER,"Robotics,Vision,Speech","Instruction interpretation,Robotic manipulation,Speech recognition,Object recognition,Object detection",Google DeepMind,,2025-03-12,Gemini Robotics brings AI into the physical world,https://storage.googleapis.com/deepmind-media/gemini-robotics/gemini_robotics_report.pdf,,,,,,,,Unspecified unreleased,"""We collected a large-scale teleoperated robot action dataset on a fleet of ALOHA 2 robots (Team et al., 2024; Zhao et al., 2025) over 12 months, which consists of thousands of hours of real-world expert robot demonstrations. This dataset contains thousands of diverse tasks, covering scenarios with varied manipulation skills, objects, task difficulties, episode horizons, and dexterity requirements.
The training data further includes non-action data such as web documents, code, multi-modal content (image, audio, video), and embodied reasoning and visual question answering data.""",,,,,"Google TPU v4,Google TPU v5e",,Unknown,"Alongside Gemini Robotics, weâ€™re introducing an advanced vision-language model called Gemini Robotics-ER (short for â€˜â€œembodied reasoningâ€). This model enhances Geminiâ€™s understanding of the world in ways necessary for robotics, focusing especially on spatial reasoning, and allows roboticists to connect it with their existing low level controllers.

Gemini Robotics-ER improves Gemini 2.0â€™s existing abilities like pointing and 3D detection by a large margin. Combining spatial reasoning and Geminiâ€™s coding abilities, Gemini Robotics-ER can instantiate entirely new capabilities on the fly. For example, when shown a coffee mug, the model can intuit an appropriate two-finger grasp for picking it up by the handle and a safe trajectory for approaching it.

Gemini Robotics-ER can perform all the steps necessary to control a robot right out of the box, including perception, state estimation, spatial understanding, planning and code generation. In such an end-to-end setting the model achieves a 2x-3x success rate compared to Gemini 2.0. And where code generation is not sufficient, Gemini Robotics-ER can even tap into the power of in-context learning, following the patterns of a handful of human demonstrations to provide a solution.",,,Unreleased,"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France",Gemini 2.0 Flash,,,,,2025-05-21 11:38,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
Hunyuan-TurboS,Language,"Language modeling/generation,Quantitative reasoning,Question answering,Code generation,Text summarization",Tencent,,2025-03-11,"Tencent HunYuan Turbo S: The fastest reasoning LLM
At par with DeepSeek, Claude 3.5 and GPT-4o","https://web.archive.org/web/20250408105622/https://www.dapingtime.com/article/2171.html

https://medium.com/data-science-in-your-pocket/tencent-hunyuan-turbo-s-the-fastest-reasoning-llm-d64a02bed5c8",,SOTA improvement,"Based on the photos published by Tencent on X, it achieved top performance on several benchmarks - MMLU, C-Eval, MATH. (https://x.com/TencentHunyuan/status/1899105803073958010/photo/1)",,,,,,,,,,,,,Unknown,"Fast + Slow Thinking
Hunyuan Turbo S incorporates a unique approach inspired by human cognitive processes â€” fast thinking and slow thinking â€” to optimize response efficiency and reasoning depth.
Fast Thinking: This is like human intuition â€” it allows for instant responses to straightforward or common queries without requiring deep analysis. Turbo S achieves this by doubling word speed and reducing first-word latency by 44%, making it highly efficient for general conversations and quick interactions.
Slow Thinking: Inspired by analytical reasoning, slow thinking is necessary for complex problem-solving, especially in math, logical reasoning, and science-related queries. Turbo S borrows knowledge from Hunyuan T1, Tencentâ€™s slow-thinking model, which was trained using a technique called long-thinking chain synthesis. This helps Turbo S reason through multi-step problems while maintaining its speed advantage.
Result: By combining both, Turbo S matches or exceeds models like GPT-4o and Claude 3.5 in reasoning-heavy tasks without compromising speed.",,,API access,China,,,,,,2025-06-02 12:38,,,,,,Industry,,,,,Unreleased,API via Tencent Cloud,Industry,,,,,,,,,
Reka Flash 3,Multimodal,"Chat,Code generation",Reka AI,,2025-03-10,Reasoning with Reka Flash 3,"https://huggingface.co/RekaAI/reka-flash-3
https://www.reka.ai/news/introducing-reka-flash",,,,21000000000.0,,,,,"This model was pretrained from scratch on a diverse set of publicly accessible and synthetic datasets. We instruction-tuned the base model on curated, high-quality data to optimize its performance.",,,,,,,Confident,,,,,United States of America,,,,,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
QwQ-32B,Language,"Language modeling/generation,Question answering,Quantitative reasoning,Code generation",Alibaba,Qwen Team,2025-03-06,QwQ-32B: Embracing the Power of Reinforcement Learning,https://qwenlm.github.io/blog/qwq-32b/,,SOTA improvement,Blog (https://qwenlm.github.io/blog/qwq-32b-preview/) lists AIME and MATH-500 scores superior to o1-preview,32500000000.0,"Architecture: transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias
Number of Parameters: 32.5B
Number of Paramaters (Non-Embedding): 31.0B
Number of Layers: 64
Number of Attention Heads (GQA): 40 for Q and 8 for KV",3.51e+24,"Assuming the same dataset size as for Qwen2.5 training (18T tokens):

6ND = 6 * 32500000000 parameters * 18 * 10^12 tokens =  3.51 Ã— 10^24

'Speculative' confidence",Unspecified unreleased,"Knowledge cutoff date is November 28, 2024, according to https://llm-stats.com/models/qwq-32b.",,Speculatively: might be similar to Qwen2.5 models (18T tokens),,,,,Speculative,"QwQ is the reasoning model of the Qwen series. Compared with conventional instruction-tuned models, QwQ, which is capable of thinking and reasoning, can achieve significantly enhanced performance in downstream tasks, especially hard problems. QwQ-32B is the medium-sized reasoning model, which is capable of achieving competitive performance against state-of-the-art reasoning models, e.g., DeepSeek-R1, o1-mini.",,,Open weights (unrestricted),China,Qwen2.5-Coder (32B),,,,,2025-05-29 15:12,,,,,,Industry,,,,,Unreleased,"https://huggingface.co/Qwen/QwQ-32B
Apache 2",Industry,,,BF16,,,Qwen,,,
Mistral OCR,Multimodal,"Character recognition,Chat",Mistral AI,,2025-03-06,"Mistral OCR
",https://mistral.ai/news/mistral-ocr,,SOTA improvement,Mistral OCR has consistently outperformed other leading OCR models in rigorous benchmark tests.,,,,,,,,,,,,,Unknown,"Mistral OCR is an Optical Character Recognition API that sets a new standard in document understanding. Unlike other models, Mistral OCR comprehends each element of documentsâ€”media, text, tables, equationsâ€”with unprecedented accuracy and cognition. It takes images and PDFs as input and extracts content in an ordered interleaved text and images.",,,API access,France,,,,,,2025-06-06 12:47,,,,,,Industry,,,,,Unreleased,available on la Plateforme,Industry,,,,,,,,,
Phi-4 Mini,Language,"Language modeling/generation,Visual question answering,Code generation,Quantitative reasoning,Translation",Microsoft,"Abdelrahman Abouelenin, Atabak Ashfaq, Adam Atkinson, Hany Awadalla, Nguyen Bach, Jianmin Bao, Alon Benhaim, Martin Cai, Vishrav Chaudhary, Congcong Chen, Dong Chen, Dongdong Chen, Junkun Chen, Weizhu Chen, Yen-Chun Chen, Yi-ling Chen, Qi Dai, Xiyang Dai, Ruchao Fan, Mei Gao, Min Gao, Amit Garg, Abhishek Goswami, Junheng Hao, Amr Hendy, Yuxuan Hu, Xin Jin, Mahmoud Khademi, Dongwoo Kim, Young Jin Kim, Gina Lee, Jinyu Li, Yunsheng Li, Chen Liang, Xihui Lin, Zeqi Lin, Mengchen Liu, Yang Liu, Gilsinia Lopez, Chong Luo, Piyush Madan, Vadim Mazalov, Ali Mousavi, Anh Nguyen, Jing Pan, Daniel Perez-Becker, Jacob Platin, Thomas Portet, Kai Qiu, Bo Ren, Liliang Ren, Sambuddha Roy, Ning Shang, Yelong Shen, Saksham Singhal, Subhojit Som, Xia Song, Tetyana Sych, Praneetha Vaddamanu, Shuohang Wang, Yiming Wang, Zhenghao Wang, Haibin Wu, Haoran Xu, Weijian Xu, Yifan Yang, Ziyi Yang, Donghan Yu, Ishmam Zabir, Jianwen Zhang, Li Lyna Zhang, Yunan Zhang, Xiren Zhou",2025-03-03,Phi-4-Mini Technical Report: Compact yet Powerful Multimodal Language Models via Mixture-of-LoRAs,https://arxiv.org/abs/2503.01743,,,,3800000000.0,"3.8-billion
dense decoder-only Transformer model",9.9561596e+22,"6ND = 6 FLOP / token / parameter * 3800000000 parameters * 5000000000000 tokens = 1.14e+23 FLOP

512 GPUs * 312000000000000 FLOP / sec * 504 hours * 3600 sec / hour * 0.3 [assumed utilization] = 8.6951854e+22 FLOP

geometric mean: 
sqrt(1.14e+23*8.6951854e+22) = 9.9561596e+22",Unspecified unreleased,"Phi-4-miniâ€™s training data includes a wide variety of sources, totaling 5 trillion tokens, and is a combination of

1. publicly available documents filtered for quality, selected high-quality educational data, and code
2. newly created synthetic, â€œtextbook-likeâ€ data for the purpose of teaching math, coding, common sense reasoning, general knowledge of the world (e.g., science, daily activities, theory of mind, etc.)
3. high quality chat format supervised data covering various topics to reflect human preferences on different aspects such as instruct-following, truthfulness, honesty and helpfulness. Focus was placed on the quality of data that could potentially improve the reasoning ability for the model, and the publicly available documents were filtered to contain a preferred level of knowledge. As an example, the result of a game in premier league on a particular day might be good training data for frontier models, but such information was removed to leave more model capacity for reasoning for the modelâ€™s small size. More details about data can be found in the Phi-4-mini-instruct technical report.
Knowledge cutoff date is February 2025, according to https://huggingface.co/microsoft/Phi-4-mini-reasoning.",5000000000000.0,"""With these techniques, we built the 5 trillion pre-training data corpus""",504.0,21 days * 24 hours / day = 504 hours,NVIDIA A100 SXM4 80 GB,,Confident,"We introduce Phi-4-Mini and Phi-4-Multimodal, compact yet highly capable language and multimodal models. Phi-4-Mini is a 3.8-billion-parameter language model trained on high-quality web and synthetic data, significantly outperforming recent open-source models of similar size and matching the performance of models twice its size on math and coding tasks requiring complex reasoning. This achievement is driven by a carefully curated synthetic data recipe emphasizing high-quality math and coding datasets. Compared to its predecessor, Phi-3.5-Mini, Phi-4-Mini features an expanded vocabulary size of 200K tokens to better support multilingual applications, as well as group query attention for more efficient long-sequence generation. Phi-4-Multimodal is a multimodal model that integrates text, vision, and speech/audio input modalities into a single model. Its novel modality extension approach leverages LoRA adapters and modality-specific routers to allow multiple inference modes combining various modalities without interference. For example, it now ranks first in the OpenASR leaderboard to date, although the LoRA component of the speech/audio modality has just 460 million parameters. Phi-4-Multimodal supports scenarios involving (vision + language), (vision + speech), and (speech/audio) inputs, outperforming larger vision-language and speech-language models on a wide range of tasks. Additionally, we experiment to further train Phi-4-Mini to enhance its reasoning capabilities. Despite its compact 3.8-billion-parameter size, this experimental version achieves reasoning performance on par with or surpassing significantly larger models, including DeepSeek-R1-Distill-Qwen-7B and DeepSeek-R1-Distill-Llama-8B.",,,Open weights (unrestricted),"United States of America,Multinational,India,Belgium",,,,512.0,,2025-05-30 14:46,,,,,,Industry,,,,,Unreleased,"the instruct model is under MIT license on hugging face 
https://huggingface.co/microsoft/Phi-4-mini-instruct",Industry,,,,401972.5996644288,"Operation counting,Hardware",microsoft,,,
Phi-4-Multimodal,"Multimodal,Language,Vision,Speech","Language modeling/generation,Question answering,Visual question answering,Speech recognition,Translation,Audio question answering,Character recognition",Microsoft,"Abdelrahman Abouelenin, Atabak Ashfaq, Adam Atkinson, Hany Awadalla, Nguyen Bach, Jianmin Bao, Alon Benhaim, Martin Cai, Vishrav Chaudhary, Congcong Chen, Dong Chen, Dongdong Chen, Junkun Chen, Weizhu Chen, Yen-Chun Chen, Yi-ling Chen, Qi Dai, Xiyang Dai, Ruchao Fan, Mei Gao, Min Gao, Amit Garg, Abhishek Goswami, Junheng Hao, Amr Hendy, Yuxuan Hu, Xin Jin, Mahmoud Khademi, Dongwoo Kim, Young Jin Kim, Gina Lee, Jinyu Li, Yunsheng Li, Chen Liang, Xihui Lin, Zeqi Lin, Mengchen Liu, Yang Liu, Gilsinia Lopez, Chong Luo, Piyush Madan, Vadim Mazalov, Ali Mousavi, Anh Nguyen, Jing Pan, Daniel Perez-Becker, Jacob Platin, Thomas Portet, Kai Qiu, Bo Ren, Liliang Ren, Sambuddha Roy, Ning Shang, Yelong Shen, Saksham Singhal, Subhojit Som, Xia Song, Tetyana Sych, Praneetha Vaddamanu, Shuohang Wang, Yiming Wang, Zhenghao Wang, Haibin Wu, Haoran Xu, Weijian Xu, Yifan Yang, Ziyi Yang, Donghan Yu, Ishmam Zabir, Jianwen Zhang, Li Lyna Zhang, Yunan Zhang, Xiren Zhou",2025-03-03,Phi-4-Mini Technical Report: Compact yet Powerful Multimodal Language Models via Mixture-of-LoRAs,https://arxiv.org/abs/2503.01743,,,,5600000000.0,"5.6B

1. base: Phi-4 Mini (3.8b parameters) 
2. ""The audio encoder and projector introduce 460M parameters while LoRAA consumes another 460M parameters.""
3. ""The image encoder and projector introduce 440M model parameters while the vision adapter LoRAV consumes another 370M model parameters.""",1.1852519e+23,"1.14e+23 (base model training compute) + 7.1724e+21 (finetune compute) = 1.211724e+23

HF instruct model:
""GPUs: 512 A100-80G
Training time: 28 days""

512 GPUs * 312000000000000 FLOP / sec * 28 days * 24 hours / day * 3600 sec / hour * 0.3 [assumed utilization] = 1.1593581e+23 FLOP

geometric mean: 
sqrt(1.211724e+23 * 1.1593581e+23) = 1.1852519e+23",Unspecified unreleased,"""The Phi-4-Multimodal modelâ€™s pre-training phase involves a rich and varied dataset, encompassing interleaved image-text documents, image-text pairs, image grounding data, synthetic datasets from OCR of PDFs and realistic images, and synthesized datasets for chart comprehension""

""For vision-speech data, Phi-4-Multimodal model is trained on a diverse set of synthetic vision-speech data, covering single-frame and multi-frame scenarios. ""
Knowledge cutoff date is June 2024, according to https://huggingface.co/microsoft/Phi-4-multimodal-instruct.",1100000000000.0,"""The pre-training process involves a total of 0.5T tokens, combining both visual and textual elements.""

""To pre-train the adapter and reduce the modality gap between
the speech and text sequences, we curate a dataset of approximately 2M hours of anonymized in-house speech-text pairs with strong/weak ASR supervisions, covering the eight supported languages""

""Note that the speech token rate is 80ms, indicating 750 tokens for 1-minute audio.""

2*10^6 hours * 60 min / hour * 750 tokens / minute = 90B tokens 

""Collectively, the multimodal SFT data comprises approximately 0.3T tokens.""

""To unlock the instruction following capability of Phi-4-Multimodal for variety of speech and audio tasks, we continue to train the model with about 100M curated speech and audio SFT samples (after weighted up) as the speech post-training stage. <..> We consider different maximum audio lengths for different tasks in post-training. For speech summarization task, we train up to 30-minute audio (22.5k tokens). For other tasks, the maximum audio exposed in training is 30s (375 tokens). "" 

geometric mean sqrt(22500*375) ~ 2900 tokens 

100M * 2900 = 2.9*10^11 = 0.29T tokens 

TOTAL ~1.1T tokens

from HF Instruct model repo: 
""Training data: 5T tokens, 2.3M speech hours, and 1.1T image-text tokens""
5T tokens is the base-model training dataset size ",672.0,24 hours / day * 28 days = 672 hours,NVIDIA A100 SXM4 80 GB,,Confident,"We introduce Phi-4-Mini and Phi-4-Multimodal, compact yet highly capable language and multimodal models. Phi-4-Mini is a 3.8-billion-parameter language model trained on high-quality web and synthetic data, significantly outperforming recent open-source models of similar size and matching the performance of models twice its size on math and coding tasks requiring complex reasoning. This achievement is driven by a carefully curated synthetic data recipe emphasizing high-quality math and coding datasets. Compared to its predecessor, Phi-3.5-Mini, Phi-4-Mini features an expanded vocabulary size of 200K tokens to better support multilingual applications, as well as group query attention for more efficient long-sequence generation. Phi-4-Multimodal is a multimodal model that integrates text, vision, and speech/audio input modalities into a single model. Its novel modality extension approach leverages LoRA adapters and modality-specific routers to allow multiple inference modes combining various modalities without interference. For example, it now ranks first in the OpenASR leaderboard to date, although the LoRA component of the speech/audio modality has just 460 million parameters. Phi-4-Multimodal supports scenarios involving (vision + language), (vision + speech), and (speech/audio) inputs, outperforming larger vision-language and speech-language models on a wide range of tasks. Additionally, we experiment to further train Phi-4-Mini to enhance its reasoning capabilities. Despite its compact 3.8-billion-parameter size, this experimental version achieves reasoning performance on par with or surpassing significantly larger models, including DeepSeek-R1-Distill-Qwen-7B and DeepSeek-R1-Distill-Llama-8B.",,,Open weights (unrestricted),"United States of America,Multinational,India,Belgium",Phi-4 Mini,7,"3.8B frozen parameters (Base LM)

1. Vision-Language Training (0.5T tokens)	
810M (440M Image Encoder + Projector + 370M LoRA_V)	

6ND = 6*0.5*10^12*810*10^6 = 2.43e+21

2. Multimodal SFT (0.3T tokens)	
810M	

6ND = 6*0.3*10^12*810*10^6 = 1.458e+21

3. Speech Pre-training (2M hours = 90B tokens, see dataset size notes)	
460M (Audio Encoder + Projector)	

6ND = 6*90*10^9*460*10^6 = 2.484e+20

4. Speech Post-training (100M samples ~ 1.1T tokens, see dataset size notes)	
460M (LoRA_A)

6ND = 6*1.1*10^12*460*10^6 = 3.036e+21

2.43e+21 + 1.458e+21 + 2.484e+20 + 3.036e+21 = 7.1724e+21
",512.0,,2025-05-16 10:30,,,,,,Industry,,,,,Unreleased,"the instruct model is under MIT license on hugging face:
https://huggingface.co/microsoft/Phi-4-multimodal-instruct",Industry,,,,401972.5996644288,"Operation counting,Hardware",microsoft,,,
Hailuo I2V-01-Director,"Video,Vision","Video generation,Image-to-video","MiniMax,Hailuo AI",,2025-03-03,Hailuo AI Advances Cinematic Storytelling with T2V-01-Director and I2V-01-Director,https://www.minimax.io/news/01-director,,,,,,,,Unspecified unreleased,,,,,,,,Unknown,"Enhanced Precision Control
T2V-01-Director and I2V-01-Director introduce a significant reduction in movement randomness, ensuring that every frame adheres strictly to your creative intent. This heightened level of control allows for more accurate and nuanced storytelling.

Cinematic Aesthetics
With a suite of pre-set camera settings, T2V-01-Director and I2V-01-Director enable you to achieve a polished and professional look from the very first shot. Whether your project calls for a dynamic action sequence or a subtle, intimate scene, our settings are optimized to enhance your visual narrative.

Superior Prompt Adherence
The AI behind T2V-01-Director and I2V-01-Director have been fine-tuned for superior prompt adherence, ensuring that your commands are executed with precision. This empowers filmmakers to focus on their creative vision without being hindered by technical limitations.",,,API access,"China,Singapore",,,,,,2025-05-19 12:06,,,,,,"Industry,Industry",,,,,Unreleased,,"Industry,Industry",,,,,,,,,
Hailuo T2V-01-Director,Video,"Video generation,Text-to-video","MiniMax,Hailuo AI",,2025-03-03,Hailuo AI Advances Cinematic Storytelling with T2V-01-Director and I2V-01-Director,https://www.minimax.io/news/01-director,,,,,,,,Unspecified unreleased,,,,,,,,Unknown,"Enhanced Precision Control
T2V-01-Director and I2V-01-Director introduce a significant reduction in movement randomness, ensuring that every frame adheres strictly to your creative intent. This heightened level of control allows for more accurate and nuanced storytelling.

Cinematic Aesthetics
With a suite of pre-set camera settings, T2V-01-Director and I2V-01-Director enable you to achieve a polished and professional look from the very first shot. Whether your project calls for a dynamic action sequence or a subtle, intimate scene, our settings are optimized to enhance your visual narrative.

Superior Prompt Adherence
The AI behind T2V-01-Director and I2V-01-Director have been fine-tuned for superior prompt adherence, ensuring that your commands are executed with precision. This empowers filmmakers to focus on their creative vision without being hindered by technical limitations.",,,API access,"China,Singapore",,,,,,2025-05-19 12:06,,,,,,"Industry,Industry",,,,,Unreleased,,"Industry,Industry",,,,,,,,,
GPT-4.5,"Language,Vision,Multimodal","Language modeling/generation,Question answering,Quantitative reasoning,Translation,Visual question answering,Code generation,Instruction interpretation",OpenAI,"Foundational contributors
Alex Paino, Ali Kamali, Amin Tootoonchian, Andrew Tulloch, Ben Sokolowsky, Clemens Winter, Colin Wei, Daniel Kappler, Daniel Levy, Felipe Petroski Such, Geoff Salmon, Ian Oâ€™Connell, Jason Teplitz, Kai Chen, Nik Tezak, Prafulla Dhariwal, Rapha Gontijo Lopes, Sam Schoenholz, Youlong Cheng, Yujia Jin, Yunxing Dai

Research
Core contributors

Aiden Low, Alec Radford, Alex Carney, Alex Nichol, Alexis Conneau, Ananya Kumar, Ben Wang, Charlotte Cole , Elizabeth Yang, Gabriel Goh, Hadi Salman, Haitang Hu, Heewoo Jun, Ian Sohl, Ishaan Gulrajani, Jacob Coxon, James Betker, Jamie Kiros, Jessica Landon, Kyle Luther, Lia Guy, Lukas Kondraciuk, Lyric Doshi, Mikhail Pavlov, Qiming Yuan, Reimar Leike, Rowan Zellers, Sean Metzger, Shengjia Zhao, Spencer Papay, Tao Wang

Contributors

Adam Lerer, Aidan McLaughlin, Alexander Prokofiev, Alexandra Barr, Allan Jabri, Ananya Kumar, Andrew Gibiansky, Andrew Schmidt, Casey Chu, Chak Li, Chelsea Voss, Chris Hallacy, Chris Koch, Christine McLeavey, David Mely, Dimitris Tsipras, Eric Sigler, Erin Kavanaugh, Farzad Khorasani, Huiwen Chang, Ilya Kostrikov, Ishaan Singal, Ji Lin, Jiahui Yu, Jing Yu Zhang, John Rizzo, Jong Wook Kim, Joyce Lee, Juntang Zhuang, Leo Liu, Li Jing, Long Ouyang, Louis Feuvrier, Mo Bavarian, Nick Stathas, Nitish Keskar, Oleg Murk, Preston Bowman, Scottie Yan, SQ Mah, Tao Xu, Taylor Gordon, Valerie Qi, Wenda Zhou, Yu Zhang

Scaling
Core contributors

Adam Goucher, Alex Chow, Alex Renzin, Aleksandra Spyra, Avi Nayak, Ben Leimberger, Christopher Hesse, Duc Phong Nguyen, Dinghua Li, Eric Peterson, Francis Zhang, Gene Oden, Kai Fricke, Kai Hayashi, Larry Lv, Leqi Zou, Lin Yang, Madeleine Thompson, Michael Petrov, Miguel Castro, Natalia Gimelshein, Phil Tillet, Reza Zamani, Ryan Cheu Stanley Hsieh, Steve Lee, Stewart Hall, Thomas Raoux, Tianhao Zheng, Vishal Kuo, Yongjik Kim, Yuchen Zhang, Zhuoran Liu

Contributors

Alvin Wan, Andrew Cann, Antoine Pelisse, Anuj Kalia, Aaron Hurst, Avital Oliver, Brad Barnes, Brian Hsu, Chen Ding, Chen Shen, Cheng Chang, Christian Gibson, Duncan Findlay, Fan Wang, Fangyuan Li, Gianluca Borello, Heather Schmidt, Henrique Ponde de Oliveira Pinto, Ikai Lan, Jiayi Weng, James Crooks, Jos Kraaijeveld, Junru Shao, Kenny Hsu, Kenny Nguyen, Kevin King, Leah Burkhardt, Leo Chen, Linden Li, Lu Zhang, Mahmoud Eariby, Marat Dukhan, Mateusz Litwin, Miki Habryn, Natan LaFontaine, Pavel Belov, Peng Su, Prasad Chakka, Rachel Lim, Rajkumar Samuel, Renaud Gaubert, Rory Carmichael, Sarah Dong, Shantanu Jain, Stephen Logsdon, Todd Underwood, Weixing Zhang, Will Sheu, Weiyi Zheng, Yinghai Lu, Yunqiao Zhang

Safety Systems
Andrea Vallone, Andy Applebaum, Cameron Raymond, Chong Zhang, Dan Mossing, Elizabeth Proehl, Eric Wallace, Evan Mays, Grace Zhao, Ian Kivlichan, Irina Kofman, Joel Parish, Kevin Liu, Keren Gu-Lemberg, Kristen Ying, Lama Ahmad, Lilian Weng , Leon Maksin, Leyton Ho, Meghan Shah, Michael Lampe, Michele Wang, Miles Wang, Olivia Watkins, Phillip Guo, Samuel Miserendino, Sam Toizer, Sandhini Agarwal, Tejal Patwardhan, Tom DuprÃ© la Tour, Tong Mu, Tyna Eloundou, Yunyun Wang

Deployment
Adam Brandon, Adam Perelman, Adele Li, Akshay Nathan, Alan Hayes, Alfred Xue, Alison Ben, Alec Gorge, Alex Guziel, Alex Iftimie, Ally Bennett, Andrew Chen, Andy Wang, Andy Wood, Angad Singh, Anoop Kotha, Antonia Woodford, Anuj Saharan, Ashley Tyra, Atty Eleti, Ben Schneider, Bessie Ji, Beth Hoover, Bill Chen, Blake Samic, Britney Smith, Brian Yu, Caleb Wang, Cary Bassin, Cary Hudson, Charlie Jatt, Chengdu Huang, Chris Beaumont, Christina Huang, Cristina Scheau, Dana Palmie, Daniel Levine, Daryl Neubieser, Dave Cummings, David Sasaki, Dibya Bhattacharjee, Dylan Hunn, Edwin Arbus, Elaine Ya Le, Enis Sert, Eric Kramer, Fred von Lohmann, Gaby Janatpour, Garrett McGrath, Garrett Ollinger, Gary Yang, Hao Sheng, Harold Hotelling, Janardhanan Vembunarayanan, Jeff Harris, Jeffrey Sabin Matsumoto, Jennifer Robinson, Jessica Liang, Jessica Shieh, Jiacheng Yang, Joel Morris, Joseph Florencio, Josh Kaplan, Kan Wu, Karan Sharma, Karen Li, Katie Pypes, Kendal Simon, Kendra Rimbach, Kevin Park, Kevin Rao, Laurance Fauconnet, Lauren Workman, Leher Pathak, Liang Wu, Liang Xiong, Lien Mamitsuka, Lindsay McCallum, Lukas Gross, Manoli Liodakis, Matt Nichols, Michelle Fradin, Minal Khan, Mingxuan Wang, Nacho Soto, Natalie Staudacher, Nikunj Handa, Niko Felix, Ning Liu, Olivier Godement, Oona Gleeson, Philip Pronin, Raymond Li, Reah Miyara, Rohan Nuttall, R.J. Marsan, Sara Culver, Scott Ethersmith, Sean Fitzgerald, Shamez Hemani, Sherwin Wu, Shiao Lee, Shuyang Cheng, Siyuan Fu, Spug Golden, Steve Coffey, Steven Heidel, Sundeep Tirumalareddy, Tabarak Khan, Thomas Degry, Thomas Dimson, Tom Stasi, Tomo Hiratsuka, Trevor Creech, Uzair Navid Iftikhar, Victoria Chernova, Victoria Spiegel, Wanning Jiang, Wenlei Xie, Yaming Lin, Yara Khakbaz, Yilei Qian, Yilong Qin, Yo Shavit, Zhi Bie

Executive Leadership
Bob McGrew, Greg Brockman, Hannah Wong, Jakub Pachocki, Johannes Heidecke, Joanne Jang, Kate Rouch, Kevin Weil, Lauren Itow, Liam Fedus, Mark Chen, Mia Glaese, Mira Murati, Nick Ryder, Sam Altman, Srinivas Narayanan, Tal Broda",2025-02-27,Introducing GPT-4.5,https://openai.com/index/introducing-gpt-4-5/,,Training cost,"Described by OpenAI as a ""new order of magnitude of compute""

https://openai.com/index/introducing-gpt-4-5/",,,2.1000000999999995e+26,"Speculative estimate based on GPT-N typically incrementing  by about 100x each version number (https://www.youtube.com/watch?v=6nJZopACRuQ) and 10x each 0.5 version number (https://x.com/karpathy/status/1895213020982472863).

OpenAI said GPT-4.5 was a â€œnew order of magnitude in computeâ€, which they could have meant somewhat loosely. But seems quite likely >1e26 based on that statement, plus 4.5's high inference costs, and <5e26 because OpenAI probably didnâ€™t have a substantially >100k H100 cluster in mid-2024.",Unspecified unreleased,"""GPT-4.5 was pre-trained and post-trained on diverse datasets, including a mix of publicly available
data, proprietary data from data partnerships, and custom datasets developed in-house, which
collectively contribute to the modelâ€™s robust conversational capabilities and world knowledge.""
This model seems to also be known as GPT-4.5 Preview, which has a knowledge cutoff date of October 2023 - which I assume means October 1, 2023 - according to https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models?tabs=global-standard%2Cstandard-chat-completions#computer-use-preview.",,,,,,,Speculative,"We advance AI capabilities by scaling two complementary paradigms: unsupervised learning and reasoning. These represent two axes of intelligence.

Unsupervised learning increases world model accuracy and intuition. Models like GPTâ€‘3.5, GPTâ€‘4, and GPTâ€‘4.5 advance this paradigm.
Scaling reasoningâ , on the other hand, teaches models to think and produce a chain of thought before they respond, allowing them to tackle complex STEM or logic problems. Models like OpenAI o1 and OpenAI o3â€‘mini advance this paradigm.
GPTâ€‘4.5 is an example of scaling unsupervised learning by scaling up compute and data, along with architecture and optimization innovations. GPTâ€‘4.5 was trained on Microsoft Azure AI supercomputers. The result is a model that has broader knowledge and a deeper understanding of the world, leading to reduced hallucinations and more reliability across a wide range of topics.",,,API access,United States of America,,,,,,2025-06-19 09:45,Azure AI,,,,,Industry,,,,,Unreleased,,Industry,,,,,Benchmarks,,,,2
Kimi 1.6,Language,Code generation,Moonshot,,2025-02-27,,,,,,,,,,,,,,,,,,Unknown,,,,Unreleased,China,,,,,,2025-05-01 10:42,,,,,,Industry,,,,,,,Industry,,,,,,,,,
Mercury,Language,Code generation,Inception Labs,,2025-02-27,Is the Mercury LLM the first of a new Generation of LLMs?,"https://machine-learning-made-simple.medium.com/is-the-mercury-llm-the-first-of-a-new-generation-of-llms-b64de1d36029
https://www.inceptionlabs.ai/news
",,SOTA improvement,"â€œWhen evaluated on standard coding benchmarks, Mercury Coder achieves excellent quality across numerous benchmarks, often surpassing the performance of speed-optimized autoregressive models like GPT-4o Mini and Claude 3.5 Haiku while being up to 10x faster.â€",,,,,,,,,,,,,Unknown,,,,API access,United States of America,,,,,,2025-06-06 12:46,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
Pika 2.2,"Video,Vision","Video generation,Text-to-video,Image-to-video",Pika Labs,,2025-02-27,"10s generations, 1080p resolution, and Pikaframesâ€” key frame transitions anywhere from 1-10s.",,,,,,,,,Unspecified unreleased,,,,,,,,Unknown,"Pika 2.2, released by Pika Labs in early 2025, is a generative video model designed to turn text and images into short video clips. With upgrades in realism, motion handling, and prompt responsiveness, Pika 2.2 supports features like text-to-video, image-to-video, and keyframe-based transitionsâ€”generating videos up to 10 seconds long in 1080p resolution. Additionally, Pika offers tools such as Pikaffects, Pikascenes, Pikadditions, and Pikaswaps, providing users with expanded capabilities for creating engaging social and entertainment videos.",,,API access,United States of America,,,,,,2025-05-19 12:07,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
Granite 3.2 8B,Language,"Mathematical reasoning,Quantitative reasoning,Language modeling/generation,Question answering",IBM,,2025-02-26,"IBM Granite 3.2: Reasoning, vision, forecasting and more",https://huggingface.co/ibm-granite/granite-3.2-8b-instruct,,,,8000000000.0,,,,Unspecified unreleased,"""Training Data: Overall, our training data is largely comprised of two key sources: (1) publicly available datasets with permissive license, (2) internal synthetically generated data targeted to enhance reasoning capabilities""",,,,,NVIDIA H100 SXM5 80GB,,Confident,"The new Granite 3.2 8B Instruct and Granite 3.2 2B Instruct offer experimental chain-of-thought reasoning capabilities that significantly improve their ability to follow complex instructions with no sacrifice to general performance. The reasoning process can be toggled on and off, allowing for efficient use of computing resources.
When combined with IBMâ€™s inference scaling techniques, Granite 3.2 8B Instructâ€™s extended thought process enables it to meet or exceed the reasoning performance of much larger models, including GPT-4o and Claude 3.5 Sonnet.
Like their predecessors, all new IBM Granite models are released open sourced under a permissive Apache 2.0 license.
Granite 3.2 models are now available on IBM watsonx.ai, Hugging Face, Ollama, LMStudio, and Replicate.",,,Open weights (unrestricted),United States of America,Granite 3.1 8B,,,,,2025-05-01 10:42,IBM,"IBM's super computing cluster, Blue Vela, which is outfitted with NVIDIA H100 GPUs. This cluster provides a scalable and efficient infrastructure for training our models over thousands of GPUs.",,,,Industry,,,,,Unreleased,"Apache 2.0
https://huggingface.co/ibm-granite/granite-3.2-8b-instruct",Industry,,,,,,ibm-granite,,,
Granite 3.2 2B ,Language,"Mathematical reasoning,Quantitative reasoning,Language modeling/generation,Question answering,Text summarization,Text classification,Translation,Code generation",IBM,,2025-02-26,"IBM Granite 3.2: Reasoning, vision, forecasting and more",https://huggingface.co/ibm-granite/granite-3.2-2b-instruct,,,,2530000000.0,,,,Unspecified unreleased,"""Training Data: Overall, our training data is largely comprised of two key sources: (1) publicly available datasets with permissive license, (2) internal synthetically generated data targeted to enhance reasoning capabilities""",,,,,NVIDIA H100 SXM5 80GB,,Confident,"The new Granite 3.2 8B Instruct and Granite 3.2 2B Instruct offer experimental chain-of-thought reasoning capabilities that significantly improve their ability to follow complex instructions with no sacrifice to general performance. The reasoning process can be toggled on and off, allowing for efficient use of computing resources.
When combined with IBMâ€™s inference scaling techniques, Granite 3.2 8B Instructâ€™s extended thought process enables it to meet or exceed the reasoning performance of much larger models, including GPT-4o and Claude 3.5 Sonnet.
Like their predecessors, all new IBM Granite models are released open sourced under a permissive Apache 2.0 license.
Granite 3.2 models are now available on IBM watsonx.ai, Hugging Face, Ollama, LMStudio, and Replicate.",,,Open weights (unrestricted),United States of America,Granite 3.1 2B,,,,,2025-04-25 16:06,IBM,"IBM's super computing cluster, Blue Vela, which is outfitted with NVIDIA H100 GPUs. This cluster provides a scalable and efficient infrastructure for training our models over thousands of GPUs.",,,,Industry,,,,,Unreleased,"Apache 2.0
https://huggingface.co/ibm-granite/granite-3.2-2b-instruct",Industry,,,,,,ibm-granite,,,
Wan 2.1 14B,"Video,Vision","Video generation,Image-to-video",Alibaba,,2025-02-25,Wan 2.1 by Wan AI :best cost efficient video generation model Now Available,"https://huggingface.co/Wan-AI
https://huggingface.co/blog/LLMhacker/wanai-wan21
",,SOTA improvement,"Wan2.1 consistently outperforms existing open-source models and state-of-the-art commercial solutions across multiple benchmarks. Achives higher score than SORA in some of the bench dimensions - single object accuracy, spatial position accuracy, scene generation quality. ",14000000000.0,14B,2.5e+23,"""Through extensive experimentation, the model is validated at scale, reaching 14 billion parameters. Subsequently, Wan has seen large-scale data comprising billions of
images and videos, amounting to O(1) trillions of tokens in total.""

So likely between 1T and 10T tokens. Assume 3T.

Transformer architecture, so 6ND should be a decent approximation.

6ND = 6 * 14e9 * 3e12 ~= 2.5e+23 FLOP",,,,,,,,,Confident,,,,Open weights (unrestricted),China,,,,,,2025-06-12 17:21,,,,,,Industry,,,,,Open source,"Apache 2.0
https://huggingface.co/Wan-AI/Wan2.1-I2V-14B-720P

Training code:
https://github.com/Wan-Video/Wan2.1",Industry,,,BF16,,,Wan-AI,,,
Claude 3.7 Sonnet,"Language,Vision,Multimodal","Language modeling/generation,Question answering,Code generation,Quantitative reasoning,Translation,Instruction interpretation,Visual question answering",Anthropic,,2025-02-24,Claude 3.7 Sonnet,https://www.anthropic.com/news/claude-3-7-sonnet,,Training cost,,,,3.3500000000000006e+25,https://docs.google.com/spreadsheets/d/10bhwdVrfHI8tysVIz62ZxtvQ30L-HojYvmU18_b-WIM/edit?gid=0#gid=0,Unspecified unreleased,"""Claude 3.7 Sonnet is trained on a proprietary mix of publicly available information on the Internet, as well as non-public data from third parties, data provided by data labeling services and paid contractors, and data we
generate internally. While trained on publicly available information on the internet through November 2024, Claude 3.7 Sonnetâ€™s knowledge cut-off date is the end of October 2024. This means the modelâ€™s knowledge base is most extensive and reliable on information and events up to October 2024.""",,,,,,,Likely,"Today, weâ€™re announcing Claude 3.7 Sonnet1, our most intelligent model to date and the first hybrid reasoning model on the market. Claude 3.7 Sonnet can produce near-instant responses or extended, step-by-step thinking that is made visible to the user. API users also have fine-grained control over how long the model can think for.

Claude 3.7 Sonnet shows particularly strong improvements in coding and front-end web development. ",,,API access,United States of America,,,,,,2025-06-12 14:56,,,,,,Industry,,1.0000000001e+25,1.0000000001e+26,,Unreleased,,Industry,,,,,,,,,6
Step-Video-T2V,Video,"Video generation,Text-to-video",StepFun,"Guoqing Ma, Haoyang Huang, Kun Yan, Liangyu Chen, Nan Duan, Shengming Yin, Changyi Wan, Ranchen Ming, Xiaoniu Song, Xing Chen, Yu Zhou, Deshan Sun, Deyu Zhou, Jian Zhou, Kaijun Tan, Kang An, Mei Chen, Wei Ji, Qiling Wu, Wen Sun, Xin Han, Yanan Wei, Zheng Ge, Aojie Li, Bin Wang, Bizhu Huang, Bo Wang, Brian Li, Changxing Miao, Chen Xu, Chenfei Wu, Chenguang Yu, Dapeng Shi, Dingyuan Hu, Enle Liu, Gang Yu, Ge Yang, Guanzhe Huang, Gulin Yan, Haiyang Feng, Hao Nie, Haonan Jia, Hanpeng Hu, Hanqi Chen, Haolong Yan, Heng Wang, Hongcheng Guo, Huilin Xiong, Huixin Xiong, Jiahao Gong, Jianchang Wu, Jiaoren Wu, Jie Wu, Jie Yang, Jiashuai Liu, Jiashuo Li, Jingyang Zhang, Junjing Guo, Junzhe Lin, Kaixiang Li, Lei Liu, Lei Xia, Liang Zhao, Liguo Tan, Liwen Huang, Liying Shi, Ming Li, Mingliang Li, Muhua Cheng, Na Wang, Qiaohui Chen, Qinglin He, Qiuyan Liang, Quan Sun, Ran Sun, Rui Wang, Shaoliang Pang, Shiliang Yang, Sitong Liu, Siqi Liu, Shuli Gao, Tiancheng Cao, Tianyu Wang, Weipeng Ming, Wenqing He, Xu Zhao, Xuelin Zhang, Xianfang Zeng, Xiaojia Liu, Xuan Yang, Yaqi Dai, Yanbo Yu, Yang Li, Yineng Deng, Yingming Wang, Yilei Wang, Yuanwei Lu, Yu Chen, Yu Luo, Yuchu Luo et al. (15 additional authors not shown)",2025-02-24,"Step-Video-T2V Technical Report: The Practice, Challenges, and Future of Video Foundation Model",https://arxiv.org/abs/2502.10248,,,,30000000000.0,30B,4.1015808e+24,"""We have constructed a datacenter comprising thousands of NVIDIA H800 GPUs""
""we have achieved 99% effective GPU
training time over more than one month.""

989000000000000 FLOP / GPU / sec [bf16 assumed] * 720 hours * 3600 sec / hour * 5000 GPUs [assumption -> ""Likely"" confidence] * 0.32 [reported utilization] = 4.1015808e+24 FLOP",Unspecified unreleased,,,"""We constructed a large-scale video dataset comprising 2B video-text pairs and 3.8B image-text pairs""
they are supposedly using only a subset of it (see Table 6):
 3.8B image-text pairs
 644M low resolution video-text pairs
+Post-filtering SFT dataset: 30M high-quality video-text pairs",720.0,"""we have achieved 99% effective GPU
training time over more than one month.""
one month ~720 hours",NVIDIA H800 SXM5,,Likely,"We present Step-Video-T2V, a state-of-the-art text-to-video pre-trained model with 30B parameters and the ability to generate videos up to 204 frames in length. A deep compression Variational Autoencoder, Video-VAE, is designed for video generation tasks, achieving 16x16 spatial and 8x temporal compression ratios, while maintaining exceptional video reconstruction quality. User prompts are encoded using two bilingual text encoders to handle both English and Chinese. A DiT with 3D full attention is trained using Flow Matching and is employed to denoise input noise into latent frames. A video-based DPO approach, Video-DPO, is applied to reduce artifacts and improve the visual quality of the generated videos. We also detail our training strategies and share key observations and insights. Step-Video-T2V's performance is evaluated on a novel video generation benchmark, Step-Video-T2V-Eval, demonstrating its state-of-the-art text-to-video quality when compared with both open-source and commercial engines. Additionally, we discuss the limitations of current diffusion-based model paradigm and outline future directions for video foundation models. We make both Step-Video-T2V and Step-Video-T2V-Eval available at this https URL. The online version can be accessed from this https URL as well. Our goal is to accelerate the innovation of video foundation models and empower video content creators.",,,Open weights (unrestricted),China,,,,5000.0,0.32,2025-05-19 12:08,,,,,,Industry,,,,,Unreleased,"MIT license

https://huggingface.co/stepfun-ai/stepvideo-t2v",Industry,,,,6870719.882854385,Hardware,stepfun-ai,,,
Helix,Robotics,"Animal (human/non-human) imitation,Robotic manipulation",Figure AI,,2025-02-20,Helix: A Vision-Language-Action Model for Generalist Humanoid Control,https://www.figure.ai/news/helix,,,,7080000000.0,"""Our system comprises two main components: S2, a VLM backbone, and S1, a latent-conditional visuomotor transformer. S2 is built on a 7B-parameter open-source, open-weight VLM pretrained on internet-scale data. <..>

S1, an 80M parameter cross-attention encoder-decoder transformer, handles low-level control.""

7B+80M = 7080000000",,,,"""We collect a high quality, multi-robot, multi-operator dataset of diverse teleoperated behaviors, ~500 hours in total. To generate natural language-conditioned training pairs, we use an auto-labeling VLM to generate hindsight instructions. The VLM processes segmented video clips from the onboard robot cameras, prompted with: ""What instruction would you have given the robot to get the action seen in this video?"" All items handled during training are excluded from evaluations to prevent contamination.""",,,,,,,Confident,"We're introducing Helix, a generalist Vision-Language-Action (VLA) model that unifies perception, language understanding, and learned control to overcome multiple longstanding challenges in robotics. Helix is a series of firsts:

Full-upper-body control: Helix is the first VLA to output high-rate continuous control of the entire humanoid upper body, including wrists, torso, head, and individual fingers.

Multi-robot collaboration: Helix is the first VLA to operate simultaneously on two robots, enabling them to solve a shared, long-horizon manipulation task with items they have never seen before.

Pick up anything: Figure robots equipped with Helix can now pick up virtually any small household object, including thousands of items they have never encountered before, simply by following natural language prompts.

One neural network: Unlike prior approaches, Helix uses a single set of neural network weights to learn all behaviorsâ€”picking and placing items, using drawers and refrigerators, and cross-robot interactionâ€”without any task-specific fine-tuning.

Commercial-ready: Helix is the first VLA that runs entirely onboard embedded low-power-consumption GPUs, making it immediately ready for commercial deployment.

",,,Hosted access (no API),,,,,,,2025-03-10 12:08,,,,,,,,,,,Unreleased,,,,,,,,,,,
Qwen2.5-VL-72B,"Language,Vision,Multimodal","Visual question answering,Video description,Language modeling/generation,Translation,Question answering,Character recognition,Quantitative reasoning",Alibaba,"Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, Junyang Lin",2025-02-19,Qwen2.5-VL Technical Report,https://arxiv.org/abs/2502.13923,,,,72000000000.0,The model is initialized with pre-trained weights from the Qwen2.5 LLM,9.5712e+24,7.8e+24 FLOP [base LLM compute] + 1.7712e+24 FLOP [VL training compute] = 9.5712e+24 FLOP,Unspecified unreleased,,4100000000000.0,"Table 2
4.1T total training tokens:",,,,,Confident,"We introduce Qwen2.5-VL, the latest flagship model of Qwen vision-language series, which demonstrates significant advancements in both foundational capabilities and innovative functionalities. Qwen2.5-VL achieves a major leap forward in understanding and interacting with the world through enhanced visual recognition, precise object localization, robust document parsing, and long-video comprehension. A standout feature of Qwen2.5-VL is its ability to localize objects using bounding boxes or points accurately. It provides robust structured data extraction from invoices, forms, and tables, as well as detailed analysis of charts, diagrams, and layouts. To handle complex inputs, Qwen2.5-VL introduces dynamic resolution processing and absolute time encoding, enabling it to process images of varying sizes and videos of extended durations (up to hours) with second-level event localization. This allows the model to natively perceive spatial scales and temporal dynamics without relying on traditional normalization techniques. By training a native dynamic-resolution Vision Transformer (ViT) from scratch and incorporating Window Attention, we reduce computational overhead while maintaining native resolution. As a result, Qwen2.5-VL excels not only in static image and document understanding but also as an interactive visual agent capable of reasoning, tool usage, and task execution in real-world scenarios such as operating computers and mobile devices. Qwen2.5-VL is available in three sizes, addressing diverse use cases from edge AI to high-performance computing. The flagship Qwen2.5-VL-72B model matches state-of-the-art models like GPT-4o and Claude 3.5 Sonnet, particularly excelling in document and diagram understanding. Additionally, Qwen2.5-VL maintains robust linguistic performance, preserving the core language competencies of the Qwen2.5 LLM.",,,Open weights (restricted use),China,Qwen2.5-72B,1,6 FLOP / parameter / token * 72 * 10^9 parameters * 4.1 * 10^12 tokens = 1.7712e+24 FLOP,,,2025-05-31 21:54,,,,,,Industry,,,,,Unreleased,"https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct

Qwen license (restriction on >100m monthly users)",Industry,,,,,Operation counting,Qwen,,,
 Qwen2.5-VL-7B ,"Language,Vision,Multimodal","Visual question answering,Video description,Language modeling/generation,Translation,Question answering,Character recognition,Quantitative reasoning",Alibaba,"Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, Junyang Lin",2025-02-19,Qwen2.5-VL Technical Report,https://arxiv.org/abs/2502.13923,,,,7000000000.0,The model is initialized with pre-trained weights from the Qwen2.5 LLM,9.9408e+23,8.2188e+23 FLOP [base LLM compute] + 1.722e+23 FLOP [VL training compute] = 9.9408e+23 FLOP,Unspecified unreleased,,4100000000000.0,"Table 2
4.1T total training tokens:",,,,,Confident,"We introduce Qwen2.5-VL, the latest flagship model of Qwen vision-language series, which demonstrates significant advancements in both foundational capabilities and innovative functionalities. Qwen2.5-VL achieves a major leap forward in understanding and interacting with the world through enhanced visual recognition, precise object localization, robust document parsing, and long-video comprehension. A standout feature of Qwen2.5-VL is its ability to localize objects using bounding boxes or points accurately. It provides robust structured data extraction from invoices, forms, and tables, as well as detailed analysis of charts, diagrams, and layouts. To handle complex inputs, Qwen2.5-VL introduces dynamic resolution processing and absolute time encoding, enabling it to process images of varying sizes and videos of extended durations (up to hours) with second-level event localization. This allows the model to natively perceive spatial scales and temporal dynamics without relying on traditional normalization techniques. By training a native dynamic-resolution Vision Transformer (ViT) from scratch and incorporating Window Attention, we reduce computational overhead while maintaining native resolution. As a result, Qwen2.5-VL excels not only in static image and document understanding but also as an interactive visual agent capable of reasoning, tool usage, and task execution in real-world scenarios such as operating computers and mobile devices. Qwen2.5-VL is available in three sizes, addressing diverse use cases from edge AI to high-performance computing. The flagship Qwen2.5-VL-72B model matches state-of-the-art models like GPT-4o and Claude 3.5 Sonnet, particularly excelling in document and diagram understanding. Additionally, Qwen2.5-VL maintains robust linguistic performance, preserving the core language competencies of the Qwen2.5 LLM.",,,Open weights (unrestricted),China,Qwen2.5-7B,1,6 FLOP / parameter / token * 7 * 10^9 parameters * 4.1 * 10^12 tokens = 1.722e+23 FLOP,,,2025-05-31 21:53,,,,,,Industry,,,,,Unreleased,"Apache 2.0
https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct",Industry,,,,,Operation counting,Qwen,,,
 Qwen2.5-VL-3B,"Language,Vision,Multimodal","Visual question answering,Video description,Language modeling/generation,Translation,Question answering,Character recognition,Quantitative reasoning",Alibaba,"Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, Junyang Lin",2025-02-19,Qwen2.5-VL Technical Report,https://arxiv.org/abs/2502.13923,,,,3000000000.0,The model is initialized with pre-trained weights from the Qwen2.5 LLM,4.0752e+23,3.3372e+23 FLOP [base LLM compute] + 7.38e+22 FLOP [VL training compute] = 4.0752e+23 FLOP,Unspecified unreleased,,4100000000000.0,"Table 2
4.1T total training tokens:",,,,,Confident,"We introduce Qwen2.5-VL, the latest flagship model of Qwen vision-language series, which demonstrates significant advancements in both foundational capabilities and innovative functionalities. Qwen2.5-VL achieves a major leap forward in understanding and interacting with the world through enhanced visual recognition, precise object localization, robust document parsing, and long-video comprehension. A standout feature of Qwen2.5-VL is its ability to localize objects using bounding boxes or points accurately. It provides robust structured data extraction from invoices, forms, and tables, as well as detailed analysis of charts, diagrams, and layouts. To handle complex inputs, Qwen2.5-VL introduces dynamic resolution processing and absolute time encoding, enabling it to process images of varying sizes and videos of extended durations (up to hours) with second-level event localization. This allows the model to natively perceive spatial scales and temporal dynamics without relying on traditional normalization techniques. By training a native dynamic-resolution Vision Transformer (ViT) from scratch and incorporating Window Attention, we reduce computational overhead while maintaining native resolution. As a result, Qwen2.5-VL excels not only in static image and document understanding but also as an interactive visual agent capable of reasoning, tool usage, and task execution in real-world scenarios such as operating computers and mobile devices. Qwen2.5-VL is available in three sizes, addressing diverse use cases from edge AI to high-performance computing. The flagship Qwen2.5-VL-72B model matches state-of-the-art models like GPT-4o and Claude 3.5 Sonnet, particularly excelling in document and diagram understanding. Additionally, Qwen2.5-VL maintains robust linguistic performance, preserving the core language competencies of the Qwen2.5 LLM.",,,Open weights (restricted use),China,Qwen2.5-3B,7,6 FLOP / parameter / token * 3 * 10^9 parameters * 4.1 * 10^12 tokens = 7.38e+22 FLOP,,,2025-05-31 21:52,,,,,,Industry,,,,,Unreleased,"unclear license
https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct",Industry,,,,,Operation counting,Qwen,,,
R1 1776,Language,"Language modeling/generation,Question answering,Chat",Perplexity,,2025-02-18,Open-sourcing R1 1776,https://www.perplexity.ai/hub/blog/open-sourcing-r1-1776,,,,671000000000.0,671B params,,,Unspecified unreleased,"""Our main focus during post-training was on gathering high-quality data related to censored topics in China. This involved collecting both queries and factual responses.

We took the following approach to collect high quality dataset:

We employed human experts to identify approximately 300 topics known to be censored by the CCP.

Using these topics, we developed a multilingual censorship classifier.

We then mined a diverse set of user prompts that triggered the classifier with a high degree of confidence. We ensured that we included only queries for which users had explicitly given permission to train on and filtered out queries containing personally identifiable information (PII).

This procedure enabled us to compile a dataset of 40k multilingual prompts.

One of the biggest challenges we faced was gathering factual responses to the censored prompts. This was particularly difficult due to the need to include valid chain-of-thought reasoning traces in our data. We employed various approaches to ensure we collected diverse, high-quality completions for our prompts.

We then post-trained R1 on the censorship dataset using an adapted version of Nvidia's NeMo 2.0 framework. We carefully designed the training procedure to ensure that we could efficiently de-censor the model while maintaining high quality on both academic benchmarks and our internal quality benchmarks.""",40000000.0,"40k multilingual prompts

assuming ~1000 tokens per prompt -> 40M tokens",,,,,Speculative,"R1 1776 is a DeepSeek-R1 reasoning model that has been post-trained by Perplexity AI to remove Chinese Communist Party censorship. The model provides unbiased, accurate, and factual information while maintaining high reasoning capabilities.",,,Open weights (unrestricted),United States of America,DeepSeek-R1,8880000000000000000,"Deepseek R1 has 37B active parameters 
fine-tuning dataset is speculatively estimated to be 40M tokens (see dataset size notes)

6 FLOP / token / parameter * 37 * 10^9 paramters * 40*10^6 tokens = 8.88e+18 FLOP ",,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,"MIT license 

https://huggingface.co/perplexity-ai/r1-1776",Industry,,,,,Operation counting,perplexity-ai,,,
Grok-3,"Language,Vision,Multimodal","Chat,Language modeling/generation,Question answering,Code generation,Visual question answering",xAI,,2025-02-17,Grok 3 Beta â€” The Age of Reasoning Agents,https://x.ai/blog/grok-3,,Training cost,,,,4.6400000000000005e+26,"Estimate based on training time for a cluster of 100,000 H100s, and xAI's statement that Grok 2 was trained on more compute than GPT-4 (2.1e25) and that Grok 3 was trained on around 15 times more compute than Grok 2. 

Full estimate here: https://docs.google.com/document/d/1C_dABuZrAqYE_ui4_GZ4bRLtq3TBjIGoBSktaPElhEU/edit?usp=sharing",Unspecified unreleased,"Knowledge cutoff date is November 17, 2024, according to https://docs.x.ai/docs/models#models-and-pricing. ",,,2400.0,Estimated to be between 3 and 4 months. We use 100 days in our estimate,NVIDIA H100 SXM5 80GB,,Confident,,,,Hosted access (no API),United States of America,,,,100000.0,,2025-05-29 13:22,,xAI Memphis Colossus,,,,Industry,,2.1e+26,1.1299999999999998e+27,,Unreleased,,Industry,,,,137435820.25246143,"Hardware,Comparison with other models",,,,1
LLaDA,Language,"Code generation,Language modeling/generation","Renmin University of China,Ant Group","Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai Lin, Ji-Rong Wen, Chongxuan Li",2025-02-14,Large Language Diffusion Models,https://arxiv.org/abs/2502.09992,3.0,,,8000000000.0,,,"LLaDA scales effectively up to a computational budget of 10*23 FLOPs, achieving comparable results to selfconstructed ARM baselines trained on the same data across six tasks, e.g., MMLU and GSM8K.",,,2300000000000.0,2.3 trillion tokens,,,NVIDIA H800 SXM5,,Confident,"Autoregressive models (ARMs) are widely regarded as the cornerstone of large language models (LLMs). We challenge this notion by introducing LLaDA, a diffusion model trained from scratch under the pre-training and supervised finetuning (SFT) paradigm. LLaDA models distributions through a forward data masking process and a reverse process, parameterized by a vanilla Transformer to predict masked tokens. By optimizing a likelihood bound, it provides a principled generative approach for probabilistic inference. Across extensive benchmarks, LLaDA demonstrates strong scalability, outperforming our self-constructed ARM baselines. Remarkably, LLaDA 8B is competitive with strong LLMs like LLaMA3 8B in in-context learning and, after SFT, exhibits impressive instruction-following abilities in case studies such as multi-turn dialogue. Moreover, LLaDA addresses the reversal curse, surpassing GPT-4o in a reversal poem completion task. Our findings establish diffusion models as a viable and promising alternative to ARMs, challenging the assumption that key LLMcapabilities discussed above are inherently tied to ARMs. Project page and codes: https: //ml-gsai.github.io/LLaDA-demo/.",,,,"China,China",,,,,,2025-05-01 10:42,,,,,,"Academia,Industry",,,,,,,"Academia,Industry",,,,,,,,,
Deephermes 3 Llama 3 8B Preview,Language,"Mathematical reasoning,Quantitative reasoning,Language modeling/generation,Question answering",Nous Research,,2025-02-14,"â€˜Personalized, unrestrictedâ€™ AI lab Nous Research launches first toggle-on reasoning model: DeepHermes-3",https://huggingface.co/NousResearch/DeepHermes-3-Llama-3-8B-Preview,,,,8000000000.0,8B,,,"Hermes 3,Unspecified unreleased",,1500000000.0,"""DeepHermes-3 builds on the Hermes 3, a meticulously curated multi-domain dataset that Nous Research developed for the broader Hermes 3 series.""
Hermes 3 -> 390M tokens

""In addition, the pseudonymous Nous Research team member @Teknium (@Teknium1 on X) wrote in response to a user of the companyâ€™s Discord server that the model was trained on â€œ1M non cots and 150K cots,â€ or 1 million non-CoT outputs and 150,000 CoT outputs.""

https://venturebeat.com/ai/personalized-unrestricted-ai-lab-nous-research-launches-first-toggle-on-reasoning-model-deephermes-3/

390M + 1.15M * 1000 [assumed length of each piece] ~ 1.5B tokens (Speculative confidence)",,,,,Speculative,"DeepHermes 3 Preview is the latest version of our flagship Hermes series of LLMs by Nous Research, and one of the first models in the world to unify Reasoning (long chains of thought that improve answer accuracy) and normal LLM response modes into one model. We have also improved LLM annotation, judgement, and function calling.

DeepHermes 3 Preview is one of the first LLM models to unify both ""intuitive"", traditional mode responses and long chain of thought reasoning responses into a single model, toggled by a system prompt.

Hermes 3, the predecessor of DeepHermes 3, is a generalist language model with many improvements over Hermes 2, including advanced agentic capabilities, much better roleplaying, reasoning, multi-turn conversation, long context coherence, and improvements across the board.

The ethos of the Hermes series of models is focused on aligning LLMs to the user, with powerful steering capabilities and control given to the end user.",,,Open weights (restricted use),United States of America,Llama 3.1-8B,72000000000000000000,"6 FLOP / parameter / token * 8 * 10^9 parameters * 1.5*10^9 tokens = 7.2e+19 FLOP

""Speculative"" confidence because exact amount of tokens is unknown as well as amount of epochs",,,2025-05-01 10:42,,,,,,Industry,,,,,,"llama 3 license
https://huggingface.co/NousResearch/DeepHermes-3-Llama-3-8B-Preview",Industry,,,,,Operation counting,NousResearch,,,
OmniHuman-1,"Video,Vision,Audio","Video generation,Image-to-video",ByteDance,"Gaojie Lin, Jianwen Jiang, Jiaqi Yang, Zerong Zheng, Chao Liang",2025-02-13,OmniHuman-1: Rethinking the Scaling-Up of One-Stage Conditioned Human Animation Models,https://arxiv.org/abs/2502.01061,,,,,,,,,"""By filtering based on aesthetics, image quality, motion amplitude, etc. (common criteria for video generation), we obtained 18.7K hours of human-related data for training. Of this, 13% was selected using lipsync and pose visibility criteria, enabling audio and pose modalities. During training, the data composition was adjusted to fit the omni-condition training strategy.""

18700*0.13 = 2431 (hours of video)",,,,,,,Confident,"End-to-end human animation, such as audio-driven talking human generation, has undergone notable advancements in the recent few years. However, existing methods still struggle to scale up as large general video generation models, limiting their potential in real applications. In this paper, we propose OmniHuman, a Diffusion Transformer-based framework that scales up data by mixing motion-related conditions into the training phase. To this end, we introduce two training principles for these mixed conditions, along with the corresponding model architecture and inference strategy. These designs enable OmniHuman to fully leverage data-driven motion generation, ultimately achieving highly realistic human video generation. More importantly, OmniHuman supports various portrait contents (face close-up, portrait, half-body, full-body), supports both talking and singing, handles human-object interactions and challenging body poses, and accommodates different image styles. Compared to existing end-to-end audio-driven methods, OmniHuman not only produces more realistic videos, but also offers greater flexibility in inputs. It also supports multiple driving modalities (audio-driven, video-driven and combined driving signals). Video samples are provided on the ttfamily project page (this https URL)",,,Unreleased,China,,,,,,2025-05-19 12:32,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
Sonar,Language,"Question answering,Chat,Language modeling/generation,Quantitative reasoning,Search",Perplexity,,2025-02-11,Meet new Sonar: A Blazing Fast Model Optimized for Perplexity Search,https://www.perplexity.ai/hub/blog/meet-new-sonar,,,,70000000000.0,70B (assuming same as base model Llama 3.3 70B),,,Unspecified unreleased,,,,,,,,Confident,"Starting today, all Perplexity Pro users will be able to try out the latest version of Sonar, Perplexity's in-house model that is optimized for answer quality and user experience. Built on top of Llama 3.3 70B, Sonar has been further trained to enhance answer factuality and readability for Perplexityâ€™s default search mode.

Through comprehensive online A/B testing, we have found that Sonar significantly outperforms models in its class, like GPT-4o mini and Claude 3.5 Haiku, while closely matching or exceeding the performance of frontier models like GPT-4o and Claude 3.5 Sonnet for user satisfaction. Powered by Cerebras inference infrastructure, Sonar runs at a blazing fast speed of 1200 tokens per second â€” enabling nearly instant answer generation.",,,API access,United States of America,Llama 3.3 70B,,,,,2025-06-11 10:38,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
OREAL 32B,"Mathematics,Language","Mathematical reasoning,Quantitative reasoning,Language modeling/generation,Question answering","Shanghai AI Lab,Shanghai Jiao Tong University,Chinese University of Hong Kong (CUHK),InnoHK","Chengqi Lyu, Songyang Gao, Yuzhe Gu, Wenwei Zhang, Jianfei Gao, Kuikun Liu, Ziyi Wang, Shuaibin Li, Qian Zhao, Haian Huang, Weihan Cao, Jiangning Liu, Hongwei Liu, Junnan Liu, Songyang Zhang, Dahua Lin, Kai Chen",2025-02-10,Exploring the Limit of Outcome Reward for Learning Mathematical Reasoning,https://arxiv.org/abs/2502.06781,,,,32000000000.0,,,,"MATH,NuminaMath,Unspecified unreleased","""The training data for the RFT models consists of in-house datasets supported by OpenDataLab [40]
and open-source datasets including Numina [41] and the training set of MATH [21]""",1509949440.0,"""During training iterations, each batch consists of 64 questions, with 16 rollouts per question. The max length of each rollout trajectory is set to 16384 tokens. Then the correctness of each response is averaged to calculate the pass rate, and questions with an overall pass rate of 0 or 1 are discarded.""

from https://github.com/InternLM/OREAL
90 steps

90*16384*64*16  = 1509949440 tokens  (upper bound since some questions were discarded)",,,NVIDIA A100,,Likely,"Reasoning abilities, especially those for solving complex math problems, are crucial components of general intelligence. Recent advances by proprietary companies, such as o-series models of OpenAI, have made remarkable progress on reasoning tasks. However, the complete technical details remain unrevealed, and the techniques that are believed certainly to be adopted are only reinforcement learning (RL) and the long chain of thoughts. This paper proposes a new RL framework, termed OREAL, to pursue the performance limit that can be achieved through \textbf{O}utcome \textbf{RE}w\textbf{A}rd-based reinforcement \textbf{L}earning for mathematical reasoning tasks, where only binary outcome rewards are easily accessible. We theoretically prove that behavior cloning on positive trajectories from best-of-N (BoN) sampling is sufficient to learn the KL-regularized optimal policy in binary feedback environments. This formulation further implies that the rewards of negative samples should be reshaped to ensure the gradient consistency between positive and negative samples. To alleviate the long-existing difficulties brought by sparse rewards in RL, which are even exacerbated by the partial correctness of the long chain of thought for reasoning tasks, we further apply a token-level reward model to sample important tokens in reasoning trajectories for learning. With OREAL, for the first time, a 7B model can obtain 94.0 pass@1 accuracy on MATH-500 through RL, being on par with 32B models. OREAL-32B also surpasses previous 32B models trained by distillation with 95.0 pass@1 accuracy on MATH-500. Our investigation also indicates the importance of initial policy models and training queries for RL. Code, models, and data will be released to benefit future research\footnote{this https URL}.",1.0,,Open weights (unrestricted),"China,China,Hong Kong,China,Hong Kong",Qwen2.5-32B,289910290000000000000,6 FLOP / token / parameter * 32 * 10^ 9 parameters * 1509949440 tokens [see dataset size notes] = 2.8991029e+20 FLOP,128.0,,2025-05-01 10:42,,,,,,"Academia,Academia,Academia,Government",,,,,Open source,"apache 2
https://github.com/InternLM/OREAL
https://huggingface.co/internlm/OREAL-32B","Academia,Academia,Academia,Government",,,,100540.15721147944,Operation counting,internlm,,,
OREAL 7B,"Mathematics,Language","Mathematical reasoning,Quantitative reasoning,Language modeling/generation,Question answering","Shanghai AI Lab,Shanghai Jiao Tong University,Chinese University of Hong Kong (CUHK),InnoHK","Chengqi Lyu, Songyang Gao, Yuzhe Gu, Wenwei Zhang, Jianfei Gao, Kuikun Liu, Ziyi Wang, Shuaibin Li, Qian Zhao, Haian Huang, Weihan Cao, Jiangning Liu, Hongwei Liu, Junnan Liu, Songyang Zhang, Dahua Lin, Kai Chen",2025-02-10,Exploring the Limit of Outcome Reward for Learning Mathematical Reasoning,https://arxiv.org/abs/2502.06781,,,,7000000000.0,,,,"MATH,NuminaMath,Unspecified unreleased","""The training data for the RFT models consists of in-house datasets supported by OpenDataLab [40] and open-source datasets including Numina [41] and the training set of MATH [21]""",1509949440.0,"""During training iterations, each batch consists of 64 questions, with 16 rollouts per question. The max length of each rollout trajectory is set to 16384 tokens. Then the correctness of each response is averaged to calculate the pass rate, and questions with an overall pass rate of 0 or 1 are discarded.""

from https://github.com/InternLM/OREAL
90 steps

90*16384*64*16  = 1509949440 tokens  (upper bound since some questions were discarded)",9.0,"""It takes about 9 hours to train the model 90 steps with 32xA100.""",NVIDIA A100,,Likely,"Reasoning abilities, especially those for solving complex math problems, are crucial components of general intelligence. Recent advances by proprietary companies, such as o-series models of OpenAI, have made remarkable progress on reasoning tasks. However, the complete technical details remain unrevealed, and the techniques that are believed certainly to be adopted are only reinforcement learning (RL) and the long chain of thoughts. This paper proposes a new RL framework, termed OREAL, to pursue the performance limit that can be achieved through \textbf{O}utcome \textbf{RE}w\textbf{A}rd-based reinforcement \textbf{L}earning for mathematical reasoning tasks, where only binary outcome rewards are easily accessible. We theoretically prove that behavior cloning on positive trajectories from best-of-N (BoN) sampling is sufficient to learn the KL-regularized optimal policy in binary feedback environments. This formulation further implies that the rewards of negative samples should be reshaped to ensure the gradient consistency between positive and negative samples. To alleviate the long-existing difficulties brought by sparse rewards in RL, which are even exacerbated by the partial correctness of the long chain of thought for reasoning tasks, we further apply a token-level reward model to sample important tokens in reasoning trajectories for learning. With OREAL, for the first time, a 7B model can obtain 94.0 pass@1 accuracy on MATH-500 through RL, being on par with 32B models. OREAL-32B also surpasses previous 32B models trained by distillation with 95.0 pass@1 accuracy on MATH-500. Our investigation also indicates the importance of initial policy models and training queries for RL. Code, models, and data will be released to benefit future research\footnote{this https URL}.",1.0,,Open weights (unrestricted),"China,China,Hong Kong,China,Hong Kong",Qwen2.5-7B,78449696000000000000,"312000000000000 FLOP / GPU / sec [fp16 assumed] * 32 GPUs * 9 hours * 3600 sec / hour * 0.3 [assumed utilization] = 9.704448e+19 FLOP

6 FLOP / token / parameter * 7 * 10^ 9 parameters * 1509949440 tokens [see dataset size notes] = 6.3417876e+19 FLOP

sqrt(9.704448e+19*6.3417876e+19) = 7.8449696e+19 FLOP",32.0,,2025-05-01 10:42,,,,,,"Academia,Academia,Academia,Government",,,,,Open source,"apache 2
https://github.com/InternLM/OREAL
https://huggingface.co/internlm/OREAL-7B","Academia,Academia,Academia,Government",,,,25135.039302869864,"Operation counting,Hardware",internlm,,,
Animate Anyone 2,Video,"Video generation,Video-to-video,Image-to-video",Alibaba,"Li Hu, Guangyuan Wang, Zhen Shen, Xin Gao, Dechao Meng, Lian Zhuo, Peng Zhang, Bang Zhang, Liefeng Bo",2025-02-10,Animate Anyone 2: High-Fidelity Character Image Animation with Environment Affordance,https://arxiv.org/abs/2502.06145,,,,,,,,Unspecified unreleased,,,"""we curated a dataset of 100,000 character videos collected from the internet, encompassing a broader range of scene types, action categories, and human-object interaction cases""",,,NVIDIA A100,,Unknown,"Recent character image animation methods based on diffusion models, such as Animate Anyone, have made significant progress in generating consistent and generalizable character animations. However, these approaches fail to produce reasonable associations between characters and their environments. To address this limitation, we introduce Animate Anyone 2, aiming to animate characters with environment affordance. Beyond extracting motion signals from source video, we additionally capture environmental representations as conditional inputs. The environment is formulated as the region with the exclusion of characters and our model generates characters to populate these regions while maintaining coherence with the environmental context. We propose a shape-agnostic mask strategy that more effectively characterizes the relationship between character and environment. Furthermore, to enhance the fidelity of object interactions, we leverage an object guider to extract features of interacting objects and employ spatial blending for feature injection. We also introduce a pose modulation strategy that enables the model to handle more diverse motion patterns. Experimental results demonstrate the superior performance of the proposed method.",,,Unreleased,China,,,,8.0,,2025-05-19 16:16,,,,,,Industry,,,,,Unreleased,,Industry,,,,6283.759825717466,,,,,
Goku-8B,"Video,Vision","Video generation,Image generation,Text-to-video","The University of Hong Kong,ByteDance","Shoufa Chen, Chongjian Ge, Yuqi Zhang, Yida Zhang, Fengda Zhu, Hao Yang, Hongxiang Hao, Hui Wu, Zhichao Lai, Yifei Hu, Ting-Che Lin, Shilong Zhang, Fu Li, Chuan Li, Xing Wang, Yanghua Peng, Peize Sun, Ping Luo, Yi Jiang, Zehuan Yuan, Bingyue Peng, Xiaobing Liu",2025-02-07,Goku: Flow Based Video Generative Foundation Models,https://arxiv.org/abs/2502.04896v1,,,,8000000000.0,"""implemented through the Goku model family, which comprises Transformer architectures with 2B and 8B parameters.""",,,"LAION,Unspecified unreleased,Panda-70M,OpenVid-1M,Pexels","""After rigorous filtering, the final training dataset for Goku
consists of approximately 160M image-text pairs and 36M video-text pairs, encompassing both publicly available datasets and internally curated proprietary datasets. The detailed composition
of these resources is outlined as follows:

â€¢ Text-to-Image Data. Our text-to-image training dataset includes 100M public samples from LAION (Schuhmann et al., 2022) and 60M high-quality, internal samples. We use public data for pre-training and internal data for fine-tuning.

â€¢ Text-to-Video Data. Our T2V training dataset includes 11M public clips and 25M in-house clips. The former include Panda-70M (Chen et al., 2024b), InternVid (Wang et al., 2023b), OpenVid-1M (Nan et al., 2024), and Pexels (Lab and etc., 2024). Rather than directly using these datasets, we apply a data curation pipeline to keep high-quality samples""",,,,,,,Confident,"This paper introduces Goku, a state-of-the-art family of joint image-and-video generation models leveraging rectified flow Transformers to achieve industry-leading performance. We detail the foundational elements enabling high-quality visual generation, including the data curation pipeline, model architecture design, flow formulation, and advanced infrastructure for efficient and robust large-scale training. The Goku models demonstrate superior performance in both qualitative and quantitative evaluations, setting new benchmarks across major tasks. Specifically, Goku achieves 0.76 on GenEval and 83.65 on DPG-Bench for text-to-image generation, and 84.85 on VBench for text-to-video tasks. We believe that this work provides valuable insights and practical advancements for the research community in developing joint image-and-video generation models.",,,,"Hong Kong,China,China",,,,,,2025-05-19 12:11,,,,,,"Academia,Industry",,,,,,,"Academia,Industry",,,,,,,,,
Goku+,"Video,Vision","Video generation,Image-to-video,Text-to-video","The University of Hong Kong,ByteDance","Shoufa Chen, Chongjian Ge, Yuqi Zhang, Yida Zhang, Fengda Zhu, Hao Yang, Hongxiang Hao, Hui Wu, Zhichao Lai, Yifei Hu, Ting-Che Lin, Shilong Zhang, Fu Li, Chuan Li, Xing Wang, Yanghua Peng, Peize Sun, Ping Luo, Yi Jiang, Zehuan Yuan, Bingyue Peng, Xiaobing Liu",2025-02-07,Video Ads Foundation Models,https://saiyan-world.github.io/goku/,,,,,,,,,,,,,,,,Confident,"Goku+ is a new family of video foundation models built on top of Goku, specifically designed to optimize advertising scenarios involving humans and products to maximize performance. Our latest research breakthroughs show how you can create advertising videos at 100 times lower cost.

Goku+: Create Marketing Avatar from Text
Goku+ transforms text into hyper-realistic human videos, significantly outperforming existing methods. In particular, it can generate videos longer than 20 seconds, featuring stable hand movements and highly expressive facial and body movements of human subjects.

Goku+: Turn Product Image To Video Clip
Goku+ converts your product images into captivating video clips, ensuring end-to-end optimization. This process enhances the appeal and effectiveness of your marketing materials.

Goku+: Product and Human Interaction
Goku+ produces realistic and highly engaging videos tailored to showcase specific products. These videos effectively capture the essence of the products, boosting viewer engagement and interest.",,,,"Hong Kong,China,China",,,,,,2025-05-19 12:11,,,,,,"Academia,Industry",,,,,,,"Academia,Industry",,,,,,,,,
Eurus-2-7B-PRIME,Mathematics,Mathematical reasoning,"Tsinghua University,University of Illinois Urbana-Champaign (UIUC),Shanghai AI Lab,Peking University,Shanghai Jiao Tong University,CUHK Shenzhen Research Institute","Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu, Qixin Xu, Weize Chen, Jiarui Yuan, Huayu Chen, Kaiyan Zhang, Xingtai Lv, Shuo Wang, Yuan Yao, Xu Han, Hao Peng, Yu Cheng, Zhiyuan Liu, Maosong Sun, Bowen Zhou, Ning Ding",2025-02-03,Process Reinforcement through Implicit Rewards,https://arxiv.org/abs/2502.01456,30.0,SOTA improvement,"Eurus-2-7B-PRIME excels at competition-level mathematics benchmarks, outperforming advanced math models and larger models.",7000000000.0,,,,Eurus-2-RL-Data,Eurus-2-RL-Data is a high-quality RL training dataset of mathematics and coding problems with outcome verifiers (LaTeX answers for math and test cases for coding). https://huggingface.co/datasets/PRIME-RL/Eurus-2-RL-Data,319700000.0,"""We finally obtain 230K SFT data and the average response length is 1390 tokens.""
Does not include the RL finetuning data",,,,,Speculative,"Dense process rewards have proven a more effective alternative to the sparse outcome-level rewards in the inference-time scaling of large language models (LLMs), particularly in tasks requiring complex multi-step reasoning. While dense rewards also offer an appealing choice for the reinforcement learning (RL) of LLMs since their fine-grained rewards have the potential to address some inherent issues of outcome rewards, such as training efficiency and credit assignment, this potential remains largely unrealized. This can be primarily attributed to the challenges of training process reward models (PRMs) online, where collecting high-quality process labels is prohibitively expensive, making them particularly vulnerable to reward hacking. To address these challenges, we propose PRIME (Process Reinforcement through IMplicit rEwards), which enables online PRM updates using only policy rollouts and outcome labels through implict process rewards. PRIME combines well with various advantage functions and forgoes the dedicated reward model training phase that existing approaches require, substantially reducing the development overhead. We demonstrate PRIMEâ€™s effectiveness on competitional math and coding. Starting from Qwen2.5-Math-7B-Base, PRIME achieves a 15.1% average improvement across several key reasoning benchmarks over the SFT model. Notably, our resulting model, Eurus-2-7B-PRIME, surpasses Qwen2.5-Math-7B-Instruct on seven reasoning benchmarks with 10% of its training data.1",3.0,,Open weights (unrestricted),"China,United States of America,China,China,China,China",Qwen2.5-Math-7B-Base,,,,,2025-06-02 14:03,,,,,,"Academia,Academia,Academia,Academia,Academia",,,,,Open source,"Apache 2.0
https://huggingface.co/PRIME-RL/Eurus-2-7B-PRIME

Apache 2.0
https://github.com/PRIME-RL/PRIME","Academia,Academia,Academia,Academia,Academia",,,BF16,,,PRIME-RL,,,
Prithvi-EO-2.0 600M,Earth science,,"IBM Research,NASA,University of Alabama,University of Iceland,Forschungszentrum Julich,Virginia Tech (Virginia Polytechnic Institute and State University),Arizona State University,Oregon State University,Boston University,University of California (UC) Berkeley,Julich Supercomputing Center","Daniela Szwarcman, Sujit Roy, Paolo Fraccaro, Ãžorsteinn ElÃ­ GÃ­slason, Benedikt Blumenstiel, Rinki Ghosal, Pedro Henrique de Oliveira, Joao Lucas de Sousa Almeida, Rocco Sedona, Yanghui Kang, Srija Chakraborty, Sizhe Wang, Carlos Gomes, Ankur Kumar, Myscon Truong, Denys Godwin, Hyunho Lee, Chia-Yu Hsu, Ata Akbari Asanjan, Besart Mujeci, Disha Shidham, Trevor Keenan, Paulo Arevalo, Wenwen Li, Hamed Alemohammad, Pontus Olofsson, Christopher Hain, Robert Kennedy, Bianca Zadrozny, David Bell, Gabriele Cavallaro, Campbell Watson, Manil Maskey, Rahul Ramachandran, Juan Bernabe Moreno",2025-02-03,Prithvi-EO-2.0: A Versatile Multi-Temporal Foundation Model for Earth Observation Applications,https://arxiv.org/abs/2412.02732,,,,600000000.0,600M,1.954368e+22,312000000000000 FLOP / GPU / sec [bf16 assumed] * 58000 GPU-hours * 3600 sec / hour * 0.3 [assumed utilization] = 1.954368e+22 FLOP,HLS / Harmonized Landsat and Sentinel-2,,,"""Trained on 4.2M global time series samples from NASAâ€™s Harmonized Landsat and
Sentinel-2 data archive at 30m resolution""

""The models were trained for 400 epochs""",,"""the 600M models were [trained] on 240 GPUs (utilizing âˆ¼58.000 GPU-hours for the full training)""",NVIDIA A100,,Confident,"This technical report presents Prithvi-EO-2.0, a new geospatial foundation model that offers significant improvements over its predecessor, Prithvi-EO-1.0. Trained on 4.2M global time series samples from NASA's Harmonized Landsat and Sentinel-2 data archive at 30m resolution, the new 300M and 600M parameter models incorporate temporal and location embeddings for enhanced performance across various geospatial tasks. Through extensive benchmarking with GEO-Bench, the 600M version outperforms the previous Prithvi-EO model by 8\% across a range of tasks. It also outperforms six other geospatial foundation models when benchmarked on remote sensing tasks from different domains and resolutions (i.e. from 0.1m to 15m). The results demonstrate the versatility of the model in both classical earth observation and high-resolution applications. Early involvement of end-users and subject matter experts (SMEs) are among the key factors that contributed to the project's success. In particular, SME involvement allowed for constant feedback on model and dataset design, as well as successful customization for diverse SME-led applications in disaster response, land use and crop mapping, and ecosystem dynamics monitoring. Prithvi-EO-2.0 is available on Hugging Face and IBM terratorch, with additional resources on GitHub. The project exemplifies the Trusted Open Science approach embraced by all involved organizations.",400.0,,Open weights (unrestricted),"United States of America,Multinational,Ireland,United Kingdom of Great Britain and Northern Ireland,Brazil,Switzerland,United States of America,United States of America,Iceland,Germany,United States of America,United States of America,United States of America,United States of America,United States of America,Germany",,,,240.0,,2025-05-22 12:52,,,,,,"Industry,Government,Academia,Academia,Government,Academia,Academia,Academia,Academia,Academia,Government",,,,58000.0,,"Apache 2.0

https://huggingface.co/ibm-nasa-geospatial/Prithvi-EO-2.0-600M","Industry,Government,Academia,Academia,Government,Academia,Academia,Academia,Academia,Academia,Government",,,,188542.18349202207,Hardware,ibm-nasa-geospatial,,,
Prithvi-EO-2.0 300M,Earth science,,"IBM Research,NASA,University of Alabama,University of Iceland,Forschungszentrum Julich,Virginia Tech (Virginia Polytechnic Institute and State University),Arizona State University,Oregon State University,Boston University,University of California (UC) Berkeley,Julich Supercomputing Center","Daniela Szwarcman, Sujit Roy, Paolo Fraccaro, Ãžorsteinn ElÃ­ GÃ­slason, Benedikt Blumenstiel, Rinki Ghosal, Pedro Henrique de Oliveira, Joao Lucas de Sousa Almeida, Rocco Sedona, Yanghui Kang, Srija Chakraborty, Sizhe Wang, Carlos Gomes, Ankur Kumar, Myscon Truong, Denys Godwin, Hyunho Lee, Chia-Yu Hsu, Ata Akbari Asanjan, Besart Mujeci, Disha Shidham, Trevor Keenan, Paulo Arevalo, Wenwen Li, Hamed Alemohammad, Pontus Olofsson, Christopher Hain, Robert Kennedy, Bianca Zadrozny, David Bell, Gabriele Cavallaro, Campbell Watson, Manil Maskey, Rahul Ramachandran, Juan Bernabe Moreno",2025-02-03,Prithvi-EO-2.0: A Versatile Multi-Temporal Foundation Model for Earth Observation Applications,https://arxiv.org/abs/2412.02732,,,,300000000.0,300M,7.076160000000001e+21,312000000000000 FLOP / GPU / sec [bf16 assumed] * 21000 GPU-hours * 3600 sec / hour * 0.3 [assumed utilization] = 7.07616e+21 FLOP,HLS / Harmonized Landsat and Sentinel-2,,,"""Trained on 4.2M global time series samples from NASAâ€™s Harmonized Landsat and
Sentinel-2 data archive at 30m resolution""

""The models were trained for 400 epochs""",,"""the 300M models were trained on 80 GPUs (utilizing âˆ¼21.000 GPU-hours for the full training)""",NVIDIA A100,,Confident,"This technical report presents Prithvi-EO-2.0, a new geospatial foundation model that offers significant improvements over its predecessor, Prithvi-EO-1.0. Trained on 4.2M global time series samples from NASA's Harmonized Landsat and Sentinel-2 data archive at 30m resolution, the new 300M and 600M parameter models incorporate temporal and location embeddings for enhanced performance across various geospatial tasks. Through extensive benchmarking with GEO-Bench, the 600M version outperforms the previous Prithvi-EO model by 8\% across a range of tasks. It also outperforms six other geospatial foundation models when benchmarked on remote sensing tasks from different domains and resolutions (i.e. from 0.1m to 15m). The results demonstrate the versatility of the model in both classical earth observation and high-resolution applications. Early involvement of end-users and subject matter experts (SMEs) are among the key factors that contributed to the project's success. In particular, SME involvement allowed for constant feedback on model and dataset design, as well as successful customization for diverse SME-led applications in disaster response, land use and crop mapping, and ecosystem dynamics monitoring. Prithvi-EO-2.0 is available on Hugging Face and IBM terratorch, with additional resources on GitHub. The project exemplifies the Trusted Open Science approach embraced by all involved organizations.",400.0,,Open weights (unrestricted),"United States of America,Multinational,Ireland,United Kingdom of Great Britain and Northern Ireland,Brazil,Switzerland,United States of America,United States of America,Iceland,Germany,United States of America,United States of America,United States of America,United States of America,United States of America,Germany",,,,80.0,,2025-05-22 12:52,,,,,,"Industry,Government,Academia,Academia,Government,Academia,Academia,Academia,Academia,Academia,Government",,,,21000.0,,"Apache 2.0

https://huggingface.co/ibm-nasa-geospatial/Prithvi-EO-2.0-300M","Industry,Government,Academia,Academia,Government,Academia,Academia,Academia,Academia,Academia,Government",,,,62847.3944973407,Hardware,ibm-nasa-geospatial,,,
o3-mini,Language,"Language modeling/generation,Question answering,Quantitative reasoning,Code generation",OpenAI,"Training
Brian Zhang, Eric Mitchell, Hongyu Ren, Kevin Lu, Max Schwarzer, Michelle Pokrass, Shengjia Zhao, Ted Sanders

Eval
Adam Kalai, Alex Tachard Passos, Ben Sokolowsky, Elaine Ya Le, Erik Ritter, Hao Sheng, Hanson Wang, Ilya Kostrikov, James Lee, Johannes Ferstad, Michael Lampe, Prashanth Radhakrishnan, Sean Fitzgerald, Sebastien Bubeck, Yann Dubois, Yu Bai

Frontier Evals & Preparedness
Andy Applebaum, Elizabeth Proehl, Evan Mays, Joel Parish, Kevin Liu, Leon Maksin, Leyton Ho, Miles Wang, Michele Wang, Olivia Watkins, Patrick Chao, Samuel Miserendino, Tejal Patwardhan

Engineering
Adam Walker, Akshay Nathan, Alyssa Huang, Andy Wang, Ankit Gohel, Ben Eggers, Brian Yu, Bryan Ashley, Chengdu Huang, Christian Hoareau, Davin Bogan, Emily Sokolova, Eric Horacek, Eric Jiang, Felipe Petroski Such, Jonah Cohen, Josh Gross, Justin Becker, Kan Wu, Kevin Whinnery, Larry Lv, Lee Byron, Manoli Liodakis, Max Johnson, Mike Trpcic, Murat Yesildal, Rasmus Rygaard, RJ Marsan, Rohit Ramchandani, Rohan Kshirsagar, Roman Huet, Sara Conlon, Shuaiqi (Tony) Xia, Siyuan Fu, Srinivas Narayanan, Sulman Choudhry, Tomer Kaftan, Trevor Creech

Search
Adam Fry, Adam Perelman, Brandon Wang, Cristina Scheau, Philip Pronin, Sundeep Tirumalareddy, Will Ellsworth, Zewei Chu

Product
Antonia Woodford, Beth Hoover, Jake Brill, Kelly Stirman, Minnia Feng, Neel Ajjarapu, Nick Turley, Nikunj Handa, Olivier Godement

Safety
Andrea Vallone, Andrew Duberstein, Enis Sert, Eric Wallace, Grace Zhao, Irina Kofman, Jieqi Yu, Joaquin Quinonero Candela, Madelaine Boyd, Mehmet Yatbaz, Mike McClay, Mingxuan Wang, Saachi Jain, Sandhini Agarwal, Sam Toizer, Santiago HernÃ¡ndez, Steve Mostovoy, Young Cha, Tao Li, Yunyun Wang

External Redteaming
Lama Ahmad, Troy Peterson


Research Program Managers
Carpus Chang, Kristen Ying

Leadership
Aidan Clark, Dane Stuckey, Jerry Tworek, Jakub Pachocki, Johannes Heidecke, Kevin Weil, Liam Fedus, Mark Chen, Sam Altman, Wojciech Zaremba",2025-01-31,Pushing the frontier of cost-effective reasoning.,https://openai.com/index/openai-o3-mini/,,Significant use,,,,,,Unspecified unreleased,"This has a knowledge cutoff date of October 2023 - which I assume means October 1, 2023 - according to https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models?tabs=global-standard%2Cstandard-chat-completions#o-series-models.",,,,,,,Unknown,"Weâ€™re releasing OpenAI o3-mini, the newest, most cost-efficient model in our reasoning series, available in both ChatGPT and the API today. Previewed in December 2024â , this powerful and fast model advances the boundaries of what small models can achieve, delivering exceptional STEM capabilitiesâ€”with particular strength in science, math, and codingâ€”all while maintaining the low cost and reduced latency of OpenAI o1-mini.

OpenAI o3-mini is our first small reasoning model that supports highly requested developer features including function callingâ (opens in a new window), Structured Outputsâ (opens in a new window), and developer messagesâ (opens in a new window), making it production-ready out of the gate. Like OpenAI o1-mini and OpenAI o1-preview, o3-mini will support streamingâ (opens in a new window). Also, developers can choose between three reasoning effortâ (opens in a new window) optionsâ€”low, medium, and highâ€”to optimize for their specific use cases. This flexibility allows o3-mini to â€œthink harderâ€ when tackling complex challenges or prioritize speed when latency is a concern. o3-mini does not support vision capabilities, so developers should continue using OpenAI o1 for visual reasoning tasks. o3-mini is rolling out in the Chat Completions API, Assistants API, and Batch API starting today to select developers in API usage tiers 3-5â ",,,API access,United States of America,,,,,,2025-05-12 19:18,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
s1-32B,Language,"Mathematical reasoning,Language modeling/generation","Stanford University,University of Washington,Allen Institute for AI,Contextual AI","Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel CandÃ¨s, Tatsunori Hashimoto",2025-01-31,s1: Simple test-time scaling,https://arxiv.org/abs/2501.19393,331.0,,,32000000000.0,,,,"MATH,NuminaMath,s1K",,4700000.0,"1000 samples, 4.7M tokens total. From initial collection of 59K questions",7.0,,NVIDIA H100 NVL,,Confident,"Test-time scaling is a promising new approach to language modeling that uses extra test-time compute to improve performance. Recently, OpenAI's o1 model showed this capability but did not publicly share its methodology, leading to many replication efforts. We seek the simplest approach to achieve test-time scaling and strong reasoning performance. First, we curate a small dataset s1K of 1,000 questions paired with reasoning traces relying on three criteria we validate through ablations: difficulty, diversity, and quality. Second, we develop budget forcing to control test-time compute by forcefully terminating the model's thinking process or lengthening it by appending ""Wait"" multiple times to the model's generation when it tries to end. This can lead the model to double-check its answer, often fixing incorrect reasoning steps. After supervised finetuning the Qwen2.5-32B-Instruct language model on s1K and equipping it with budget forcing, our model s1-32B exceeds o1-preview on competition math questions by up to 27% (MATH and AIME24). Further, scaling s1-32B with budget forcing allows extrapolating beyond its performance without test-time intervention: from 50% to 57% on AIME24. Our model, data, and code are open-source at this https URL",5.0,,Open weights (unrestricted),"United States of America,United States of America,United States of America,United States of America",Qwen2.5 Instruct (32B),902400000000,"4.7M tokens for 32B model: 4.7 * 32000 * 6 = 902400M FLOP, or 902400000000",16.0,,2025-06-19 12:42,,,,16.0,,"Academia,Academia,Research collective,Industry",,,,,,,"Academia,Academia,Research collective,Industry",,,,12570.318670696644,,,,,
Mistral Small 3,Language,"Language modeling/generation,Question answering,Quantitative reasoning,Code generation,Translation",Mistral AI,,2025-01-30,"Mistral Small 3, a latency-optimized 24B-parameter model released under the Apache 2.0 license.",https://mistral.ai/news/mistral-small-3/,,,,24000000000.0,24B,1.152e+24,6ND = 6*8T tokens * 24B parameters = 1.152e+24 FLOP,Unspecified unreleased,"""Notably, Mistral Small 3 was developed without reinforcement learning or synthetic training data, techniques commonly used by competitors. Lample said this â€œrawâ€ approach helps avoid embedding unwanted biases that could be difficult to detect later.""",8000000000000.0,"8 trillion tokens

Source: https://venturebeat.com/ai/mistral-small-3-brings-open-source-ai-to-the-masses-smaller-faster-and-cheaper/",,,,,Confident,"Mistral Small 3 is competitive with larger models such as Llama 3.3 70B or Qwen 32B, and is an excellent open replacement for opaque proprietary models like GPT4o-mini. Mistral Small 3 is on par with Llama 3.3 70B instruct, while being more than 3x faster on the same hardware.

Mistral Small 3 is a pre-trained and instructed model catered to the â€˜80%â€™ of generative AI tasksâ€”those that require robust language and instruction following performance, with very low latency.

We designed this new model to saturate performance at a size suitable for local deployment. Particularly, Mistral Small 3 has far fewer layers than competing models, substantially reducing the time per forward pass. At over 81% accuracy on MMLU and 150 tokens/s latency, Mistral Small is currently the most efficient model of its category.

Weâ€™re releasing both a pretrained and instruction-tuned checkpoint under Apache 2.0. The checkpoints can serve as a powerful base for accelerating progress. Note that Mistral Small 3 is neither trained with RL nor synthetic data, so is earlier in the model production pipeline than models like Deepseek R1 (a great and complementary piece of open-source technology!). It can serve as a great base model for building accrued reasoning capacities. We look forward to seeing how the open-source community adopts and customizes it.",,,Open weights (unrestricted),France,,,,,,2025-05-29 00:47,,,,,,Industry,,,,,Unreleased,"Apache 2.0

https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501",Industry,,,,,Operation counting,mistralai,,,
Qwen2.5-Max,Language,"Language modeling/generation,Question answering,Code generation",Alibaba,Qwen Team,2025-01-28,Qwen2.5-Max: Exploring the Intelligence of Large-scale MoE Model,https://qwenlm.github.io/blog/qwen2.5-max/,,,,,,,,Unspecified unreleased,,20000000000000.0,"""Concurrently, we are developing Qwen2.5-Max, a large-scale MoE model that has been pretrained on over 20 trillion tokens and further post-trained with curated Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) methodologies. """,,,,,Unknown,"It is widely recognized that continuously scaling both data size and model size can lead to significant improvements in model intelligence. However, the research and industry community has limited experience in effectively scaling extremely large models, whether they are dense or Mixture-of-Expert (MoE) models. Many critical details regarding this scaling process were only disclosed with the recent release of DeepSeek V3. Concurrently, we are developing Qwen2.5-Max, a large-scale MoE model that has been pretrained on over 20 trillion tokens and further post-trained with curated Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) methodologies. Today, we are excited to share the performance results of Qwen2.5-Max and announce the availability of its API through Alibaba Cloud. We also invite you to explore Qwen2.5-Max on Qwen Chat!",,,API access,China,,,,,,2025-04-16 12:04,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
Janus-Pro-7B,"Image generation,Vision,Language,Multimodal","Image generation,Text-to-image,Visual question answering",DeepSeek,"Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan",2025-01-27,Janus-Pro: Unified Multimodal Understanding and Generation with Data and Model Scaling,https://github.com/deepseek-ai/Janus/blob/main/janus_pro_tech_report.pdf,,,,7000000000.0,"7B

In our experiments, we utilize DeepSeek-LLM (1.5B and 7B) [3] with a maximum supported sequence length of 4096 as the base language model. For the vision encoder used in understanding tasks, we select SigLIP-Large-Patch16-384 [53].",,,"ImageNet,YFCC-100M,Docmatrix,Unspecified unreleased","â€¢ Longer Training in Stage I: We increase the training steps in Stage I, allowing sufficient training on the ImageNet dataset. 
â€¢ Focused Training in Stage II: In Stage II, we drop ImageNet data and directly utilize normal text-to-image data to train the model to generate images based on dense descriptions. 

For the Stage II pretraining data, we refer to DeepSeek-VL2 [49] and add approximately 90 million samples. These include image caption datasets (e.g., YFCC [31]), as well as data for table, chart, and document understanding (e.g., Docmatix [20]). 

For the Stage III supervised fine-tuning data, we also incorporate additional datasets from DeepSeek-VL2, such as MEME understanding, Chinese conversational data, and datasets aimed at enhancing dialogue experiences.

In Janus-Pro, we incorporate approximately 72 million samples of synthetic aesthetic data, bringing the ratio of real to synthetic data to 1:1 during the unified pretraining stage. The prompts for these synthetic data samples are publicly available, such as those in [43].
",3138560000.0,"from table 2
assuming each sample is 384*384/(16*16) = 576 tokens (it might be less since it is not just image but text data as well but not OOM less)

20000*256 + 360000*512 + 40000*128) * 576 = 3138560000 tokens 

high confidence since it aligns well with the compute estimate",336.0,"""The whole training process took about 9/14 days on a cluster of 16/32 nodes for 1.5B/7B model, each equipped with 8 Nvidia A100 (40GB) GPUs.""

14*24 = 336 hours",NVIDIA A100 SXM4 40 GB,,Confident,"In this work, we introduce Janus-Pro, an advanced version of the previous work Janus. Specifically, Janus-Pro incorporates (1) an optimized training strategy, (2) expanded training data, and (3) scaling to larger model size. With these improvements, Janus-Pro achieves significant advancements in both multimodal  understanding and text-to-image instruction-following capabilities,
while also enhancing the stability of text-to-image generation. We hope this work will inspire further exploration in the field. Code and models are publicly available.",1.0,,Open weights (restricted use),China,"DeepSeek LLM 7B,SigLIP 400M",905748000000000000000,"312000000000000 FLOP / GPU / sec * 336 hours * 3600 sec / hour * 8 GPUs * 0.3 [assumed utilization] = 9.05748Ã—10^20 FLOP
(it might be more if utilization is higher than 0.3)

operation counting:
from table 2
assuming each sample is 384*384/(16*16) = 576 tokens (it might be less since it is not just image but text data as well but not OOM less)

6 FLOP / parameter / token * ((20000*256 + 360000*512 + 40000*128) * 576) tokens * 7*10^9 parameters = 4.70679552 Ã— 10^21 FLOP",8.0,,2025-05-30 14:45,,,,,,Industry,,,,,,"""This code repository is licensed under the MIT License. The use of Janus-Pro models is subject to DeepSeek Model License.""
""we added use-based restrictions not permitting the use of the model in very specific scenarios""

https://huggingface.co/deepseek-ai/Janus-Pro-7B",Industry,,,,6285.719226471854,"Hardware,Operation counting",deepseek-ai,,,
Computer-Using Agent (CUA),"Vision,Language,Multimodal","Instruction interpretation,System control",OpenAI,,2025-01-23,"Powering Operator with Computer-Using Agent, a universal interface for AI to interact with the digital world.",https://openai.com/index/computer-using-agent/,,SOTA improvement,SOTA at OSWorld,,,,,Unspecified unreleased,"This model seems to be based off the computer-use-preview, which had a knowledge cutoff date of October 2023 - which I assume means October 1, 2023 - according to https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models?tabs=global-standard%2Cstandard-chat-completions#computer-use-preview.",,,,,,,Unknown,"Today we introduced a research preview of Operatorâ (opens in a new window), an agent that can go to the web to perform tasks for you. Powering Operator is Computer-Using Agent (CUA), a model that combines GPTâ€‘4o's vision capabilities with advanced reasoning through reinforcement learning. CUA is trained to interact with graphical user interfaces (GUIs)â€”the buttons, menus, and text fields people see on a screenâ€”just as humans do. This gives it the flexibility to perform digital tasks without using OS-or web-specific APIs. 

CUA builds off of years of foundational research at the intersection of multimodal understanding and reasoning. By combining advanced GUI perception with structured problem-solving, it can break tasks into multi-step plans and adaptively self-correct when challenges arise. This capability marks the next step in AI development, allowing models to use the same tools humans rely on daily and opening the door to a vast range of new applications.

While CUA is still early and has limitations, it sets new state-of-the-art benchmark results, achieving a 38.1% success rate on OSWorld for full computer use tasks, and 58.1% on WebArena and 87% on WebVoyager for web-based tasks. These results highlight CUAâ€™s ability to navigate and operate across diverse environments using a single general action space. ",,,Hosted access (no API),United States of America,,,,,,2025-05-12 19:18,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
DoMINO,3D modeling,Aerodynamics simulations,NVIDIA,"Rishikesh Ranade, Mohammad Amin Nabian, Kaustubh Tangsali, Alexey Kamenev, Oliver Hennigh, Ram Cherukuri, Sanjay Choudhry",2025-01-23,DoMINO: A Decomposable Multi-scale Iterative Neural Operator for Modeling Large Scale Engineering Simulations,https://arxiv.org/abs/2501.13350,,,,,,,,DrivAerML,"""the DoMINO model is trained using DrivAerML dataset from the CAE ML Dataset collection. This high-fidelity, open-source (CC-BY-SA) public dataset is specifically designed for automotive aerodynamics research. It comprises 500 parametrically morphed variants of the widely utilized DrivAer notchback generic vehicle. Mesh generation and scale-resolving computational fluid dynamics (CFD) simulations were executed using consistent and validated automatic workflows that represent the industrial state-of-the-art. Geometries and comprehensive aerodynamic data are published in open-source formats. """,,,,,,,Unknown,"Numerical simulations play a critical role in design and development of engineering products and processes. Traditional computational methods, such as CFD, can provide accurate predictions but are computationally expensive, particularly for complex geometries. Several machine learning (ML) models have been proposed in the literature to significantly reduce computation time while maintaining acceptable accuracy. However, ML models often face limitations in terms of accuracy and scalability and depend on significant mesh downsampling, which can negatively affect prediction accuracy and generalization. In this work, we propose a novel ML model architecture, DoMINO (Decomposable Multi-scale Iterative Neural Operator) developed in NVIDIA Modulus to address the various challenges of machine learning based surrogate modeling of engineering simulations. DoMINO is a point cloudbased ML model that uses local geometric information to predict flow fields on discrete points. The DoMINO model is validated for the automotive aerodynamics use case using the DrivAerML dataset. Through our experiments we demonstrate the scalability, performance, accuracy and generalization of our model to both in-distribution and out-of-distribution testing samples. Moreover, the results are analyzed using a range of engineering specific metrics important for validating numerical simulations.",500.0,,,United States of America,,,,,,2025-05-12 12:31,,,,,,Industry,,,,,Open source,"https://github.com/NVIDIA/physicsnemo/tree/main/examples/cfd/external_aerodynamics/domino

Apache 2",Industry,,,FP16,,,,,,
Luma Ray2,Video,"Video generation,Text-to-video",LumaLabs,,2025-01-23,Introducing Luma Ray2,"https://aws.amazon.com/bedrock/luma-ai/?nc1=h_ls

https://lumalabs.ai/ray",,,,,,,,Unspecified unreleased,,,,,,,,Unknown,"Luma Ray2 is a large-scale video-generation model capable of creating realistic visuals with natural, coherent motion. It marks the beginning of a new generation of models able to produce fast, coherent motion, ultra-realistic details, and logical event sequences ready for production-level work.

Luma Ray2
Luma Ray2 is Luma AIâ€™s newest state-of-the-art video-generating model. Create high-quality and stunningly realistic video clips from natural language prompts. The model currently supports 5- and 9-second video generations with 540p and 720p resolution.

Max input tokens: 300

Languages: English

Fine-tuning supported: No

Supported use cases: Rapid dynamic video content creation for the gaming, film, entertainment, media, and ecommerce industries",,,API access,United States of America,,,,,,2025-05-19 12:16,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
Doubao-1.5-pro,Language,Language generation,ByteDance,,2025-01-22,Doubao-1.5-pro,https://team.doubao.com/zh/special/doubao_1_5_pro,,Training cost,,,"Not directly reported. We are told it is a MoE model, and that it matches the performance of a dense model trained on the same data, while using 1/7th of the activated parameters. Additionally they say ""The number of parameters of the Doubao dense model is also much smaller than that of Llama3.1-405B"", which suggests that the number of activated parameters on the forward pass is ""much less"" than 405B/7 = 58B parameters.",,"The model appears to have been trained on 9T tokens; since we believe the MoE model uses ""much less"" than 58B parameters (see parameter notes), training compute is likely to be less than 6 * 9T * 58B = 3.132e24

It is possible the 9T token training run was for comparison sake against the dense model (they label it as ""doubao-MoE"", not doubao-1.5-pro), and that they continued training beyond this. They would need to train for at least 29T tokens to ",,,9000000000000.0,9T tokens,,,,Self-supervised learning,Unknown,The model uses the MoE architecture and explores the ultimate balance between model performance and reasoning performance through integrated training-thinking design. Doubao-1.5-pro can use only a smaller activation parameter to exceed the performance of a first-class super-large pre-training model and achieve excellent results on multiple evaluation benchmarks.,1.0,,Hosted access (no API),China,,,,,,2025-05-09 11:32,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
Kimi k1.5,"Multimodal,Language,Vision","Language modeling/generation,Code generation,Quantitative reasoning,Question answering,Visual question answering,Translation,Image captioning",Moonshot,"Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, Chuning Tang, Congcong Wang, Dehao Zhang, Enming Yuan, Enzhe Lu, Fengxiang Tang, Flood Sung, Guangda Wei, Guokun Lai, Haiqing Guo, Han Zhu, Hao Ding, Hao Hu, Hao Yang, Hao Zhang, Haotian Yao, Haotian Zhao, Haoyu Lu, Haoze Li, Haozhen Yu, Hongcheng Gao, Huabin Zheng, Huan Yuan, Jia Chen, Jianhang Guo, Jianlin Su, Jianzhou Wang, Jie Zhao, Jin Zhang, Jingyuan Liu, Junjie Yan, Junyan Wu, Lidong Shi, Ling Ye, Longhui Yu, Mengnan Dong, Neo Zhang, Ningchen Ma, Qiwei Pan, Qucheng Gong, Shaowei Liu, Shengling Ma, Shupeng Wei, Sihan Cao, Siying Huang, Tao Jiang, Weihao Gao, Weimin Xiong, Weiran He, Weixiao Huang, Wenhao Wu, Wenyang He, Xianghui Wei, Xianqing Jia, Xingzhe Wu, Xinran Xu, Xinxing Zu, Xinyu Zhou, Xuehai Pan, Y. Charles, Yang Li, Yangyang Hu, Yangyang Liu, Yanru Chen, Yejie Wang, Yibo Liu, Yidao Qin, Yifeng Liu, Ying Yang, Yiping Bao, Yulun Du, Yuxin Wu, Yuzhi Wang, Zaida Zhou, Zhaoji Wang, Zhaowei Li, Zhen Zhu, Zheng Zhang, Zhexu Wang, Zhilin Yang, Zhiqi Huang, Zihao Huang, Ziyao Xu, Zonghan Yang",2025-01-22,Kimi k1.5: Scaling Reinforcement Learning with LLMs,https://arxiv.org/abs/2501.12599v1,0.0,SOTA improvement,"""Sota short-CoT performance, outperforming GPT-4o and Claude Sonnet 3.5 on AIME,  MATH-500, LiveCodeBench by a large margin (up to +550%)""",,,,,Unspecified unreleased,"""The Kimi k1.5 base model is trained on a diverse, high-quality multimodal corpus. The language data covers five domains: English, Chinese, Code, Mathematics Reasoning, and Knowledge. Multimodal data, including Captioning, Image-text Interleaving, OCR, Knowledge, and QA datasets, enables our model to acquire vision-language capabilities.
Rigorous quality control ensures relevance, diversity, and balance in the overall pretrain dataset.""",,,,,,,Unknown,"Language model pretraining with next token prediction has proved effective for scaling compute but is limited to the amount of available training data. Scaling reinforcement learning (RL) unlocks a new axis for the continued improvement of artificial intelligence, with the promise that large language models (LLMs) can scale their training data by learning to explore with rewards. However, prior published work has not produced competitive results. In light of this, we report on the training practice of Kimi k1.5, our latest multi-modal LLM trained with RL, including its RL training techniques, multi-modal data recipes, and infrastructure optimization. Long context scaling and improved policy optimization methods are key ingredients of our approach, which establishes a simplistic, effective RL framework without relying on more complex techniques such as Monte Carlo tree search, value functions, and process reward models. Notably, our system achieves state-of-the-art reasoning performance across multiple benchmarks and modalitiesâ€”e.g., 77.5 on AIME, 96.2 on MATH 500, 94-th percentile on Codeforces, 74.9 on MathVistaâ€”matching OpenAIâ€™s o1. Moreover, we present effective long2short methods that use long-CoT techniques to improve short-CoT models, yielding state-of-the-art short-CoT reasoning resultsâ€”e.g., 60.8 on AIME, 94.6 on MATH500, 47.3 on LiveCodeBenchâ€”outperforming existing short-CoT models such as GPT-4o and Claude Sonnet 3.5 by a large margin (up to +550%).",,,API access,China,,,,,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
DeepSeek-R1-Distill-Llama-70B,Language,"Language modeling/generation,Question answering,Quantitative reasoning,Code generation",DeepSeek,"DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z.F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J.L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R.J. Chen, R.L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S.S. Li et al. (100 additional authors not shown)",2025-01-22,DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning,https://arxiv.org/abs/2501.12948,,,,70000000000.0,70B,,"base model compute: 6.8649768e+24 FLOP
fine tune compute: 2.016e+22 FLOP",,"Estimated knowledge cutoff date of deepseek-r1-distill-llama is May 2024, according to https://hub.docker.com/r/ai/deepseek-r1-distill-llama.",24000000000.0,"""To equip more efficient smaller models with reasoning capabilities like DeepSeek-R1, we directly fine-tuned open-source models like Qwen (Qwen, 2024b) and Llama (AI@Meta, 2024) using the 800k samples curated with DeepSeek-R1, as detailed in Â§2.3.3""

assuming ~30000 tokens per sample

30000 * 800000 = 24 000 000 000 tokens (speculative)",,,,,Speculative,"We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.",2.0,,Open weights (unrestricted),China,Llama 3.3 70B,2,6 FLOP / token / parameter * 70 * 10^9 parameters * 24 * 10^9 tokens * 2 epochs = 2.016e+22 FLOP (speculative since number of tokens could be +- 1 OOM),,,2025-05-13 01:24,,,,,,Industry,,,,,,"https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B
MIT license",Industry,,,,,Operation counting,deepseek-ai,,,
DeepSeek-R1-Distill-Qwen-14B,Language,"Language modeling/generation,Quantitative reasoning,Question answering,Mathematical reasoning,Code generation",DeepSeek,"DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z.F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J.L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R.J. Chen, R.L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S.S. Li et al. (100 additional authors not shown)",2025-01-22,DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning,https://arxiv.org/abs/2501.12948,,,,14800000000.0,14.8B (safetensors),,,,"""finetuned with 800k samples curated with DeepSeek-R1""",24000000000.0,"""finetuned with 800k samples curated with DeepSeek-R1""

assuming ~30000 tokens per sample

30000 * 800000 = 24 000 000 000 tokens (speculative)",,,,,Speculative,"We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.",2.0,,Open weights (unrestricted),China,Qwen2.5-14B,4,6 FLOP / token / parameter * 14.8 * 10^9 parameters * 24 * 10^9 tokens * 2 epochs = 4.2624e+21 FLOP (speculative since number of tokens could be +- 1 OOM),,,2025-06-17 10:11,,,,,,Industry,,,,,,"https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B

MIT license",Industry,,,,,Operation counting,deepseek-ai,,,
DeepSeek-R1-Distill-Qwen-1.5B,Language,"Language modeling/generation,Quantitative reasoning,Question answering,Mathematical reasoning,Code generation",DeepSeek,"DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z.F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J.L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R.J. Chen, R.L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S.S. Li et al. (100 additional authors not shown)",2025-01-22,DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning,https://arxiv.org/abs/2501.12948,,,,1780000000.0,1.78B (safetensors),,,,"""finetuned with 800k samples curated with DeepSeek-R1""",24000000000.0,"""finetuned with 800k samples curated with DeepSeek-R1""

assuming ~30000 tokens per sample

30000 * 800000 = 24 000 000 000 tokens (speculative)",,,,,Speculative,"We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.",2.0,,Open weights (unrestricted),China,Qwen2.5-Math-1.5B,512640000000000000000,6 FLOP / token / parameter * 1.78 * 10^9 parameters * 24 * 10^9 tokens * 2 epochs = 5.1264e+20 FLOP (speculative since number of tokens could be +- 1 OOM),,,2025-06-17 10:11,,,,,,Industry,,,,,,"https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B

MIT license",Industry,,,,,Operation counting,deepseek-ai,,,
Hunyuan3D 2.0,"Language,Vision,3D modeling",3D reconstruction,Tencent,"Project Sponsors: Jie Jiang, Yuhong Liu, Di Wang, Yong Yang, Tian Liu
â€¢ Project Leaders: Chunchao Guo, Jingwei Huang
â€¢ Core Contributors:
â€“ Data: Lifu Wang, Jihong Zhang, Meng Chen, Liang Dong, Yiwen Jia, Yulin Cai, Jiaao Yu,
Yixuan Tang, Hao Zhang, Zheng Ye, Peng He, Runzhou Wu, Chao Zhang, Yonghao Tan
â€“ Shape Generation: Zeqiang Lai, Qingxiang Lin, Yunfei Zhao, Haolin Liu, Zibo Zhao
â€“ Texture Synthesis: Shuhui Yang, Yifei Feng, Mingxin Yang, Sheng Zhang
â€“ Downstream Tasks: Xianghui Yang, Huiwen Shi, Sicong Liu, Junta Wu, Yihang Lian, Fan
Yang, Ruining Tang, Zebin He, Xinzhou Wang, Jian Liu, Xuhui Zuo
â€“ Studio: Zhuo Chen, Biwen Lei, Haohan Weng, Jing Xu, Yiling Zhu, Xinhai Liu, Lixin Xu,
Changrong Hu, Tianyu Huang
â€¢ Contributors: Jie Xiao, Yangyu Tao, Jianchen Zhu, Jinbao Xue, Kai Liu, Chongqing Zhao,
Xinming Wu, Zhichao Hu, Lei Qin, Jianbing Peng, Zhan Li, Minghui Chen, Xipeng Zhang, Lin Niu,
Paige Wang, Yingkai Wang, Haozhao Kuang, Zhongyi Fan, Xu Zheng, Weihao Zhuang, YingPing
He",2025-01-21,"Hunyuan3D 2.0: Scaling Diffusion Models for High
Resolution Textured 3D Assets Generation",https://github.com/Tencent/Hunyuan3D-2/blob/main/assets/report/Tencent_Hunyuan3D_2_0.pdf,,,,,,,,Objaverse,,,,,,,,Unknown,"We present Hunyuan3D 2.0, an advanced large-scale 3D synthesis system for generating high-resolution textured 3D assets. This system includes two foundation components: a large-scale shape generation model â€“ Hunyuan3D-DiT, and a largescale texture synthesis model â€“ Hunyuan3D-Paint. The shape generative model,
built on a scalable flow-based diffusion transformer, aims to create geometry that properly aligns with a given condition image, laying a solid foundation for downstream applications. The texture synthesis model, benefiting from strong geometric
and diffusion priors, produces high-resolution and vibrant texture maps for either generated or hand-crafted meshes. Furthermore, we build Hunyuan3D-Studio â€“ a versatile, user-friendly production platform that simplifies the re-creation process of 3D assets. It allows both professional and amateur users to manipulate or even animate their meshes efficiently. We systematically evaluate our models, showing that Hunyuan3D 2.0 outperforms previous state-of-the-art models, including the open-source models and closed-source models in geometry details, condition alignment, texture quality, and etc. Hunyuan3D 2.0 is publicly released in order to fill the gaps in the open-source 3D community for large-scale foundation generative models. The code and pre-trained weights of our models are available at:
https://github.com/Tencent/Hunyuan3D-2.",,,Open weights (restricted use),China,,,,,,2025-05-01 10:42,,,,,,Industry,,,,,,"tencent-hunyuan-community license

https://huggingface.co/tencent/Hunyuan3D-2",Industry,,,,,,tencent,,,
DeepSeek-R1,Language,"Language modeling/generation,Code generation,Quantitative reasoning,Question answering",DeepSeek,,2025-01-20,DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning,https://api-docs.deepseek.com/news/news250120,,"Training cost,SOTA improvement","Best score on SuperCLUE Math6o in Jan 2025
https://www.superclueai.com/",671000000000.0,"671B total
37B activated
https://github.com/deepseek-ai/DeepSeek-R1/tree/main",4.020010000000001e+24,"Estimates by Ege Erdil in Gradient Updates:
https://epoch.ai/gradient-updates/what-went-into-training-deepseek-r1
""A dataset size of 14.8 trillion tokens is reasonable and in line with other models of this scale. Assuming thatâ€™s valid, the pretraining of this model would have required 6 * (37 billion) * (14.8 trillion) = 3e24 FLOP. If we assume DeepSeekâ€™s training cluster consists of H800s with the PCIe form factor, then each should be capable of 1.5e15 FP8 per second, and the implied model FLOP utilization (MFU) of DeepSeek v3â€™s 55 day training run ends up being around 23%.""

6 FLOP/token/param * 14.8T tokens * 37B active params = 3.29e24 FLOP (pretraining)
1.2e23 FLOP (post-training)
6.1e23 FLOP (fine-tuning)

Total compute: 3.29e24 + 1.2e23 + 6.1e23 = 4.02e24",Unspecified unreleased,"RL + SFT

When reasoning-oriented RL converges, we utilize the resulting checkpoint to collect SFT (Supervised Fine-Tuning) data for the subsequent round. Unlike the initial cold-start data, which primarily focuses on reasoning, this stage incorporates data from other domains to enhance the modelâ€™s capabilities in writing, role-playing, and other general-purpose tasks.",,,,,,,Confident,"We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities.
Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.",,,Open weights (unrestricted),China,DeepSeek-V3,6,6.1e23 FLOP from these estimations: https://epoch.ai/gradient-updates/what-went-into-training-deepseek-r1,,,2025-06-20 12:23,,,,,,Industry,,,,,Unreleased,MIT licensed,Industry,$6770000.00,,"FP8,BF16,FP32",,Operation counting,deepseek-ai,,,
DeepSeek-R1-Zero,Language,"Language modeling/generation,Code generation,Quantitative reasoning,Question answering",DeepSeek,,2025-01-20,DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning,https://api-docs.deepseek.com/news/news250120,,,,671000000000.0,"671B total
37B activated
https://github.com/deepseek-ai/DeepSeek-R1/tree/main",,,Unspecified unreleased,RL without (!) SFT,,,,,,,Confident,"We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities.
Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-
R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.",,,Open weights (unrestricted),China,DeepSeek-V3,,,,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,MIT licensed,Industry,,,,,,deepseek-ai,,,
Eagle 2,"Vision,Robotics,Language",,"NVIDIA,Nanjing University,Tsinghua University,Hong Kong Polytechnic University,Johns Hopkins University,New York University (NYU)","Zhiqi Li, Guo Chen, Shilong Liu, Shihao Wang, Vibashan VS, Yishen Ji, Shiyi Lan, Hao Zhang, Yilin Zhao, Subhashree Radhakrishnan, Nadine Chang, Karan Sapra, Amala Sanjay Deshmukh, Tuomas Rintamaki, Matthieu Le, Ilia Karmanov, Lukas Voegtle, Philipp Fischer, De-An Huang, Timo Roman, Tong Lu, Jose M. Alvarez, Bryan Catanzaro, Jan Kautz, Andrew Tao, Guilin Liu, Zhiding Yu",2025-01-20,Eagle 2: Building Post-Training Data Strategies from Scratch for Frontier Vision-Language Models,arxiv.org/abs/2501.14818,9.0,SOTA improvement,"Figure 1: ""Overview of Eagle2-9Bâ€™s result across different
multimodal benchmarks, in comparison to state-of-the-art
open-source and commercial frontier models.""
Claim SOTA on OCRBench, InfoVQA, ChartQA (Test), MathVista, AI2D (Test), MMStar (relative to the selected open-source + commercial frontier models)

Todo: check Table 7 to confirm they didn't just omit larger models from Figure 1

VLM backbone of GR00T N1",8930000000.0,"Table 4

https://huggingface.co/nvidia/Eagle2-9B
""8.93B params""
",4.7156e+22,"Appendix A. Compute: ""We show our training resource for Eagle2-9B in Tab. A. In actual development, we rarely iterate the Stage-1 model. Usually, we iterate Stage-1.5 once after iterating Stage-2 >10 times.""
Assume "">10 times"" -> 12
Assume ""H100"" -> NVIDIA H100 SXM5 80GB
Assume bf16
Assume 0.4 utilization (NVIDIA in-house)
H100 SXM5 performance = 989400000000000 FLOP/s = 9.894e14 FLOP/s

Stage 1:     (H100 * 128) * (2.5 hr * 1)
Stage 1.5:  (H100 * 256) * (28 hr * 2)
Stage 2:     (H100 * 256) * (6 hr * 12)

Stage 1:      0.4 * (9.894e14 FLOP/s * 128) * (3600 s / 1 hr) * (2.5 hr *1) ~= 4.56e20 FLOP
Stage 1.5:   0.4 * (9.894e14 FLOP/s * 256) * (3600 s / 1 hr) * (28 hr * 2) ~= 2.04e22 FLOP
Stage 2:      0.4 * (9.894e14 FLOP/s * 256) * (3600 s / 1 hr) * (6 hr * 12) ~= 2.63e22 FLOP

(4.56e20 + 2.04e22 + 2.63e22) FLOP ~= 4.72e22 FLOP
",,"

",,Table 4: ,130.5,"Appendix A. Compute: ""We show our training resource for Eagle2-9B in Tab. A. In actual development, we rarely iterate the Stage-1 model. Usually, we iterate Stage-1.5 once after iterating Stage-2 >10 times.
Assume "">10 times"" -> 12
Stage 1.0:  2.5 hr * 1
Stage 1.5   28 hr * 2
Stage 2.0:  6 hr * 12
(2.5 hr * 1) + (28 hr * 2) + (6 hr * 12) = 130.5 hr
 ",NVIDIA H100 SXM5 80GB,,Confident,"Recently, promising progress has been made by open-source vision-language models (VLMs) in bringing their capabilities closer to those of proprietary frontier models. However, most open-source models only publish their final model weights, leaving the critical details of data strategies and implementation largely opaque. In this work, we address VLM post-training from a data-centric perspective, showing the key role of data strategy in developing frontier VLMs. By studying and building our post-training data strategy from scratch, we share detailed insights into the development processes, aiming to benefit the development of competitive models for the open-source community. Our introduced data strategy, together with training recipes and model design, leads to a family of performant VLMs named Eagle2. Specifically, Eagle2-9B achieves state-of-the-art results across various multimodal benchmarks, matching certain competitive models with up to 70B parameters.",,,Open weights (non-commercial),"United States of America,China,China,Hong Kong,China,United States of America,United States of America",Qwen2.5-7B,,,256.0,,2025-06-06 12:06,,,,,,"Industry,Academia,Academia,Academia,Academia,Academia",,,,,Unreleased,"https://huggingface.co/nvidia/Eagle2-9B
""Creative Commons Attribution Non Commercial 4.0""

Apache 2.0 for code
https://github.com/NVlabs/EAGLE","Industry,Academia,Academia,Academia,Academia,Academia",,,,352055.152733459,,nvidia,,,
INTELLECT-MATH,Mathematics,Mathematical reasoning,Prime Intellect,,2025-01-17,"INTELLECT-MATH: Frontier Mathematical Reasoning through Better Initializations for Reinforcement Learning
","https://huggingface.co/PrimeIntellect/INTELLECT-MATH
https://www.primeintellect.ai/blog/intellect-math",,SOTA improvement,"Outperforms other models on several benchmarks - Math-500, OlympiadBench, AMC, MINERVA MATH, AVG.",7000000000.0,,,,,,,"""The resulting SFT dataset consisted of 733k questions with corresponding responses.""",,,,,Confident,,,,Open weights (unrestricted),United States of America,Qwen2.5-Math-7B-Base,,,,,2025-06-02 10:29,,,,,,Industry,,,,,Open source,"MIT license
https://huggingface.co/PrimeIntellect/INTELLECT-MATH

https://github.com/PrimeIntellect-ai/INTELLECT-MATH",Industry,,,BF16,,,PrimeIntellect,,,
GPT-4b micro,Biology,"Protein generation,Proteins,Protein or nucleotide language model (pLM/nLM)","OpenAI,Retro Biosciences","John Hallman, Rico Meinl",2025-01-17,"OpenAIâ€™s new model, called GPT-4b micro, was trained to suggest ways to re-engineer the protein factors to increase their function.",https://www.technologyreview.com/2025/01/17/1110086/openai-has-created-an-ai-model-for-longevity-science/,,,,,,,GPT-4b is a micro model compared to the other ChatGPT options as it was only trained on protein data.,Unspecified unreleased,"""The model was trained on examples of protein sequences from many species, as well as information on which proteins tend to interact with one another. While thatâ€™s a lot of data, itâ€™s just a fraction of what OpenAIâ€™s flagship chatbots were trained on, making GPT-4b an example of a â€œsmall language modelâ€ that works with a focused data set.""",,,,,,,Unknown,"OpenAIâ€™s new model, called GPT-4b micro, was trained to suggest ways to re-engineer the protein factors to increase their function. According to OpenAI, researchers used the modelâ€™s suggestions to change two of the Yamanaka factors to be more than 50 times as effectiveâ€”at least according to some preliminary measures. 

â€œJust across the board, the proteins seem better than what the scientists were able to produce by themselves,â€ says John Hallman, an OpenAI researcher.",,,Unreleased,"United States of America,United States of America",,,,,,2025-05-01 10:42,,,,,,"Industry,Industry",,,,,Unreleased,"Outside scientists wonâ€™t be able to tell if the results are real until theyâ€™re published, something the companies say they are planning. Nor is the model available for wider useâ€”itâ€™s still a bespoke demonstration, not an official product launch.","Industry,Industry",,,,,,,,,
MatterGen,Materials science,"Materials design,Crystal discovery",Microsoft Research AI for Science,"Claudio Zeni, Robert Pinsler, Daniel ZÃ¼gner, Andrew Fowler, Matthew Horton, Xiang Fu, Zilong Wang, Aliaksandra Shysheya, Jonathan CrabbÃ©, Shoko Ueda, Roberto Sordillo, Lixin Sun, Jake Smith, Bichlien Nguyen, Hannes Schulz, Sarah Lewis, Chin-Wei Huang, Ziheng Lu, Yichi Zhou, Han Yang, Hongxia Hao, Jielan Li, Chunlei Yang, Wenjie Li, Ryota Tomioka, Tian Xie",2025-01-16,A generative model for inorganic materials design,https://www.nature.com/articles/s41586-025-08628-5,,,,46800000.0,"""MatterGen contains 46.8M parameters""

https://huggingface.co/microsoft/mattergen

""The model architecture is based on GemNet (Gasteiger et al. 2021).""",2.69568e+19,312000000000000*3600*10*8*0.3 = 2.69568e+19,"Materials Project,Alexandria","""MP-20 (Jain et al., 2013): contains 45k general inorganic materials, including most experimentally known materials with no more than 20 atoms in unit cell.

Alex-MP-20: Training dataset consisting of around 600k structures from MP-20 and Alexandria (Schmidt et al. 2022) with at most 20 atoms inside the unit cell and below 0.1 eV/atom of the convex hull. See the venn diagram below and the MatterGen paper for more details.""",960032000.0,"""It is trained on 608,000 stable materials from the Materials Project(opens in new tab) (MP) and Alexandria(opens in new tab) (Alex) databases.""

26956800000000000000 (estimated training compute) / (46800000 parameters * 6 * 100 epochs * 608000 materials) ~ 1579 tokens per materials 

1579*608,000 = 960032000",10.0,"""One training epoch of around 600K training samples takes around 6 minutes on 8 NVIDIA A100 GPUs""

100 epochs -> 600 minutes = 10 hours",NVIDIA A100,,Confident,"The design of functional materials with desired properties is essential in driving technological advances in areas like energy storage, catalysis, and carbon capture1â€“3. Generative models provide a new paradigm for materials design by directly generating novel materials given desired property constraints, but current methods have low success rate in proposing stable crystals or can only satisfy a limited set of property constraints 4âˆ’11. Here, we present MatterGen, a model that generates stable, diverse inorganic materials across the periodic table and can further be fine-tuned to steer the generation towards a broad range of property constraints. Compared to prior generative models 4,12, structures produced by MatterGen are more than twice as likely to be novel and stable, and more than 10 times closer to the local energy minimum. After fine-tuning, MatterGen successfully generates stable, novel materials with desired chemistry, symmetry, as well as mechanical, electronic and magnetic properties. As a proof of concept, we synthesize one of the generated structures and measure its property value to be within 20 % of our target. We believe that the quality of generated materials and the breadth of MatterGenâ€™s capabilities represent a major advancement towards creating a foundational generative model for materials design.",100.0,,Open weights (unrestricted),"United States of America,United Kingdom of Great Britain and Northern Ireland,China,Netherlands,Germany",,,,8.0,,2025-05-01 10:42,,,,,,Industry,,,,,Open source,"MIT license

https://huggingface.co/microsoft/mattergen

https://github.com/microsoft/mattergen",Industry,,,,6287.259184247614,Hardware,microsoft,,,
s1,Language,"Language modeling/generation,Question answering,Quantitative reasoning","Stanford University,University of Washington,Allen Institute for AI,Contextual AI","Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel CandÃ¨s, Tatsunori Hashimoto",2025-01-14,s1: Simple test-time scaling,https://arxiv.org/abs/2501.19393,,,,32000000000.0,32B,,,s1K,,4700000.0, 4.7M tokens (table 5),0.3,"""We perform supervised fine-tuning (SFT) of an off-the-shelf pretrained model on our small dataset requiring just 26 minutes of training on 16 H100 GPUs""",NVIDIA H100 SXM5 80GB,,Confident,"Test-time scaling is a promising new approach to language modeling that uses extra test-time compute to improve performance. Recently, OpenAI's o1 model showed this capability but did not publicly share its methodology, leading to many replication efforts. We seek the simplest approach to achieve test-time scaling and strong reasoning performance. First, we curate a small dataset s1K of 1,000 questions paired with reasoning traces relying on three criteria we validate through ablations: difficulty, diversity, and quality. Second, we develop budget forcing to control test-time compute by forcefully terminating the model's thinking process or lengthening it by appending ""Wait"" multiple times to the model's generation when it tries to end. This can lead the model to double-check its answer, often fixing incorrect reasoning steps. After supervised finetuning the Qwen2.5-32B-Instruct language model on s1K and equipping it with budget forcing, our model s1-32B exceeds o1-preview on competition math questions by up to 27% (MATH and AIME24). Further, scaling s1-32B with budget forcing allows extrapolating beyond its performance without test-time intervention: from 50% to 57% on AIME24. Our model, data, and code are open-source at this https URL",5.0,,Open weights (unrestricted),"United States of America,United States of America,United States of America,United States of America",Qwen2.5 Instruct (72B),5781963700000000000,"989500000000000 FLOP / GPU / sec * 16 GPUs * 26 minutes * 60 sec / min * 0.3 [assumed utilization] = 7.409376e+18 FLOP

6ND = 6 FLOP / token / parameter * 32 * 10^9 parameters * 4700000 tokens * 5 epochs = 4.512e+18 FLOP

sqrt(4.512e+18 * 7.409376e+18) = 5.7819637e+18",16.0,,2025-05-01 10:42,,,,,,"Academia,Academia,Research collective,Industry",,,,,Open source,"Apache 2.0 https://huggingface.co/simplescaling/s1-32B

Apache 2.0 https://github.com/simplescaling/s1","Academia,Academia,Research collective,Industry",,,,22006.387259856583,"Hardware,Operation counting",simplescaling,,,
MiniMax-Text-01,Language,"Language modeling/generation,Question answering",MiniMax,"MiniMax, Aonian Li, Bangwei Gong, Bo Yang, Boji Shan, Chang Liu, Cheng Zhu, Chunhao Zhang, Congchao Guo, Da Chen, Dong Li, Enwei Jiao, Gengxin Li, Guojun Zhang, Haohai Sun, Houze Dong, Jiadai Zhu, Jiaqi Zhuang, Jiayuan Song, Jin Zhu, Jingtao Han, Jingyang Li, Junbin Xie, Junhao Xu, Junjie Yan, Kaishun Zhang, Kecheng Xiao, Kexi Kang, Le Han, Leyang Wang, Lianfei Yu, Liheng Feng, Lin Zheng, Linbo Chai, Long Xing, Meizhi Ju, Mingyuan Chi, Mozhi Zhang, Peikai Huang, Pengcheng Niu, Pengfei Li, Pengyu Zhao, Qi Yang, Qidi Xu, Qiexiang Wang, Qin Wang, Qiuhui Li, Ruitao Leng, Shengmin Shi, Shuqi Yu, Sichen Li, Songquan Zhu, Tao Huang, Tianrun Liang, Weigao Sun, Weixuan Sun, Weiyu Cheng, Wenkai Li, Xiangjun Song, Xiao Su, Xiaodong Han, Xinjie Zhang, Xinzhu Hou, Xu Min, Xun Zou, Xuyang Shen, Yan Gong, Yingjie Zhu, Yipeng Zhou, Yiran Zhong, Yongyi Hu, Yuanxiang Fan, Yue Yu, Yufeng Yang, Yuhao Li, Yunan Huang, Yunji Li, Yunpeng Huang, Yunzhi Xu, Yuxin Mao, Zehan Li, Zekang Li, Zewei Tao, Zewen Ying, Zhaoyang Cong, Zhen Qin, Zhenhua Fan, Zhihang Yu, Zhuo Jiang, Zijia Wu",2025-01-14,MiniMax-01: Scaling Foundation Models with Lightning Attention,https://arxiv.org/abs/2501.08313,,,,456000000000.0,"Total Parameters: 456B
Activated Parameters per Token: 45.9B
Number Layers: 80
Hybrid Attention: a softmax attention is positioned after every 7 lightning attention.
Number of attention heads: 64
Attention head dimension: 128
Mixture of Experts:
Number of experts: 32
Expert hidden dimension: 9216
Top-2 routing strategy
Positional Encoding: Rotary Position Embedding (RoPE) applied to half of the attention head dimension with a base frequency of 10,000,000
Hidden Size: 6144
Vocab Size: 200,064",1.98288e+24,"6 FLOP / token / parameter * 45.9 * 10^9 activated parameters * 7.2 * 10^12 tokens = 1.98288e+24 FLOP

""Likely"" confidence because the model is MoE (formula might not be that accurate) + I am not confidentely sure about dataset size",Unspecified unreleased,,7200000000000.0,7.2T tokens,,,NVIDIA H800 SXM5,,Likely,"We introduce MiniMax-01 series, including MiniMax-Text-01 and MiniMax-VL-01, which are comparable to top-tier models while offering superior capabilities in processing longer contexts. The core lies in lightning attention and its efficient scaling. To maximize computational capacity, we integrate it with Mixture of Experts (MoE), creating a model with 32 experts and 456 billion total parameters, of which 45.9 billion are activated for each token. We develop an optimized parallel strategy and highly efficient computation-communication overlap techniques for MoE and lightning attention. This approach enables us to conduct efficient training and inference on models with hundreds of billions of parameters across contexts spanning millions of tokens. The context window of MiniMax-Text-01 can reach up to 1 million tokens during training and extrapolate to 4 million tokens during inference at an affordable cost. Our vision-language model, MiniMax-VL-01 is built through continued training with 512 billion vision-language tokens. Experiments on both standard and in-house benchmarks show that our models match the performance of state-of-the-art models like GPT-4o and Claude-3.5-Sonnet while offering 20-32 times longer context window. We publicly release MiniMax-01 at this https URL.",,,Open weights (restricted use),China,,,,,,2025-05-30 14:44,,,,,,Industry,,,,,Unreleased,"https://huggingface.co/MiniMaxAI/MiniMax-Text-01

""MiniMax may terminate this Agreement if you are in breach of any term or condition of this Agreement.""

code seems to be just inference code: 
https://github.com/MiniMax-AI/MiniMax-01",Industry,,,,,Operation counting,MiniMaxAI,,,
MiniMax-VL-01,"Vision,Language,Multimodal","Visual question answering,Language modeling/generation,Question answering",MiniMax,"MiniMax, Aonian Li, Bangwei Gong, Bo Yang, Boji Shan, Chang Liu, Cheng Zhu, Chunhao Zhang, Congchao Guo, Da Chen, Dong Li, Enwei Jiao, Gengxin Li, Guojun Zhang, Haohai Sun, Houze Dong, Jiadai Zhu, Jiaqi Zhuang, Jiayuan Song, Jin Zhu, Jingtao Han, Jingyang Li, Junbin Xie, Junhao Xu, Junjie Yan, Kaishun Zhang, Kecheng Xiao, Kexi Kang, Le Han, Leyang Wang, Lianfei Yu, Liheng Feng, Lin Zheng, Linbo Chai, Long Xing, Meizhi Ju, Mingyuan Chi, Mozhi Zhang, Peikai Huang, Pengcheng Niu, Pengfei Li, Pengyu Zhao, Qi Yang, Qidi Xu, Qiexiang Wang, Qin Wang, Qiuhui Li, Ruitao Leng, Shengmin Shi, Shuqi Yu, Sichen Li, Songquan Zhu, Tao Huang, Tianrun Liang, Weigao Sun, Weixuan Sun, Weiyu Cheng, Wenkai Li, Xiangjun Song, Xiao Su, Xiaodong Han, Xinjie Zhang, Xinzhu Hou, Xu Min, Xun Zou, Xuyang Shen, Yan Gong, Yingjie Zhu, Yipeng Zhou, Yiran Zhong, Yongyi Hu, Yuanxiang Fan, Yue Yu, Yufeng Yang, Yuhao Li, Yunan Huang, Yunji Li, Yunpeng Huang, Yunzhi Xu, Yuxin Mao, Zehan Li, Zekang Li, Zewei Tao, Zewen Ying, Zhaoyang Cong, Zhen Qin, Zhenhua Fan, Zhihang Yu, Zhuo Jiang, Zijia Wu",2025-01-14,MiniMax-01: Scaling Foundation Models with Lightning Attention,https://arxiv.org/abs/2501.08313,,,,,,2.1238848e+24,1.98288e+24 FLOP [base model compute] + 1.410048e+23 FLOP [addtional vision-language finetune compute] = 2.1238848e+24 FLOP,Unspecified unreleased,,512000000000.0,"""MiniMax-VL-01 undergoes additional training with 512 billion vision-language tokens""",,,NVIDIA H800 SXM5,,Likely,"We introduce MiniMax-01 series, including MiniMax-Text-01 and MiniMax-VL-01, which are comparable to top-tier models while offering superior capabilities in processing longer contexts. The core lies in lightning attention and its efficient scaling. To maximize computational capacity, we integrate it with Mixture of Experts (MoE), creating a model with 32 experts and 456 billion total parameters, of which 45.9 billion are activated for each token. We develop an optimized parallel strategy and highly efficient computation-communication overlap techniques for MoE and lightning attention. This approach enables us to conduct efficient training and inference on models with hundreds of billions of parameters across contexts spanning millions of tokens. The context window of MiniMax-Text-01 can reach up to 1 million tokens during training and extrapolate to 4 million tokens during inference at an affordable cost. Our vision-language model, MiniMax-VL-01 is built through continued training with 512 billion vision-language tokens. Experiments on both standard and in-house benchmarks show that our models match the performance of state-of-the-art models like GPT-4o and Claude-3.5-Sonnet while offering 20-32 times longer context window. We publicly release MiniMax-01 at this https URL.",,,Open weights (restricted use),China,MiniMax-Text-01,1,"Assuming same amount of activated parameters (45.9 * 10^9) as for the base model:

6 FLOP / parameter / token * 45.9 * 10^9 activated parameters * 512 * 10^9 tokens = 1.410048e+23 FLOP",,,2025-05-30 14:44,,,,,,Industry,,,,,Unreleased,"https://huggingface.co/MiniMaxAI/MiniMax-VL-01

""MiniMax may terminate this Agreement if you are in breach of any term or condition of this Agreement.""

code seems to be just inference code: 
https://github.com/MiniMax-AI/MiniMax-01",Industry,,,,,Operation counting,MiniMaxAI,,,
"Cosmos-1.0-
Diffusion-14B Video2World","Robotics,Vision,Video","Robotic manipulation,Self-driving car,Video generation",NVIDIA,"NVIDIA: Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, Daniel Dworakowski, Jiaojiao Fan, Michele Fenzi, Francesco Ferroni, Sanja Fidler, Dieter Fox, Songwei Ge, Yunhao Ge, Jinwei Gu, Siddharth Gururani, Ethan He, Jiahui Huang, Jacob Huffman, Pooya Jannaty, Jingyi Jin, Seung Wook Kim, Gergely KlÃ¡r, Grace Lam, Shiyi Lan, Laura Leal-Taixe, Anqi Li, Zhaoshuo Li, Chen-Hsuan Lin, Tsung-Yi Lin, Huan Ling, Ming-Yu Liu, Xian Liu, Alice Luo, Qianli Ma, Hanzi Mao, Kaichun Mo, Arsalan Mousavian, Seungjun Nah, Sriharsha Niverty, David Page, Despoina Paschalidou, Zeeshan Patel, Lindsey Pavao, Morteza Ramezanali, Fitsum Reda, Xiaowei Ren, Vasanth Rao Naik Sabavat, Ed Schmerling, Stella Shi, Bartosz Stefaniak, Shitao Tang, Lyne Tchapmi, Przemek Tredak, Wei-Cheng Tseng, Jibin Varghese, Hao Wang, Haoxiang Wang, Heng Wang, Ting-Chun Wang, Fangyin Wei, Xinyue Wei, Jay Zhangjie Wu, Jiashu Xu, Wei Yang, Lin Yen-Chen, Xiaohui Zeng, Yu Zeng, Jing Zhang, Qinsheng Zhang, Yuxuan Zhang, Qingqing Zhao, Artur Zolkowski",2025-01-07,Cosmos World Foundation Model Platform for Physical AI,https://arxiv.org/abs/2501.03575,,,,14000000000.0,14B,6.1554816e+24,"""We train all of the WFM models reported in the paper using a cluster of 10,000 NVIDIA H100 GPUs in a time span of three months.""

989500000000000 FLOP / sec / GPU * 0.4 [assumed utilization] * 10000 GPUs * 3600 sec / hour * 3 months * 30 days / month * 24 hours / day  = 3.0777408e+25 FLOP

(total training compute)

assuming this model is 1/5 of it:

3.0777408e+25 / 5 = 6.1554816e+24 (Likely confidence)",Unspecified unreleased,,9000000000000000.0,"""Suite of first-generation video models trained on 9,000 trillion tokens, including 20 million hours of robotics and driving data - generating high-quality videos from multimodal inputs like images, text, or video."" - https://www.nvidia.com/en-us/ai/cosmos/ ",,"""We train all of the WFM models reported in the paper using a cluster of 10,000 NVIDIA H100 GPUs in a time span of three months.""",NVIDIA H100 SXM5 80GB,,Likely,"Physical AI needs to be trained digitally first. It needs a digital twin of itself, the policy model, and a digital twin of the world, the world model. In this paper, we present the Cosmos World Foundation Model Platform to help developers build customized world models for their Physical AI setups. We position a world foundation model as a general-purpose world model that can be fine-tuned into customized world models for downstream applications. Our platform covers a video curation pipeline, pre-trained world foundation models, examples of post-training of pre-trained world foundation models, and video tokenizers. To help Physical AI builders solve the most critical problems of our society, we make our platform open-source and our models open-weight with permissive licenses available via this https URL.",,,Open weights (restricted use),United States of America,,,,10000.0,,2025-05-26 16:12,,,,,,Industry,,,,,Unreleased,"NVIDIA Open Model License Agreement
Under the NVIDIA Open Model License, NVIDIA confirms:

Models are commercially usable.
You are free to create and distribute Derivative Models.
NVIDIA does not claim ownership to any outputs generated using the Models or Derivative Models.

Important Note: If you bypass, disable, reduce the efficacy of, or circumvent any technical limitation, safety guardrail or associated safety guardrail hyperparameter, encryption, security, digital rights management, or authentication mechanism contained in the Model, your rights under NVIDIA Open Model License Agreement will automatically terminate.

https://huggingface.co/nvidia/Cosmos-1.0-Diffusion-14B-Video2World",Industry,,,,13756136.253818648,Hardware,nvidia,,,
OLMo 2 Furious 7B,Language,"Language modeling/generation,Question answering","Allen Institute for AI,University of Washington,New York University (NYU)","Team OLMo, Pete Walsh, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Shane Arora, Akshita Bhagia, Yuling Gu, Shengyi Huang, Matt Jordan, Nathan Lambert, Dustin Schwenk, Oyvind Tafjord, Taira Anderson, David Atkinson, Faeze Brahman, Christopher Clark, Pradeep Dasigi, Nouha Dziri, Michal Guerquin, Hamish Ivison, Pang Wei Koh, Jiacheng Liu, Saumya Malik, William Merrill, Lester James V. Miranda, Jacob Morrison, Tyler Murray, Crystal Nam, Valentina Pyatkin, Aman Rangapur, Michael Schmitz, Sam Skjonsberg, David Wadden, Christopher Wilhelm, Michael Wilson, Luke Zettlemoyer, Ali Farhadi, Noah A. Smith, Hannaneh Hajishirzi",2024-12-31,2 OLMo 2 Furious,https://arxiv.org/abs/2501.00656,,,,7000000000.0,7B,1.8e+23,1.8*10^23 FLOPs (Table 6 - developers calculated using 6ND formula),"OLMo-Mix-1124,Dolmino-Mix-1124,Tulu 3",,4000000000000.0,"Pretraining Stage 1
(OLMo-Mix-1124)	4 trillion tokens (= 1 epoch)	

Pretraining Stage 2
(Dolmino-Mix-1124)	50B tokens (3 runs)
merged	

Post-training
(Tulu 3 SFT OLMo mix)	SFT + DPO + PPO
(preference mix)	",,,NVIDIA H100 SXM5 80GB,,Confident,"We present OLMo 2, the next generation of our fully open language models. OLMo 2 includes dense autoregressive models with improved architecture and training recipe, pretraining data mixtures, and instruction tuning recipes. Our modified model architecture and training recipe achieve both better training stability and improved per-token efficiency. Our updated pretraining data mixture introduces a new, specialized data mix called Dolmino Mix 1124, which significantly improves model capabilities across many downstream task benchmarks when introduced via late-stage curriculum training (i.e. specialized data during the annealing phase of pretraining). Finally, we incorporate best practices from TÃ¼lu 3 to develop OLMo 2-Instruct, focusing on permissive data and extending our final-stage reinforcement learning with verifiable rewards (RLVR). Our OLMo 2 base models sit at the Pareto frontier of performance to compute, often matching or outperforming open-weight only models like Llama 3.1 and Qwen 2.5 while using fewer FLOPs and with fully transparent training data, code, and recipe. Our fully open OLMo 2-Instruct models are competitive with or surpassing open-weight only models of comparable size, including Qwen 2.5, Llama 3.1 and Gemma 2. We release all OLMo 2 artifacts openly -- models at 7B and 13B scales, both pretrained and post-trained, including their full training data, training code and recipes, training logs and thousands of intermediate checkpoints. The final instruction model is available on the Ai2 Playground as a free research demo.",1.0,,Open weights (unrestricted),"United States of America,United States of America,United States of America",,,,,,2025-05-16 10:30,,,,,,"Research collective,Academia,Academia",,,,,Open source,"apache 2
https://huggingface.co/allenai/OLMo-2-1124-7B
https://github.com/allenai/OLMo","Research collective,Academia,Academia",,,,,"Reported,Operation counting",allenai,,,
OLMo 2 Furious 13B,Language,"Language modeling/generation,Question answering","Allen Institute for AI,University of Washington,New York University (NYU)","Team OLMo, Pete Walsh, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Shane Arora, Akshita Bhagia, Yuling Gu, Shengyi Huang, Matt Jordan, Nathan Lambert, Dustin Schwenk, Oyvind Tafjord, Taira Anderson, David Atkinson, Faeze Brahman, Christopher Clark, Pradeep Dasigi, Nouha Dziri, Michal Guerquin, Hamish Ivison, Pang Wei Koh, Jiacheng Liu, Saumya Malik, William Merrill, Lester James V. Miranda, Jacob Morrison, Tyler Murray, Crystal Nam, Valentina Pyatkin, Aman Rangapur, Michael Schmitz, Sam Skjonsberg, David Wadden, Christopher Wilhelm, Michael Wilson, Luke Zettlemoyer, Ali Farhadi, Noah A. Smith, Hannaneh Hajishirzi",2024-12-31,2 OLMo 2 Furious,https://arxiv.org/abs/2501.00656,,,,13000000000.0,13B,4.600000000000001e+23,"4.6*10^23 FLOPs (Table 6 - developers calculated using 6ND formula)
","OLMo-Mix-1124,Dolmino-Mix-1124,Tulu 3",,4000000000000.0,"Pretraining Stage 1
(OLMo-Mix-1124)	5 trillion tokens ( = 1.2 epochs)
Pretraining Stage 2
(Dolmino-Mix-1124) 100B tokens (3 runs)
300B tokens (1 run)
merged
Post-training
(Tulu 3 SFT OLMo mix)	SFT + DPO + PPO
(preference mix)	",,,NVIDIA H100 SXM5 80GB,,Confident,"We present OLMo 2, the next generation of our fully open language models. OLMo 2 includes dense autoregressive models with improved architecture and training recipe, pretraining data mixtures, and instruction tuning recipes. Our modified model architecture and training recipe achieve both better training stability and improved per-token efficiency. Our updated pretraining data mixture introduces a new, specialized data mix called Dolmino Mix 1124, which significantly improves model capabilities across many downstream task benchmarks when introduced via late-stage curriculum training (i.e. specialized data during the annealing phase of pretraining). Finally, we incorporate best practices from TÃ¼lu 3 to develop OLMo 2-Instruct, focusing on permissive data and extending our final-stage reinforcement learning with verifiable rewards (RLVR). Our OLMo 2 base models sit at the Pareto frontier of performance to compute, often matching or outperforming open-weight only models like Llama 3.1 and Qwen 2.5 while using fewer FLOPs and with fully transparent training data, code, and recipe. Our fully open OLMo 2-Instruct models are competitive with or surpassing open-weight only models of comparable size, including Qwen 2.5, Llama 3.1 and Gemma 2. We release all OLMo 2 artifacts openly -- models at 7B and 13B scales, both pretrained and post-trained, including their full training data, training code and recipes, training logs and thousands of intermediate checkpoints. The final instruction model is available on the Ai2 Playground as a free research demo.",1.2,,Open weights (unrestricted),"United States of America,United States of America,United States of America",,,,,,2025-06-10 17:14,,,,,,"Research collective,Academia,Academia",,,,,Open source,"apache 2
https://huggingface.co/allenai/OLMo-2-1124-13B
https://github.com/allenai/OLMo","Research collective,Academia,Academia",,,,,"Reported,Operation counting",allenai,,,
HiDream Foundation Model 3.0,"Video,Image generation,Vision,3D modeling","Video generation,Image-to-video,Text-to-video,Image generation,Text-to-image",HiDream,,2024-12-28,The world's Most Powerful Multimodality Foundation Model,https://www.chinaitcapital.com/detail/126,,,,10000000000.0,"10B - 13B 
Diffusion Transformer (DiT)",,,Unspecified unreleased,,,,,,,,Likely,"The large image multi-model model is the large model developed independently by the future of (HiDream.ai). It is the first image and video generated by the world to be open for online use to generate Diffusion Transformer (DiT) architecture model. The scale of the model parameters has exceeded 1 billion, realizing the joint modeling of text, image, video, 3D It has been filed through both the model and the algorithm. Based on the â€ Imagine AIâ€œ series of products constructed by the large image model â€, the functions of image generation editing, 4K HD picture, global/local controllable, script multi-lens video generation, etc. have been realized in AIGC technology and digital creativity. Has a leading advantage in commercialization.",,,API access,China,,,,,,2025-05-22 12:02,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
QVQ,"Language,Vision,Multimodal","Visual question answering,Video description,Language modeling/generation,Translation,Question answering,Character recognition,Quantitative reasoning",Alibaba,"Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, Junyang Lin",2024-12-25,QVQ: To See the World with Wisdom,"https://qwenlm.github.io/blog/qvq-72b-preview/
https://arxiv.org/abs/2502.13923",,,,72000000000.0,,,,Unspecified unreleased,,,,,,,,Confident,"QVQ - an open-weight model for multimodal reasoning, built upon Qwen2-VL-72B. QVQ represents a significant leap forward in AIâ€™s capacity for visual understanding and complex problem-solving. QVQ achieves a score of 70.3 on MMMU and shows substantial improvements across math-related benchmarks compared to Qwen2-VL-72B-Instruct. Through careful step-by-step reasoning, QVQ demonstrates enhanced capabilities in visual reasoning tasks, particularly excelling in domains that demand sophisticated analytical thinking.",,,Open weights (restricted use),China,Qwen2-VL-72B,,,,,2025-05-31 21:22,,,,,,Industry,,,,,Unreleased,"https://huggingface.co/Qwen/QVQ-72B-Preview

Qwen license (restriction on >100m monthly users)",Industry,,,,,,Qwen,,,
DeepSeek-V3,Language,"Language modeling/generation,Code generation,Quantitative reasoning,Question answering",DeepSeek,"DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J.L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jiawei Wang, Jin Chen, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, Junxiao Song, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Litong Wang, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qiancheng Wang, Qihao Zhu, Qinyu Chen, Qiushi Du, R.J. Chen, R.L. Jin, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, Runxin Xu, Ruoyu Zhang, Ruyi Chen, S.S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Shuting Pan, T. Wang, Tao Yun, Tian Pei, Tianyu Sun, W.L. Xiao, Wangding Zeng et al. (100 additional authors not shown)",2024-12-24,DeepSeek-V3 Technical Report,https://arxiv.org/abs/2412.19437,,Training cost,training cost was $5.3million USD (Table 1),671000000000.0,Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token.,3.407799999999999e+24,"""At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model. The subsequent training stages after pre-training require only 0.1M GPU hours.""

6 * 37B (active params) * 14.8T = 3.2856e24 for pretraining.

We know they trained in FP8. H800s get 1.513e15 FLOP/s in FP8:
2.688M * 3600 * 1.513e15 * MFU = 3.2856e24

Suggests a MFU of 0.2244 in pre-training. If we assume MFU was the same in post-training, that adds an additional:

0.1M * 3600 * 1.513e15 * 0.2244 = 1.222e23 FLOP from post-training

Total: 3.2856e24 + 1.222e23 = 3.4078e24 FLOP",,,14800000000000.0,"""We pre-train DeepSeek-V3 on 14.8 trillion diverse and
high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities""",,"""DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training""",NVIDIA H800 SXM5,,Confident,"We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training. In addition, its training process is remarkably stable. Throughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks.",,,Open weights (restricted use),China,,,,2048.0,0.1947,2025-05-29 15:15,,Paper on DeepSeek-V3,,,,Industry,,,7.5928392e+24,2788000.0,Unreleased,"MIT and deepseek license
https://github.com/deepseek-ai/DeepSeek-V3?tab=readme-ov-file

I cannot see training code in this repo https://github.com/deepseek-ai/DeepSeek-V3?tab=readme-ov-file",Industry,$5390000.00,"Table 4: Training metric comparison


""Causal MFU only takes into account the flops of the lower triangle of the attention matrix (in line with FlashAttention), while non-causal MFU includes the flops of the whole attention matrix (in line with Megatron)""

TFLOPS (causal) = 385
H800 maximum FP8 performance = 1979 TFLOPS

therefore MFU = 385/1979 = 0.1947","FP8,BF16,FP32",2818135.181214284,"Operation counting,Hardware",deepseek-ai,,,
o3,"Language,Vision,Multimodal","Language modeling/generation,Question answering,Quantitative reasoning,Code generation,Visual question answering,Search,Instruction interpretation,Visual puzzles",OpenAI,,2024-12-20,"Our most powerful reasoning model with leading performance on coding, math, science, and vision",https://openai.com/index/introducing-o3-and-o4-mini/,,Significant use,,,,,,Unspecified unreleased,"""The OpenAI o-series models are trained with large-scale reinforcement learning on chains of thought.""

""OpenAI o3 and o4-mini were trained on diverse datasets,
including information that is publicly available on the internet, information that we partner with third parties to access, and information that our users or human trainers and researchers provide or generate. Our data processing pipeline includes rigorous filtering to maintain data quality and mitigate potential risks. We use advanced data filtering processes to reduce personal information
from training data. We also employ a combination of our Moderation API and safety classifiers to help prevent the use of harmful or sensitive content, including explicit materials such as sexual content involving a minor.""

This has a knowledge cutoff date of May 31, 2024, according to https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models?tabs=global-standard%2Cstandard-chat-completions#o-series-models. ",,,,,,,Unknown,"Today, weâ€™re releasing OpenAI o3 and o4-mini, the latest in our o-series of models trained to think for longer before responding. These are the smartest models weâ€™ve released to date, representing a step change in ChatGPT's capabilities for everyone from curious users to advanced researchers. For the first time, our reasoning models can agentically use and combine every tool within ChatGPTâ€”this includes searching the web, analyzing uploaded files and other data with Python, reasoning deeply about visual inputs, and even generating images. Critically, these models are trained to reason about when and how to use tools to produce detailed and thoughtful answers in the right output formats, typically in under a minute, to solve more complex problems. This allows them to tackle multi-faceted questions more effectively, a step toward a more agentic ChatGPT that can independently execute tasks on your behalf. The combined power of state-of-the-art reasoning with full tool access translates into significantly stronger performance across academic benchmarks and real-world tasks, setting a new standard in both intelligence and usefulness.
<..>
OpenAI o3 is our most powerful reasoning model that pushes the frontier across coding, math, science, visual perception, and more. It sets a new SOTA on benchmarks including Codeforces, SWE-bench (without building a custom model-specific scaffold), and MMMU. Itâ€™s ideal for complex queries requiring multi-faceted analysis and whose answers may not be immediately obvious. It performs especially strongly at visual tasks like analyzing images, charts, and graphics. In evaluations by external experts, o3 makes 20 percent fewer major errors than OpenAI o1 on difficult, real-world tasksâ€”especially excelling in areas like programming, business/consulting, and creative ideation. Early testers highlighted its analytical rigor as a thought partner and emphasized its ability to generate and critically evaluate novel hypothesesâ€”particularly within biology, math, and engineering contexts.

________
model was announced 2024/12/20 
from ARS technica: ""On Friday, during Day 12 of its ""12 days of OpenAI,"" OpenAI CEO Sam Altman announced its latest AI ""reasoning"" models, o3 and o3-mini, which build upon the o1 models launched earlier this year. The company is not releasing them yet but will make these models available for public safety testing and research access today.""

https://arstechnica.com/information-technology/2024/12/openai-announces-o3-and-o3-mini-its-next-simulated-reasoning-models/

model was released 2025/04/16",,,API access,United States of America,,,,,,2025-05-12 19:12,,,,,,Industry,,,,,Unreleased,"""Both o3 and o4-mini are also available to developers today via the Chat Completions API and Responses API (some developers will need to verify their organizationsâ (opens in a new window) to access these models)""",Industry,,,,,,,,,
Gemini 2.0 Flash Thinking,"Language,Vision,Multimodal","Language modeling/generation,Quantitative reasoning,Question answering,Visual question answering,Code generation","Google DeepMind,Google",Gemini Team,2024-12-19,"Our enhanced reasoning model, capable of showing its thoughts to improve performance and explainability",https://deepmind.google/technologies/gemini/flash-thinking/,,,,,,,,Unspecified unreleased,Knowledge cutoff	June 2024,,,,,,,Unknown,"Combining speed and performance, 2.0 Flash Thinking Experimental also excels in science and math, showing its thinking to solve complex problems.

Enhanced performance
Improvements on math and science benchmarks.

Long context
A one-million token context window enables deeper analysis of long-form text.

Improved thinking
More consistency between thoughts and answers.

Tool use
Turn on code execution to run and evaluate code.

Best for	Complex tasks without the need for low latency",,,API access,"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France,United States of America",,,,,,2025-05-12 20:14,,,,,,"Industry,Industry",,,,,Unreleased,"Availability	
Google AI Studio
Gemini API
Vertex AI
Gemini App","Industry,Industry",,,,,,,,,
Llama 3.1 Typhoon 2 70B,Language,"Language modeling/generation,Question answering,Quantitative reasoning",Typhoon / SCB 10X,"Kunat Pipatanakul, Potsawee Manakul, Natapong Nitarach, Warit Sirichotedumrong, Surapon Nonesung, Teetouch Jaknamon, Parinthapat Pengpun, Pittawat Taveekitworachai, Adisai Na-Thalang, Sittipong Sripaisarnmongkol, Krisanapong Jirayoot, Kasima Tharnpipitchai",2024-12-19,Typhoon 2: A Family of Open Text and Multimodal Thai Large Language Models,https://arxiv.org/abs/2412.13702,,,,70000000000.0,70B,,,"Unspecified unreleased,Typhoon 1 General Corpus","CPT: the entire dataset is 44B tokens but the model was trained on a subset of it (unreported) 
trained with LoRA ~10% of dense compute
assuming (""Likely"" confidence) ~25b tokens

SFT: ""Smaller models and 70B model â€“ approximately 600-800M tokens.""",10000000000.0,,,,NVIDIA H100 SXM5 80GB,,Speculative,"This paper introduces Typhoon 2, a series of text and multimodal large language models optimized for the Thai language. The series includes models for text, vision, and audio. Typhoon2-Text builds on state-of-the-art open models, such as Llama 3 and Qwen2, and we perform continual pre-training on a mixture of English and Thai data. We employ post-training techniques to enhance Thai language performance while preserving the base models' original capabilities. We release text models across a range of sizes, from 1 to 70 billion parameters, available in both base and instruction-tuned variants. To guardrail text generation, we release Typhoon2-Safety, a classifier enhanced for Thai cultures and language. Typhoon2-Vision improves Thai document understanding while retaining general visual capabilities, such as image captioning. Typhoon2-Audio introduces an end-to-end speech-to-speech model architecture capable of processing audio, speech, and text inputs and generating both text and speech outputs.",,,Open weights (restricted use),Thailand,Llama 3.1-70B,4,"6 FLOP / parameter / token * 10 *10^9 tokens [see dataset size] * 70 * 10^9 parameters = 4.2e+21 FLOP

""Speculative"" confidence since LoRA might decrease compute",8.0,,2025-05-06 11:50,,,,,,Industry,,,,,Unreleased,"llama 3.1 license
https://huggingface.co/scb10x/llama3.1-typhoon2-70b",Industry,,,,11009.566363976552,Operation counting,scb10x,,,
Llama 3.1 Typhoon 2 8B,Language,"Language modeling/generation,Question answering,Quantitative reasoning",Typhoon / SCB 10X,"Kunat Pipatanakul, Potsawee Manakul, Natapong Nitarach, Warit Sirichotedumrong, Surapon Nonesung, Teetouch Jaknamon, Parinthapat Pengpun, Pittawat Taveekitworachai, Adisai Na-Thalang, Sittipong Sripaisarnmongkol, Krisanapong Jirayoot, Kasima Tharnpipitchai",2024-12-19,Typhoon 2: A Family of Open Text and Multimodal Thai Large Language Models,https://arxiv.org/abs/2412.13702,,,,8000000000.0,8B,,,"Unspecified unreleased,Typhoon 1 General Corpus","CPT: the entire dataset is 44B tokens but the model was trained on a subset of it (unreported)
assuming (""Likely"" confidence)  ~25b tokens

SFT: ""7-8B parameter models â€“ a total of approximately 1.2B tokens.""",25000000000.0,,,,NVIDIA H100 SXM5 80GB,,Likely,"This paper introduces Typhoon 2, a series of text and multimodal large language models optimized for the Thai language. The series includes models for text, vision, and audio. Typhoon2-Text builds on state-of-the-art open models, such as Llama 3 and Qwen2, and we perform continual pre-training on a mixture of English and Thai data. We employ post-training techniques to enhance Thai language performance while preserving the base models' original capabilities. We release text models across a range of sizes, from 1 to 70 billion parameters, available in both base and instruction-tuned variants. To guardrail text generation, we release Typhoon2-Safety, a classifier enhanced for Thai cultures and language. Typhoon2-Vision improves Thai document understanding while retaining general visual capabilities, such as image captioning. Typhoon2-Audio introduces an end-to-end speech-to-speech model architecture capable of processing audio, speech, and text inputs and generating both text and speech outputs.",,,Open weights (restricted use),Thailand,Llama 3.1-8B,1,6 FLOP / parameter / token * 25*10^9 tokens [see dataset size] * 8 * 10^9 parameters = 1.2e+21 FLOP,8.0,,2025-05-06 11:50,,,,,,Industry,,,,,Unreleased,"llama 3.1 license
https://huggingface.co/scb10x/llama3.1-typhoon2-8b",Industry,,,,11009.566363976552,Operation counting,scb10x,,,
Typhoon 2 7B,Language,"Language modeling/generation,Question answering,Quantitative reasoning",Typhoon / SCB 10X,"Kunat Pipatanakul, Potsawee Manakul, Natapong Nitarach, Warit Sirichotedumrong, Surapon Nonesung, Teetouch Jaknamon, Parinthapat Pengpun, Pittawat Taveekitworachai, Adisai Na-Thalang, Sittipong Sripaisarnmongkol, Krisanapong Jirayoot, Kasima Tharnpipitchai",2024-12-19,Typhoon 2: A Family of Open Text and Multimodal Thai Large Language Models,https://arxiv.org/abs/2412.13702,,,,7000000000.0,7B,,,"Unspecified unreleased,Typhoon 1 General Corpus","CPT: the entire dataset is 44B tokens but the model was trained on a subset of it (unreported)
assuming (""Likely"" confidence)  ~25b tokens

SFT: ""7-8B parameter models â€“ a total of approximately 1.2B tokens.""",25000000000.0,,,,NVIDIA H100 SXM5 80GB,,Likely,"This paper introduces Typhoon 2, a series of text and multimodal large language models optimized for the Thai language. The series includes models for text, vision, and audio. Typhoon2-Text builds on state-of-the-art open models, such as Llama 3 and Qwen2, and we perform continual pre-training on a mixture of English and Thai data. We employ post-training techniques to enhance Thai language performance while preserving the base models' original capabilities. We release text models across a range of sizes, from 1 to 70 billion parameters, available in both base and instruction-tuned variants. To guardrail text generation, we release Typhoon2-Safety, a classifier enhanced for Thai cultures and language. Typhoon2-Vision improves Thai document understanding while retaining general visual capabilities, such as image captioning. Typhoon2-Audio introduces an end-to-end speech-to-speech model architecture capable of processing audio, speech, and text inputs and generating both text and speech outputs.",,,Open weights (unrestricted),Thailand,Qwen2.5-7B,1,6 FLOP / parameter / token * 25*10^9 tokens [see dataset size] * 7 * 10^9 parameters = 1.05e+21 FLOP,8.0,,2025-05-06 11:50,,,,,,Industry,,,,,Unreleased,"Apache 2.0
https://huggingface.co/scb10x/typhoon2-qwen2.5-7b",Industry,,,,11009.566363976552,Operation counting,scb10x,,,
LLama 3.2 Typhoon 2 3B,Language,"Language modeling/generation,Question answering,Quantitative reasoning",Typhoon / SCB 10X,"Kunat Pipatanakul, Potsawee Manakul, Natapong Nitarach, Warit Sirichotedumrong, Surapon Nonesung, Teetouch Jaknamon, Parinthapat Pengpun, Pittawat Taveekitworachai, Adisai Na-Thalang, Sittipong Sripaisarnmongkol, Krisanapong Jirayoot, Kasima Tharnpipitchai",2024-12-19,Typhoon 2: A Family of Open Text and Multimodal Thai Large Language Models,https://arxiv.org/abs/2412.13702,,,,3000000000.0,3B,,,Unspecified unreleased,"CPT: supposedly none for this model

SFT: ""Smaller models and 70B model â€“ approximately 600-800M tokens.""",700000000.0,,,,NVIDIA H100 SXM5 80GB,,Likely,"This paper introduces Typhoon 2, a series of text and multimodal large language models optimized for the Thai language. The series includes models for text, vision, and audio. Typhoon2-Text builds on state-of-the-art open models, such as Llama 3 and Qwen2, and we perform continual pre-training on a mixture of English and Thai data. We employ post-training techniques to enhance Thai language performance while preserving the base models' original capabilities. We release text models across a range of sizes, from 1 to 70 billion parameters, available in both base and instruction-tuned variants. To guardrail text generation, we release Typhoon2-Safety, a classifier enhanced for Thai cultures and language. Typhoon2-Vision improves Thai document understanding while retaining general visual capabilities, such as image captioning. Typhoon2-Audio introduces an end-to-end speech-to-speech model architecture capable of processing audio, speech, and text inputs and generating both text and speech outputs.",,,Open weights (restricted use),Thailand,Llama 3.2 3B,12600000000000000000,6 FLOP / parameter / token * 700000000 tokens [see dataset size] * 3*10^9 parameters = 1.26e+19 FLOP,8.0,,2025-05-06 11:50,,,,,,Industry,,,,,Unreleased,"llama 3.2 license
https://huggingface.co/scb10x/llama3.2-typhoon2-3b",Industry,,,,11009.566363976552,Operation counting,scb10x,,,
LLama 3..2 Typhoon 2 1B,Language,"Language modeling/generation,Question answering,Quantitative reasoning",Typhoon / SCB 10X,"Kunat Pipatanakul, Potsawee Manakul, Natapong Nitarach, Warit Sirichotedumrong, Surapon Nonesung, Teetouch Jaknamon, Parinthapat Pengpun, Pittawat Taveekitworachai, Adisai Na-Thalang, Sittipong Sripaisarnmongkol, Krisanapong Jirayoot, Kasima Tharnpipitchai",2024-12-19,Typhoon 2: A Family of Open Text and Multimodal Thai Large Language Models,https://arxiv.org/abs/2412.13702,,,,1000000000.0,1B,,,Unspecified unreleased,"CPT: supposedly none for this model

SFT: ""Smaller models and 70B model â€“ approximately 600-800M tokens.""",700000000.0,,,,NVIDIA H100 SXM5 80GB,,Likely,"This paper introduces Typhoon 2, a series of text and multimodal large language models optimized for the Thai language. The series includes models for text, vision, and audio. Typhoon2-Text builds on state-of-the-art open models, such as Llama 3 and Qwen2, and we perform continual pre-training on a mixture of English and Thai data. We employ post-training techniques to enhance Thai language performance while preserving the base models' original capabilities. We release text models across a range of sizes, from 1 to 70 billion parameters, available in both base and instruction-tuned variants. To guardrail text generation, we release Typhoon2-Safety, a classifier enhanced for Thai cultures and language. Typhoon2-Vision improves Thai document understanding while retaining general visual capabilities, such as image captioning. Typhoon2-Audio introduces an end-to-end speech-to-speech model architecture capable of processing audio, speech, and text inputs and generating both text and speech outputs.",,,Open weights (restricted use),Thailand,Llama 3.2 1B,4200000000000000000,6 FLOP / parameter / token * 700000000 tokens [see dataset size] * 10^9 parameters = 4.2e+18 FLOP,8.0,,2025-05-06 11:50,,,,,,Industry,,,,,Unreleased,"llama 3.2 license
https://huggingface.co/scb10x/llama3.2-typhoon2-1b",Industry,,,,11009.566363976552,Operation counting,scb10x,,,
Typhoon2-Vision ,"Vision,Language,Multimodal","Language modeling/generation,Visual question answering",Typhoon / SCB 10X,"Kunat Pipatanakul, Potsawee Manakul, Natapong Nitarach, Warit Sirichotedumrong, Surapon Nonesung, Teetouch Jaknamon, Parinthapat Pengpun, Pittawat Taveekitworachai, Adisai Na-Thalang, Sittipong Sripaisarnmongkol, Krisanapong Jirayoot, Kasima Tharnpipitchai",2024-12-19,Typhoon 2: A Family of Open Text and Multimodal Thai Large Language Models,https://arxiv.org/abs/2412.13702,,,,7000000000.0,7B (assuming same as base model),,, Cambrian-737K,,,,,,NVIDIA A100 SXM4 80 GB,,Confident,"This paper introduces Typhoon 2, a series of text and multimodal large language models optimized for the Thai language. The series includes models for text, vision, and audio. Typhoon2-Text builds on state-of-the-art open models, such as Llama 3 and Qwen2, and we perform continual pre-training on a mixture of English and Thai data. We employ post-training techniques to enhance Thai language performance while preserving the base models' original capabilities. We release text models across a range of sizes, from 1 to 70 billion parameters, available in both base and instruction-tuned variants. To guardrail text generation, we release Typhoon2-Safety, a classifier enhanced for Thai cultures and language. Typhoon2-Vision improves Thai document understanding while retaining general visual capabilities, such as image captioning. Typhoon2-Audio introduces an end-to-end speech-to-speech model architecture capable of processing audio, speech, and text inputs and generating both text and speech outputs.",,,Open weights (unrestricted),Thailand,Qwen2-VL-7B,,,4.0,,2025-05-06 11:50,,,,,,Industry,,,,,Unreleased,"Apache 2.0
https://huggingface.co/scb10x/typhoon2-qwen2vl-7b-vision-instruct",Industry,,,,3145.5903897075864,,scb10x,,,
Kling 1.6 Pro,"Video,Vision","Video generation,Text-to-video,Image-to-video",Kuaishou Technology,,2024-12-19,,https://ir.kuaishou.com/news-releases/news-release-details/kuaishou-kling-ai-unveils-multi-image-reference-feature-further/,,,,,,,,Unspecified unreleased,,,,,,,,Unknown,"We're excited to introduce the KLING AI 1.6 Model: With this update, we have significantly improved the response to prompt, the visual aesthetics, and the physical actions, hoping to bring more consistent and vivid results!
What's new:
1. New KLING AI 1.6 Model:
-Improved prompt adherence, more consistent and dynamic results
-Supports Standard & Professional Modes, achieving a 195% overall improvement compared with KLING 1.5 Model

In December 2024, the video generation model Kling AI 1.6 was officially launched, building upon the foundation model and elevating its capabilities significantly. Kling AI 1.6 introduced major advancements in text responsiveness, increasing its ability to interpret text descriptions related to motion, temporal action, and camera movement. Additionally, the upgraded model enhanced motion quality with smoother motion and more natural expressions. Kling AI 1.6 also delivered comprehensive upgrades in color accuracy, lighting dynamics, and detailed rendering, resulting in a striking improvement in overall video quality. ",,,API access,China,,,,,,2025-05-19 12:17,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
Granite 3.1 2B,Language,"Language modeling/generation,Question answering,Translation",IBM,,2024-12-18,,https://huggingface.co/ibm-granite/granite-3.1-2b-base,,,,2500000000.0,"2.5B
Model Architecture: Granite-3.1-2B-Base is based on a decoder-only dense transformer architecture. Core components of this architecture are: GQA and RoPE, MLP with SwiGLU, RMSNorm, and shared input/output embeddings.",1.8e+23,6 FLOP / token / parameter * 2.5 * 10^9 parameters * 12*10^12 tokens = 1.8e+23 FLOP,Unspecified unreleased,"Training Data: This model is trained on a mix of open source and proprietary data following a three-stage training strategy.

Stage 1 data: The data for stage 1 is sourced from diverse domains, such as: web, code, academic sources, books, and math data.
Stage 2 data: The data for stage 2 comprises a curated mix of high-quality data from the same domains, plus multilingual and instruction data. The goal of this second training phase is to enhance the modelâ€™s performance on specific tasks.
Stage 3 data: The data for stage 3 consists of original stage-2 pretraining data with additional synthetic long-context data in form of QA/summary pairs where the answer contains a recitation of the related paragraph before the answer.",12000000000000.0,12T,,,NVIDIA H100 SXM5 80GB,,Confident,Model Summary: Granite-3.1-2B-Base extends the context length of Granite-3.0-2B-Base from 4K to 128K using a progressive training strategy by increasing the supported context length in increments while adjusting RoPE theta until the model has successfully adapted to desired length of 128K. This long-context pre-training stage was performed using approximately 500B tokens.,,,Open weights (unrestricted),United States of America,,,,,,2025-05-16 10:30,IBM,"IBM's super computing cluster, Blue Vela, which is outfitted with NVIDIA H100 GPUs. This cluster provides a scalable and efficient infrastructure for training our models over thousands of GPUs.",,,,Industry,,,,,Unreleased,"https://huggingface.co/ibm-granite/granite-3.1-2b-base
Apache 2.0",Industry,,,,,Operation counting,ibm-granite,,,
Granite 3.1 8B,Language,"Language modeling/generation,Question answering,Translation",IBM,,2024-12-18,,https://huggingface.co/ibm-granite/granite-3.1-8b-base,,,,8100000000.0,"8.1B
Model Architecture: Granite-3.1-8B-Base is based on a decoder-only dense transformer architecture. Core components of this architecture are: GQA and RoPE, MLP with SwiGLU, RMSNorm, and shared input/output embeddings.",5.832e+23,6ND = 6 FLOP / parameter / token * 8.1*10^9 parameters * 12*10^12 tokens = 5.832e+23 FLOP,Unspecified unreleased,"**Training Data:** This model is trained on a mix of open source and proprietary data following a three-stage training strategy. * Stage 1 data: The data for stage 1 is sourced from diverse domains, such as: web, code, academic sources, books, and math data. * Stage 2 data: The data for stage 2 comprises a curated mix of high-quality data from the same domains, plus multilingual and instruction data. The goal of this second training phase is to enhance the modelâ€™s performance on specific tasks. * Stage 3 data: The data for stage 3 consists of original stage-2 pretraining data with additional synthetic long-context data in form of QA/summary pairs where the answer contains a recitation of the related paragraph before the answer.",12000000000000.0,12T,,,NVIDIA H100 SXM5 80GB,,Confident,Model Summary: Granite-3.1-8B-Base extends the context length of Granite-3.0-8B-Base from 4K to 128K using a progressive training strategy by increasing the supported context length in increments while adjusting RoPE theta until the model has successfully adapted to desired length of 128K. This long-context pre-training stage was performed using approximately 500B tokens.,,,Open weights (unrestricted),United States of America,,,,,,2025-05-16 10:30,IBM,"IBM's super computing cluster, Blue Vela, which is outfitted with NVIDIA H100 GPUs. This cluster provides a scalable and efficient infrastructure for training our models over thousands of GPUs.",,,,Industry,,,,,Unreleased,"https://huggingface.co/ibm-granite/granite-3.1-8b-base
Apache 2.0",Industry,,,,,Operation counting,ibm-granite,,,
Veo 2,"Video,Vision","Video generation,Text-to-video,Image-to-video",Google DeepMind,"Agrim Gupta, Ali Razavi, Ankush Gupta, Dumitru Erhan, Eric Lau, Frank Belletti, Gabe Barth-Maron, Hakan Erdogan, Hakim Sidahmed, Henna Nandwani, Hernan Moraldo, Hyunjik Kim, Jeff Donahue, JosÃ© Lezama, Kory Mathewson, Kurtis David, Marc van Zee, Medhini Narasimhan, Miaosen Wang, Mohammad Babaeizadeh, Nelly Papalampidi, Nick Pezzotti, Nilpa Jha, Parker Barnes, Pieter-Jan Kindermans, Rachel Hornung, Ruben Villegas, Ryan Poplin, Salah Zaiem, Sander Dieleman, Sayna Ebrahimi, Scott Wisdom, Serena Zhang, Shlomi Fruchter, Weizhe Hua, Xinchen Yan, Yuqing Du and Yutian Chen.",2024-12-16,Our state-of-the-art video generation model,https://deepmind.google/technologies/veo/veo-2/,,SOTA improvement,"""Veo has achieved state of the art results in head-to-head comparisons of outputs by human raters over top video generation models.

Participants viewed 1003 prompts and respective videos on MovieGenBench, a benchmark dataset released by Meta. Veo 2 performs best on overall preference, and for its capability to follow prompts accurately.""

SOTA qualification is unclear solely from MovieGenBench, which is subjective and depends on human raters. But Veo 2 seems to be SOTA over Meta Movie Gen, Kling, Minimax, and Sora Turbo.
Updated Sora could be better, but was released later this same month.",,,,,Unspecified unreleased,,,,,,,,Unknown,,,,API access,"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France",,,,,,2025-05-27 10:02,,,,,,Industry,,,,,Unreleased,https://cloud.google.com/vertex-ai/generative-ai/docs/models/veo/2-0-generate-001,Industry,,,,,,,,,
F5-TTS,Speech,"Speech synthesis,Translation","Shanghai Jiao Tong University,University of Cambridge,Geely Automobile Research Institute (Ningbo) Company","Yushen Chen, Zhikang Niu, Ziyang Ma, Keqi Deng, Chunhui Wang, Jian Zhao, Kai Yu, Xie Chen",2024-12-15,F5-TTS: A Fairytaler that Fakes Fluent and Faithful Speech with Flow Matching,https://arxiv.org/abs/2410.06885,,,,335800000.0,"The F5-TTS base model has 22 layers, 16 attention heads, 1024/2048 embedding/feed-forward network (FFN) dimension for DiT; and 4 layers, 512/1024 embedding/FFN dimension for ConvNeXt V2; in total 335.8M parameters.",4.5287424e+20,312000000000000*8*168*3600*0.3 = 4.5287424e+20,Emilia,"""We utilize the in-the-wild multilingual speech dataset Emilia to train our base models. After simply filtering out transcription failure and misclassified language speech, we retain approximately 95K hours of English and Chinese data. """,,"""Our base models are trained to 1.2M updates with a batch size of 307,200 audio frames (0.91 hours)""",168.0,"""over one week on 8 NVIDIA A100 80G GPUs""

7*24 = 168 h",NVIDIA A100 SXM4 80 GB,,Confident,"This paper introduces F5-TTS, a fully non-autoregressive text-to-speech system based on flow matching with Diffusion Transformer (DiT). Without requiring complex designs such as duration model, text encoder, and phoneme alignment, the text input is simply padded with filler tokens to the same length as input speech, and then the denoising is performed for speech generation, which was originally proved feasible by E2 TTS. However, the original design of E2 TTS makes it hard to follow due to its slow convergence and low robustness. To address these issues, we first model the input with ConvNeXt to refine the text representation, making it easy to align with the speech. We further propose an inference-time Sway Sampling strategy, which significantly improves our model's performance and efficiency. This sampling strategy for flow step can be easily applied to existing flow matching based models without retraining. Our design allows faster training and achieves an inference RTF of 0.15, which is greatly improved compared to state-of-the-art diffusion-based TTS models. Trained on a public 100K hours multilingual dataset, our Fairytaler Fakes Fluent and Faithful speech with Flow matching (F5-TTS) exhibits highly natural and expressive zero-shot ability, seamless code-switching capability, and speed control efficiency. Demo samples can be found at this https URL. We release all code and checkpoints to promote community development.",,,Open weights (non-commercial),"China,United Kingdom of Great Britain and Northern Ireland,China",,,,8.0,,2025-05-01 10:42,,,,307200.0,,"Academia,Academia,Industry",,,,,Open source,"CC-BY-NC-4.0
https://huggingface.co/SWivid/F5-TTS

MIT license for training code
https://github.com/SWivid/F5-TTS","Academia,Academia,Industry",,,,6291.741206937689,Hardware,SWivid,,,
Apollo 7B,"Video,Language,Multimodal",Video description,"Meta AI,Stanford University","Orr Zohar, Xiaohan Wang, Yann Dubois, Nikhil Mehta, Tong Xiao, Philippe Hansen-Estruch, Licheng Yu, Xiaofang Wang, Felix Juefei-Xu, Ning Zhang, Serena Yeung-Levy, Xide Xia",2024-12-13,Apollo: An Exploration of Video Understanding in Large Multimodal Models,https://arxiv.org/abs/2412.10360,,,,7000000000.0,"""We employed the Qwen2.5 (Yang et al., 2024) series of Large Language Models (LLMs) at varying scales to serve as the backbone for Apollo. Specifically, we utilized models with 1.5B, 3B, and 7B parameters. Following our analysis in Sec. 4, we used a SigLIP-SO400M (Zhai et al., 2023) encoder combined with an InternVideo2 (Wang et al., 2024d) video encoder""

there is a confusion with Qwen2.5 link - it points to Qwen 2 paper instead of Qwen 2.5 release notes",,,Unspecified unreleased,"""We curated a diverse mixture of publicly available and licensed datasets spanning text, image-text, multi-image, and video modalities. Due to licensing constraints, we omitted non-permissive sources (e.g., those reliant on ChatGPT), limiting the inclusion of some commonly used datasets. To further enhance our training
corpus, we generated multi-turn video-based conversations via an annotation tool powered by LLaMA 3.1 70B (Touvron et al., 2023).""",,"1. Alignment: In this phase, we trained on a 198K mixture of 50/50 image and video captions.
2. Vision Pretraining: We tuned the encoders using a video-only caption dataset of 396K samples.
3. Supervised Fine-tuning (SFT): We trained on a mixture of text, image, multi-image, and video data, with a total of 3.2 million samples.

3200000 videos *36%*~40 sec*~128 tokens / sec = 5898240000 video tokens during SFT

assuming about the same amount of image and text tokens. total size of the dataset was likely around 12B tokens
",,,NVIDIA A100,,Confident,"Despite the rapid integration of video perception capabilities into Large Multimodal Models (LMMs), the underlying mechanisms driving their video understanding remain poorly understood. Consequently, many design decisions in this domain are made without proper justification or analysis. The high computational cost of training and evaluating such models, coupled with limited open research, hinders the development of video-LMMs. To address this, we present a comprehensive study that helps uncover what effectively drives video understanding in LMMs.
We begin by critically examining the primary contributors to the high computational requirements associated with video-LMM research and discover Scaling Consistency, wherein design and training decisions made on smaller models and datasets (up to a critical size) effectively transfer to larger models. Leveraging these insights, we explored many video-specific aspects of video-LMMs, including video sampling, architectures, data composition, training schedules, and more. For example, we demonstrated that fps sampling during training is vastly preferable to uniform frame sampling and which vision encoders are the best for video representation.
Guided by these findings, we introduce Apollo, a state-of-the-art family of LMMs that achieve superior performance across different model sizes. Our models can perceive hour-long videos efficiently, with Apollo-3B outperforming most existing 7B models with an impressive 55.1 on LongVideoBench. Apollo-7B is state-of-the-art compared to 7B LMMs with a 70.9 on MLVU, and 63.3 on Video-MME.",,,,"United States of America,United States of America","Qwen2.5-7B,SigLIP 400M",,,128.0,,2025-05-01 10:42,,,,,,"Industry,Academia",,,,,,,"Industry,Academia",,,,100672.34303072088,,,,,
Phi-4,Language,"Language modeling/generation,Question answering,Code generation,Quantitative reasoning",Microsoft Research,"Marah Abdin, Jyoti Aneja, Harkirat Behl, SÃ©bastien Bubeck, Ronen Eldan, Suriya Gunasekar, Michael Harrison, Russell J. Hewett, Mojan Javaheripi, Piero Kauffmann, James R. Lee, Yin Tat Lee, Yuanzhi Li, Weishung Liu, Caio C. T. Mendes, Anh Nguyen, Eric Price, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Xin Wang, Rachel Ward, Yue Wu, Dingli Yu, Cyril Zhang, Yi Zhang",2024-12-12,Phi-4 Technical Report,https://arxiv.org/abs/2412.08905,,,,14000000000.0,"14B parameters, dense decoder-only Transformer model",9.3202015e+23,"6ND = 6* 14*10^9 parameters * 10*10^12 tokens = 8.4e+23 FLOP

989500000000000 FLOP / sec [assumed bf16 precision] * 1920 GPUs * 504 hours * 3600 sec / hour * 0.3 [assumed utilization] = 1.0341209e+24 FLOP

geometric mean
sqrt(8.4e+23 * 1.0341209e+24) = 9.3202015e+23",Unspecified unreleased,"""The pretraining phase of phi-4 relies heavily on synthetic datasets generated through a variety of techniques. ""
""We collected a wide variety of high-quality organic data sources
for phi-4, prioritizing reasoning-dense and nuanced material (e.g., academic papers, educational forums,
and programming tutorials).""
""Our post-training data is composed of:
â€¢ Supervised Fine-Tuning (SFT) Datasets
â€¢ Direct Preference Optimization (DPO)",10000000000000.0,"""The model was pretrained for approximately 10T tokens using linear warm-up and decay schedules with peak learning rate of 0.0003, constant weight decay of 0.1, and global batch size of 5760. ""

Table 5:
Web 15% 1.3T unique tokens 1.2 epochs
Web rewrites 15% 290B unique tokens 5.2 epochs
Synthetic 40% 290B unique tokens 13.8 epochs
Code data 20% 820B unique tokens 2.4 epochs
Acquired sources 10% 580B unique tokens  1.7 epochs
",504.0,"https://huggingface.co/microsoft/phi-4
21 days * 24 hours / day = 504 hours",NVIDIA H100 SXM5 80GB,,Confident,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",,,Open weights (unrestricted),"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational",,,,1920.0,,2025-05-16 10:30,,,,,,Industry,,,,,Unreleased,"""Phi-4 is currently available on Azure AI Foundry under a Microsoft Research License Agreement (MSRLA) and will be available on Hugging Face next week.  ""
Hugging Face: MIT license

https://huggingface.co/microsoft/phi-4",Industry,,,,2642707.855343539,"Operation counting,Hardware",microsoft,,,
Gemini 2.0 Flash,"Language,Vision,Audio,Speech,Video,Multimodal","Language modeling/generation,Question answering,Visual question answering,Speech recognition,Code generation,Quantitative reasoning,Video description,Translation,Chat,Table tasks,Search,Text summarization","Google DeepMind,Google",Gemini Team,2024-12-11,Introducing Gemini 2.0: our new AI model for the agentic era,https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,,,,,,,"""We used Trillium TPUs to train the new Gemini 2.0, Googleâ€™s most capable AI model yet"" according to https://cloud.google.com/blog/products/compute/trillium-tpu-is-ga",Unspecified unreleased,"Knowledge cutoff June 2024, according to https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-0-flash. ",,,,,Google TPU v6e Trillium,,Unknown,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.",,,API access,"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France,United States of America",,,,,,2025-06-10 12:49,,,,,,"Industry,Industry",,,,,Unreleased,"Availability	
Google AI Studio
Gemini API
Vertex AI
Gemini App","Industry,Industry",,,,,,,,,
Gemini 2.0 Pro,"Language,Multimodal,Vision,Video,Audio","Code generation,Language modeling/generation,Question answering,Visual question answering,Speech recognition,Video description",Google DeepMind,Gemini Team,2024-12-11,Our best model yet for coding performance and complex prompts,https://deepmind.google/technologies/gemini/pro/,,Training cost,,,,,,Unspecified unreleased,"May have been renamed to Gemini 2.5 Pro, which has a knowledge cutoff date of January 2025, according to https://ai.google.dev/gemini-api/docs/models#gemini-2.5-pro-preview-05-06. If that's not the case, then Gemini 2.0 Pro may have a knowledge cutoff date of August 2024, according to https://docsbot.ai/models/gemini-2-0-pro, which seems in line with other Gemini 2.0 models, which have a knowledge cutoff date of June 2024.",,,,,,,Unknown,"Today, weâ€™re releasing an experimental version of Gemini 2.0 Pro that responds to that feedback. It has the strongest coding performance and ability to handle complex prompts, with better understanding and reasoning of world knowledge, than any model weâ€™ve released so far. It comes with our largest context window at 2 million tokens, which enables it to comprehensively analyze and understand vast amounts of information, as well as the ability to call tools like Google Search and code execution.",,,Hosted access (no API),"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France",,,,,,2025-06-18 10:56,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
EXAONE 3.5 2.4B,Language,"Language modeling/generation,Question answering,Translation",LG AI Research,"Soyoung An, Kyunghoon Bae, Eunbi Choi, Kibong Choi, Stanley Jungkyu Choi, Seokhee Hong, Junwon Hwang, Hyojin Jeon, Gerrard Jeongwon Jo, Hyunjik Jo, Jiyeon Jung, Yountae Jung, Hyosang Kim, Joonkee Kim, Seonghwan Kim, Soyeon Kim, Sunkyoung Kim, Yireun Kim, Yongil Kim, Youchul Kim, Edward Hwayoung Lee, Haeju Lee, Honglak Lee, Jinsik Lee, Kyungmin Lee, Woohyung Lim, Sangha Park, Sooyoun Park, Yongmin Park, Sihoon Yang, Heuiyeen Yeen, Hyeongu Yun",2024-12-09,EXAONE 3.5: Series of Large Language Models for Real-world Use Cases,https://arxiv.org/abs/2412.04862,,,,2400000000.0, 2.4B,9.36e+22,9.36 Ã— 10^22 (Table 2),Unspecified unreleased,,6500000000000.0,6.5T tokens (Table 2),,,,,Confident,"This technical report introduces the EXAONE 3.5 instruction-tuned language models, developed and released by LG AI Research. The EXAONE 3.5 language models are offered in three configurations: 32B, 7.8B, and 2.4B. These models feature several standout capabilities: 1) exceptional instruction following capabilities in real-world scenarios, achieving the highest scores across seven benchmarks, 2) outstanding long-context comprehension, attaining the top performance in four benchmarks, and 3) competitive results compared to state-of-the-art open models of similar sizes across nine general benchmarks. The EXAONE 3.5 language models are open to anyone for research purposes and can be downloaded from this https URL. For commercial use, please reach out to the official contact point of LG AI Research: contact_us@lgresearch.ai.

Currently, the 2.4B model achieves the highest average performance in the edge device category on the Hugging Face Open LLM Leaderboard.
",,,Open weights (non-commercial),Korea (Republic of),,,,,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,Exaone license (allows only non-commercial usage),Industry,,,,,Reported,LGAI-EXAONE,,,
EXAONE 3.5 32B,Language,"Language modeling/generation,Question answering,Translation",LG AI Research,"Soyoung An, Kyunghoon Bae, Eunbi Choi, Kibong Choi, Stanley Jungkyu Choi, Seokhee Hong, Junwon Hwang, Hyojin Jeon, Gerrard Jeongwon Jo, Hyunjik Jo, Jiyeon Jung, Yountae Jung, Hyosang Kim, Joonkee Kim, Seonghwan Kim, Soyeon Kim, Sunkyoung Kim, Yireun Kim, Yongil Kim, Youchul Kim, Edward Hwayoung Lee, Haeju Lee, Honglak Lee, Jinsik Lee, Kyungmin Lee, Woohyung Lim, Sangha Park, Sooyoun Park, Yongmin Park, Sihoon Yang, Heuiyeen Yeen, Hyeongu Yun",2024-12-09,EXAONE 3.5: Series of Large Language Models for Real-world Use Cases,https://arxiv.org/abs/2412.04862,,Training cost,,32000000000.0,32B,1.25e+24,1.25 Ã— 10^24 (Table 2) ,Unspecified unreleased,,6500000000000.0,6.5T tokens (Table 2),,,,,Confident,"This technical report introduces the EXAONE 3.5 instruction-tuned language models, developed and released by LG AI Research. The EXAONE 3.5 language models are offered in three configurations: 32B, 7.8B, and 2.4B. These models feature several standout capabilities: 1) exceptional instruction following capabilities in real-world scenarios, achieving the highest scores across seven benchmarks, 2) outstanding long-context comprehension, attaining the top performance in four benchmarks, and 3) competitive results compared to state-of-the-art open models of similar sizes across nine general benchmarks. The EXAONE 3.5 language models are open to anyone for research purposes and can be downloaded from this https URL. For commercial use, please reach out to the official contact point of LG AI Research: contact_us@lgresearch.ai.",,,Open weights (non-commercial),Korea (Republic of),,,,,,2025-05-16 10:30,,,,,,Industry,,,,,Unreleased,Exaone license (allows only non-commercial usage),Industry,,,,,Reported,LGAI-EXAONE,,,
Llama 3.3 70B,Language,"Language modeling/generation,Question answering,Translation,Code generation",Meta AI,,2024-12-06,Meta Llama 3.3 multilingual large language model (LLM) ,https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_3/,,Training cost,,70000000000.0,70B,6.8649768e+24,"6ND = 6 FLOP / parameter / token * 70*10^9 parameters * 15*10^12 tokens = 6.3e+24 FLOP

7000000 GPU-hours * 3600 sec / hour * 989500000000000 FLOP / second * 0.3 [assumed utilization]= 7.48062e+24 FLOP

sqrt(7.48062e+24*6.3e+24) = 6.8649768e+24",Unspecified unreleased,"""A new mix of publicly available online data.""
Knowledge cutoff date is December 2023, according to https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct",15000000000000.0,"""Overview: Llama 3.3 was pretrained on ~15 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over 25M synthetically generated examples.

Data Freshness: The pretraining data has a cutoff of December 2023.""",,"""Training utilized a cumulative of 39.3M GPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency.""

Llama 3.3 70B: Training Time (GPU hours): 7M
",NVIDIA H100 SXM5 80GB,,Confident,"The Meta Llama 3.3 multilingual large language model (LLM) is a pretrained and instruction tuned generative model in 70B (text in/text out). The Llama 3.3 instruction tuned text only model is optimized for multilingual dialogue use cases and outperforms many of the available open source and closed chat models on common industry benchmarks.

Model developer: Meta

Model Architecture: Llama 3.3 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.",,,Open weights (restricted use),United States of America,,,,,,2025-05-16 10:30,,,,,,Industry,,,7.48062e+24,7000000.0,Unreleased,"License A custom commercial license, the Llama 3.3 Community License Agreement, is available at: https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/LICENSE

""Llama 3.3 is intended for commercial and research use in multiple languages.""",Industry,,,,,"Operation counting,Hardware",,,,
o1,"Language,Mathematics,Multimodal","Code generation,Language modeling/generation,Quantitative reasoning,Chat,Question answering,Translation",OpenAI,,2024-12-05,Introducing ChatGPT Pro: Broadening usage of frontier AI.,https://openai.com/index/introducing-chatgpt-pro/,,"SOTA improvement,Significant use",SOTA in GPQA among others: https://openai.com/index/learning-to-reason-with-llms/ ,,,,,Unspecified unreleased,"""Both models were trained on a variety of publicly available datasets, including web data and open-source datasets. Key components include reasoning data and scientific literature. <..> To further enhance the capabilities of o1-preview and o1-mini, we formed partnerships to access high-value non-public datasets. These proprietary data sources include paywalled content, specialized archives, and other domain-specific datasets that provide deeper insights into industry-specific knowledge and use cases.""

This has a knowledge cutoff date of October 2023 - which I assume means October 1, 2023 - according to https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models?tabs=global-standard%2Cstandard-chat-completions#o-series-models.",,,,,,,Unknown,"We've developed a new series of AI models designed to spend more time thinking before they respond. They can reason through complex tasks and solve harder problems than previous models in science, coding, and math.

Today, we are releasing the first of this series in ChatGPT and our API. This is a preview and we expect regular updates and improvements. Alongside this release, weâ€™re also including evaluations for the next update, currently in development.",,,API access,United States of America,,,,,,2025-06-12 15:52,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
NVILA 15B,"Vision,Language,Multimodal,Video","Visual question answering,Video description","NVIDIA,Massachusetts Institute of Technology (MIT),University of California (UC) Berkeley,University of California San Diego,University of Washington,Tsinghua University","Zhijian Liu, Ligeng Zhu, Baifeng Shi, Zhuoyang Zhang, Yuming Lou, Shang Yang, Haocheng Xi, Shiyi Cao, Yuxian Gu, Dacheng Li, Xiuyu Li, Yunhao Fang, Yukang Chen, Cheng-Yu Hsieh, De-An Huang, An-Chieh Cheng, Vishwesh Nath, Jinyi Hu, Sifei Liu, Ranjay Krishna, Daguang Xu, Xiaolong Wang, Pavlo Molchanov, Jan Kautz, Hongxu Yin, Song Han, Yao Lu",2024-12-05,NVILA: Efficient Frontier Visual Language Models,https://arxiv.org/abs/2412.04468,,,,15000000000.0,15B,,,"Docmatrix,PDFA,COYO-700M,ShareGPT4V,MMC4 / Multimodal C4",,,,,"""We train all models using 128 NVIDIA H100 GPUs with a global batch size of 2048 across all stages.""",NVIDIA H100 SXM5 80GB,,Confident,"Visual language models (VLMs) have made significant advances in accuracy in recent years. However, their efficiency has received much less attention. This paper introduces NVILA, a family of open VLMs designed to optimize both efficiency and accuracy. Building on top of VILA, we improve its model architecture by first scaling up the spatial and temporal resolutions, and then compressing visual tokens. This ""scale-then-compress"" approach enables NVILA to efficiently process high-resolution images and long videos. We also conduct a systematic investigation to enhance the efficiency of NVILA throughout its entire lifecycle, from training and fine-tuning to deployment. NVILA matches or surpasses the accuracy of many leading open and proprietary VLMs across a wide range of image and video benchmarks. At the same time, it reduces training costs by 4.5X, fine-tuning memory usage by 3.4X, pre-filling latency by 1.6-2.2X, and decoding latency by 1.2-2.8X. We will soon make our code and models available to facilitate reproducibility.",,,Open weights (non-commercial),"United States of America,United States of America,United States of America,United States of America,United States of America,China",,,,128.0,,2025-05-01 10:42,,,,,,"Industry,Academia,Academia,Academia,Academia,Academia",,,,,,"Creative Commons Attribution Non Commercial 4.0

https://huggingface.co/Efficient-Large-Model/NVILA-15B","Industry,Academia,Academia,Academia,Academia,Academia",,,FP8,176207.9898367568,,Efficient-Large-Model,,,
NVILA 8B,"Vision,Language,Multimodal,Video","Visual question answering,Video description","NVIDIA,Massachusetts Institute of Technology (MIT),University of California (UC) Berkeley,University of California San Diego,University of Washington,Tsinghua University","Zhijian Liu, Ligeng Zhu, Baifeng Shi, Zhuoyang Zhang, Yuming Lou, Shang Yang, Haocheng Xi, Shiyi Cao, Yuxian Gu, Dacheng Li, Xiuyu Li, Yunhao Fang, Yukang Chen, Cheng-Yu Hsieh, De-An Huang, An-Chieh Cheng, Vishwesh Nath, Jinyi Hu, Sifei Liu, Ranjay Krishna, Daguang Xu, Xiaolong Wang, Pavlo Molchanov, Jan Kautz, Hongxu Yin, Song Han, Yao Lu",2024-12-05,NVILA: Efficient Frontier Visual Language Models,https://arxiv.org/abs/2412.04468,,,,8000000000.0,8B,2.2794518e+21,"""We train all models using 128 NVIDIA H100 GPUs with a global batch size of 2048 across all stages.""

from Figure 1
NVILA 8B takes 4.5x times less gpu hours than LLAVA OneVision which is reported to take 400 GPU days (""For example, training a state-of-the-art 7B VLM [5] can take up to 400 GPU days"")

400/4.5 ~ 89 GPU days ~ 2133 GPU hours

989500000000000 FLOP / sec / GPU * 2133 GPU*hours * 3600 sec / hour * 0.3 [assumed utilization] = 2.2794518e+21 FLOP

Confidence: Likely (precision is FP8 not FP16 and utilization could be different from 0.3)","Docmatrix,PDFA,COYO-700M,ShareGPT4V,MMC4 / Multimodal C4",,47488579166.0,2.2794518e+21 FLOP / (6*8*10^9) = 47488579166.7 tokens ~47B,16.7,"""We train all models using 128 NVIDIA H100 GPUs with a global batch size of 2048 across all stages.""

from Figure 1
NVILA 8B takes 4.5x times less gpu hours than LLAVA OneVision which is reported to take 400 GPU days (""For example, training a state-of-the-art 7B VLM [5] can take up to 400 GPU days"")

400/4.5 ~ 89 GPU days ~ 2133 GPU hours ~ 16.7 clock hours",NVIDIA H100 SXM5 80GB,,Likely,"Visual language models (VLMs) have made significant advances in accuracy in recent years. However, their efficiency has received much less attention. This paper introduces NVILA, a family of open VLMs designed to optimize both efficiency and accuracy. Building on top of VILA, we improve its model architecture by first scaling up the spatial and temporal resolutions, and then compressing visual tokens. This ""scale-then-compress"" approach enables NVILA to efficiently process high-resolution images and long videos. We also conduct a systematic investigation to enhance the efficiency of NVILA throughout its entire lifecycle, from training and fine-tuning to deployment. NVILA matches or surpasses the accuracy of many leading open and proprietary VLMs across a wide range of image and video benchmarks. At the same time, it reduces training costs by 4.5X, fine-tuning memory usage by 3.4X, pre-filling latency by 1.6-2.2X, and decoding latency by 1.2-2.8X. We will soon make our code and models available to facilitate reproducibility.",,,Open weights (non-commercial),"United States of America,United States of America,United States of America,United States of America,United States of America,China",,,,128.0,,2025-06-16 23:33,,,,,,"Industry,Academia,Academia,Academia,Academia,Academia",,,,2133.0,,"Creative Commons Attribution Non Commercial 4.0

https://huggingface.co/Efficient-Large-Model/NVILA-15B","Industry,Academia,Academia,Academia,Academia,Academia",,,FP8,176207.9898367568,Hardware,Efficient-Large-Model,,,
Infinity,Image generation,"Image generation,Text-to-image",ByteDance,"Jian Han, Jinlai Liu, Yi Jiang, Bin Yan, Yuqi Zhang, Zehuan Yuan, Bingyue Peng, Xiaobing Liu",2024-12-05,Infinityâˆž: Scaling Bitwise AutoRegressive Modeling for High-Resolution Image Synthesis,https://arxiv.org/abs/2412.04431v1,25.0,SOTA improvement,"Infinity achieved best performance at severrat text-to-image benchmarks, including GenEval and ImageReward. ",2000000000.0,,,,"LAION,COYO-700M","The pre-training dataset is constructed by collecting and cleaning open-source academic datasets such as LAION, COYO, OpenImages.",,,,,,,Confident,"We present Infinity, a Bitwise Visual AutoRegressive Modeling capable of generating high-resolution, photorealistic images following language instruction. Infinity redefines visual autoregressive model under a bitwise token prediction framework with an infinite-vocabulary tokenizer & classifier and bitwise self-correction mechanism, remarkably improving the generation capacity and details. By theoretically scaling the tokenizer vocabulary size to infinity and concurrently scaling the transformer size, our method significantly unleashes powerful scaling capabilities compared to vanilla VAR. Infinity sets a new record for autoregressive text-to-image models, outperforming top-tier diffusion models like SD3-Medium and SDXL. Notably, Infinity surpasses SD3-Medium by improving the GenEval benchmark score from 0.62 to 0.73 and the ImageReward benchmark score from 0.87 to 0.96, achieving a win rate of 66%. Without extra optimization, Infinity generates a high-quality 1024x1024 image in 0.8 seconds, making it 2.6x faster than SD3-Medium and establishing it as the fastest text-to-image model. Models and codes will be released to promote further exploration of Infinity for visual generation and unified tokenizer modeling.",,,Open weights (unrestricted),China,,,,,,2025-05-28 17:56,,,,,,Industry,,,,,Open source,"MIT license

https://huggingface.co/FoundationVision/Infinity
https://github.com/FoundationVision/Infinity",Industry,,,FP16,,,FoundationVision,,,
Pleias 1.0 1.2B,Language,"Language modeling/generation,Question answering,Translation",PleIAs,,2024-12-05,Pleias-nano-1.2b-Preview ,https://huggingface.co/PleIAs/Pleias-1.2b-Preview,,,,1200000000.0,1.2B,2.9770787e+22,"6 FLOP / parameter / token * 1.2 * 10^9 parameters * 5 * 10^12 tokens = 3.6e+22 FLOP

989400000000000 FLOP / GPU / sec [bf16 assumed] * 192 GPUs * 5 days * 24 hour / day * 3600 sec / hour * 0.3 [assumed utilization] = 2.4619438e+22 FLOP

sqrt(3.6e+22*2.4619438e+22) = 2.9770787e+22 ",Common Corpus,"""A lightly filtered version of Common Corpus (1.6 trillion tokens):
A filtered and enhanced version of Common Corpus (1,086,324,736,000 tokens).
A repeat of the previous set.""",1600000000000.0,"""Training schedule includes 518,000 steps (batch size 1,024) on over three epochs (nearly 5 trillions tokens):""

",120.0,"""Pleias-nano-1.2b-Preview was fully pretrained on TractoAI on ISEG GPU cluster by Nebius AI on 192 h100s for 5 days""",NVIDIA H100 SXM5 80GB,,Confident,"Description
Pleias-nano-1.2b-Preview is a transformer base model, entirely pretrained from scratch, using an architecture similar to Llama/GPT-Neox for easier deployment/inference.

It includes the following features, that would apply to any responsibly trained variant:

Only trained on open data under a permissible license and in compliance with the European AI Act. By design, all Pleias model are unable to output copyrighted content.
Extensive multilingual support for main European languages.
A new tokenizer designed for enhanced document processing tasks and better multilingual support.
Extremely low level of toxicity and problematic content.
Pleias-nano-1.2b-Preview has demonstrated unusual abilities for multilingual generation in its size range. Fully supported languages include English, French, Spanish, German, Italian, Dutch, Latin and Portuguese.

Given its size, Pleias-nano-1.2b-Preview can run on CPU without any compression loss. We provide a first GGUF variant as part of our release.",3.0,,Open weights (unrestricted),France,,,,192.0,,2025-05-12 13:40,Nebius AI,,,,,Industry,,,,,Unreleased,"Apache 2.0

https://huggingface.co/PleIAs/Pleias-350m-Preview",Industry,,,,264311.9847551352,"Hardware,Operation counting",PleIAs,,,
Genie 2,"Games,Video,3D modeling","Video generation,Text-to-video",Google DeepMind,"Jack Parker-Holder, Philip Ball, Jake Bruce, Vibhavari Dasagi, Kristian Holsheimer, Christos Kaplanis, Alexandre Moufarek, Guy Scully, Jeremy Shar, Jimmy Shi, Stephen Spencer, Jessica Yung, Michael Dennis, Sultan Kenjeyev, Shangbang Long, Vlad Mnih, Harris Chan, Maxime Gazeau, Bonnie Li, Fabio Pardo, Luyu Wang, Lei Zhang, Frederic Besse, Tim Harley, Anna Mitenkova, Jane Wang, Jeff Clune, Demis Hassabis, Raia Hadsell, Adrian Bolton, Satinder Singh, Tim RocktÃ¤schel",2024-12-04,Genie 2: A large-scale foundation world model,https://deepmind.google/discover/blog/genie-2-a-large-scale-foundation-world-model/,,,,,,,,,,,,,,,,Unknown,"Today we introduce Genie 2, a foundation world model capable of generating an endless variety of action-controllable, playable 3D environments for training and evaluating embodied agents. Based on a single prompt image, it can be played by a human or AI agent using keyboard and mouse inputs.

Games play a key role in the world of artificial intelligence (AI) research. Their engaging nature, unique blend of challenges, and measurable progress make them ideal environments to safely test and advance AI capabilities.

Indeed, games have been important to Google DeepMind since our founding. From our early work with Atari games, breakthroughs such as AlphaGo and AlphaStar, to our research on generalist agents in collaboration with game developers, games have been center stage in our research. However, training more general embodied agents has been traditionally bottlenecked by the availability of sufficiently rich and diverse training environments.

As we show, Genie 2 could enable future agents to be trained and evaluated in a limitless curriculum of novel worlds. Our research also paves the way for new, creative workflows for prototyping interactive experiences.",,,Unreleased,"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France",,,,,,2025-05-19 12:19,,,,,,Industry,,,,,,,Industry,,,,,,,,,
Amazon Nova Pro,"Multimodal,Language,Video,Vision","Language modeling/generation,Retrieval-augmented generation,Video generation,Visual question answering",Amazon,,2024-12-03,Introducing Amazon Nova foundation models: Frontier intelligence and industry leading price performance,https://aws.amazon.com/es/blogs/aws/introducing-amazon-nova-frontier-intelligence-and-industry-leading-price-performance/,,SOTA improvement,"""It achieves state-of-the-art performance on key benchmarks including visual question answering (TextVQA) and video understanding (VATEX).""",,,6.000010000000001e+24,"""probably just below 1e25 stemming from the Llama 70B serving speed.  If Llama 70B is trained proportionally to 405B, then it's at ~ 6.6e24. Nova Pro is served at 100tk/s, while Llama 70B is served at 70tk/s on average, and 100tk/s by together.ai at FP8. So Nova Pro would be >1e25 if they roughly 2x the amount of training compared to Llama 70B which [seems unlikely]""",,,,,,,,,Speculative," A highly capable multimodal model with the best combination of accuracy, speed, and cost for a wide range of tasks. Amazon Nova Pro is capable of processing up to 300K input tokens and sets new standards in multimodal intelligence and agentic workflows that require calling APIs and tools to complete complex workflows. It achieves state-of-the-art performance on key benchmarks including visual question answering (TextVQA) and video understanding (VATEX). Amazon Nova Pro demonstrates strong capabilities in processing both visual and textual information and excels at analyzing financial documents. With an input context of 300K tokens, it can process code bases with over fifteen thousand lines of code. Amazon Nova Pro also serves as a teacher model to distill custom variants of Amazon Nova Micro and Lite.",,,API access,United States of America,,,,,,2025-06-09 15:38,,,,,,Industry,,,1.9e+26,,Unreleased,,Industry,,,,,Comparison with other models,,,,
Luma Photon,Image generation,"Image generation,Text-to-image",LumaLabs,,2024-12-03,,https://www.luma-ai.com/luma-photon/,,,,,,,,Unspecified unreleased,,,,,,,,Unknown,"Introducing the all-new Luma Photon text-to-image models, now available in the Luma API. Photon and Photon Flash are the most creative, personalizable, and intelligent visual models for creatives, bringing a step-function change in the cost of high-quality image generation.

The fastest and most efficient models yet. Powered by Lumaâ€™s groundbreaking research, Photon costs just $0.015 per 1080p image, and just $0.002 with Photon Flash. Achieve more for less with unparalleled speed and affordability.

",,,API access,United States of America,,,,,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
Hunyuan Video,Video,"Video generation,Text-to-video",Tencent,"Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, Kathrina Wu, Qin Lin, Junkun Yuan, Yanxin Long, Aladdin Wang, Andong Wang, Changlin Li, Duojun Huang, Fang Yang, Hao Tan, Hongmei Wang, Jacob Song, Jiawang Bai, Jianbing Wu, Jinbao Xue, Joey Wang, Kai Wang, Mengyang Liu, Pengyu Li, Shuai Li, Weiyan Wang, Wenqing Yu, Xinchi Deng, Yang Li, Yi Chen, Yutao Cui, Yuanbo Peng, Zhentao Yu, Zhiyu He, Zhiyong Xu, Zixiang Zhou, Zunnan Xu, Yangyu Tao, Qinglin Lu, Songtao Liu, Daquan Zhou, Hongfa Wang, Yong Yang, Di Wang, Yuhong Liu, Jie Jiang, Caesar Zhong",2024-12-03,HunyuanVideo: A Systematic Framework For Large Video Generative Models,https://www.arxiv.org/abs/2412.03603,,,,13000000000.0,13b,1.4814815e+23,"from Figure 10:

the optimal model has 13b parameters, 5.8e+07PF (image training) + 7.0e+07PF (video training) of compute and 740B (image tokens) + 928B (video tokens) 

5.8e+07PF + 7.0e+07PF = 12.8e+07PF = 12.8*10^7*10^20/(24*3600) = 1.4814815e+23 FLOPs

6ND = 6*13*10^9*(740+928)*10^9 = 1.30104e+23
",Unspecified unreleased,"""We employ various filters for data filtering and progressively increase their thresholds to build 4 training datasets, i.e., 256p, 360p, 540p, and
720p, while the final SFT dataset is built through manual annotation.""",,,,,,,Confident,"Recent advancements in video generation have significantly impacted daily life for both individuals and industries. However, the leading video generation models remain closed-source, resulting in a notable performance gap between industry capabilities and those available to the public. In this report, we introduce HunyuanVideo, an innovative open-source video foundation model that demonstrates performance in video generation comparable to, or even surpassing, that of leading closed-source models. HunyuanVideo encompasses a comprehensive framework that integrates several key elements, including data curation, advanced architectural design, progressive model scaling and training, and an efficient infrastructure tailored for large-scale model training and inference. As a result, we successfully trained a video generative model with over 13 billion parameters, making it the largest among all open-source models. We conducted extensive experiments and implemented a series of targeted designs to ensure high visual quality, motion dynamics, text-video alignment, and advanced filming techniques. According to evaluations by professionals, HunyuanVideo outperforms previous state-of-the-art models, including Runway Gen-3, Luma 1.6, and three top-performing Chinese video generative models. By releasing the code for the foundation model and its applications, we aim to bridge the gap between closed-source and open-source communities. This initiative will empower individuals within the community to experiment with their ideas, fostering a more dynamic and vibrant video generation ecosystem. The code is publicly available at this https URL. https://github.com/Tencent/HunyuanVideo.",,,Open weights (restricted use),China,,,,,,2025-05-19 12:20,,,,,,Industry,,,,,Unreleased,"""THIS LICENSE AGREEMENT DOES NOT APPLY IN THE EUROPEAN UNION, UNITED KINGDOM AND SOUTH KOREA"" 

also requires additional licensing in case of massive commercial use

https://huggingface.co/tencent/HunyuanVideo/blob/main/LICENSE

the code seems to be just inference code not training code
",Industry,,,,,"Reported,Operation counting",,,,
Intelligent Go-Explore (IGE),Games,Game of 24 (24 puzzle),"University of British Columbia (UBC),Vector Institute,CIFAR AI Research","Cong Lu, Shengran Hu, Jeff Clune",2024-12-03,Intelligent Go-Explore: Standing on the Shoulders of Giant Foundation Models,https://arxiv.org/abs/2405.15143,,,,,"We used GPT-4-Turbo [API] for Game of 24 and GPT-4o for BabyAI and TextWorld. This was purely done to select the version of GPT-4 that was available and the cheapest at the time of running the
experiments. The version of GPT-4 is consistent per environment. ",,,,,,,,,,,Unknown,"Go-Explore is a powerful family of algorithms designed to solve hard-exploration problems built on the principle of archiving discovered states, and iteratively returning to and exploring from the most promising states. This approach has led to superhuman performance across a wide variety of challenging problems including Atari games and robotic control, but requires manually designing heuristics to guide exploration (i.e., determine which states to save and explore from, and what actions to consider next), which is time-consuming and infeasible in general. To resolve this, we propose Intelligent Go-Explore (IGE) which greatly extends the scope of the original Go-Explore by replacing these handcrafted heuristics with the intelligence and internalized human notions of interestingness captured by giant pretrained foundation models (FMs). This provides IGE with a human-like ability to instinctively identify how interesting or promising any new state is (e.g., discovering new objects, locations, or behaviors), even in complex environments where heuristics are hard to define. Moreover, IGE offers the exciting opportunity to recognize and capitalize on serendipitous discoveries-states encountered during exploration that are valuable in terms of exploration, yet where what makes them interesting was not anticipated by the human user. We evaluate our algorithm on a diverse range of language and vision-based tasks that require search and exploration. Across these tasks, IGE strongly exceeds classic reinforcement learning and graph search baselines, and also succeeds where prior state-of-the-art FM agents like Reflexion completely fail. Overall, Intelligent Go-Explore combines the tremendous strengths of FMs and the powerful Go-Explore algorithm, opening up a new frontier of research into creating more generally capable agents with impressive exploration capabilities.",,,Open weights (unrestricted),"Canada,Canada,Canada",GPT-4 Turbo,,,,,2025-05-01 10:42,,,,,,"Academia,Academia,Research collective",,,,,Unreleased,"MIT license https://github.com/conglu1997/intelligent-go-explore

seems to be just inference code, not training code","Academia,Academia,Research collective",,,,,,,,,
Hailuo I2V-01-Live,"Video,Vision","Video generation,Image-to-video","MiniMax,Hailuo AI",,2024-12-03,Introducing Hailuo I2V-01-Live: Transform Static Art into Dynamic Masterpieces,https://hailuoai.video/discover/329354592735850496,,,,,,,,Unspecified unreleased,,,,,,,,Unknown,"Introducing Hailuo I2V-01-Live: Transform Static Art into Dynamic Masterpieces

Live is our latest addition to the I2V family, designed to revolutionize how 2D illustrations come to life. With enhanced smoothness and vivid motion, this model lets your characters move, speak, and shine like never before.

Optimized for stability and subtle expression, Hailuo I2V-01-Live supports a wide range of artistic styles, empowering you to expand your creative expression and bring your art to life with unparalleled fluidity and finesse.",,,API access,"China,Singapore",,,,,,2025-05-19 12:20,,,,,,"Industry,Industry",,,,,Unreleased,https://intl.minimaxi.com/document/video_generation?key=66d1439376e52fcee2853049,"Industry,Industry",,,,,,,,,
Amazon Nova Canvas,"Image generation,Vision","Image generation,Text-to-image,Image captioning",Amazon,,2024-12-03,,"https://docs.aws.amazon.com/ai/responsible-ai/nova-canvas/overview.html#:~:text=plan%20to%20use.-,This%20Service%20Card%20applies%20to%20the%20release%20of%20Amazon%20Nova,as%20of%20December%203%2C%202024.",,,,,,,,,,,,,,,,Unknown,"an image generation model that creates professional grade images from text and image inputs. Amazon Nova Canvas is ideal for a wide range of applications such as advertising, marketing, and entertainment.",,,API access,United States of America,,,,,,2025-06-09 15:46,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
RNAformer,Biology,RNA structure prediction,University of Freiburg,"JÃ¶rg K.H. Franke, Frederic Runge, Ryan KÃ¶ksal,  Rolf Backofen, Frank Hutter",2024-12-01,RNAformer: A Simple Yet Effective Deep Learning Model for RNA Secondary Structure Prediction,https://www.biorxiv.org/content/10.1101/2024.02.12.579881v1.abstract,6.0,,,32000000.0,,,,,,410408.0,"'- Biophysical Model: 410,408 (table b1)
",,,NVIDIA A100,,Confident,"Traditional RNA secondary structure prediction methods, based on dynamic programming, often fall short in accuracy. Recent advances in deep learning have aimed to address this, but may not adequately learn the biophysical model of RNA folding. Many deep learning approaches are also too complex, incorporating multi-model systems, ensemble strategies, or requiring external data like multiple sequence alignments. In this study, we demonstrate that a single deep learning model, relying solely on RNA sequence input, can effectively learn a biophysical model and outperform existing deep learning methods in standard benchmarks, as well as achieve comparable results to methods that utilize multi-sequence alignments. We dub this model RNAformer and achieve these benefits by a two-dimensional latent space, axial attention, and recycling in the latent space. Further, we found that our model performance improves when we scale it up. We also demonstrate how to refine a pre-trained RNAformer with fine-tuning techniques, which are particularly efficient when applied to a limited amount of high-quality data. A further aspect of our work is addressing the challenges in dataset curation in deep learning, especially regarding data homology. We tackle this through an advanced data processing pipeline that allows for training and evaluation of our model across various levels of sequence similarity. Our models and datasets are openly accessible, offering a simplified yet effective tool for RNA secondary structure prediction.",,,,Germany,,,,4.0,,2025-05-01 10:42,,,,,,Academia,,,,,,,Academia,,,,3146.8515482224,,,,,
INTELLECT-1,Language,,"Prime Intellect,Hugging Face,Arcee AI","Sami Jaghouar, Jack Min Ong, Manveer Basra, Fares Obeid, Jannik Straube, Michael Keiblinger, Elie Bakouch, Lucas Atkins, Maziyar Panahi, Charles Goddard, Max Ryabinin, Johannes Hagemann",2024-11-29,INTELLECT-1 Technical Report,https://github.com/PrimeIntellect-ai/prime/blob/main/INTELLECT_1_Technical_Report.pdf,,,Trained with respectable MFU despite globally distributed compute,10000000000.0,,6.000001e+22,10B parameters trained on 1T tokens: 6 * 10B * 1T = 6e22,,"Table 1. ""The total number of tokens in our data mix, processed with the Llama-3 tokenizer, consists of over 6 trillion tokens"" however they only train the model with 1T.",1000000000000.0,"Table 1. ""The total number of tokens in our data mix, processed with the Llama-3 tokenizer, consists of over 6 trillion tokens"" however they only train the model with 1T.",1008.0,"""The pre-training of INTELLECT-1 for 1 trillion tokens took place over 42 days, from October 10, 2024, to November 22, 2024""",,Self-supervised learning,Confident,,,,,"United States of America,Multinational,United States of America,United States of America",,,,,,2025-05-09 11:32,,,,,,"Industry,Industry,Industry",,,,,,,"Industry,Industry,Industry",,,,,Operation counting,,,,
DeepThought-8B,Language,Chat,Ruliad,,2024-11-27,"Deepthought-8B
",https://huggingface.co/ruliad/deepthought-8b-llama-v0.01-alpha,,,,8000000000.0,,,,,,,,,,,,Confident,"Deepthought-8B is a small and capable reasoning model built on LLaMA-3.1 8B, designed to make AI reasoning more transparent and controllable. Despite its relatively small size, it achieves sophisticated reasoning capabilities that rival much larger models.

",,,API access,United Kingdom of Great Britain and Northern Ireland,Llama 3.1-8B,,,,,2025-05-01 10:42,,,,,,Industry,,,,,,,Industry,,,,,,,,,
Ovis1.6-Gemma2-27B,"Multimodal,Language,Vision","Language modeling/generation,Visual question answering",Alibaba,,2024-11-26,Ovis1.6-Gemma2-27B,https://huggingface.co/AIDC-AI/Ovis1.6-Gemma2-27B,,,,28900000000.0,,,,,,,,,,,,Confident,"We are excited to announce the open-sourcing of Ovis-1.6, our latest multi-modal large language model. Ovis is a novel Multimodal Large Language Model (MLLM) architecture, designed to structurally align visual and textual embeddings.",,,Open weights (unrestricted),China,"Gemma 2 27B,SigLIP 400M",,,,,2025-05-01 10:42,,,,,,Industry,,,,,,"https://huggingface.co/AIDC-AI/Ovis1.6-Gemma2-27B
apache 2.0",Industry,,,,,,AIDC-AI,,,
Fugatto 1,"Multimodal,Language,Audio",Audio generation,NVIDIA,"Rafael Valle, Rohan Badlani, Zhifeng Kong, Sang-gil Lee, Arushi Goel, Sungwon Kim,
Joao Felipe Santos, Shuqi Dai, Siddharth Gururani, Aya AlJa'fari, Alex Liu, Kevin Shih, Wei Ping, Bryan Catanzaro
",2024-11-25,"Fugatto 1 - Foundational Generative Audio Transformer Opus 1
",https://research.nvidia.com/publication/2024-11_fugatto-1-foundational-generative-audio-transformer-opus-1,,SOTA improvement,"""We showcase Fugattoâ€™s performance on traditional TTA benchmarks that
measure a modelâ€™s ability to synthesize general sounds (AudioCAPS) and music (MusicCAPS) that follow instructions provided in text. We use the metrics (FD, FAD, and IS) and data splits (train, test) used in Kong et al. (2024b). Results in Table 3a and Table 3b shows that our model achieves strictly better scores than existing generalist models, while occasionally outperforming expert models""",2500000000.0,,,,,,,"""our dataset is comprised of at least 50,000 hours of audio""",,,NVIDIA A100,Supervised,Confident,"Fugatto is a versatile audio synthesis and transformation model capable of following free-form text instructions with optional audio inputs. While large language models (LLMs) trained with text on a simple next-token prediction objective can learn to infer instructions directly from the data, models trained solely on audio data lack this capacity. This is because audio data does not inherently contain the instructions that were used to generate it. To overcome this challenge, we introduce a specialized dataset generation approach optimized for producing a wide range of audio generation and transformation tasks, ensuring the data reveals meaningful relationships between audio and language. Another challenge lies in achieving compositional abilities â€“ such as combining, interpolating between, or negating instructions â€“ using data alone. To address it, we propose ComposableART, an inference-time technique that extends classifier-free guidance to compositional guidance. It enables the seamless and flexible composition of instructions, leading to highly customizable audio outputs outside the training distribution. Our evaluations across a diverse set of tasks demonstrate that Fugatto performs competitively with specialized models, while ComposableART enhances its sonic palette and control over synthesis. Most notably, we highlight our frameworkâ€™s ability to synthesize emergent sounds â€“ sonic phenomena that transcend conventional audio generation â€“ unlocking new creative possibilities. Demo Website.",,,Unreleased,United States of America,,,,32.0,,2025-06-04 17:47,,,,,,Industry,,,,,Unreleased,"demos only
https://fugatto.github.io/",Industry,,,Unknown,25178.17637397875,,,,,
ConfRank,Biology,Drug discovery,"University of Bonn,Institute for Numerical Simulation,Fraunhofer Institute for Algorithms and Scientific Computing","Christian HÃ¶lzer, Rick Oerderm, Stefan Grimme, Jan Hamaekers,",2024-11-24,ConfRank: Improving GFN-FF Conformer Ranking with Pairwise Training,https://pubs.acs.org/doi/abs/10.1021/acs.jcim.4c01524,,,,150000.0,,,,,,1400001.0,"7,349 molecular ensembles Ã— 20 conformers = 146,980 conformers
Pairs per ensemble = (20 Ã— 19) Ã· 2 = 190 pairs
Total pairs = 7,349 ensembles Ã— 190 pairs = 1,396,310 datapoints",,,NVIDIA GeForce RTX 3070,,Confident,"Conformer ranking is a crucial task for drug discovery, with methods for generating conformers often based on molecular (meta)dynamics or sophisticated sampling techniques. These methods are constrained by the underlying force computation regarding runtime and energy ranking accuracy, limiting their effectiveness for large-scale screening applications. To address these ranking limitations, we introduce ConfRank, a machine learning-based approach that enhances conformer ranking using pairwise training. We demonstrate its performance using GFN-FF-generated conformer ensembles, leveraging the DimeNet++ architecture trained on pairs of 159 760 uncharged organic compounds from the GEOM data set with r2SCAN-3c reference level. Instead of predicting only on single molecules, this approach captures relative energy differences between conformers, leading to a significant improvement of the overall conformational ranking, outperforming GFN-FF and GFN2-xTB. Thereby, the pairwise RMSD of the relative energy difference of two conformers can be reduced from 5.65 to 0.71 kcal molâ€“1 on the test data set, allowing to correctly identify up to 81% of all lowest lying conformers correctly (GFN-FF: 10%, GFN2-xTB: 47%). The ConfRank approach is cost-effective, allowing for scalable deployment on both CPU and GPU, achieving runtime accelerations by up to 2 orders of magnitude compared to GFN2-xTB. Out-of-sample investigations on CREST-generated conformer ensembles from the QM9 data set and conformers taken from an extended GMTKN55 data set show promising results for the robustness of this approach. Thereby, ranking correlation coefficient such as Spearman can be improved to 0.90 (GFN-FF: 0.39, GFN2-xTB: 0.84) reducing the probability of an incorrect sign flip in pairwise energy comparison from 32 to 7%. On the extended GMTKN55 subsets the pairwise MAD (RMSD) could be reduced on almost all subsets by up to 62% (58%) with an average improvement of 30% (29%). Moreover, an exemplary case study on vancomycin shows similar performance, indicating applicability to larger (bio)molecular structures. Furthermore, we motivate the usage of the pairwise training approach from a theoretical perspective, highlighting that while pairwise training can lead to a decline in single sample prediction of absolute energies for ML models, it significantly enhances conformer ranking performance. The data and models used in this study are available at https://github.com/grimme-lab/confrank.",,,,"Germany,Germany,Germany",,,,1.0,,2025-05-01 10:42,,,,,,Academia,,,,,,,Academia,,,,237.77996901285815,,,,,
 Tulu 3 (TÃ¼lu 3) 70B,Language,"Language modeling/generation,Protein question answering","Allen Institute for AI,University of Washington","Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James V. Miranda, Alisa Liu, Nouha Dziri, Xinxi Lyu, Yuling Gu, Saumya Malik, Victoria Graf, Jena D. Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind Tafjord, Chris Wilhelm, Luca Soldaini, Noah A. Smith, Yizhong Wang, Pradeep Dasigi, Hannaneh Hajishirzi",2024-11-21,TÃœLU 3: Pushing Frontiers in Open Language Model Post-Training,https://allenai.org/papers/tulu-3-report.pdf,,,,70000000000.0,70B,,,Tulu 3,"Training Approach:
SFT: Supervised finetuning
DPO: Preference tuning using curated datasets and new algorithms
RLVR: Reinforcement learning with verifiable rewards to target skills like math and reasoning.

SFT:
""To train our TÃœLU 3 models, we used between 4 and 16 8xH100 nodes with high speed interconnect. We used an effective batch size of 128 and a maximum sequence length of 4,096 tokens. We trained for two epochs..""

DPO:
Final DPO Training Hyperparameters
Batch Size (effective) 128
Max Token Length 2,048
Number of Epochs 1
1400 steps",,,,,NVIDIA H100 SXM5 80GB,,Confident,"Language model post-training is applied to refine behaviors and unlock new skills across a wide range of recent language models, but open recipes for applying these techniques lag behind proprietary ones. The underlying training data and recipes for post-training are simultaneously the most important pieces of the puzzle and the portion with the least transparency. To bridge this gap, we introduce TÃœLU 3, a family of fully-open state-of-the-art post-trained models, alongside its data, code, and training recipes, serving as a comprehensive guide for modern post-training techniques. TÃœLU 3, which builds on Llama 3.1 base models, achieves results surpassing the instruct versions of Llama 3.1, Qwen 2.5, Mistral, and even closed models such as GPT-4o-mini and Claude 3.5-Haiku. The training algorithms for our models include
supervised finetuning (SFT), Direct Preference Optimization (DPO), and a novel method we call Reinforcement Learning with Verifiable Rewards (RLVR). With TÃœLU 3, we introduce a multi-task evaluation scheme for post-training recipes with development and unseen evaluations, standard benchmark implementations, and substantial decontamination of existing open datasets
on said benchmarks. We conclude with analysis and discussion of training methods that did not reliably improve performance.
In addition to the TÃœLU 3 model weights and demo, we release the complete recipe â€“ including datasets for diverse core skills, a robust toolkit for data curation and evaluation, the training code and infrastructure, and, most importantly, a detailed report for reproducing and further adapting the TÃœLU 3 approach to more domains.",,,Open weights (restricted use),"United States of America,United States of America",Llama 3.1-70B,,,80.0,,2025-01-22 11:29,,,,,,"Research collective,Academia",,,,,Open source,"https://huggingface.co/allenai/Llama-3.1-Tulu-3-70B
llama license

https://github.com/allenai/open-instruct
apache 2","Research collective,Academia",,,,110164.33436095688,,,,,
DeepSeek-R1-Lite-Preview,Language,Language modeling/generation,DeepSeek,,2024-11-20,,https://x.com/deepseek_ai/status/1859200141355536422,,,,,,,,,,,,,,,,Unknown,,,,Hosted access (no API),China,,,,,,2025-05-01 10:42,,,,,,Industry,,,,,,,Industry,,,,,,,,,
Suno v4,Audio,Audio generation,Suno,Team Suno,2024-11-19,Introducing v4,https://suno.com/blog/v4,,Significant use,https://www.semrush.com/website/suno.ai/overview/,,,,,,,,,,,,,Unknown,"Today, weâ€™re excited to introduce v4â€”the next step toward enabling you to make music at the speed of your ideas. When we launched v3 earlier this year, it opened up new possibilities for music creation. Post v3 launch, weâ€™ve refined what worked and added more where it mattered most. The result is v4â€”a major update that takes music creation to the next level. v4 delivers cleaner audio, sharper lyrics, and more dynamic song structures.",,,Hosted access (no API),United States of America,,,,,,2025-06-06 12:52,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
Boltz-1,Biology,"Protein folding prediction,Protein-ligand contact prediction,Protein interaction prediction,Protein structure comparison","Massachusetts Institute of Technology (MIT), Genesis Therapeutics","Jeremy Wohlwend, Gabriele Corso, Saro Passaro,  Mateo Reveiz, Ken Leidal, Wojtek Swiderski,  Tally Portnoi,  Itamar Chinn,  Jacob Silterra,  Tommi Jaakkola, Regina Barzilay",2024-11-19,Boltz-1: Democratizing Biomolecular Interaction Modeling,https://www.biorxiv.org/content/10.1101/2024.11.19.624167v2.abstract,,,,,,,,PDB (Protein Data Bank),"""For training we use all PDB structures [Berman et al., 2000] released before 2021-09-30 (same training cut-off date as AlphaFold3) and with a resolution of at least 9ËšA.""
""We construct MSAs for the full PDB data""",3588096000.0,"""We train the structure prediction model <..> for a total of 68k steps with a batch size of 128. During the first 53k iterations, we use a crop size of 384 tokens and 3456 atoms <..>. For the last 15k iterations, we only sampled from the PDB structures and had a crop size of 512 tokens and 4608 atoms. ""

53000*384*128+15000*512*128 = 3 588 096 000 tokens",,"""As a comparison AlphaFol3 trained a
similar architecture <..>, which required approximately four
times the computing time""

Alphafold 3 was trained on 256 A100 GPUs for 480 hours",,,Likely,"Understanding biomolecular interactions is fundamental to advancing fields like drug discovery and protein design. In this paper, we introduce Boltz-1, an open-source deep learning model incorporating innovations in model architecture, speed optimization, and data processing achieving AlphaFold3-level accuracy in predicting the 3D structures of biomolecular complexes. Boltz-1 demonstrates a performance on-par with state-of-the-art commercial models on a range of diverse benchmarks, setting a new benchmark for commercially accessible tools in structural biology. By releasing the training and inference code, model weights, datasets, and benchmarks under the MIT open license, we aim to foster global collaboration, accelerate discoveries, and provide a robust platform for advancing biomolecular modeling.",,,Open weights (unrestricted),"United States of America,United States of America",,,,,,2025-05-06 15:25,,,,,,"Academia,Industry",,,,,Open source,"MIT license

https://github.com/jwohlwend/boltz","Academia,Industry",,,,,,,,,
Pixtral Large,Multimodal,Vision-language generation,Mistral AI,"Albert Jiang, Alexandre Sablayrolles, Alexis Tacnet, Alok Kothari, Antoine Roux, Arthur Mensch, Audrey Herblin-Stoop, Augustin Garreau, Austin Birky, Bam4d, Baptiste Bout, Baudouin de Monicault, Blanche Savary, Carole Rambaud, Caroline Feldman, Devendra Singh Chaplot, Diego de las Casas, Diogo Costa, Eleonore Arcelin, Emma Bou Hanna, Etienne Metzger, Gaspard Blanchet, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Harizo Rajaona, Henri Roussez, Hichem Sattouf, Ian Mack, Jean-Malo Delignon, Jessica Chudnovsky, Justus Murke, Kartik Khandelwal, Lawrence Stewart, Louis Martin, Louis Ternon, Lucile Saulnier, LÃ©lio Renard Lavaud, Margaret Jennings, Marie Pellat, Marie Torelli, Marie-Anne Lachaux, Marjorie Janiewicz, MickaÃ«l Seznec, Nicolas Schuhl, Niklas Muhs, Olivier de Garrigues, Patrick von Platen, Paul Jacob, Pauline Buche, Pavan Kumar Reddy, Perry Savas, Pierre Stock, Romain Sauvestre, Sagar Vaze, Sandeep Subramanian, Saurabh Garg, Sophia Yang, Szymon Antoniak, Teven Le Scao, Thibault Schueller, Thibaut Lavril, Thomas Wang, ThÃ©ophile Gervet, TimothÃ©e Lacroix, Valera Nemychnikova, Wendy Shang, William El Sayed, William Marshall",2024-11-18,Pixtral Large,https://mistral.ai/news/pixtral-large/,,"Significant use,SOTA improvement",Number of downloads not visible,124000000000.0,"123B multimodal decoder, 1B parameter vision encoder",,,,,,,,,,Supervised,Confident,"Today we announce Pixtral Large, a 124B open-weights multimodal model built on top of Mistral Large 2. Pixtral Large is the second model in our multimodal family and demonstrates frontier-level image understanding. Particularly, the model is able to understand documents, charts and natural images, while maintaining the leading text-only understanding of Mistral Large 2.",,,Open weights (non-commercial),France,Mistral Large 2,,,,,2025-06-06 11:49,,,,,,Industry,,,,,Unreleased,"mrl license (research only), separate license is needed for commercial usage

https://huggingface.co/mistralai/Pixtral-Large-Instruct-2411",Industry,,,,,,mistralai,,,
360Zhinao2-7B,Language,"Question answering,Language modeling/generation,Code generation,Quantitative reasoning",360 Security Technology,,2024-11-18,360Zhinao2 (360æ™ºè„‘),https://github.com/Qihoo360/360zhinao2,,,,7000000000.0,,4.242e+23,6 FLOP / parameter / token * 7000000000 parameters * 10100000000000 tokens = 4.242e+23 FLOP,Unspecified unreleased,,10100000000000.0,"""Base Model: Using popular two-stage training method, In the first stage we totally train 10T tokens with a cosine learning rate schedule. In the second stage we increase the proportion of high-quality data and totally train 100B tokens, with the learning rate decaying directly to 0. The total training data for 360Zhinao2-7B amounts to 10.1T tokens.""",,,,,Confident,"[2024.11.18] We release 360Zhinao2-7B, providing access to both the Base model and Chat models with text lengths of 4K, 32K, and 360K.

We used the open-source tool OpenCompass to evaluate the model and compared it with open-source models under 10B from the past six months. The 360Zhinao2-7B model is competive. The 360Zhinao2-7B model performs well on Chinese benchmarks such as CEval, C3 and LCSTS. The average socres of Chinese benchmarks is No 1. It also ranks No 1 on Math which is a challenging competition math dataset. The 360Zhinao2-7B model has advantages in Chinese benchmark and challenging competition math.",,,Open weights (unrestricted),China,360Zhinao-7B,,,,,2025-05-26 20:13,,,,,,Industry,,,,,Unreleased,"https://huggingface.co/qihoo360/360Zhinao2-7B-Base
Apache 2.0

It seems that there is only fine-tuning and inference code here, no pretraining code

https://github.com/Qihoo360/360zhinao2",Industry,,,,,Operation counting,qihoo360,,,
k0-math,"Mathematics,Language","Mathematical reasoning,Quantitative reasoning",Moonshot,,2024-11-16,Chinese AI start-up unveils latest reasoning model comparable to OpenAI o1 series,https://web.archive.org/web/20250124103542/https://www.globaltimes.cn/page/202411/1323248.shtml,,SOTA improvement,"In four mathematical benchmark tests - China's high school entrance examination, college entrance examination, postgraduate entrance examination and math with introductory competition problems - the k0-math initial model outperformed OpenAI's o1-mini and o1-preview models, according to a statement sent from Moonshot AI to the Global Times on Sunday. ",,This tweet mentions that this LLM has 100B params but I have not found the information anywhere else. https://x.com/kimmonismus/status/1902710969534460109,,,,,,,,,,,Unknown,"Artificial general intelligence start-up Kimi, owned by Chinese AI start-up Moonshot AI, on Saturday launched its first reasoning AI model k0-math. The model can be compared to US-based OpenAI's reasoning AI series o1, including the o1-mini and o1-preview, in certain mathematics tests.

In four mathematical benchmark tests - China's high school entrance examination, college entrance examination, postgraduate entrance examination and math with introductory competition problems - the k0-math initial model outperformed OpenAI's o1-mini and o1-preview models, according to a statement sent from Moonshot AI to the Global Times on Sunday. 

In the two more challenging competition-level math problem sets - OMNI-MATH and AIME tests - the performance of the k0-math initial model reached 90 percent and 83 percent of the highest scores achieved by o1-mini, respectively.",,,Unreleased,China,,,,,,2025-06-01 16:30,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
LLaVA-CoT,"Language,Vision,Multimodal","Visual question answering,Language modeling/generation,Quantitative reasoning","Peking University,Tsinghua University,Peng Cheng Laboratory,Alibaba DAMO Academy,Lehigh University","Guowei Xu, Peng Jin, Hao Li, Yibing Song, Lichao Sun, Li Yuan",2024-11-15,"LLaVA-CoT: Let Vision Language Models Reason Step-by-Step
",https://arxiv.org/abs/2411.10440,1.0,,,11000000000.0,11b,,8 H100 GPUs,Llava-CoT-100K,,198000.0,,,,NVIDIA H100 SXM5 80GB,,Confident,"In this work, we introduce LLaVA-CoT, a novel VLM designed to conduct autonomous multistage reasoning. Unlike chain-of-thought prompting, LLaVA-CoT independently engages in sequential stages of summarization, visual interpretation, logical reasoning, and conclusion generation. This structured approach enables LLaVA-CoT to achieve marked improvements in precision on reasoning-intensive tasks. ",3.0,,Open weights (non-commercial),"China,China,China,China,United States of America",Llama 3.2 11B,,,8.0,,2025-06-09 10:39,,,,,,"Academia,Academia,Academia,Industry,Academia",,,,,Open (non-commercial),"Apache 2.0 for weights (but I assume they still need to comply with Llama license):

https://huggingface.co/Xkev/Llama-3.2V-11B-cot

Apache 2.0 for code:
https://github.com/PKU-YuanGroup/LLaVA-CoT

repo disclaimer:
""The majority of this project is released under the Apache 2.0 license as found in the LICENSE file.
The service is a research preview intended for non-commercial use only, subject to LLAMA 3.2 COMMUNITY LICENSE AGREEMENT, and Terms of Use of the data generated by OpenAI.""","Academia,Academia,Academia,Industry,Academia",,,,11017.905508717806,,Xkev,,,
Athene-V2,Language,"Language modeling/generation,Quantitative reasoning,Question answering",Nexusflow,,2024-11-14,Introducing Athene-V2: Advancing Beyond the Limits of Scaling with Targeted Post-training,https://nexusflow.ai/blogs/athene-v2,,,,72000000000.0,72B,,,Unspecified unreleased,,,,,,,,Confident,"Weâ€™re thrilled to announce Athene-V2, our latest 72B model suite. Fine-tuned from Qwen 2.5 72B, Athene-V2 competes with GPT-4o across key capabilities, powered by a meticulously designed data and RLHF pipeline. As the industry recognizes the slow-down of scaling lawâ€”where increasing model size alone no longer delivers universal capability improvementsâ€”thereâ€™s a growing need for specialized customization to enhance specific capabilities. Our post-training process illustrates this shift, demonstrating how our data and tuning solutions allow us to finely optimize for distinct skills and use cases.

Hereâ€™s a look at the unique specializations that position Athene-V2 models along the Pareto frontier of LLM post-training:

Athene-V2-Chat-72B: A state-of-the-art chat model, matching GPT-4o across multiple benchmarks. It outperforms GPT-4o in chat helpfulness (Arena-Hard), excels in code completion (ranking #2 on bigcode-bench-hard), mathematics (MATH), and handles long log extraction with higher precision (our internal benchmark).
Athene-V2-Agent-72B: Striking a balance between chat and agent capabilities, this model offers concise, directive chat responses, surpassing GPT-4o in our latest Nexus-V2 function calling benchmarks that focus on hard enterprise-level function calling use cases.
",,,Open weights (non-commercial),United States of America,Qwen2.5-72B,,,,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,License: Nexusflow Research License,Industry,,,,,,Nexusflow,,,
Gemma2 9B CPT Sahabat-AI,Language,"Language modeling/generation,Question answering","Indosat,Tech Mahindra,AI Singapore,GoTo","AI Singapore: Chan Adwin, Cheng Nicholas, Choa Esther, Huang Yuli, Lau Wayne, Lee Chwan Ren, Leong Wai Yi, Leong Wei Qi, Limkonchotiwat Peerat, Liu Bing Jie Darius, Montalan Jann Railey, 
Ng Boon Cheong Raymond, Ngui Jian Gang, Nguyen Thanh Ngan, Ong Brandon, Ong Tat-Wee David, Ong Zhi Hao, Rengarajan Hamsawardhini, Siow Bryan, Susanto Yosephine, Tai Ngee Chia, Tan Choon Meng, Teng Walter, Teo Eng Sipp Leslie, Teo Wei Yi, Tjhi William, Yeo Yeow Tong, Yong Xianbin

PT GoTo Gojek Tokopedia Tbk: Anissa Dininta, Chau Shiau Ching, Choiri Hendra Hadhil, Goel Priyank, Saini Ajay Kumar, Shalev Ofir, Tan Daryl, Tep Kilian Rithi, Tiwari Anupam, Widjojo Daniel",2024-11-14,The First Large Language Model In Bahasa Indonesia Developed by Indonesians,"https://sahabat-ai.com/

https://huggingface.co/GoToCompany/gemma2-9b-cpt-sahabatai-v1-instruct",,,,9000000000.0,9B,,,Unspecified unreleased,"""Gemma2 9B CPT Sahabat-AI v1 Instruct was trained on a wide range of synthetic instructions, alongside publicly available instructions hand-curated by the team with the assistance of native speakers. In addition, special care was taken to ensure that the datasets used had commercially permissive licenses through verification with the original data source.""",,,6.0,"""The training process for fine-tuning was approximately 4 hours, with alignment taking 2 hours, both on 8x H100-80GB GPUs""",NVIDIA H100 SXM5 80GB,,Confident,"Sahabat-AI (Indonesian language for â€œclose friendsâ€) is a collection of Large Language Models (LLMs) which has been pretrained and instruct-tuned for Indonesian language and its various dialects. Sahabat-AI ecosystem is co-initiated by Indonesian tech and telecommunication companies: GoTo Group and Indosat Ooredoo Hutchison.

Gemma2 9B CPT Sahabat-AI v1 Instruct is an Indonesian-focused model which has been fine-tuned with around 448,000 Indonesian instruction-completion pairs alongside an Indonesian-dialect pool consisting of 96,000 instruction-completion pairs in Javanese and 98,000 instruction-completion pairs in Sundanese. Additionally, we added a pool of 129,000 instruction-completion pairs in English.

Co-initiated by: PT GoTo Gojek Tokopedia Tbk, Indosat Ooredoo Hutchison
Developed by: PT GoTo Gojek Tokopedia Tbk, AI Singapore
Model type: Decoder
Languages: English, Indonesian, Javanese, Sundanese
License: Gemma Community License",,,Open weights (restricted use),"Indonesia,India,Singapore,Indonesia",Gemma 2 9B,51290496000000000000,989400000000000 FLOP / GPU / sec [bf16 assumed] ** 8 GPUs * 6 hours * 3600 sec / hour * 0.3 [assumed utilization] = 5.1290496e+19 FLOP,8.0,,2025-05-22 14:27,,,,,,"Industry,Industry,Government,Government",,,,,Unreleased,"Gemma Community License

https://huggingface.co/GoToCompany/gemma2-9b-cpt-sahabatai-v1-instruct","Industry,Industry,Government,Government",,,,11018.150873278302,Hardware,GoToCompany,,,
Qwen2.5-Coder (32B),Language,"Language modeling/generation,Code generation",Alibaba,"Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, Kai Dang, Yang Fan, Yichang Zhang, An Yang, Rui Men, Fei Huang, Bo Zheng, Yibo Miao, Shanghaoran Quan, Yunlong Feng, Xingzhang Ren, Xuancheng Ren, Jingren Zhou, Junyang Lin",2024-11-12,Qwen2.5-Coder Technical Report,https://arxiv.org/abs/2409.12186,,,,32500000000.0,32.5B (31B - non emb),1.0725e+24,"Assuming 1 epoch

6ND = 6*32.5 parameters *10^9*5.5*10^12 tokens = 1.0725e+24","GitHub,Common Crawl,Unspecified unreleased","""We collected public repositories from GitHub created before February 2024""

""We curated a large-scale and high-quality text-code mixed
dataset from Common Crawl, which includes code-related documentation, tutorials, blogs,
and more""

""We used CodeQwen1.5, the predecessor of Qwen2.5-Coder, to generate large-scale synthetic datasets.""

Knowledge cutoff date is March 2024, according to https://llm-stats.com/models/qwen-2.5-coder-32b-instruct. ",5500000000000.0,"""As a code-specific model, Qwen2.5-Coder is built upon the Qwen2.5 architecture and continues pretrained on a vast corpus of over 5.5 trillion tokens.""",,,,,Confident,"In this report, we introduce the Qwen2.5-Coder series, a significant upgrade from its predecessor, CodeQwen1.5. This series includes six models: Qwen2.5-Coder-(0.5B/1.5B/3B/7B/14B/32B). As a code-specific model, Qwen2.5-Coder is built upon the Qwen2.5 architecture and continues pretrained on a vast corpus of over 5.5 trillion tokens. Through meticulous data cleaning, scalable synthetic data generation, and balanced data mixing, Qwen2.5-Coder demonstrates impressive code generation capabilities while retaining general and math skills. These models have been evaluated on a wide range of code-related tasks, achieving state-of-the-art (SOTA) performance across more than 10 benchmarks, including code generation, completion, reasoning, and repair, consistently outperforming larger models of the same model size. We believe that the release of the Qwen2.5-Coder series will advance research in code intelligence and, with its permissive licensing, support wider adoption by developers in real-world applications.",1.0,,Open weights (unrestricted),China,,,,,,2025-05-29 08:57,,"The paper does not mention any hardware, GPUs or any information regarding the hardware used.",,,,Industry,,,,,Unreleased,"Apache 2.0
https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct

though they have apache 2.0 github repository it seems to be inference code rather than training code",Industry,,,,,Operation counting,Qwen,,,
stFormer,Biology,Spatial Transcriptomics,"Shanghai Jiao Tong University,Chinese Academy of Sciences","Shenghao Cao, Ye Yuan",2024-11-09,"stFormer: a foundation model for spatial transcriptomics
",https://www.biorxiv.org/content/10.1101/2024.09.27.615337v5.abstract,,,,,,,,,,580000000.0,"
Minimal datapoint: gene expression value in a cell
Total vocabulary: 19,264
Number of genes per cell: unknown, estimated at ~1000
Number of cells: 580,000
Total datapoints = 580,000 Ã— 1000 = 580000000",,,,,Likely,"Recent foundation models for single-cell transcriptomics data generate informative, context-aware gene representations. The spatially resolved transcriptomics data offer extra positional insights, yet corresponding gene representation methods that integrate both intracellular and spatial context are still lacking. Here, we introduce a gene representation framework tailored for spatial transcriptomics data. It incorporates ligand genes within the spatial niche into the transformer encoder of single-cell transcriptomics. We further propose a biased cross-attention method to enable the framework to do learning with single-cell resolution on low-resolution, whole-transcriptome Visium data. We implemented our framework on a hybrid Visium dataset derived from two human tissue types with distinct developmental and disease states, and tested on various downstream applications. Compared with the latest foundation model for single-cell transcriptomics, our spatially informed gene representations could identify cell groups and gene functions more accurately, and could predict the perturbation effects of cell-cell ligand-receptor interactions on downstream targets.",,,,"China,China",,,,,,2025-05-01 10:42,,,,,,"Academia,Academia",,,,,,,"Academia,Academia",,,,,,,,,
Hunyuan-Large,Language,"Language modeling/generation,Question answering,Code generation,Translation",Tencent,"Xingwu Sun, Yanfeng Chen, Yiqing Huang, Ruobing Xie, Jiaqi Zhu, Kai Zhang, Shuaipeng Li, Zhen Yang, Jonny Han, Xiaobo Shu, Jiahao Bu, Zhongzhi Chen, Xuemeng Huang, Fengzong Lian,
Saiyong Yang, Jianfeng Yan, Yuyuan Zeng, Xiaoqin Ren, Chao Yu, Lulu Wu, Yue Mao, Jun Xia, Tao Yang, Suncong Zheng, Kan Wu, Dian Jiao, Jinbao Xue, Xipeng Zhang, Decheng Wu, Kai Liu, Dengpeng Wu, Guanghui Xu, Shaohua Chen, Shuang Chen, Xiao Feng, Yigeng Hong, Junqiang Zheng, Chengcheng Xu, Zongwei Li, Xiong Kuang, Jianglu Hu, Yiqi Chen, Yuchi Deng, Guiyang Li, Ao Liu, Chenchen Zhang, Shihui Hu, Zilong Zhao, Zifan Wu, Yao Ding, Weichao Wang, Han Liu, Roberts Wang, Hao Fei, Peijie Yu, Ze Zhao, Xun Cao, Hai Wang, Fusheng Xiang, Mengyuan Huang, Zhiyuan Xiong, Bin Hu, Xuebin Hou, Lei Jiang, Jianqiang Ma, Jiajia Wu, Yaping Deng, Yi Shen, Qian Wang, Weijie Liu, Jie Liu, Meng Chen, Liang Dong, Weiwen Jia, Hu Chen, Feifei Liu, Rui Yuan, Huilin Xu, Zhenxiang Yan, Tengfei Cao, Zhichao Hu, Xinhua Feng, Dong Du, Tinghao Yu, Yangyu Tao, Feng Zhang, Jianchen Zhu, Chengzhong Xu, Xirui Li, Chong Zha, Wen Ouyang, Yinben Xia, Xiang Li, Zekun He, Rongpeng Chen, Jiawei Song, Ruibin Chen, Fan Jiang, Chongqing Zhao, Bo Wang, Hao Gong, Rong Gan, Winston Hu, Zhanhui Kang, Yong Yang, Yuhong Liu, Di Wang, and Jie Jiang.
",2024-11-06,Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated Parameters by Tencent,https://arxiv.org/abs/2411.02265,,Training cost,,389000000000.0,"""a total of 389 billion parameters and 52 billion activation parameters""",3.49237e+24,"52B activated parameters

6ND = 6*52*10^9*7*10^12 = 2.184 Ã— 10^24

They also suggest more precise formula to calculate MoE compute budget:

9.59ND + 2.3 Ã— 10^8D = 9.59*52*10^9*7*10^12 + 2.3 Ã— 10^8 Ã—  7*10^12 = 3.49237Ã—10^24

which seems closer to projected compute on Figure 3",Unspecified unreleased,,7000000000000.0,"""# Trained Tokens 7T""  Table 1",,,,,Confident,"In this paper, we introduce Hunyuan-Large, which is currently the largest open-source Transformer-based mixture of experts model, with a total of 389 billion parameters and 52 billion activation parameters, capable of handling up to 256K tokens. We conduct a thorough evaluation of Hunyuan-Large's superior performance across various benchmarks including language understanding and generation, logical reasoning, mathematical problem-solving, coding, long-context, and aggregated tasks, where it outperforms LLama3.1-70B and exhibits comparable performance when compared to the significantly larger LLama3.1-405B model. Key practice of Hunyuan-Large include large-scale synthetic data that is orders larger than in previous literature, a mixed expert routing strategy, a key-value cache compression technique, and an expert-specific learning rate strategy. Additionally, we also investigate the scaling laws and learning rate schedule of mixture of experts models, providing valuable insights and guidances for future model development and optimization. The code and checkpoints of Hunyuan-Large are released to facilitate future innovations and applications.",,,Open weights (restricted use),China,,,,,,2025-05-01 10:42,,"The paper does not mention any hardware, GPUs or any information regarding the hardware used.",,,,Industry,,,3.49237e+24,,Open (restricted use),"the license doesn't regulate usage in the EU
also requires additional licensing in case of massive commercial use",Industry,,,"BF16,FP16",,Operation counting,,,,
FLUX1.1 [pro] Ultra,Image generation,Image generation,Black Forest Labs,,2024-11-06,"Our best model for photo-realistic images at 2k resolution, with a ""raw"" option for extra realism.",https://blackforestlabs.ai/flux-1-1-ultra/,,,,,,,,Unspecified unreleased,,,,,,,,Unknown,"FLUX1.1 [pro] â€“ ultra mode: This option enables image generation at four times the resolution of standard FLUX1.1 [pro], without sacrificing prompt adherence. Unlike many high-resolution models that experience significant slowdowns at higher resolutions, our performance benchmarks show sustained fast generation timesâ€”over 2.5x faster than comparable high-resolution offerings. This model is available at a competitive price of $0.06 per image.",,,API access,Germany,,,,,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
FLUX1.1 [pro] Raw,Image generation,Image generation,Black Forest Labs,,2024-11-06,Our best and most efficient model for large-scale image generation workloads.,https://blackforestlabs.ai/flux-1-1-ultra/,,,,,,,,Unspecified unreleased,,,,,,,,Unknown,"FLUX1.1 [pro] â€“ raw mode: For creators seeking authenticity, our new raw mode captures the genuine feel of candid photography. Toggle this feature to generate images with a less synthetic, more natural aesthetic. Compared to other text-to-image models, raw mode significantly increases diversity in human subjects and enhances the realism of nature photography",,,API access,Germany,,,,,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
Llama-3.1-Minitron-4B,Language,"Language modeling/generation,Chat,Question answering",NVIDIA,"Saurav Muralidharan, Sharath Turuvekere Sreenivas, Raviraj Joshi, Marcin Chochowski, Mostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro, Jan Kautz, Pavlo Molchanov",2024-11-04,Compact Language Models via Pruning and Knowledge Distillation,https://arxiv.org/abs/2407.14679,,,,4000000000.0,"4b

Llama-3.1-Minitron-4B-Width-Base uses a model embedding size of 3072, 32 attention heads, MLP intermediate dimension of 9216, with 32 layers in total. Additionally, it uses Grouped-Query Attention (GQA) and Rotary Position Embeddings (RoPE).

Architecture Type: Transformer Decoder (Auto-Regressive Language Model)

Network Architecture: Llama-3.1",,,Unspecified unreleased,"""Properties: The training corpus for Llama-3.1-Minitron-4B-Width-Base consists of English and multilingual text, as well as code. Our sources cover a variety of document types such as: webpages, dialogue, articles, and other written materials. The corpus spans domains including legal, math, science, finance, and more. In our continued training set, we introduce a small portion of question-answering, and alignment style data to improve model performance.

Data Freshness: The pretraining data has a cutoff of June 2023.""",,,,"""Llama-3.1-Minitron-4B-Width-Base was trained between July 29, 2024 and Aug 3, 2024.""
5 days
",NVIDIA A100 SXM4 80 GB,,Confident,"Large language models (LLMs) targeting different deployment scales and sizes are currently produced by training each variant from scratch; this is extremely compute-intensive. In this paper, we investigate if pruning an existing LLM and then re-training it with a fraction (<3%) of the original training data can be a suitable alternative to repeated, full retraining. To this end, we develop a set of practical and effective compression best practices for LLMs that combine depth, width, attention and MLP pruning with knowledge distillation-based retraining; we arrive at these best practices through a detailed empirical exploration of pruning strategies for each axis, methods to combine axes, distillation strategies, and search techniques for arriving at optimal compressed architectures. We use this guide to compress the Nemotron-4 family of LLMs by a factor of 2-4x, and compare their performance to similarly-sized models on a variety of language modeling tasks. Deriving 8B and 4B models from an already pretrained 15B model using our approach requires up to 40x fewer training tokens per model compared to training from scratch; this results in compute cost savings of 1.8x for training the full model family (15B, 8B, and 4B). Minitron models exhibit up to a 16% improvement in MMLU scores compared to training from scratch, perform comparably to other community models such as Mistral 7B, Gemma 7B and Llama-3 8B, and outperform state-of-the-art compression techniques from the literature. We have open-sourced Minitron model weights on Huggingface, with corresponding supplementary material including example code available on GitHub.",,,Open weights (unrestricted),United States of America,Llama 3.1-8B,,,8.0,,2025-05-01 10:42,,,,,,Industry,,,,,,"This model is released under the NVIDIA Open Model License Agreement.

""This model is ready for commercial use.""

https://huggingface.co/nvidia/Llama-3.1-Minitron-4B-Width-Base",Industry,,,,6297.48846824743,,,,,
Minitron 8B,Language,"Language modeling/generation,Chat,Question answering",NVIDIA,"Saurav Muralidharan, Sharath Turuvekere Sreenivas, Raviraj Joshi, Marcin Chochowski, Mostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro, Jan Kautz, Pavlo Molchanov",2024-11-04,Compact Language Models via Pruning and Knowledge Distillation,https://arxiv.org/abs/2407.14679,,,,8300000000.0,"8.3B (table 2)

(including  6.2B non-embedding parameters)

Minitron-8B-Base is a large language model (LLM) obtained by pruning Nemotron-4 15B; specifically, we prune model embedding size, number of attention heads, and MLP intermediate dimension.

Minitron-8B-Base uses a model embedding size of 4096, 48 attention heads, and an MLP intermediate dimension of 16384. It also uses Grouped-Query Attention (GQA) and Rotary Position Embeddings (RoPE).",,,Unspecified unreleased,"""Following pruning, we perform continued training with distillation using 94 billion tokens to arrive at the final model; we use the continuous pre-training data corpus used in Nemotron-4 15B for this purpose.""

""Properties: The training corpus for Minitron-8B-Base consists of English and multilingual text, as well as code. Our sources cover a variety of document types such as: webpages, dialogue, articles, and other written materials. The corpus spans domains including legal, math, science, finance, and more. In our continued training set, we introduce a small portion of question-answering, and alignment style data to improve model performance.

Data Freshness: The pretraining data has a cutoff of June 2023.""",94000000000.0,94B training tokens (table 2),,"All experiments were performed on 16Ã— NVIDIA DGX A100 nodes (8Ã— A100 80GB) for short
turnaround times.",NVIDIA A100 SXM4 80 GB,,Confident,"Large language models (LLMs) targeting different deployment scales and sizes are currently produced by training each variant from scratch; this is extremely compute-intensive. In this paper, we investigate if pruning an existing LLM and then re-training it with a fraction (<3%) of the original training data can be a suitable alternative to repeated, full retraining. To this end, we develop a set of practical and effective compression best practices for LLMs that combine depth, width, attention and MLP pruning with knowledge distillation-based retraining; we arrive at these best practices through a detailed empirical exploration of pruning strategies for each axis, methods to combine axes, distillation strategies, and search techniques for arriving at optimal compressed architectures. We use this guide to compress the Nemotron-4 family of LLMs by a factor of 2-4x, and compare their performance to similarly-sized models on a variety of language modeling tasks. Deriving 8B and 4B models from an already pretrained 15B model using our approach requires up to 40x fewer training tokens per model compared to training from scratch; this results in compute cost savings of 1.8x for training the full model family (15B, 8B, and 4B). Minitron models exhibit up to a 16% improvement in MMLU scores compared to training from scratch, perform comparably to other community models such as Mistral 7B, Gemma 7B and Llama-3 8B, and outperform state-of-the-art compression techniques from the literature. We have open-sourced Minitron model weights on Huggingface, with corresponding supplementary material including example code available on GitHub.",,,Open weights (non-commercial),United States of America,Nemotron-4 15B,4,6*8300000000.00*94*10^9 = 4.6812e+21,8.0,,2025-05-01 10:42,,,,,,Industry,,,,,,"This model is for research and development only.

https://huggingface.co/nvidia/Minitron-8B-Base",Industry,,,,6297.48846824743,Operation counting,,,,
Minitron 4B,Language,"Language modeling/generation,Chat,Question answering",NVIDIA,"Saurav Muralidharan, Sharath Turuvekere Sreenivas, Raviraj Joshi, Marcin Chochowski, Mostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro, Jan Kautz, Pavlo Molchanov",2024-11-04,Compact Language Models via Pruning and Knowledge Distillation,https://arxiv.org/abs/2407.14679,,,,4200000000.0," 4.2B (table 4)

including 2.6B non-embedding parameters

""Minitron-4B-Base uses a model embedding size of 3072, 32 attention heads, and an MLP intermediate dimension of 9216. It also uses Grouped-Query Attention (GQA) and Rotary Position Embeddings (RoPE).""",,,Unspecified unreleased,"""Properties: The training corpus for Minitron-4B-Base consists of English and multilingual text, as well as code. Our sources cover a variety of document types such as: webpages, dialogue, articles, and other written materials. The corpus spans domains including legal, math, science, finance, and more. In our continued training set, we introduce a small portion of question-answering, and alignment style data to improve model performance.

Data Freshness: The pretraining data has a cutoff of June 2023.""",94000000000.0,94B training tokens (table 3),,"All experiments were performed on 16Ã— NVIDIA DGX A100 nodes (8Ã— A100 80GB) for short
turnaround times.",NVIDIA A100 SXM4 80 GB,,Confident,"Large language models (LLMs) targeting different deployment scales and sizes are currently produced by training each variant from scratch; this is extremely compute-intensive. In this paper, we investigate if pruning an existing LLM and then re-training it with a fraction (<3%) of the original training data can be a suitable alternative to repeated, full retraining. To this end, we develop a set of practical and effective compression best practices for LLMs that combine depth, width, attention and MLP pruning with knowledge distillation-based retraining; we arrive at these best practices through a detailed empirical exploration of pruning strategies for each axis, methods to combine axes, distillation strategies, and search techniques for arriving at optimal compressed architectures. We use this guide to compress the Nemotron-4 family of LLMs by a factor of 2-4x, and compare their performance to similarly-sized models on a variety of language modeling tasks. Deriving 8B and 4B models from an already pretrained 15B model using our approach requires up to 40x fewer training tokens per model compared to training from scratch; this results in compute cost savings of 1.8x for training the full model family (15B, 8B, and 4B). Minitron models exhibit up to a 16% improvement in MMLU scores compared to training from scratch, perform comparably to other community models such as Mistral 7B, Gemma 7B and Llama-3 8B, and outperform state-of-the-art compression techniques from the literature. We have open-sourced Minitron model weights on Huggingface, with corresponding supplementary material including example code available on GitHub.",,,Open weights (non-commercial),United States of America,Nemotron-4 15B,2,6*4200000000.00*94*10^9 = 2.3688e+21,8.0,,2025-05-01 10:42,,,,,,Industry,,,,,,"""This model is for research and development only.""

https://huggingface.co/nvidia/Minitron-4B-Base",Industry,,,,6297.48846824743,Operation counting,,,,
Uni-Med,,,,"Xun Zhu, Ying Hu, Fanbin Mo, Miao Li, Ji Wu",2024-11-01,"Uni-Med: A Unified Medical Generalist Foundation
Model For Multi-Task Learning Via Connector-MoE",https://arxiv.org/pdf/2409.17508,1.0,,,8800000000.0,7B (LLama) + 1843000000 (ViT) = 8.8B,1.425e+23,"Finetune: 10*60*60*312000000000000*0.5=5.616e+18

Base models: 8.4e+22, 5.85e+22
Total: 1.4250562e+23",,,,,10.0,,NVIDIA A800 SXM,,Likely,"We present Uni-Med, an open-source medical generalist foundation model with a unified
interface and shared parameters, which can perform six different medical tasks including
question answering, visual question answering, report generation, referring expression
comprehension, referring expression generation and image classification.",,,Unreleased,,"Llama 2-7B,ViT-G/14",5616000000000000000,,1.0,,2025-05-23 11:56,,,,,,,,,,,Open (restricted use),"https://github.com/MSIIP/Uni-Med 
(undefined license)

I haven't found released pre-trained weights",,,,,432.54870917774826,,,,,
SimPO,Language,"Language modeling/generation,Question answering","Princeton University,University of Virginia","Yu Meng, Mengzhou Xia, Danqi Chen",2024-11-01,SimPO: Simple Preference Optimization with a Reference-Free Reward,https://arxiv.org/abs/2405.14734,,,,9000000000.0,,,,"UltraChat,UltraFeedback","""Applying SimPO to Gemma 2 models presents a different trend. We evaluate SimPO using Googleâ€™s recently released Gemma-2-9B-it model [77], which represents a strong open-source model.
For training data, we generate up to 5 responses per prompt from the UltraFeedback dataset [23]""",,,1.7,"""Fine-tuning the google/gemma-2-9b-it on princeton-nlp/gemma2-ultrafeedback-armorm takes around 100 mins to finish on 8xH100 GPUs.""",NVIDIA H100 SXM5 80GB,,Confident,"Direct Preference Optimization (DPO) is a widely used offline preference optimization algorithm that reparameterizes reward functions in reinforcement learning from human feedback (RLHF) to enhance simplicity and training stability. In this work, we propose SimPO, a simpler yet more effective approach. The effectiveness of SimPO is attributed to a key design: using the average log probability of a sequence as the implicit reward. This reward formulation better aligns with model generation and eliminates the need for a reference model, making it more compute and memory efficient. Additionally, we introduce a target reward margin to the Bradley-Terry objective to encourage a larger margin between the winning and losing responses, further improving the algorithm's performance. We compare SimPO to DPO and its latest variants across various state-of-the-art training setups, including both base and instruction-tuned models such as Mistral, Llama 3, and Gemma 2. We evaluate on extensive chat-based evaluation benchmarks, including AlpacaEval 2, MT-Bench, and Arena-Hard. Our results demonstrate that SimPO consistently and significantly outperforms existing approaches without substantially increasing response length. Specifically, SimPO outperforms DPO by up to 6.4 points on AlpacaEval 2 and by up to 7.5 points on Arena-Hard. Our top-performing model, built on Gemma-2-9B-it, achieves a 72.4% length-controlled win rate on AlpacaEval 2, a 59.1% win rate on Arena-Hard, and ranks 1st on Chatbot Arena among <10B models with real user votes.",1.0,,Open weights (restricted use),"United States of America,United States of America",Gemma 2 9B,14248800000000000000,989500000000000*8*100*60*0.3 = 1.42488e+19,8.0,,2025-05-01 10:42,,,,,,"Academia,Academia",,,,,Open source,"License: gemma
https://huggingface.co/princeton-nlp/gemma-2-9b-it-SimPO

License: MIT
https://github.com/princeton-nlp/SimPO/tree/main","Academia,Academia",,,BF16,11021.341109849023,Hardware,princeton-nlp,,,
Ï€0 (pi-zero),"Robotics,Vision",Robotic manipulation,Physical Intelligence,"Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, Szymon Jakubczak, Tim Jones, Liyiming Ke, Sergey Levine, Adrian Li-Bell, Mohith Mothukuri, Suraj Nair, Karl Pertsch, Lucy Xiaoyang Shi, James Tanner, Quan Vuong, Anna Walling, Haohuan Wang, Ury Zhilinsky",2024-10-31,Ï€0: Our First Generalist Policy,https://www.physicalintelligence.company/download/pi0.pdf,,,Top10 recent paper from Sebastian Sartor 2025-05-14,3300000000.0,"""While in principle our model can be initialized from scratch or fine-tuned from any VLM backbone, in practice we use PaliGemma [5] as our base model. PaliGemma is an opensource 3 billion parameter VLM that offers a convenient tradeoff between size and performance. We add 300M parameters for the action expert (which is initialized from scratch) for a total of 3.3 billion parameters.""",,,"Unspecified unreleased,Open X-Embodiment","""The pre-training mixture consists of a subset of OXE [10] and the Ï€ dataset.""",,"10,000 hours, and ~1 billion timesteps

""We evaluate our approach by pre-training on over 10,000 hours of robot
data, and fine-tuning to a variety of dexterous tasks""
...

""We provide an overview of our pre-training mixture in Figure 4. Since each training example corresponds to a timestep
â€” i.e., a tuple (ot, At), â€” we will quantify data in terms
of timesteps in this discussion. 9.1% of the training mixture
consists of open-source datasets, including OXE [10], Bridge
v2 [52], and DROID [23]. The robots and tasks in these
datasets typically have one or two cameras and use lowfrequency control, between 2 and 10 Hz. However, these
datasets cover a wide range of objects and environments. To
learn dexterous and more complex tasks, we also use 903M
timesteps of data from our own datasets, where 106M steps are
from single-arm robots and 797M are from dual-arm robots.",,,,,Confident,"Robot learning holds tremendous promise to unlock the full potential of flexible, general, and dexterous robot systems, as well as to address some of the deepest questions in artificial intelligence. However, bringing robot learning to the level of generality required for effective real-world systems faces major obstacles in terms of data, generalization, and robustness. In this paper, we discuss how generalist robot policies (i.e., robot foundation models) can address these challenges, and how we can design effective generalist robot policies for complex and highly dexterous tasks. We propose a novel flow matching architecture built on top of a pre-trained vision-language model (VLM) to inherit Internet-scale semantic knowledge. We then discuss how this model can be trained on a large and diverse dataset from multiple dexterous robot platforms, including single-arm robots, dual-arm robots, and mobile manipulators. We evaluate our model in terms of its ability to perform tasks in zero shot after pre-training, follow language instructions from people and froma high-level VLM policy, and its ability to acquire new skills via fine-tuning. Our results cover a wide variety of tasks, such as laundry folding, table cleaning, and assembling box",,,Unreleased,United States of America,PaliGemma,,,,,2025-06-11 21:39,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
aiXcoder-7B Base,Language,Code autocompletion,Peking University,"Siyuan Jiang, Jia Li, He Zong, Huanyu Liu, Hao Zhu, Shukai Hu, Erlu Li, Jiazheng Ding, Yu Han, Wei Ning, Gen Wang, Yihong Dong, Kechi Zhang, Ge Li
",2024-10-30,"aiXcoder-7B: A Lightweight and Effective Large Language Model for Code Completion
",https://arxiv.org/html/2410.13187v2,1.0,,,7000000000.0,,,128 A100 40GB GPUs,"WuDao Corpora,RefinedWeb",,1200000000000.0,,,,NVIDIA A100 SXM4 40 GB,,Confident,"Compared to existing LLMs, aiXcoder-7B achieves higher code completion accuracy while having smaller scales (i.e., 7 billion parameters). We attribute the superiority of aiXcoder-7B to three key factors: â¶ Multi-objective training. We employ three training objectives, one of which is our proposed Structured Fill-In-the-Middle (SFIM). SFIM considers the syntax structures in code and effectively improves the performance of LLMs for code. â· Diverse data sampling strategies. They consider inter-file relationships and enhance the capability of LLMs in understanding cross-file contexts. â¸ Extensive high-quality data.",,,,China,,,,128.0,,2025-05-01 10:42,,,,,,Academia,,,,,,,Academia,,,,100771.03540530246,,,,,
Recraft V3,Image generation,"Image generation,Text-to-image",Recraft,,2024-10-30,Recraft introduces a revolutionary AI model that thinks in design language,https://www.recraft.ai/blog/recraft-introduces-a-revolutionary-ai-model-that-thinks-in-design-language,,,,,,,,Unspecified unreleased,,,,,,,,Unknown,"Today, we are announcing Recraft V3, our latest model that sets a new quality standard in the image generation space, outperforming all competitor models proven by the Hugging Faceâ€™s industry-leading Text-to-Image Benchmark by Artificial Analysis.

The new Recraft V3 delivers across-the-board improvements, with particularly notable advances in text generation.

We are also launching several new important features that allow our users to have more control over AI generation: the possibility to specify text size and positions in the image, precise style control, improved inpainting, and new outpainting capabilities.

The model is now available for both free and paid users in the desktop app on Canvas, in the mobile app (available on iOS and Android), and via API. ",,,API access,United Kingdom of Great Britain and Northern Ireland,,,,,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
Universal-2-TF,Speech,Speech recognition,AssemblyAI,"Core Research
Luka Chkhetiani (lead), Andrea Vanzo (lead), Yash Khare, Taufiquzzaman Peyash, Ilya Sklyar

Research Contributors
Michael Liang, Rami Botros, Ruben Bousbib

Research Data
Ahmed Etefy

Benchmarking
Pegah Ghahremani, Gabriel Oexle, Jaime Lorenzo Trueba

Research Infrastructure
William Pipsico Ferreira

Production Engineering
Ben Gotthold, Soheyl Bahadoori, Mimi Chiang, Aleksandar Mitov, Enver Fakhan

Technical Leadership
Takuya Yoshioka, Travis Kupsche

Technical Writing
Ryan Oâ€™Connor

Quality Assurance
Rajpreet Thethy (lead), Sergio Ramirez Martin",2024-10-30,Robust All-Neural Text Formatting for ASR,https://www.assemblyai.com/research/universal-2,,,,600000000.0,"1. ASR decoder: ""Our models are based on an ASR decoder architecture called the Recurrent Neural Network Transducer (RNN-T), which offers advantages in scalability, robustness against hallucinations, and timestamp accuracyâ€”key factors for real-world usage at scale. We use a 600M parameter Conformer RNN-T model.""

2. Text Formatting module.",,,Unspecified unreleased,,,"1. ASR decoder:: ""For Universal-2, we doubled the size of the supervised training dataset from 150,000 hours to 300,000 hours.""
""we first pre-trained an RNN-T encoder using 12.5 million hours of diverse, multilingual audio. After the pre-training, the encoder was combined with a randomly initialized decoder, and the entire model was fine-tuned using a combination of the supervised dataset described above and a pseudo-labeled dataset, similar to the approach used in Universal-1.""

2. Text formatting: ""Universal-2â€™s Text Formatting module has been trained using a mix of open-source and in-house data, focusing on English texts"" 
""Our multi-objective tagging model has been trained for 100k steps with a batch size of 256 over 23 million training samples, totalling approximately 5.2 billion words of which 72% are publicly available data, while 28% have been sourced in-house. Each training sequence averages 230 words.""

3. Seq2seq: ""The seq2seq model was trained for more than 500k steps using a batch size of 512 on heterogeneous data. The training dataset contains 8.9 billion words with 62% of the data sourced in-house, 22% synthetic, while 16% are publicly available data. Each training sequence averages 157 words.""",,,,,Unknown,"Universal-2-TF introduces a two-stage neural text formatting model for ASR that combines token classification and sequence-to-sequence approaches to efficiently handle punctuation, capitalization, and text normalization while achieving superior accuracy across diverse domains.",,,API access,,Conformer,,,,,2025-05-12 15:06,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
Pro-PRIME,Biology,Protein design,"Shanghai Jiao Tong University,Shanghai AI Lab,East China University of Science and Technology,Shanghai Tech University,Guangzhou Inernational Bio Island,Chinese Academy of Sciences,Shanghai Academy of Experimental Medicine","Fan Jiang, Mingchen Li, Jiajun Dong, Yuanxi Yu, Xinyu Sun, Banghao Wu, Jin Huang, Liqi Kang, Yufeng Pei, Liang Zhang, Shaojie Wang, Wenxue Xu, Jingyao Xin, Wanli Ouyang, Guisheng Fan, Lirong Zheng, Yang Tan, Zhiqiang Hu, Yi Xiong, Yan Feng, Guangyu Yang, Qian Liu, Jie Song, Jia Liu, Liang Hong, Pan Tan",2024-10-28,Pro-PRIME: A general Temperature-Guided Language model to engineer enhanced Stability and Activity in Proteins,https://arxiv.org/abs/2307.12682,1.0,,,650000000.0,,8.18e+20,"Tokens per step; 4096*8*32=1048576
Total training tokens 1048576*200000=209715200000
Epochs: 209715200000 / 28800000001 = 7.28
FLOP: 6*650000000*209715200000=8.1788928e+20",,,28800000001.0,"96,000,000 sequences Ã— 300 residues/sequence = 28,800,000,000 tokens (2.88 Ã— 10Â¹â°)",,,,,Confident,"Designing protein mutants of both high stability and activity is a critical yet challenging task in protein engineering. Here, we introduce PRIME, a deep learning model, which can suggest protein mutants of improved stability and activity without any prior experimental mutagenesis data of the specified protein. Leveraging temperature-aware language modeling, PRIME demonstrated superior predictive power compared to current state-of-the-art models on the public mutagenesis dataset over 283 protein assays. Furthermore, we validated PRIME's predictions on five proteins, examining the top 30-45 single-site mutations' impact on various protein properties, including thermal stability, antigen-antibody binding affinity, and the ability to polymerize non-natural nucleic acid or resilience to extreme alkaline conditions. Remarkably, over 30% of the AI-recommended mutants exhibited superior performance compared to their pre-mutation counterparts across all proteins and desired properties. Moreover, we have developed an efficient, and successful method based on PRIME to rapidly obtain multi-site mutants with enhanced activity and stability. Hence, PRIME demonstrates the general applicability in protein engineering.",7.0,,,"China,China,China,China,China,China,China",,,,,,2025-05-01 10:42,,,,,,"Academia,Academia,Academia,Academia,Academia",,,,,,,"Academia,Academia,Academia,Academia,Academia",,,,,Hardware,,,,
Doubao-pro,Language,"Language modeling/generation,Question answering,Text summarization,Text classification",ByteDance,,2024-10-28,Doubao General Model Pro (Doubao-pro),https://www.volcengine.com/docs/6360/1264663,,Training cost,,500000000000.0,"[Speculative] Doubao's large language model has scaled up from 35 billion parameters to 800 billion, with 500 billion and 800 billion parameter models currently under training.
https://xueqiu.com/9637001584/309910396?md5__1038=7qmx2DyDuie4cDBqDTQEWqDtMvO4iTphD
",2.505e+25,6ND = 6 * 500*10^9 * 8350*10^9 = 2.505e+25,Unspecified unreleased,"Doubao's data sources primarily rely on proprietary business data, accounting for 50-60%; externally sourced data comprises 15-20%; and synthetic data has been used since June of this year, although Doubao is cautious in feeding synthetic data due to its uncertain quality. ",8350000000000.0,"[Speculative] Doubao's pre-training data volume is approximately 500TB, with only about 10% of this data actually used for training. The current version employs a non-Mixture-of-Experts (MoE) architecture. In the future, MoE architecture may be introduced to increase parameter count and performance, while also integrating multimodal data solutions.

So this model is dense, and the training data is probably all text tokens, not multimodal.

50TB * 167M tokens/GB ~= 8.35 trillion tokens
",,,,,Speculative,"A professional-grade, self-developed LLM supporting up to 128k tokens, enabling fine-tuning across the entire series. ",,,API access,China,,,,,,2025-05-02 12:03,,"There is no paper to reference, also no information in media.",,,,Industry,,,,,Unreleased,,Industry,,,,,Operation counting,,,,7
Aya Expanse 32B,Language,"Language modeling/generation,Translation",Cohere for AI,,2024-10-24,"Cohere For AI launches Aya Expanse, a state-of-the-art multilingual family of models to help close the language gap with AI.","https://cohere.com/blog/aya-expanse-connecting-our-world

https://huggingface.co/CohereForAI/aya-expanse-32b?ref=cohere-ai.ghost.io",,,,32300000000.0,,6.688684e+21,"'Likely' confidence for dataset size and epochs

34513333333*32300000000*6 = 6.688684e+21",Aya Collection,"Seems like it wsa trained on Aya Collection (155 GB of text) but I don't find information about amount of epochs (could be less than 1)

https://huggingface.co/datasets/CohereForAI/aya_collection?ref=cohere-ai.ghost.io",34513333333.0,155GB * 167M words per GB (assuming on average across languages) * 4/3 tokens per word on average = 34513333333 tokens,,,,,Likely,"Today, Cohere For AI, Cohereâ€™s research arm, is proud to announce Aya Expanse, a family of highly performant multilingual models that excels across 23 languages and outperforms other leading open-weights models. 

We are releasing Aya Expanse as both 8 and 32 billion open-weights models available on Kaggle and Hugging Face, as part of our continued commitment to multilingual research and to accelerate the frontier for multilingual AI. The 8 billion parameters model makes breakthroughs more accessible to researchers worldwide, and our 32 billion parameters model offers state-of-the-art multilingual capabilities. 

Aya Expanse marks an important step to expand high-quality coverage of languages in LLMs. Since we first launched the Aya initiative two years ago, we have collaborated with over 3,000 researchers from 119 countries to expand cutting-edge multilingual research. This included releasing the Aya collection, the largest multilingual dataset collection to-date, with 513 million examples, and critical evaluation sets for multilingual performance and safety. It has also included the release of Aya-101, the most comprehensive multilingual model to-date covering 101 languages.",,,Open weights (restricted use),"Multinational,Canada",,,,,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,"CC-BY-NC, requires also adhering to C4AI's Acceptable Use Policy",Industry,,,,,Operation counting,,,,
Aya Expanse 8B,Language,"Language modeling/generation,Translation",Cohere for AI,,2024-10-24,"Cohere For AI launches Aya Expanse, a state-of-the-art multilingual family of models to help close the language gap with AI.","https://cohere.com/blog/aya-expanse-connecting-our-world

https://huggingface.co/CohereForAI/aya-expanse-8b?ref=cohere-ai.ghost.io",,,,8000000000.0,,1.65664e+21,"'Likely' confidence for dataset size and epochs

34513333333*8000000000*6 = 1.65664e+21",Aya Collection,"Seems like it wsa trained on Aya Collection (155 GB of text) but I don't find information about amount of epochs (could be less than 1)

https://huggingface.co/datasets/CohereForAI/aya_collection?ref=cohere-ai.ghost.io",34513333333.0,155GB * 167M words per GB (assuming on average across languages) * 4/3 tokens per word on average = 34513333333 tokens,,,,,Likely,"Today, Cohere For AI, Cohereâ€™s research arm, is proud to announce Aya Expanse, a family of highly performant multilingual models that excels across 23 languages and outperforms other leading open-weights models. 

We are releasing Aya Expanse as both 8 and 32 billion open-weights models available on Kaggle and Hugging Face, as part of our continued commitment to multilingual research and to accelerate the frontier for multilingual AI. The 8 billion parameters model makes breakthroughs more accessible to researchers worldwide, and our 32 billion parameters model offers state-of-the-art multilingual capabilities. 

Aya Expanse marks an important step to expand high-quality coverage of languages in LLMs. Since we first launched the Aya initiative two years ago, we have collaborated with over 3,000 researchers from 119 countries to expand cutting-edge multilingual research. This included releasing the Aya collection, the largest multilingual dataset collection to-date, with 513 million examples, and critical evaluation sets for multilingual performance and safety. It has also included the release of Aya-101, the most comprehensive multilingual model to-date covering 101 languages.",,,Open weights (restricted use),"Multinational,Canada",,,,,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,"CC-BY-NC, requires also adhering to C4AI's Acceptable Use Policy",Industry,,,,,Operation counting,,,,
YOLOv11,Vision,Object detection,Huddersfield University,"Rahima Khanam, Muhammad Hussain",2024-10-23,YOLOv11: An Overview of the Key Architectural Enhancements,https://arxiv.org/html/2410.17725v1,253.0,,,,,,,,,,,,,,,Unknown,"This study presents an architectural analysis of YOLOv11, the latest iteration in the YOLO (You Only Look Once) series of object detection models. We examine the models architectural innovations, including the introduction of the C3k2 (Cross Stage Partial with kernel size 2) block, SPPF (Spatial Pyramid Pooling - Fast), and C2PSA (Convolutional block with Parallel Spatial Attention) components, which contribute in improving the models performance in several ways such as enhanced feature extraction. The paper explores YOLOv11â€™s expanded capabilities across various computer vision tasks, including object detection, instance segmentation, pose estimation, and oriented object detection (OBB). We review the modelâ€™s performance improvements in terms of mean Average Precision (mAP) and computational efficiency compared to its predecessors, with a focus on the trade-off between parameter count and accuracy. Additionally, the study discusses YOLOv11â€™s versatility across different model sizes, from nano to extra-large, catering to diverse application needs from edge devices to high-performance computing environments. Our research provides insights into YOLOv11â€™s position within the broader landscape of object detection and its potential impact on real-time computer vision applications.",,,,United Kingdom of Great Britain and Northern Ireland,,,,,,2025-05-01 10:42,,,,,,Academia,,,,,,,Academia,,,,,,,,,
NVLM-D 72B,"Vision,Language","Language modeling/generation,Vision-language generation,Question answering,Code generation,Translation,Quantitative reasoning",NVIDIA,"Wenliang Dai, Nayeon Lee, Boxin Wang, Zhuolin Yang, Zihan Liu, Jon Barker, Tuomas Rintamaki, Mohammad Shoeybi, Bryan Catanzaro, Wei Ping",2024-10-22,NVLM: Open Frontier-Class Multimodal LLMs,https://arxiv.org/abs/2409.11402,,SOTA improvement,SOTA on OCRBench and VQAv2,72000000000.0,72B,3.02e+24,"Uses Qwen2-72B as a backbone, which trained with 3.02e24 FLOP, as well as InternViT-6B. It's unclear how many FLOP were spent training but probably negligible; e.g. PaLI trained ViT-e with ~4B parameters using 1.07e23 FLOP.

Fine-tuning FLOPs:
57,016,320,000 image/text tokens over all stages
6 * 72B * 57,016,320,000 = 2.463e22
","COCO,Conceptual Captions (CC3M),SBU,VQAv2,VisualGenome,TextVQA,OCR-VQA","Captioning COCO [72], CC3M [127], SBU [114], LAION-115M (sanitized) [123; 66]
VQA (natural image) VQAv2 [38], Visual Genome [59]
Chart DVQA [51]
Document Docmatix [90]
OCR /
Scene-Text
OCR-VQA [98], COCO-Text [144], TextOCR [132], ReCTs [170], RRC-ArT [22], RRC-LSVT [134]
RCTW [128], synthdog-en [57], pdfa-eng-wds [117]
Math CLEVR-Math [73]",57016320000.0,"Pre-training
Global batch size 2,048
Sequence length in the LLM decoder 512
Downsampling of visual tokens 1024->256
# of visual token per tile 256
# of tiles 1
# of training steps 20K

2048 * (512 + 256 * 1) * 20000 = 31,457,280,000

SFT:
Global batch size 128
Sequence length in the LLM decoder 3,200
# of visual token per tile 256
# of tiles 6+1
# of training steps 40K

128 * (3200 + 256*7) * 40000 = 25,559,040,000

31,457,280,000 + 25,559,040,000 = 57,016,320,000",,,NVIDIA H100 SXM5 80GB,,Confident,"We introduce NVLM 1.0, a family of frontier-class multimodal large language models (LLMs) that achieve state-of-the-art results on vision-language tasks, rivaling the leading proprietary models (e.g., GPT-4o) and open-access models (e.g., Llama 3-V 405B and InternVL 2). Remarkably, NVLM 1.0 shows improved text-only performance over its LLM backbone after multimodal training. In terms of model design, we perform a comprehensive comparison between decoder-only multimodal LLMs (e.g., LLaVA) and cross-attention-based models (e.g., Flamingo). Based on the strengths and weaknesses of both approaches, we propose a novel architecture that enhances both training efficiency and multimodal reasoning capabilities. Furthermore, we introduce a 1-D tile-tagging design for tile-based dynamic high-resolution images, which significantly boosts performance on multimodal reasoning and OCR-related tasks. Regarding training data, we meticulously curate and provide detailed information on our multimodal pretraining and supervised fine-tuning datasets. Our findings indicate that dataset quality and task diversity are more important than scale, even during the pretraining phase, across all architectures. Notably, we develop production-grade multimodality for the NVLM-1.0 models, enabling them to excel in vision-language tasks while maintaining and even improving text-only performance compared to their LLM backbones. To achieve this, we craft and integrate a high-quality text-only dataset into multimodal training, alongside a substantial amount of multimodal math and reasoning data, leading to enhanced math and coding capabilities across modalities. To advance research in the field, we release the model weights at this https URL and will open-source the training code for the community soon.",1.0,,Open weights (non-commercial),United States of America,"Qwen2-72B,InternViT-6B",2,"Fine-tuning FLOPs:
57,016,320,000 image/text tokens over all stages
6 * 72B * 57,016,320,000 = 2.463e22",128.0,,2025-05-26 17:50,,,,,,Industry,,,5.000010000000001e+24,,Open source,"https://huggingface.co/nvidia/NVLM-D-72B
Creative Commons Attribution: Non-Commercial 4.0 International

https://github.com/NVIDIA/Megatron-LM/tree/NVLM-1.0/examples/multimodal/nvlm
license for code seems to be Apache 2.0",Industry,,,"FP32,BF16",176380.73226445544,Operation counting,nvidia,,,
NVLM-H 72B,"Vision,Language","Language modeling/generation,Vision-language generation,Question answering,Code generation,Translation,Quantitative reasoning",NVIDIA,"Wenliang Dai, Nayeon Lee, Boxin Wang, Zhuolin Yang, Zihan Liu, Jon Barker, Tuomas Rintamaki, Mohammad Shoeybi, Bryan Catanzaro, Wei Ping",2024-10-22,NVLM: Open Frontier-Class Multimodal LLMs,https://arxiv.org/abs/2409.11402,,Training cost,,72000000000.0,72B,3.02e+24,Additional compute in this paper is negligible relative to the compute used to train the language model backbone (Qwen2-72B at 3.02e24 FLOP),"COCO,Conceptual Captions (CC3M),SBU,VQAv2,VisualGenome,TextVQA,OCR-VQA","Captioning COCO [72], CC3M [127], SBU [114], LAION-115M (sanitized) [123; 66]
VQA (natural image) VQAv2 [38], Visual Genome [59]
Chart DVQA [51]
Document Docmatix [90]
OCR /
Scene-Text
OCR-VQA [98], COCO-Text [144], TextOCR [132], ReCTs [170], RRC-ArT [22], RRC-LSVT [134]
RCTW [128], synthdog-en [57], pdfa-eng-wds [117]
Math CLEVR-Math [73]",125829120000.0,"Pre-training:
Global batch size 2,048
Sequence length in the LLM decoder 512
Downsampling of visual tokens 1024->256
# of visual token per tile 256
# of tiles 6+1
# of training steps 20K

2048 * (512+256*7) * 20000 = 94,371,840,000

SFT:
Global batch size  256
Sequence length in the LLM decoder  1,280
# of visual token per tile 256
# of tiles 6+1
# of training steps 40K

256*(1280+256*7)*40000 = 31,457,280,000

94,371,840,000 + 31,457,280,000 = 125,829,120,000",,,NVIDIA H100 SXM5 80GB,,Likely,"We introduce NVLM 1.0, a family of frontier-class multimodal large language models (LLMs) that achieve state-of-the-art results on vision-language tasks, rivaling the leading proprietary models (e.g., GPT-4o) and open-access models (e.g., Llama 3-V 405B and InternVL 2). Remarkably, NVLM 1.0 shows improved text-only performance over its LLM backbone after multimodal training. In terms of model design, we perform a comprehensive comparison between decoder-only multimodal LLMs (e.g., LLaVA) and cross-attention-based models (e.g., Flamingo). Based on the strengths and weaknesses of both approaches, we propose a novel architecture that enhances both training efficiency and multimodal reasoning capabilities. Furthermore, we introduce a 1-D tile-tagging design for tile-based dynamic high-resolution images, which significantly boosts performance on multimodal reasoning and OCR-related tasks. Regarding training data, we meticulously curate and provide detailed information on our multimodal pretraining and supervised fine-tuning datasets. Our findings indicate that dataset quality and task diversity are more important than scale, even during the pretraining phase, across all architectures. Notably, we develop production-grade multimodality for the NVLM-1.0 models, enabling them to excel in vision-language tasks while maintaining and even improving text-only performance compared to their LLM backbones. To achieve this, we craft and integrate a high-quality text-only dataset into multimodal training, alongside a substantial amount of multimodal math and reasoning data, leading to enhanced math and coding capabilities across modalities. To advance research in the field, we release the model weights at this https URL and will open-source the training code for the community soon.",1.0,,Unreleased,United States of America,"Qwen2-72B,InternViT-6B",5,"6ND = 6*125,829,120,000*72000000000.00 = 5.436e22
",128.0,,2025-05-28 15:56,,,,,,Industry,,,5.000010000000001e+24,,Unreleased,,Industry,,,BF16,176380.73226445544,Operation counting,,,,
NVLM-X 72B,"Vision,Language","Language modeling/generation,Vision-language generation,Question answering,Code generation,Translation,Quantitative reasoning",NVIDIA,"Wenliang Dai, Nayeon Lee, Boxin Wang, Zhuolin Yang, Zihan Liu, Jon Barker, Tuomas Rintamaki, Mohammad Shoeybi, Bryan Catanzaro, Wei Ping",2024-10-22,NVLM: Open Frontier-Class Multimodal LLMs,https://arxiv.org/abs/2409.11402,,Training cost,,72000000000.0,72B,3.0398181e+24,3.02e24 FLOP (Qwen2-72B compute) + 19818086000000000000000 = 3.0398181e+24,"COCO,Conceptual Captions (CC3M),SBU,VQAv2,VisualGenome,TextVQA,OCR-VQA","Captioning COCO [72], CC3M [127], SBU [114], LAION-115M (sanitized) [123; 66]
VQA (natural image) VQAv2 [38], Visual Genome [59]
Chart DVQA [51]
Document Docmatix [90]
OCR /
Scene-Text
OCR-VQA [98], COCO-Text [144], TextOCR [132], ReCTs [170], RRC-ArT [22], RRC-LSVT [134]
RCTW [128], synthdog-en [57], pdfa-eng-wds [117]
Math CLEVR-Math [73]",45875200000.0,"Pre-training
Global batch size 2,048
Sequence length in the LLM decoder 512
Downsampling of visual tokens 1024->256
# of visual token per tile 256
# of tiles 1
# of training steps 20K

2048 * (512 + 256 * 1) * 20000 = 31,457,280,000

SFT:
Global batch size 256
Sequence length in the LLM decoder  1,024
# of visual token per tile 256
# of tiles 6+1
# of training steps 20K

256 * (1,024 + 256*7) * 20000 = 14417920000

31,457,280,000 +14417920000 = 45875200000",,,NVIDIA H100 SXM5 80GB,,Likely,"We introduce NVLM 1.0, a family of frontier-class multimodal large language models (LLMs) that achieve state-of-the-art results on vision-language tasks, rivaling the leading proprietary models (e.g., GPT-4o) and open-access models (e.g., Llama 3-V 405B and InternVL 2). Remarkably, NVLM 1.0 shows improved text-only performance over its LLM backbone after multimodal training. In terms of model design, we perform a comprehensive comparison between decoder-only multimodal LLMs (e.g., LLaVA) and cross-attention-based models (e.g., Flamingo). Based on the strengths and weaknesses of both approaches, we propose a novel architecture that enhances both training efficiency and multimodal reasoning capabilities. Furthermore, we introduce a 1-D tile-tagging design for tile-based dynamic high-resolution images, which significantly boosts performance on multimodal reasoning and OCR-related tasks. Regarding training data, we meticulously curate and provide detailed information on our multimodal pretraining and supervised fine-tuning datasets. Our findings indicate that dataset quality and task diversity are more important than scale, even during the pretraining phase, across all architectures. Notably, we develop production-grade multimodality for the NVLM-1.0 models, enabling them to excel in vision-language tasks while maintaining and even improving text-only performance compared to their LLM backbones. To achieve this, we craft and integrate a high-quality text-only dataset into multimodal training, alongside a substantial amount of multimodal math and reasoning data, leading to enhanced math and coding capabilities across modalities. To advance research in the field, we release the model weights at this https URL and will open-source the training code for the community soon.",1.0,,Unreleased,United States of America,"Qwen2-72B,InternViT-6B",1,6*72B*45875200000 = 1.9818086e+22,128.0,,2025-05-28 15:56,,,,,,Industry,,,5.000010000000001e+24,,Unreleased,,Industry,,,BF16,176380.73226445544,Operation counting,,,,
Stable Diffusion 3.5 Large,Image generation,"Image generation,Text-to-image",Stability AI,,2024-10-22,Introducing Stable Diffusion 3.5,https://stability.ai/news/introducing-stable-diffusion-3-5,,,,8100000000.0,8.1B,,,Unspecified unreleased,"""This model was trained on a wide variety of data, including synthetic data and filtered publicly available data.""",,,,,,,Confident,"Stable Diffusion 3.5 Large: At 8.1 billion parameters, with superior quality and prompt adherence, this base model is the most powerful in the Stable Diffusion family. This model is ideal for professional use cases at 1 megapixel resolution.",,,Open weights (restricted use),United Kingdom of Great Britain and Northern Ireland,,,,,,2025-05-30 14:39,,,,,,Industry,,,,,Unreleased,"Stability AI Community License: Free for research, non-commercial, and commercial use for organisations or individuals with less than $1M annual revenue. You only need a paid Enterprise license if your yearly revenues exceed USD$1M and you use Stability AI models in commercial products or services.

https://huggingface.co/stabilityai/stable-diffusion-3.5-large/blob/main/LICENSE.md",Industry,,,,,,stabilityai,,,
Stable Diffusion 3.5 Large Turbo,Image generation,"Image generation,Text-to-image",Stability AI,,2024-10-22,Introducing Stable Diffusion 3.5,https://stability.ai/news/introducing-stable-diffusion-3-5,,,,,,,,Unspecified unreleased,"""This model was trained on a wide variety of data, including synthetic data and filtered publicly available data.""",,,,,,,Unknown,"Stable Diffusion 3.5 Large Turbo: A distilled version of Stable Diffusion 3.5 Large generates high-quality images with exceptional prompt adherence in just 4 steps, making it considerably faster than Stable Diffusion 3.5 Large.",,,Open weights (restricted use),United Kingdom of Great Britain and Northern Ireland,,,,,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,"Stability AI Community License: Free for research, non-commercial, and commercial use for organisations or individuals with less than $1M annual revenue. You only need a paid Enterprise license if your yearly revenues exceed USD$1M and you use Stability AI models in commercial products or services.

https://huggingface.co/stabilityai/stable-diffusion-3.5-large/blob/main/LICENSE.md",Industry,,,,,,stabilityai,,,
Granite 3.0 8B,Language,"Language modeling/generation,Question answering,Translation,Text summarization,Text classification,Code generation",IBM,Granite Team IBM,2024-10-21,Granite 3.0 Language Models,https://github.com/ibm-granite/granite-3.0-language-models/tree/main,,,,8100000000.0,8.1B,5.832e+23,"6ND = 6 FLOP / parameter / token * 8.1*10^9 parameters * 12*10^12 tokens = 5.832e+23 FLOP

""All our Granite 3.0 models are trained using a
compute budget of 8.35 Ã— 10^23 FLOPS.""

8.35 Ã— 10^23 * 757.0 (model's power consumption) / (174.6+757.0+64.5+121.2) = 5.6573436e+23

hardware estimation:
832102 GPU-hours * 3600 sec / hour * 989500000000000 FLOP / GPU / sec * 0.3 [assumed utilization] = 8.8923412e+23",Unspecified unreleased,"Granite 3.0 language models are trained using data from various sources such as unstructured natural language text and code data from theWeb curated by IBM, a collection of synthetic datasets generated by IBM, and publicly available high-quality datasets with permissible licenses.",12000000000000.0,12T tokens,,Table 7,NVIDIA H100 SXM5 80GB,,Confident,"This report presents Granite 3.0, a new set of lightweight, state-of-the-art, open foundation models ranging in scale from 400 million to 8 billion active parameters.
Equipped with native support of multilingual, coding, function 
calling, and strong safety performance, these models target enterprise use cases, including on-premise and on-device settings. Evaluations on a comprehensive set of tasks demonstrate that our models consistently reach state-of-the-art performance for their size (as shown in Figure 1 and 2). This report also discloses technical details of pre-training and post-training that may help the research community accelerate the collective efforts to develop open foundation models. We publicly release pre-trained and post-trained versions of all our Granite 3.0 models under a standard permissive Apache 2.0 license allowing both research and commercial use. With support from the open source community, the Granite 3.0 models have been integrated with a
range of existing tools for quantization, fine-tuning, and deployment.",,,Open weights (unrestricted),United States of America,,,,256.0,,2025-05-16 10:30,,,,,,Industry,,,,832102.0,Unreleased,"Apache 2.0 license
https://huggingface.co/ibm-granite/granite-3.0-8b-instruct",Industry,,,,352769.3203924083,"Operation counting,Hardware",ibm-granite,,,
Granite 3.0 2B,Language,"Language modeling/generation,Question answering,Translation,Text summarization,Text classification,Code generation",IBM,Granite Team IBM,2024-10-21,Granite 3.0 Language Models,https://github.com/ibm-granite/granite-3.0-language-models/tree/main,,,,2500000000.0,2.5B,1.8e+23,"6ND = 6 FLOP / token / parameter * 2.5*10^9 parameters * 12*10^12 tokens = 1.8e+23 FLOP

""""All our Granite 3.0 models are trained using a
compute budget of 8.35 Ã— 10^23 FLOPS.""

8.35 Ã— 10^23 * 174.6 (model's power consumption) / (174.6+757.0+64.5+121.2) =1.304851e+23

hardware estimation:
192030 GPU-hours * 3600 sec / hour *989500000000000 FLOP / GPU / sec * 0.3 [assumed utilization] = 2.0521478e+23 FLOP",Unspecified unreleased,"Granite 3.0 language models are trained using data from various sources such as unstructured natural language text and code data from theWeb curated by IBM, a collection of synthetic datasets generated by IBM, and publicly available high-quality datasets with permissible licenses.",12000000000000.0,12T tokens,,Table 7,NVIDIA H100 SXM5 80GB,,Confident,"This report presents Granite 3.0, a new set of lightweight, state-of-the-art, open foundation models ranging in scale from 400 million to 8 billion active parameters.
Equipped with native support of multilingual, coding, function 
calling, and strong safety performance, these models target enterprise use cases, including on-premise and on-device settings. Evaluations on a comprehensive set of tasks demonstrate that our models consistently reach state-of-the-art performance for their size (as shown in Figure 1 and 2). This report also discloses technical details of pre-training and post-training that may help the research community accelerate the collective efforts to develop open foundation models. We publicly release pre-trained and post-trained versions of all our Granite 3.0 models under a standard permissive Apache 2.0 license allowing both research and commercial use. With support from the open source community, the Granite 3.0 models have been integrated with a
range of existing tools for quantization, fine-tuning, and deployment.",,,Open weights (unrestricted),United States of America,,,,768.0,,2025-05-16 10:30,,,,,,Industry,,,,192030.0,Unreleased,"Apache 2.0 license
https://huggingface.co/ibm-granite/granite-3.0-2b-instruct",Industry,,,,1058307.961177225,"Operation counting,Hardware",ibm-granite,,,
Emu3,"Video,Multimodal,Image generation,Vision","Video generation,Text-to-video,Image-to-video,Video-to-video,Image generation,Text-to-image,Visual question answering,Language modeling/generation,Question answering",Beijing Academy of Artificial Intelligence / BAAI,"Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, Yingli Zhao, Yulong Ao, Xuebin Min, Tao Li, Boya Wu, Bo Zhao, Bowen Zhang, Liangdong Wang, Guang Liu, Zheqi He, Xi Yang, Jingjing Liu, Yonghua Lin, Tiejun Huang, Zhongyuan Wang",2024-10-21,"Emu3: Next-Token Prediction is All You Need
",https://arxiv.org/abs/2409.18869,12.0,,,8000000000.0,,,,,"""Language Data. We use the same language data as in Aquila [101], which is a high-quality corpus consisting of both Chinese and English data.""

""Image Data. We curate a large-scale image-text dataset comprising open-source web data, AIgenerated data, and high-quality in-house data""

""Video Data. We collect videos covering a wide range of categories, such as landscapes, animals, plants, games, and actions.""",,,,,,,Confident,"In this paper, we introduce Emu3, a new suite of state-of-the-art multimodal models trained solely with next-token prediction. By tokenizing images, text, and videos into a discrete space, we train a single transformer from scratch on a mixture of multimodal sequences. Emu3 outperforms several well-established task-specific models in both generation and perception tasks, surpassing flagship models such as SDXL and LLaVA-1.6, while eliminating the need for diffusion or compositional architectures. Emu3 is also capable of generating high-fidelity video via predicting the next token in a video sequence.",,,Open weights (unrestricted),China,,,,,,2025-06-12 13:18,,,,,,Academia,,,,,Unreleased,"Apache 2.0
https://huggingface.co/BAAI/Emu3-Gen

Apache 2.0 for inference code and SFT training
https://github.com/baaivision/Emu3

pre-training code release is still in the to-do liest as of June 2025",Academia,,,,,,BAAI,,,
Allegro,Video,"Video generation,Text-to-video",Rhymes AI,"Yuan Zhou, Qiuyue Wang, Yuxuan Cai, Huan Yang",2024-10-20,Allegro: Open the Black Box of Commercial-Level Video Generation Model,https://arxiv.org/abs/2410.15458,,,,2975000000.0,"VAE: 175M
DiT: 2.8B",6.565847e+22,"Assuming (!) 10 days of training (we might make a better assumption if we know what is the average throughput steps/sec for such setups because steps are given):
989500000000000*10*24*3600*256*0.3 = 6.565847e+22","WebVid-10M,HD-VILA-100M,Panda-70M,HD-VG,OpenVid-1M","WebVid [Bain et al., 2021], Panda-70M [Chen et al., 2024b], HD-VILA [Xue et al., 2022], HD-VG [Wang et al., 2023] and OpenVid-1M [Nan et al., 2024]",,"Text-to-Image Pre-training
Wâ‰¥ 640, Hâ‰¥ 368
107M datapoints

Text-to-Video Pre-training-360p
Duration âˆ’ [2s, 16s]
FPS âˆ’ (23, 61)
Wâ‰¥ 640, Hâ‰¥ 368
48M datapoints

Text-to-Video Pre-training-720p
Duration âˆ’ [2s, 16s], [6s, 16s
FPS âˆ’ (23, 61)
Wâ‰¥ 1280, Hâ‰¥ 720 
18M datapoints

Text-to-Video Fine-tuning
Duration âˆ’  [6s, 16s]
FPS âˆ’ (23, 61)
Wâ‰¥ 1280, Hâ‰¥ 720 
2M datapoints",,"GPUs 
T2I Pre-train: 128Ã—H100 
T2V Pre-train: 256Ã—H100 
T2V Fine-tune: 256Ã—H100",NVIDIA H100 SXM5 80GB,,Speculative,"Significant advancements have been made in the field of video generation, with the open-source community contributing a wealth of research papers and tools for training high-quality models. However, despite these efforts, the available information and resources remain insufficient for achieving commercial-level performance. In this report, we open the black box and introduce Allegro, an advanced video generation model that excels in both quality and temporal consistency. We also highlight the current limitations in the field and present a comprehensive methodology for training high-performance, commercial-level video generation models, addressing key aspects such as data, model architecture, training pipeline, and evaluation. Our user study shows that Allegro surpasses existing open-source models and most commercial models, ranking just behind Hailuo and Kling. Code: this https URL , Model: this https URL , Gallery: this https URL .",,,Open weights (unrestricted),United States of America,,,,256.0,,2025-05-19 12:29,,,,,,Industry,,,,,Unreleased,"Apache 2
https://huggingface.co/rhymes-ai/Allegro

the code seems to be just inference code
https://github.com/rhymes-ai/Allegro
(apache 2)",Industry,,,,352777.1764308529,Hardware,,,,
Yi-Lightning,Language,Language modeling/generation,01.AI,,2024-10-18,Yi-Lightning,https://www.lingyiwanwu.com/en https://platform.lingyiwanwu.com/,,Training cost,"On the blind test list LMSYS, Yi-Lightning surpassed GPT-4o-2024-05-13 released by OpenAI and Anthropic, as well as Claude 3.5 Sonnet, ranking sixth in the world and first in China.",,,1.5e+24,"The CEO of 01.AI tweeted that Yi-Lightning was trained for 1 month on 2000 H100s: https://x.com/kaifulee/status/1846310645849047524
Assuming this is accurate:
(9.9e14 * 2000) FLOP/s * 1 month * 30.5 days/month * 24hr/day * 3600 s/hr * 0.3 utilization assumption = 1.565e24",Unspecified unreleased,,,,720.0,"https://x.com/kaifulee/status/1846310645849047524
""it was trained on 2000 H100s for 1 month""",NVIDIA H100 SXM5 80GB,,Confident,,,,API access,China,,,,2000.0,,2025-05-16 10:30,,Yi-Lightning training cluster,,,,Industry,,,,,Unreleased,https://platform.lingyiwanwu.com/,Industry,,,,2756194.445567179,Hardware,,,,
Janus 1.3B,"Language,Vision,Multimodal","Language modeling/generation,Question answering,Visual question answering","DeepSeek,The University of Hong Kong,Peking University","Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, Ping Luo",2024-10-17,Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation,https://arxiv.org/abs/2410.13848,,,,1300000000.0,"1.3B
! In the paper they say that they ""utilize DeepSeek-LLM (1.3B) [5] with a maximum supported sequence length of 4096 as the base language model.""
but I haven't found any mentioning of 1.3B model in that Deepseek paper (only 7B and 67B)",,,"ShareGPT4V,ImageNet-1k,wikiHow,Unspecified unreleased","Stage I. We use a dataset that includes 1.25 million image-text paired captions from ShareGPT4V [10] for multimodal understanding and approximately 1.2 million samples from ImageNet-1k [18] for visual generation

Stage II. We organize the data into the following categories. 
(1) Text-only data. We use pretraining text copus from DeepSeek-LLM [5]. 
(2) Interleaved image-text data. We use WikiHow [39] and WIT [72] dataset. (3) Image caption data. We use images from [17, 18, 23, 38, 40, 45, 47, 49, 70]. Among them, we employ open-source multimodal model to re-caption
images in [17, 40]. 
(4) Table and chart data. We use corresponding table and chart data from DeepSeek-VL [55]. 
(5) Visual generation data. We utilize image-caption pairs from
various datasets including [17, 38, 40, 57, 58, 60, 63, 70], along with 2M in-house data. 

Stage III. For text understanding, we use data from [43]. For multimodal understanding, we use instruct tuning data from [31, 34, 35, 43, 56, 69]. For visual generation, we use image-text pairs from [17, 60, 70] (a subset of that in stage II) and 4M in-house data",,"""All images are resized to 384 Ã— 384 pixels.""

Stage 1
Training steps 10, 000
Batch size 256

Stage 2
Training steps 180, 000
Batch size 512

Stage 3
Training steps 24, 000
Batch size 256

Since they are using  SigLIP-Large-Patch16-384, patching size of 16*16
""For image generation, Janus uses the tokenizer from here with a downsample rate of 16.""

It is not enough information about text tokens and split between text and image tokens to calculate the total count 
",168.0,"""The whole training process took 7 days on a cluster of 16 nodes, each equipped with 8 Nvidia A100 (40GB) GPUs.""",NVIDIA A100 SXM4 40 GB,,Confident,"In this paper, we introduce Janus, an autoregressive framework that unifies multimodal understanding and generation. Prior research often relies on a single visual encoder for both tasks, such as Chameleon. However, due to the differing levels of information granularity required by multimodal understanding and generation, this approach can lead to suboptimal performance, particularly in multimodal understanding. To address this issue, we decouple visual encoding into separate pathways, while still leveraging a single, unified transformer architecture for processing. The decoupling not only alleviates the conflict between the visual encoder's roles in understanding and generation, but also enhances the framework's flexibility. For instance, both the multimodal understanding and generation components can independently select their most suitable encoding methods. Experiments show that Janus surpasses previous unified model and matches or exceeds the performance of task-specific models. The simplicity, high flexibility, and effectiveness of Janus make it a strong candidate for next-generation unified multimodal models.",,,Open weights (restricted use),"China,Hong Kong,China,China",DeepSeek-LLM-1.3b-base,905284000000000000000,311840000000000 FLOP / GPU / sec * 16 GPUs * 168 hours * 3600 sec / hour * 0.3 [assumed utilization] = 9.05284Ã—10^20 FLOP,16.0,,2025-05-06 12:50,,,,,,"Industry,Academia,Academia",,,,,Open source,"MIT License for code
https://github.com/deepseek-ai/Janus

https://huggingface.co/deepseek-ai/Janus-1.3B

Deepseek License for weights

https://github.com/deepseek-ai/DeepSeek-LLM/blob/HEAD/LICENSE-MODEL","Industry,Academia,Academia",,,,12600.026628425288,Hardware,deepseek-ai,,,
Belle-whisper-larger-v3-turbo-zh ,Speech,Speech recognition,KE Holdings Inc. (â€œBeikeâ€),,2024-10-16,,https://huggingface.co/BELLE-2/Belle-whisper-large-v3-turbo-zh,,,,,,,,,,,,,,,,Unknown,"Fine tune whisper-large-v3-turbo-zh to enhance Chinese speech recognition capabilities, Belle-whisper-large-v3-turbo-zh demonstrates a 24-64% relative improvement in performance to whisper-large-v3-turbo on Chinese ASR benchmarks, including AISHELL1, AISHELL2, WENETSPEECH, and HKUST.",,,Open weights (unrestricted),China,,,,,,2024-12-08 14:05,,,,,,Industry,,,,,,Apache 2.0,Industry,,,,,,,,,
Ministral 3B,Language,"Language modeling/generation,Question answering,Code generation,Quantitative reasoning",Mistral AI,"Albert Jiang, Alexandre Abou Chahine, Alexandre Sablayrolles, Alexis Tacnet, Alodie Boissonnet, Alok Kothari, AmÃ©lie HÃ©liou, Andy Lo, Anna Peronnin, Antoine Meunier, Antoine Roux, Antonin Faure, Aritra Paul, Arthur Darcet, Arthur Mensch, Audrey Herblin-Stoop, Augustin Garreau, Austin Birky, Avinash Sooriyarachchi, Baptiste RoziÃ¨re, Barry Conklin, Bastien Bouillon, Blanche Savary de Beauregard, Carole Rambaud, Caroline Feldman, Charles de Freminville, Charline Mauro, Chih-Kuan Yeh, Chris Bamford, Clement Auguy, Corentin Heintz, Cyriaque Dubois, Devendra Singh Chaplot, Diego Las Casas, Diogo Costa, ElÃ©onore Arcelin, Emma Bou Hanna, Etienne Metzger, Fanny Olivier Autran, Francois Lesage, Garance Gourdel, Gaspard Blanchet, Gaspard Donada Vidal, Gianna Maria Lengyel, Guillaume Bour, Guillaume Lample, Gustave Denis, Harizo Rajaona, Himanshu Jaju, Ian Mack, Ian Mathew, Jean-Malo Delignon, Jeremy Facchetti, Jessica Chudnovsky, Joachim Studnia, Justus Murke, Kartik Khandelwal, Kenneth Chiu, Kevin Riera, Leonard Blier, Leonard Suslian, Leonardo Deschaseaux, Louis Martin, Louis Ternon, Lucile Saulnier, LÃ©lio Renard Lavaud, Sophia Yang, Margaret Jennings, Marie Pellat, Marie Torelli, Marjorie Janiewicz, Mathis Felardos, Maxime Darrin, Michael Hoff, MickaÃ«l Seznec, Misha Jessel Kenyon, Nayef Derwiche, Nicolas Carmont Zaragoza, Nicolas Faurie, Nicolas Moreau, Nicolas Schuhl, Nikhil Raghuraman, Niklas Muhs, Olivier de Garrigues, Patricia RozÃ©, Patricia Wang, Patrick von Platen, Paul Jacob, Pauline Buche, Pavankumar Reddy Muddireddy, Perry Savas, Pierre Stock, Pravesh Agrawal, Renaud de Peretti, Romain Sauvestre, Romain Sinthe, Roman Soletskyi, Sagar Vaze, Sandeep Subramanian, Saurabh Garg, Soham Ghosh, Sylvain Regnier, Szymon Antoniak, Teven Le Scao, Theophile Gervet, Thibault Schueller, Thibaut Lavril, Thomas Wang, TimothÃ©e Lacroix, Valeriia Nemychnikova, Wendy Shang, William El Sayed, William Marshall",2024-10-16,"Un Ministral, des Ministraux
Introducing the worldâ€™s best edge models.",https://mistral.ai/news/ministraux/,,,,3000000000.0,,,,Unspecified unreleased,,,,,,,,Confident,"On the first anniversary of the release of Mistral 7B, the model that revolutionized independent frontier AI innovation for millions, we are proud to introduce two new state-of-the-art models for on-device computing and at-the-edge use cases. We call them les Ministraux: Ministral 3B and Ministral 8B.

These models set a new frontier in knowledge, commonsense, reasoning, function-calling, and efficiency in the sub-10B category, and can be used or tuned to a variety of uses, from orchestrating agentic workflows to creating specialist task workers. Both models support up to 128k context length (currently 32k on vLLM) and Ministral 8B has a special interleaved sliding-window attention pattern for faster and memory-efficient inference.

Ministral 3B is a state-of-the-art Small Language Model (SLM) optimized for edge computing and on-device applications. As it is designed for low-latency and compute-efficient inference, it it also the perfect model for standard GenAI applications that have real-time requirements and high-volume.",,,API access,France,,,,,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,"Mistral Commercial License

For self-deployed use, please reach out to us for commercial licenses. We will also assist you in lossless quantization of the models for your specific use-cases to derive maximum performance.

The model weights for Ministral 8B Instruct are available for research use. Both models will be available from our cloud partners shortly.",Industry,,,,,,,,,
Ministral 8B,Language,"Language modeling/generation,Question answering,Code generation,Quantitative reasoning",Mistral AI,"Albert Jiang, Alexandre Abou Chahine, Alexandre Sablayrolles, Alexis Tacnet, Alodie Boissonnet, Alok Kothari, AmÃ©lie HÃ©liou, Andy Lo, Anna Peronnin, Antoine Meunier, Antoine Roux, Antonin Faure, Aritra Paul, Arthur Darcet, Arthur Mensch, Audrey Herblin-Stoop, Augustin Garreau, Austin Birky, Avinash Sooriyarachchi, Baptiste RoziÃ¨re, Barry Conklin, Bastien Bouillon, Blanche Savary de Beauregard, Carole Rambaud, Caroline Feldman, Charles de Freminville, Charline Mauro, Chih-Kuan Yeh, Chris Bamford, Clement Auguy, Corentin Heintz, Cyriaque Dubois, Devendra Singh Chaplot, Diego Las Casas, Diogo Costa, ElÃ©onore Arcelin, Emma Bou Hanna, Etienne Metzger, Fanny Olivier Autran, Francois Lesage, Garance Gourdel, Gaspard Blanchet, Gaspard Donada Vidal, Gianna Maria Lengyel, Guillaume Bour, Guillaume Lample, Gustave Denis, Harizo Rajaona, Himanshu Jaju, Ian Mack, Ian Mathew, Jean-Malo Delignon, Jeremy Facchetti, Jessica Chudnovsky, Joachim Studnia, Justus Murke, Kartik Khandelwal, Kenneth Chiu, Kevin Riera, Leonard Blier, Leonard Suslian, Leonardo Deschaseaux, Louis Martin, Louis Ternon, Lucile Saulnier, LÃ©lio Renard Lavaud, Sophia Yang, Margaret Jennings, Marie Pellat, Marie Torelli, Marjorie Janiewicz, Mathis Felardos, Maxime Darrin, Michael Hoff, MickaÃ«l Seznec, Misha Jessel Kenyon, Nayef Derwiche, Nicolas Carmont Zaragoza, Nicolas Faurie, Nicolas Moreau, Nicolas Schuhl, Nikhil Raghuraman, Niklas Muhs, Olivier de Garrigues, Patricia RozÃ©, Patricia Wang, Patrick von Platen, Paul Jacob, Pauline Buche, Pavankumar Reddy Muddireddy, Perry Savas, Pierre Stock, Pravesh Agrawal, Renaud de Peretti, Romain Sauvestre, Romain Sinthe, Roman Soletskyi, Sagar Vaze, Sandeep Subramanian, Saurabh Garg, Soham Ghosh, Sylvain Regnier, Szymon Antoniak, Teven Le Scao, Theophile Gervet, Thibault Schueller, Thibaut Lavril, Thomas Wang, TimothÃ©e Lacroix, Valeriia Nemychnikova, Wendy Shang, William El Sayed, William Marshall",2024-10-16,"Un Ministral, des Ministraux
Introducing the worldâ€™s best edge models.",https://mistral.ai/news/ministraux/,,,,8000000000.0,"Architecture	Dense Transformer
Parameters	8,019,808,256
Layers	36
Heads	32",,,Unspecified unreleased,Trained on a large proportion of multilingual and code data,,,,,,,Confident,"On the first anniversary of the release of Mistral 7B, the model that revolutionized independent frontier AI innovation for millions, we are proud to introduce two new state-of-the-art models for on-device computing and at-the-edge use cases. We call them les Ministraux: Ministral 3B and Ministral 8B.

These models set a new frontier in knowledge, commonsense, reasoning, function-calling, and efficiency in the sub-10B category, and can be used or tuned to a variety of uses, from orchestrating agentic workflows to creating specialist task workers. Both models support up to 128k context length (currently 32k on vLLM) and Ministral 8B has a special interleaved sliding-window attention pattern for faster and memory-efficient inference.",,,Open weights (non-commercial),France,,,,,,2025-05-30 14:39,,,,,,Industry,,,,,Unreleased,"Mistral Commercial License
Mistral Research License

For self-deployed use, please reach out to us for commercial licenses. We will also assist you in lossless quantization of the models for your specific use-cases to derive maximum performance.

The model weights for Ministral 8B Instruct are available for research use. Both models will be available from our cloud partners shortly.

https://huggingface.co/mistralai/Ministral-8B-Instruct-2410",Industry,,,,,,mistralai,,,
Marco-o1,Language,"Language modeling/generation,Question answering,Quantitative reasoning",Alibaba,"Yu Zhao, Huifeng Yin, Bo Zeng, Hao Wang, Tianqi Shi, Chenyang Lyu, Longyue Wang, Weihua Luo, Kaifu Zhang
",2024-10-16,Marco-o1: Towards Open Reasoning Models for Open-Ended Solutions,https://arxiv.org/abs/2411.14405,1.0,,,7000000000.0,,,,,,,,,,,,Confident,"Building on this momentum, Marco-o1 not only focuses on disciplines with standard answers, such as mathematics, physics, and codingâ€”which are well-suited for reinforcement learning (RL)â€”but also places greater emphasis on open-ended resolutions. We aim to address the question: â€œCan the o1 model effectively generalize to broader domains where clear standards are absent and rewards are challenging to quantify?â€ Marco-o1 is powered by Chain-of-Thought (CoT) fine-tuning, Monte Carlo Tree Search (MCTS), reflection mechanisms, and innovative reasoning strategiesâ€”optimized for complex real-world problem-solving tasks. ",,,Open weights (unrestricted),China,Qwen2-7B,,,,,2025-06-09 15:30,,,,,,Industry,,,,,Unreleased,"Apache 2,0 for weights:
https://huggingface.co/AIDC-AI/Marco-o1

https://github.com/AIDC-AI/Marco-o1",Industry,,,,,,AIDC-AI,,,
CHAI-1,Biology,Protein folding prediction,Chai discovery,"Jacques Boitreaud, Jack Dent, Matthew McPartlon, Joshua Meier, Vinicius Reis, Alex Rogozhnikov, Kevin Wu",2024-10-15,Introducing Chai-1: Decoding the molecular interactions of life,"https://www.chaidiscovery.com/blog/introducing-chai-1
https://www.biorxiv.org/content/10.1101/2024.10.10.615955v2",0.0,SOTA improvement,Matches or beats AF3 on Ligand PoseBusters,,,7.760572400000001e+21,"From paper: 128 A100s for 30 days; assumptions: 30% utilization rate, FP16 precision","PDB (Protein Data Bank), AlphaFold database (AFDB)",,,,720.0,Taken from paper: 128 A100s for 30 days,NVIDIA A100,,Confident,"We introduce Chai-1, a multi-modal foundation model for molecular structure prediction that performs at the state-of-the-art across a variety of tasks relevant to drug discovery. Chai-1 can optionally be prompted with experimental restraints (e.g. derived from wet-lab data) which boosts
performance by double-digit percentage points. Chai-1 can also be run in single-sequence mode without MSAs while preserving most of its performance. We release Chai-1 model weights and inference code as a Python package for non-commercial use and via a web interface where it can be used for free including for commercial drug discovery purposes.",,,Open weights (non-commercial),United States of America,,,,128.0,,2025-05-01 10:42,,,,128.0,Taken from paper,Industry,,,,,Open (non-commercial),https://github.com/chaidiscovery/chai-lab?tab=License-1-ov-file,Industry,,,BF16,100804.70264211958,Hardware,,,,
Pika 1.5,Video,"Video generation,Text-to-video",Pika Labs,,2024-10-15,Pika 1.5: Transforming Text to Video with Enhanced AI Features,https://pikartai.com/,,,,,,,,Unspecified unreleased,,,,,,,,Unknown,"Pika Labs has recently unveiled Pika 1.5, the latest iteration of their AI video generator, designed to push the boundaries of creativity and realism in video production. This new version introduces groundbreaking tools and enhancements, offering both amateurs and professionals a more dynamic and user-friendly platform for creating high-quality, visually engaging videos. From advanced special effects to hyper-realistic animations, Pika 1.5 opens up new possibilities for digital storytelling.",,,API access,United States of America,,,,,,2025-05-19 12:29,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
Firefly Video,"Video,Vision","Video generation,Text-to-video,Image-to-video",Adobe,Adobe,2024-10-14,"Adobe Launches Firefly Video Model and Enhances Image, Vector and Design Models",https://news.adobe.com/news/2024/10/101424-adobe-launches-firefly-video-model,,,"Possible it will see significant usage. The Firefly image generator has created 13B images since March 2023; paid users get 100 generations per month (and can continue at a slower rate after that). Assuming an average of 100 monthly generations per user, that's around 6.7M monthly average users across 19.5 months. Video generation is probably a smaller fraction of Adobe users, at least for now.",,,,,,,,,,,,,Unknown,,,,Hosted access (no API),United States of America,,,,,,2025-05-19 12:36,,,,,,Industry,,,,,,,Industry,,,,,,,,,
RNADiffFold,Biology,RNA structure prediction,"Hangzhou Institute of Medicine,Zhejiang University (ZJU),University of Chinese Academy of Sciences","Zhen Wang, Yizhen Feng, Qingwen Tian, Ziqi Liu, Pengju Yan, Xiaolin Li",2024-10-13,RNADiffFold: Generative RNA Secondary Structure Prediction using Discrete Diffusion Models,https://www.biorxiv.org/content/10.1101/2024.05.28.596177v2.abstract,0.0,,,,,,"
",,,135486.0,"RNAStrAlign (30,451) + bpRNA TR0 (102,318) + Mutate-seq (2,717) = 135,486 unique sequences

30,451 + 102,318 + 2,717 = 135,486

Final result: 135,486 (1.35e5)",,,NVIDIA A40 PCIe,,Confident,"RNA molecules are essential macromolecules that perform diverse biological functions in living beings. Precise prediction
of RNA secondary structures is instrumental in deciphering their complex three-dimensional architecture and functionality.
Traditional methodologies for RNA structure prediction, including energy-based and learning-based approaches, often
depict RNA secondary structures from a static perspective and rely on stringent a priori constraints. Inspired by the
success of diffusion models, in this work, we introduce RNADiffFold, an innovative generative prediction approach
of RNA secondary structures based on multinomial diffusion. We reconceptualize the prediction of contact maps as
akin to pixel-wise segmentation and accordingly train a denoising model to refine the contact maps starting from a
noise-infused state progressively. We also devise a potent conditioning mechanism that harnesses features extracted from
RNA sequences to steer the model toward generating an accurate secondary structure. These features encompass one-hot
encoded sequences, probabilistic maps generated from a pre-trained scoring network, and embeddings and attention
maps derived from RNA-FM. Experimental results on both within- and cross-family datasets demonstrate RNADiffFoldâ€™s
competitive performance compared with current state-of-the-art methods. Additionally, RNADiffFold has shown a notable
proficiency in capturing the dynamic aspects of RNA structures, a claim corroborated by its performance on datasets
comprising multiple conformations.",400.0,,Open weights (unrestricted),"China,China,China",,,,1.0,,2025-06-16 15:37,,,,,,"Academia,Academia",,,,,Open source,The code to reproduce our experiments and source data is available at https://github.com/HIM-AIM/RNADiffFold under an MIT License.,"Academia,Academia",,,,324.54882530306617,Hardware,,,,
PROPERMAB,Biology,Antibody property prediction,Regeneron,"Bian Li, Shukun Luo, Wenhua Wang, Jiahui Xu, Dingjiang Liu, Mohammed Shameem, John Mattila, Matthew Franklin, Peter G. Hawkins, Gurinder S. Atwal",2024-10-12,PROPERMAB: an integrative framework for in silico prediction of antibody developability using machine learning,https://www.biorxiv.org/content/10.1101/2024.10.10.616558v1,0.0,,,,,,,,,12001.0,"Feature Prediction: 12,000 mAbs
HIC Retention Time: 135 mAbs
Viscosity Prediction: 60 mAbs

Total = 12,000 + 135 + 60 = 12,195 mAbs (1.2e4)",,,,,Confident,"Selection of lead therapeutic molecules is often driven predominantly by pharmacological efficacy and safety. Candidate developability, such as biophysical properties that affect the formulation of the molecule into a product, is usually evaluated only toward the end of the drug development pipeline. The ability to evaluate developability properties early in the process of antibody therapeutic development could accelerate the timeline from discovery to clinic and save considerable resources. In silico predictive approaches, such as machine learning models, which map molecules to predictions of developability properties could offer a cost-effective and high-throughput alternative to experiments for antibody developability assessment. We developed a computational framework, PROPERMAB, for large-scale and efficient in silico prediction of developability properties for monoclonal antibodies, using custom molecular features and machine learning modeling. We demonstrate the power of PROPERMAB by using it to develop models to predict antibody hydrophobic interaction chromatography retention time and high-concentration viscosity. We further show that structure-derived features can be rapidly and accurately predicted directly from sequences by pre-training simple models for molecular features, thus providing the ability to scale these approaches to repertoire-scale sequence datasets.",,,,United States of America,,,,,,2025-05-01 10:42,,,,,,Industry,,,,,,,Industry,,,,,,,,,
EvoBind2,Biology,Protein design,"Stockholm University,Science for Life Laboratory","Qiuzhen Li,  Efstathios Nikolaos Vlachos,  Patrick Bryant",2024-10-12,Design of linear and cyclic peptide binders of different lengths from protein sequence information,https://www.biorxiv.org/content/10.1101/2024.06.20.599739v2.abstract,,,,,,,,,,,,,,,,Unknown,"Structure prediction technology has revolutionised the field of protein design, but key questions such as how to design new functions remain. Many proteins exert their functions through interactions with other proteins, and a significant challenge is designing these interactions effectively. While most efforts have focused on larger, more stable proteins, shorter peptides offer advantages such as lower manufacturing costs, reduced steric hindrance, and the ability to traverse cell membranes when cyclized. However, less structural data is available for peptides and their flexibility makes them harder to design. Here, we present a method to design both novel linear and cyclic peptide binders of varying lengths based solely on a protein target sequence. Our approach does not specify a binding site or the length of the binder, making the procedure completely blind. We demonstrate that linear and cyclic peptide binders of different lengths can be designed with nM affinity in a single shot, and adversarial designs can be avoided through orthogonal in silico evaluation, tripling the success rate. Our protocol, EvoBind2 is freely available https://github.com/patrickbryant1/EvoBind.",,,,"Sweden,Sweden",AlphaFold 2,,,,,2025-05-01 10:42,,,,,,"Academia,Research collective",,,,,,,"Academia,Research collective",,,,,,,,,
ALICE,Biology,Protein design,"Nanhu Brain-Computer Interface Institute,Lingang Laboratory,Medical School of Nantong University,Zhejiang University School of Medicine","Hanyu Zheng, Binjie Guo, Aisheng Mo, Hongyan Wei, Yile Wu, Xurong Lin, Haohan Jiang, Hengguang Li, Yunshuo Zhang, Zhuoyuan Song, Xuebin Ni, Yan Huang, Xiaosong Gu, Bin Yu, Ningtao Cheng, Xuhua Wang",2024-10-11,Mapping AAV capsid sequences to functions through function-guided in silico evolution,https://www.biorxiv.org/content/10.1101/2024.10.11.617764v1,0.0,,,,,,,UniProtKB,,20230001.0,2.89e6 sequences Ã— 7 residues/sequence = 2.023e7 datapoints,,,,,Confident,"Artificial intelligence (AI) has been suggested to facilitate time- and cost-effective functional engineering of adeno-associated virus (AAV) capsid sequences. Nevertheless, an AI-empowered approach to identify AAV capsid sequence-to-multifunction relationships remains elusive. To overcome this challenge, we propose a machine-intelligent design method to map an AAV capsid sequence to multiple functions, thereby enabling direct in silico engineering of AAV capsids. To fuse multiple functions into a single capsid sequence, a heuristic algorithm coupled with contrastive learning and reinforcement learning, named function-guided evolution (FE), was introduced to steer further evolution of the high-performing capsid sequences generated by a naive language model toward functions. We then illustrated the evolutionary mechanism of the FE approach for function-guided generation of capsid sequences. Further optimization steers the evolution toward desired functions within a function-guided landscape. Despite the constraint of datasets of only 129 entries, we successfully constructed a model to map AAV capsid sequences to multiple functions of improved viability coupled with central nervous system (CNS) tropism. In vivo experiments confirmed that two of the top eight engineered variants exhibited enhanced viability and remarkable CNS tropism. This interpretable machine-intelligent design method represents a pioneering effort enabling direct in silico engineering of AAV capsids for effective gene delivery",25.0,,,"China,China,China,China",,,,,,2025-05-01 10:42,,,,,,Academia,,,,,,,Academia,,,,,,,,,
T2V-Turbo-v2,Video,"Video generation,Text-to-video","University of California Santa Barbara (UCSB),University of California Los Angeles (UCLA),Amazon,University of Waterloo","Jiachen Li, Qian Long, Jian Zheng, Xiaofeng Gao, Robinson Piramuthu, Wenhu Chen, William Yang Wang",2024-10-11,"T2V-Turbo-v2: Enhancing Video Generation Model Post-Training through Data, Reward, and Conditional Guidance Design",https://arxiv.org/abs/2410.05677,,,,,"""We distill our T2V-Turbo-v2 from VideoCrafter2 (Chen et al., 2024a).""",,,"WebVid-10M,VidGen-1M","""We train on a mixed dataset VG + WV, which consists of equal portions of VidGen-1M (Tan et al., 2024) and WebVid-10M (Bain et al., 2021). """,,8K gradient steps without gradient accumulation.,,,NVIDIA A100,,Unknown,"In this paper, we focus on enhancing a diffusion-based text-to-video (T2V) model during the post-training phase by distilling a highly capable consistency model from a pretrained T2V model. Our proposed method, T2V-Turbo-v2, introduces a significant advancement by integrating various supervision signals, including high-quality training data, reward model feedback, and conditional guidance, into the consistency distillation process. Through comprehensive ablation studies, we highlight the crucial importance of tailoring datasets to specific learning objectives and the effectiveness of learning from diverse reward models for enhancing both the visual quality and text-video alignment. Additionally, we highlight the vast design space of conditional guidance strategies, which centers on designing an effective energy function to augment the teacher ODE solver. We demonstrate the potential of this approach by extracting motion guidance from the training datasets and incorporating it into the ODE solver, showcasing its effectiveness in improving the motion quality of the generated videos with the improved motion-related metrics from VBench and T2V-CompBench. Empirically, our T2V-Turbo-v2 establishes a new state-of-the-art result on VBench, with a Total score of 85.13, surpassing proprietary systems such as Gen-3 and Kling.",,,Open weights (unrestricted),"United States of America,United States of America,United States of America,Canada",VideoCrafter2,,,8.0,,2025-06-04 17:06,,,,,,"Academia,Academia,Industry,Academia",,,,,Open (non-commercial),"apache-2.0 for weights
https://huggingface.co/jiachenli-ucsb/T2V-Turbo-v2

no clear license for training code
https://github.com/Ji4chenLi/t2v-turbo","Academia,Academia,Industry,Academia",,,,6300.855154466357,,,,,
Baichuan-Omni,"Language,Vision,Multimodal,Audio,Speech,Video","Visual question answering,Language modeling/generation,Video description,Speech synthesis,Image captioning","Baichuan,Westlake University,Zhejiang University (ZJU)","Yadong Li, Haoze Sun, Mingan Lin, Tianpeng Li, Guosheng Dong, Tao Zhang, Bowen Ding, Wei Song, Zhenglin Cheng, Yuqi Huo, Song Chen, Xu Li, Da Pan, Shusen Zhang, Xin Wu, Zheng Liang, Jun Liu, Tao Zhang, Keer Lu, Yaqi Zhao, Yanjun Shen, Fan Yang, Kaicheng Yu, Tao Lin, Jianhua Xu, Zenan Zhou",2024-10-11,"Baichuan-Omni Technical Report
",https://arxiv.org/abs/2410.08565v1,1.0,,,7000000000.0,,,,,,,,,,,,Confident,"In this paper, we introduce Baichuan-Omni , the first open-source 7B Multimodal Large Language Model (MLLM) adept at concurrently processing and analyzing modalities of image, video, audio, and text, while delivering an advanced multimodal interactive experience and strong performance. We propose an effective multimodal training schema starting with 7B model and proceeding through two stages of multimodal alignment and multitask fine-tuning across audio, image, video, and text modal.",,,Unreleased,"China,China,China",,,,,,2025-06-09 10:50,,,,,,"Industry,Academia,Academia",,,,,Unreleased,"code and weights are supposed to be here, but not yet:

https://github.com/westlake-baichuan-mllm/bc-omni?tab=readme-ov-file","Industry,Academia,Academia",,,,,,,,,
RDT-1B,Robotics,Robotic manipulation,Tsinghua University,"Songming Liu, Lingxuan Wu, Bangguo Li, Hengkai Tan, Huayu Chen, Zhengyi Wang, Ke Xu, Hang Su, Jun Zhu",2024-10-10,RDT-1B: a Diffusion Foundation Model for Bimanual Manipulation,https://arxiv.org/abs/2410.07864,91.0,SOTA improvement,"""Results show that RDT achieves state-of-the-art performance, outperforming baselines by achieving an improvement of 56% in success rates across a wide spectrum of challenging tasks.""",1200000000.0,"Model Training and Inference: ""We scale the size of RDT up to 1.2B parameters, establishing it as the currently largest diffusion-based robotic foundation model.""",3.69e+22,"Model Training and Inference: ""The model is pre-trained on 48 H100 80GB GPUs for a month, giving a total of 1M training iteration steps. It takes three days to fine-tune this model using the same GPUs for 130K steps.""
Table 10: ""Mixed Precision, bf16""
Assume 48xGPU, ""scheduling reasons""  -> NVIDIA H100 SXM5 -> 9.894e14 FLOP/s/GPU
Assume 0.3 utilization
Assume 1 month -> 30 days = 720 hr = 2.592e6 s
0.3 * 48 GPU * 2.592e6 s * 9.894e14 FLOP/s/GPU ~=  3.69e22 FLOP

",RDT [pre-train],,,"Appendix D: ""Our pre-training dataset collection includes 46 datasets, with a total scale of 1M+ trajectories and 21TB, making it the largest pre-training collection of robotics datasets to date.""



",720.0,"Model Training and Inference: ""The model is pre-trained on 48 H100 80GB GPUs for a month, giving a total of 1M training iteration steps. It takes three days to fine-tune this model using the same GPUs for 130K steps.""",NVIDIA H100 PCIe,,Likely,"Bimanual manipulation is essential in robotics, yet developing foundation models is extremely challenging due to the inherent complexity of coordinating two robot arms (leading to multi-modal action distributions) and the scarcity of training data. In this paper, we present the Robotics Diffusion Transformer (RDT), a pioneering diffusion foundation model for bimanual manipulation. RDT builds on diffusion models to effectively represent multi-modality, with innovative designs of a scalable Transformer to deal with the heterogeneity of multi-modal inputs and to capture the nonlinearity and high frequency of robotic data. To address data scarcity, we further introduce a Physically Interpretable Unified Action Space, which can unify the action representations of various robots while preserving the physical meanings of original actions, facilitating learning transferrable physical knowledge. With these designs, we managed to pre-train RDT on the largest collection of multi-robot datasets to date and scaled it up to 1.2B parameters, which is the largest diffusion-based foundation model for robotic manipulation. We finally fine-tuned RDT on a self-created multi-task bimanual dataset with over 6K+ episodes to refine its manipulation capabilities. Experiments on real robots demonstrate that RDT significantly outperforms existing methods. It exhibits zero-shot generalization to unseen objects and scenes, understands and follows language instructions, learns new skills with just 1~5 demonstrations, and effectively handles complex, dexterous tasks. We refer to this https URL for the code and videos.",,,Open weights (unrestricted),China,,3,"Model Training and Inference: ""The model is pre-trained on 48 H100 80GB GPUs for a month, giving a total of 1M training iteration steps. It takes three days to fine-tune this model using the same GPUs for 130K steps.""
Table 10: ""Mixed Precision, bf16""
Assume 48xGPU, ""scheduling reasons""  -> NVIDIA H100 SXM5 -> 9.894e14 FLOP/s/GPU
Assume 0.3 utilization
3 days = 72 hr = 259200 s
0.3 * 48 GPU * 259200 s *  9.894e14 FLOP/s/GPU ~= 3.69e21 FLOP",48.0,,2025-06-04 15:07,,,,1536.0,"Table 10: Batch size, 32Ã—48",Academia,,,,,Open source,"https://huggingface.co/robotics-diffusion-transformer/rdt-1b
""All the code, pre-trained model weights, and data are licensed under the MIT license.""",Academia,,,BF16,33080.22622858553,,,,,
ProCALM (Uniref9B),Biology,"Proteins,Protein generation","Profluent Bio,California Institute of Technology","Jason Yang, Aadyot Bhatnagar, Jeffrey A. Ruffolo, Ali Madani",2024-10-09,Conditional Enzyme Generation Using Protein Language Models with Adapters,https://www.arxiv.org/abs/2410.03634,0.0,,,764000000.0,,,"
",Uniref9B,,9000000000.0,"""We trained two different ProCALM models â€“ one on 9 billion tokens of
enzyme sequences from Uniref, and one on 1.5 billion tokens of enzyme sequences from Swissprot Train""",60.0,,NVIDIA A100,,Confident,"The conditional generation of proteins with desired functions and/or properties is a key goal for generative models. Existing methods based on prompting of language models can generate proteins conditioned on a target functionality, such as a desired enzyme family. However, these methods are limited to simple, tokenized conditioning and have not been shown to generalize to unseen functions. In this study, we propose ProCALM (Protein Conditionally Adapted Language Model), an approach for the conditional generation of proteins using adapters to protein language models. Our specific implementation of ProCALM involves finetuning ProGen2 to incorporate conditioning representations of enzyme function and taxonomy. ProCALM matches existing methods at conditionally generating sequences from target enzyme families. Impressively, it can also generate within the joint distribution of enzymatic function and taxonomy, and it can generalize to rare and unseen enzyme families and taxonomies. Overall, ProCALM is a flexible and computationally efficient approach, and we expect that it can be extended to a wide range of generative language models.",,,,"United States of America,United States of America",ProGen2-base,80800000000000000000,"""Impressively, our Uniref9B and Swissprot-1.5B models only required 240 and 40 A100-hours to train, respectively""

240/4=60h across 4 A100

Finetuning compute:
240*60*60*312000000000000*0.3=8.08704e+19",4.0,,2025-05-01 10:42,,,,,,"Industry,Academia",,,,,,,"Industry,Academia",,,,3150.5678964407625,Hardware,,,,
SCUBA-D,Biology,Protein design,"University of Science and Technology of China (USTC),Oristruct Biotech Company,iFLYTEK Research","Yufeng Liu, Sheng Wang, Jixin Dong, Linghui Chen, Xinyu Wang, Lei Wang, Fudong Li, Chenchen Wang, Jiahai Zhang, Yuzhu Wang, Si Wei, Quan Chen, Haiyan Liu ",2024-10-09,De novo protein design with a denoising diffusion network independent of pretrained structure prediction models,https://www.nature.com/articles/s41592-024-02437-w,0.0,,,,,,,,,,,,,,,Unknown,"The recent success of RFdiffusion, a method for protein structure design with a denoising diffusion probabilistic model, has relied on fine-tuning the RoseTTAFold structure prediction network for protein backbone denoising. Here, we introduce SCUBA-diffusion (SCUBA-D), a protein backbone denoising diffusion probabilistic model freshly trained by considering co-diffusion of sequence representation to enhance model regularization and adversarial
losses to minimize data-out-of-distribution errors. While matching the performance of the pretrained RoseTTAFold-based RFdiffusion in generating experimentally realizable protein structures, SCUBA-D readily generates protein structures with not-yet-observed overall folds that are different from those predictable with RoseTTAFold. The accuracy of SCUBA-D was confirmed by the X-ray structures of 16 designed proteins and a protein complex, and by experiments validating designed heme-binding proteins and Ras-binding proteins. Our work shows that deep generative models of images or texts can be fruitfully extended to complex physical objects like protein structures by addressing outstanding issues such as the data-out-of-distribution errors.",,,,"China,United Kingdom of Great Britain and Northern Ireland,China",,,,,,2025-05-01 10:42,,,,,,"Academia,Industry,Industry",,,,,,,"Academia,Industry,Industry",,,,,,,,,
CLEAN-Contact,Biology,Enzyme function prediction,"Cleveland Clinic,Kent State University,Pacific Northwest National Laboratory","Yuxin Yang, Abby Jerger, Song Feng, Zixu Wang, Christina Brasfield, Margaret S. Cheung, Jeremy Zucker, Qiang Guan",2024-10-08,CLEAN-Contact: Contrastive Learning-enabled Enzyme Functional Annotation Prediction with Structural Inference,https://www.biorxiv.org/content/10.1101/2024.05.14.594148v2.abstract,0.0,,,,,,,SwissProt,,224743.0,"224,742 = 2.24742e5 datapoints (unique protein sequences with contact maps from Swiss-Prot database)",,,,,Confident,"Recent years have witnessed the remarkable progress of deep learning within the realm of scientific disciplines, yielding a wealth of promising outcomes. A prominent challenge within this domain has been the task of predicting enzyme function, a complex problem that has seen the development of numerous computational methods, particularly those rooted in deep learning techniques. However, the majority of these methods have primarily focused on either amino acid sequence data or protein structure data, neglecting the potential synergy of combining of both modalities. To address this gap, we propose a novel Contrastive Learning framework for Enzyme functional ANnotation prediction combined with protein amino acid sequences and Contact maps (CLEAN-Contact). We rigorously evaluated the performance of our CLEAN-Contact framework against the state-of-the-art enzyme function prediction model using multiple benchmark datasets. Using CLEAN-Contact, we predicted novel enzyme functions within the proteome of Prochlorococcus marinus MED4. Our findings convincingly demonstrate the substantial superiority of our CLEAN-Contact framework, marking a significant step forward in enzyme function prediction accuracy.",,,Unreleased,"United States of America,United States of America,United States of America",,,,,,2025-06-09 13:31,,,,,,"Academia,Academia,Government",,,,,Open (non-commercial),"All codes and data used in training and testing are available
at https://github.com/pnnl-predictive-phenomics/clean-contact. 
( NON-EXCLUSIVE RESEARCH USE LICENSE FOR CLEAN SOFTWARE)
CLEAN-Contact is also freely accessible through an easy-to-use web server: https://ersa.guans.cs.kent.edu/","Academia,Academia,Government",,,,,,,,,
SO3LR,Biology,"Protein folding prediction,Molecular simulation","University of Luxembourg,Technische Universitat Berlin,Berlin Institute for the Foundations of Learning and Data,DeepMind,Max Planck Institute for Informatics,Korea University","Adil Kabylda, J. Thorben Frank, Sergio Suarez Dou, Almaz Khabibrakhmanov, Leonardo Medrano Sandonas, Oliver T. Unke, Stefan Chmiela, Klaus-Robert Muller, Alexandre Tkatchenko",2024-10-08,Molecular Simulations with a Pretrained Neural Network and Universal Pairwise Force Fields,https://chemrxiv.org/engage/chemrxiv/article-details/6704263051558a15ef6478b6,0.0,,,,,,,,,120000000.0,"GEMS fragments: 2,700,000
QM7-X molecules: 1,000,000
AQM gas-phase drugs: 60,000
SPICE dipeptides: 33,000
DES molecular dimers: 15,000
Gas-phase water clusters: 10,000

2,700,000 + 1,000,000 + 60,000 + 33,000 + 15,000 + 10,000 = 3,818,000 â‰ˆ 4,000,000 datapoints

Assuming 30 atoms per molecule and 1 token per atom: 120000000 tokens

",86.0,,NVIDIA H100 PCIe,,Likely,"Machine Learning Force Fields (MLFFs) promise to enable general molecular simulations that can simultaneously achieve efficiency, accuracy, transferability, and scalability for diverse molecules, materials, and hybrid interfaces. A key step toward this goal has been made with the GEMS approach to biomolecular dynamics [Sci. Adv. 10, eadn4397 (2024)]. This work introduces the SO3LR method that integrates the fast and stable SO3krates neural network for semi-local interactions with universal pairwise force fields designed for short-range repulsion, long-range electrostatics, and dispersion interactions. SO3LR is trained on a diverse set of 4 million neutral and charged molecular complexes
computed at the PBE0+MBD level of quantum mechanics, ensuring a comprehensive coverage of covalent and non-covalent interactions. Our approach is characterized by computational and data efficiency, scalability to 200 thousand atoms on a single GPU, and reasonable to high accuracy across the chemical space of organic (bio)molecules. SO3LR is applied to study units of four major biomolecule types, polypeptide folding, and nanosecond dynamics of larger systems such as a protein, a glycoprotein, and a lipid bilayer, all in explicit solvent. Finally, we discuss the future challenges toward truly general molecular simulations by combining MLFFs with traditional atomistic models.",,,,"Luxembourg,Germany,Germany,United Kingdom of Great Britain and Northern Ireland,Germany,Korea (Republic of)",,,,,,2025-05-01 10:42,,,,,,"Academia,Research collective,Industry,Academia,Academia",,,,,,,"Academia,Research collective,Industry,Academia,Academia",,,,,Hardware,,,,
CogVideoX,Video,"Video generation,Text-to-video","Zhipu AI,Tsinghua University","Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, Da Yin, Xiaotao Gu, Yuxuan Zhang, Weihan Wang, Yean Cheng, Ting Liu, Bin Xu, Yuxiao Dong, Jie Tang",2024-10-08,CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer,https://arxiv.org/abs/2408.06072,,,,5000000000.0,5 billion,,,"LAION,COYO-700M","""We construct a collection of relatively high-quality video clips with text descriptions with video filters and recaption models. After filtering, approximately 35M single-shot clips remain, with each clip averaging about 6 seconds. We additionally use 2B images filtered with aesthetics score from LAION-5B (Schuhmann et al., 2022) and COYO-700M (Byeon et al., 2022) datasets to assist training.""",,"""35M single-shot clips remain, with each clip averaging about 6 seconds""
""During training, we first train a 3D VAE at 256 Ã— 256 resolution and 17 frames to save computation. Randomly select 8 or 16 fps to enhance the modelâ€™s robustness""
""we conduct a two-stage training process by first training on a video of 17 frames and finetuning by context parallel on videos of 161 frames.""

They provide many training details for smaller models but not this one",,,,,Confident,"We present CogVideoX, a large-scale text-to-video generation model based on diffusion transformer, which can generate 10-second continuous videos aligned with text prompt, with a frame rate of 16 fps and resolution of 768 * 1360 pixels. Previous video generation models often had limited movement and short durations, and is difficult to generate videos with coherent narratives based on text. We propose several designs to address these issues. First, we propose a 3D Variational Autoencoder (VAE) to compress videos along both spatial and temporal dimensions, to improve both compression rate and video fidelity. Second, to improve the text-video alignment, we propose an expert transformer with the expert adaptive LayerNorm to facilitate the deep fusion between the two modalities. Third, by employing a progressive training and multi-resolution frame pack technique, CogVideoX is adept at producing coherent, long-duration, different shape videos characterized by significant motions. In addition, we develop an effective text-video data processing pipeline that includes various data preprocessing strategies and a video captioning method, greatly contributing to the generation quality and semantic alignment. Results show that CogVideoX demonstrates state-of-the-art performance across both multiple machine metrics and human evaluations. The model weight of both 3D Causal VAE, Video caption model and CogVideoX are publicly available at this https URL.",,,Open weights (non-commercial),"China,China",,,,,,2025-05-19 12:38,,,,,,"Industry,Academia",,,,,Open source,"https://huggingface.co/THUDM/CogVideoX-5b

https://github.com/THUDM/CogVideo
Apache 2.0 for code","Industry,Academia",,,,,,,,,
GR-2,Robotics,"Video,Action recognition,Video generation,Instruction interpretation,Robotic manipulation",ByteDance,"Hang Li, Yifeng Li, Yuxiao Liu, Hongtao Wu, Jiafeng Xu, Yichu Yang, Hanbo Zhang, Minzhao Zhu",2024-10-08,GR-2: A Generative Video-Language-Action Model with Web-Scale Knowledge for Robot Manipulation,https://arxiv.org/abs/2410.06158v1,47.0,SOTA improvement,Top perfoming model on CALVIN benchmark. ,230000000.0,"the default GR-2 model contains 230M parameters, of which 95M are trainable",,,,"GR-2 is pre-trained on 38 million text-video data (amounting to over 50 billion tokens), and is capable of accomplishing over 100 manipulation tasks and performing bin-picking of over 100 objects. It significantly scales up the pre-training data and number of tasks.",,"This large-scale pre-training, involving 38 million video clips
and over 50 billion tokens",,,,,Confident,"We present GR-2, a state-of-the-art generalist robot agent for versatile and generalizable robot manipulation. GR-2 is first pre-trained on a vast number of Internet videos to capture the dynamics of the world. This large-scale pre-training, involving 38 million video clips and over 50 billion tokens, equips GR-2 with the ability to generalize across a wide range of robotic tasks and environments during subsequent policy learning. Following this, GR-2 is fine-tuned for both video generation and action prediction using robot trajectories. It exhibits impressive multi-task learning capabilities, achieving an average success rate of 97.7% across more than 100 tasks. Moreover, GR-2 demonstrates exceptional generalization to new, previously unseen scenarios, including novel backgrounds, environments, objects, and tasks. Notably, GR-2 scales effectively with model size, underscoring its potential for continued growth and application. Project page: \url{this https URL}.",,,Unreleased,China,,,,,,2025-05-28 17:55,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
Pyramid Flow,Video,"Video generation,Text-to-video","Peking University,Kuaishou Technology,Beijing University of Posts and Telecommunications","Yang Jin, Zhicheng Sun, Ningyuan Li, Kun Xu, Kun Xu, Hao Jiang, Nan Zhuang, Quzhe Huang, Yang Song, Yadong Mu, Zhouchen Lin",2024-10-08,Pyramidal Flow Matching for Efficient Video Generative Modeling,https://arxiv.org/abs/2410.05954,60.0,,,2000000000.0,"""We utilize the prevailing MM-DiT architecture from SD3 Medium (Esser
et al., 2024) as the base model, with 2B parameters in total"" (https://arxiv.org/pdf/2410.05954, page 7).",7.670000000000002e+21,"â€œIt is trained only on open-source datasets within 20.7k A100 GPU hoursâ€ (https://huggingface.co/spaces/Pyramid-Flow/pyramid-flow), (https://arxiv.org/pdf/2410.05954, page 1). The training was done in BF16 (https://arxiv.org/pdf/2410.05954, page 18). 

Both A100 models deliver peak 312 teraFLOPS without sparsity (https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/a100-80gb-datasheet-update-nvidia-us-1521051-r2-web.pdf).

Assuming 33% utilization rate,
Training compute
= 0.33 * 3.12e14 FLOPS * 2.07e4 hours * 3.6e3 s / hour
~= 7.67e21 FLOPS","JourneyDB,OpenVid-1M",See page 7 of https://arxiv.org/pdf/2410.05954 for full list of training datasets used.,,See page 7 of https://arxiv.org/pdf/2410.05954 for sizes of training datasets used.,162.0,See Table 4 on page 18 of https://arxiv.org/pdf/2410.05954.,NVIDIA A100,,Confident,"Video generation requires modeling a vast spatiotemporal space, which demands significant computational resources and data usage. To reduce the complexity, the prevailing approaches employ a cascaded architecture to avoid direct training with full resolution latent. Despite reducing computational demands, the separate optimization of each sub-stage hinders knowledge sharing and sacrifices flexibility. This work introduces a unified pyramidal flow matching algorithm. It reinterprets the original denoising trajectory as a series of pyramid stages, where only the final stage operates at the full resolution, thereby enabling more efficient video generative modeling. Through our sophisticated design, the flows of different pyramid stages can be interlinked to maintain continuity. Moreover, we craft autoregressive video generation with a temporal pyramid to compress the full-resolution history. The entire framework can be optimized in an end-to-end manner and with a single unified Diffusion Transformer (DiT). Extensive experiments demonstrate that our method supports generating high-quality 5-second (up to 10-second) videos at 768p resolution and 24 FPS within 20.7k A100 GPU training hours. All code and models are open-sourced at https://pyramid-flow.github.io.",,,Open weights (unrestricted),"China,China,China",,,,128.0,,2025-05-19 12:29,,,,,,"Academia,Industry,Academia",,,,20700.0,,,"Academia,Industry,Academia",,,,100820.41786842256,,,,,
MLDD3UTRmRRNAS,Biology,"Protein or nucleotide language model (pLM/nLM),Nucleotide generation",Ginkgo Bioworks,"Alyssa Kramer Morrow, Ashley Thornal, Elise Duboscq Flynn, Emily Hoelzli, Meimei Shan, Gorkem Garipler, Rory Kirchner, Aniketh Janardhan Reddy, Sophia Tabchouri, Ankit Gupta, Jean-Baptiste Michel, Uri Laserson",2024-10-07,ML-driven design of 3â€™ UTRs for mRNA stability,https://www.biorxiv.org/content/10.1101/2024.10.07.616676v1,0.0,,,44000000.0,,,,,,18532000.0,"TOKENS = 113,000 Ã— 164 = 18532000",,,NVIDIA A100,,Likely,"Using mRNA as a therapeutic has received enormous attention in the last few years, but instability of the molecule remains a hurdle to achieving long-lasting therapeutic levels of protein expression. In this study, we describe our approach for designing stable mRNA molecules by combining machine learning-driven sequence design with high-throughput experimental assays. We developed a high-throughput massively parallel reporter assay (MPRA) that, in a single experiment, measures the half-life of tens of thousands of unique mRNA sequences containing designed 3' UTRs. Over multiple design-build-test iterations, we have accumulated 180,000 unique measurements of mRNA stability covering unique genomic and synthetic 3' UTRs, representing the largest such dataset of sequences. We trained highly-accurate machine learning models to map from 3' UTR sequence to mRNA stability, and used them to guide the design of synthetic 3' UTRs that increase mRNA stability in cell lines. Finally, we validated the function of several ML-designed 3' UTRs in mouse models, resulting in up to 2-fold more protein production over time and 30--100-fold higher protein output at later time points compared to a commonly used benchmark. These results highlight the potential of ML-driven sequence design for mRNA therapeutics.",,,API access,United States of America,,,,1.0,,2025-06-16 13:37,,,,,,Industry,,,,,Unreleased,"""weâ€™re making our generative model used for 3â€™ UTR design trained on genomic 3â€™ UTRs available via our API for $0.18 per 1M tokens.""

https://biopharma.ginkgo.bio/resources/white-papers/engineering-stable-mrna-insights-from-ml-driven-design-of-3-utrs",Industry,,,,432.7895909200725,,,,,
scHyena,Biology,Protein or nucleotide language model (pLM/nLM),Korea Advanced Institute of Science and Technology (KAIST),"Gyutaek Oh, Baekgyu Choi, Inkyung Jung, Jong Chul Ye",2024-10-04,scHyena: Foundation Model for Full-Length Single-Cell RNA-Seq Analysis in Brain,https://arxiv.org/abs/2310.02713,3.0,,,,,8.6e+18,3.5*24*60*60*2*35580000000000*0.4=8.6075136e+18,,,11000000001.0,"Cells: 430,312 + 145,170 = 575,482
Genes per cell: 19,306
Total data points: 575,482 Ã— 19,306 = 11,111,255,492 (1.11 Ã— 10Â¹â°)
Final estimate: 1.11 Ã— 10Â¹â° tokens",,,NVIDIA GeForce RTX 3090,,Confident,"Single-cell RNA sequencing (scRNA-seq) has made significant strides in unraveling the intricate cellular diversity within complex tissues. This is particularly critical in the brain, presenting a greater diversity of cell types than other tissue types, to gain a deeper understanding of brain function within various cellular contexts. However, analyzing scRNA-seq data remains a challenge due to inherent measurement noise stemming from dropout events and the limited utilization of extensive gene expression information. In this work, we introduce scHyena, a foundation model designed to address these challenges and enhance the accuracy of scRNA-seq analysis in the brain. Specifically, inspired by the recent Hyena operator, we design a novel Transformer architecture called singe-cell Hyena (scHyena) that is equipped with a linear adaptor layer, the positional encoding via gene-embedding, and a {bidirectional} Hyena operator. This enables us to process full-length scRNA-seq data without losing any information from the raw data. In particular, our model learns generalizable features of cells and genes through pre-training scHyena using the full length of scRNA-seq data. We demonstrate the superior performance of scHyena compared to other benchmark methods in downstream tasks, including cell type classification and scRNA-seq imputation.",,,,Korea (Republic of),,,,2.0,,2025-05-01 10:42,,,,,,Academia,,,,,,,Academia,,,,1378.5269407888657,Hardware,,,,
Movie Gen Video,"Video,Vision","Video generation,Text-to-video,Image-to-video",Meta AI,"Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen
Shi, Chih-Yao Ma, Ching-Yao Chuang, David Yan, Dhruv Choudhary, Dingkang Wang, Geet Sethi, Guan
Pang, Haoyu Ma, Ishan Misra, Ji Hou, Jialiang Wang, Kiran Jagadeesh, Kunpeng Li, Luxin Zhang, Mannat
Singh, Mary Williamson, Matt Le, Mitesh Kumar Singh, Peizhao Zhang, Peter Vajda, Quentin Duval, Rohit
Girdhar, Roshan Sumbaly, Sai Saketh Rambhatla, Sam Tsai, Samaneh Azadi, Samyak Datta, Sanyuan Chen,
Sean Bell, Sharadh Ramaswamy, Shelly Sheynin, Siddharth Bhattacharya, Tao Xu, Tingbo Hou, Wei-Ning
Hsu, Xi Yin, Xiaoliang Dai, Yaniv Taigman, Yaqiao Luo, Yen-Cheng Liu, Yi-Chiao Wu, Yue Zhao, Yuval
Kirstain, Zecheng He, Zijian He",2024-10-04,Movie Gen: A Cast of Media Foundation Models,https://ai.meta.com/static-resource/movie-gen-research-paper,,Training cost,BOTE estimate of cost is ~$3 million,30000000000.0,30B,1.65e+24,"Model size = 30B
Broken down by training stage (table 3):
256px T2I: samples seen = 1.94E9; sample token length = 256; flops = 6ND = 8.94E22
256px T2I/V: samples seen = 3.95E8; sample token length = 8192; flops = 6ND = 5.82E23
768px T2I/V: samples seen = 7.38E7; sample token length = 73,728; flops = 6ND = 9.79E23
Total flops = 1.65E24",,,26600000000.0,"O(1B) images
O(100M) videos, each with 256 frames ~= 25M images",331.0,"54 hours for 256px T2I
128 hours for 256px T2I/V
149 hours for 768px T2I/V",NVIDIA H100 SXM5 80GB,,Confident,"We present Movie Gen, a cast of foundation models that generates high-quality, 1080p HD videos with different aspect ratios and synchronized audio. We also show additional capabilities such as precise instruction-based video editing and generation of personalized videos based on a userâ€™s image.
Our models set a new state-of-the-art on multiple tasks: text-to-video synthesis, video personalization, video editing, video-to-audio generation, and text-to-audio generation. Our largest video generation model is a 30B parameter transformer trained with a maximum context length of 73K video tokens, corresponding to a generated video of 16 seconds at 16 frames-per-second. We show multiple technical innovations and simplifications on the 
 architecture, latent spaces, training objectives and recipes, data curation, evaluation protocols, parallelization techniques, and inference optimizations that allow us to reap the benefits of scaling pre-training data, model size, and training compute for training large scale media generation models. We hope this paper helps the research community to accelerate progress and innovation in media generation models.
All videos from this paper are available at https://go.fb.me/MovieGenResearchVideos.",,,Unreleased,United States of America,,,,6144.0,,2025-05-26 18:58,,,,,,Industry,,,,,Unreleased,,Industry,,,,8469669.524206791,Operation counting,,,,
Movie Gen Audio,Audio,Audio generation,Meta AI,"Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen
Shi, Chih-Yao Ma, Ching-Yao Chuang, David Yan, Dhruv Choudhary, Dingkang Wang, Geet Sethi, Guan
Pang, Haoyu Ma, Ishan Misra, Ji Hou, Jialiang Wang, Kiran Jagadeesh, Kunpeng Li, Luxin Zhang, Mannat
Singh, Mary Williamson, Matt Le, Mitesh Kumar Singh, Peizhao Zhang, Peter Vajda, Quentin Duval, Rohit
Girdhar, Roshan Sumbaly, Sai Saketh Rambhatla, Sam Tsai, Samaneh Azadi, Samyak Datta, Sanyuan Chen,
Sean Bell, Sharadh Ramaswamy, Shelly Sheynin, Siddharth Bhattacharya, Tao Xu, Tingbo Hou, Wei-Ning
Hsu, Xi Yin, Xiaoliang Dai, Yaniv Taigman, Yaqiao Luo, Yen-Cheng Liu, Yi-Chiao Wu, Yue Zhao, Yuval
Kirstain, Zecheng He, Zijian He",2024-10-04,Movie Gen: A Cast of Media Foundation Models,https://ai.meta.com/static-resource/movie-gen-research-paper,,,,13000000000.0,13B,1.4e+23,Pre trained for 14 days on 384 H100s. I assumed a 0.3 utilization rate,,,,It was trained on O(1k) hours of audio,360.0,"14 days of pretraining, 1 day of fine tuning",NVIDIA H100 SXM5 80GB,,Confident,"We present Movie Gen, a cast of foundation models that generates high-quality, 1080p HD videos
with different aspect ratios and synchronized audio. We also show additional capabilities such as
precise instruction-based video editing and generation of personalized videos based on a userâ€™s image.
Our models set a new state-of-the-art on multiple tasks: text-to-video synthesis, video personalization,
video editing, video-to-audio generation, and text-to-audio generation. Our largest video generation
model is a 30B parameter transformer trained with a maximum context length of 73K video tokens,
corresponding to a generated video of 16 seconds at 16 frames-per-second. We show multiple technical
innovations and simplifications on the architecture, latent spaces, training objectives and recipes, data
curation, evaluation protocols, parallelization techniques, and inference optimizations that allow us to
reap the benefits of scaling pre-training data, model size, and training compute for training large scale
media generation models. We hope this paper helps the research community to accelerate progress
and innovation in media generation models.
All videos from this paper are available at https://go.fb.me/MovieGenResearchVideos.",,,Unreleased,United States of America,,,,384.0,,2025-05-26 18:58,,,,,,Industry,,,,,Unreleased,,Industry,,,,529354.3452629244,Hardware,,,,
EnzymeFlow,Biology,Protein design,"McGill University,Mila - Quebec AI (originally Montreal Institute for Learning Algorithms),Hong Kong University of Science and Technology (HKUST),University of Washington,Microsoft Research,DeepMind,Shanghai Jiao Tong University,University of Montreal / UniversitÃ© de MontrÃ©al","Chenqing Hua, Yong Liu, Dinghuai Zhang, Odin Zhang, Sitao Luan, Kevin K. Yang, Guy Wolf, Doina Precup, Shuangjia Zheng",2024-10-01,EnzymeFlow: Reaction-conditioned Enzyme Catalytic Pocket Design,https://arxiv.org/abs/2410.00327,,,,,,,,,,53484.0,"53,483 enzyme-reaction pairs were used to train EnzymeFlow after homology filtering

Initial dataset: 328,192
Raw dataset before filtering: 232,520
Final filtered dataset: 53,483",,,,,Confident,"Enzyme design is a critical area in biotechnology, with applications ranging from drug development to synthetic biology. Traditional methods for enzyme function prediction or protein binding pocket design often fall short in capturing the dynamic and complex nature of enzyme-substrate interactions, particularly in catalytic processes. To address the challenges, we introduce EnzymeFlow, a generative model that employs flow matching with hierarchical pre-training and enzyme-reaction co-evolution to generate catalytic pockets for specific substrates and catalytic reactions. Additionally, we introduce a large-scale, curated, and validated dataset of enzyme-reaction pairs, specifically designed for the catalytic pocket generation task, comprising a total of 328, 192 pairs. By incorporating evolutionary dynamics and reaction-specific adaptations, EnzymeFlow becomes a powerful model for designing enzyme pockets, which is capable of catalyzing a wide range of biochemical reactions. Experiments on the new dataset demonstrate the modelâ€™s effectiveness in designing high-quality, functional enzyme catalytic pockets, paving the way for advancements in enzyme engineering and synthetic biology. We provide EnzymeFlow code at https://github.com/WillHua127/EnzymeFlow with notebook demonstration at https://github.com/WillHua127/ EnzymeFlow/blob/main/enzymeflow_demo.ipynb.",,,,"Canada,Canada,Hong Kong,China,United States of America,United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,United Kingdom of Great Britain and Northern Ireland,China,Canada",,,,,,2025-05-01 10:42,,,,,,"Academia,Academia,Academia,Academia,Industry,Industry,Academia,Academia",,,,,,,"Academia,Academia,Academia,Academia,Industry,Industry,Academia,Academia",,,,,,,,,
BindCraft,Biology,Protein design,"Ecole Polytechnique FÂ´edÂ´erale de Lausanne (EPFL),University of Zurich,University of Lausanne,Massachusetts Institute of Technology (MIT),Visterra Inc,Swiss Federal Institute of Technology","Martin Pacesa, Lennart Nickel, Joseph Schmidt, Ekaterina Pyatova, Christian Schellhaas, Lucas Kissling, Ana Alcaraz-Serna, Yehlin Cho, Kourosh H. Ghamary, Laura VinuÃ©, Brahm J. Yachnin, Andrew M. Wollacott, Stephen Buckley, Sandrine Georgeon, Casper A. Goverde, Georgios N. Hatzopoulos, Pierre GÃ¶nczy, Yannick D. Muller, Gerald Schwank, Sergey Ovchinnikov, Bruno E. Correia",2024-10-01,BindCraft: one-shot design of functional protein binders,https://www.biorxiv.org/content/10.1101/2024.09.30.615802v1.abstract,,,,,,,,,,,,,,,,Unknown,"Proteinâ€“protein interactions (PPIs) are at the core of all key biological processes. However, the complexity of the structural features that determine PPIs makes their design challenging. We present BindCraft, an open-source and automated pipeline for de novo protein binder design with experimental success rates of 10-100%. BindCraft leverages the trained deep learning weights of AlphaFold21 to generate nanomolar binders without the need for high-throughput screening or experimental optimization, even in the absence of known binding sites. We successfully designed binders against a diverse set of challenging targets, including cell-surface receptors, common allergens, de novo designed proteins, and multi-domain nucleases, such as CRISPR-Cas9. We showcase their functional and therapeutic potential by demonstrating that designed binders can reduce IgE binding to birch allergen in patient-derived samples. This work represents a significant advancement towards a â€œone design-one binderâ€ approach in computational design, with immense potential in therapeutics, diagnostics, and biotechnology.

",,,Unreleased,"Switzerland,Switzerland,Switzerland,United States of America,United States of America,Switzerland",AlphaFold 2,,,,,2025-06-16 12:16,,,,,,"Academia,Academia,Academia,Academia,Industry,Academia",,,,,,"The full BindCraft code along with installation instructions and binder design protocols are available on GitHub under MIT license
(https://github.com/martinpacesa/BindCraft). A Google Colab notebook for running BindCraft is available at
https://github.com/martinpacesa/BindCraft/blob/main/notebooks/BindCraft.ipynb","Academia,Academia,Academia,Academia,Industry,Academia",,,,,,,,,
PlasmidGPT,Biology,Plasmid Design,Harvard University,"Bin Shao
",2024-10-01,PlasmidGPT: a generative framework for plasmid design and annotation,https://www.biorxiv.org/content/10.1101/2024.09.30.615762v1.abstract,,,,110000000.0,"""The model consists of 12 230 layers with a dimension of 512, with 8 attention heads and a total of 110M parameters""",,,,,923000000.0,"""We introduce PlasmidGPT, a generative language model pretrained on 153k engineered  8 plasmid sequences from Addgene.""

""Sequences shorter than 2kb222
were removed, resulting in a training dataset of 923M base pairs (bp).""

Assuming each base pair is one token.",,,NVIDIA A100,,Likely,"We introduce PlasmidGPT, a generative language model pretrained on 153k engineered plasmid sequences from Addgene. PlasmidGPT generates de novo sequences that share similar characteristics with engineered plasmids but show low sequence identity to the training data. We demonstrate its ability to generate plasmids in a controlled manner based on the input sequence or specific design constraint. Moreover, our model learns informative embeddings of both engineered and natural plasmids, allowing for efficient prediction of a wide range of sequence-related attributes.

",,,Open weights (unrestricted),United States of America,,,,1.0,,2025-05-01 10:42,,,,,,Academia,,,,,,,Academia,,,,432.8474224961108,Operation counting,,,,
LFM 40B,Language,"Language modeling/generation,Question answering",Liquid,,2024-09-30,Liquid Foundation Models: Our First Series of Generative AI Models,https://www.liquid.ai/liquid-foundation-models,,,,40300000000.0,"""A 40.3B Mixture of Experts (MoE) model, designed for tackling more complex tasks.""

""LFM-40B offers a new balance between model size and output quality. It leverages 12B activated parameters at use.""",,,,,,,,,,,Confident,"We announce the first series of Liquid Foundation Models (LFMs), a new generation of generative AI models built from first principles.
Our 1B, 3B, and 40B LFMs achieve state-of-the-art performance in terms of quality at each scale, while maintaining a smaller memory footprint and more efficient inference.
Try LFMs today on Liquid Playground, Lambda (Chat UI and API), Perplexity Labs, and soon on Cerebras Inference. The LFM stack is being optimized for NVIDIA, AMD, Qualcomm, Cerebras, and Apple hardware.
We build private, edge, and on-premise AI solutions for enterprises of any size.
We are scaling LFMs and expect to introduce new and better capabilities across various industries, such as financial services, biotechnology, and consumer electronics.",,,API access,United States of America,,,,,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,"""Try LFMs today on Liquid Playground, Lambda (Chat UI and API), Perplexity Labs, and soon on Cerebras Inference. The LFM stack is being optimized for NVIDIA, AMD, Qualcomm, Cerebras, and Apple hardware.""",Industry,,,,,,,,,
FragLlama,Biology,Drug discovery,YDS Pharmatech,"Jian Shen, Shengmin Zhou, Xing Che",2024-09-30,FragLlama: Next-fragment prediction for molecular design,https://www.biorxiv.org/content/10.1101/2024.09.28.615626v1.abstract,,,,779000000.0,,,,,,70000000001.0,"70 billion tokens = 7.0e10 datapoints

Calculation: 70,000,000,000 = 7.0e10",,,NVIDIA A100,,Confident,"The emergence of ChatGPT has drawn significant attention to Large Language Models (LLMs) due to their impressive performance. While LLMs primarily focus on next token/word prediction, we apply this principle to molecular design by reframing the task as predicting the next token/fragment. We present FragLlama, a large language model trained for molecular design, featuring custom tokens that represent molecular fragments and functional groups. The model is for generating molecules given one or two fragments, for application scenarios like general hit-to-lead and lead optimization stage drug design, PROTAC linker design; mapping to commonly used drug design strategies like fragment growing and scaffold hopping. In the pre-training stage, we adapted the Llama 3 architecture to create FragLlama, training it to learn conditional probabilities of these fragment-level tokens. The subsequent alignment stage employed fine-tuning to guide the model towards generating molecules with desired properties. The effectiveness of FragLlama is demonstrated through its applications in designing molecular glue libraries, PROTAC linkers and EGFR binders. FragLlama demonstrates proficiency in reproducing expert-level designs while also exploring novel and promising chemical spaces, highlighting its potential to augment the capabilities of medicinal chemists in drug design.",,,,United States of America,,,,7.0,,2025-05-01 10:42,,,,,,Industry,,,,,,,Industry,,,,5514.598967884899,,,,,
Takane,Language,"Language modeling,Translation,Japanese language modeling,Language modeling/generation,Question answering","Fujitsu,Cohere",,2024-09-30,"Fujitsu launches ""Takane"" - A large language model for enterprises offering the highest Japanese language proficiency in the world",https://www.fujitsu.com/global/about/resources/news/press-releases/2024/0930-01.html,,,,,"""Takane is a medium-sized LLM that can be used in a secure, private environment.""

""Takane is based on Cohereâ€™s LLM Command R+ and combines Fujitsuâ€™s extensive knowledge in developing Japanese-specialized LLMs with Cohereâ€™s expertise in creating task-specific language models. """,,,Unspecified unreleased,,,,,,,,Unknown,"Fujitsu launches Takane, a Japanese-language large language model (LLM) designed for secure enterprise use
Integrated into Fujitsuâ€™s AI service Fujitsu Kozuchi and offered through the all-in-one operation platform Fujitsu Data Intelligence PaaS (DI PaaS)
Specialized and tailored for industry-specific use and trained for the Japanese language (world-class score on the JGLUE benchmark) to address the challenges of using LLMs in sensitive industries that require a high degree of accuracy and reliability",,,Hosted access (no API),"Japan,Canada",Command R+,,,,,2025-05-01 10:42,,,,,,"Industry,Industry",,,,,Unreleased,,"Industry,Industry",,,,,,,,,
PocketFlow,Biology,"Protein design,Drug discovery","University of Science and Technology of China (USTC),State Key Laboratory of Cognitive Intelligence,Harvard University","Zaixi Zhang, Marinka Zitnik, Qi Liu",2024-09-29,Generalized Protein Pocket Generation with Prior-Informed Flow Matching,https://arxiv.org/abs/2409.19520,,,,,,,,,,21000001.0,"CrossDocked: 100,000 complexes Ã— (100 residues + 50 atoms) = 15,000,000
MOAD: 40,000 pairs Ã— (100 residues + 50 atoms) = 6,000,000
Total: 15,000,000 + 6,000,000 = 21,000,000 datapoints = 2.1 Ã— 10â·",,,NVIDIA A100,,Likely,"Designing ligand-binding proteins, such as enzymes and biosensors, is essential in bioengineering and protein biology. One critical step in this process involves designing protein pockets, the protein interface binding with the ligand. Current approaches to pocket generation often suffer from time-intensive physical computations or template-based methods, as well as compromised generation quality due to the overlooking of domain knowledge. To tackle these challenges, we propose PocketFlow, a generative model that incorporates protein-ligand interaction priors based on flow matching. During training, PocketFlow learns to model key types of protein-ligand interactions, such as hydrogen bonds. In the sampling, PocketFlow leverages multi-granularity guidance (overall binding affinity and interaction geometry constraints) to facilitate generating high-affinity and valid pockets. Extensive experiments show that PocketFlow outperforms baselines on multiple benchmarks, e.g., achieving an average improvement of 1.29 in Vina Score and 0.05 in scRMSD. Moreover, modeling interactions make PocketFlow a generalized generative model across multiple ligand modalities, including small molecules, peptides, and RNA.",20.0,,,"China,China,United States of America",,,,1.0,,2025-05-01 10:42,,,,,,"Academia,Academia",,,,,,,"Academia,Academia",,,,432.8667014053512,Hardware,,,,
FlexSBDD,Biology,Drug discovery,"University of Science and Technology of China (USTC),State Key Laboratory of Cognitive Intelligence,Princeton University","Zaixi Zhang, Mengdi Wang, Qi Liu",2024-09-29,FlexSBDD: Structure-Based Drug Design with Flexible Protein Modeling,https://arxiv.org/abs/2409.19645,,,,,,1.6000000000000008e+19,"1. Hardware: 1x NVIDIA A100 PCIe GPU (3.12e14 FP16 Tensor FLOP/s)
2. Training duration: 36 hours (directly provided) = 129,600 seconds
3. Utilization: 40% (assumed)
4. Calculation: 3.12e14 FLOP/s Ã— 1 GPU Ã— 129,600s Ã— 0.4 = 1.6e19 FLOPs",,,42000000.0,"Original pairs: 40,000 + 100,000 = 140,000
Estimated length ~300 tokens: 140000*300=42000000",36.0,,NVIDIA A100,,Confident,"Structure-based drug design (SBDD), which aims to generate 3D ligand molecules binding to target proteins, is a fundamental task in drug discovery. Existing SBDD methods typically treat protein as rigid and neglect protein structural change when binding with ligand molecules, leading to a big gap with real-world scenarios and inferior generation qualities (e.g., many steric clashes). To bridge the gap, we propose FlexSBDD, a deep generative model capable of accurately modeling the flexible protein-ligand complex structure for ligand molecule generation. FlexSBDD adopts an efficient flow matching framework and leverages E(3)-equivariant network with scalar-vector dual representation to model dynamic structural changes. Moreover, novel data augmentation schemes based on structure relaxation/sidechain repacking are adopted to boost performance. Extensive experiments demonstrate that FlexSBDD achieves state-of-the-art performance in generating high-affinity molecules and effectively modeling the protein's conformation change to increase favorable protein-ligand interactions (e.g., Hydrogen bonds) and decrease steric clashes.",,,,"China,China,United States of America",,,,1.0,,2025-05-01 10:42,,,,,,"Academia,Academia",,,,,,,"Academia,Academia",,,,432.8667014053512,Hardware,,,,
ConoDL,Biology,Toxin prediction,"Chongqing University,Ministry of Natural Resources (China)","Menghan Guo, Zengpeng Li, Xuejin Deng, Ding Luo, Jingyi Yang, Yingjun Chen, Weiwei Xue",2024-09-28,ConoDL: A Deep Learning Framework for Rapid Generation and Prediction of Conotoxins,https://www.biorxiv.org/content/10.1101/2024.09.27.614001v1.abstract,,,,1200000000.0,"""ConoGen is a conotoxin generation model that adopts an architecture consistent with the pre-training model ProGen (Figure 2A) 15 . ConoGen is also constructed using a Transformer-based neural network architecture. One major advantage of the Transformer lies in its self-attention mechanism, which enables the model to encode distant signals of information when making sequence predictions, thereby affording it the capability to comprehend complex semantics. Its Transformer architecture consists of 36 layers, each comprising 8 self-attention heads, totaling 1.2 billion trainable parameters to ensure the training and optimization of the model.""",,,SwissProt,,650040.0,"ConoGen: 2310 sequences Ã— 40 tokens/sequence = 92,400 tokens
ConoPred: (2310 + 13,941) sequences Ã— 40 tokens/sequence = 650,040 tokens
Total: 92,400 + 650,040 = 742,440 tokens (7.4 Ã— 10âµ)",,,NVIDIA GeForce RTX 3080,,Confident,"Conotoxins, being small disulfide-rich and bioactive peptides, manifest notable pharmacological potential and find extensive applications. However, the exploration of conotoxinsâ€™ vast molecular space using traditional methods is severely limited, necessitating the urgent need of developing novel approaches. Recently, deep learning (DL)-based methods have advanced to the molecular generation of proteins and peptides. Nevertheless, the limited data and the intricate structure of conotoxins constrain the application of deep learning models in the generation of conotoxins. We propose ConoDL, a framework for the generation and prediction of conotoxins, comprising the end-to-end conotoxin generation model (ConoGen) and the conotoxin prediction model (ConoPred). ConoGen employs transfer learning and a large language model (LLM) to tackle the challenges in conotoxin generation. Meanwhile, ConoPred filters artificial conotoxins generated by ConoGen, narrowing down the scope for subsequent research. A comprehensive evaluation of the peptide properties at both sequence and structure levels indicates that the artificial conotoxins generated by ConoDL exhibit a certain degree of similarity to natural conotoxins. Furthermore, ConoDL has generated artificial conotoxins with novel cysteine scaffolds. Therefore, ConoDL may uncover new cysteine scaffolds and conotoxin molecules, facilitating further exploration of the molecular space of conotoxins and the discovery of pharmacologically active variants. The code is available at https://github.com/xueww/ConoDL.The supplementary material and model are available at https://zenodo.org/records/10679280.",15.0,,Open weights (non-commercial),"China,China",ProGen,,,1.0,,2025-06-13 15:16,,,,,,"Academia,Government",,,,,Open (non-commercial),"The code of ConoDL and other in-house script for data analysis are open for academic usage and available in GitHub (https://github.com/xueww/ConoDLï¼‰
Additionally, the supplementary material and model are deposited on Zenodo (https://zenodo.org/records/10679280).","Academia,Government",,,,346.3010729455784,,,,,
DeepREAD,Biology,Protein or nucleotide language model (pLM/nLM),Shape Therapeutics,"Yue Jiang, Lina R. Bagepalli, Bora S. Banjanin, Yiannis A. Savva, Yingxin Cao, Lan Guo, Adrian W. Briggs, Brian Booth, Ronald J. Hause",2024-09-28,Generative Machine Learning of ADAR Substrates for Precise and Efficient RNA Editing,https://www.biorxiv.org/content/10.1101/2024.09.27.613923v1.abstract,,,,,,,,,,12656000.0,112000 gRNAs *113 (sequence length)=12656000,,,,,Confident,"Adenosine Deaminase Acting on RNA (ADAR) converts adenosine to inosine within certain double-stranded RNA structures. However, ADARâ€™s promiscuous editing and poorly understood specificity hinder therapeutic applications. We present an integrated approach combining high-throughput screening (HTS) with generative deep learning to rapidly engineer efficient and specific guide RNAs (gRNAs) to direct ADARâ€™s activity to any target. Our HTS quantified ADAR-mediated editing across millions of unique gRNA sequences and structures, identifying key determinants of editing outcomes. We leveraged these data to develop DeepREAD (Deep learning for RNA Editing by ADAR Design), a diffusion-based model that elucidates complex design rules to generate novel gRNAs outperforming existing design heuristics. DeepREADâ€™s gRNAs achieve highly efficient and specific editing, including challenging multi-site edits. We demonstrate DeepREADâ€™s therapeutic potential by designing gRNAs targeting the MECP2R168X mutation associated with Rett syndrome, achieving both allelic specificity and species cross-reactivity. This approach significantly accelerates the development of ADAR-based RNA therapeutics for diverse genetic diseases.",,,,United States of America,,,,,,2025-05-01 10:42,,,,,,Industry,,,,,,,Industry,,,,,,,,,
Loop-Diffusion,Biology,Protein design,University of Washington,"Kevin Borisiak, Gian Marco Visani, Armita Nourmohammad
",2024-09-26,Loop-Diffusion: an equivariant diffusion model for designing and scoring protein loops,https://arxiv.org/abs/2409.18201,,,,,,,,,,433001.0,"433,000 total datapoints = 433k atomic neighborhoods from 20,000 protein structures
Final result: 4.33e5 datapoints",,,,,Confident,"Predicting protein functional characteristics from structure remains a central problem in protein science, with broad implications from understanding the mechanisms of disease to designing novel therapeutics. Unfortunately, current machine learning methods are limited by scarce and biased experimental data, and physics-based methods are either too slow to be useful, or too simplified to be accurate. In this work, we present Loop-Diffusion, an energy based diffusion model which leverages a dataset of general protein loops from the entire protein universe to learn an energy function that generalizes to functional prediction tasks. We evaluate Loop-Diffusion's performance on scoring TCR-pMHC interfaces and demonstrate state-of-the-art results in recognizing binding-enhancing mutations.",,,,United States of America,,,,,,2025-05-01 10:42,,,,,,Academia,,,,,,,Academia,,,,,,,,,
RWKV-5 (Eagle) 7B,Language,"Language modeling/generation,Question answering,Chat,Code generation,Translation","RWKV Foundation,EleutherAI,Ohio State University,University of California Santa Barbara (UCSB),Wroclaw Tech (WrocÅ‚aw University of Science and Technology),Guangdong Laboratory of Artificial Intelligence and Digital Economy (Pazhou Lab),New York University (NYU),Harvard University,Contextual AI,University of Chinese Academy of Sciences,University of California Santa Cruz,Tsinghua University,University of Edinburgh,University of British Columbia (UBC),Pennsylvania State University","Bo Peng, Daniel Goldstein, Quentin Anthony, Alon Albalak, Eric Alcaide, Stella Biderman, Eugene Cheah, Xingjian Du, Teddy Ferdinan, Haowen Hou, PrzemysÅ‚aw Kazienko, Kranthi Kiran GV, Jan KocoÅ„, BartÅ‚omiej Koptyra, Satyapriya Krishna, Ronald McClelland Jr., Jiaju Lin, Niklas Muennighoff, Fares Obeid, Atsushi Saito, Guangyu Song, Haoqin Tu, Cahya Wirawan, StanisÅ‚aw WoÅºniak, Ruichong Zhang, Bingchen Zhao, Qihang Zhao, Peng Zhou, Jian Zhu, Rui-Jie Zhu",2024-09-26,Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence,https://arxiv.org/abs/2404.05892,,,,7520000000.0,7.52Ã—10^9,4.983e+22,"4.53Ã—10^10 FLOPs per token (Table 11)

1100000000000 tokens *  4.53Ã—10^10 FLOPs per token = 4.983e+22

1100000000000 tokens * 7*10^9 parameters * 6 = 4.62e+22",RWKV World v2 Dataset,"multilingual 1.12 trillion token dataset drawn from a wide variety of hand selected publicly available data sources. The source data is approximately 70% English, 15% multilingual, and 15% code. ",1120000000000.0,1.1Ã—10^12 tokens ,,,NVIDIA H800 SXM5,,Confident,"We present Eagle (RWKV-5) and Finch (RWKV-6), sequence models improving upon the RWKV (RWKV-4) architecture. Our architectural design advancements include multi-headed matrix-valued states and a dynamic recurrence mechanism that improve expressivity while maintaining the inference efficiency characteristics of RNNs. We introduce a new multilingual corpus with 1.12 trillion tokens and a fast tokenizer based on greedy matching for enhanced multilinguality. We trained four Eagle models, ranging from 0.46 to 7.5 billion parameters, and two Finch models with 1.6 and 3.1 billion parameters and find that they achieve competitive performance across a wide variety of benchmarks. We release all our models on HuggingFace under the Apache 2.0 license. Models at: this https URL Training code at: this https URL Inference code at: this https URL Time-parallel training code at: this https URL",,,Open weights (unrestricted),"Multinational,Multinational,United States of America,United States of America,United States of America,Poland,China,United States of America,United States of America,United States of America,China,United States of America,China,United Kingdom of Great Britain and Northern Ireland,Canada,United States of America",,,,64.0,,2025-05-01 10:42,,,,2359296.0,,"Research collective,Research collective,Academia,Academia,Government,Academia,Academia,Industry,Academia,Academia,Academia,Academia,Academia,Academia",,,,,Open source,"apache 2
https://github.com/RWKV/RWKV-LM
https://huggingface.co/RWKV/v5-Eagle-7B-HF","Research collective,Research collective,Academia,Academia,Government,Academia,Academia,Industry,Academia,Academia,Academia,Academia,Academia,Academia",,,BF16,88241.44346194508,Operation counting,,,,
RWKV-6 (Finch) 3B,Language,"Language modeling/generation,Question answering,Chat,Code generation,Translation","RWKV Foundation,EleutherAI,Ohio State University,University of California Santa Barbara (UCSB),Wroclaw Tech (WrocÅ‚aw University of Science and Technology),Guangdong Laboratory of Artificial Intelligence and Digital Economy (Pazhou Lab),New York University (NYU),Harvard University,Contextual AI,University of Chinese Academy of Sciences,University of California Santa Cruz,Tsinghua University,University of Edinburgh,University of British Columbia (UBC),Pennsylvania State University","Bo Peng, Daniel Goldstein, Quentin Anthony, Alon Albalak, Eric Alcaide, Stella Biderman, Eugene Cheah, Xingjian Du, Teddy Ferdinan, Haowen Hou, PrzemysÅ‚aw Kazienko, Kranthi Kiran GV, Jan KocoÅ„, BartÅ‚omiej Koptyra, Satyapriya Krishna, Ronald McClelland Jr., Jiaju Lin, Niklas Muennighoff, Fares Obeid, Atsushi Saito, Guangyu Song, Haoqin Tu, Cahya Wirawan, StanisÅ‚aw WoÅºniak, Ruichong Zhang, Bingchen Zhao, Qihang Zhao, Peng Zhou, Jian Zhu, Rui-Jie Zhu",2024-09-26,Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence,https://arxiv.org/abs/2404.05892,,,,3100000000.0, 3.10Ã—10^9,2.0944e+22,"1.87Ã—10^10 FLOPs per token (Table 11)

1120000000000 tokens*1.87Ã—10^10 = 2.0944e+22

1120000000000 tokens *3*10^9 parameters *6 = 2.016e+22",RWKV World v2 Dataset,"multilingual 1.12 trillion token dataset drawn from a wide variety of hand selected publicly available data sources. The source data is approximately 70% English, 15% multilingual, and 15% code. ",1120000000000.0,,,,NVIDIA A100,,Confident,"We present Eagle (RWKV-5) and Finch (RWKV-6), sequence models improving upon the RWKV (RWKV-4) architecture. Our architectural design advancements include multi-headed matrix-valued states and a dynamic recurrence mechanism that improve expressivity while maintaining the inference efficiency characteristics of RNNs. We introduce a new multilingual corpus with 1.12 trillion tokens and a fast tokenizer based on greedy matching for enhanced multilinguality. We trained four Eagle models, ranging from 0.46 to 7.5 billion parameters, and two Finch models with 1.6 and 3.1 billion parameters and find that they achieve competitive performance across a wide variety of benchmarks. We release all our models on HuggingFace under the Apache 2.0 license. Models at: this https URL Training code at: this https URL Inference code at: this https URL Time-parallel training code at: this https URL",,,Open weights (unrestricted),"Multinational,Multinational,United States of America,United States of America,United States of America,Poland,China,United States of America,United States of America,United States of America,China,United States of America,China,United Kingdom of Great Britain and Northern Ireland,Canada,United States of America",,,,48.0,,2025-05-01 10:42,,,,786432.0,,"Research collective,Research collective,Academia,Academia,Government,Academia,Academia,Industry,Academia,Academia,Academia,Academia,Academia,Academia",,,,,Open source,"apache 2
https://github.com/RWKV/RWKV-LM
https://huggingface.co/RWKV/v6-Finch-3B-HF","Research collective,Research collective,Academia,Academia,Government,Academia,Academia,Industry,Academia,Academia,Academia,Academia,Academia,Academia",,,BF16,37817.761483690745,Operation counting,,,,
RNA-DCGen,Biology,"RNA sequence generation,Protein or nucleotide language model (pLM/nLM)","Bangladesh University of Engineering and Technology,University of California Riverside","Haz Sameen Shahgir, Md. Rownok Zahan Ratul, Md Toki Tahmid, Khondker Salman Sayeed, Atif Rahman",2024-09-25,RNA-DCGen: Dual Constrained RNA Sequence Generation with LLM-Attack,https://www.biorxiv.org/content/10.1101/2024.09.23.614570v1,0.0,,,117000000.0,,,,,,,,,,,,Confident,"Designing RNA sequences with specific properties is critical for developing personalized medications and therapeutics. While recent diffusion and flow-matching-based generative models have made strides in conditional sequence design, they face two key limitations: specialization for fixed constraint types, such as tertiary structures, and lack of flexibility in imposing additional conditions beyond the primary property of interest. To address these challenges, we introduce RNA-DCGen, a generalized framework for RNA sequence generation that is adaptable to any structural or functional properties through straightforward finetuning with an RNA language model (RNA-LM). Additionally, RNA-DCGen can enforce conditions on the generated sequences by fixing specific conserved regions. On RNA generation conditioned on RNA distance maps, RNA-DCGen generates sequences with an average R2 score of 0.625 compared to random sequences that score only 0.118 over 250 generations as judged by a separate more capable RNA-LM. When conditioned on RNA secondary structures, RNA-DCGen achieves an average F1 score of 0.4 against a random baseline of 0.006.",,,Unreleased,"India,United States of America",BiRNA-BERT,,,,,2025-06-11 18:07,,,,,,"Academia,Academia",,,,,Unreleased,,"Academia,Academia",,,,,,,,,
GeoAB,Biology,Protein design,"Zhejiang University (ZJU),Westlake University","Haitao Lin, Lirong Wu, Yufei Huang, Yunfan Liu, Odin Zhang, Yuanqing Zhou, Rui Sun, Stan Z. Li",2024-09-25,GeoAB: Towards Realistic Antibody Design and Reliable Affinity Maturation,https://www.biorxiv.org/content/10.1101/2024.05.15.594274v2.abstract,,,,,,,"Time per epoch: 154s (GeoAB-D, table 10)
Total training time: 154*20/3600=0.85",,,,,0.9,,,,Unknown,"Increasing works for antibody design are emerging to generate sequences and structures in Complementarity Determining Regions (CDRs), but problems still exist. We focus on two of them: (i) authenticity of the generated structure and (ii) rationality of the affinity maturation, and propose GEOAB as a solution. In specific, GeoABDesigner generates CDR structures with realistic internal geometries, composed of a generative geometry initializer (Geo-Initializer) and a position refiner (Geo-Refiner); GeoAB-Optimizer achieves affinity maturation by accurately predicting both the mutation effects and structures of mutant antibodies with the same network architecture as Geo-Refiner. Experiments show that GEOAB achieves state-of-the-art performance in CDR co-design and mutation effect predictions, and fulfills the discussed tasks effectively.",20.0,,,"China,China",,,,,,2025-05-01 10:42,,,,,,"Academia,Academia",,,,,,,"Academia,Academia",,,,,Hardware,,,,
PPFlow,Biology,Protein design,"Zhejiang University (ZJU),Westlake University","Haitao Lin, Odin Zhang, Huifeng Zhao, Dejun Jiang, Lirong Wu, Zicheng Liu, Yufei Huang, Stan Z. Li",2024-09-25,"PPFLOW: Target-aware Peptide Design with Torsional Flow Matching
",https://www.biorxiv.org/content/10.1101/2024.03.07.583831v5.abstract,,,,,,,,,,4850001.0,"Summary of calculations:
15,593 protein-peptide pairs
Average residues per pair = 300 (protein) + 12 (peptide) = 312
Total datapoints = 15,593 Ã— 312 = 4.85 Ã— 10^6",,,,,Likely,"Therapeutic peptides have proven to have great pharmaceutical value and potential in recent decades. However, methods of AI-assisted peptide drug discovery are not fully explored. To fill the gap, we propose a target-aware peptide design method called PPFlow, based on conditional flow matching on torus manifolds, to model the internal geometries of torsion angles for the peptide structure design. Besides, we establish a protein-peptide binding dataset named PPBench2024 to fill the void of massive data for the task of structure-based peptide drug design and to allow the training of deep learning methods. Extensive experiments show that PPFlow reaches state-of-the-art performance in tasks of peptide drug generation and optimization in comparison with baseline models, and can be generalized to other tasks including docking and side-chain packing.",,,,"China,China",,,,,,2025-05-01 10:42,,,,,,"Academia,Academia",,,,,,,"Academia,Academia",,,,,,,,,
SeaMoon,Biology,Protein folding prediction,"Sorbonne University,UniversitÃ© Grenoble Alpes,Institut Universitaire de France (IUF)","Valentin Lombard, Dan Timsit, Sergei Grudinin, Elodie Laine",2024-09-25,SeaMoon: Prediction of molecular motions based on language models,https://www.biorxiv.org/content/10.1101/2024.09.23.614585v1.abstract,,,,1000000.0,,,,,,11000001.0,"7,339 collections Ã— 5 conformations/collection = 36,695 conformations
36,695 conformations Ã— 300 residues/protein = 11,008,500 datapoints
Final estimate: 1.1e7 datapoints",,,,,Likely,"How protein move and deform determines their interactions with the environment and is thus of utmost importance for cellular functioning. Following the revolution in single protein 3D structure prediction, researchers have focused on repurposing or developing deep learning models for sampling alternative protein conformations. In this work, we explored whether continuous compact representations of protein motions could be predicted directly from protein sequences, without exploiting nor sampling protein structures. Our approach, called SeaMoon, leverages protein Language Model (pLM) embeddings as input to a lightweight (âˆ¼1M trainable parameters) convolutional neural network. SeaMoon achieves a success rate of up to 40% when assessed against âˆ¼1 000 collections of experimental conformations exhibiting a wide range of motions. SeaMoon capture motions not accessible to the normal mode analysis, an unsupervised physics-based method relying solely on a protein structureâ€™s 3D geometry, and generalises to proteins that do not have any detectable sequence similarity to the training set. SeaMoon is easily retrainable with novel or updated pLMs.",500.0,,,"France,France,France",,,,,,2025-05-01 10:42,,,,,,"Academia,Academia,Research collective",,,,,,,"Academia,Academia,Research collective",,,,,,,,,
CodonMPNN,Biology,Codon design,"Harvard Medical School,Massachusetts Institute of Technology (MIT)","Hannes Stark, Umesh Padia, Julia Balla, Cameron Diao, George Church",2024-09-25,CodonMPNN for Organism Specific and Codon Optimal Inverse Folding,https://arxiv.org/abs/2409.17265,,,,,,,, AlphaFold database (AFDB),,,,,,,,Unknown,"Generating protein sequences conditioned on protein structures is an impactful technique for protein engineering. When synthesizing engineered proteins, they are commonly translated into DNA and expressed in an organism such as yeast. One difficulty in this process is that the expression rates can be low due to suboptimal codon sequences for expressing a protein in a host organism. We propose CodonMPNN, which generates a codon sequence conditioned on a protein backbone structure and an organism label. If naturally occurring DNA sequences are close to codon optimality, CodonMPNN could learn to generate codon sequences with higher expression yields than heuristic codon choices for generated amino acid sequences. Experiments show that CodonMPNN retains the performance of previous inverse folding approaches and recovers wild-type codons more frequently than baselines. Furthermore, CodonMPNN has a higher likelihood of generating high-fitness codon sequences than low-fitness codon sequences for the same protein sequence. Code is available at this https URL.",,,Open weights (unrestricted),"United States of America,United States of America",,,,,,2025-06-16 15:44,,,,,,"Academia,Academia",,,,,Open source,"MIT license
https://github.com/HannesStark/CodonMPNN","Academia,Academia",,,,,,,,,
Molmo 72B,"Language,Vision,Multimodal","Language modeling/generation,Visual question answering,Question answering","Allen Institute for AI,University of Washington","Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, Jiasen Lu, Taira Anderson, Erin Bransom, Kiana Ehsani, Huong Ngo, YenSung Chen, Ajay Patel, Mark Yatskar, Chris Callison-Burch, Andrew Head, Rose Hendrix, Favyen Bastani, Eli VanderBilt, Nathan Lambert, Yvonne Chou, Arnavi Chheda, Jenna Sparks, Sam Skjonsberg, Michael Schmitz, Aaron Sarnat, Byron Bischoff, Pete Walsh, Chris Newell, Piper Wolters, Tanmay Gupta, Kuo-Hao Zeng, Jon Borchardt, Dirk Groeneveld, Jen Dumas, Crystal Nam, Sophie Lebrecht, Caitlin Wittlif, Carissa Schoenick, Oscar Michel, Ranjay Krishna, Luca Weihs, Noah A. Smith, Hannaneh Hajishirzi, Ross Girshick, Ali Farhadi, Aniruddha Kembhavi",2024-09-25,Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Multimodal Models,https://arxiv.org/abs/2409.17146,,,"Our best-in-class Molmo model, Molmo-72B, achieves the highest academic benchmark score and ranks second on human evaluation, just slightly behind GPT-4o.
Our best Molmo model also outperforms several state-of-the-art proprietary systems, including Gemini 1.5 Pro and Flash and Claude 3.5 Sonnet.",72000000000.0,,1.33583e+22,"from Table 8:
h100 gpus [assuming NVIDIA H100 SXM5 80GB]
pre-train 128 gpus 33.3 hours 4.2k gpu-hours
fine-tune 256 gpus 32.4 hours 8.3k gpu-hours

989500000000000 FLOP/sec [bf16 precision] * 3600 s/hour * 0.3 [assumed utilization] * (4200 + 8300) = 1.33583Ã—10^22 FLOP

""We set a maximum sequence length of 2304 for both pre-training and fine-tuning""
from Table 6:
6 FLOP/parameter/token * 2304 max tokens / sequence * 72B paramters 
[Pre-train]: ((128 sequences [batch size] * 22300 steps) +
[Fine-tune]: (256 sequences [batch size] * 20000 steps)) = 7.9371436032 Ã— 10^21 FLOP

hardware estimation seems more correct since operation counting doesn't account for vision module training",PixMo,,,"""In total, we trained on 712k distinct images with âˆ¼1.3M captions (including the augmentation).""",,"Table 8:

pre-train 128 gpus 33.3 hours 4.2k gpu-hours
fine-tune 256 gpus 32.4 hours 8.3k gpu-hours",NVIDIA H100 SXM5 80GB,,Confident,"Today's most advanced multimodal models remain proprietary. The strongest open-weight models rely heavily on synthetic data from proprietary VLMs to achieve good performance, effectively distilling these closed models into open ones. As a result, the community is still missing foundational knowledge about how to build performant VLMs from scratch. We present Molmo, a new family of VLMs that are state-of-the-art in their class of openness. Our key innovation is a novel, highly detailed image caption dataset collected entirely from human annotators using speech-based descriptions. To enable a wide array of user interactions, we also introduce a diverse dataset mixture for fine-tuning that includes in-the-wild Q&A and innovative 2D pointing data. The success of our approach relies on careful choices for the model architecture details, a well-tuned training pipeline, and, most critically, the quality of our newly collected datasets, all of which will be released. The best-in-class 72B model within the Molmo family not only outperforms others in the class of open weight and data models but also compares favorably against proprietary systems like GPT-4o, Claude 3.5, and Gemini 1.5 on both academic benchmarks and human evaluation.
We will be releasing all of our model weights, captioning and fine-tuning data, and source code in the near future. Select model weights, inference code, and demo are available at this https URL.",,,Open weights (unrestricted),"United States of America,United States of America","Qwen2-72B,CLIP (ViT L/14@336px)",,,,,2025-05-30 14:38,,,,,,"Research collective,Academia",,,,,Open source,"Apache 2

This checkpoint is a preview of the Molmo release. All artifacts used in creating Molmo (PixMo dataset, training code, evaluations, intermediate checkpoints) will be made available at a later date, furthering our commitment to open-source AI development and reproducibility.
https://huggingface.co/allenai/Molmo-72B-0924
https://github.com/allenai/molmo","Research collective,Academia",,,BF16,,"Hardware,Operation counting",,,,
dnaGrinder,Biology,Protein or nucleotide language model (pLM/nLM),Hong Kong Polytechnic University,"Qihang Zhao, Chi Zhang, Weixiong Zhang",2024-09-24,dnaGrinder: a lightweight and high-capacity genomic foundation model,https://arxiv.org/abs/2409.15697,0.0,,,63600000.0,,2.7713e+19,"1. Hardware setup:
- Initial: 8x NVIDIA H100 SXM5 (7.56e14 FLOP/s per GPU)
- Further: 1x NVIDIA A800 (7.80e13 FLOP/s)

2. Training duration (calculated from steps):
- Initial: 119,000 steps Ã— (256Ã—2314) tokens/step = 7.04e10 tokens
- Further: 31,000 steps Ã— (32Ã—2241) tokens/step = 2.22e9 tokens

4. Final calculation:
Initial: 6 Ã— 6.36e7 params Ã— 7.04e10 tokens = 2.68e19 FLOPs
Further: 6 Ã— 6.36e7 params Ã— 2.22e9 tokens = 8.46e17 FLOPs
Total: 2.7712964e+19",,,69400000001.0,"Main Pretraining: 69 billion
Further Pretraining: 0.41 billion
Total: 69 + 0.41 = 69.41 billion (6.94e10) tokens",,,NVIDIA H100 SXM5 80GB,,Confident,"The task of understanding and interpreting the complex information encoded within genomic sequences remains a grand challenge in biological research and clinical applications. In this context,
recent advancements in large language model research have led to the development of both encoderonly and decoder-only foundation models designed to decode intricate information in DNA sequences.
However, several issues persist, particularly regarding the efficient management of long-range dependencies inherent in genomic sequences, the effective representation of nucleotide variations, and the
considerable computational costs associated with large model architectures and extensive pretraining
datasets. Current genomic foundation models often face a critical tradeoff: smaller models with
mediocre performance versus large models with improved performance. To address these challenges,
we introduce dnaGrinder, a unique and efficient genomic foundation model. dnaGrinder excels at
managing long-range dependencies within genomic sequences while minimizing computational costs
without compromising performance. It achieves results that are not just comparable but often superior
to leading DNA models such as Nucleotide Transformer and DNABERT-2. Furthermore, dnaGrinder
is designed for easy fine-tuning on workstation-grade GPUs, accommodating input lengths exceeding
17,000 tokens. On a single high-performance GPU, it supports sequences longer than 140,000
tokens, making it a highly efficient and accessible tool for both basic biological research and clinical
applications.",,,,"Hong Kong,China",,,,8.0,,2025-05-01 10:42,,,,,,Academia,,,,,,,Academia,,,,11030.67171405016,,,,,
Protein-Mamba,Biology,Protein or nucleotide language model (pLM/nLM),"Rensselaer Polytechnic Institute,Stanford University,University of Minnesota,Korea Advanced Institute of Science and Technology (KAIST),University of Illinois Urbana-Champaign (UIUC)","Bohao Xu, Yingzhou Lu, Yoshitaka Inoue, Namkyeong Lee, Tianfan Fu, Jintai Chen",2024-09-24,"Protein-Mamba: Biological Mamba Models for Protein Function
Prediction",https://arxiv.org/abs/2409.14617,0.0,,,,,,"
",,,8100001.0,"Total number of datapoints = 27,043 proteins Ã— 300 amino acids/protein = 8,112,900

This equals approximately 8.1 Ã— 10â¶ datapoints

27,043 Ã— 300 = 8,112,900",,,NVIDIA GeForce RTX 3090,,Confident,"Protein function prediction is a pivotal task in drug discovery, significantly impacting the development of effective and safe therapeutics. Traditional machine learning models often struggle with the complexity and variability inherent in predicting protein functions, necessitating more sophisticated approaches. In this work, we
introduce Protein-Mamba, a novel two-stage model that
leverages both self-supervised learning and fine-tuning
to improve protein function prediction. The pre-training
stage allows the model to capture general chemical structures and relationships from large, unlabeled datasets,
while the fine-tuning stage refines these insights using
specific labeled datasets, resulting in superior prediction
performance. Our extensive experiments demonstrate that
Protein-Mamba achieves competitive performance, compared with a couple of state-of-the-art methods across a
range of protein function datasets. This modelâ€™s ability to
effectively utilize both unlabeled and labeled data highlights the potential of self-supervised learning in advancing protein function prediction and offers a promising direction for future research in drug discovery.",150.0,,,"United States of America,United States of America,United States of America,Korea (Republic of),United States of America",,,,1.0,,2025-05-01 10:42,,,,,,"Academia,Academia,Academia,Academia,Academia",,,,,,,"Academia,Academia,Academia,Academia,Academia",,,,378.8005396308434,Hardware,,,,
ProtBFN,Biology,Protein or nucleotide language model (pLM/nLM),InstaDeep,"Timothy Atkinson, Thomas D. Barrett, Scott Cameron, Bora Guloglu, Matthew Greenig, Louis Robinson, Alex Graves, Liviu Copoiu, Alexandre Laterre",2024-09-24,Protein Sequence Modelling with Bayesian Flow Networks,https://www.biorxiv.org/content/10.1101/2024.09.24.614734v1.abstract,1.0,,,650000000.0,,3.900000000000003e+22,"1. Hardware setup:
- ProtBFN: 256 TPU v4 chips (2.75e14 FLOP/s per chip)
- AbBFN: 128 TPU v4 chips (2.75e14 FLOP/s per chip)

2. Training duration:
- ProtBFN: 14 days = 1,209,600 seconds
- AbBFN: 4 days = 345,600 seconds

3. Utilization rate: 40%

4. Final calculation:
- ProtBFN: 2.75e14 Ã— 256 Ã— 1,209,600 Ã— 0.4 = 3.406e22 FLOPs
- AbBFN: 2.75e14 Ã— 128 Ã— 345,600 Ã— 0.4 = 4.866e21 FLOPs
- Total: 3.406e22 + 4.866e21 = 3.9e22 FLOPs",UniProtKB,,21000000001.0,"Total tokens = 71,000,000 sequences Ã— 300 residues = 21,300,000,000 tokens (2.13Ã—10Â¹â°)
Alternative calculation with 256 residues:
71,000,000 sequences Ã— 256 residues = 18,176,000,000 tokens (1.82Ã—10Â¹â°)
Final estimate: 2.1Ã—10Â¹â° tokens",432.0,,Google TPU v4,,Confident,"Exploring the vast and largely uncharted territory of amino acid sequences is crucial for understanding complex protein functions and the engineering of novel therapeutic proteins. Whilst generative machine learning has advanced protein sequence modelling, no existing approach is proficient for both unconditional and conditional generation. In this work, we propose that Bayesian Flow Networks (BFNs), a recently introduced framework for generative modelling, can address these challenges. We present ProtBFN, a 650M parameter model trained on protein sequences curated from UniProtKB, which generates natural-like, diverse, structurally coherent, and novel protein sequences, significantly outperforming leading autoregressive and discrete diffusion models. Further, we fine-tune ProtBFN on heavy chains from the Observed Antibody Space (OAS) to obtain an antibody-specific model, AbBFN, which we use to evaluate zero-shot conditional generation capabilities. AbBFN is found to be competitive with, or better than, antibody-specific BERT-style models, when applied to predicting individual framework or complimentary determining regions (CDR).",,,,United Kingdom of Great Britain and Northern Ireland,,,,256.0,,2025-05-01 10:42,,,,,,Industry,,,,,,,Industry,,,,85724.0773206184,Hardware,,,,
Llama 3.2 11B,"Multimodal,Vision,Language","Visual question answering,Image captioning,Object detection",Meta AI,Meta AI,2024-09-24,"Llama 3.2: Revolutionizing edge AI and vision with open, customizable models",https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/,,Significant use,"1,019,539 downloads on HuggingFace last month at time of writing (https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct).",10600000000.0,https://huggingface.co/meta-llama/Llama-3.2-11B-Vision,5.79e+23,"Tensor type is BF16 (https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct).

â€œTraining utilized a cumulative of 2.02M GPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiencyâ€¦ Training time: Stage 1 pretraining: 147K H100 hours Stage 2 annealing: 98K H100 hours SFT: 896 H100 hours RLHF: 224 H100 hours.â€ (https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD_VISION.md#hardware-and-software).

The Nvidia H100 80GB is the H100 SXM. BFLOAT16 Tensor Core peak FLOPS with sparsity is 1,979 teraFLOPS (https://resources.nvidia.com/en-us-tensor-core/nvidia-tensor-core-gpu-datasheet?ncid=no-ncid).

Assuming 33% utilization rate,
Training compute
~= 0.33 * ( 147000 + 98000 + 896 + 224 ) hours * 3600 s / hour * 1979e12 FLOPS / GPU
~= 5.79e23 FLOPS",Unspecified unreleased,"""Llama 3.2-Vision was pretrained on 6B image and text pairs. The instruction tuning data includes publicly available vision instruction datasets, as well as over 3M synthetically generated examples... The pretraining data has a cutoff of December 2023."" (https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD_VISION.md#training-data).",6000000000.0,"Since the model can also be used for image captioning, I assume the dataset size is measured in numbers of image-caption pairs (https://docs.google.com/document/d/1XWLyMzcVfDv4eFQX3yPgM8MZ3_Q1phtIFz9GKv4_KaM/edit?tab=t.0#heading=h.ny4fw3njk98p). """"Llama 3.2-Vision was pretrained on 6B image and text pairs"" (https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD_VISION.md#training-data).",,"""Stage 1 pretraining: 147K H100 hours Stage 2 annealing: 98K H100 hours SFT: 896 H100 hours RLHF: 224 H100 hours""

https://huggingface.co/meta-llama/Llama-3.2-11B-Vision",NVIDIA H100 SXM5 80GB,,Confident,"Today, weâ€™re releasing Llama 3.2, which includes small and medium-sized vision LLMs (11B and 90B), and lightweight, text-only models (1B and 3B) that fit onto edge and mobile devices, including pre-trained and instruction-tuned versions.
Supported by a broad ecosystem, the Llama 3.2 11B and 90B vision models are drop-in replacements for their corresponding text model equivalents, while exceeding on image understanding tasks compared to closed models, such as Claude 3 Haiku. Unlike other open multimodal models, both pre-trained and aligned models are available to be fine-tuned for custom applications using torchtune and deployed locally using torchchat. Theyâ€™re also available to try using our smart assistant, Meta AI.
Weâ€™re sharing the first official Llama Stack distributions, which will greatly simplify the way developers work with Llama models in different environments, including single-node, on-prem, cloud, and on-device, enabling turnkey deployment of retrieval-augmented generation (RAG) and tooling-enabled applications with integrated safety.
Weâ€™ve been working closely with partners like AWS, Databricks, Dell Technologies, Fireworks, Infosys, and Together AI to build Llama Stack distributions for their downstream enterprise clients. On-device distribution is via PyTorch ExecuTorch, and single-node distribution is via Ollama.
We continue to share our work because we believe openness drives innovation and is good for developers, Meta, and the world. Llama is already leading the way on openness, modifiability, and cost efficiencyâ€”enabling more people to have creative, useful, and life-changing breakthroughs using generative AI.
Weâ€™re making Llama 3.2 models available for download on llama.com and Hugging Face, as well as available for immediate development on our broad ecosystem of partner platforms, including AMD, AWS, Databricks, Dell, Google Cloud, Groq, IBM, Intel, Microsoft Azure, NVIDIA, Oracle Cloud, Snowflake, and more.",,,Open weights (restricted use),United States of America,Llama 3.1-8B,3,147000+98000+896+224 GPU-hours => 246120 GPU-hours * 60 * 60 * 989e12 FLOP * 0.4 (utilization) = 3.5e23 FLOP,,,2025-05-16 10:30,,,,,,Industry,,,,246120.0,Unreleased,"LLAMA 3.2 COMMUNITY LICENSE AGREEMENT

https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/LICENSE",Industry,,,,,Hardware,,,,
Llama 3.2 90B,"Multimodal,Vision,Language","Visual question answering,Image captioning,Object detection",Meta AI,,2024-09-24,"Llama 3.2: Revolutionizing edge AI and vision with open, customizable models",https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/,,,,88600000000.0,https://huggingface.co/meta-llama/Llama-3.2-90B-Vision,,,Unspecified unreleased,"Knowledge cutoff date is December 2023, according to https://huggingface.co/meta-llama/Llama-3.2-11B-Vision.",6000000000.0,"""6B (image, text) pairs""",,"""Stage 1 pretraining: 885K H100 hours Stage 2 annealing: 885K H100 hours SFT: 3072 H100 hours RLHF: 2048 H100 hours""

https://huggingface.co/meta-llama/Llama-3.2-90B-Vision",NVIDIA H100 SXM5 80GB,,Confident,"Today, weâ€™re releasing Llama 3.2, which includes small and medium-sized vision LLMs (11B and 90B), and lightweight, text-only models (1B and 3B) that fit onto edge and mobile devices, including pre-trained and instruction-tuned versions.
Supported by a broad ecosystem, the Llama 3.2 11B and 90B vision models are drop-in replacements for their corresponding text model equivalents, while exceeding on image understanding tasks compared to closed models, such as Claude 3 Haiku. Unlike other open multimodal models, both pre-trained and aligned models are available to be fine-tuned for custom applications using torchtune and deployed locally using torchchat. Theyâ€™re also available to try using our smart assistant, Meta AI.
Weâ€™re sharing the first official Llama Stack distributions, which will greatly simplify the way developers work with Llama models in different environments, including single-node, on-prem, cloud, and on-device, enabling turnkey deployment of retrieval-augmented generation (RAG) and tooling-enabled applications with integrated safety.
Weâ€™ve been working closely with partners like AWS, Databricks, Dell Technologies, Fireworks, Infosys, and Together AI to build Llama Stack distributions for their downstream enterprise clients. On-device distribution is via PyTorch ExecuTorch, and single-node distribution is via Ollama.
We continue to share our work because we believe openness drives innovation and is good for developers, Meta, and the world. Llama is already leading the way on openness, modifiability, and cost efficiencyâ€”enabling more people to have creative, useful, and life-changing breakthroughs using generative AI.
Weâ€™re making Llama 3.2 models available for download on llama.com and Hugging Face, as well as available for immediate development on our broad ecosystem of partner platforms, including AMD, AWS, Databricks, Dell, Google Cloud, Groq, IBM, Intel, Microsoft Azure, NVIDIA, Oracle Cloud, Snowflake, and more.",,,Open weights (restricted use),United States of America,Llama 3.1-70B,2,885000Ã—2+3072+2048 GPU-hours => 1775120 GPU-hours * 60 * 60 * 989e12 FLOP * 0.4 (utilization) = 2.5e24 FLOP,,,2025-05-30 14:36,,,,,,Industry,,,,1770000.0,Unreleased,"LLAMA 3.2 COMMUNITY LICENSE AGREEMENT

https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/LICENSE",Industry,,,,,Hardware,,,,
Llama 3.2 1B,Language,"Language modeling/generation,Text summarization,Question answering,Quantitative reasoning,Translation",Meta AI,,2024-09-24,"Llama 3.2: Revolutionizing edge AI and vision with open, customizable models",https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/,,,,1230000000.0,https://huggingface.co/meta-llama/Llama-3.2-1B,6.642000000000001e+22,"6ND = 6*1230000000.00*9000000000000 = 6.642e+22

370000 hours * 3600 s * 133800000000000 FLOPS/s* 0.3 = 5.346648e+22",Unspecified unreleased,"Knowledge cutoff date is December 2023, according to, https://huggingface.co/meta-llama/Llama-3.2-1B.",9000000000000.0,"""Llama 3.2 was pretrained on up to 9 trillion tokens of data from publicly available sources.""",,https://huggingface.co/meta-llama/Llama-3.2-1B,NVIDIA H100 SXM5 80GB,,Confident,"Today, weâ€™re releasing Llama 3.2, which includes small and medium-sized vision LLMs (11B and 90B), and lightweight, text-only models (1B and 3B) that fit onto edge and mobile devices, including pre-trained and instruction-tuned versions.
The Llama 3.2 1B and 3B models support context length of 128K tokens and are state-of-the-art in their class for on-device use cases like summarization, instruction following, and rewriting tasks running locally at the edge. These models are enabled on day one for Qualcomm and MediaTek hardware and optimized for Arm processors.
Weâ€™re sharing the first official Llama Stack distributions, which will greatly simplify the way developers work with Llama models in different environments, including single-node, on-prem, cloud, and on-device, enabling turnkey deployment of retrieval-augmented generation (RAG) and tooling-enabled applications with integrated safety.
Weâ€™ve been working closely with partners like AWS, Databricks, Dell Technologies, Fireworks, Infosys, and Together AI to build Llama Stack distributions for their downstream enterprise clients. On-device distribution is via PyTorch ExecuTorch, and single-node distribution is via Ollama.
We continue to share our work because we believe openness drives innovation and is good for developers, Meta, and the world. Llama is already leading the way on openness, modifiability, and cost efficiencyâ€”enabling more people to have creative, useful, and life-changing breakthroughs using generative AI.
Weâ€™re making Llama 3.2 models available for download on llama.com and Hugging Face, as well as available for immediate development on our broad ecosystem of partner platforms, including AMD, AWS, Databricks, Dell, Google Cloud, Groq, IBM, Intel, Microsoft Azure, NVIDIA, Oracle Cloud, Snowflake, and more.",,,Open weights (restricted use),United States of America,,,,,,2025-05-12 23:35,,,,,,Industry,,,,370000.0,Unreleased,"LLAMA 3.2 COMMUNITY LICENSE AGREEMENT

https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/LICENSE",Industry,,,,,"Operation counting,Hardware",,,,
Llama 3.2 3B,Language,"Language modeling/generation,Text summarization,Question answering,Quantitative reasoning,Translation",Meta AI,,2024-09-24,"Llama 3.2: Revolutionizing edge AI and vision with open, customizable models",https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/,,,,3210000000.0,https://huggingface.co/meta-llama/Llama-3.2-1B,1.7334e+23,"6ND = 6*3210000000.00*9000000000000 = 1.7334e+23

460000 hours * 3600 s * 133800000000000 FLOPS/s* 0.3 = 6.647184e+22",Unspecified unreleased,"Knowledge cutoff date is December 2023, according to, https://huggingface.co/meta-llama/Llama-3.2-1B.",9000000000000.0,"""Llama 3.2 was pretrained on up to 9 trillion tokens of data from publicly available sources.""",,https://huggingface.co/meta-llama/Llama-3.2-1B,NVIDIA H100 SXM5 80GB,,Confident,"Today, weâ€™re releasing Llama 3.2, which includes small and medium-sized vision LLMs (11B and 90B), and lightweight, text-only models (1B and 3B) that fit onto edge and mobile devices, including pre-trained and instruction-tuned versions.
The Llama 3.2 1B and 3B models support context length of 128K tokens and are state-of-the-art in their class for on-device use cases like summarization, instruction following, and rewriting tasks running locally at the edge. These models are enabled on day one for Qualcomm and MediaTek hardware and optimized for Arm processors.
Weâ€™re sharing the first official Llama Stack distributions, which will greatly simplify the way developers work with Llama models in different environments, including single-node, on-prem, cloud, and on-device, enabling turnkey deployment of retrieval-augmented generation (RAG) and tooling-enabled applications with integrated safety.
Weâ€™ve been working closely with partners like AWS, Databricks, Dell Technologies, Fireworks, Infosys, and Together AI to build Llama Stack distributions for their downstream enterprise clients. On-device distribution is via PyTorch ExecuTorch, and single-node distribution is via Ollama.
We continue to share our work because we believe openness drives innovation and is good for developers, Meta, and the world. Llama is already leading the way on openness, modifiability, and cost efficiencyâ€”enabling more people to have creative, useful, and life-changing breakthroughs using generative AI.
Weâ€™re making Llama 3.2 models available for download on llama.com and Hugging Face, as well as available for immediate development on our broad ecosystem of partner platforms, including AMD, AWS, Databricks, Dell, Google Cloud, Groq, IBM, Intel, Microsoft Azure, NVIDIA, Oracle Cloud, Snowflake, and more.",,,Open weights (restricted use),United States of America,,,,,,2025-05-16 10:30,,,,,,Industry,,,,460000.0,Unreleased,"LLAMA 3.2 COMMUNITY LICENSE AGREEMENT

https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/LICENSE",Industry,,,,,"Operation counting,Hardware",,,,
DrugTar,Biology,"Protein property prediction,Drug discovery",Isfahan University of Technology,"Niloofar Borhani, Iman Izadi,  View ORCID ProfileAli Motahharynia, Mahsa Sheikholeslami, Yousof Gheisari",2024-09-24,DrugTar Improves Druggability Prediction by Integrating Large Language Models and Gene Ontologies,https://www.biorxiv.org/content/10.1101/2024.09.21.614218v1.abstract,,,,650000000.0,,,,,,1220400.0,4068 (ProTar-II dataset)*300(estimated tokens per protein)=1220400,,,,,Likely,"Target discovery is crucial in drug development, especially for complex chronic diseases. Recent advances in high-throughput technologies and the explosion of biomedical data have highlighted the potential of computational druggability prediction methods. However, most current methods rely on sequence-based features with machine learning, which often face challenges related to hand-crafted features, reproducibility, and accessibility. Moreover, the potential of raw sequence and protein structure has not been fully investigated. Here, we leveraged both protein sequence and structure using deep learning techniques, revealing that protein sequence, especially pre- trained embeddings, is more informative than protein structure. Next, we developed DrugTar, a highl7lperformance deep learning algorithm integrating sequence embeddings from the ESM-2 pre-trained protein language model with protein ontologies to predict druggability. DrugTar achieved areas under the curve and precision-recall curve values above 0.90, outperforming state-of-the-art methods. In conclusion, DrugTar streamlines target discovery as a bottleneck in developing novel therapeutics.",15.0,,,Iran (Islamic Republic of),ESM2-650M,,,,,2025-05-01 10:42,,,,,,Academia,,,,,,,Academia,,,,,,,,,
Thermostable protein design,Biology,Protein design,"""Indraprastha Institute of Information Technology
Delhi""","Purva Tijare, Nishant Kumar, Gajendra P. S. Raghava",2024-09-24,Designing of thermostable proteins with a desired melting temperature,https://www.biorxiv.org/content/10.1101/2024.09.21.614294v1.abstract,,,,738000000.0,"""This model comprises 36  layers and has a dimensionality of 1280, amounting to a total of 738 million parameters [28].""",1.8397012e+16,6*4154700*738000000=1.8397012e+16,,,4154700.0,"""Training Dataset 13849 sequences"" in Figure 1
Assumed length of 300 tokens per sequence: 13849*300",,,,,Confident,"The stability of proteins at higher temperatures is crucial for its functionality that is measured by their melting temperature (Tm). The Tm is the temperature at which 50% of the protein loses its native structure and activity. Existing methods for predicting Tm have two major limitations: first, they are often trained on redundant proteins, and second, they do not allow users to design proteins with the desired Tm. To address these limitations, we developed a regression method for predicting the Tm value of proteins using 17,312 non-redundant proteins, where no two proteins are more than 40% similar. We used 80% of the data for training and testing; remaining 20% of the data for validation. Initially, we developed a machine learning model using standard features from protein sequences. Our best model, developed using Shannon entropy for all residues, achieved the highest Pearson correlation of 0.80 with an RÂ² of 0.63 between the predicted and actual Tm of proteins on the validation dataset. Next, we fine-tuned large language models (e.g., ProtBert, ProtGPT2, ProtT5) on our training dataset and generated embeddings. These embeddings have been used for developing machine learning models. Our best model, developed using ProtBert embeddings, achieved a maximum correlation of 0.89 with an RÂ² of 0.80 on the validation dataset. Finally, we developed an ensemble method that combines standard protein features and embeddings. One of the aims of the study is to assist the scientific community in the design of targeted melting temperatures. We created a user-friendly web server and a python package for predicting and designing thermostable proteins. Our standalone software can be used to screen thermostable proteins in genomes and metagenomes. We demonstrated the application of PPTstab in identifying thermostable proteins in different organisms from their genomes, the model and data is available at: https://webs.iiitd.edu.in/raghava/pptstab.",,,,India,,,,,,2025-05-20 10:55,,,,,,Academia,,,,,,,Academia,,,,,Operation counting,,,,
Jaeger,Biology,Bacteriophage screening,"University Medicine Greifswald,Utrecht University,Friedrich Schiller University Jena","Yasas Wijesekara, Ling-Yi Wu, Rick Beeloo, Piotr Rozwalak, Ernestina Hauptfeld, Swapnil P. Doijad, Bas E. Dutilh, Lars Kaderali",2024-09-24,Jaeger: an accurate and fast deep-learning tool to detect bacteriophage sequences,https://www.biorxiv.org/content/10.1101/2024.09.24.612722v1.abstract,,,,943964.0,,,,,,2112001.0,"Total fragments = 265,959 + 1,375,939 + 915,817 + 459,660 = 3,017,375
Training set (70%) = 3,017,375 Ã— 0.70 = 2,112,162.5
Final estimate â‰ˆ 2.112 Ã— 10â¶ unique data points",,,NVIDIA A100,,Confident,"Viruses are integral to every biome on Earth, yet we still need a more comprehensive picture of their identity and global distribution. Global metagenomics sequencing efforts revealed the genomic content of tens of thousands of environmental samples, however identifying the viral sequences in these datasets remains challenging due to their vast genomic diversity. Here, we address identifying bacteriophage sequences in unlabeled sequencing data. In a recent benchmarking paper, we observed that existing deep-learning tools show a high true positive rate, but may also produce many false positives when confronted with divergent sequences. To tackle this challenge, we introduce Jaeger, a novel deep-learning method designed specifically for identifying bacteriophage genome fragments. Extensive benchmarking on the IMG/VR database and real-world metagenomes reveals Jaegerâ€™s consistent high sensitivity (0.87) and precision (0.92). Applying Jaeger to over 16,000 metagenomic assemblies from the MGnify database yielded over five million putative phage contigs. On average, Jaeger is around 20 times faster than the other state-of-the-art methods. Jaeger is available at https://github.com/MGXlab/Jaeger.

",,,,"Germany,Netherlands,Germany",,,,4.0,,2025-05-01 10:42,,,,,,"Academia,Academia,Academia",,,,,,,"Academia,Academia,Academia",,,,3151.620489728617,,,,,
AlphaMut,Biology,Mutation prediction,Indian Institute of Science Education and Research,"Prathith Bhargav,  Arnab Mukherjee",2024-09-24,AlphaMut: a deep reinforcement learning model to suggest helix-disrupting mutations,https://www.biorxiv.org/content/10.1101/2024.09.21.614241v1.abstract,,,,,,,,,,491625.0,"New estimate: 
Helix-only: 1874*30=56220
Helix-in-protein: 32775*15=491625

Old estimate:
Total Data Points = Helix-only Training Steps + Helix-in-protein Training Steps
168,000 + 190,000 = 358,000 = 3.58e5 datapoints",,,,,Confident,"Helices are important secondary structural motifs within proteins and are pivotal in numerous physiological processes. While amino acids (AA) such as alanine and leucine are known to promote helix formation, proline and glycine disfavor it. Helical structure formation, however, also depends on its environment, and hence, prior prediction of a mutational effect on a helical structure is difficult. Here, we employ a reinforcement learning algorithm to develop a predictive model for helix-disrupting mutations. We start with a toy model consisting of helices with only 30 AA and train different models. Our results show that only a few mutations lead to a drastic disruption of the target helix. We further extend our approach to helices in proteins and validate the results using rigorous free energy calculations. Our strategy identifies amino acids crucial for maintaining structural integrity and predicts key mutations that could alter protein function. Through our work, we present a new use case for reinforcement learning in protein structure disruption.",,,,India,,,,,,2025-05-01 10:42,,,,,,Government,,,,,,,Government,,,,,,,,,
PocketGen,Biology,Protein-ligand contact prediction,"University of Science and Technology of China (USTC),Hefei Comprehensive National Science Center,Harvard University,Broad Institute,Harvard Data Science Initiative","Zaixi Zhang, Wan Xiang Shen, Qi Liu, Marinka Zitnik",2024-09-23,Efficient Generation of Protein Pockets with PocketGen,https://www.biorxiv.org/content/10.1101/2024.02.25.581968v4.abstract,1.0,,,7900000.0,"""As a result, PocketGen requires significantly fewer trainable parameters than RFDiffusionAA [16] (7.9M versus 82.9M trainable parameters).""",2.1e+19,"""It takes around 48 hours to finish training on 1 Tesla A100 GPU from scratch."" Assume 40% utilization, FP16 tensor precision. ",,,12000000.0,"""Finally, we have 40k protein-ligand pairs for training, 100 pairs for validation, and 100 pairs for testing.""

40000*300 (assumed length) = 12000000",48.0,,NVIDIA A100,,Confident,"Designing protein-binding proteins is critical for drug discovery. However, the AI-based design of such proteins is challenging due to the complexity of ligand-protein interactions, the flexibility of ligand molecules and amino acid side chains, and sequence-structure dependencies. We introduce PocketGen, a deep generative model that simultaneously produces both the residue sequence and atomic structure of the protein regions where ligand interactions occur. PocketGen ensures consistency between sequence and structure by using a graph transformer for structural encoding and a sequence refinement module based on a protein language model. The bilevel graph transformer captures interactions at multiple scales, including atom, residue, and ligand levels. To enhance sequence refinement, PocketGen integrates a structural adapter into the protein language model, ensuring that structure-based predictions align with sequence-based predictions. PocketGen can generate high-fidelity protein pockets with superior binding affinity and structural validity. It operates ten times faster than physics-based methods and achieves a 95% success rate, defined as the percentage of generated pockets with higher binding affinity than reference pockets. Additionally, it attains an amino acid recovery rate exceeding 64%.",,,,"China,China,United States of America,United States of America,United States of America",,,,1.0,,2025-05-01 10:42,,,,,,"Academia,Research collective,Academia,Research collective,Academia",,,,,,,"Academia,Research collective,Academia,Research collective,Academia",,,,432.9245432852901,Hardware,,,,
Spark 4.0,Language,Language modeling,iFlytek,iFlytek,2024-09-23,"""SPARK"" Ignites a New Engine for Manufacturing Development: iFLYTEK Shines at 2024 World Manufacturing Convention",https://www.iflytek.com/en/news-events/news/218.html,,,,,,,,,,,,,,,Supervised,Unknown,"From September 20 to 23, the 2024 World Manufacturing Convention, themed ""Intelligent Manufacturing for a Better Future"", was held at the Hefei Binhu International Convention & Exhibition Center. iFLYTEK SPARK Large Model V4.0 (hereinafter referred to as "" iFLYTEK SPARK""), along with humanoid robot equipped with iFLYTEK SPARK, the Anhui Provincial Imaging Cloud Platform, and other products and solutions, made a grand appearance at the exhibition. Antelope Industrial Internet Co., Ltd. unveiled the Antelope Industrial Large Model V2.0 and a series of industrial application scenarios, demonstrating in a comprehensive manner the diverse ecosystem of general artificial intelligence empowering various industries.",,,Unreleased,China,,,,,,2025-05-09 11:32,,,,,,Industry,,,,,,,Industry,,,,,,,,,
AMPLIFY,Biology,Protein or nucleotide language model (pLM/nLM),"Chandar Research Lab,Mila - Quebec AI (originally Montreal Institute for Learning Algorithms),Amgen,Polytechnique Montreal,CIFAR AI Research","Quentin Fournier, Robert M. Vernon, Almer van der Sloot, Benjamin Schulz, Sarath Chandar,  Christopher James Langmead",2024-09-23,Protein Language Models: Is Scaling Necessary?,https://www.biorxiv.org/content/10.1101/2024.09.23.614603v1,3.0,,,350000000.0,,1.1000000000000008e+22,"1. Hardware: A100 GPUs with 3.12Ã—10Â¹â´ FLOP/s per GPU (bf16/fp16)

2. Duration: Directly provided - 1,014 GPU days = 8.75Ã—10â· seconds

3. Utilization: 40%

4. Calculation:
3.12Ã—10Â¹â´ FLOP/s Ã— 8.75Ã—10â· s Ã— 0.40 = 1.09Ã—10Â²Â² FLOPs",,,200000000001.0,"Dataset: 391,000,000 sequences
Sequence length: 512 tokens/sequence
Total tokens = 391,000,000 Ã— 512 = 2.00 Ã— 10Â¹Â¹ tokens

Steps processed = 1,000,000 Ã— 4,096 = 4,096,000,000 sequences
Number of epochs = 4,096,000,000 / 391,000,000 â‰ˆ 10.47

Final result: 2.00 Ã— 10Â¹Â¹ tokens",,,NVIDIA A100,,Confident,"Public protein sequence databases contain samples from the fitness landscape explored by nature. Protein language models (pLMs) pre-trained on these sequences aim to capture this landscape for tasks like property prediction and protein design. Following the same trend as in natural language processing, pLMs have continuously been scaled up. However, the premise that scale leads to better performance assumes that source databases provide accurate representation of the underlying fitness landscape, which is likely false. By developing an efficient codebase, designing a modern architecture, and addressing data quality concerns such as sample bias, we introduce AMPLIFY, a best-in-class pLM that is orders of magnitude less expensive to train and deploy than previous models. Furthermore, to support the scientific community and democratize the training of pLMs, we have open-sourced AMPLIFYâ€™s pre-training codebase, data, and model checkpoints.",,,Open weights (unrestricted),"Canada,United States of America,Canada,Canada",,,,,,2025-06-12 15:05,,,,,,"Academia,Industry,Academia,Research collective",,,,,Open source,"MIT license
https://github.com/chandar-lab/AMPLIFY

MIT license
https://huggingface.co/chandar-lab/AMPLIFY_350M_base","Academia,Industry,Academia,Research collective",,,,,Hardware,chandar-lab,,,
MoEFold2D,Biology,RNA structure prediction,George Washington University,Xiangyun Qiu,2024-09-22,"MoEFold2D: Safeguarding RNA Secondary Structure Prediction with a Mixture of Deep
Learning and Physics-based Experts",https://www.biorxiv.org/content/10.1101/2024.09.18.613732v1,0.0,,,960000.0,,,,,,3000001.0,"Number of Sequences (9,995) Ã— Average Sequence Length (300) = 2,998,500 â‰ˆ 3,000,000 data points

Key calculations:
9,995 Ã— 300 = 2,998,500 â‰ˆ 3.0e6 tokens",,,,,Confident,"A mixture of experts (MoE) approach is developed to mitigate poor out-of-distribution (OOD) generalization of deep learning (DL) models for single-sequence-based prediction of RNA secondary structure. The main idea is to use DL models for in-distribution (ID) test sequences to take advantage of their superior ID performances, while relying on physics-based models for OOD sequences to ensure robust predictions. One key ingredient of the pipeline, named MoEFold2D, is automated ID/OOD detection via consensus analysis of an ensemble of DL model predictions without accessing training data during inference. Specifically, motivated by the clustered distribution of known RNA structures, a collection of distinct DL models are trained by iteratively leaving one cluster out. Each DL model hence serves as an expert on all but one cluster in the training data. Consequently, for an ID sequence, all but one DL model makes accurate predictions consistent with one another, while an OOD sequence yields highly inconsistent predictions among all DL models. Consensus analysis of DL predictions categorizes test sequences as ID or OOD. ID sequences are then predicted by averaging the DL models in consensus, and OOD sequences are predicted using physics-based models. Instead of remediating generalization gaps with alternative approaches such as transfer learning and sequence alignment, MoEFold2D circumvents unpredictable ID-OOD gaps and combines the strengths of DL and physics-based models to achieve accurate ID and robust OOD predictions.",,,Open weights (unrestricted),United States of America,,,,,,2025-06-16 13:25,,,,,,Academia,,,,,Open source,"MoEFold2D is open-source software available in the GitHub repository
(https://github.com/qiuresearch/MoEFold2D).

GPL-3.0 license",Academia,,,,,,,,,
IgGM,Biology,Protein design,"Chinese Academy of Sciences,University of Chinese Academy of Sciences,Tencent","Rubo Wang, Fandi Wu, Xingyu Gao, Jiaxiang Wu, Peilin Zhao, Jianhua Yao",2024-09-22,IgGM: A Generative Model for Functional Antibody and Nanobody Design,https://www.biorxiv.org/content/10.1101/2024.09.19.613838v1.abstract,,,,,,8.599999999999976e+20,"1. Hardware setup: 8x NVIDIA A100 SXM4 80GB GPUs, 3.12e14 FLOP/s per GPU

2. Training duration: Directly provided as 10 days = 864,000 seconds

3. Utilization rate: 40%

4. Final calculation:
3.12e14 FLOP/s Ã— 864,000s Ã— 0.4 Ã— 8 GPUs = 8.6e20 FLOPs",,,9200001.0,"Complexes: 6,448 + 1,907 = 8,355
Amino acids per complex: 450 + 350 + 300 = 1,100
Total amino acids: 8,355 * 1,100 = 9,190,500",240.0,,NVIDIA A100,,Likely,"Immunoglobulins are crucial proteins produced by the immune system to identify and bind to foreign substances, playing an essential role in shielding organisms from infections and diseases. Designing specific antibodies opens new pathways for disease treatment. With the rise of deep learning, AI-driven drug design has become possible, leading to several methods for antibody design. However, many of these approaches require additional conditions that differ from real-world scenarios, making it challenging to incorporate them into existing antibody design processes. Here, we introduce IgGM, generative model that combines a diffusion model and the consistency model for generating antibodies with functional specificity. IgGM produces antibody sequences and structures simultaneously for a given antigen, consisting of three core components: a pre-trained language model for extracting sequence features, a feature learning module for identifying pertinent features, and a prediction module that outputs designed antibody sequences and the predicted complete antibody-antigen complex structure. IgGM has shown effectiveness in both predicting structures and designing novel antibodies and nanobodies, making it relevant in various practical scenarios of antibody and nanobody design.",,,Open weights (non-commercial),"China,China,China",,,,8.0,,2025-06-13 15:04,,,,,,"Academia,Academia,Industry",,,,,Unreleased,"PolyForm Noncommercial License 1.0.0
https://github.com/TencentAI4S/IgGM?tab=readme-ov-file","Academia,Academia,Industry",,,,6303.521724136415,Hardware,,,,
Kling 1.5 Pro,"Video,Vision","Video generation,Text-to-video,Image-to-video",Kuaishou Technology,,2024-09-22,"Kling 1.5 AI Video Generator is Finally Here With Major Upgrades
",https://www.zeniteq.com/blog/kling-1-5-ai-video-generator-is-finally-here-with-major-upgrades,,,,,,,"Plausibly used more compute than Meta Movie Gen, as it had higher preference scores against Veo-2 than MMG did.",Unspecified unreleased,,,,,,,,Unknown,"Kling AI, one of the most popular AI video generators and a strong competitor to Runway Gen-3 and OpenAIâ€™s Sora, has just released Kling AI 1.5 Pro. This update brings some major improvements: a new and improved video model, the introduction of a motion brush feature, and much more.

Developed by Kuaishou, a Beijing-based company that competes directly with TikTok, Kling AI allows users to create videos up to 10 seconds long with 30 frames per second at 1080p resolution. It also supports flexible aspect ratios, making it a versatile tool for content creators.",,,API access,China,,,,,,2025-05-20 06:25,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
BTFBS,Biology,Protein-DNA binding prediction,Nanjing Agricultural University,"Bingbing Jin, Song Liang, Xiaoqian Liu, Rui Zhang, Yun Zhu, Yuanyuan Chen, Guangjin Liu, Tao Yang
",2024-09-22,BTFBS: binding-prediction of bacterial transcription factors and binding sites based on deep learning,https://www.biorxiv.org/content/10.1101/2024.09.19.613986v1,,,,,,,,,,5778080.0,"""In order to ensure the consistency of sequence length, the maximum length of
a DNA sequence is set to 100 and the maximum length of a protein sequence is set to 600. Sequences shorter than
the maximum length are padded with zeros to match the fixed length.""

""As a result, 5159
non-redundant positive sequences are obtained. Approximately 10% of the sequences are randomly selected from
the datasets as the independent dataset, while the remaining 90% of the sequences are used as the training and test
datasets.""

""Subsequently, 27,236 non-redundant sequences are obtained, and 19% of the data
are randomly selected as negative samples, matching the number of positive sample""

Total tokens: 2*5159*700*0.8=5778080
",,,,,Likely,"Background The binding of transcription factors (TFs) to TF-binding sites plays a vital role in the process of regulating gene expression and evolution. With the development of machine learning and deep learning, some successes have been achieved in predicting transcription factors and binding sites. Then a natural question arises: for a given transcription factor and a binding site, do they bind? This is the main motivation of this work.
Results In this paper, we develop a model BTFBS, which predicts whether the bacterial transcription factors and binding sites combine or not. The model takes both the amino acid sequences of bacterial transcription factors and the nucleotide sequences of binding sites as inputs, and extracts features through convolutional neural network and MultiheadAttention.
For the model inputs, we use two negative sample sampling methods: RS and EE. On the test dataset of RS, the accuracy, sensitivity, specificity, F1-score and MCC of BTFBS are 0.91446, 0.89746, 0.93134, 0.91264 and 0.82946, respectively. And on the test dataset of EE, the accuracy, sensitivity, specificity, F1-score and MCC of BTFBS are 0.87868, 0.89354, 0.86394, 0.87996 and 0.75796, respectively. Meanwhile, our findings indicate that the optimal approach for obtaining negative samples in the context of bacterial research is to utilize the whole genome sequences of the corresponding bacteria, as opposed to the shuffling method.
Conclusions The above results on the test dataset have shown that the proposed BTFBS model has a good performance in predicting the combination of bacterial transcription factors and their binding sites and provides an experimental guide. BTFBS is publicly available at https://github.com/Vceternal/BTFBS.",,,,China,,,,,,2025-05-01 10:42,,,,,,Academia,,,,,,,Academia,,,,,,,,,
PepINVENT,Biology,"Protein generation,Protein design","AstraZeneca,Chalmers University of Technology","GÃ¶kÃ§e Geylan, Jon Paul Janet, Alessandro Tibo, Jiazhen He, Atanas Patronov, Mikhail Kabeshov, Florian David, Werngard Czechtizky, Ola Engkvist, Leonardo De Maria",2024-09-21,PepINVENT: Generative peptide design beyond the natural amino acids,https://arxiv.org/abs/2409.14040,0.0,,,,,,"
",,,10800001.0,"1,000,000 peptides * 0.9 training split = 900,000 training peptides
900,000 peptides * 12 tokens per peptide = 10,800,000 tokens
Final estimate: 10,800,000 tokens (1.08e7)",,,NVIDIA V100,,Confident,"Peptides play a crucial role in the drug design and discovery whether as a therapeutic modality or a delivery agent. Non-natural amino acids (NNAAs) have been used to enhance the peptide properties from binding affinity, plasma stability to permeability. Incorporating novel NNAAs facilitates the design of more effective peptides with improved properties. The generative models used in the field, have focused on navigating the peptide sequence space. The sequence space is formed by combinations of a predefined set of amino acids. However, there is still a need for a tool to explore the peptide landscape beyond this enumerated space to unlock and effectively incorporate de novo design of new amino acids. To thoroughly explore the theoretical chemical space of the peptides, we present PepINVENT, a novel generative AI-based tool as an extension to the small molecule molecular design platform, REINVENT. PepINVENT navigates the vast space of natural and non-natural amino acids to propose valid, novel, and diverse peptide designs. The generative model can serve as a central tool for peptide-related tasks, as it was not trained on peptides with specific properties or topologies. The prior was trained to understand the granularity of peptides and to design amino acids for filling the masked positions within a peptide. PepINVENT coupled with reinforcement learning enables the goal-oriented design of peptides using its chemistry-informed generative capabilities. This study demonstrates PepINVENT's ability to explore the peptide space with unique and novel designs, and its capacity for property optimization in the context of therapeutically relevant peptides. Our tool can be employed for multi-parameter learning objectives, peptidomimetics, lead optimization, and variety of other tasks within the peptide domain.",24.0,,Open weights (unrestricted),"Sweden,United Kingdom of Great Britain and Northern Ireland,Sweden",,,,,,2025-06-11 17:31,,,,,,"Industry,Academia",,,,,Open source,"Apache 2.0
https://github.com/MolecularAI/PepINVENT","Industry,Academia",,,,,Hardware,,,,
Prithvi WxC,Earth science,Weather forecasting,"IBM Research,University of Alabama,Stanford University,Colorado State University,Oak Ridge National Laboratory,NASA","Johannes Schmude, Sujit Roy, Will Trojak, Johannes Jakubik, Daniel Salles Civitarese, Shraddha Singh, Julian Kuehnert, Kumar Ankur, Aman Gupta, Christopher E Phillips, Romeo Kienzler, Daniela Szwarcman, Vishal Gaur, Rajat Shinde, Rohit Lal, Arlindo Da Silva, Jorge Luis Guevara Diaz, Anne Jones, Simon Pfreundschuh, Amy Lin, Aditi Sheshadri, Udaysankar Nair, Valentine Anantharaj, Hendrik Hamann, Campbell Watson, Manil Maskey, Tsengdar J Lee, Juan Bernabe Moreno, Rahul Ramachandran",2024-09-20,Prithvi WxC: Foundation Model for Weather and Climate,https://arxiv.org/abs/2409.13598,8.0,,,2300000000.0,"a 2.3 billion parameter foundation model developed using 160 variables from the Modern-Era Retrospective Analysis for Research and Applications, Version 2 (MERRA-2).",7.150000000000001e+19,"""we train the model on 64 A100 GPUs and batch size 1 for 100,000 gradient descent steps""
""With these choices we are dealing with 51,840
tokens per sample yet are keeping the length of the global and local sequence roughly balanced""

Total number of tokens: 100000*51840=5184000000
Training compute: 6*5184000000*2300000000=7.15392e+19","MERRA-2,ERA5",,5184000000.0,,,,NVIDIA A100,,Confident,"Triggered by the realization that AI emulators can rival the performance of traditional numerical weather prediction models running on HPC systems, there is now an increasing number of large AI models that address use cases such as forecasting, downscaling, or nowcasting. While the parallel developments in the AI literature focus on foundation modelsâ€“ models that can be effectively tuned to address multiple, different use casesâ€“ the developments on the weather and climate side largely focus on single-use cases with particular emphasis on mid-range forecasting. We close this gap by introducing Prithvi WxC, a 2.3 billion parameter foundation model developed using 160 variables from the Modern-Era Retrospective Analysis for Research and Applications, Version 2 (MERRA-2). Prithvi WxC employs an encoder-decoder-based architecture, incorporating concepts from various recent transformer models to effectively capture both regional and global dependencies in the input data. The model has been designed to accommodate large token counts to model weather phenomena in different topologies at fine resolutions. Furthermore, it is trained with a mixed objective that combines the paradigms of masked reconstruction with forecasting. We test the model on a set of challenging downstream tasks namely: Autoregressive rollout forecasting, Downscaling, Gravity wave flux parameterization, and Extreme events estimation. The pretrained model with 2.3 billion parameters, along with the associated finetuning workflows, has been publicly released as an open-source contribution via Hugging Face.",,,,"United States of America,Multinational,Ireland,United Kingdom of Great Britain and Northern Ireland,Brazil,Switzerland,United States of America,United States of America,United States of America,United States of America",,,,64.0,,2025-05-01 10:42,,,,,,"Industry,Academia,Academia,Government,Government",,,,,,,"Industry,Academia,Academia,Government,Government",,,,50430.41985055912,,,,,
Qwen2.5-72B,Language,"Language modeling/generation,Question answering,Quantitative reasoning",Alibaba,Qwen Team,2024-09-19,Qwen2.5: A Party of Foundation Models!,https://qwenlm.github.io/blog/qwen2.5/,,Training cost,"High compute, near 1e25",72700000000.0,72.7B,7.8e+24,"Training dataset size was 18 trillion

6ND = 6 * 72.7 billion parameters * 18 trillion tokens = 7.8e24",Unspecified unreleased,"""In terms of Qwen2.5, the language models, all models are pretrained on our latest large-scale dataset, encompassing up to 18 trillion tokens.""",18000000000000.0,"""In terms of Qwen2.5, the language models, all models are pretrained on our latest large-scale dataset, encompassing up to 18 trillion tokens""",,,,,Confident,"In the past three months since Qwen2â€™s release, numerous developers have built new models on the Qwen2 language models, providing us with valuable feedback. During this period, we have focused on creating smarter and more knowledgeable language models. Today, we are excited to introduce the latest addition to the Qwen family: Qwen2.5. We are announcing what might be the largest opensource release in history! Letâ€™s get the party started!",1.0,,Open weights (unrestricted),China,,,,,,2025-05-29 11:45,,"There is no paper to reference, no information about hardware used for training found in media.",,,,Industry,,,7.8e+24,,Unreleased,"license: allows commercial. weights only
https://huggingface.co/Qwen/Qwen2.5-72B/blob/main/LICENSE ",Industry,,,BF16,,Operation counting,Qwen,,,19
pKALM,Biology,Protein property prediction,Hokkaido University,"Shijie Xu,  Akira Onoda",2024-09-19,Accurate and Rapid Prediction of Protein pKa: Protein Language Models Reveal the Sequence-pKa Relationship,https://www.biorxiv.org/content/10.1101/2024.09.16.613101v1.abstract,,,,5083905.0,Loaded checkpoint and counted parameters.,,,,,37002000.0,"Two datasets were used:

""The revised data set now includes 1,450  pKa values for 165 wild-type proteins and 262 pKa values for 47 mutant proteins.""

""These data sets, derived from references containing experimen-
tal pI values, comprise 119,092 peptide pI values and 2,324 protein pI values.""

Total assuming avg 300 tokens per example: (1450+165+262+47+119092+2324)*300=37002000",,,NVIDIA GeForce RTX 3090 Ti,,Speculative,"Protein pKa prediction is a key challenge in computational biology. In this study, we present pKALM, a novel deep learning-based method for high-throughput protein pKa prediction. pKALM uses a protein language model (PLM) to capture the complex sequence-structure relationship of proteins. While traditionally considered a structure-based problem, our results show that a PLM pre-trained on large-scale protein sequence databases can effectively learn this relationship and achieve state-of-the-art performance. pKALM accurately predicts the pKa values of six residues (Asp, Glu, His, Lys, Cys, and Tyr) and two termini with high precision and efficiency. It excels at predicting both exposed and buried residues, which often deviate from standard pKa values measured in solvent. We demonstrate a novel finding that predicted protein isoelectric points (pI) can be used to improve the accuracy of pKa prediction. High-throughput pKa prediction of the human proteome using pKALM achieves a speed of 4,965 pKa predictions per second, which is several orders of magnitude faster than existing state-of-the-art methods. The case studies illustrate the efficacy of pKALM in estimating pKa values and the constraints of the method. pKALM will thus be a valuable tool for researchers in the fields of biochemistry, biophysics, and drug design.",130.0,,,Japan,ESM2-35M,,,2.0,,2025-05-01 10:42,,,,,,Academia,,,,,,,Academia,,,,1772.983930669006,Operation counting,,,,
Qwen2.5 Instruct (7B),Language,"Code generation,Code autocompletion,Quantitative reasoning,Question answering,Language modeling/generation",Alibaba,Qwen Team,2024-09-19,Qwen2.5: A Party of Foundation Models!,https://qwenlm.github.io/blog/qwen2.5/,,,,7610000000.0,,8.2188e+23,6ND = 6*7610000000.00 parameters *18000000000000 tokens = 8.2188e+23 (might be less if not entire training dataset was used),Unspecified unreleased,,18000000000000.0,"""In terms of Qwen2.5, the language models, all models are pretrained on our latest large-scale dataset, encompassing up to 18 trillion tokens.""",,,,,Likely,"Qwen2.5 is the latest series of Qwen large language models. For Qwen2.5, we release a number of base language models and instruction-tuned language models ranging from 0.5 to 72 billion parameters. Qwen2.5 brings the following improvements upon Qwen2:

Significantly more knowledge and has greatly improved capabilities in coding and mathematics, thanks to our specialized expert models in these domains.
Significant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON. More resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.
Long-context Support up to 128K tokens and can generate up to 8K tokens.
Multilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.",,,Open weights (unrestricted),China,Qwen2.5-7B,,,,,2025-05-26 18:58,,,,,,Industry,,,,,Unreleased,"Apache 2.0

https://huggingface.co/Qwen/Qwen2.5-7B-Instruct

It seems that there is no pretraining code here, only fine-tuning
https://github.com/QwenLM/Qwen3",Industry,,,,,Operation counting,Qwen,,,
Qwen2.5 Instruct (72B),Language,"Code generation,Code autocompletion,Quantitative reasoning,Question answering,Language modeling/generation",Alibaba,Qwen Team,2024-09-19,Qwen2.5: A Party of Foundation Models!,https://qwenlm.github.io/blog/qwen2.5/,,Training cost,,72700000000.0,"Number of Parameters: 72.7B
Number of Paramaters (Non-Embedding): 70.0B",7.851600000000001e+24,6ND = 6*72700000000 parameters *18000000000000 tokens = 7.8516e+24,Unspecified unreleased,,18000000000000.0,"""In terms of Qwen2.5, the language models, all models are pretrained on our latest large-scale dataset, encompassing up to 18 trillion tokens.""",,,,,Confident,"Qwen2.5 is the latest series of Qwen large language models. For Qwen2.5, we release a number of base language models and instruction-tuned language models ranging from 0.5 to 72 billion parameters. Qwen2.5 brings the following improvements upon Qwen2:

Significantly more knowledge and has greatly improved capabilities in coding and mathematics, thanks to our specialized expert models in these domains.
Significant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON. More resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.
Long-context Support up to 128K tokens and can generate up to 8K tokens.
Multilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.",,,Open weights (restricted use),China,Qwen2.5-72B,,,,,2025-05-26 18:52,,"There is no paper to reference, no information about hardware used for training found in media.",,,,Industry,,,7.851600000000001e+24,,Unreleased,"requires permission to use in applications with 100M+ users

https://huggingface.co/Qwen/Qwen2.5-72B-Instruct

seems that there is no pretraining code here
https://github.com/QwenLM/Qwen3",Industry,,,,,Operation counting,Qwen,,,
Qwen2.5-Math-7B-Base,Language,"Language modeling/generation,Quantitative reasoning,Question answering",Alibaba,,2024-09-19,Qwen2.5-Math: The world's leading open-sourced mathematical LLMs,https://qwenlm.github.io/blog/qwen2.5-math/,,,,7000000000.0,7B,8.6388e+23,"base model: 8.2188e+23 FLOP
fine-tuning: 4.2e+22 FLOP 

Total: 8.6388e+23 FLOP",Unspecified unreleased,"Synthetic math pre-training data 

",1000000000000.0,Math corpus over 1T tokens,,,,,Confident,"A month ago, we released the first series of mathematical LLMs - Qwen2-Math - of our Qwen family. Today, we have upgraded it and open-sourced Qwen2.5-Math series, including base models Qwen2.5-Math-1.5B/7B/72B, instruction-tuned models Qwen2.5-Math-1.5B/7B/72B-Instruct, and mathematical reward model Qwen2.5-Math-RM-72B.

Unlike Qwen2-Math series which only supports using Chain-of-Thought (CoT) to solve English math problems, Qwen2.5-Math series is expanded to support using both CoT and Tool-integrated Reasoning (TIR) to solve math problems in both Chinese and English. The Qwen2.5-Math series models have achieved significant performance improvements compared to the Qwen2-Math series models on the Chinese and English mathematics benchmarks with CoT.

",,,Open weights (unrestricted),China,Qwen2.5-7B,4,6 FLOP / token / parameter * 7 * 10^9 parameters * 1*10^12 tokens = 4.2e+22 FLOP,,,2025-05-23 16:47,,,,,,Industry,,,,,Unreleased,"Apache 2

https://huggingface.co/Qwen/Qwen2.5-Math-7B

no training code here
https://github.com/QwenLM/Qwen2.5-Math",Industry,,,,,Operation counting,Qwen,,,
Oryx 34B,"Multimodal,Vision,3D modeling,Video,Language","Visual question answering,Video compression,Image captioning,Video description,Language modeling/generation","Tsinghua University,Tencent,Nanyang Technological University","Zuyan Liu, Yuhao Dong, Ziwei Liu, Winston Hu, Jiwen Lu, Yongming Rao",2024-09-19,"Oryx MLLM: On-Demand Spatial-Temporal Understanding at Arbitrary Resolution
",https://arxiv.org/abs/2409.12961v1,52.0,SOTA improvement,"Best performance on MLVU benchmark (long-form temporal understanding), MMBench and TextVQA (image understanding).",34000000000.0,,,,Oryx-SFT-Data,Dataset: https://huggingface.co/datasets/THUdyh/Oryx-SFT-Data (A mixture of 1.2M image/video data) Paper supposed to be published with more details.,,,,,NVIDIA A800 SXM,,Confident,"Visual data comes in various forms, ranging from small icons of just a few pixels to long videos spanning hours. Existing multi-modal LLMs usually standardize these diverse visual inputs to a fixed resolution for visual encoders and yield similar numbers of tokens for LLMs. This approach is non-optimal for multimodal understanding and inefficient for processing inputs with long and short visual contents. To solve the problem, we propose Oryx, a unified multimodal architecture for the spatial-temporal understanding of images, videos, and multi-view 3D scenes. Oryx offers an on-demand solution to seamlessly and efficiently process visual inputs with arbitrary spatial sizes and temporal lengths through two core innovations: 1) a pre-trained OryxViT model that can encode images at any resolution into LLM-friendly visual representations; 2) a dynamic compressor module that supports 1x to 16x compression on visual tokens by request. These design features enable Oryx to accommodate extremely long visual contexts, such as videos, with lower resolution and high compression while maintaining high recognition precision for tasks like document understanding with native resolution and no compression. Beyond the architectural improvements, enhanced data curation and specialized training on long-context retrieval and spatial-aware data help Oryx achieve strong capabilities in image, video, and 3D multimodal understanding simultaneously",,,Open weights (unrestricted),"China,China,Singapore",Yi-1.5-34B,,,64.0,,2025-06-02 13:54,,,,128.0,,"Academia,Industry,Academia",,,,,Open source,"Apache 2.0
https://huggingface.co/THUdyh/Oryx-1.5-32B

MIT license
https://github.com/Oryx-mllm/Oryx","Academia,Industry,Academia",,,,50431.54291680728,,THUdyh,,,
Qwen2.5-Math-1.5B,Language,"Language modeling/generation,Quantitative reasoning,Mathematical reasoning",Alibaba,,2024-09-19,Qwen2.5-Math: The world's leading open-sourced mathematical LLMs,https://qwenlm.github.io/blog/qwen2.5-math/,,,,1500000000.0,1.5B,1.7532e+23,"base model: 1.6632e+23 FLOP
fine-tuning: 9e+21 FLOP

Total: 1.7532e+23 FLOP",Unspecified unreleased,"Synthetic math pre-training data 

",1000000000000.0,Math corpus over 1T tokens,,,,,Confident,"In August 2024, we released the first series of mathematical LLMs - Qwen2-Math - of our Qwen family. A month later, we have upgraded it and open-sourced Qwen2.5-Math series, including base models Qwen2.5-Math-1.5B/7B/72B, instruction-tuned models Qwen2.5-Math-1.5B/7B/72B-Instruct, and mathematical reward model Qwen2.5-Math-RM-72B.

Unlike Qwen2-Math series which only supports using Chain-of-Thught (CoT) to solve English math problems, Qwen2.5-Math series is expanded to support using both CoT and Tool-integrated Reasoning (TIR) to solve math problems in both Chinese and English. The Qwen2.5-Math series models have achieved significant performance improvements compared to the Qwen2-Math series models on the Chinese and English mathematics benchmarks with CoT.",,,Open weights (unrestricted),China,Qwen2.5-1.5B,9,6 FLOP / token / parameter * 1.5 * 10^9 parameters * 1*10^12 tokens = 9e+21 FLOP,,,2025-05-16 10:30,,,,,,Industry,,,,,Unreleased,"https://huggingface.co/Qwen/Qwen2.5-Math-1.5B

Apache 2",Industry,,,,,Operation counting,Qwen,,,
Qwen2-VL-72B,"Language,Vision,Multimodal","Visual question answering,Video description,Language modeling/generation,Translation,Question answering,Character recognition,Quantitative reasoning",Alibaba,"Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, Junyang Lin",2024-09-18,Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution,https://arxiv.org/abs/2409.12191,,,,72000000000.0,72 billion (language model) and 675M (vision encoder),6.048e+23,6ND = 6 FLOP / token / parameter Ã— 1.4Ã—10^12 tokens Ã— 7.2Ã—10^10 parameters = 6.048e+23 FLOP,Unspecified unreleased,"""The model is pre-trained on a diverse dataset that includes image-text pairs, optical character recognition (OCR) data, interleaved image-text articles, visual question answering datasets, video dialogues, and image knowledge datasets. Our data sources primarily comprise cleaned web pages, open-source datasets, and synthetic data. The cutoff date for our data knowledge is June 2023.""",1400000000000.0,"""Throughout the pre-training stages, Qwen2-VL processes a cumulative total of 1.4 trillion tokens. Specifically, these tokens encompass not only text tokens but also image tokens""",,,,,Likely,"We present the Qwen2-VL Series, an advanced upgrade of the previous Qwen-VL models that redefines the conventional predetermined-resolution approach in visual processing. Qwen2-VL introduces the Naive Dynamic Resolution mechanism, which enables the model to dynamically process images of varying resolutions into different numbers of visual tokens. This approach allows the model to generate more efficient and accurate visual representations, closely aligning with human perceptual processes. The model also integrates Multimodal Rotary Position Embedding (M-RoPE), facilitating the effective fusion of positional information across text, images, and videos. We employ a unified paradigm for processing both images and videos, enhancing the model's visual perception capabilities. To explore the potential of large multimodal models, Qwen2-VL investigates the scaling laws for large vision-language models (LVLMs). By scaling both the model size-with versions at 2B, 8B, and 72B parameters-and the amount of training data, the Qwen2-VL Series achieves highly competitive performance. Notably, the Qwen2-VL-72B model achieves results comparable to leading models such as GPT-4o and Claude3.5-Sonnet across various multimodal benchmarks, outperforming other generalist models. Code is available at \url{this https URL}.",,,Open weights (unrestricted),China,,,,,,2025-05-26 19:17,,,,,,Industry,,,,,Unreleased,https://huggingface.co/Qwen/Qwen2-VL-72B-Instruct,Industry,,,,,Operation counting,Qwen,,,
Qwen2-VL-2B,"Language,Vision,Multimodal","Visual question answering,Video description,Language modeling/generation,Translation,Question answering,Character recognition,Quantitative reasoning",Alibaba,"Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, Junyang Lin",2024-09-18,Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution,https://arxiv.org/abs/2409.12191,,,,2000000000.0,1.5 billion (language model) and 675M (vision encoder),1.68e+22,6ND = 6Ã—1.4Ã—10^12Ã—2Ã—10^9 = 1.68e+22,Unspecified unreleased,"""The model is pre-trained on a diverse dataset that includes image-text pairs, optical character recognition (OCR) data, interleaved image-text articles, visual question answering datasets, video dialogues, and image knowledge datasets. Our data sources primarily comprise cleaned web pages, open-source datasets, and synthetic data. The cutoff date for our data knowledge is June 2023.""",1400000000000.0,"""Throughout the pre-training stages, Qwen2-VL processes a cumulative total of 1.4 trillion tokens. Specifically, these tokens encompass not only text tokens but also image tokens""",,,,,Likely,"We present the Qwen2-VL Series, an advanced upgrade of the previous Qwen-VL models that redefines the conventional predetermined-resolution approach in visual processing. Qwen2-VL introduces the Naive Dynamic Resolution mechanism, which enables the model to dynamically process images of varying resolutions into different numbers of visual tokens. This approach allows the model to generate more efficient and accurate visual representations, closely aligning with human perceptual processes. The model also integrates Multimodal Rotary Position Embedding (M-RoPE), facilitating the effective fusion of positional information across text, images, and videos. We employ a unified paradigm for processing both images and videos, enhancing the model's visual perception capabilities. To explore the potential of large multimodal models, Qwen2-VL investigates the scaling laws for large vision-language models (LVLMs). By scaling both the model size-with versions at 2B, 8B, and 72B parameters-and the amount of training data, the Qwen2-VL Series achieves highly competitive performance. Notably, the Qwen2-VL-72B model achieves results comparable to leading models such as GPT-4o and Claude3.5-Sonnet across various multimodal benchmarks, outperforming other generalist models. Code is available at \url{this https URL}.",,,Open weights (unrestricted),China,,,,,,2025-05-26 19:07,,,,,,Industry,,,,,,https://huggingface.co/Qwen/Qwen2-VL-2B-Instruct,Industry,,,,,Operation counting,Qwen,,,
Qwen2-VL-7B,"Language,Vision,Multimodal","Visual question answering,Video description,Language modeling/generation,Translation,Question answering,Character recognition,Quantitative reasoning",Alibaba,"Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, Junyang Lin",2024-09-18,Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution,https://arxiv.org/abs/2409.12191,,,,8000000000.0,7.6 billion (language model) and 675M (vision encoder),6.72e+22,6ND = 6Ã—1.4Ã—10^12Ã—8Ã—10^9 = 6.72e+22,Unspecified unreleased,"""The model is pre-trained on a diverse dataset that includes image-text pairs, optical character recognition (OCR) data, interleaved image-text articles, visual question answering datasets, video dialogues, and image knowledge datasets. Our data sources primarily comprise cleaned web pages, open-source datasets, and synthetic data. The cutoff date for our data knowledge is June 2023.""",1400000000000.0,"""Throughout the pre-training stages, Qwen2-VL processes a cumulative total of 1.4 trillion tokens. Specifically, these tokens encompass not only text tokens but also image tokens""",,,,,Likely,"We present the Qwen2-VL Series, an advanced upgrade of the previous Qwen-VL models that redefines the conventional predetermined-resolution approach in visual processing. Qwen2-VL introduces the Naive Dynamic Resolution mechanism, which enables the model to dynamically process images of varying resolutions into different numbers of visual tokens. This approach allows the model to generate more efficient and accurate visual representations, closely aligning with human perceptual processes. The model also integrates Multimodal Rotary Position Embedding (M-RoPE), facilitating the effective fusion of positional information across text, images, and videos. We employ a unified paradigm for processing both images and videos, enhancing the model's visual perception capabilities. To explore the potential of large multimodal models, Qwen2-VL investigates the scaling laws for large vision-language models (LVLMs). By scaling both the model size-with versions at 2B, 8B, and 72B parameters-and the amount of training data, the Qwen2-VL Series achieves highly competitive performance. Notably, the Qwen2-VL-72B model achieves results comparable to leading models such as GPT-4o and Claude3.5-Sonnet across various multimodal benchmarks, outperforming other generalist models. Code is available at \url{this https URL}.",,,Open weights (unrestricted),China,,,,,,2025-05-30 14:35,,,,,,Industry,,,,,,https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct,Industry,,,,,Operation counting,Qwen,,,
Qwen2.5-Coder (7B),Language,"Code generation,Code autocompletion,Quantitative reasoning,Question answering,Language modeling/generation",Alibaba,"Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Kai Dang, An Yang, Rui Men, Fei Huang, Xingzhang Ren, Xuancheng Ren, Jingren Zhou, Junyang Lin",2024-09-18,Qwen2.5-Coder Technical Report,https://arxiv.org/abs/2409.12186,,,,7610000000.0,Number of Parameters: 7.61B,2.5113e+23,6ND = 6 FLOP / token / parameter *7610000000 parameters *5.5T tokens =2.5113e+23 FLOP,"GitHub,Common Crawl","""we constructed a dataset named Qwen2.5-Coder-Data. This dataset comprises five key data types: Source Code Data, Text-Code Grounding Data, Synthetic Data, Math Data, and Text Data.""",5500000000000.0,,,,,,Confident,"In this report, we introduce the Qwen2.5-Coder series, a significant upgrade from its predecessor, CodeQwen1.5. This series includes two models: Qwen2.5-Coder-1.5B and Qwen2.5-Coder-7B. As a code-specific model, Qwen2.5-Coder is built upon the Qwen2.5 architecture and continues pretrained on a vast corpus of over 5.5 trillion tokens. Through meticulous data cleaning, scalable synthetic data generation, and balanced data mixing, Qwen2.5-Coder demonstrates impressive code generation capabilities while retaining general versatility. The model has been evaluated on a wide range of code-related tasks, achieving state-of-the-art (SOTA) performance across more than 10 benchmarks, including code generation, completion, reasoning, and repair, consistently outperforming larger models of the same model size. We believe that the release of the Qwen2.5-Coder series will not only push the boundaries of research in code intelligence but also, through its permissive licensing, encourage broader adoption by developers in real-world applications.",,,Open weights (unrestricted),China,,,,,,2025-05-29 08:56,,,,,,Industry,,,,,Unreleased,"Apache 2.0 
https://huggingface.co/Qwen/Qwen2.5-Coder-7B",Industry,,,,,Operation counting,Qwen,,,
Qwen2.5-Coder (1.5B),Language,"Code generation,Code autocompletion,Quantitative reasoning,Question answering,Language modeling/generation",Alibaba,"Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Kai Dang, An Yang, Rui Men, Fei Huang, Xingzhang Ren, Xuancheng Ren, Jingren Zhou, Junyang Lin",2024-09-18,Qwen2.5-Coder Technical Report,https://arxiv.org/abs/2409.12186,,,,1540000000.0,Number of Parameters: 1.54B,5.082e+22,6ND =  6*1540000000 parameters *5.5T tokens =5.082e+22,"GitHub,Common Crawl","""we constructed a dataset named Qwen2.5-Coder-Data. This dataset comprises five key data types: Source Code Data, Text-Code Grounding Data, Synthetic Data, Math Data, and Text Data.""",5500000000000.0,,,,,,Confident,"In this report, we introduce the Qwen2.5-Coder series, a significant upgrade from its predecessor, CodeQwen1.5. This series includes two models: Qwen2.5-Coder-1.5B and Qwen2.5-Coder-7B. As a code-specific model, Qwen2.5-Coder is built upon the Qwen2.5 architecture and continues pretrained on a vast corpus of over 5.5 trillion tokens. Through meticulous data cleaning, scalable synthetic data generation, and balanced data mixing, Qwen2.5-Coder demonstrates impressive code generation capabilities while retaining general versatility. The model has been evaluated on a wide range of code-related tasks, achieving state-of-the-art (SOTA) performance across more than 10 benchmarks, including code generation, completion, reasoning, and repair, consistently outperforming larger models of the same model size. We believe that the release of the Qwen2.5-Coder series will not only push the boundaries of research in code intelligence but also, through its permissive licensing, encourage broader adoption by developers in real-world applications.",,,Open weights (unrestricted),China,,,,,,2025-05-29 08:55,,,,,,Industry,,,,,Unreleased,"Apache 2.0 
https://huggingface.co/Qwen/Qwen2.5-Coder-1.5B",Industry,,,,,Operation counting,Qwen,,,
AtomFlow,Biology,Protein generation,"Peking University,Chinese University of Hong Kong (CUHK),Mila - Quebec AI (originally Montreal Institute for Learning Algorithms),University of Montreal / UniversitÃ© de MontrÃ©al,HEC Montreal,CIFAR AI Research","Junqi Liu, Shaoning Li, Chence Shi, Zhi Yang, Jian Tang",2024-09-18,Design of Ligand-Binding Proteins with Atomic Flow Matching,https://arxiv.org/abs/2409.12080,0.0,,,,,,,"PDBbind,SCOPe 2.05",,,,,,NVIDIA GeForce RTX 4090,,Unknown,"Designing novel proteins that bind to small molecules is a long-standing challenge in computational biology, with applications in developing catalysts, biosensors, and more. Current computational methods rely on the assumption that the binding pose of the target molecule is known, which is not always feasible, as conformations of novel targets are often unknown and tend to change upon binding. In this work, we formulate proteins and molecules as unified biotokens, and present AtomFlow, a novel deep generative model under the flow-matching framework for the design of ligand-binding proteins from the 2D target molecular graph alone. Operating on representative atoms of biotokens, AtomFlow captures the flexibility of ligands and generates ligand conformations and protein backbone structures iteratively. We consider the multi-scale nature of biotokens and demonstrate that AtomFlow can be effectively trained on a subset of structures from the Protein Data Bank, by matching flow vector field using an SE(3) equivariant structure prediction network. Experimental results show that our method can generate high fidelity ligand-binding proteins and achieve performance comparable to the state-of-the-art model RFDiffusionAA, while not requiring bound ligand structures. As a general framework, AtomFlow holds the potential to be applied to various biomolecule generation tasks in the future.",,,,"China,Hong Kong,China,Canada,Canada,Canada,Canada",,,,10.0,,2025-05-01 10:42,,,,,,"Academia,Academia,Academia,Academia,Academia,Research collective",,,,,,,"Academia,Academia,Academia,Academia,Academia,Research collective",,,,8865.1170717303,,,,,
Whale Bioacoustics Model,Audio,Audio classification,"Google Research,National Oceanic and Atmospheric Administration (NOAA),Oregon State University","Ann N. Allen, Matt Harvey, Lauren Harrell, Megan Wood, Angela R. Szesciorka, Jennifer L. K. McCullough, Erin M. Oleson",2024-09-18,"Whistles, songs, boings, and biotwangs: Recognizing whale vocalizations with AI","https://research.google/blog/whistles-songs-boings-and-biotwangs-recognizing-whale-vocalizations-with-ai/

https://www.frontiersin.org/journals/marine-science/articles/10.3389/fmars.2024.1394695/full",,,,,,,,Pacific Islands Passive Acoustic Network (PIPAN) 10kHz Data,,,"""200,000 hours of underwater recordings""",,,,,Unknown,"In order to protect animals that live in remote environments, researchers must be able to find them to understand the movements of their populations over time. As long-term passive acoustic monitoring capabilities have grown more technologically sophisticated, automatic animal species identification tools built on large datasets from these recorded soundscapes have become an increasingly vital tool for conservation and ecological research. While models such as Google Perch have emerged that can classify thousands of bird vocalizations, similar models that can classify vocalizations from several whale species at once have proven more challenging to develop.

The acoustic range of whale species is incredibly broad, ranging from as low as 10 Hz for blue whales to above 120kHz for odontocetes (toothed whales), and recordings also vary dramatically by location and with time, which can make model development difficult. Additionally, researchers often donâ€™t know what types of vocalizations are made by some especially elusive whale species, which complicates identifying those animals in the soundscapes. This is illustrated in the decades-old mystery surrounding a sound, called a â€œBiotwangâ€, that was first recorded almost a decade ago in the depths of the Mariana Trench. The sound has a ""metallic"" or ""chime-like"" quality, quite unlike the tonal moans more typical of whale vocalizations. In a recent paper, our collaborators at the U.S. National Oceanic and Atmospheric Administration (NOAA) determined that the Biotwang sound is uniquely produced by the elusive Brydeâ€™s whales (pronounced ""broodus"").

Today we are delighted to share Googleâ€™s latest whale bioacoustics model, which can identify eight distinct species, as well as multiple calls for two of those species. Following on our collaboratorâ€™s discovery connecting Biotwangs to the Brydeâ€™s whale and in the same paper, we expanded the model to include Biotwangs and used it to label more than 200,000 hours of underwater recordings. Here we describe the model and discuss some of the new insights into the ecology of whale species it is helping researchers to unlock. The model is now available for download via Kaggle Models.

This multi-label audio classification model, when applied to a 5-second context window of 24kHz mono audio, outputs 12 scores representing confidence that the audio contains a whale vocalization from a particular species (7 classes) or of a particular type (5 classes).",,,Open weights (non-commercial),"Multinational,United States of America,Canada,Switzerland,United States of America,United States of America",,,,,,2025-05-01 10:42,,,,,,"Industry,Government,Academia",,,,,Open source,"""This model has been developed as part of the AI for Nature and Society program at Google. The developers request that users adhere to Googleâ€™s AI principles, in particular #1 â€œBe socially beneficial."" in only pursuing applications which have societal and/or environmental benefit, as well as wildlife conservation for not-for-profit decision-making, education, or research. (The official license remains Apache 2.0.)""

https://github.com/google-research/perch","Industry,Government,Academia",,,,,,,,,
1X World Model,"Robotics,Video","Robotic manipulation,Video generation,Animal (human/non-human) imitation",1X,"Jack Monas, Eric Jang",2024-09-17,1X World Model,https://www.1x.tech/discover/1x-world-model,,,,,,,,,"""Over the last year, weâ€™ve gathered thousands of hours of data on EVE humanoids doing diverse mobile manipulation tasks in homes and offices and interacting with people. We combined the video and action data to train a world model that can anticipate future video from observations and actions.""",,,,,,,Unknown,"In machine learning, a world model is a computer program that can imagine how the world evolves in response to an agentâ€™s behavior. Building on advancements in video generation and world models for autonomous vehicles, we have trained a world model that serves as a virtual simulator for our robots.
From the same starting image sequence, our world model can imagine multiple futures from different robot action proposals.
It can also predict non-trivial object interactions like rigid bodies, effects of dropping objects, partial observability, deformable objects (curtains, laundry), and articulated objects (doors, drawers, curtains, chairs).",,,,"United States of America,Norway",,,,,,2025-05-01 10:42,,,,,,Industry,,,,,,,Industry,,,,,,,,,
Mistral Small v24.09,Language,"Language modeling/generation,Question answering",Mistral AI,"Albert Jiang, Alexandre Sablayrolles, Alexis Tacnet, Alok Kothari, Antoine Roux, Arthur Mensch, Audrey Herblin-Stoop, Augustin Garreau, Austin Birky, Bam4d, Baptiste Bout, Baudouin de Monicault, Blanche Savary, Carole Rambaud, Caroline Feldman, Devendra Singh Chaplot, Diego de las Casas, Diogo Costa, Eleonore Arcelin, Emma Bou Hanna, Etienne Metzger, Gaspard Blanchet, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Harizo Rajaona, Henri Roussez, Hichem Sattouf, Ian Mack, Jean-Malo Delignon, Jessica Chudnovsky, Justus Murke, Kartik Khandelwal, Lawrence Stewart, Louis Martin, Louis Ternon, Lucile Saulnier, LÃ©lio Renard Lavaud, Margaret Jennings, Marie Pellat, Marie Torelli, Marie-Anne Lachaux, Marjorie Janiewicz, MickaÃ«l Seznec, Nicolas Schuhl, Niklas Muhs, Olivier de Garrigues, Patrick von Platen, Paul Jacob, Pauline Buche, Pavan Kumar Reddy, Perry Savas, Pierre Stock, Romain Sauvestre, Sagar Vaze, Sandeep Subramanian, Saurabh Garg, Sophia Yang, Szymon Antoniak, Teven Le Scao, Thibault Schueller, Thibaut Lavril, Thomas Wang, ThÃ©ophile Gervet, TimothÃ©e Lacroix, Valera Nemychnikova, Wendy Shang, William El Sayed, William Marshall",2024-09-17,AI in abundance,"https://mistral.ai/news/september-24-release/
https://huggingface.co/mistralai/Mistral-Small-Instruct-2409",,,,22000000000.0,22B,,,,,,,,,,,Confident,"We are proud to unveil Mistral Small v24.09, our latest enterprise-grade small model, an upgrade of Mistral Small v24.02. Available under the Mistral Research License, this model offers customers the flexibility to choose a cost-efficient, fast, yet reliable option for use cases such as translation, summarization, sentiment analysis, and other tasks that do not require full-blown general purpose models.

With 22 billion parameters, Mistral Small v24.09 offers customers a convenient mid-point between Mistral NeMo 12B and Mistral Large 2, providing a cost-effective solution that can be deployed across various platforms and environments. As shown below, the new small model delivers significant improvements in human alignment, reasoning capabilities, and code over the previous model.",,,Open weights (non-commercial),France,,,,,,2025-05-01 10:42,,,,,,Industry,,,,,,,Industry,,,,,,mistralai,,,
OmniGen,Image generation,Image generation,Beijing Academy of Artificial Intelligence / BAAI,"Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Shuting Wang, Tiejun Huang, Zheng Liu",2024-09-17,OmniGen: Unified Image Generation,https://arxiv.org/abs/2409.11340,,,,3800000000.0,Table 2,,training time is not given and I am unable to calculate amount of tokens,,"""In this work, we have constructed a large-scale unified image generation
dataset for the first time, which we refer to as the X2I dataset, meaning ""anything to image"". ",100000000.0,"""The entire dataset comprises approximately 0.1 billion images.""

Stage Image Resolution Training Steps (K) Batch Size 
1 256Ã—256 500 1040 
2 512Ã—512 300 520 
3 1024Ã—1024 100 208 
4 2240Ã—2240 30 104 
5 Multiple 80 104 
",,,NVIDIA A800 PCIe 40 GB,,Confident,"In this work, we introduce OmniGen, a new diffusion model for unified image generation. Unlike popular diffusion models (e.g., Stable Diffusion), OmniGen no longer requires additional modules such as ControlNet or IP-Adapter to process diverse control conditions. OmniGenis characterized by the following features: 1) Unification: OmniGen not only demonstrates text-to-image generation capabilities but also inherently supports other downstream tasks, such as image editing, subject-driven generation, and visual-conditional generation. Additionally, OmniGen can handle classical computer vision tasks by transforming them into image generation tasks, such as edge detection and human pose recognition. 2) Simplicity: The architecture of OmniGen is highly simplified, eliminating the need for additional text encoders. Moreover, it is more user-friendly compared to existing diffusion models, enabling complex tasks to be accomplished through instructions without the need for extra preprocessing steps (e.g., human pose estimation), thereby significantly simplifying the workflow of image generation. 3) Knowledge Transfer: Through learning in a unified format, OmniGen effectively transfers knowledge across different tasks, manages unseen tasks and domains, and exhibits novel capabilities. We also explore the model's reasoning capabilities and potential applications of chain-of-thought mechanism. This work represents the first attempt at a general-purpose image generation model, and there remain several unresolved issues. We will open-source the related resources at this https URL to foster advancements in this field.",,,Open weights (unrestricted),China,,,,104.0,,2025-05-01 10:42,,,,,,Academia,,,,,Open source,"MIT License
https://github.com/VectorSpaceLab/OmniGen",Academia,,,,51221.81707940268,,,,,
Pixtral 12B,"Vision,Language,Multimodal","Language modeling/generation,Question answering,Visual question answering,Code generation",Mistral AI,Mistral AI Team,2024-09-17,Pixtral 12B - the first-ever multimodal Mistral model. Apache 2.0.,https://mistral.ai/news/pixtral-12b/,,,,12400000000.0,"""New 400M parameter vision encoder trained from scratch""
+
""12B parameter multimodal decoder based on Mistral Nemo""",,,Unspecified unreleased,,,"""We simply pass images through the vision encoder at their native resolution and aspect ratio, converting them into image tokens for each 16x16 patch in the image""",,,,,Confident,"Natively multimodal, trained with interleaved image and text data
Strong performance on multimodal tasks, excels in instruction following
Maintains state-of-the-art performance on text-only benchmarks
New 400M parameter vision encoder trained from scratch
12B parameter multimodal decoder based on Mistral Nemo
Supports variable image sizes and aspect ratios
Supports multiple images in the long context window of 128k tokens
License: Apache 2.0
Pixtral is trained to understand both natural images and documents, achieving 52.5% on the MMMU reasoning benchmark, surpassing a number of larger models. The model shows strong abilities in tasks such as chart and figure understanding, document question answering, multimodal reasoning and instruction following. Pixtral is able to ingest images at their natural resolution and aspect ratio, giving the user flexibility on the number of tokens used to process an image. Pixtral is also able to process any number of images in its long context window of 128K tokens. Unlike previous open-source models, Pixtral does not compromise on text benchmark performance to excel in multimodal tasks.",,,Open weights (unrestricted),France,,,,,,2025-05-30 14:34,,,,,,Industry,,,,,Unreleased,Apache 2.0,Industry,,,,,,,,,
Qwen2.5-32B,Language,"Language modeling/generation,Question answering,Quantitative reasoning",Alibaba,Qwen Team,2024-09-17,Qwen2.5: A Party of Foundation Models!,https://qwenlm.github.io/blog/qwen2.5/ ,,Training cost,,32500000000.0,32.5B,3.51e+24,6 * 32.5B parameters * 18 trillion tokens = 3.51 Ã— 10^24,Unspecified unreleased,,18000000000000.0,"""In terms of Qwen2.5, the language models, all models are pretrained on our latest large-scale dataset, encompassing up to 18 trillion tokens""",,,,,Confident,"In the past three months since Qwen2â€™s release, numerous developers have built new models on the Qwen2 language models, providing us with valuable feedback. During this period, we have focused on creating smarter and more knowledgeable language models. Today, we are excited to introduce the latest addition to the Qwen family: Qwen2.5. We are announcing what might be the largest opensource release in history! Letâ€™s get the party started!

The Qwen2.5-7B model surpasses its predecessors and counterparts in numerous benchmarks, despite having fewer non-embedding parameters. It demonstrates significant improvements across various tasks, achieving 74.2 on general benchmarks like MMLU, 49.8 on math challenges such as MATH, and 57.9 on coding tasks like HumanEval.",,,Open weights (unrestricted),China,,,,,,2025-05-28 15:57,,,,,,Industry,,,,,Unreleased,"Apache 2.0
https://huggingface.co/Qwen/Qwen2.5-32B",Industry,,,BF16,,Operation counting,Qwen,,,
Ovis1.6-Gemma2-9B,"Multimodal,Language,Vision","Language modeling/generation,Visual question answering",Alibaba,,2024-09-16,Ovis1.6-Gemma2-9B,https://huggingface.co/AIDC-AI/Ovis1.6-Gemma2-9B,,,,10200000000.0,,,,,,,,,,,,Confident,"We are excited to announce the open-sourcing of Ovis-1.6, our latest multi-modal large language model. Ovis is a novel Multimodal Large Language Model (MLLM) architecture, designed to structurally align visual and textual embeddings.",,,Open weights (unrestricted),China,"Gemma 2 9B,SigLIP 400M",,,,,2025-05-01 10:42,,,,,,Industry,,,,,,"https://huggingface.co/AIDC-AI/Ovis1.6-Gemma2-9B
apache 2.0",Industry,,,,,,AIDC-AI,,,
RNAdiffusion,Biology,RNA sequence generation,"Princeton University,Tsinghua University,Stanford University","Kaixuan Huang, Yukang Yang, Kaidi Fu, Yanyi Chu, Le Cong, Mengdi Wang",2024-09-15,Latent Diffusion Models for Controllable RNA Sequence Generation,https://arxiv.org/abs/2409.09828,0.0,,,,,2.481192e+19,"Autoencoder: 8.5h per epoch on single H100, 2 epochs total
Diffusion model: 3.5h per epoch on single H100, 3 epochs total
Reward model: 7h on a single A100

Total training time: 2*8.5+3*3.5+7=34.5

Compute: 
((2*8.5+3*3.5)*756000000000000+7*312000000000000)*60*60*0.3=2.481192e+19",,,421800000.0,"Diffusion part: 1.1M sequences. 205k UTR sequences

Reward model: 101k sequences

Total: 1100000+205000+101000=1406000 sequences

Assuming average token length of 300: 1406000*300=421800000",34.5,,"NVIDIA H100 PCIe,NVIDIA A100",,Likely,"This paper presents RNAdiffusion, a latent diffusion model for generating and optimizing discrete RNA sequences. RNA is a particularly dynamic and versatile molecule in biological processes. RNA sequences exhibit high variability and diversity, characterized by their variable lengths, flexible three-dimensional structures, and diverse functions. We utilize pretrained BERT-type models to encode raw RNAs into token-level biologically meaningful representations. A Q-Former is employed to compress these representations into a fixed-length set of latent vectors, with an autoregressive decoder trained to reconstruct RNA sequences from these latent variables. We then develop a continuous diffusion model within this latent space. To enable optimization, we train reward networks to estimate functional properties of RNA from the latent variables. We employ gradient-based guidance during the backward diffusion process, aiming to generate RNA sequences that are optimized for higher rewards. Empirical experiments confirm that RNAdiffusion generates non-coding RNAs that align with natural distributions across various biological indicators. We fine-tuned the diffusion model on untranslated regions (UTRs) of mRNA and optimize sample sequences for protein translation efficiencies. Our guided diffusion model effectively generates diverse UTR sequences with high Mean Ribosome Loading (MRL) and Translation Efficiency (TE), surpassing baselines. These results hold promise for studies on RNA sequence-function relationships, protein synthesis, and enhancing therapeutic RNA design.",,,,"United States of America,China,United States of America",RNA-FM,,,1.0,,2025-05-01 10:42,,,,,,"Academia,Academia,Academia",,,,,,,"Academia,Academia,Academia",,,,,Hardware,,,,
UdanDTI,Biology,"Protein-ligand contact prediction,Drug discovery",Tsinghua University,"Pei-Dong Zhang, Jianzhu Ma, Ting Chen",2024-09-15,Escaping the Drug-Bias Trap: Using Debiasing Design to Improve Interpretability and Generalization of Drug-Target Interaction Prediction,https://www.biorxiv.org/content/10.1101/2024.09.12.612771v1.abstract,,,,,,,"
",,,37869000.0,"BindingDB:
20,675 + 20,675 = 41,350

BioSNAP:
13,830 + 13,830 = 27,660

Human
3364+3364 = 6728

Total interactions: 75738

""The maximum length of the sequence input to
the encoder was 290 for drugs and 1200 for proteins""

Assuming 500 tokens on average per drug-protein interaction

Dataset size: 75738*500",,,NVIDIA A40 PCIe,,Confident,"Considering the high cost associated with determining reaction affinities through in-vitro experiments, virtual screening of potential drugs bound with specific protein pockets from vast compounds is critical in AI-assisted drug discovery. Deep-leaning approaches have been proposed for Drug-Target Interaction (DTI) prediction. However, they have shown overestimated accuracy because of the drug-bias trap, a challenge that results from excessive reliance on the drug branch in the traditional drug-protein dual-branch network approach. This casts doubt on the interpretability and generalizability of existing Drug-Target Interaction (DTI) models. Therefore, we introduce UdanDTI, an innovative deep-learning architecture designed specifically for predicting drug-protein interactions. UdanDTI applies an unbalanced dual-branch system and an attentive aggregation module to enhance interpretability from a biological perspective. Across various public datasets, UdanDTI demonstrates outstanding performance, outperforming state-of-the-art models under in-domain, cross-domain, and structural interpretability settings. Notably, it demonstrates exceptional accuracy in predicting drug responses of two crucial subgroups of Epidermal Growth Factor Receptor (EGFR) mutations associated with non-small cell lung cancer, consistent with experimental results. Meanwhile, UdanDTI could complement the advanced molecular docking software DiffDock. The codes and datasets of UdanDTI are available at https://github.com/CQ-zhang-2016/UdanDTI.",200.0,,,China,,,,1.0,,2025-05-01 10:42,,,,,,Academia,,,,,,,Academia,,,,324.75125836135885,Hardware,,,,
ProtRNA,Biology,"RNA structure prediction,RNA-Protein interaction prediction,Mean ribosome load prediction","Fudan University,Shanghai AI Lab","Ruoxi Zhang, Ben Ma, Gang Xu, Jianpeng Ma",2024-09-14,ProtRNA: A Protein-derived RNA Language Model by Cross-Modality Transfer Learning,https://www.biorxiv.org/content/10.1101/2024.09.10.612218v1.abstract,0.0,,,650000000.0,,,,,,3072000001.0,"Number of sequences: 6 million
Tokens per sequence: 512
Total tokens: 6,000,000 Ã— 512 = 3.072 Ã— 10^9

Final estimate: 3.072 billion data points",,,NVIDIA V100,,Confident,"Protein language models (PLM), such as the highly successful ESM-2, have proven to be particularly effective. However, language models designed for RNA continue to face challenges. A key question is: can the information derived from PLMs be harnessed and transferred to RNA? To investigate this, a model termed ProtRNA has been developed by cross-modality transfer learning strategy for addressing the challenges posed by RNAâ€™s limited and less conserved sequences. By leveraging the evolutionary and physicochemical information encoded in protein sequences, the ESM-2 model is adapted to processing ""low-resource"" RNA sequence data. The results show comparable or even superior performance in various RNA downstream tasks, with only 1/8 the trainable parameters and 1/6 the training data employed by other baseline RNA language models. This approach highlights the potential of cross-modality transfer learning in biological language models.",6.0,,Open weights (unrestricted),"China,China",ESM2-650M,,,4.0,,2025-06-16 13:11,,,,,,"Academia,Academia",,,,,Unreleased,"Apache 2.0 for weights and inference code
https://github.com/roxie-zhang/ProtRNA

trainiing script seems to be not fully provided","Academia,Academia",,,,2364.241810463115,,,,,
PLTNUM,Biology,Protein property prediction,"Kyoto University,National Institute of Biomedical Innovation,RIKEN","Tatsuya Sagawa, Eisuke Kanao, Kosuke Ogata, Koshi Imami, Yasushi Ishihama",2024-09-14,Prediction of Protein Half-lives from Amino Acid Sequences by Protein Language Models,https://www.biorxiv.org/content/10.1101/2024.09.10.612367v1.abstract,,,,650000000.0,,,,,,2100001.0,"Total Datapoints = Number of Proteins Ã— Sequence Length
Total Datapoints = 4,162 Ã— 510 = 2,122,620 tokens
Final result â‰ˆ 2.1 Ã— 10^6 tokens",,,,,Confident,"We developed a protein half-life prediction model, PLTNUM, based on a protein language model using an extensive dataset of protein sequences and protein half-lives from the NIH3T3 mouse embryo fibroblast cell line as a training set. PLTNUM achieved an accuracy of 71% on validation data and showed robust performance with an ROC of 0.73 when applied to a human cell line dataset. By incorporating Shapley Additive Explanations (SHAP) into PLTNUM, we identified key factors contributing to shorter protein half-lives, such as cysteine-containing domains and intrinsically disordered regions. Using SHAP values, PLTNUM can also predict potential degron sequences that shorten protein half-lives. This model provides a platform for elucidating the sequence dependency of protein half-lives, while the uncertainty in predictions underscores the importance of biological context in influencing protein half-lives.

",10.0,,,"Japan,Japan,Japan",SaProt,,,,,2025-05-01 10:42,,,,,,"Academia,Government",,,,,,,"Academia,Government",,,,,,,,,
LEGO,Biology,Protein-ligand binding affinity prediction,"Chinese Academy of Sciences,Beijing Academy of Artificial Intelligence / BAAI,University of Chinese Academy of Sciences","Yuancheng Sun, Kai Chen, Kang Liu, Qiwei Ye",2024-09-14,3D Molecular Pretraining via Localized Geometric Generation,https://www.biorxiv.org/content/10.1101/2024.09.10.612249v1,,,,,,,,,,,,,,,,Unknown,"Self-supervised learning on 3D molecular structures is gaining importance in data-driven scientific research and applications due to the high costs of annotating bio-chemical data. However, the strategic selection of semantic units for modeling 3D molecular structures remains underexplored, despite its crucial role in effective pre-trainingâ€”a concept well-established in language processing and computer vision. We introduce Localized Geometric Generation (LEGO), a novel approach that treats tetrahedrons within 3D molecular structures as fundamental building blocks, leveraging their geometric simplicity and widespread presence across chemical functional patterns. Inspired by masked modeling, LEGO perturbs tetrahedral local structures and learns to reconstruct them in a self-supervised manner. Experimental results demonstrate LEGO consistently enhances molecular representations across biochemistry and quantum property prediction benchmarks. Additionally, the tetrahedral modeling and pretraining generalize from small molecules to larger molecular systems, validating by protein-ligand affinity prediction. Our results highlight the potential of selecting semantic units to build more expressive and interpretable neural networks for scientific AI applications.",,,Unreleased,"China,China,China",,,,,,2025-06-13 15:01,,,,,,"Academia,Academia,Academia",,,,,Unreleased,,"Academia,Academia,Academia",,,,,,,,,
MolSnapper,Biology,Drug discovery,University of Oxford,"Yael Ziv, Brian Marsden, Charlotte M. Deane",2024-09-14,MolSnapper: Conditioning Diffusion for Structure Based Drug Design ,https://www.biorxiv.org/content/10.1101/2024.03.28.586278v2.abstract,,,,,,,,,,,"Evaluated on:
CrossDocked2020 (100,000) + Binding MOAD (40,344) = 140,344 total data points

100,000 + 40,344 = 140,344 â‰ˆ 1.4e5",,,,,Unknown,"Generative models have emerged as potentially powerful methods for molecular design, yet challenges persist in generating molecules that effectively bind to the intended target. The ability to control the design process and incorporate prior knowledge would be highly beneficial for better tailoring molecules to fit specific binding sites. In this paper, we introduce MolSnapper, a novel tool that is able to condition diffusion models for structure-based drug design by seamlessly integrating expert knowledge in the form of 3D pharmacophores. We demonstrate through comprehensive testing on both the CrossDocked and Binding MOAD datasets, that our method generates molecules better tailored to fit a given binding site, achieving high structural and chemical similarity to the original molecules. It also, when compared to alternative methods, yields approximately twice as many valid molecules.",,,,United Kingdom of Great Britain and Northern Ireland,,,,,,2025-05-01 10:42,,,,,,Academia,,,,,,,Academia,,,,,,,,,
YOLOv8 (reCAPTCHA fine-tuned),Vision,"Image classification,Image segmentation",ETH Zurich,"Andreas Plesner, Tobias Vontobel, Roger Wattenhofer",2024-09-13,Breaking reCAPTCHAv2,https://arxiv.org/abs/2409.08831,,,,,,,,,,14000.0,"Combined with the public data, this resulted in around 14k image/label
pairs for fine-tuning the classification model.",,,,,Confident,"Our work examines the efficacy of employing advanced machine learning methods to solve captchas from Google's reCAPTCHAv2 system. We evaluate the effectiveness of automated systems in solving captchas by utilizing advanced YOLO models for image segmentation and classification. Our main result is that we can solve 100% of the captchas, while previous work only solved 68-71%. Furthermore, our findings suggest that there is no significant difference in the number of challenges humans and bots must solve to pass the captchas in reCAPTCHAv2. This implies that current AI technologies can exploit advanced image-based captchas. We also look under the hood of reCAPTCHAv2, and find evidence that reCAPTCHAv2 is heavily based on cookie and browser history data when evaluating whether a user is human or not. The code is provided alongside this paper.",,,,Switzerland,YOLOv8x,,,,,2025-06-04 17:06,,,,,,Academia,,,,,Open (non-commercial),"https://github.com/aplesner/Breaking-reCAPTCHAv2
no clear license",Academia,,,,,,,,,
IDPFold,Biology,Protein folding prediction,"Shandong University,BioMap Research,Fuzhou University,Shanghai Jiao Tong University","Junjie Zhu, Zhengxin Li, Zhuoqi Zheng, Bo Zhang, Bozitao Zhong, Jie Bai, Xiaokun Hong, Taifeng Wang, Ting Wei, Jianyi Yang, Hai-Feng Chen",2024-09-13,Precise Generation of Conformational Ensembles for Intrinsically Disordered Proteins via Fine-tuned Diffusion Models,https://www.biorxiv.org/content/10.1101/2024.05.05.592611.abstract,,,,17800000.0,Taken from Table 2,2.59999999999998e+20,"1. Hardware setup: 1x NVIDIA A100 GPU with 3.12Ã—10Â¹â´ FLOP/s (fp16_tensor)

2. Training duration: Directly provided as 24 GPU days (9 days initial + 15 days second phase) = 2.0736Ã—10â¶ seconds

3. Utilization rate: 40%

4. Final calculation:
3.12Ã—10Â¹â´ FLOP/s Ã— 1 GPU Ã— 2.0736Ã—10â¶ s Ã— 0.4 = 2.6Ã—10Â²â° FLOP",,,7500001.0,"Initial Phase: 25,495 sequences Ã— 200 residues = 5,099,000 points
Second Phase: 3,880 sequences Ã— 300 residues = 1,164,000 points
Total: 5,099,000 + 1,164,000 = 6,263,000 points
Rounded: 7.5 million data points",,,NVIDIA A100,,Likely,"Intrinsically disordered proteins (IDPs) play pivotal roles in various biological functions and are closely linked to many human diseases including cancer, diabetes and Alzheimer disease. Structural investigations of IDPs typically involve a combination of molecular dynamics (MD) simulations and experimental data to correct for intrinsic biases in simulation methods. However, these simulations are hindered by their high computational cost and a scarcity of experimental data, severely limiting their applicability. Despite the recent advancements in structure prediction for structured proteins, understanding the conformational properties of IDPs remains challenging partly due to the poor conservation of disordered protein sequences and limited experimental characterization. Here, we introduce IDPFold, a method capable of generating conformational ensembles for IDPs directly from their sequences using fine-tuned diffusion models. IDPFold bypasses the need for Multiple Sequence Alignments (MSA) or experimental data, achieving accurate predictions of ensemble properties across numerous IDPs. By sampling conformations at the backbone level, IDPFold provides more detailed structural features and more precise property estimation compared to other state-of-the-art methods. IDPFold is ready to be used in the elucidate the sequence-disorder-function paradigm of IDPs.",,,,"China,China,China,China",ESM2-650M,,,,,2025-05-01 10:42,,,,,,"Academia,Industry,Academia,Academia",,,,,,,"Academia,Industry,Academia,Academia",,,,,Hardware,,,,
Novae,Biology,Spatial Transcriptomics,"CentraleSupelec,Gustave Roussy,UniversitÃ© Paris CitÃ©","Quentin Blampey, Hakim Benkirane, NadÃ¨ge Bercovici, Fabrice AndrÃ©, Paul-Henry CournÃ¨de",2024-09-13,Novae: a graph-based foundation model for spatial transcriptomics data,https://www.biorxiv.org/content/10.1101/2024.09.09.612009v1.abstract,,,,32000000.0,32M (safetensors),1.1e+19,"""Novae was trained on a Nvidia HGX A100 GPU for 24 hours."" Assume FP16 tensor precision and 40% utilization. ",,,30000000.0,"""This allowed us  169 to train Novae on a dataset composed of nearly 30 million cells using a GPU with 40GB of RAM (see  170 subsection 4.14 for more details).""",24.0,,NVIDIA A100 SXM4 40 GB,,Confident,"Spatial transcriptomics is advancing molecular biology by providing high-resolution insights into gene expression within the spatial context of tissues. This context is essential for identifying spatial domains, enabling the understanding of micro-environment organizations and their implications for tissue function and disease progression. To improve current model limitations on multiple slides, we have designed Novae (https://github.com/MICS-Lab/novae), a graph-based foundation model that extracts representations of cells within their spatial contexts. Our model was trained on a large dataset of nearly 30 million cells across 18 tissues, allowing Novae to perform zero-shot domain inference across multiple gene panels, tissues, and technologies. Unlike other models, it also natively corrects batch effects and constructs a nested hierarchy of spatial domains. Furthermore, Novae supports various downstream tasks, including spatially variable gene or pathway analysis and spatial domain trajectory analysis. Overall, Novae provides a robust and versatile tool for advancing spatial transcriptomics and its applications in biomedical research.",,,Open weights (non-commercial),"France,France,France",,,,1.0,,2025-06-12 14:48,,,,,,Academia,,,,,Open source,"BSD-3 Clause license
https://github.com/MICS-Lab/novae

No clear license:
https://huggingface.co/MICS-Lab/novae-human-0",Academia,,,,433.0209635948764,Hardware,MICS-Lab,,,
CodonTransformer,Biology,Codon optimization,"Vector Institute,University of Toronto,UniversitÃ© Paris CitÃ©","Adibvafa Fallahpour, Vincent Gureghian, Guillaume J. Filion, Ariel B. Lindner, Amir Pandi",2024-09-13,CodonTransformer: a multispecies codon optimizer using context-aware neural networks,https://www.biorxiv.org/content/10.1101/2024.09.13.612903v1.abstract,,,,89600000.0,"""total number of parameters to 89.6 million.""",,,,,300359100.0,"1,001,197 sequences Ã— 300 tokens/sequence = 300359100",,,NVIDIA V100,,Confident,"The genetic code is degenerate allowing a multitude of possible DNA sequences to encode the same protein. This degeneracy impacts the efficiency of heterologous protein production due to the codon usage preferences of each organism. The process of tailoring organism-specific synonymous codons, known as codon optimization, must respect local sequence patterns that go beyond global codon preferences. As a result, the search space faces a combinatorial explosion that makes exhaustive exploration impossible. Nevertheless, throughout the diverse life on Earth, natural selection has already optimized the sequences, thereby providing a rich source of data allowing machine learning algorithms to explore the underlying rules. Here, we introduce CodonTransformer, a multispecies deep learning model trained on over 1 million DNA-protein pairs from 164 organisms spanning all kingdoms of life. The model demonstrates context-awareness thanks to the attention mechanism and bidirectionality of the Transformers we used, and to a novel sequence representation that combines organism, amino acid, and codon encodings. CodonTransformer generates host-specific DNA sequences with natural-like codon distribution profiles and with negative cis-regulatory elements. This work introduces a novel strategy of Shared Token Representation and Encoding with Aligned Multi-masking (STREAM) and provides a state-of-the-art codon optimization framework with a customizable open-access model and a user-friendly interface.",20.0,,,"Canada,Canada,France",,,,16.0,,2025-05-01 10:42,,,,,,"Academia,Academia,Academia",,,,,,,"Academia,Academia,Academia",,,,9457.1778449121,,,,,
o1-mini,Language,"Code generation,Language modeling/generation,Quantitative reasoning,Chat,Question answering,Translation",OpenAI,,2024-09-12,Learning to reason with LLMs,https://openai.com/index/learning-to-reason-with-llms/,,Significant use,"Model available in ChatGPT, likely widely used ",,,,,Unspecified unreleased,"""Both models were trained on a variety of publicly available datasets, including web data and open-source datasets. Key components include reasoning data and scientific literature. <..> To further enhance the capabilities of o1-preview and o1-mini, we formed partnerships to access high-value non-public datasets. These proprietary data sources include paywalled content, specialized archives, and other domain-specific datasets that provide deeper insights into industry-specific knowledge and use cases.""

This has a knowledge cutoff date of October 2023 - which I assume means October 1, 2023 - according to https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models?tabs=global-standard%2Cstandard-chat-completions#o-series-models.",,,,,,,Unknown,"We've developed a new series of AI models designed to spend more time thinking before they respond. They can reason through complex tasks and solve harder problems than previous models in science, coding, and math.

...

Weâ€™re also releasing OpenAI o1-mini, a faster, cheaper reasoning model that is particularly effective at coding. As a smaller model, o1-mini is 80% cheaper than o1-preview, making it a powerful, cost-effective model for applications that require reasoning but not broad world knowledge.",,,API access,United States of America,,,,,,2025-06-12 14:54,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
o1-preview,"Language,Mathematics,Biology","Code generation,Language modeling/generation,Quantitative reasoning,Chat,Question answering,Translation",OpenAI,,2024-09-12,A new series of reasoning models for solving hard problems.,https://openai.com/index/introducing-openai-o1-preview/,,"SOTA improvement,Significant use",SOTA in GPQA among others: https://openai.com/index/learning-to-reason-with-llms/ ,,,,,Unspecified unreleased,"""Both models were trained on a variety of publicly available datasets, including web data and open-source datasets. Key components include reasoning data and scientific literature. <..> To further enhance the capabilities of o1-preview and o1-mini, we formed partnerships to access high-value non-public datasets. These proprietary data sources include paywalled content, specialized archives, and other domain-specific datasets that provide deeper insights into industry-specific knowledge and use cases.""

This has a knowledge cutoff date of October 2023 - which I assume means October 1, 2023 - according to https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models?tabs=global-standard%2Cstandard-chat-completions#o-series-models.",,,,,,,Unknown,"We've developed a new series of AI models designed to spend more time thinking before they respond. They can reason through complex tasks and solve harder problems than previous models in science, coding, and math.

Today, we are releasing the first of this series in ChatGPT and our API. This is a preview and we expect regular updates and improvements. Alongside this release, weâ€™re also including evaluations for the next update, currently in development.",,,API access,United States of America,,,,,,2025-05-12 19:11,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
4D Diffusion for Dynamic Protein Structure Prediction with Reference Guided Motion Alignment,Biology,Protein folding prediction,"Fudan University,Shanghai Academy of Artificial Intelligence for Science,Nanjing University","Kaihui Cheng, Ce Liu, Qingkun Su, Jun Wang, Liwei Zhang, Yining Tang, Yao Yao, Siyu Zhu, Yuan Qi",2024-09-12,"4D Diffusion for Dynamic Protein Structure Prediction with Reference Guided Motion Alignment
",https://arxiv.org/abs/2408.12419,,,,,,,"
",,,24448.0,"
This model is not trained on a token level and processes a protein in a single forward pass. 
764 proteins * 32 steps = 24448",,,NVIDIA A100,,Confident,"Protein structure prediction is pivotal for understanding the structure-function relationship of proteins, advancing biological research, and facilitating pharmaceutical development and experimental design. While deep learning methods and the expanded availability of experimental 3D protein structures have accelerated structure prediction, the dynamic nature of protein structures has received limited attention. This study introduces an innovative 4D diffusion model incorporating molecular dynamics (MD) simulation data to learn dynamic protein structures. Our approach is distinguished by the following components: (1) a unified diffusion model capable of generating dynamic protein structures, including both the backbone and side chains, utilizing atomic grouping and side-chain dihedral angle predictions; (2) a reference network that enhances structural consistency by integrating the latent embeddings of the initial 3D protein structures; and (3) a motion alignment module aimed at improving temporal structural coherence across multiple time steps. To our knowledge, this is the first diffusion-based model aimed at predicting protein trajectories across multiple time steps simultaneously. Validation on benchmark datasets demonstrates that our model exhibits high accuracy in predicting dynamic 3D structures of proteins containing up to 256 amino acids over 32 time steps, effectively capturing both local flexibility in stable states and significant conformational changes.",550.0,,Unreleased,"China,China,China",,,,1.0,,2025-06-16 13:07,,,,,,"Academia,Academia",,,,,Unreleased,the page is not accessible anymore https://github.com/fudan-generative-vision/AlphaFolding,"Academia,Academia",,,,433.0306068068584,Hardware,,,,
Demostart (in progress),Robotics,Robotic manipulation,Google DeepMind,"Maria Bauza, Jose Enrique Chen, Valentin Dalibard, Nimrod Gileadi, Roland Hafner, Murilo F. Martins, Joss Moore, Rugile Pevceviciute, Antoine Laurens, Dushyant Rao, Martina Zambelli, Martin Riedmiller, Jon Scholz, Konstantinos Bousmalis, Francesco Nori, Nicolas Heess",2024-09-12,DemoStart: Demonstration-led auto-curriculum applied to sim-to-real with multi-fingered robots,https://arxiv.org/abs/2409.06613,,,,,,,,,,,"""We trained our agents with a batch size of 256. Agents were trained until convergence which typically took under 10M learner updates but took 32M learner updates on the Screwdriver in cup task.""",,,,,Unknown,"We present DemoStart, a novel auto-curriculum reinforcement learning method capable of learning complex manipulation behaviors on an arm equipped with a three-fingered robotic hand, from only a sparse reward and a handful of demonstrations in simulation. Learning from simulation drastically reduces the development cycle of behavior generation, and domain randomization techniques are leveraged to achieve successful zero-shot sim-to-real transfer. Transferred policies are learned directly from raw pixels from multiple cameras and robot proprioception. Our approach outperforms policies learned from demonstrations on the real robot and requires 100 times fewer demonstrations, collected in simulation. More details and videos in this https URL.",,,Unreleased,"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France",,,,,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
ALOHA Unleashed,Robotics,Robotic manipulation,Google DeepMind,"Tony Z. Zhao, Jonathan Tompson, Danny Driess, Pete Florence, Kamyar Ghasemipour, Chelsea Finn, Ayzaan Wahid
",2024-09-12,ALOHA Unleashed: A Simple Recipe for Robot Dexterity,https://aloha-unleashed.github.io/assets/aloha_unleashed.pdf,,,,217000000.0,"""the Base model consists of 217M learnable parameters""",3.6084096e+21,265 hours *3600 sec / hour *64 GPUs *197000000000000 FLOP / s *0.3 [assumed utilization] = 3.6084096e+21 FLOP,,,,"""we collect over 26k episodes for 5 real tasks, on 10 different robots in 2 different buildings over the course of 8 months""

""images resized to 256x256""",265.0,"""We train our models with JAX [51] using 64 TPUv5e chips with a data parallel mesh. We use a batch size of 256 and train for 2M steps (about 265 hours of training). """,Google TPU v5e,,Confident,"Recent work has shown promising results for learning end-to-end robot policies using imitation learning. In this work we address the question of how far can we push imitation learning for challenging dexterous anipulation tasks. We show that a simple recipe of large scale data collection on the ALOHA 2 platform, combined with expressive models such as Diffusion Policies, can be effective in learning challenging bimanual manipulation tasks involving deformable objects and complex contact rich dynamics. We demonstrate our recipe on 5 challenging real-world and 3 simulated tasks and demonstrate improved performance over state-of-the-art baselines. The project website and videos can be found at alohaunleashed.github.io.",,,Unreleased,"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France",,,,64.0,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,,Industry,,,,17653.791778302002,Hardware,,,,
DataGemma,Language,Language modeling/generation,Google,"Prashanth Radhakrishnan, Jennifer Chen, Bo Xu, Prem Ramaswami, Hannah Pho, Adriana Olmos, James Manyika, R. V. Guha",2024-09-12,Knowing When to Ask -- Bridging Large Language Models and Data,https://arxiv.org/abs/2409.13741,,,,27200000000.0,27.2B,,,Data Commons,,,,,,Google TPU v5e,,Confident,"Large Language Models (LLMs) are prone to generating factually incorrect information when responding to queries that involve numerical and statistical data or other timely facts. In this paper, we present an approach for enhancing the accuracy of LLMs by integrating them with Data Commons, a vast, open-source repository of public statistics from trusted organizations like the United Nations (UN), Center for Disease Control and Prevention (CDC) and global census bureaus. We explore two primary methods: Retrieval Interleaved Generation (RIG), where the LLM is trained to produce natural language queries to retrieve data from Data Commons, and Retrieval Augmented Generation (RAG), where relevant data tables are fetched from Data Commons and used to augment the LLM's prompt. We evaluate these methods on a diverse set of queries, demonstrating their effectiveness in improving the factual accuracy of LLM outputs. Our work represents an early step towards building more trustworthy and reliable LLMs that are grounded in verifiable statistical data and capable of complex factual reasoning.",,,Open weights (restricted use),United States of America,Gemma 2 27B,,,,,2025-05-01 10:42,,,,,,Industry,,,,,,"Gemma license
https://huggingface.co/google/datagemma-rag-27b-it",Industry,,,,,,,,,
E2 TTS,Speech,"Text-to-speech,Speech synthesis",Microsoft,"Sefik Emre Eskimez, Xiaofei Wang, Manthan Thakker, Canrun Li, Chung-Hsien Tsai, Zhen Xiao, Hemin Yang, Zirun Zhu, Min Tang, Xu Tan, Yanqing Liu, Sheng Zhao, Naoyuki Kanda",2024-09-12,E2 TTS: Embarrassingly Easy Fully Non-Autoregressive Zero-Shot TTS,https://arxiv.org/abs/2406.18009,,,,335000000.0,"""The architecture incorporated U-Net [23] style skip connections, 24 layers, 16 attention heads, an embedding dimension of 1024, a linear layer dimension of 4096. The character embedding vocabulary size was 399.5 The total number of parameters amounted to 335 million""",4.939776e+20,6 FLOP / token / parameter * 245760000000 tokens * 335000000 parameters = 4.939776e+20 FLOP ,"LibriHeavy,Unspecified unreleased",,245760000000.0,"""We utilized the Libriheavy dataset [30] to train our models. The Libriheavy dataset comprises 50,000 hours of read English speech from 6,736 speakers, accompanied by transcriptions that preserve case and punctuation marks.""

""We also used a proprietary 200,000 hours of training data to investigate the scalability of the E2 TTS model.""

""All models were trained for 800,000 mini-batch updates with an effective mini-batch size of 307,200 audio frames.""

Token count estimate: 307,200 Ã— 800,000  = 245760000000 tokens",,,,,Confident,"This paper introduces Embarrassingly Easy Text-to-Speech (E2 TTS), a fully non-autoregressive zero-shot text-to-speech system that offers human-level naturalness and state-of-the-art speaker similarity and intelligibility. In the E2 TTS framework, the text input is converted into a character sequence with filler tokens. The flow-matching-based mel spectrogram generator is then trained based on the audio infilling task. Unlike many previous works, it does not require additional components (e.g., duration model, grapheme-to-phoneme) or complex techniques (e.g., monotonic alignment search). Despite its simplicity, E2 TTS achieves state-of-the-art zero-shot TTS capabilities that are comparable to or surpass previous works, including Voicebox and NaturalSpeech 3. The simplicity of E2 TTS also allows for flexibility in the input representation. We propose several variants of E2 TTS to improve usability during inference. See this https URL for demo samples.",,,Unreleased,"United States of America,Multinational,India,Belgium",,,,,,2025-05-06 16:23,,,,,,Industry,,,,,Unreleased,,Industry,,,,,Operation counting,,,,
Solar Pro,Language,"Language modeling/generation,Question answering,Translation",Upstage,,2024-09-11,Upstage to Release Preview of Next-Generation LLM â€˜Solar Proâ€™,https://www.upstage.ai/news/solar-pro-preview,,,,22000000000.0,22B,,,,,,,,,,,Confident,"Upstage today announced the release of a preview version of its next-generation large language model (LLM), Solar Pro. This preview, available as an open-source model with free API access, gives developers and businesses the opportunity to test and provide feedback on the model ahead of its official release in November.

As the flagship model of the Solar LLM series, Solar Pro features 22 billion parameters, more than double the size of its predecessor, Solar Mini. Despite its increased size, Solar Pro is optimized to run efficiently on a single GPU, thanks to Upstage's proprietary Depth-Up Scaling (DUS) method and advanced data recipe. This innovation enables Solar Pro to deliver state-of-the-art performance while significantly reducing model sizeâ€”an essential advantage in the face of rising GPU costs and supply constraints in the AI landscape.

Solar Pro's advanced capabilities have translated to impressive gains in key LLM benchmarks, with an average improvement of 51% compared to Solar Mini. It achieved an accuracy score of 52.11 on the MMLU Pro benchmark, which measures multi-disciplinary language understanding and reasoning across 14 domains. Additionally, Solar Pro excelled in the IFEval benchmark with a score of 84.37, showcasing its ability to follow complex instructions with intelligence comparable to that of humans.

These results surpass those of similar-sized models from leading tech companies, including Microsoft's Phi 3 Medium, Metaâ€™s Llama 3.1 8B, Mistral NeMo 12B, and Google's Gemma 2 27B. Solar Pro even competes with much larger models that require multiple GPUs, such as Llama 3.1 70B, which has three times the parameter count. By setting a new standard in both general and specialized tasks, Solar Pro positions itself as the most intelligent and efficient LLM available on the market today.",,,API access,Korea (Republic of),,,,,,2025-05-12 15:20,,,,,,Industry,,,,,Unreleased,"preview model (limited languages, limited context length) is open sourced under MIT license: https://huggingface.co/upstage/solar-pro-preview-instruct

production model is available via API",Industry,,,,,,,,,
GenMS,Materials science,Crystal discovery,Google DeepMind,"Sherry Yang, Simon Batzner, Ruiqi Gao, Muratahan Aykol, Alexander L. Gaunt, Brendan McMorrow, Danilo J. Rezende, Dale Schuurmans, Igor Mordatch, Ekin D. Cubuk",2024-09-10,Generative Hierarchical Materials Search,https://arxiv.org/abs/2409.06762,,,,,,,,"ICSD (inorganic crystal structure database),Materials Project","""we observe that there is a wealth of language-to-formula data available online, including Wikipedia articles, research papers, and textbooks. This data can be complemented by formula-to-structure information from specialized
materials databases such as the Materials Project (Jain et al., 2013), ICSD (Hellenbrandt, 2004), OQMD (Kirklin et al., 2015), etc.""

""GenMS consists of (1) a large language model (LLM) pretrained on high-level materials science knowledge from the internet, (2) a diffusion model trained on specialized crystal structure databases, and (3) a graph neural network (GNN) for property prediction. ""

the ICSD contains more than 250,000 crystal structure entries
Materials Project contains over 140,000 entries for crystal structures
",,"for training dissusion model (there were also language and GNN modeules trained):

Batch size 512
Training steps 200000",,,Google TPU v4,,Unknown,"Generative models trained at scale can now produce text, video, and more recently, scientific data such as crystal structures. In applications of generative approaches to materials science, and in particular to crystal structures, the guidance from the domain expert in the form of high-level instructions can be essential for an automated system to output candidate crystals that are viable for downstream research. In this work, we formulate end-to-end language-to-structure generation as a multi-objective optimization problem, and propose Generative Hierarchical Materials Search (GenMS) for controllable generation of crystal structures. GenMS consists of (1) a language model that takes high-level natural language as input and generates intermediate textual information about a crystal (e.g., chemical formulae), and (2) a diffusion model that takes intermediate information as input and generates low-level continuous value crystal structures. GenMS additionally uses a graph neural network to predict properties (e.g., formation energy) from the generated crystal structures. During inference, GenMS leverages all three components to conduct a forward tree search over the space of possible structures. Experiments show that GenMS outperforms other alternatives of directly using language models to generate structures both in satisfying user request and in generating low-energy structures. We confirm that GenMS is able to generate common crystal structures such as double perovskites, or spinels, solely from natural language input, and hence can form the foundation for more complex structure generation in near future.",,,Unreleased,"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France",,,,64.0,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,,Industry,,,,21437.701946391924,,,,,
MolPhenix,Biology,Cell Biology,"Valence Labs,University of British Columbia (UBC),Vector Institute,University of Toronto,University of Montreal / UniversitÃ© de MontrÃ©al,Mila - Quebec AI (originally Montreal Institute for Learning Algorithms)","Philip Fradkin, Puria Azadi, Karush Suri, Frederik Wenkel, Ali Bashashati, Maciej Sypetkowski, Dominique Beaini",2024-09-10,How Molecules Impact Cells: Unlocking Contrastive PhenoMolecular Retrieval,https://arxiv.org/abs/2409.08302,,,,38700000.0,"Taken from table 7, ""model size"" row; ""medium (38.7M)""",4.26816e+18,"""We utilized an NVIDIA A100 GPU to train Molphenix using Phenom1 and MolGPS embeddings, which takes approximately âˆ¼4.75 hours each."" Assume FP16 precision, 40% utilization. 

9.5*60*60*312000000000000*0.4=4.26816e+18",,,2150001.0,"2,150,000 datapoints

Based on:
- 1,316,283 molecule-concentration pairs
- >2,150,000 phenomic experiments 
Final count = 2,150,000 unique datapoints
(2.15e6)",9.5,"""We utilized an NVIDIA A100 GPU to train Molphenix using Phenom1 and MolGPS embeddings, which takes approximately âˆ¼4.75 hours each.""",NVIDIA A100,,Confident,"Predicting molecular impact on cellular function is a core challenge in therapeutic design. Phenomic experiments, designed to capture cellular morphology, utilize microscopy based techniques and demonstrate a high throughput solution for uncovering molecular impact on the cell. In this work, we learn a joint latent space between molecular structures and microscopy phenomic experiments, aligning paired samples with contrastive learning. Specifically, we study the problem ofContrastive PhenoMolecular Retrieval, which consists of zero-shot molecular structure identification conditioned on phenomic experiments. We assess challenges in multi-modal learning of phenomics and molecular modalities such as experimental batch effect, inactive molecule perturbations, and encoding perturbation concentration. We demonstrate improved multi-modal learner retrieval through (1) a uni-modal pre-trained phenomics model, (2) a novel inter sample similarity aware loss, and (3) models conditioned on a representation of molecular concentration. Following this recipe, we propose MolPhenix, a molecular phenomics model. MolPhenix leverages a pre-trained phenomics model to demonstrate significant performance gains across perturbation concentrations, molecular scaffolds, and activity thresholds. In particular, we demonstrate an 8.1x improvement in zero shot molecular retrieval of active molecules over the previous state-of-the-art, reaching 77.33% in top-1% accuracy. These results open the door for machine learning to be applied in virtual phenomics screening, which can significantly benefit drug discovery applications.",100.0,,,"Canada,Canada,Canada,Canada,Canada,Canada",,,,1.0,,2025-05-01 10:42,,,,,,"Industry,Academia,Academia,Academia,Academia,Academia",,,,,,,"Industry,Academia,Academia,Academia,Academia,Academia",,,,433.0498938750792,Hardware,,,,
DiffForce,Biology,Protein design,"University of Cambridge,Shanghai Jiao Tong University,University of New South Wales","Paulina KulytÄ—, Francisco Vargas, Simon Valentin Mathis, Yu Guang Wang, JosÃ© Miguel HernÃ¡ndez-Lobato, Pietro LiÃ²",2024-09-09,Improving Antibody Design with Force-Guided Sampling in Diffusion Models,https://arxiv.org/abs/2406.05832,1.0,,,,,,,,,,,,,,,Unknown,"Antibodies, crucial for immune defense, primarily rely on complementarity-determining regions (CDRs) to bind and neutralize antigens, such as viruses. The design of these CDRs determines the antibody's affinity and specificity towards its target. Generative models, particularly denoising diffusion probabilistic models (DDPMs), have shown potential to advance the structure-based design of CDR regions. However, only a limited dataset of bound antibody-antigen structures is available, and generalization to out-of-distribution interfaces remains a challenge. Physics based force-fields, which approximate atomic interactions, offer a coarse but universal source of information to better mold designs to target interfaces. Integrating this foundational information into diffusion models is, therefore, highly desirable. Here, we propose a novel approach to enhance the sampling process of diffusion models by integrating force field energy-based feedback. Our model, DiffForce, employs forces to guide the diffusion sampling process, effectively blending the two distributions. Through extensive experiments, we demonstrate that our method guides the model to sample CDRs with lower energy, enhancing both the structure and sequence of the generated antibodies.",,,,"United Kingdom of Great Britain and Northern Ireland,China,Australia",,,,,,2025-05-01 10:42,,,,,,"Academia,Academia,Academia",,,,,,,"Academia,Academia,Academia",,,,,,,,,
ProteinMPNN-DDG,Biology,Protein inverse folding,Peptone,"Oliver Dutton, Sandro Bottaro, Michele Invernizzi, Istvan Redl, Albert Chung, Falk Hoffmann, Louie Henderson, Stefano Ruschetta, Fabio Airoldi, Benjamin M J Owens, Patrik Foerch, Carlo Fisicaro, Kamil Tamiola",2024-09-09,Improving Inverse Folding models at Protein Stability Prediction without additional Training or Data,https://www.biorxiv.org/content/10.1101/2024.06.15.599145v4.abstract,0.0,,,,,,,,,,"Total Residues = 5,615,050 = 5.6e6",,,,,Unknown,Deep learning protein sequence models have shown outstanding performance at de novo protein design and variant effect prediction. We substantially improve performance without further training or use of additional experimental data by introducing a second term derived from the models themselves which align outputs for the task of stability prediction. On a task to predict variants which increase protein stability the absolute success probabilities of ProteinMPNN and ESMif are improved by 11% and 5% respectively. We term these models ProteinMPNN-ddG and ESMif-ddG.,,,,United Kingdom of Great Britain and Northern Ireland,ProteinMPNN,,,,,2025-05-01 10:42,,,,,,Industry,,,,,,,Industry,,,,,,,,,
ESMIF-DDG,Biology,Protein inverse folding,Peptone,"Oliver Dutton, Sandro Bottaro, Michele Invernizzi, Istvan Redl, Albert Chung, Falk Hoffmann, Louie Henderson, Stefano Ruschetta, Fabio Airoldi, Benjamin M J Owens, Patrik Foerch, Carlo Fisicaro, Kamil Tamiola",2024-09-09,Improving Inverse Folding models at Protein Stability Prediction without additional Training or Data,https://www.biorxiv.org/content/10.1101/2024.06.15.599145v4.abstract,0.0,,,,,,,,,,"Total Residues = 5,615,050 = 5.6e6",,,,,Unknown,Deep learning protein sequence models have shown outstanding performance at de novo protein design and variant effect prediction. We substantially improve performance without further training or use of additional experimental data by introducing a second term derived from the models themselves which align outputs for the task of stability prediction. On a task to predict variants which increase protein stability the absolute success probabilities of ProteinMPNN and ESMif are improved by 11% and 5% respectively. We term these models ProteinMPNN-ddG and ESMif-ddG.,,,,United Kingdom of Great Britain and Northern Ireland,,,,,,2025-05-01 10:42,,,,,,Industry,,,,,,,Industry,,,,,,,,,
VespaG,Biology,"Protein property prediction,Protein or nucleotide language model (pLM/nLM)","Technical University of Munich,Sorbonne University,Institute for Advanced Study,UniversitÃ© Paris CitÃ©,Institut Universitaire de France (IUF)","CÃ©line Marquet, Julius Schlensok, Marina Abakarova, Burkhard Rost, Elodie Laine",2024-09-09,Expert-guided protein Language Models enable accurate and blazingly fast fitness prediction,https://www.biorxiv.org/content/10.1101/2024.04.24.590982v2.abstract,,,,660000.0,,,,,,39000001.0,"Total datapoints = 39,000,000

39,000,000 = 3.9 x 10^7

Final data estimate: 3.9e7",,,,,Confident,"Exhaustive experimental annotation of the effect of all known protein variants remains daunting and expensive, stressing the need for scalable effect predictions. We introduce VespaG, a blazingly fast missense amino acid variant effect predictor, leveraging protein Language Model (pLM) embeddings as input to a minimal deep learning model. To overcome the sparsity of experimental training data, we created a dataset of 39 million single amino acid variants from the human proteome applying the multiple sequence alignment-based effect predictor GEMME as a pseudo standard-of-truth. This setup increases interpretability compared to the baseline pLM and is easily retrainable with novel or updated pLMs. Assessed against the ProteinGym benchmark (217 multiplex assays of variant effect - MAVE - with 2.5 million variants), VespaG achieved a mean Spearman correlation of 0.48Â±0.02, matching top-performing methods evaluated on the same data. VespaG has the advantage of being orders of magnitude faster, predicting all mutational landscapes of all proteins in proteomes such as Homo sapiens or Drosophila melanogaster in under 30 minutes on a consumer laptop (12-core CPU, 16 GB RAM).",200.0,,,"Germany,France,United States of America,France,France",,,,,,2025-05-01 10:42,,,,,,"Academia,Academia,Academia,Academia,Research collective",,,,,,,"Academia,Academia,Academia,Academia,Research collective",,,,,,,,,
AbGPT,Biology,Protein design,Carnegie Mellon University (CMU),"Desmond Kuan, Amir Barati Farimani",2024-09-09,AbGPT: De Novo Antibody Design via Generative Language Modeling,https://arxiv.org/abs/2409.06090,,,,734000000.0,,4.2506168e+21,"Finetuned ProtGPT2 on 57M sequences with average length of ~120 (Figure 6)

FT compute: 734000000*6840000000*5*6=1.506168e+20

Base model compute: 4.1e+21

Total: 4.2506168e+21",,,6840000000.0,"Finetuned ProtGPT2 on 57M sequences with average length of ~120 (Figure 6)
FT dataset in tokens: 57000000*120=6840000000",,,NVIDIA RTX A6000,,Confident,"The adaptive immune response, largely mediated by B-cell receptors (BCRs), plays a crucial role for effective pathogen neutralization due to its diversity and antigen specificity. Designing BCRs de novo, or from scratch, has been challenging because of their complex structure and diverse binding requirements. Protein language models (PLMs) have shown remarkable performance in contextualizing and performing various downstream tasks without relying on structural information. However, these models often lack a comprehensive understanding of the entire protein space, which limits their application in antibody design. In this study, we introduce Antibody Generative Pretrained Transformer (AbGPT), a model fine-tuned from a foundational PLM to enable a more informed design of BCR sequences. Using a custom generation and filtering pipeline, AbGPT successfully generated a high-quality library of 15,000 BCR sequences, demonstrating a strong understanding of the intrinsic variability and conserved regions within the antibody repertoire. https://github.com/deskk/AbGPT",5.0,,Unreleased,United States of America,ProtGPT2,,,,,2025-06-12 14:31,,,,,,Academia,,,,,Unreleased,"MIT license:
https://github.com/deskk/AbGPT",Academia,,,,,,,,,
BetterBodies,Biology,Protein design,"University of Freiburg,Collaborative Research Institute Intelligent Oncology ,BrainLinks-BrainTools","Yannick Vogt, Mehdi Naouar, Maria Kalweit, Christoph Cornelius Miething, Justus Duyster, Joschka Boedecker, Gabriel Kalweit",2024-09-09,BETTERBODIES: Reinforcement Learning Guided Diffusion for Antibody Sequence Design,https://arxiv.org/abs/2409.16298,,,,,,,,,,30283.0,"New estimate: 2753*11=30283

Total sequences: 2,500 + 2,753 + 2,167 = 7,420
Data points = 7,420 sequences Ã— 11 residues/sequence = 81,620 data points

Final estimate: 81,620",,,,,Confident,"Antibodies offer great potential for the treatment of various diseases. However, the discovery of therapeutic antibodies through traditional wet lab methods is expensive and time-consuming. The use of generative models in designing antibodies therefore holds great promise, as it can reduce the time and resources required. Recently, the class of diffusion models has gained considerable traction for their ability to synthesize diverse and high-quality samples. In their basic form, however, they lack mechanisms to optimize for specific properties, such as binding affinity to an antigen. In contrast, the class of offline Reinforcement Learning (RL) methods has demonstrated strong performance in navigating large search spaces, including scenarios where frequent real-world interaction, such as interaction with a wet lab, is impractical. Our novel method, BetterBodies, which combines Variational Autoencoders (VAEs) with RL guided latent diffusion, is able to generate novel sets of antibody CDRH3 sequences from different data distributions. Using the Absolut! simulator, we demonstrate the improved affinity of our novel sequences to the SARS-CoV spike receptor-binding domain. Furthermore, we reflect biophysical properties in the VAE latent space using a contrastive loss and add a novel Q-function based filtering to enhance the affinity of generated sequences. In conclusion, methods such as ours have the potential to have great implications for real-world biological sequence design, where the generation of novel high-affinity binders is a cost-intensive endeavor.",700.0,,,"Germany,Germany,Germany",,,,,,2025-05-01 10:42,,,,,,"Academia,Research collective,Academia",,,,,,,"Academia,Research collective,Academia",,,,,,,,,
RiboCode,Biology,RNA design,"Sun Yat-sen University,Rhegen Biotechnology,Chinese Academy of Sciences","Yupeng Li, Fan Wang, Jiaqi Yang, Zirong Han, Linfeng Chen, Wenbing Jiang, Hao Zhou, Tong Li, Zehua Tang, Jianxiang Deng, Xin He, Gaofeng Zha, Jiekai Hu, Yong Hu, Linping Wu, Changyou Zhan, Caijun Sun, Yao He, Zhi Xie",2024-09-08,Deep Generative Optimization of mRNA Codon Sequences for Enhanced Protein Production and Therapeutic Efficacy,https://www.biorxiv.org/content/10.1101/2024.09.06.611590v1,,,,,,,,,,3200001.0,"320 datasets Ã— 10,000 mRNAs/dataset = 3,200,000 data points
(3.2 Ã— 10^6 data points)",,,,,Confident,"Messenger RNA (mRNA) therapeutics show immense promise, but their efficacy is limited by suboptimal protein expression. Here, we present RiboCode, a deep learning framework that generates mRNA codon sequences for enhanced protein production. RiboCode introduces several advances, including direct learning from large-scale ribosome profiling data, context-aware mRNA optimization and generative exploration of a large sequence space. In silico analysis demonstrate RiboCodeâ€™s robust predictive accuracy for unseen genes and cellular environments. In vitro experiments show substantial improvements in protein expression, with up to a 72-fold increase, significantly outperforming past methods. In addition, RiboCode achieves cell-type specific expression and demonstrates robust performance across different mRNA formats, including m1Î¨-modified and circular mRNAs, an important feature for mRNA therapeutics. In vivo mouse studies show that optimized influenza hemagglutinin mRNAs induce ten times stronger neutralizing antibody responses against influenza virus compared to the unoptimized sequence. In an optic nerve crush model, optimized nerve growth factor mRNAs achieve equivalent neuroprotection of retinal ganglion cells at one-fifth the dose of the unoptimized sequence. Collectively, RiboCode represents a paradigm shift from rule-based to data-driven, context-sensitive approach for mRNA therapeutic applications, enabling the development of more potent and dose-efficient treatments.",,,,"China,China,China",,,,,,2025-05-01 10:42,,,,,,"Academia,Industry,Academia",,,,,,,"Academia,Industry,Academia",,,,,,,,,
DDGemb,Biology,"Mutation prediction,Protein stability prediction",University of Bologna,"Castrense Savojardo, Matteo Manfredi, Pier Luigi Martelli, Rita Casadio",2024-09-07,DDGemb: predicting protein stability change upon single- and multi-point variations with embeddings and deep learning,https://www.biorxiv.org/content/10.1101/2024.09.05.611455v1.abstract,,,,,,,,,,2450.0,"
",,,AMD EPYC 7413,,Confident,"The knowledge of protein stability upon residue variation is an important step for functional protein design and for understanding how protein variants can promote disease onset. Computational methods are important to complement experimental approaches and allow a fast screening of large datasets of variations. In this work we present DDGemb, a novel method combining protein language model embeddings and transformer architectures to predict protein ðš«ðš«G upon both single- and multipoint variations. DDGemb has been trained on a high-quality dataset derived from literature and tested on available benchmark datasets of single- and multi-point variations. DDGemb performs at the state of the art in both single- and multi-point variations.",500.0,,,Italy,,,,2.0,,2025-05-01 10:42,,,,,,Academia,,,,,,,Academia,,,,709.3831171323475,,,,,
MPDF,Biology,Drug discovery,"Chinese University of Hong Kong (CUHK),Lanzhou University,Zhejiang Lab,Zhejiang University (ZJU)","Chunbin Gu, Mutian He, Hanqun Cao, Guangyong Chen, Chang-yu Hsieh, Pheng Ann Heng",2024-09-07,Unlocking Potential Binders: Multimodal Pretraining DEL-Fusion for Denoising DNA-Encoded Libraries,https://arxiv.org/abs/2409.05916,2.0,,,,,3.074112e+17,"1. Hardware: 1x NVIDIA GeForce RTX 3090

2. Training duration: 6 hours (max of 1-6 hour range) = 21,600 seconds

3. Utilization: 40%

4. Calculation: 


35580000000000*6*60*60*0.4=3.074112e+17",,,,,6.0,"""with training times ranging from 1 to 6 hours, depending on the complexity and size of the different building blocks.""",NVIDIA GeForce RTX 3090,,Confident,"In the realm of drug discovery, DNA-encoded library (DEL) screening technology has emerged as an efficient method for identifying high-affinity compounds. However, DEL screening faces a significant challenge: noise arising from nonspecific interactions within complex biological systems. Neural networks trained on DEL libraries have been employed to extract compound features, aiming to denoise the data and uncover potential binders to the desired therapeutic target. Nevertheless, the inherent structure of DEL, constrained by the limited diversity of building blocks, impacts the performance of compound encoders. Moreover, existing methods only capture compound features at a single level, further limiting the effectiveness of the denoising strategy. To mitigate these issues, we propose a Multimodal Pretraining DEL-Fusion model (MPDF) that enhances encoder capabilities through pretraining and integrates compound features across various scales. We develop pretraining tasks applying contrastive objectives between different compound representations and their text descriptions, enhancing the compound encoders' ability to acquire generic features. Furthermore, we propose a novel DEL-fusion framework that amalgamates compound information at the atomic, submolecular, and molecular levels, as captured by various compound encoders. The synergy of these innovations equips MPDF with enriched, multi-scale features, enabling comprehensive downstream denoising. Evaluated on three DEL datasets, MPDF demonstrates superior performance in data processing and analysis for validation tasks. Notably, MPDF offers novel insights into identifying high-affinity molecules, paving the way for improved DEL utility in drug discovery.",,,Unreleased,"Hong Kong,China,China,China,China",,,,1.0,,2025-06-13 15:10,,,,,,"Academia,Academia,Academia",,,,,Unreleased,,"Academia,Academia,Academia",,,,378.9439728271087,Hardware,,,,
DeepSeek-V2.5,Language,"Language modeling/generation,Chat,Code generation",DeepSeek,"DeepSeek-AI, Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Hanwei Xu, Hao Yang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J.L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jin Chen, Jingyang Yuan, Junjie Qiu, Junxiao Song, Kai Dong, Kaige Gao, Kang Guan, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qihao Zhu, Qinyu Chen, Qiushi Du, R.J. Chen, R.L. Jin, Ruiqi Ge, Ruizhe Pan, Runxin Xu, Ruyi Chen, S.S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Size Zheng, T. Wang, Tian Pei, Tian Yuan, Tianyu Sun, W.L. Xiao, Wangding Zeng, Wei An, Wen Liu, Wenfeng Liang, Wenjun Gao, Wentao Zhang, X.Q. Li, Xiangyue Jin, Xianzu Wang, Xiao Bi, Xiaodong Liu, Xiaohan Wang, Xiaojin Shen, Xiaokang Chen, Xiaosha Chen, Xiaotao Nie, Xiaowen Sun",2024-09-06,DeepSeek-V2.5,https://huggingface.co/deepseek-ai/DeepSeek-V2.5,,Training cost,,236000000000.0,"21B active params, 236B total",1.7892000000000004e+24,"V2.5 is a merge of V2-coder and V2-chat
V2-coder is trained for 6T additional tokens from an intermediate checkpoint of V2, which had been trained for 4.2T tokens. Total: 10.2T
V2-chat is fine-tuned from V2, saw 8.2T tokens in pre-training
Unique steps: 8.2T + 6T = 14.2T
FLOPs: 6 * 21B * 14.2T = 1.7892e24","GitHub,Common Crawl",,,"The original V2 had a dataset of 8.1T unique tokens, and coder-V2 added an additional 1.391T unique tokens of code and math. But it appears no additional training was done to combine them into this model.",,,,Self-supervised learning,Confident,,,,Open weights (restricted use),China,,,,,,2025-05-16 10:30,,"There is no paper to reference, no information about hardware used for training found in media.",,36864000.0,"Maximum batch size comes from training of V2-coder, which used long context training with 288 batches of 128k tokens = 36,864,000",Industry,,,,,Unreleased,,Industry,,,,,Operation counting,deepseek-ai,,,
ProtSSN,Biology,Protein design,"Shanghai Jiao Tong University,East China University of Science and Technology,Shanghai AI Lab","Yang Tan, Bingxin Zhou, Lirong Zheng, Guisheng Fan, Liang Hong",2024-09-06,Semantical and Geometrical Protein Encoding Toward Enhanced Bioactivity and Thermostability,https://www.biorxiv.org/content/10.1101/2023.12.01.569522v3.abstract,7.0,,,1467000000.0,,,,CATH,,9284400.0,"30,948 proteins * 300 amino acids/protein = 9,284,400 tokens â‰ˆ 1.2e7 tokens

30,948 * 300 = 9,284,400",,,NVIDIA A100,,Likely,"Protein engineering is a pivotal aspect of synthetic biology, involving the modification of amino acids within existing protein sequences to achieve novel or enhanced functionalities and physical properties. Accurate prediction of protein variant effects requires a thorough understanding of protein sequence, structure, and function. Deep learning methods have demonstrated remarkable performance in guiding protein modification for improved functionality. However, existing approaches predominantly rely on protein sequences, which face challenges in efficiently encoding the geometric aspects of amino acidsâ€™ local environment and often fall short in capturing crucial details related to protein folding stability, internal molecular interactions, and bio-functions. Furthermore, there lacks a fundamental evaluation for developed methods in predicting protein thermostability, although it is a key physical property that is frequently investigated in practice. To address these challenges, this paper introduces a novel pre-training framework that integrates sequential and geometric encoders for protein primary and tertiary structures. This framework guides mutation directions toward desired traits by simulating natural selection on wild-type proteins and evaluates variant effects based on their fitness to perform specific functions. We assess the proposed approach using three benchmarks comprising over 300 deep mutational scanning assays. The prediction results showcase exceptional performance across extensive experiments when compared to other zero-shot learning methods, all while maintaining a minimal cost in terms of trainable parameters. This study not only proposes an effective framework for more accurate and comprehensive predictions to facilitate efficient protein engineering, but also enhances the in silico assessment system for future deep learning models to better align with empirical requirements. The PyTorch implementation are available at https://github.com/tyang816/ProtSSN.",,,,"China,China,China",,,,1.0,,2025-05-01 10:42,,,,,,"Academia,Academia,Academia",,,,,,,"Academia,Academia,Academia",,,,433.088470588682,,,,,
AlphaProteo,Biology,"Protein generation,Proteins",Google DeepMind,"Vinicius Zambaldi, David La, Alexander E. Chu, Harshnira Patani, Amy E. Danson, Tristan O. C. Kwan, Thomas Frerix, Rosalia G. Schneider, David Saxton, Ashok Thillaisundaram, Zachary Wu, Isabel Moraes, Oskar Lange, Eliseo Papa, Gabriella Stanton, Victor Martin, Sukhdeep Singh, Lai H. Wong, Russ Bates, Simon A. Kohl, Josh Abramson, Andrew W. Senior, Yilmaz Alguel, Mary Y. Wu,
Irene M. Aspalter, Katie Bentley, David L.V. Bauer, Peter Cherepanov, Demis Hassabis, Pushmeet Kohli, Rob Fergus, Jue Wang",2024-09-05,De novo design of high-affinity protein binders with AlphaProteo,https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/alphaproteo-generates-novel-proteins-for-biology-and-health-research/AlphaProteo2024.pdf,,Historical significance,Economic impacts from development of commercially and socially valuable protein designs and materials,,,,,PDB (Protein Data Bank),Trained on large amounts of protein data from the Protein Data Bank (PDB) and more than 100 million predicted structures from AlphaFold,,,,,,,Unknown,"Computational design of protein-binding proteins is a fundamental capability with broad utility in biomedical research and biotechnology. Recent methods have made strides against some target proteins, but on-demand creation of high-affinity binders without multiple rounds of experimental testing remains
an unsolved challenge. This technical report introduces AlphaProteo, a family of machine learning models for protein design, and details its performance on the de novo binder design problem. With AlphaProteo, we achieve 3- to 300-fold better binding affinities and higher experimental success rates than the best existing methods on seven target proteins. Our results suggest that AlphaProteo can generate binders ""ready-to-use"" for many research applications using only one round of medium-throughput screening
and no further optimization.",,,Unreleased,"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France",,,,,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
ÂµFormer,Biology,Protein design,Microsoft Research AI for Science,"Haoran Sun, Liang He, Pan Deng, Guoqing Liu, Haiguang Liu, Chuan Cao, Fusong Ju, Lijun Wu, Tao Qin, Tie-Yan Liu",2024-09-05,Accelerating protein engineering with fitness landscape modeling and reinforcement learning,https://www.biorxiv.org/content/10.1101/2023.11.16.565910v3.abstract,0.0,,,670000000.0,,,,,,9000000001.0,"30,000,000 proteins Ã— 300 amino acids/protein = 9,000,000,000 datapoints

Total datapoints: 9.0e9",,,,,Confident,"Protein engineering plays a pivotal role in designing novel proteins with desired functions, yet the rugged fitness landscape of proteins within their mutant space presents a major challenge, limiting the effective discovery of optimal sequences. To address this, we introduce ÂµFormer, a deep learning framework that combines a pre-trained protein language model with custom-designed scoring modules to predict the mutational effects of proteins. ÂµFormer achieves state-of-the-art performance in predicting high-order mutants, modeling epistatic interactions, and handling insertion. By integrating ÂµFormer with a reinforcement learning framework, we enable efficient exploration of vast mutant spaces, encompassing trillions of mutation candidates, to design protein variants with enhanced activity. Remarkably, we successfully predicted mutants that exhibited a 2000-fold increase in bacterial growth rate due to enhanced enzymatic activity. These results highlight the effectiveness of our approach in identifying impactful mutations across diverse protein targets and fitness metrics, offering a powerful tool for optimizing proteins with significantly higher success rates.",,,,"United States of America,United Kingdom of Great Britain and Northern Ireland,China,Netherlands,Germany",,,,,,2025-05-01 10:42,,,,,,Industry,,,,,,,Industry,,,,,,,,,
Hunyuan Turbo,Language,Language modeling/generation,Tencent,,2024-09-05,"è…¾è®¯ç‰ˆâ€œGPT-4oâ€æ¥äº†ï¼æ··å…ƒTurboé¦–å‘å¹¶ä¸Šçº¿ï¼Œæ•ˆçŽ‡ç¿»å€ä»·æ ¼ç åŠ
",https://zhidx.com/p/442250.html,,SOTA improvement,"Best score on SuperCLUEæ€»æŽ’è¡Œæ¦œï¼ˆ2024å¹´8æœˆï¼‰- SuperCLUE general benchmak from Aug 2024 (https://www.superclueai.com/) in terms of ""science"" and ""liberal arts"" evaluation.",,,,,Unspecified unreleased,,,,,,,,Unknown,,,,API access,China,,,,,,2025-06-02 12:56,,,,,,Industry,,,,,Unreleased,"Available via Tencent Cloud

https://www.tencentcloud.com/document/product/647/68338",Industry,,,,,,,,,
OLMoE,Language,"Language modeling/generation,Chat","Allen Institute for AI,Contextual AI,University of Washington,Princeton University","Niklas Muennighoff, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Jacob Morrison, Sewon Min, Weijia Shi, Pete Walsh, Oyvind Tafjord, Nathan Lambert, Yuling Gu, Shane Arora, Akshita Bhagia, Dustin Schwenk, David Wadden, Alexander Wettig, Binyuan Hui, Tim Dettmers, Douwe Kiela, Ali Farhadi, Noah A. Smith, Pang Wei Koh, Amanpreet Singh, Hannaneh Hajishirzi",2024-09-03,OLMoE: Open Mixture-of-Experts Language Models,https://arxiv.org/abs/2409.02060v1,,,,7000000000.0,"""6.9B total parameters, of which only 1.3B are activated for each input token""",5.1741608015e+22,"""we pretrain OLMOE-1B-7B for 5.1 trillion tokens with 6.9B total parameters, of which only 1.3B are activated for each input token""
So 6 * 5.1T * 1.3B = 3.978e22

In appendix B: ""We pretrain OLMOE-1B-7B on 256 H100 GPUs for approximately 10 days ... For adaptation, we use 32 H100 GPUs for 33 hours to instruction tune and for another 14 hours to preference tune via DPO.""
Which would suggest 9.90e14 * (256*240 + 32*(33+14)) * 3600 * 0.3 = 6.730e22 including instruction/preference tuning

geometric mean: sqrt(3.978e22 * 6.730e22) = 5.174e22",,Table 2 and 3,4060000000000.0,"Table 2,  4.06T tokens. From Table 3, SFT data appears to be negligible in comparison (average length of response would need to be 60k tokens in order to bump total dataset size by 1%)",287.0,"""We pretrain OLMOE-1B-7B on 256 H100 GPUs for approximately 10 days ... For adaptation, we use 32 H100 GPUs for 33 hours to instruction tune and for another 14 hours to preference tune via DPO.""
10*24 + 33 + 14 = 287",NVIDIA H100 SXM5 80GB,,Confident,"We introduce OLMoE, a fully open, state-of-the-art language model leveraging sparse Mixture-of-Experts (MoE). OLMoE-1B-7B has 7 billion (B) parameters but uses only 1B per input token. We pretrain it on 5 trillion tokens and further adapt it to create OLMoE-1B-7B-Instruct. Our models outperform all available models with similar active parameters, even surpassing larger ones like Llama2-13B-Chat and DeepSeekMoE-16B. We present various experiments on MoE training, analyze routing in our model showing high specialization, and open-source all aspects of our work: model weights, training data, code, and logs.",1.26,,Open weights (unrestricted),"United States of America,United States of America,United States of America,United States of America",,,,256.0,,2025-05-01 10:42,,,,4194304.0,Table 10.,"Research collective,Industry,Academia,Academia",,,,,Open source,Dataset is open source but requires attribution,"Research collective,Industry,Academia,Academia",,,BF16,353146.6076498626,"Operation counting,Hardware",,,,
DrugCLIP,Biology,"Molecular screening,Drug discovery","Tsinghua University,Tsinghua-Peiking Center for Life Sciences,Peking University,Beijing Academy of Artificial Intelligence / BAAI","Yinjun Jia, Bowen Gao, Jiaxin Tan, Xin Hong, Wenyu Zhu, Haichuan Tan, Yuan Xiao, Yanwen Huang, Yue Jin, Yafei Yuan, Jiekang Tian, Weiying Ma, Yaqin Zhang, Chuangye Yan, Wei Zhang, Yanyan Lan",2024-09-03,Deep contrastive learning enables genome-wide virtual screening,https://www.biorxiv.org/content/10.1101/2024.09.02.610777v1?rss=1,0.0,,,,,,"
",,,1662000000.0,"Data for DrugCLIP:
Pretraining: 5.5 Ã— 10^6 (pseudo-pocket and ligand pairs)
Fine-tuning: 4.0 Ã— 10^4 (experimental complexes)

Total 5,540,000 datapoints

Assuming 300 tokens per protein-ligand pair: 5540000*300=1662000000",,,,,Confident,"Numerous protein-coding genes are associated with human diseases, yet approximately 90% of them lack targeted therapeutic intervention. While conventional computational methods such as molecular docking have facilitated the discovery of potential hit compounds, the development of genome-wide virtual screening against the expansive chemical space remains a formidable challenge. Here we introduce DrugCLIP, a novel framework that combines contrastive learning and dense retrieval to achieve rapid and accurate virtual screening. Compared to traditional docking methods, DrugCLIP improves the speed of virtual screening by several orders of magnitude. In terms of performance, DrugCLIP not only surpasses docking and other deep learning-based methods across two standard benchmark datasets but also demonstrates high efficacy in wet-lab experiments. Specifically, DrugCLIP successfully identified agonists with < 100 nM affinities for 5HT2AR, a key target in psychiatric diseases. For another target NET, whose structure is newly solved and not included in the training set, our method achieved a hit rate of 15%, with 12 diverse molecules exhibiting affinities better than Bupropion. Additionally, two chemically novel inhibitors were validated by structure determination with Cryo-EM. Building on this foundation, we present the results of a pioneering trillion-scale genome-wide virtual screening, encompassing approximately 10,000 AlphaFold2 predicted proteins within the human genome and 500 million molecules from the ZINC and Enamine REAL database. This work provides an innovative perspective on drug discovery in the post-AlphaFold era, where comprehensive targeting of all disease-related proteins is within reach.",,,,"China,China,China,China",Uni-Mol Molecular Model,,,,,2025-05-01 10:42,,,,,,"Academia,Research collective,Academia,Academia",,,,,,,"Academia,Research collective,Academia,Academia",,,,,Hardware,,,,
Alphaflow,Biology,Protein folding prediction,Massachusetts Institute of Technology (MIT),"Bowen Jing, Bonnie Berger, Tommi Jaakkola",2024-09-02,AlphaFold Meets Flow Matching for Generating Protein Ensembles,https://arxiv.org/abs/2402.04845,43.0,,,,,1.64975616e+21,"Training stages, Table 2:
267+105+11+28+9+39=459 hours

459*60*60*312000000000000*8*0.4=1649756160000000000000",,,338688000.0,"AlphaFLOW:
1.28M Ã— 256 = 327,680,000
43K Ã— 256 = 11,008,000
Total: 327,680,000 + 11,008,000 = 338,688,000

",,,NVIDIA A100,,Confident,,,,,United States of America,,,,8.0,,2025-05-01 10:42,,,,,,Academia,,,,,,,Academia,,,,6306.329858756263,Hardware,,,,
ESMFlow,Biology,Protein folding prediction,Massachusetts Institute of Technology (MIT),"Bowen Jing, Bonnie Berger, Tommi Jaakkola",2024-09-02,AlphaFold Meets Flow Matching for Generating Protein Ensembles,https://arxiv.org/abs/2402.04845,43.0,,,,,7.6197888e+20,"1. Hardware setup: 8x NVIDIA A100 GPUs, 3.12e+14 FLOP/s per GPU
2. Training duration: 371 hours (267h AlphaFLOW + 104h ESMFLOW) = 1,335,600 seconds
3. Utilization rate: 40%
4. Final calculation: 8 GPUs Ã— 3.12e+14 FLOP/s Ã— 1,335,600s Ã— 0.4 = 1.34e+21 FLOPs

ESMFlow training time, table 2
104+37+5+34+9+23=212

212hours*8*3.12e14*0.4=761978880000000000000",,,191232000.0,"
ESMFLOW:
720K Ã— 256 = 184,320,000
27K Ã— 256 = 6,912,000
Total: 184,320,000 + 6,912,000 = 191,232,000
",,,NVIDIA A100,,Confident,"The biological functions of proteins often depend on dynamic structural ensembles. In this work, we develop a flow-based generative modeling approach for learning and sampling the conformational landscapes of proteins. We repurpose highly accurate single-state predictors such as AlphaFold and ESMFold and fine-tune them under a custom flow matching framework to obtain sequence-conditoned generative models of protein structure called AlphaFlow and ESMFlow. When trained and evaluated on the PDB, our method provides a superior combination of precision and diversity compared to AlphaFold with MSA subsampling. When further trained on ensembles from all-atom MD, our method accurately captures conformational flexibility, positional distributions, and higher-order ensemble observables for unseen proteins. Moreover, our method can diversify a static PDB structure with faster wall-clock convergence to certain equilibrium properties than replicate MD trajectories, demonstrating its potential as a proxy for expensive physics-based simulations. Code is available at this https URL.",,,,United States of America,,,,8.0,,2025-05-01 10:42,,,,,,Academia,,,,,,,Academia,,,,6306.329858756263,Reported,,,,
MiniMax Video-01,"Video,Vision","Video generation,Text-to-video,Image-to-video",MiniMax,,2024-08-31,Video model (Video Generation): Supports the generation of high-definition videos at 720p resolution and 25fps,https://www.minimaxi.com/en/news/video-01,,,,,"""Our models seamlessly combine text, speech, and vision, each with support for hundreds of billions of parameters.""

https://intl.minimaxi.com/document/platform%20introduction?key=66701c8e1d57f38758d58198",,,Unspecified unreleased,,,,,,,,Unknown,"MiniMax has officially launched its first AI-native video generation model, video-01.

This model supports the generation of high-definition videos at 720p resolution and 25fps, featuring cinematic camera movement effects. It can quickly create visually striking content based on text descriptions.

Video-01 boasts high compression rates, excellent text responsiveness, and diverse styles, while supporting native high resolution and high frame rate videos, comparable to cinematic quality.
",,,API access,China,,,,,,2025-05-19 12:43,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
HelixFold3,Biology,"Protein folding prediction,Protein interaction prediction,Protein nucleotide interaction prediction,RNA-Protein interaction prediction,Protein design,Drug discovery","Tecorigin LTD,Tsinghua University,Baidu","Lihang Liu, Shanzhuo Zhang, Yang Xue, Xianbin Ye, Kunrui Zhu, Yuxin Li, Yang Liu, Wenlai Zhao, Hongkun Yu, Zhihua Wu, Xiaonan Zhang, Xiaomin Fang",2024-08-30,Technical Report of HelixFold3 for Biomolecular Structure Prediction,https://arxiv.org/abs/2408.16975,,,,,,,,PDB (Protein Data Bank),,,,,,,,Unknown,"The AlphaFold series has transformed protein structure prediction with remarkable accuracy, often matching experimental methods. AlphaFold2, AlphaFold-Multimer, and the latest AlphaFold3 represent significant strides in predicting single protein chains, protein complexes, and biomolecular structures. While AlphaFold2 and AlphaFold-Multimer are open-sourced, facilitating rapid and reliable predictions, AlphaFold3 remains partially accessible through a limited online server and has not been open-sourced, restricting further development. To address these challenges, the PaddleHelix team is developing HelixFold3, aiming to replicate AlphaFold3's capabilities. Using insights from previous models and extensive datasets, HelixFold3 achieves an accuracy comparable to AlphaFold3 in predicting the structures of conventional ligands, nucleic acids, and proteins. The initial release of HelixFold3 is available as open source on GitHub for academic research, promising to advance biomolecular research and accelerate discoveries. We also provide online service at PaddleHelix website at this https URL.",,,API access,"China,China,China",,,,,,2025-06-11 17:58,,,,,,"Industry,Academia,Industry",,,,,Open (non-commercial),"https://paddlehelix.baidu.com/
https://paddlehelix.baidu.com/app/tut/guide/all/helixfold3sdk

CC BY-NC-SA 4.0
https://github.com/PaddlePaddle/PaddleHelix","Industry,Academia,Industry",,,,,,,,,
NV-Embed-v2,Language,"Language modeling/generation,Question answering",NVIDIA,,2024-08-30,,https://huggingface.co/nvidia/NV-Embed-v2,,,,7000000000.0,,,,,,,,,,,,Likely,"We present NV-Embed-v2, a generalist embedding model that ranks No. 1 on the Massive Text Embedding Benchmark (MTEB benchmark)(as of Aug 30, 2024) with a score of 72.31 across 56 text embedding tasks. It also holds the No. 1 in the retrieval sub-category (a score of 62.65 across 15 tasks) in the leaderboard, which is essential to the development of RAG technology.

NV-Embed-v2 presents several new designs, including having the LLM attend to latent vectors for better pooled embedding output, and demonstrating a two-staged instruction tuning method to enhance the accuracy of both retrieval and non-retrieval tasks. Additionally, NV-Embed-v2 incorporates a novel hard-negative mining methods that take into account the positive relevance score for better false negatives removal.",,,Open weights (non-commercial),United States of America,Mistral 7B,104595460000000000000,I assume it is very similar to NV-Embed-v1 and don't see any significant differences. I expect the compute is likely to be the same.,,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,"Creative Commons Attribution Non Commercial 4.0

https://huggingface.co/nvidia/NV-Embed-v1",Industry,,,,,,,,,
OPUS-Design,Biology,Protein design,"Fudan University,Shanghai AI Lab,Harcam Biomedicines","Gang Xu, Yulu Yang, Yiqiu Zhang, Qinghua Wang, Jianpeng Ma",2024-08-29,OPUS-Design: Designing Protein Sequence from Backbone Structure with 3DCNN and Protein Language Model,https://www.biorxiv.org/content/10.1101/2024.08.20.608889v2.abstract,0.0,,,,,,,,"In OPUS-Design, we employ the same training dataset as utilized by trRosetta
(28), consisting of 15,051 protein targets released before May 2018. ",,,,,NVIDIA V100,,Unknown,"Protein sequence design, also known as protein inverse folding, is a crucial task in protein engineering and design. Despite the recent advancements in this field, which have facilitated the identification of amino acid sequences based on backbone structures, achieving higher levels of accuracy in sequence recovery rates remains challenging. It this study, we introduce a two-stage protein sequence design method named OPUS-Design. Our evaluation on recently released targets from CAMEO and CASP15 shows that OPUS-Design significantly surpasses several other leading methods on both monomer and oligomer targets in terms of sequence recovery rate. Furthermore, by utilizing its finetune version OPUS-Design-ft and our previous work OPUS-Mut, we have successfully designed a thermal-tolerant double-point mutant of T4 lysozyme that demonstrates a residual enzyme activity exceeding that of the wild-type T4 by more than twofold when both are subjected to extreme heat treatment at 70Â°C. Importantly, this accomplishment is achieved through the experimental verification of less than 10 mutant candidates, thus significantly alleviating the burden of experimental verification process.",,,Unreleased,"China,China,China",,,,4.0,,2025-06-16 13:12,,,,,,"Academia,Academia",,,,,Unreleased,"The code and pre-trained models of OPUS-Design as well as the test sets used in
the study can be downloaded from http://github.com/OPUS-MaLab/opus_design.
They are freely available for academic usage.

[unaccessible now]","Academia,Academia",,,,2365.084363417789,,,,,
GLM-4-Plus,Language,Language modeling,Zhipu AI,Zhipu AI,2024-08-29,GLM-4-Plus,https://bigmodel.cn/dev/howuse/glm-4,,Training cost,,,,3.600000000000001e+25,Estimated using benchmark imputation,,,,,,,,,Unknown,"At the KDD International Conference on Data Mining and Knowledge Discovery, the Zhipu GLM team unveiled the new generation of base large modelâ€”GLM-4-Plus. As the latest version of Zhipuâ€™s fully self-developed GLM large model, GLM-4-Plus signifies Zhipu AIâ€™s continuous dedication in the field of general artificial intelligence, advancing the independent and autonomous innovation of large model technology.",,,API access,China,,,,,,2025-05-26 18:38,,Check references for hardware details.,,,,Industry,,,,,Unreleased,,Industry,,,,,Benchmarks,,,,4
Hairuo,Language,Language modeling/generation,Inspur,,2024-08-29,,"https://encloud.inspur.com/hairuo/cp/index.html

https://www.eguizhou.gov.cn/guiyang/2024-08/29/c_1016827.htm",,SOTA improvement,Highest score at SuperGLUE Benchmark https://super.gluebenchmark.com/leaderboard/,,,,,,,,,,,,,Unknown,,,,Hosted access (no API),China,,,,,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
GLA Transformer 1.3B,Language,"Language modeling/generation,Question answering","MIT-IBM Watson AI Lab,Massachusetts Institute of Technology (MIT)","Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, Yoon Kim",2024-08-27,Gated Linear Attention Transformers with Hardware-Efficient Training,https://arxiv.org/abs/2312.06635,,,,1300000000.0,1.3b,7.8e+20,6ND = 6*1300000000.00*100*10^9 = 7.8e+20,SlimPajama,"We use the SlimPajama dataset (Soboleva et al., 2023) and tokenize it
using the Mistral tokenizer (Jiang et al., 2023). The original dataset contains 627B tokens; we use a 100B subset.",100000000000.0,100b,,,,,Confident,"Transformers with linear attention allow for efficient parallel training but can simultaneously be formulated as an RNN with 2D (matrix-valued) hidden states, thus enjoying linear-time inference complexity. However, linear attention generally underperforms ordinary softmax attention. Moreover, current implementations of linear attention lack I/O-awareness and are thus slower than highly optimized implementations of softmax attention. This work describes a hardware-efficient algorithm for linear attention that trades off memory movement against parallelizability. The resulting implementation, dubbed FLASHLINEARATTENTION, is faster than FLASHATTENTION-2 (Dao, 2023) as a standalone layer even on short sequence lengths (e.g., 1K). We then generalize this algorithm to a more expressive variant of linear attention with data-dependent gates. When used as a replacement for the standard attention layer in Transformers, the resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer (Touvron et al., 2023) as well recent linear-time-inference baselines such as RetNet (Sun et al., 2023a) and Mamba (Gu & Dao, 2023) on moderate-scale language modeling experiments. GLA Transformer is especially effective at length generalization, enabling a model trained on 2K to generalize to sequences longer than 20K without significant perplexity degradations. For training speed, the GLA Transformer has higher throughput than a similarly-sized Mamba model.
",,,Open weights (unrestricted),"United States of America,United States of America",,,,,,2025-05-01 10:42,,,,2000000.0,,"Academia,Industry,Academia",,,,,Open source,"https://github.com/sustcsonglin/flash-linear-attention/blob/main/fla/layers/gla.py

MIT license for code (seems like training code)

https://huggingface.co/fla-hub/gla-1.3B-100B
MIT license","Academia,Industry,Academia",,,,,Operation counting,fla-hub,,,
GLA Transformer 340M,Language,"Language modeling/generation,Question answering","MIT-IBM Watson AI Lab,Massachusetts Institute of Technology (MIT)","Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, Yoon Kim",2024-08-27,Gated Linear Attention Transformers with Hardware-Efficient Training,https://arxiv.org/abs/2312.06635,,,,340000000.0,"340m
",3.06e+19,6ND = 6*340*10^6*15*10^9 = 3.06e+19,SlimPajama,"We use the SlimPajama dataset (Soboleva et al., 2023) and tokenize it
using the Mistral tokenizer (Jiang et al., 2023). The original dataset contains 627B tokens; we use a 100B subset.",15000000000.0,15B Tokens,,,,,Confident,"Transformers with linear attention allow for efficient parallel training but can simultaneously be formulated as an RNN with 2D (matrix-valued) hidden states, thus enjoying linear-time inference complexity. However, linear attention generally underperforms ordinary softmax attention. Moreover, current implementations of linear attention lack I/O-awareness and are thus slower than highly optimized implementations of softmax attention. This work describes a hardware-efficient algorithm for linear attention that trades off memory movement against parallelizability. The resulting implementation, dubbed FLASHLINEARATTENTION, is faster than FLASHATTENTION-2 (Dao, 2023) as a standalone layer even on short sequence lengths (e.g., 1K). We then generalize this algorithm to a more expressive variant of linear attention with data-dependent gates. When used as a replacement for the standard attention layer in Transformers, the resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer (Touvron et al., 2023) as well recent linear-time-inference baselines such as RetNet (Sun et al., 2023a) and Mamba (Gu & Dao, 2023) on moderate-scale language modeling experiments. GLA Transformer is especially effective at length generalization, enabling a model trained on 2K to generalize to sequences longer than 20K without significant perplexity degradations. For training speed, the GLA Transformer has higher throughput than a similarly-sized Mamba model.
",,,Open weights (unrestricted),"United States of America,United States of America",,,,,,2025-05-01 10:42,,,,500000.0,,"Academia,Industry,Academia",,,,,Open source,"https://github.com/sustcsonglin/flash-linear-attention/blob/main/fla/layers/gla.py

MIT license for code (seems like training code)

https://huggingface.co/fla-hub/gla-340M-15B
MIT license","Academia,Industry,Academia",,,,,Operation counting,fla-hub,,,
Pharia-1-LLM-7B,Language,"Language modeling/generation,Translation,Question answering",Aleph Alpha,,2024-08-26,Introducing Pharia-1-LLM: transparent and compliant,https://huggingface.co/Aleph-Alpha/Pharia-1-LLM-7B-control,,,,7041544704.0,,4.43e+23,"reported by the authors: 2.75*10^23 + 1.68*10^23 = 4.43*10^23 FLOP

https://huggingface.co/Aleph-Alpha/Pharia-1-LLM-7B-control#compute--training-efficiency",Common Crawl,"The training data of our models comprises two components: web-crawled data and structured datasets with a total size of 7.7T, with a cutoff date 04/2023. We performed some additional web scraping to augment these datasets.

Web-crawled data was obtained by filtering and deduplicating data available in public datasets, derived from Common Crawl, in the following languages: English, French, German, Italian, Spanish, Dutch, Portuguese.

To deduplicate the data, we applied a Bloomfilter for exact document deduplication in English, French, German, Italian and Spanish. Portuguese and Dutch data was deduplicated using both URLs and fuzzy-deduplication with MinHashLSH.",7700000000000.0,4.7T + 3T = 7.7T tokens,,,"NVIDIA A100 SXM4 80 GB,NVIDIA H100 SXM5 80GB",,Confident,"We are pleased to announce our new foundation model family that includes Pharia-1-LLM-7B-control and Pharia-1-LLM-7B-control-aligned, now publicly available under the Open Aleph License, which explicitly allows for non-commercial research and educational use. Pharia-1-LLM-7B-control is engineered to deliver concise, length-controlled responses that match the performance of leading open-source models in the 7B to 8B parameter range and is culturally and linguistically optimized for German, French, and Spanish by being trained on a multilingual base corpus. Pharia-1-LLM-7B-control is trained on carefully curated data in compliance with applicable EU and national regulations, including copyright and data privacy laws. With improved token efficiency, Pharia-1-LLM-7B-control excels in domain-specific applications, particularly in the automotive and engineering industries, and can be aligned to user preferences, making it suitable for critical applications without the risk of shutdown behavior. As such, it serves as a valuable addition to the communityâ€™s selection of weight-available foundation models. Pharia-1-LLM-7B-control-aligned has been added with additional safety guardrails via alignment methods.",,,Open weights (non-commercial),Germany,,,,256.0,,2025-05-26 19:05,,,,,,Industry,,,,,Open (non-commercial),"https://huggingface.co/Aleph-Alpha/Pharia-1-LLM-7B-control

training framework is released here: https://github.com/Aleph-Alpha-Research/scaling",Industry,,,,,Reported,Aleph-Alpha,,,
DISTRO,Language,"Language modeling/generation,Chat,Question answering",Nous Research,"Bowen Peng, Jeffrey Quesnelle, Dillon Rolnick, Ari Lotter, Umer H. Adil, Esteban La Rocca",2024-08-26,A PRELIMINARY REPORT ON DISTRO,https://github.com/NousResearch/DisTrO/blob/main/A_Preliminary_Report_on_DisTrO.pdf,,,,1200000000.0,1.2B,7.1497946e+20,"6 FLOP / parameter / token * 1.2*10^9 parameters * 104857600000 tokens = 7.5497472e+20 FLOP

989500000000000 FLOP / GPU / sec [bf16 assumed] *19.8 hours * 3600 sec / hour * 32 GPUs * 0.3 [assumed utilization] = 6.7710298e+20 FLOP

sqrt(6.7710298e+20*7.5497472e+20) = 7.1497946e+20",Dolma,"""we used the first 105B tokens of a randomly shuffled 10% representative sampling of the Dolma v1.7 [17]5 dataset""",104857600000.0,"Total Steps 25000
Sequence Length 2048
Batch Size 2048 (4M tokens)
Total Tokens 104.8576B",19.8,,NVIDIA H100 SXM5 80GB,,Confident,"Training large scale neural networks typically involves sharing gradients between all accelerators, which necessitates specialized, high-speed interconnects. To address this, we introduce DisTrO, a family of architecture-agnostic and network-agnostic distributed optimizers that reduces the inter-GPU communication requirements by four to five orders of magnitude without relying on amortized analysis, enabling low-latency training of large neural networks on slow internet bandwidths with heterogeneous networking hardware. In this preliminary report we are excited to show the first and
earliest empirical proof that DisTrO-AdamW matches standard AdamW+All-Reduce in convergence rate while massively reducing the required bandwidth during pre-training of a 1.2B LLM. When
using Distributed Data Parallelism, DisTrO may enable future large scale foundation model training to bypass the need for high-speed interconnects entirely.",,,,United States of America,,,,32.0,,2025-05-01 10:42,,,,,,Industry,,,,,,"code should be released ""soon""",Industry,,,,44151.1910097316,"Operation counting,Hardware",,,,
Xinyu,Language,Language modeling/generation,"Zhejiang University (ZJU),Institute for Advanced Algorithms Research,Northeastern University (China),China Telecom,State Key Laboratory of Media Convergence Production Technology and Systems","Yiquan Wu, Bo Tang, Chenyang Xi, Yu Yu, Pengyu Wang, Yifei Liu, Kun Kuang, Haiying Deng, Zhiyu Li, Feiyu Xiong, Jie Hu, Peng Cheng, Zhonghao Wang, Yi Wang, Yi Luo, Mingchuan Yang",2024-08-23,Xinyu: An Efficient LLM-based System for Commentary Generation,https://arxiv.org/abs/2408.11609,0.0,,,13000000000.0,,,"128 Nvidia A800 80G GPUs

","Belle-2M,Unspecified unreleased",,500000000000.0,"""we continued pre-training on LLaMA13B using a corpus comprising 500B tokens, which contains both English and Chinese corpus.""",528.0,"""The continued pre-training phase lasted 20 days on 128 Nvidia A800 80G GPUs, while the supervised fine-tuning (SFT) process took 2 days on 8 Nvidia A800 80G GPUs.""",NVIDIA A800 PCIe 80 GB,,Confident,"In this paper, we introduce Xinyu, an efficient LLM-based system designed to assist commentators in generating Chinese commentaries. To meet the fundamental requirements, we deconstruct the generation process into sequential steps, proposing targeted strategies and supervised fine-tuning (SFT) for each step. ",,,Unreleased,"China,China,China,China,China",Llama 2-13B,2,"pre-training: 312000000000000 FLOP / GPU / sec [A800 80GB reported, bf 16 assumed] * 128 GPUs * 20 days * 24 hours / day * 3600 sec / hour * 0.3 [assumed utilization] = 2.0702822e+22 FLOP   

SFT: 312000000000000 FLOP / GPU / sec [A800 80GB reported, bf 16 assumed] * 8 GPUs * 2 days * 24 hours / day * 3600 sec / hour * 0.3 [assumed utilization] = 1.2939264e+20 FLOP

Total: 2.0832215e+22 FLOP

6 FLOP / token / parameter * 13 * 10^9 parameters * 500 * 10^9 tokens = 3.9e+22 FLOP

sqrt(3.9e+22*2.0832215e+22) = 2.8503621e+22 

",128.0,,2025-06-09 10:29,,,,,,"Academia,Academia,Academia,Industry,Academia",,,,,Unreleased,,"Academia,Academia,Academia,Industry,Academia",,,,63077.34395151266,"Hardware,Operation counting",,,,
Jamba 1.5-Large,Language,"Language modeling/generation,Chat,Translation,Question answering",AI21 Labs,"Barak Lenz, Alan Arazi, Amir Bergman, Avshalom Manevich, Barak Peleg, Ben Aviram, Chen Almagor, Clara Fridman, Dan Padnos, Daniel Gissin, Daniel Jannai, Dor Muhlgay, Dor Zimberg, Edden M Gerber, Elad Dolev, Eran Krakovsky, Erez Safahi, Erez Schwartz, Gal Cohen, Gal Shachaf, Haim Rozenblum, Hofit Bata, Ido Blass, Inbal Magar, Itay Dalmedigos, Jhonathan Osin, Julie Fadlon, Maria Rozman, Matan Danos, Michael Gokhman, Mor Zusman, Naama Gidron, Nir Ratner, Noam Gat, Noam Rozen, Oded Fried, Ohad Leshno, Omer Antverg, Omri Abend, Opher Lieber, Or Dagan, Orit Cohavi, Raz Alon, Ro'i Belson, Roi Cohen, Rom Gilad, Roman Glozman, Shahar Lev, Shaked Meirom, Tal Delbari, Tal Ness, Tomer Asida, Tom Ben Gal, Tom Braude, Uriya Pumerantz, Yehoshua Cohen, Yonatan Belinkov, Yuval Globerson, Yuval Peleg Levy, Yoav Shoham",2024-08-22,Jamba-1.5: Hybrid Transformer-Mamba Models at Scale,"https://arxiv.org/abs/2408.12570
https://www.ai21.com/blog/announcing-jamba-model-family
https://huggingface.co/ai21labs/AI21-Jamba-1.5-Large",,Training cost,,398000000000.0,94B active/398B total,,,Unspecified unreleased,,,,,,NVIDIA H100 SXM5 80GB,Self-supervised learning,Confident,"We present Jamba-1.5, new instruction-tuned large language models based on our Jamba architecture. Jamba is a hybrid Transformer-Mamba mixture of experts architecture, providing high throughput and low memory usage across context lengths, while retaining the same or better quality as Transformer models. We release two model sizes: Jamba-1.5-Large, with 94B active parameters, and Jamba-1.5-Mini, with 12B active parameters. Both models are fine-tuned for a variety of conversational and instruction-following capabilties, and have an effective context length of 256K tokens, the largest amongst open-weight models. To support cost-effective inference, we introduce ExpertsInt8, a novel quantization technique that allows fitting Jamba-1.5-Large on a machine with 8 80GB GPUs when processing 256K-token contexts without loss of quality. When evaluated on a battery of academic and chatbot benchmarks, Jamba-1.5 models achieve excellent results while providing high throughput and outperforming other open-weight models on long-context benchmarks. The model weights for both sizes are publicly available under the Jamba Open Model License and we release ExpertsInt8 as open source.",,,Open weights (restricted use),Israel,,,,,,2025-05-30 14:31,,,,,,Industry,,,,,Unreleased,Commercial use allowed up to $50M USD annual revenue.,Industry,,,BF16,,,ai21labs,,,
GameNGen,Games,Doom,"Google Research,Google DeepMind,Tel Aviv University","Dani Valevski, Yaniv Leviathan, Moab Arar, Shlomi Fruchter",2024-08-22,Diffusion Models Are Real-Time Game Engines,https://arxiv.org/abs/2408.14837,,,,,"the base model (Stable Diffusion) has 1.45B parameters, but they don't report parameters count for GamenGen",,,,,,""" We use a batch size of 128 "" (for the main training)
""Unless noted otherwise, all results in the paperare after 700,000 training steps. <..> We use a batch size of 2,048 for optimizing the latent decoder, other training parameters are identical to those of the denoiser. <..> Overall we generate 900M frames for training. All image frames (during training, inference, and conditioning) are at a resolution of 320x240 padded to 320x256.""",,"""We train using 128 TPU-v5e devices with data parallelization""",Google TPU v5e,,Unknown,"We present GameNGen, the first game engine powered entirely by a neural model that enables real-time interaction with a complex environment over long trajectories at high quality. GameNGen can interactively simulate the classic game DOOM at over 20 frames per second on a single TPU. Next frame prediction achieves a PSNR of 29.4, comparable to lossy JPEG compression. Human raters are only slightly better than random chance at distinguishing short clips of the game from clips of the simulation. GameNGen is trained in two phases: (1) an RL-agent learns to play the game and the training sessions are recorded, and (2) a diffusion model is trained to produce the next frame, conditioned on the sequence of past frames and actions. Conditioning augmentations enable stable auto-regressive generation over long trajectories.",,,Unreleased,"Multinational,United States of America,Canada,Switzerland,United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France,Israel",Stable Diffusion (LDM-KL-8-G),,,128.0,,2025-05-01 10:42,,,,,,"Industry,Industry,Academia",,,,,Unreleased,,"Industry,Industry,Academia",,,,35324.09924956927,,,,,
GITIII,Biology,Cell-cell interaction prediction,Yale School of Public Health,Xiao Xiao,2024-08-22,Investigation of pair-wise single-cell interactions by statistically interpreting spatial cell state correlation learned by self-supervised graph inductive bias transformer,https://www.biorxiv.org/content/10.1101/2024.08.21.608964v1,0.0,,,,,,,,,,,,,NVIDIA RTX A5000,,Unknown,"Image-based spatial transcriptomics (ST) offers spatial gene expression profile at the single-cell resolution and provides information to understand intercellular communication that is critical for maintaining tissue development and organ function. Disruption of normal cell-cell interactions (CCI) can lead to disease onset and progression. Current CCI analysis methods face several limitations, including subjection to the number of measured ligand-receptor genes in image-based spatial transcriptomics, limited graph encoding power, inadequate use of spatial information, and low interpretability. Here, we present GITIII, an interpretable self-supervised graph transformerbased language model that treats cells as words (nodes) and their cell neighborhood as a sentence to explore the communications among cells. Enhanced by multilayer perceptron-based distance scaler, physics-informed attention mechanism, and a state-of-the-art, expressive, and lightweight graph transformer model, GITIII infers CCI by investigating how the state of a cell is influenced by the spatial organization, ligand expression, cell types and states of neighboring cells. With its interpretable architecture, GITIII can be used to understand how the sender cell influences target genes in the receiver cell, visualize the spatial pattern and utility of CCI, identify significant CCI networks, perform CCI-informed cell subtyping, and compare CCI strength between disease groups. Applications to four ST datasets from several species, organs, and platforms, GITIII effectively identified and quantitatively interpreted key CCI patterns driving within-sample heterogeneity and disease progression, thus improving our understanding of brain structures, tumor microenvironments, and the interplay among different cell types responding to neighboring CCIs.",50.0,,,,,,,,,2025-05-01 10:42,,,,,,,,,,,,,,,,,,,,,,
Jamba 1.5 Mini,Language,"Language modeling/generation,Translation,Question answering",AI21 Labs,"Barak Lenz, Alan Arazi, Amir Bergman, Avshalom Manevich, Barak Peleg, Ben Aviram, Chen Almagor, Clara Fridman, Dan Padnos, Daniel Gissin, Daniel Jannai, Dor Muhlgay, Dor Zimberg, Edden M Gerber, Elad Dolev, Eran Krakovsky, Erez Safahi, Erez Schwartz, Gal Cohen, Gal Shachaf, Haim Rozenblum, Hofit Bata, Ido Blass, Inbal Magar, Itay Dalmedigos, Jhonathan Osin, Julie Fadlon, Maria Rozman, Matan Danos, Michael Gokhman, Mor Zusman, Naama Gidron, Nir Ratner, Noam Gat, Noam Rozen, Oded Fried, Ohad Leshno, Omer Antverg, Omri Abend, Opher Lieber, Or Dagan, Orit Cohavi, Raz Alon, Ro'i Belson, Roi Cohen, Rom Gilad, Roman Glozman, Shahar Lev, Shaked Meirom, Tal Delbari, Tal Ness, Tomer Asida, Tom Ben Gal, Tom Braude, Uriya Pumerantz, Yehoshua Cohen, Yonatan Belinkov, Yuval Globerson, Yuval Peleg Levy, Yoav Shoham",2024-08-22,Jamba-1.5: Hybrid Transformer-Mamba Models at Scale,https://arxiv.org/abs/2408.12570,,,,52000000000.0,12B active/52B total,,,Unspecified unreleased,"Knowledge cutoff date: March 5, 2024
Supported languages: English, Spanish, French, Portuguese, Italian, Dutch, German, Arabic and Hebrew",,,,,,,Confident,"* Jamba-1.5-Mini is an updated and instruction-tuned version of our earlier Jamba release 

We present Jamba-1.5, new instruction-tuned large language models based on our Jamba architecture. Jamba is a hybrid Transformer-Mamba mixture of experts architecture, providing high throughput and low memory usage across context lengths, while retaining the same or better quality as Transformer models. We release two model sizes: Jamba-1.5-Large, with 94B active parameters, and Jamba-1.5-Mini, with 12B active parameters. Both models are fine-tuned for a variety of conversational and instruction-following capabilties, and have an effective context length of 256K tokens, the largest amongst open-weight models. To support cost-effective inference, we introduce ExpertsInt8, a novel quantization technique that allows fitting Jamba-1.5-Large on a machine with 8 80GB GPUs when processing 256K-token contexts without loss of quality. When evaluated on a battery of academic and chatbot benchmarks, Jamba-1.5 models achieve excellent results while providing high throughput and outperforming other open-weight models on long-context benchmarks. The model weights for both sizes are publicly available under the Jamba Open Model License and we release ExpertsInt8 as open source.",,,Open weights (restricted use),Israel,,,,,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,"Commercial use allowed up to $50M USD annual revenue.

https://huggingface.co/ai21labs/AI21-Jamba-1.5-Mini",Industry,,,BF16,,,ai21labs,,,
CoPRA,Biology,Protein-RNA binding affinity prediction,"Tsinghua University,University College London (UCL),Monash University,Beijing University of Posts and Telecommunications","Rong Han, Xiaohong Liu, Tong Pan, Jing Xu, Xiaoyu Wang, Wuyang Lan, Zhenyu Li, Zixuan Wang, Jiangning Song, Guangyu Wang, Ting Chen",2024-08-21,CoPRA: Bridging Cross-domain Pretrained Sequence Models with Complex Structures for Protein-RNA Binding Affinity Prediction,https://arxiv.org/abs/2409.03773,0.0,,,,,,,,,27000001.0,"Summary:
1. Chain pairs: 30,000
2. Poses per pair: 3
Total datapoints: 30,000 Ã— 3 = 90,000

3. Tokens per datapoint:
- Protein residues: 200
- RNA bases: 100
Total tokens per datapoint: 200 + 100 = 300

4. Final calculation:
90,000 datapoints Ã— 300 tokens/datapoint = 27,000,000 tokens",,,NVIDIA A100,,Likely,"Accurately measuring protein-RNA binding affinity is crucial in many biological processes and drug design. Previous computational methods for protein-RNA binding affinity prediction rely on either sequence or structure features,
unable to capture the binding mechanisms comprehensively.
The recent emerging pre-trained language models trained on
massive unsupervised sequences of protein and RNA have
shown strong representation ability for various in-domain
downstream tasks, including binding site prediction. However, applying different-domain language models collaboratively for complex-level tasks remains unexplored. In this paper, we propose CoPRA to bridge pre-trained language models from different biological domains via Complex structure
for Protein-RNA binding Affinity prediction. We demonstrate
for the first time that cross-biological modal language models can collaborate to improve binding affinity prediction. We
propose a Co-Former to combine the cross-modal sequence
and structure information and a bi-scope pre-training strategy for improving Co-Formerâ€™s interaction understanding.
Meanwhile, we build the largest protein-RNA binding affinity
dataset PRA310 for performance evaluation. We also test our
model on a public dataset for mutation effect prediction. CoPRA reaches state-of-the-art performance on all the datasets.
We provide extensive analyses and verify that CoPRA can
(1) accurately predict the protein-RNA binding affinity; (2)
understand the binding affinity change caused by mutations;
and (3) benefit from scaling data and model size.1",,,,"China,United Kingdom of Great Britain and Northern Ireland,Australia,China",,,,4.0,,2025-05-01 10:42,,,,,,"Academia,Academia,Academia,Academia",,,,,,,"Academia,Academia,Academia,Academia",,,,3154.0076699829947,,,,,
Ideogram v2,Image generation,"Image generation,Text-to-image",Ideogram,,2024-08-21,Ideogram 2.0,https://about.ideogram.ai/2.0,,,,,,,,Unspecified unreleased,,,,,,,,Unknown,"We are excited to release Ideogram 2.0, our new frontier text to image model with industry leading capabilities in generating realistic images, graphic design, typography, and more. Trained from scratch like all Ideogram models, Ideogram 2.0 significantly outperforms other text to image models across many quality metrics, including image-text alignment, overall subjective preference, and text rendering accuracy.",,,API access,"United States of America,Canada",,,,,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
Kosmos-2.5,"Multimodal,Language,Vision","Character recognition,Document classification,Language modeling/generation,Visual question answering,Document representation",Microsoft,"Tengchao Lv, Yupan Huang, Jingye Chen, Yuzhong Zhao, Yilin Jia, Lei Cui, Shuming Ma, Yaoyao Chang, Shaohan Huang, Wenhui Wang, Li Dong, Weiyao Luo, Shaoxiang Wu, Guoxin Wang, Cha Zhang, Furu Wei",2024-08-21,KOSMOS-2.5: A Multimodal Literate Model,https://arxiv.org/abs/2309.11419,,,,1300000000.0,"""KOSMOS-2.5 contains a total of 1.3 billion parameters""

""The vision encoder is initialized from the Pix2Struct-Large modelâ€™s encoder (Lee et al. 2023), which is based on the Vision Transformer (ViT) (Dosovitskiy et al. 2021)""",2.2018015e+21,"6 FLOP / token / parameter * 1.3 * 10^9 parameters * 260 * 10^9 tokens [1 epoch assumed] = 2.028e+21 FLOP

+ 1.7380147e+20 FLOP [""likely"" base model Pix2struct-Large training compute] = 2.2018015e+21 FLOP",Unspecified unreleased,,260000000000.0,"""The total training involved approximately 260 billion tokens.""

""Our training data is collected using an automated pipeline from diverse sources, resulting in a large corpus of 357.4 million document images, annotated with text lines using bounding boxes or in markdown format.""",,,,,Confident,"The automatic reading of text-intensive images represents a significant advancement toward achieving Artificial General Intelligence (AGI). In this paper we present KOSMOS-2.5, a multimodal literate model for machine reading of text-intensive images. Pre-trained on a large-scale corpus of text-intensive images, KOSMOS-2.5 excels in two distinct yet complementary transcription tasks: (1) generating spatially-aware text blocks, where each block of text is assigned spatial coordinates within the image, and (2) producing structured text output that captures both style and structure in markdown format. This unified multimodal literate capability is achieved through a shared decoder-only autoregressive Transformer architecture and task-specific prompts. Building on this foundation, we fine-tune KOSMOS-2.5 for document understanding tasks, resulting in a document understanding generalist named KOSMOS-2.5-CHAT. Additionally, a large corpus of 357.4 million document pages spanning diverse domains was curated for pre-training. We evaluate KOSMOS-2.5 on two newly proposed benchmarks, OCREval and MarkdownEval, for document-level text recognition and image-to-markdown generation, demonstrating impressive literate capabilities comparable to GPT-4o. KOSMOS-2.5-CHAT achieves performance comparable to other state-of-the-art generalists that are five times larger (1.3B vs. 7B) across nine text-rich visual question answering benchmarks. Models and code have been available at \url{this https URL}.",,,Open weights (unrestricted),"United States of America,Multinational,India,Belgium",Pix2Struct-Large,,,,,2025-06-01 14:52,,,,,,Industry,,,,,Unreleased,"MIT license
https://github.com/microsoft/unilm/tree/master/kosmos-2.5
https://huggingface.co/microsoft/kosmos-2.5

I don't see training code in the repo, it seems it is only inference and fine-tuning code",Industry,,,,,Operation counting,microsoft,,,
Rethinking Molecular Design: Integrating Latent Variable and Auto-Regressive Models for Goal Directed Generation,Biology,Drug discovery,"ETH Zurich,University of Zurich,ETH AI Center","Heath Arthur-Loui, Amina Mollaysa, Michael Krauthammer",2024-08-19,Rethinking Molecular Design: Integrating Latent Variable and Auto-Regressive Models for Goal Directed Generation,https://arxiv.org/abs/2409.00046,0.0,,,,,,"
",,,35000001.0,"QM9 Dataset: 113,885 molecules Ã— 40 tokens = 4,555,400 tokens
ZINC250k Dataset: 250,000 molecules Ã— 120 tokens = 30,000,000 tokens
Total: 4,555,400 + 30,000,000 = 34,555,400 tokens â‰ˆ 3.5 Ã— 10^7 tokens",,,NVIDIA GeForce RTX 4090,,Confident,"De novo molecule design has become a highly active research area, advanced significantly through the use of state-of-the-art generative models. Despite these advances, several fundamental questions remain unanswered as the field increasingly focuses on more complex generative models and sophisticated molecular representations as an answer to the challenges of drug design. In this paper, we return to the simplest representation of molecules, and investigate overlooked limitations of classical generative approaches, particularly Variational Autoencoders (VAEs) and auto-regressive models. We propose a hybrid model in the form of a novel regularizer that leverages the strengths of both to improve validity, conditional generation, and style transfer of molecular sequences. Additionally, we provide an in depth discussion of overlooked assumptions of these models' behaviour.",500.0,,Open weights (non-commercial),"Switzerland,Switzerland,Switzerland",,,,1.0,,2025-06-16 12:23,,,,,,"Academia,Academia,Research collective",,,,,Open (non-commercial),"no clear license
https://github.com/HeathArhturLouis/Rethinking-Molecular-Design-Integrating-Latent-Variable-and-Autoregressive-Models-for-Enhanced-Goal","Academia,Academia,Research collective",,,,487.41987187195446,Hardware,,,,
CLR_ESP,Biology,Enzyme substrate pair prediction,Kansas State University,"Zhenjiao Du, Weiming Fu, Xiaolong Guo, Doina Caragea, Yonghui Li",2024-08-16,CLR_ESP: Improved enzyme-substrate pair prediction using contrastive learning,https://www.biorxiv.org/content/10.1101/2024.08.13.607829v1,,,,,,2.11e+17,3*60*60*65130000000000*0.3=2.110212e+17,,"""In contrast, the phylogenetic evidence-based dataset comprised a total
of 765,635 pairs.""

Estimating 350 tokens per enzyme pair: 765635 * 350=267972250",267972250.0,,3.0,,NVIDIA Tesla T4,,Likely,"To reduce the cost of experimental characterization of the potential substrates for enzymes, machine learning prediction model offer an alternative solution. Pretrained language models, as powerful approaches for protein and molecule representation, have been employed in the development of enzyme-substrate prediction models, achieving promising performance. In addition to continuing improvements in language models, effectively fusing encoders to handle multimodal prediction tasks is critical for further enhancing model performance using available representation methods. Here, we present CLR_ESP, a multimodal classifier that integrates protein and chemistry language models with a newly designed contrastive learning strategy for predicting enzyme-substrate pairs. Our best model achieved SOTA performance with an accuracy of 94.70% on independent test data while requiring fewer computational resources and training data. It also confirmed our hypothesis that embeddings of positive pairs are closer to each other in high-dimension space, while negative pairs exhibit the opposite trend. The proposed architecture is expected to be further applied to enhance performance in additional multimodality prediction tasks in biology. A user-friendly web server of CLR_ESP is established and freely accessible at https://78k6imn5wp.us-east-1.awsapprunner.com/.",500.0,,Open weights (non-commercial),United States of America,,,,1.0,,2025-06-09 13:52,,,,,,Academia,,,,,Open (non-commercial),"""The datasets and codes used to generate the results of this paper are available from https://github.com/dzjxzyd/CLR_ESP"" 

no clear license",Academia,,,,75.82593457631364,,,,,
DeepSeek-Prover-V1.5,Language,Quantitative reasoning,DeepSeek,"Huajian Xin, Z.Z. Ren, Junxiao Song, Zhihong Shao, Wanjia Zhao, Haocheng Wang, Bo Liu, Liyue Zhang, Xuan Lu, Qiushi Du, Wenjun Gao, Qihao Zhu, Dejian Yang, Zhibin Gou, Z.F. Wu, Fuli Luo, Chong Ruan",2024-08-15,DeepSeek-Prover-V1.5: Harnessing Proof Assistant Feedback for Reinforcement Learning and Monte-Carlo Tree Search,https://arxiv.org/abs/2408.08152,,,,7000000000.0,,,,,,,,,,,,Confident,"We introduce DeepSeek-Prover-V1.5, an open-source language model designed for theorem proving in Lean 4, which enhances DeepSeek-Prover-V1 by optimizing both training and inference processes. Pre-trained on DeepSeekMath-Base with specialization in formal mathematical languages, the model undergoes supervised fine-tuning using an enhanced formal theorem proving dataset derived from DeepSeek-Prover-V1. Further refinement is achieved through reinforcement learning from proof assistant feedback (RLPAF). Beyond the single-pass whole-proof generation approach of DeepSeek-Prover-V1, we propose RMaxTS, a variant of Monte-Carlo tree search that employs an intrinsic-reward-driven exploration strategy to generate diverse proof paths. DeepSeek-Prover-V1.5 demonstrates significant improvements over DeepSeek-Prover-V1, achieving new state-of-the-art results on the test set of the high school level miniF2F benchmark (63.5%) and the undergraduate level ProofNet benchmark (25.3%).",,,Open weights (restricted use),China,DeepSeekMath 7B,,,,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,"deepseek license 
https://huggingface.co/deepseek-ai/DeepSeek-Prover-V1.5-Base",Industry,,,,,,deepseek-ai,,,
Gen-3 Alpha Turbo,"Video,Vision","Video generation,Image-to-video",Runway,,2024-08-15,Gen-3 Alpha Turbo is a faster model in the Gen-3 Alpha family that generates at a lower cost. The Turbo model is available on all plan levels and requires an input image.,"https://runwayml.com/news/introducing-the-runway-api

https://help.runwayml.com/hc/en-us/articles/30266515017875-Creating-with-Text-Image-to-Video-on-Gen-3-Alpha-and-Turbo",,,,,,,,Unspecified unreleased,,,,,,,,Unknown,"Gen-3 Alpha Turbo Image to Video is now available and can generate 7x faster for half the price of the original Gen-3 Alpha. All while still matching performance across many use cases. Turbo is available for all plans, including trial for free users.

More improvements to the model, control mechanisms and possibilities for real-time interactivity to come.",,,API access,United States of America,,,,,,2025-05-29 14:58,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
Hermes 3 405B,Language,"Language modeling/generation,Chat,Question answering",Nous Research,"teknium, Jeffrey Quesnelle, Chen Guang",2024-08-14,Hermes 3 Technical Report,https://nousresearch.com/wp-content/uploads/2024/08/Hermes-3-Technical-Report.pdf,,,,405000000000.0,405B,,,Hermes 3,"""The Hermes 3 dataset comprises a diverse collection of high-quality instruction data, meticulously curated and generated
to encompass a wide range of domains and use cases. The construction of this dataset, which commenced in March
2024 and concluded in August 2024, involved a rigorous selection process to ensure the highest quality and relevance
of the included data""",390000000.0,390M tokens (Table 1),,,NVIDIA H100 SXM5 80GB,,Confident,"Instruct (or â€œchatâ€) tuned models have become the primary way in which most people interact with
large language models. As opposed to â€œbaseâ€ or â€œfoundationâ€ models, instruct-tuned models are
optimized to respond to imperative statements. We present Hermes 3, a neutrally-aligned generalist
instruct and tool use model with strong reasoning and creative abilities. Its largest version, Hermes 3
405B, achieves state of the art performance among open weight models on several public benchmarks.
The weights for all models are available at https://huggingface.co/NousResearch.",4.0,,Open weights (restricted use),United States of America,Llama 3.1-405B,2,"6 FLOP / parameter / token * 405*10^9 parameters * 390*10^6 tokens * 4 epochs = 3.7908e+21 FLOP

989500000000000 FLOP / GPU / sec * 2086 GPU-hours * 3600 sec / hour * 0.3 [assumed utilization] = 2.2292248e+21 FLOP

sqrt(2.2292248e+21*3.7908e+21) = 2.9069822e+21",128.0,,2025-05-01 10:42,,,,,,Industry,,,,2086.0,Unreleased,"Llama license
https://huggingface.co/NousResearch/Hermes-3-Llama-3.1-405B",Industry,,,,176651.9648701551,"Hardware,Operation counting",,,,
Grok-2,"Language,Vision,Multimodal","Chat,Language modeling/generation,Question answering,Code generation,Visual question answering",xAI,,2024-08-13,Grok-2 Beta Release,https://x.ai/blog/grok-2,,Training cost,,,,2.9599999999999996e+25,Estimate based on xAI statements comparing Grok-2 compute to GPT-4 and Grok-3. Full estimate here: https://docs.google.com/document/d/1C_dABuZrAqYE_ui4_GZ4bRLtq3TBjIGoBSktaPElhEU/edit?usp=sharing,Unspecified unreleased,"Knowledge cutoff date is August 2024, according to https://llm-stats.com/models/grok-2.",,,,,NVIDIA H100 SXM5 80GB,,Confident,Grok-2 is our frontier language model with state-of-the-art reasoning capabilities. This release includes two members of the Grok family: Grok-2 and Grok-2 mini. Both models are now being released to Grok users on the ð• platform.,,,Hosted access (no API),United States of America,,,,,,2025-05-16 10:30,,,,,,Industry,,2.1e+25,,,Unreleased,,Industry,,,,,"Comparison with other models,Reported",,,,4
Grok-2 mini,"Language,Vision,Multimodal","Chat,Language modeling/generation,Question answering,Code generation,Visual question answering",xAI,,2024-08-13,Grok-2 Beta Release,https://x.ai/blog/grok-2,,,,,,,,Unspecified unreleased,"Knowledge cutoff date is August 2024, according to https://llm-stats.com/models/grok-2-mini. ",,,,,,,Unknown,Grok-2 is our frontier language model with state-of-the-art reasoning capabilities. This release includes two members of the Grok family: Grok-2 and Grok-2 mini. Both models are now being released to Grok users on the ð• platform.,,,Hosted access (no API),United States of America,,,,,,2025-05-13 01:38,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
Cosine Genie,Language,Code generation,Cosine,Alistair Pullen,2024-08-12,Genie is the best software engineering model in the world scoring a state-of-the-art score of 30.08% on the SWE-Bench evaluation and 50.67% on SWE-Lite.,https://cosine.sh/blog/genie-technical-report,,,,,,,,Unspecified unreleased,"Genie was trained on proprietary data that codifies human reasoning, representing perfect information lineage, incremental knowledge discovery, and step by step decision making derived from real examples of Software Engineers doing their jobs.",,,,,,,Unknown,"Genie is the best AI software engineer in the world by far - achieving a 30% eval score on the industry standard benchmark SWE-Bench.

Genie is able to solve bugs, build features, refactor code, and everything in between either fully autonomously or paired with the user, like working with a colleague, not just a copilot.

If you wish to talk to us about our model feel free to reach out.",,,Unreleased,United Kingdom of Great Britain and Northern Ireland,,,,,,2025-05-01 10:42,,,,,,Industry,,,,,,https://github.com/CosineAI/experiments/tree/cos/swe-bench-submission/evaluation/test/20230726_cosine_genie,Industry,,,,,,,,,
Falcon Mamba,Language,"Language modeling/generation,Question answering,Code generation,Quantitative reasoning",Technology Innovation Institute,"Jingwei Zuo, Maksim Velikanov, Dhia Eddine Rhaiem,  Ilyas Chahed,  Younes Belkada, Guillaume Kunsch, Hakim Hacid",2024-08-12,Falcon Mamba: The First Competitive Attention-free 7B Language Model,https://huggingface.co/tiiuae/falcon-mamba-7b,,,,7000000000.0,,3.9391101e+23,989400000000000 FLOP / GPU / sec [bf16 assumed] * 1440 hours [see training time notes] * 3600 sec / hour * 256 GPUs * 0.3 [assumed utilization] = 3.9391101e+23 FLOP,RefinedWeb,,,"""Falcon Mamba was trained with ~ 5500GT of data, mainly composed of RefinedWeb data with addition of high-quality technical data and code data from public sources.""

""Batch size	2048""
""Sequence length	8192	During the last training stages""",1440.0,"""Falcon-Mamba-7B was trained on 256 H100 80GB GPUs for the majority of the training, using a 3D parallelism strategy (TP=1, PP=1, DP=256) combined with ZeRO.""

""The model training took roughly two months.""

30*2*24=1440 hours",NVIDIA H100 SXM5 80GB,,Confident,Falcon Mamba is a new model by Technology Innovation Institute (TII) in Abu Dhabi released under the TII Falcon Mamba 7B License 1.0. The model is open access and available within the Hugging Face ecosystem here for anyone to use for their research or application purposes.,,,Open weights (restricted use),United Arab Emirates,,,,256.0,,2025-05-30 14:47,,,,,,Government,,,,,,"""The model is open access and available within the Hugging Face ecosystem here for anyone to use for their research or application purposes.""
TII Falcon-Mamba License 2.0
https://huggingface.co/tiiuae/falcon-mamba-7b",Government,,,,353319.6658035087,Hardware,tiiuae,,,
P-LLama3,Biology,Protein design,University of Siena,"Kamyar Zeinalipour, Neda Jamshidi, Monica Bianchini, Marco Maggini, Marco Gori",2024-08-12,Design Proteins Using Large Language Models: Enhancements and Comparative Analyses,https://arxiv.org/abs/2408.06396,0.0,,,8000000000.0,,7.20000786432e+23,"Llama 3 compute, other models were also trained
Finetune compute 6*8000000000*16384000=786432000000000000
Llama 3 base: 7.2e+23
Total: 720000786432000000000000",,,16384000.0,"42,000 protein sequences Ã— 512 tokens/sequence = 21,504,000 total tokens (2.15 Ã— 10â· datapoints)

Training was not done on all data: 
""The training configuration
utilized a sequence length of 512, with a maximum
training step limit of 2000 and a batch size of 1,
coupled with a gradient accumulation step size of
16 for enhanced training efficiency.""

2000*512*16=16384000

Epochs: (2000*512*16)/21504000=0.76",,,NVIDIA RTX A6000,,Confident,"Pre-trained LLMs have demonstrated substantial capabilities across a range of conventional natural language processing (NLP) tasks, such as summarization and entity recognition. In this paper, we explore the application of LLMs in the generation of high-quality protein sequences. Specifically, we adopt a suite of pre-trained LLMs, including Mistral-7B1, Llama-2-7B2, Llama-3-8B3, and gemma-7B4, to produce valid protein sequences. All of these models are publicly available.5 Unlike previous work in this field, our approach utilizes a relatively small dataset comprising 42,000 distinct human protein sequences. We retrain these models to process protein-related data, ensuring the generation of biologically feasible protein structures. Our findings demonstrate that even with limited data, the adapted models exhibit efficiency comparable to established protein-focused models such as ProGen varieties, ProtGPT2, and ProLLaMA, which were trained on millions of protein sequences. To validate and quantify the performance of our models, we conduct comparative analyses employing standard metrics such as pLDDT, RMSD, TM-score, and REU. Furthermore, we commit to making the trained versions of all four models publicly available, fostering greater transparency and collaboration in the field of computational biology.",0.76,,Open weights (restricted use),Italy,Llama 3-8B,,,4.0,,2025-05-23 12:49,,,,,,Academia,,,,,Unreleased,"llama license
https://huggingface.co/Kamyar-zeinalipour/P-Llama3-8B",Academia,,,,2365.97990493421,Operation counting,Kamyar-zeinalipour,,,
P-Mistral,Biology,"Protein generation,Protein design",University of Siena,"Kamyar Zeinalipour, Neda Jamshidi, Monica Bianchini, Marco Maggini, Marco Gori",2024-08-12,Design Proteins Using Large Language Models: Enhancements and Comparative Analyses,https://arxiv.org/abs/2408.06396,,,,7000000000.0,7B,,,UniRef50,,16384000.0,"42,000 protein sequences Ã— 512 tokens/sequence = 21,504,000 total tokens (2.15 Ã— 10â· datapoints)

Training was not done on all data: 
""The training configuration
utilized a sequence length of 512, with a maximum
training step limit of 2000 and a batch size of 1,
coupled with a gradient accumulation step size of
16 for enhanced training efficiency.""

2000*512*16=16384000

Epochs: (2000*512*16)/21504000=0.76",,,NVIDIA RTX A6000,,Confident,"Pre-trained LLMs have demonstrated substantial capabilities across a range of conventional natural language processing (NLP) tasks, such as summarization and entity recognition. In this paper, we explore the application of LLMs in the generation of high-quality protein sequences. Specifically, we adopt a suite of pre-trained LLMs, including Mistral-7B1, Llama-2-7B2, Llama-3-8B3, and gemma-7B4, to produce valid protein sequences. All of these models are publicly available.5 Unlike previous work in this field, our approach utilizes a relatively small dataset comprising 42,000 distinct human protein sequences. We retrain these models to process protein-related data, ensuring the generation of biologically feasible protein structures. Our findings demonstrate that even with limited data, the adapted models exhibit efficiency comparable to established protein-focused models such as ProGen varieties, ProtGPT2, and ProLLaMA, which were trained on millions of protein sequences. To validate and quantify the performance of our models, we conduct comparative analyses employing standard metrics such as pLDDT, RMSD, TM-score, and REU. Furthermore, we commit to making the trained versions of all four models publicly available, fostering greater transparency and collaboration in the field of computational biology.",0.76,,Open weights (unrestricted),Italy,Mistral 7B,688128000000000000,6 FLOP / token / parameter * 7*10^9 parameters * 16384000 tokens [see dataset size notes] = 6.88128e+17 FLOP,4.0,,2025-05-01 10:42,,,,,,Academia,,,,,,"https://huggingface.co/Kamyar-zeinalipour/P-Mistral-7B
no specific license",Academia,,,,2365.97990493421,Operation counting,Kamyar-zeinalipour,,,
P-Llama2,Biology,"Protein generation,Protein design",University of Siena,"Kamyar Zeinalipour, Neda Jamshidi, Monica Bianchini, Marco Maggini, Marco Gori",2024-08-12,Design Proteins Using Large Language Models: Enhancements and Comparative Analyses,https://arxiv.org/abs/2408.06396,,,,7000000000.0,7B,,"base model compute: 8.4e+22 FLOP
fine-tune compute: 6.88128e+17 FLOP",UniRef50,,16384000.0,"42,000 protein sequences Ã— 512 tokens/sequence = 21,504,000 total tokens (2.15 Ã— 10â· datapoints)

Training was not done on all data: 
""The training configuration
utilized a sequence length of 512, with a maximum
training step limit of 2000 and a batch size of 1,
coupled with a gradient accumulation step size of
16 for enhanced training efficiency.""

2000*512*16=16384000

Epochs: (2000*512*16)/21504000=0.76",,,NVIDIA RTX A6000,,Confident,"Pre-trained LLMs have demonstrated substantial capabilities across a range of conventional natural language processing (NLP) tasks, such as summarization and entity recognition. In this paper, we explore the application of LLMs in the generation of high-quality protein sequences. Specifically, we adopt a suite of pre-trained LLMs, including Mistral-7B1, Llama-2-7B2, Llama-3-8B3, and gemma-7B4, to produce valid protein sequences. All of these models are publicly available.5 Unlike previous work in this field, our approach utilizes a relatively small dataset comprising 42,000 distinct human protein sequences. We retrain these models to process protein-related data, ensuring the generation of biologically feasible protein structures. Our findings demonstrate that even with limited data, the adapted models exhibit efficiency comparable to established protein-focused models such as ProGen varieties, ProtGPT2, and ProLLaMA, which were trained on millions of protein sequences. To validate and quantify the performance of our models, we conduct comparative analyses employing standard metrics such as pLDDT, RMSD, TM-score, and REU. Furthermore, we commit to making the trained versions of all four models publicly available, fostering greater transparency and collaboration in the field of computational biology.",0.76,,Open weights (restricted use),Italy,Llama 2-7B,688128000000000000,6 FLOP / token / parameter * 7*10^9 parameters * 16384000 tokens [see dataset size notes] = 6.88128e+17 FLOP,4.0,,2025-05-01 10:42,,,,,,Academia,,,,,,"https://huggingface.co/Kamyar-zeinalipour/P-Llama2-7B
no specific license, but they have to at least comply with Llama license

https://github.com/KamyarZeinalipour/protein-design-LLMs
just inference code",Academia,,,BF16,2365.97990493421,Operation counting,Kamyar-zeinalipour,,,
P-LLama3,Biology,"Protein generation,Protein design",University of Siena,"Kamyar Zeinalipour, Neda Jamshidi, Monica Bianchini, Marco Maggini, Marco Gori",2024-08-12,Design Proteins Using Large Language Models: Enhancements and Comparative Analyses,https://arxiv.org/abs/2408.06396,,,,8000000000.0,8B,,"base model compute: 7.2e+23 FLOP
fine-tune compute: 7.86432e+17 FLOP",UniRef50,,16384000.0,"42,000 protein sequences Ã— 512 tokens/sequence = 21,504,000 total tokens (2.15 Ã— 10â· datapoints)

Training was not done on all data: 
""The training configuration
utilized a sequence length of 512, with a maximum
training step limit of 2000 and a batch size of 1,
coupled with a gradient accumulation step size of
16 for enhanced training efficiency.""

2000*512*16=16384000

Epochs: (2000*512*16)/21504000=0.76",,,NVIDIA RTX A6000,,Confident,"Pre-trained LLMs have demonstrated substantial capabilities across a range of conventional natural language processing (NLP) tasks, such as summarization and entity recognition. In this paper, we explore the application of LLMs in the generation of high-quality protein sequences. Specifically, we adopt a suite of pre-trained LLMs, including Mistral-7B1, Llama-2-7B2, Llama-3-8B3, and gemma-7B4, to produce valid protein sequences. All of these models are publicly available.5 Unlike previous work in this field, our approach utilizes a relatively small dataset comprising 42,000 distinct human protein sequences. We retrain these models to process protein-related data, ensuring the generation of biologically feasible protein structures. Our findings demonstrate that even with limited data, the adapted models exhibit efficiency comparable to established protein-focused models such as ProGen varieties, ProtGPT2, and ProLLaMA, which were trained on millions of protein sequences. To validate and quantify the performance of our models, we conduct comparative analyses employing standard metrics such as pLDDT, RMSD, TM-score, and REU. Furthermore, we commit to making the trained versions of all four models publicly available, fostering greater transparency and collaboration in the field of computational biology.",0.76,,Open weights (restricted use),Italy,Llama 3-8B,786432000000000000,6 FLOP / token / parameter * 8*10^9 parameters * 16384000 tokens [see dataset size notes] = 7.86432e+17 FLOP,4.0,,2025-05-01 10:42,,,,,,Academia,,,,,,"https://huggingface.co/Kamyar-zeinalipour/P-Llama3-8B
no specific license, but they have to at least comply with Llama license

https://github.com/KamyarZeinalipour/protein-design-LLMs
just inference code",Academia,,,,2365.97990493421,Operation counting,Kamyar-zeinalipour,,,
P-gemma,Biology,"Protein generation,Protein design",University of Siena,"Kamyar Zeinalipour, Neda Jamshidi, Monica Bianchini, Marco Maggini, Marco Gori",2024-08-12,Design Proteins Using Large Language Models: Enhancements and Comparative Analyses,https://arxiv.org/abs/2408.06396,,,,7000000000.0,7B,,"base model compute: 3.07e+23 FLOP
fine-tune compute: 6.88128e+17 FLOP",UniRef50,,16384000.0,"42,000 protein sequences Ã— 512 tokens/sequence = 21,504,000 total tokens (2.15 Ã— 10â· datapoints)

Training was not done on all data: 
""The training configuration
utilized a sequence length of 512, with a maximum
training step limit of 2000 and a batch size of 1,
coupled with a gradient accumulation step size of
16 for enhanced training efficiency.""

2000*512*16=16384000

Epochs: (2000*512*16)/21504000=0.76",,,NVIDIA RTX A6000,,Confident,"Pre-trained LLMs have demonstrated substantial capabilities across a range of conventional natural language processing (NLP) tasks, such as summarization and entity recognition. In this paper, we explore the application of LLMs in the generation of high-quality protein sequences. Specifically, we adopt a suite of pre-trained LLMs, including Mistral-7B1, Llama-2-7B2, Llama-3-8B3, and gemma-7B4, to produce valid protein sequences. All of these models are publicly available.5 Unlike previous work in this field, our approach utilizes a relatively small dataset comprising 42,000 distinct human protein sequences. We retrain these models to process protein-related data, ensuring the generation of biologically feasible protein structures. Our findings demonstrate that even with limited data, the adapted models exhibit efficiency comparable to established protein-focused models such as ProGen varieties, ProtGPT2, and ProLLaMA, which were trained on millions of protein sequences. To validate and quantify the performance of our models, we conduct comparative analyses employing standard metrics such as pLDDT, RMSD, TM-score, and REU. Furthermore, we commit to making the trained versions of all four models publicly available, fostering greater transparency and collaboration in the field of computational biology.",0.76,,Open weights (unrestricted),Italy,Gemma 7B,688128000000000000,6 FLOP / token / parameter * 7*10^9 parameters * 16384000 tokens [see dataset size notes] = 6.88128e+17 FLOP,4.0,,2025-05-01 10:42,,,,,,Academia,,,,,,"https://huggingface.co/Kamyar-zeinalipour/P-gemma-7B
no specific license",Academia,,,,2365.97990493421,Operation counting,Kamyar-zeinalipour,,,
3-ensemble of Self-ensembles on CIFAR-100,Vision,Image classification,Google DeepMind,"Stanislav Fort, Balaji Lakshminarayanan",2024-08-08,Ensemble everything everywhere: Multi-scale aggregation for adversarial robustness,https://www.arxiv.org/abs/2408.05446,,,,60200000.0,as in resnet-152,,,CIFAR-100,,204000000.0,"""We focused on the CIFAR-* datasets (Krizhevsky, 2009; Krizhevsky et al.) that comprise 50,000 32 Ã— 32 Ã— 3 images. We arbitrarily chose ð‘ = 4 and the resolutions we used are 32 Ã— 32, 16 Ã— 16, 8 Ã— 8, 4 Ã— 4 (see Figure 3).""

Assuming (!) each image (32Ã—32+16Ã—16+8Ã—8+4Ã—4)Ã—3 = 4080 tokens
then the entire dataset was 50,000*4080 = 204000000 tokens",,,NVIDIA A100,,Likely,"Adversarial examples pose a significant challenge to the robustness, reliability and alignment of deep neural networks. We propose a novel, easy-to-use approach to achieving high-quality representations that lead to adversarial robustness through the use of multi-resolution input representations and dynamic self-ensembling of intermediate layer predictions. We demonstrate that intermediate layer predictions exhibit inherent robustness to adversarial attacks crafted to fool the full classifier, and propose a robust aggregation mechanism based on Vickrey auction that we call \textit{CrossMax} to dynamically ensemble them. By combining multi-resolution inputs and robust ensembling, we achieve significant adversarial robustness on CIFAR-10 and CIFAR-100 datasets without any adversarial training or extra data, reaching an adversarial accuracy of â‰ˆ72% (CIFAR-10) and â‰ˆ48% (CIFAR-100) on the RobustBench AutoAttack suite (Lâˆž=8/255) with a finetuned ImageNet-pretrained ResNet152. This represents a result comparable with the top three models on CIFAR-10 and a +5 % gain compared to the best current dedicated approach on CIFAR-100. Adding simple adversarial training on top, we get â‰ˆ78% on CIFAR-10 and â‰ˆ51% on CIFAR-100, improving SOTA by 5 % and 9 % respectively and seeing greater gains on the harder dataset. We validate our approach through extensive experiments and provide insights into the interplay between adversarial robustness, and the hierarchical nature of deep representations. We show that simple gradient-based attacks against our model lead to human-interpretable images of the target classes as well as interpretable image changes. As a byproduct, using our multi-resolution prior, we turn pre-trained classifiers and CLIP models into controllable image generators and develop successful transferable attacks on large vision language models.",13.0,,Unreleased,"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France",ResNet-152 (ImageNet),957902400000000000,"6ND = 6 FLOP / token / parameter * 60200000 parameters* 204000000 tokens * 13 epochs = 9.579024e+17 FLOP

""On an A100, the Colab runs for 20 min for the backbone training (6 epochs) but more epochs and especially lower learning rate training later helps. ""

312000000000000 FLOP / GPU / sec [bf16 assumed] * 1 GPU * 20 minutes *60 sec / minute * 0.3 [assumed utilization] = 1.1232e+17 FLOP (an underestimation since the training took more epochs)",,,2025-06-16 15:31,,,,,,Industry,,,,,Open source,"MIT license
https://github.com/stanislavfort/ensemble-everything-everywhere",Industry,,,,,"Operation counting,Hardware",,,,
EXAONE 3.0,Language,"Language modeling/generation,Code generation,Question answering",LG AI Research,"LG AI Research: Soyoung An, Kyunghoon Bae, Eunbi Choi, Stanley Jungkyu Choi, Yemuk Choi, Seokhee Hong, Yeonjung Hong, Junwon Hwang, Hyojin Jeon, Gerrard Jeongwon Jo, Hyunjik Jo, Jiyeon Jung, Yountae Jung, Euisoon Kim, Hyosang Kim, Joonkee Kim, Seonghwan Kim, Soyeon Kim, Sunkyoung Kim, Yireun Kim, Youchul Kim, Edward Hwayoung Lee, Haeju Lee, Honglak Lee, Jinsik Lee, Kyungmin Lee, Moontae Lee, Seungjun Lee, Woohyung Lim, Sangha Park, Sooyoun Park, Yongmin Park, Boseong Seo, Sihoon Yang, Heuiyeen Yeen, Kyungjae Yoo, Hyeongu Yun",2024-08-07,EXAONE 3.0 7.8B Instruction Tuned Language Model,https://arxiv.org/abs/2408.03541,,,,7820000000.0,7.8B,4e+23,"6ND = 6*7.8B parameters *8T tokens  = 3.744e+23 FLOP

""EXAONE language models were trained using Google Cloud Platform and a cluster powered by NVIDIA H100 GPUs and NVIDIA NeMo Framework. Then, they were optimized by NVIDIA TensorRT-LLM. The total amount of computation used for model training was about 4 Ã— 1023 FLOPS""",Unspecified unreleased,"8T training data (tokens)

the token per word ration is 2.46 and given in the paper",8000000000000.0,,,,NVIDIA H100 SXM5 80GB,,Confident,"We introduce EXAONE 3.0 instruction-tuned language model, the first open model in the family of Large Language Models (LLMs) developed by LG AI Research. Among different model sizes, we publicly release the 7.8B instruction-tuned model to promote open research and innovations. Through extensive evaluations across a wide range of public and in-house benchmarks, EXAONE 3.0 demonstrates highly competitive real-world performance with instruction-following capability against other state-of-the-art open models of similar size. Our comparative analysis shows that EXAONE 3.0 excels particularly in Korean, while achieving compelling performance across general tasks and complex reasoning. With its strong real-world effectiveness and bilingual proficiency, we hope that EXAONE keeps contributing to advancements in Expert AI. Our EXAONE 3.0 instruction-tuned model is available at this https URL",,,Open weights (non-commercial),Korea (Republic of),,,,,,2025-05-26 19:18,,,,,,Industry,,,,,Unreleased,"https://huggingface.co/LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct

Exaone license (allows only non-commercial usage)",Industry,,,,,"Reported,Operation counting",LGAI-EXAONE,,,
Table Tennis Agent,Robotics,Sports,Google DeepMind,"David B. D'Ambrosio, Saminda Abeyruwan, Laura Graesser, Atil Iscen, Heni Ben Amor, Alex Bewley, Barney J. Reed, Krista Reymann, Leila Takayama, Yuval Tassa, Krzysztof Choromanski, Erwin Coumans, Deepali Jain, Navdeep Jaitly, Natasha Jaques, Satoshi Kataoka, Yuheng Kuang, Nevena Lazic, Reza Mahjourian, Sherry Moore, Kenneth Oslund, Anish Shankar, Vikas Sindhwani, Vincent Vanhoucke, Grace Vesom, Peng Xu, and Pannag R. Sanketi",2024-08-07,Achieving Human Level Competitive Robot Table Tennis,https://deepmind.google/research/publications/107741/,,SOTA improvement,"""first learned robot agent that reaches amateur human-level performance in competitive table tennis""",185000.0,"17 low level controllers with 10k parameters each: 
""Each policy is a dilated-gated CNN
[22] following the architecture in [23] with 10k parameters... The final
system contained 17 LLCs""

One high-level controller with 4.5k parameters: ""The style policy architecture, similar to the LLC but with
only 4.5k parameters, has a (8, 128) observation space""

spin classifier that is a 2-layer MLP of hidden sizes (128, 64) and input size 18, which is 10k parameters per o1 and Claude.

So ~185k parameters total",,unclear,,"Trained in simulation via reinforcement learning with feedback from real world

""This iterative cycle of training models in simulation on
the latest dataset, evaluating it in the real world, and using
the annotated evaluation data to extend the dataset, can be
repeated as many times as needed. We completed 7 cycles
for rally balls and 2 cycles for serving balls over the course
of 3 months with over 50 different human opponents, leading
to a final dataset size of 14.2k initial ball states for rallies
and 3.4k for serves. A summary of the dataset evolution is
presented in Table I and Figure 6.""",,"~18k ball states

""This iterative cycle of training models in simulation on
the latest dataset, evaluating it in the real world, and using
the annotated evaluation data to extend the dataset, can be
repeated as many times as needed. We completed 7 cycles
for rally balls and 2 cycles for serving balls over the course
of 3 months with over 50 different human opponents, leading
to a final dataset size of 14.2k initial ball states for rallies
and 3.4k for serves. A summary of the dataset evolution is
presented in Table I and Figure 6.""",,,,Reinforcement learning,Likely,"Achieving human-level speed and performance on real world tasks is a north star for the robotics research community. This work takes a step towards that goal and presents the first learned robot agent that reaches amateur human-level performance in competitive table tennis. Table tennis is a physically demanding sport which requires human players to undergo years of training to achieve an advanced level of proficiency. In this paper, we contribute (1) a hierarchical and modular policy architecture consisting of (i) low level controllers with their detailed skill descriptors which model the agent's capabilities and help to bridge the sim-to-real gap and (ii) a high level controller that chooses the low level skills, (2) techniques for enabling zero-shot sim-to-real including an iterative approach to defining the task distribution that is grounded in the real-world and defines an automatic curriculum, and (3) real time adaptation to unseen opponents. Policy performance was assessed through 29 robot vs. human matches of which the robot won 45% (13/29). All humans were unseen players and their skill level varied from beginner to tournament level. Whilst the robot lost all matches vs. the most advanced players it won 100% matches vs. beginners and 55% matches vs. intermediate players, demonstrating solidly amateur human-level performance. Videos of the matches can be viewed at this https URL",,,Unreleased,"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France",,,,,,2025-05-09 11:32,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
MiniCPM-V 2.6,"Vision,Language,Video","Visual question answering,Language modeling/generation,Image captioning,Character recognition,Video description",OpenBMB (Open Lab for Big Model Base),,2024-08-06,"A GPT-4V Level MLLM for Single Image, Multi Image and Video on Your Phone",https://huggingface.co/openbmb/MiniCPM-V-2_6,,,,8000000000.0,8b,,,,,,"from MiniCPM-Llama3-V (previous model from the series) paper (https://arxiv.org/pdf/2408.01800)

Stage 1. 224*224 resolution. ""200M data from the Image Captioning data in Table 1.""

Stage-2. 448*448 resolution. ""we additionally select 200M data from the Image Captioning data in Table 1.""

Stage 3. ""Different from the previous stages with only image captioning data, during the highresolution pre-training stage, we additionally introduce OCR data to enhance the visual encodersâ€™
OCR capability""",,,,,Confident,"The latest and most capable model in the MiniCPM-V series. With a total of 8B parameters, the model surpasses GPT-4V in single image, multi-image and video understanding. It outperforms GPT-4o mini, Gemini 1.5 Pro and Claude 3.5 Sonnet in single image understanding, and advances MiniCPM-Llama3-V 2.5's features such as strong OCR capability, trustworthy behavior, multilingual support, and end-side deployment. Due to its superior token density, MiniCPM-V 2.6 can for the first time support real-time video understanding on end-side devices such as iPad.",,,Open weights (restricted use),China,,,,,,2025-05-01 10:42,,,,,,,,,,,Unreleased,"https://huggingface.co/openbmb/MiniCPM-V-2_6
""The models and weights of MiniCPM are completely free for academic research. After filling out a ""questionnaire"" for registration, MiniCPM-V 2.6 weights are also available for free commercial use.""",,,,,,,,,,
LLaVA-OV-7B,"Multimodal,Video,Vision","Image captioning,Visual question answering,Video description,Object recognition,Action recognition","ByteDance,Nanyang Technological University,Chinese University of Hong Kong (CUHK),Hong Kong University of Science and Technology (HKUST)","Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, Chunyuan Li",2024-08-06,"LLaVA-OneVision: Easy Visual Task Transfer
",https://arxiv.org/abs/2408.03326,514.0,,,7600000000.0,,,,LLaVA-OneVision,dataset here: https://huggingface.co/datasets/lmms-lab/LLaVA-OneVision-Data,4830000000.0,"OneVision-1.6M comprises 1.6 million samples - single-image, multi-image, video. While the exact total number of tokens isn't explicitly stated in the available documentation, we can estimate the token count based on the dataset's composition and tokenization strategies:

Single-Image: ~412M
Multi-Image: ~3.08B
Video: ~1.34B
Total â‰ˆ 4.83 billion tokens",, 256 * Nvidia Tesla A100,,,Confident,"We present LLaVA-OneVision, a family of open large multimodal models (LMMs) developed by consolidating our insights into data, models, and visual representations in the LLaVA-NeXT blog series. Our experimental results demonstrate that LLaVA-OneVision is the first single model that can simultaneously push the performance boundaries of open LMMs in three important computer vision scenarios: single-image, multi-image, and video scenarios. Importantly, the design of LLaVAOneVision allows strong transfer learning across different modalities/scenarios, yielding new emerging capabilities. In particular, strong video understanding and cross-scenario capabilities are demonstrated through task transfer from images to videos.",1.0,,Open weights (unrestricted),"China,Singapore,Hong Kong,China,Hong Kong,China",Qwen2-7B,,,,,2025-06-12 14:17,,,,256.0,"We use a global batch size of 512 for the 0.5B model, and 256 for the 7B and 72B models.","Industry,Academia,Academia,Academia",,,,,Open source,"Apache 2.0 license for weights:
https://huggingface.co/lmms-lab/llava-onevision-qwen2-7b-ov-chat

Apache 2.0 for inference and training code:
https://github.com/LLaVA-VL/LLaVA-NeXT?tab=readme-ov-file

Dataset is also under Apache 2.0 ","Industry,Academia,Academia,Academia",,,BF16,,,lmms-lab,,,
LLaVA-OV-72B,"Multimodal,Vision","Image captioning,Visual question answering,Video description,Object recognition,Action recognition","ByteDance,Nanyang Technological University,Chinese University of Hong Kong (CUHK),Hong Kong University of Science and Technology (HKUST)","Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, Chunyuan Li",2024-08-06,"LLaVA-OneVision: Easy Visual Task Transfer
",https://arxiv.org/abs/2408.03326,514.0,SOTA improvement,"As shown in Table 4, LLaVA-OneVision (SI) consistently outperforms existing multi-image LMMs in all benchmarks. After additional tuning on multi-image and video data, LLaVA-OneVision shows a marked improvement over GPT-4V in specific areas, with significant margins. This highlights its strong performance in complex tasks such as multi-image reasoning, identifying differences, and understanding 3D environments",72000000000.0,,3.036551985824e+24,"FineTune: 6*72000000000*38314782000=1.655199e+22

Base model: 3.02e+24
Total: 3.036552e+24",LLaVA-OneVision,dataset here: https://huggingface.co/datasets/lmms-lab/LLaVA-OneVision-Data,38314782000.0,"Stage 1: 558000*729=406782000
Stage 1.5: 4M*729*5=14580000000
Stage 2-single: 3.2M*729*5=11664000000
Stage 2-one vision: 1.6M*729*10=11664000000
Total: 38314782000",,,,,Confident,"We present LLaVA-OneVision, a family of open large multimodal models (LMMs) developed by consolidating our insights into data, models, and visual representations in the LLaVA-NeXT blog series. Our experimental results demonstrate that LLaVA-OneVision is the first single model that can simultaneously push the performance boundaries of open LMMs in three important computer vision scenarios: single-image, multi-image, and video scenarios. Importantly, the design of LLaVAOneVision allows strong transfer learning across different modalities/scenarios, yielding new emerging capabilities. In particular, strong video understanding and cross-scenario capabilities are demonstrated through task transfer from images to
videos.",1.0,,Open weights (unrestricted),"China,Singapore,Hong Kong,China,Hong Kong,China",Qwen2-72B,,,,,2025-06-13 14:09,,,,256.0,"We use a global batch size of 512 for the 0.5B model, and 256 for the 7B and 72B models.","Industry,Academia,Academia,Academia",,,,,Open source,"Apache 2

https://huggingface.co/lmms-lab/llava-onevision-qwen2-72b-ov-sft

https://github.com/LLaVA-VL/LLaVA-NeXT","Industry,Academia,Academia,Academia",,,BF16,,,lmms-lab,,,
CXR Foundation,"Vision,Medicine","Image embedding,Image classification",Google,,2024-08-02,CXR Foundation model card,https://developers.google.com/health-ai-developer-foundations/cxr-foundation,,,,,"""The model uses the EfficientNet-L2 architecture and BERT architecture""",,,"MIMIC-CXR,Unspecified unreleased","""It was trained on 821,544 CXRs from India and the US using abnormal vs. normal labels, i.e. the image contained any kind of abnormality, and the Supervised Contrastive loss as well as accompanying radiology reports and the CLIP loss and BLIP-2 losses. ""

""CXR Foundation was trained using the following de-identified datasets:

MIMIC-CXR, comprising of 243,324 images of 60,523 unique patients (cited below);
A private US dataset from an AMC in Illinois comprising of 165,182 images of 12,988 unique patients; and
A private Indian dataset from five hospitals comprising of 485,082 patients of 348,335 unique patients""",,,,,,,Unknown,"CXR Foundation is a machine learning (ML) model that produces embeddings based on images of chest X-rays. The embeddings can be used to efficiently build AI models for chest X-ray related tasks, requiring less data and less compute than having to fully train a model without the embeddings or the pretrained model.

The model has been optimized for chest X-rays, but researchers have reported success using it for other types of X-rays, including X-rays of other body parts and even veterinary X-rays.

Trained on large scale datasets, CXR Foundation helps businesses and institutions in healthcare and life sciences do more with their chest X-ray data with less data, accelerating their ability to build AI models for chest X-ray image analysis.",,,Open weights (restricted use),United States of America,EfficientNet-L2,,,,,2025-05-12 13:56,,,,,,Industry,,,,,,"https://huggingface.co/google/cxr-foundation
not for clinical use",Industry,,,,,,google,,,
Flux.1 [pro],Image generation,Image generation,Black Forest Labs,"Andreas Blattmann, Axel Sauer, Dominik Lorenz, Dustin Podell, Frederic Boesel, Harry Saini, Jonas MÃ¼ller, Kyle Lacey, Patrick Esser, Robin Rombach, Sumith Kulal, Tim Dockhorn, Yam Levi, Zion English",2024-08-01,Flux.1,https://blackforestlabs.ai/announcing-black-forest-labs/ ,,,Plausibly significant usage.,12000000000.0,"""All public FLUX.1 models are based on a hybrid architecture of multimodal and parallel diffusion transformer blocks and scaled to 12B parameters.""",,,Unspecified unreleased,,,,,,,,Confident,,,,API access,Germany,,,,,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
Flux.1 [dev],Image generation,Image generation,Black Forest Labs,"Andreas Blattmann, Axel Sauer, Dominik Lorenz, Dustin Podell, Frederic Boesel, Harry Saini, Jonas MÃ¼ller, Kyle Lacey, Patrick Esser, Robin Rombach, Sumith Kulal, Tim Dockhorn, Yam Levi, Zion English",2024-08-01,Flux.1,https://blackforestlabs.ai/announcing-black-forest-labs/ ,,,"Plausibly significant usage.

Currently the image generator used in the X Grok integra",12000000000.0,"12 billion parameters, guidance-distilled from Flux.1 [pro]",,"No details given, but this is guidance-distilled from Flux.1 [pro]. See https://arxiv.org/pdf/2210.03142 for details on guidance distillation.",Unspecified unreleased,,,,,,,,Confident,,,,Open weights (non-commercial),Germany,,,,,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
OmniParser: Interactable Region Detection Model,Vision,Object detection,Microsoft Research,"Yadong Lu, Jianwei Yang, Yelong Shen, Ahmed Awadallah",2024-08-01,OmniParser for Pure Vision Based GUI Agent,https://arxiv.org/abs/2408.00203,,,,,,,,,"""Training Datasets include: 1) an interactable icon detection dataset, which was curated from popular web pages and automatically annotated to highlight clickable and actionable regions, and 2) an icon description dataset, designed to associate each UI element with its corresponding function.""",63641.0,"""We collect in total of 66990 samples where we split 95% (63641) for training, and 5% (3349) for validation. We train for 20 epochs with batch size of 256, learning rate of 1eâˆ’3, and the Adam
optimizer on 4 GPUs.""",,,,,Confident,"The recent success of large vision language models shows great potential in driving the agent system operating on user interfaces. However, we argue that the power multimodal models like GPT-4V as a general agent on multiple operating systems across different applications is largely underestimated due to the lack of a robust screen parsing technique capable of: 1) reliably identifying interactable icons within the user interface, and 2) understanding the semantics of various elements in a screenshot and accurately associate the intended action with the corresponding region on the screen. To fill these gaps, we introduce \textsc{OmniParser}, a comprehensive method for parsing user interface screenshots into structured elements, which significantly enhances the ability of GPT-4V to generate actions that can be accurately grounded in the corresponding regions of the interface. We first curated an interactable icon detection dataset using popular webpages and an icon description dataset. These datasets were utilized to fine-tune specialized models: a detection model to parse interactable regions on the screen and a caption model to extract the functional semantics of the detected elements. \textsc{OmniParser} significantly improves GPT-4V's performance on ScreenSpot benchmark. And on Mind2Web and AITW benchmark, \textsc{OmniParser} with screenshot only input outperforms the GPT-4V baselines requiring additional information outside of screenshot.",20.0,,Open weights (unrestricted),"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational",YOLOv8x,,,4.0,,2025-05-01 10:42,,,,,,Industry,,,,,,"https://github.com/microsoft/OmniParser
CC-BY-4.0

https://huggingface.co/microsoft/OmniParser
""icon_detect model is under AGPL license""",Industry,,,,,,,,,
OmniParser: Icon Description Model,Vision,Image captioning,Microsoft Research,"Yadong Lu, Jianwei Yang, Yelong Shen, Ahmed Awadallah",2024-08-01,OmniParser for Pure Vision Based GUI Agent,https://arxiv.org/abs/2408.00203,,,,,,,,,,7185.0,"""We finetune BLIP-2 model for 1 epoch on the generated dataset with constant learning rate of 1eâˆ’5 ,no weight decay and Adam optimizer. ""

""We we curate a dataset of 7k icon-description pairs using GPT-4o, and finetune a BLIP-v2 model [LLSH23] on this dataset""

""we collected 7185 icon-description pairs for finetuning""",,,,,Confident,"The recent success of large vision language models shows great potential in driving the agent system operating on user interfaces. However, we argue that the power multimodal models like GPT-4V as a general agent on multiple operating systems across different applications is largely underestimated due to the lack of a robust screen parsing technique capable of: 1) reliably identifying interactable icons within the user interface, and 2) understanding the semantics of various elements in a screenshot and accurately associate the intended action with the corresponding region on the screen. To fill these gaps, we introduce \textsc{OmniParser}, a comprehensive method for parsing user interface screenshots into structured elements, which significantly enhances the ability of GPT-4V to generate actions that can be accurately grounded in the corresponding regions of the interface. We first curated an interactable icon detection dataset using popular webpages and an icon description dataset. These datasets were utilized to fine-tune specialized models: a detection model to parse interactable regions on the screen and a caption model to extract the functional semantics of the detected elements. \textsc{OmniParser} significantly improves GPT-4V's performance on ScreenSpot benchmark. And on Mind2Web and AITW benchmark, \textsc{OmniParser} with screenshot only input outperforms the GPT-4V baselines requiring additional information outside of screenshot.",1.0,,Open weights (unrestricted),"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational",BLIP-2 (Q-Former),,,,,2025-05-01 10:42,,,,,,Industry,,,,,,"https://github.com/microsoft/OmniParser
CC-BY-4.0

https://huggingface.co/microsoft/OmniParser
""icon_caption_blip2 & icon_caption_florence is under MIT license. """,Industry,,,,,,,,,
BaiLing TTS,Speech,Text-to-speech,Ant Group,"Xinhan Di, Zihao Chen, Yunming Liang, Junjie Zheng, Yihua Wang, Chaofan Ding",2024-08-01,Bailing-TTS: Chinese Dialectal Speech Synthesis Towards Human-like Spontaneous Representation,https://arxiv.org/abs/2408.00284v1,,,,,,,,Unspecified unreleased,"""A custom dataset containing high-quality Mandarin and multiple Chinese dialects data is used for
the training, inference and evaluation. This dataset contains 200k hours of labeled speech data.
The annotation contains fine-grained labels between text and speech modalities. In addition, this
large-scale dataset contains high-quality speech-text pairs. Furthermore, this custom Chinese dialect dataset is used for the pre-training stage of the continual semi-supervised learning""",,,,,NVIDIA A100,,Unknown,"Large-scale text-to-speech (TTS) models have made significant progress this http URL, they still fall short in the generation of Chinese dialectal speech. Toaddress this, we propose Bailing-TTS, a family of large-scale TTS models capable of generating high-quality Chinese dialectal speech. Bailing-TTS serves as a foundation model for Chinese dialectal speech generation. First, continual semi-supervised learning is proposed to facilitate the alignment of text tokens and speech tokens. Second, the Chinese dialectal representation learning is developed using a specific transformer architecture and multi-stage training processes. With the proposed design of novel network architecture and corresponding strategy, Bailing-TTS is able to generate Chinese dialectal speech from text effectively and efficiently. Experiments demonstrate that Bailing-TTS generates Chinese dialectal speech towards human-like spontaneous representation. Readers are encouraged to listen to demos at \url{this https URL}.",,,Unreleased,China,,,,,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
EPInformer,Biology,Gene expression profile generation,"The University of Hong Kong,Harvard Medical School","Jiecong Lin, Ruibang Luo, Luca Pinello",2024-08-01,EPInformer: a scalable deep learning framework for gene expression prediction by integrating promoter-enhancer sequences with multimodal epigenomic data,https://www.biorxiv.org/content/10.1101/2024.08.01.606099v1,2.0,,,447149.0,,3.3696e+17,1*60*60*312000000000000*0.3=3.3696e+17,,,,,1.0,,NVIDIA A100,,Confident,"Transcriptional regulation, critical for cellular differentiation and adaptation to environmental changes, involves coordinated interactions among DNA sequences, regulatory proteins, and chromatin architecture. Despite extensive data from consortia like ENCODE, understanding the dynamics of cis-regulatory elements (CREs) in gene expression remains challenging. Deep learning is a powerful tool for learning gene expression and epigenomic signals from DNA sequences, exhibiting superior performance compared to conventional machine learning approaches. However, even the most advanced deep learning-based methods may fall short in capturing the regulatory effects of distal elements such as enhancers, limiting their predictive accuracy. In addition, these methods may require significant resources to train or to adapt to newly generated data. To address these challenges, we present EPInformer, a scalable deep-learning framework for predicting gene expression by integrating promoter-enhancer interactions with their sequences, epigenomic signals, and chromatin contacts. Our model outperforms existing gene expression prediction models in rigorous cross-chromosome validation, accurately recapitulates enhancer-gene interactions validated by CRISPR perturbation experiments, and identifies crucial transcription factor motifs within regulatory sequences. EPInformer is available as open-source software at https://github.com/pinellolab/EPInformer.",,,,"Hong Kong,China,United States of America",,,,1.0,,2025-05-01 10:42,,,,,,"Academia,Academia",,,,,,,"Academia,Academia",,,,433.4358156889826,,,,,
InternLM2.5,Language,"Language modeling/generation,Question answering",Shanghai AI Lab,,2024-08-01,InternLM2.5-20B Model Card,https://github.com/InternLM/InternLM/blob/main/model_cards/internlm2.5_20b.md,,,,20000000000.0,20b,,,,,,,,,,,Confident,"InternLM2.5, the 2.5th generation InternLM, has open-sourced a 20 billion parameter base model and a chat model tailored for practical scenarios. For the convenience of users and researchers, we have open-sourced two versions of each scale of the model, which are:

InternLM2.5-20B: Further pretrain with general domain data and domain-enhanced corpus, obtaining state-of-the-art performance in evaluation with good language capability. InternLM2.5 models are recommended for consideration in most applications.
InternLM2.5-chat-20B: Further aligned on top of InternLM2.5 through supervised fine-tuning (SFT) and online RLHF. InternLM2.5-Chat exhibits better instruction following, chat experience, and function calling, which is recommended for downstream applications.
The model has the following characteristics:

Outstanding reasoning capability: State-of-the-art performance on Math reasoning, surpassing models like Llama3 and Gemma2-27B.

Stronger tool use: InternLM2.5 supports gathering information from more than 100 web pages, corresponding implementation has be released in MindSearch. InternLM2.5 has better tool utilization-related capabilities in instruction following, tool selection and reflection. See examples.",,,Open weights (restricted use),China,,,,,,2025-05-01 10:42,,,,,,Academia,,,,,,"https://huggingface.co/internlm/internlm2_5-20b

he code is licensed under Apache-2.0, while model weights are fully open for academic research and also allow free commercial usage. To apply for a commercial license, please fill in the application form (English)/ç”³è¯·è¡¨ï¼ˆä¸­æ–‡ï¼‰.",Academia,,,,,,internlm,,,
Midjourney V6.1,Image generation,Image generation,Midjourney,,2024-07-30,,https://updates.midjourney.com/version-6-1/,,,,,,,,Unspecified unreleased,,,,,,,,Unknown,"More coherent images (arms, legs, hands, bodies, plants, animals, etc)
Much better image quality (reduced pixel artifacts, enhanced textures, skin, etc)
More precise, detailed, and correct small image features (eyes, small faces, far away hands, etc)
New 2x upscalers with much better image / texture quality
Roughly 25% faster for standard image jobs
Improved text accuracy (when drawing words via â€œquotationsâ€ in prompts)
A new personalization model with improved nuance, surprise, and accuracy
Personalization code versioning (use any personalization code from old jobs to use the personalization model and data from that job)
A new --q 2 mode which takes 25% longer to (sometimes) add more texture at the cost of reduced image coherence
Things should look â€œgenerally more beautifulâ€ across the board",,,Hosted access (no API),United States of America,,,,,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
AFM-on-device,Language,Language modeling/generation,Apple,"Andy Narayanan, Aonan Zhang, Bowen Zhang, Chen Chen, Chong Wang, Chung-Cheng Chiu, David Qiu, Deepak Gopinath, Dian Ang Yap, Dong Yin, Feng Nan, Floris Weers, Guoli Yin, Haoshuo Huang, Jianyu Wang, Jiarui Lu, John Peebles, Ke Ye, Mark Lee, Nan Du, Qibin Chen, Quentin Keunebroek, Ruoming Pang, Sam Wiseman, Syd Evans, Tao Lei, Tom Gunter, Vivek Rathod, Xiang Kong, Xianzhi Du, Yanghao Li, Yongqiang Wang, Yuan Gao, Zaid Ahmed, Zhaoyang Xu, Zhiyun Lu, Zirui Wang, Al Rashid, Albin Madappally Jose, Alec Doane, Alfredo Bencomo, Allison Vanderby, Andrew Hansen, Ankur Jain, Anupama Mann Anupama, Areeba
Kamal, Bugu Wu, Carolina Brum, Charlie Maalouf, Chinguun Erdenebileg,
Chris Dulhanty, Dominik Moritz, Doug Kang, Eduardo Jimenez, Evan Ladd,
Fangping Shi, Felix Bai, Frank Chu, Fred Hohman, Hadas Kotek, Hannah
Gillis Coleman, Jane Li, Jeffrey Bigham, Jeffery Cao, Jeff Lai, Jessica Cheung, Jiulong Shan, Joe Zhou, John Li, Jun Qin, Karanjeet Singh, Karla Vega, Ke Ye, Kelvin Zou, Laura Heckman, Lauren Gardiner, Margit Bowler, Mark Lee, Maria Cordell, Meng Cao, Nicole Hay, Nilesh Shahdadpuri, Otto Godwin, Pranay Dighe, Pushyami Rachapudi, Ramsey Tantawi, Roman Frigg, Sam Davarnia, Sanskruti Shah, Saptarshi Guha, Sasha Sirovica, Shen Ma, Shuang Ma, Simon Wang, Sulgi Kim, Suma Jayaram, Vaishaal Shankar, Varsha Paidi, Vivek Kumar, Xiang Kong, Xin Wang, Xin Zheng, Walker Cheng, Yael Shrager, Yang Ye, Yasu Tanaka, Yihao Guo, Yunsong Meng, Zhao Tang Luo, Zhi Ouyang, Zhiyun Lu, Alp Aygar, Alvin Wan, Andrew Walkingshaw, Andy Narayanan, Antonie Lin, Arsalan Farooq, Brent Ramerth, Chong Wang, Colorado Reed, Chris Bartels, Chris Chaney, David Riazati, Eric Liang Yang, Erin Feldman, Gabriel Hochstrasser, Guillaume Seguin, Guoli Yin, Irina Belousova, Jianyu Wang, Joris Pelemans, Karen Yang, Keivan Alizadeh Vahid, Liangliang Cao, Mahyar Najibi , Marco Zuliani, Max Horton, Minsik Cho, Nikhil Bhendawade, Patrick Dong, Piotr Maj, Pulkit Agrawal, Qi Shan, Qibin Chen, Qichen Fu, Regan Poston, Sam Xu, Shuangning Liu, Sushma Rao, Tashweena Heeramun, Thomas Merth, Uday Rayala, Victor Cui, Vivek Rangarajan Sridhar, Vivek Rathod, Wencong Zhang, Wenqi Zhang, Wentao Wu, Xiang Kong, Xingyu Zhou, Xinwen Liu, Yang Zhao, Yin Xia, Zhile Ren, Zhongzheng Ren",2024-07-29,Apple Intelligence Foundation Language Models,https://machinelearning.apple.com/research/apple-intelligence-foundation-language-models,,Significant use,"Currently in beta access only, but will be integrated into millions or billions of iPhones.",2730000000.0,"Table 1, sum of non-embedding and embedding parameters",4.5126e+23,"Model was initialized from a pruned version of a 6.4B parameter model trained using the same recipe as AFM-server. Assuming ""same recipe"" involves training for the full 6.3T tokens, this implies 6 * 6.3T * 6.4B = 2.42e23 FLOP. 

The pruning masks are learned by training over 188B tokens, which suggests 6 * 188B * 6.4B = 7.22e21 FLOPs.

Pretraining is then run over 6.3T tokens; however, labels are a convex combination of true labels and the predicted labels from the unpruned 6.4B model. Since this involves running the 6.3T tokens forward through both the 6.4B and the 2.73B model, but only calculating gradients for the smaller model, FLOPs here are equal to (6 * 6.3T * 2.73B) + (2 * 6.3T * 6.4B) = 1.84e23. 

Finally, there is a 1T ""continuation"" pretraining stage without distillation loss, for 6 * 1T * 2.73B = 1.64e22 FLOP, and a 100B context-lengthening stage for another 6 * 100B * 2.73B = 1.64e21 FLOP

In total: 2.42e23 + 7.22e21 + 1.84e23 + 1.64e22 + 1.64e21 = 4.51e23 FLOP",,"188B of tokens are used to train a pruning mask to reduce a 6.4B model to the 2.73B used for AFM-on-device. Main pre-training data is 6.3T tokens of web text, code, and math, plus another 1T in the second pre-training stage and 100B in the third. See section 3.1 for details. Post-training details do not give details on dataset size.",7588000000000.0,"Not explicitly mentioned, but I assume the 7.588T tokens do not involve multiple epochs.",,"Trained on ""one slice of 2048 TPUv5p chips""; wall-time not given.",Google TPU v5p,Self-supervised learning,Confident,"We present foundation language models developed to power Apple Intelligence features, including a âˆ¼3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute [Apple, 2024b]. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",1.0,,Hosted access (no API),United States of America,,,,2048.0,0.52,2025-05-28 15:57,Google Cloud,Google us-east (either us-east1 or us-east5),,18949753.0,"Main pretraining uses sequence length of 4096 tokens; 4096 sequences per batch. During the ""continued"" pre-training stage, sequence length is upped to 8192 while batch size remains 4096. During context lengthening, sequence length is upped to 32768 while ""the recipe is similar to that used for continued pre-training"" implies same batch size of 4096.

Weighting batch sizes by number of tokens seen in each stage:

exp((6.3T * ln(4096 * 4096) + 1T * ln(8192 * 4096) + 100B * ln(32768 * 4096))/ 7.4T) = 18,949,753",Industry,,,,,Unreleased,,Industry,,"MFU is stated for AFM-server, assumed similar. If anything, this is likely a lower bound as training is done on a single slice of TPU chips compared to the 8 slices for AFM-server.",BF16,,Operation counting,,,,
AFM-server,Language,Language modeling/generation,Apple,"Andy Narayanan, Aonan Zhang, Bowen Zhang, Chen Chen, Chong Wang, Chung-Cheng Chiu, David Qiu, Deepak Gopinath, Dian Ang Yap, Dong Yin, Feng Nan, Floris Weers, Guoli Yin, Haoshuo Huang, Jianyu Wang, Jiarui Lu, John Peebles, Ke Ye, Mark Lee, Nan Du, Qibin Chen, Quentin Keunebroek, Ruoming Pang, Sam Wiseman, Syd Evans, Tao Lei, Tom Gunter, Vivek Rathod, Xiang Kong, Xianzhi Du, Yanghao Li, Yongqiang Wang, Yuan Gao, Zaid Ahmed, Zhaoyang Xu, Zhiyun Lu, Zirui Wang, Al Rashid, Albin Madappally Jose, Alec Doane, Alfredo Bencomo, Allison Vanderby, Andrew Hansen, Ankur Jain, Anupama Mann Anupama, Areeba
Kamal, Bugu Wu, Carolina Brum, Charlie Maalouf, Chinguun Erdenebileg,
Chris Dulhanty, Dominik Moritz, Doug Kang, Eduardo Jimenez, Evan Ladd,
Fangping Shi, Felix Bai, Frank Chu, Fred Hohman, Hadas Kotek, Hannah
Gillis Coleman, Jane Li, Jeffrey Bigham, Jeffery Cao, Jeff Lai, Jessica Cheung, Jiulong Shan, Joe Zhou, John Li, Jun Qin, Karanjeet Singh, Karla Vega, Ke Ye, Kelvin Zou, Laura Heckman, Lauren Gardiner, Margit Bowler, Mark Lee, Maria Cordell, Meng Cao, Nicole Hay, Nilesh Shahdadpuri, Otto Godwin, Pranay Dighe, Pushyami Rachapudi, Ramsey Tantawi, Roman Frigg, Sam Davarnia, Sanskruti Shah, Saptarshi Guha, Sasha Sirovica, Shen Ma, Shuang Ma, Simon Wang, Sulgi Kim, Suma Jayaram, Vaishaal Shankar, Varsha Paidi, Vivek Kumar, Xiang Kong, Xin Wang, Xin Zheng, Walker Cheng, Yael Shrager, Yang Ye, Yasu Tanaka, Yihao Guo, Yunsong Meng, Zhao Tang Luo, Zhi Ouyang, Zhiyun Lu, Alp Aygar, Alvin Wan, Andrew Walkingshaw, Andy Narayanan, Antonie Lin, Arsalan Farooq, Brent Ramerth, Chong Wang, Colorado Reed, Chris Bartels, Chris Chaney, David Riazati, Eric Liang Yang, Erin Feldman, Gabriel Hochstrasser, Guillaume Seguin, Guoli Yin, Irina Belousova, Jianyu Wang, Joris Pelemans, Karen Yang, Keivan Alizadeh Vahid, Liangliang Cao, Mahyar Najibi , Marco Zuliani, Max Horton, Minsik Cho, Nikhil Bhendawade, Patrick Dong, Piotr Maj, Pulkit Agrawal, Qi Shan, Qibin Chen, Qichen Fu, Regan Poston, Sam Xu, Shuangning Liu, Sushma Rao, Tashweena Heeramun, Thomas Merth, Uday Rayala, Victor Cui, Vivek Rangarajan Sridhar, Vivek Rathod, Wencong Zhang, Wenqi Zhang, Wentao Wu, Xiang Kong, Xingyu Zhou, Xinwen Liu, Yang Zhao, Yin Xia, Zhile Ren, Zhongzheng Ren",2024-07-29,Apple Intelligence Foundation Language Models,https://machinelearning.apple.com/research/apple-intelligence-foundation-language-models,,Significant use,"Currently in beta access only, but will be integrated into millions or billions of iPhones.",,,4.3e+24,"""The AFM base models are dense decoder-only models that build on the
Transformer architecture""

""We train AFM-server from scratch for 6.3T tokens on 8192
TPUv4 chips, using a sequence length of 4096 and a batch-size of 4096 sequences.""

""For both models we perform continued pre-training at a sequence length of
8192, with another 1T tokens from a mixture that upweights math and code,
and down-weights the bulk web-crawl.""

""The sustained model-flop-utilization (MFU) for this training run was approximately 52%.""

Parameter count is not specified other than it being ""larger"" than 3 billion.

Counting FLOP: Chinchilla scaling laws would suggest 7.3T / 20 = 365B parameters. 

365B parameters * 7.3T tokens * 6 ~= 1.6e25 FLOP.

However, the attention to inference optimization in the technical report suggests a smaller size, even for this ""server"" model. One point of reference is Llama 3 70B being overtrained by a factor of 10. If this is true of AFM-server, the parameter count would be ~37B and training compute would be 1.6e24 FLOP.

GPU-time: assume a wall-clock training time of 30 days based on the current trend value for notable models.

8192 chips * 275e12 FLOP/s per chip * 0.52 utilization * 30 * 24 * 60 * 60 s ~= 3.0e24 FLOP

The geometric mean of these three estimates is 4.3e24 FLOP.",,"6.3T tokens of web text, code, and math, plus another 1T in the second stage and 100B in the third. See section 3.1 for details.",7400000000000.0,"Not explicitly mentioned, but I assume the 7.4T tokens do not involve multiple epochs.",,,Google TPU v4,Self-supervised learning,Likely,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",1.0,,Hosted access (no API),United States of America,,,,8192.0,0.52,2025-05-28 16:01,Google Cloud,Google us-central2-b,,18949753.0,"Main pretraining uses sequence length of 4096 tokens; 4096 sequences per batch. During the ""continued"" pre-training stage, sequence length is upped to 8192 while batch size remains 4096. During context lengthening, sequence length is upped to 32768 while ""the recipe is similar to that used for continued pre-training"" implies same batch size of 4096.

Weighting batch sizes by number of tokens seen in each stage:

exp((6.3T * ln(4096 * 4096) + 1T * ln(8192 * 4096) + 100B * ln(32768 * 4096))/ 7.4T) = 18,949,753",Industry,,,4.3e+25,,Unreleased,,Industry,,,BF16,2746654.740036072,"Operation counting,Hardware",,,,20
RNACG,Biology,Protein or nucleotide language model (pLM/nLM),Tsinghua University,"Letian Gao, Zhi John Lu",2024-07-29,RNACG: A Universal RNA Sequence Conditional Generation model based on Flow-Matching,https://arxiv.org/abs/2407.19838,0.0,,,4500000.0,,,,,,,,,,,,Confident,"RNA plays a crucial role in diverse life processes. In contrast to the rapid advancement of protein design methods, the work related to RNA is more demanding. Most current RNA design approaches concentrate on specified target attributes and rely on extensive experimental searches. However, these methods remain costly and inefficient due to practical limitations. In this paper, we characterize all sequence design issues as conditional generation tasks and offer parameterized representations for multiple problems. For these problems, we have developed a universal RNA sequence generation model based on flow matching, namely RNACG. RNACG can accommodate various conditional inputs and is portable, enabling users to customize the encoding network for conditional inputs as per their requirements and integrate it into the generation network. We evaluated RNACG in RNA 3D structure inverse folding, 2D structure inverse folding, family-specific sequence generation, and 5'UTR translation efficiency prediction. RNACG attains superior or competitive performance on these tasks compared with other methods. RNACG exhibits extensive applicability in sequence generation and property prediction tasks, providing a novel approach to RNA sequence design and potential methods for simulation experiments with large-scale RNA sequence data.",,,,China,,,,,,2025-05-01 10:42,,,,,,Academia,,,,,,,Academia,,,,,,,,,
Florence-2-L (large),Vision,"Image captioning,Visual question answering,Image classification,Object detection",Microsoft,"Bin Xiao, Haiping Wu, Weijian Xu, Xiyang Dai, Houdong Hu, Yumao Lu, Michael Zeng, Ce Liu, Lu Yuan",2024-07-29,Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks,https://arxiv.org/abs/2311.06242,,,,771000000.0,771 million,1.2406517e+22,6ND = 6*771000000.00*2681910200000 = 1.2406517e+22,FLD-5B,"""we codeveloped FLD-5B that consists of 5.4 billion comprehensive visual annotations on 126 million images, using an iterative strategy of automated image annotation and model
refinement""",2681910200000.0,"We train our models with a mini-batch size of 2048/3072
(base/large) and an image size of 384Ã—384 until reaching
3 billion effective training samples. Similar to [15, 29, 64,
92, 95], we further conduct high-resolution tuning with an
image size of 768Ã—768 for 0.5 billion samples for the base
model and 0.1 billion samples for the large model.

assuming (!) patches 14*14 and avg of 40 text tokens per image: 

3*10^9*((384/14)^2+40) + 0.1*10^9*((768/14)^2+40) = 2.6819102e+12 tokens 
(Likely confidence)",,,,,Likely,"We introduce Florence-2, a novel vision foundation model with a unified, prompt-based representation for a variety of computer vision and vision-language tasks. While existing large vision models excel in transfer learning, they struggle to perform a diversity of tasks with simple instructions, a capability that implies handling the complexity of various spatial hierarchy and semantic granularity. Florence-2 was designed to take text-prompt as task instructions and generate desirable results in text forms, whether it be captioning, object detection, grounding or segmentation. This multi-task learning setup demands large-scale, high-quality annotated data. To this end, we co-developed FLD-5B that consists of 5.4 billion comprehensive visual annotations on 126 million images, using an iterative strategy of automated image annotation and model refinement. We adopted a sequence-to-sequence structure to train Florence-2 to perform versatile and comprehensive vision tasks. Extensive evaluations on numerous tasks demonstrated Florence-2 to be a strong vision foundation model contender with unprecedented zero-shot and fine-tuning capabilities.",,,Unreleased,"United States of America,Multinational,India,Belgium",,,,,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,,Industry,,,,,Operation counting,,,,
Yi Vision,Vision,"Visual question answering,Character recognition",01.AI,,2024-07-29,Yi Vision,https://platform.01.ai/docs#models-and-pricing,,,,,,,,Unspecified unreleased,,,,,,,,Unknown,"Ideal for scenarios that require analysis and interpretation of images and charts, such as image question answering, chart understanding, OCR, visual reasoning, education, research report understanding, or multilingual document reading.",,,API access,China,,,,,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
ProRNA3D-Single,Biology,Protein folding prediction,Virginia Tech (Virginia Polytechnic Institute and State University),"Rahmatullah Roche, Sumit Tarafder,  Debswapna Bhattacharya",2024-07-28,Single-sequence protein-RNA complex structure prediction by geometric attention-enabled pairing of biological language models,https://www.biorxiv.org/content/10.1101/2024.07.27.605468v1.abstract,,,,,,,"
",PDB (Protein Data Bank),,940001.0,"750 complexes Ã— (1000 protein residues + 250 RNA nucleotides) per complex
= 750 Ã— 1250
= 937,500 datapoints
â‰ˆ 9.4e5 datapoints",,,NVIDIA A100,,Confident,"Ground-breaking progress has been made in structure prediction of biomolecular assemblies, including the recent breakthrough of AlphaFold 3. However, it remains challenging for AlphaFold 3 and other state-of-the-art deep learning-based methods to accurately predict protein-RNA complex structures, in part due to the limited availability of evolutionary and structural information related to protein-RNA interactions that are used as inputs to the existing approaches. Here, we introduce ProRNA3D-single, a new deep-learning framework for protein-RNA complex structure prediction with only single-sequence input. Using a novel geometric attention-enabled pairing of biological language models of protein and RNA, a previously unexplored avenue, ProRNA3D-single enables the prediction of interatomic protein-RNA interaction maps, which are then transformed into multi-scale geometric restraints for modeling 3D structures of protein-RNA complexes via geometry optimization. Benchmark tests show that ProRNA3D-single convincingly outperforms current stateof-the-art methods including AlphaFold 3, particularly when evolutionary information is limited; and exhibits remarkable robustness and performance resilience by attaining better accuracy with only single-sequence input than what most methods can achieve even with explicit evolutionary information. Freely available at https://github.com/Bhattacharya-Lab/ProRNA3D-single, ProRNA3Dsingle should be broadly useful for modeling 3D structures of protein-RNA complexes at scale, regardless of the availability of evolutionary information.",100.0,,Open weights (unrestricted),United States of America,,,,1.0,,2025-06-09 13:36,,,,,,Academia,,,,,Unreleased,"An open-source software implementation of ProRNA3D-single, licensed under the GNU General Public License v3, is freely available at https://github.com/Bhattacharya-Lab/ProRNA3D-single. ",Academia,,,,433.4744267810611,Hardware,,,,
SaulLM-large,Language,Question answering,Equall.ai,"Pierre Colombo, Telmo Pires, Malik Boudiaf, Rui Melo, Dominic Culver, Sofia Morgado, Etienne Malaboeuf, Gabriel Hautreux, Johanne Charpentier, Michael Desa",2024-07-28,SaulLM-54B & SaulLM-141B: Scaling Up Domain Adaptation for the Legal Domain,https://arxiv.org/abs/2407.19584,7.0,,,141000000000.0,"SaulLM-large is based on Mixtral-141B-Instruct, a Transformer model with a Mixture of Experts that selects only 2 out of 8 experts to process tokens for each layer [1]. 
https://arxiv.org/abs/2407.19584",,"According to the developers, the continued pretraining involves a base corpus with over 540 billion of legal tokens [1], so I will assume the pre-training dataset comprises 540B tokens. 

Mixtral 8x7B has 8 experts, with 2 chosen to process tokens at each layer. This model has 46.7B parameters and uses only 12.9B parameters per token [2], corresponding to an active-to-total parameter ratio of 0.276. Mixtral-8x22B-v0.1 also has 8 experts, with 2 chosen to process tokens at each layer. This model has 140.6B parameters and uses only 39.2B parameters per token [3], corresponding to an active-to-total parameter ratio of 0.279. These figures appear to indicate that active-to-total parameter ratio changes little with total parameter count given constant expert utilization ratio. Mixtral-141B contains almost the same number of parameters as Mixtral-8x22B-v0.1, so I will assume it has the same active-to-total parameter ratio of 0.279.

Then, since the experts that the model utilizes are dense transformer layers, using the 6ND approximation yields
Pre-training compute
= # of active parameters / forward pass * # of tokens * 6 FLOPS / token
~= 0.279 active parameters / total parameters * 141e9 total parameters * 540e9 tokens * 6 FLOPS / token
~= 127458e18 FLOPS
~= 1.27e23 FLOPS

More details here: https://docs.google.com/document/d/1BTmyZ9KVTIwkp9z9tRRFsWM5A0QcXiufXrIhujffyso/edit?tab=t.2eg0wpinbs1n#heading=h.14tg63u7sqro

1. https://arxiv.org/abs/2407.19584
2. https://mistral.ai/news/mixtral-of-experts 
3. https://huggingface.co/mistral-community/Mixtral-8x22B-v0.1/discussions/6 ","FreeLaw,Wikipedia,GitHub",See pages 3 and 4 of arxiv.org/abs/2407.19584 for the complete composition of the training dataset.,405000000000.0,"This is a text generation model so dataset size is measured in number of words. Assuming the pre-training dataset contains 540B tokens and English words only,
Dataset size
~= 540e9 tokens * 0.75 words / token
= 405e9 words
= 4.05e11 words

1. https://arxiv.org/abs/2407.19584
2. https://docs.google.com/document/d/1XWLyMzcVfDv4eFQX3yPgM8MZ3_Q1phtIFz9GKv4_KaM/edit?tab=t.0#heading=h.ieihc08p8dn0",,,AMD Radeon Instinct MI250,,Likely,"In this paper, we introduce SaulLM-54B and SaulLM-141B, two large language models (LLMs) tailored for the legal sector. These models, which feature architectures of 54 billion and 141 billion parameters, respectively, are based on the Mixtral architecture. The development of SaulLM-54B and SaulLM-141B is guided by large-scale domain adaptation, divided into three strategies: (1) the exploitation of continued pretraining involving a base corpus that includes over 540 billion of legal tokens, (2) the implementation of a specialized legal instruction-following protocol, and (3) the alignment of model outputs with human preferences in legal interpretations. The integration of synthetically generated data in the second and third steps enhances the models' capabilities in interpreting and processing legal texts, effectively reaching state-of-the-art performance and outperforming previous open-source models on LegalBench-Instruct. This work explores the trade-offs involved in domain-specific adaptation at this scale, offering insights that may inform future studies on domain adaptation using strong decoder models. Building upon SaulLM-7B, this study refines the approach to produce an LLM better equipped for legal tasks. We are releasing base, instruct, and aligned versions on top of SaulLM-54B and SaulLM-141B under the MIT License to facilitate reuse and collaborative research.",,,Open weights (unrestricted),United States of America,Mixtral 8x22B,1,"The paper doesnâ€™t provide a clear description of the fine-tuning dataset, so thereâ€™s insufficient information to calculate the fine-tuning compute.",384.0,0.4,2025-05-01 10:42,,,,4.0,,Industry,,,,,,,Industry,,,,378683.2592359349,,,,,
AlphaProof,Mathematics,"Automated theorem proving,Quantitative reasoning",Google DeepMind,"AlphaProof development was led by Thomas Hubert, Rishi Mehta and Laurent Sartran
AlphaProof was developed with key contributions from Hussain Masoom, Aja Huang, MiklÃ³s Z. HorvÃ¡th, Tom Zahavy, Vivek Veeriah, Eric Wieser, Jessica Yung, Lei Yu, Yannick Schroecker, Julian Schrittwieser, Ottavia Bertolli, Borja Ibarz, Edward Lockhart, Edward Hughes, Mark Rowland, Grace Margand. Alex Davies and Daniel Zheng led the development of informal systems such as final answer determination, with key contributions from Iuliya Beloshapka, Ingrid von Glehn, Yin Li, Fabian Pedregosa, Ameya Velingker and Goran Å½uÅ¾iÄ‡. Oliver Nash, Bhavik Mehta, Paul Lezeau, Salvatore Mercuri, Lawrence Wu, Calle Soenne, Thomas Murrills, Luigi Massacci and Andrew Yang advised and contributed as Lean experts. Past contributors include Amol Mandhane, Tom Eccles, Eser AygÃ¼n, Zhitao Gong, Richard Evans, SoÅˆa MokrÃ¡, Amin Barekatain, Wendy Shang, Hannah Openshaw, Felix Gimeno. This work was advised by David Silver and Pushmeet Kohli.",2024-07-25,"AlphaProof, a new reinforcement-learning based system for formal math reasoning",https://deepmind.google/discover/blog/ai-solves-imo-problems-at-silver-medal-level/,,,,,,,,,approximately 100M formal problems,,,,,,Reinforcement learning,Unknown,"AlphaProof is a system that trains itself to prove mathematical statements in the formal language Lean. It couples a pre-trained language model with the AlphaZero reinforcement learning algorithm, which previously taught itself how to master the games of chess, shogi and Go.

",,,Unreleased,"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France",,,,,,2025-06-16 15:15,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
AlphaGeometry 2,"Mathematics,Language","Language modeling/generation,Geometry,Quantitative reasoning",Google DeepMind,"AlphaGeometry 2 and natural language reasoning efforts were led by Thang Luong.

The development of AlphaGeometry 2 was led by Trieu Trinh and Yuri Chervonyi, with key contributions by Mirek OlÅ¡Ã¡k, Xiaomeng Yang, Hoang Nguyen, Junehyuk Jung, Dawsen Hwang and Marcelo Menegali. The development of the natural language reasoning system was led by Golnaz Ghiasi, Garrett Bingham, YaGuang Li, with key contributions by Swaroop Mishra, Nigamaa Nayakanti, Sidharth Mudgal, Qijun Tan, Junehyuk Jung, Hoang Nguyen, Alex Zhai, Dawsen Hwang, Mingyang Deng, Clara Huiyi Hu, Jarrod Kahn, Maciej Kula, Cosmo Du.",2024-07-25,AlphaGeometry 2 is a significantly improved version of AlphaGeometry.,https://deepmind.google/discover/blog/ai-solves-imo-problems-at-silver-medal-level/,,,,,,,,,approximately 100M formal problems,,,,,,Reinforcement learning,Unknown,"AlphaGeometry 2 is a significantly improved version of AlphaGeometry. Itâ€™s a neuro-symbolic hybrid system in which the language model was based on Gemini and trained from scratch on an order of magnitude more synthetic data than its predecessor. This helped the model tackle much more challenging geometry problems, including problems about movements of objects and equations of angles, ratio or distances.

AlphaGeometry 2 employs a symbolic engine that is two orders of magnitude faster than its predecessor. When presented with a new problem, a novel knowledge-sharing mechanism is used to enable advanced combinations of different search trees to tackle more complex problems.",,,Unreleased,"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France",,,,,,2025-06-16 15:17,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
X-Portrait,"Video,Vision","Portrait animation,Video generation,Image-to-video",ByteDance,"You Xie, Hongyi Xu, Guoxian Song, Chao Wang, Yichun Shi, Linjie Luo",2024-07-25,X-Portrait: Expressive Portrait Animation with Hierarchical Motion Attention,https://arxiv.org/abs/2403.15931v4,32.0,,,,,,,,We train our model using an in-house dataset including monocular camera recordings of 42 expressions and 20-min talkings from 550 subjects in both indoor and outdoor scenes.,,,,,,,Unknown,"We propose X-Portrait, an innovative conditional diffusion model tailored for generating expressive and temporally coherent portrait animation. Specifically, given a single portrait as appearance reference, we aim to animate it with motion derived from a driving video, capturing both highly dynamic and subtle facial expressions along with wide-range head movements. As its core, we leverage the generative prior of a pre-trained diffusion model as the rendering backbone, while achieve fine-grained head pose and expression control with novel controlling signals within the framework of ControlNet. In contrast to conventional coarse explicit controls such as facial landmarks, our motion control module is learned to interpret the dynamics directly from the original driving RGB inputs. The motion accuracy is further enhanced with a patch-based local control module that effectively enhance the motion attention to small-scale nuances like eyeball positions. Notably, to mitigate the identity leakage from the driving signals, we train our motion control modules with scaling-augmented cross-identity images, ensuring maximized disentanglement from the appearance reference modules. Experimental results demonstrate the universal effectiveness of X-Portrait across a diverse range of facial portraits and expressive driving sequences, and showcase its proficiency in generating captivating portrait animations with consistently maintained identity characteristics.",,,Open weights (unrestricted),China,,,,,,2025-06-12 13:57,,,,,,Industry,,,,,Unreleased,"Apache 2.0:
https://github.com/bytedance/X-Portrait",Industry,,,,,,,,,
Mistral Large 2,Language,"Language modeling/generation,Translation,Code generation",Mistral AI,"Albert Jiang, Alexandre Sablayrolles, Alexis Tacnet, Alok Kothari, Antoine Roux, Arthur Mensch, Audrey Herblin-Stoop, Augustin Garreau, Austin Birky, Bam4d, Baptiste Bout, Baudouin de Monicault, Blanche Savary, Carole Rambaud, Caroline Feldman, Devendra Singh Chaplot, Diego de las Casas, Diogo Costa, Eleonore Arcelin, Emma Bou Hanna, Etienne Metzger, Gaspard Blanchet, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Harizo Rajaona, Henri Roussez, Hichem Sattouf, Ian Mack, Jean-Malo Delignon, Jessica Chudnovsky, Justus Murke, Kartik Khandelwal, Lawrence Stewart, Louis Martin, Louis Ternon, Lucile Saulnier, LÃ©lio Renard Lavaud, Margaret Jennings, Marie Pellat, Marie Torelli, Marie-Anne Lachaux, Marjorie Janiewicz, MickaÃ«l Seznec, Nicolas Schuhl, Niklas Muhs, Olivier de Garrigues, Patrick von Platen, Paul Jacob, Pauline Buche, Pavan Kumar Reddy, Perry Savas, Pierre Stock, Romain Sauvestre, Sagar Vaze, Sandeep Subramanian, Saurabh Garg, Sophia Yang, Szymon Antoniak, Teven Le Scao, Thibault Schueller, Thibaut Lavril, Thomas Wang, ThÃ©ophile Gervet, TimothÃ©e Lacroix, Valera Nemychnikova, Wendy Shang, William El Sayed, William Marshall",2024-07-24,"Top-tier reasoning for high-complexity tasks, for your most sophisticated needs.",https://mistral.ai/news/mistral-large-2407/,,Training cost,likely high training cost since previous Mistral Large cost around 20 million,123000000000.0,,2.13e+25,"Details are sparse, but we can hazard a guess based on evidence about the training cluster they may have used, the scale up in compute they likely would have used relative to Mistral Large 1, and from the model's MMLU score. Extended reasoning given here: https://docs.google.com/document/d/1I2ZWBLFMpRZYcdMMUfKAGZFJrOJpduNDS9ZeVFIHnd8/edit?usp=sharing",Unspecified unreleased,,,,,,,,Likely,"Today, we are announcing Mistral Large 2, the new generation of our flagship model. Compared to its predecessor, Mistral Large 2 is significantly more capable in code generation, mathematics, and reasoning. It also provides a much stronger multilingual support, and advanced function calling capabilities.",,,Open weights (non-commercial),France,,,,,,2025-05-06 15:10,,,,,,Industry,,2.0000000001e+25,,,Unreleased,"""We are releasing Mistral Large 2 under the Mistral Research License, that allows usage and modification for research and non-commercial usages. For commercial usage of Mistral Large 2 requiring self-deployment, a Mistral Commercial License must be acquired by contacting us.""",Industry,,,,,"Hardware,Cost,Benchmarks",mistralai,,,6
Stable Video 4D (SV4D),"Vision,Video,3D modeling",3D reconstruction,"Stability AI,Northeastern University","Yiming Xie, Chun-Han Yao, Vikram Voleti, Huaizu Jiang, Varun Jampani",2024-07-24,SV4D: Dynamic 3D Content Generation with Multi-Frame and Multi-View Consistency,https://sv4d.github.io/static/sv4d_technical_report.pdf,,,,,,,,Objaverse,"This model is trained to generate 40 frames (5 video frames x 8 camera views) at 576x576 resolution, given 5 reference frames of the same size.

We use renders from the Objaverse dataset, available under the Open Data Commons Attribution License, utilizing our enhanced rendering method that more closely replicates the distribution of images found in the real world, significantly improving our model's ability to generalize. We filter objects based on the review of licenses and curated a subset suitable for our training needs.",,,,We use an effective batch size of 16 during training on 2 nodes of 8 80GB H100 GPUs,NVIDIA H100 SXM5 80GB,,Unknown,"We present Stable Video 4D (SV4D) â€” a latent video diffusion model for multiframe and multi-view consistent dynamic 3D content generation. Unlike previous methods that rely on separately trained generative models for video generation and novel view synthesis, we design a unified diffusion model to generate novel view videos of dynamic 3D objects. Specifically, given a monocular reference video, SV4D generates novel views for each video frame that are temporally consistent. We then use the generated novel view videos to optimize an implicit 4D representation (dynamic NeRF) efficiently, without the need for cumbersome SDS-based optimization used in most prior works. To train our unified novelview video generation model, we curated a dynamic 3D object dataset from the existing Objaverse dataset. Extensive experimental results on multiple datasets and user studies demonstrate SV4Dâ€™s state-of-the-art performance on novel-view video synthesis as well as 4D generation compared to prior works. Project page:
https://sv4d.github.io.
",,,Open weights (restricted use),"United Kingdom of Great Britain and Northern Ireland,United States of America",,,,8.0,,2025-05-01 10:42,,,,,,"Industry,Academia",,,,,,"Community License: Free for research, non-commercial, and commercial use by organizations and individuals generating annual revenue of US $1,000,000 (or local currency equivalent) or less, regardless of the source of that revenue.","Industry,Academia",,,,11045.912292646875,,,,,
PrePR-CT,Biology,Transcriptomic prediction,"King Abdullah University of Science and Technology (KAUST),Karolinska Institute","Reem Alsulami, Robert Lehmann, Sumeer A. Khan, Vincenzo Lagani, David GÂ´omez-Cabrero, Narsis A. Kiani, Jesper Tegner",2024-07-24,"PrePR-CT: Predicting Perturbation Responses in Unseen Cell Types Using Cell-Type-Specific Graphs
",https://www.biorxiv.org/content/10.1101/2024.07.24.604816v1,0.0,,,,,,,,"Table 2 shows different experiments with different datasets, the largest being from NeurIPS",,"56,668 perturbed samples (Table 2)",,,,,Unknown,"Predicting the transcriptional response of chemical perturbations is crucial to understanding gene function and developing drug candidates, promising a streamlined drug development process. Single-cell sequencing has provided an ideal data basis for training machine learning models for this task. Recent advances in deep learning have led to significant improvements in predictions of chemical as well as genetic perturbations at the single cell level. Experiments have shown that different cell types exhibit distinct transcriptional patterns and responses to perturbation. This poses a fundamental problem for predicting transcriptional responses of drugs or cell types outside the training data. Accordingly, existing methods lack cell-type-specific modeling or do not explicitly provide an interpretable mechanism for the gene features. In this study, we introduce a novel approach that employs a network representation of various cell types as an inductive bias, improving prediction performance in scenarios with limited data while acknowledging cellular differences. We applied our framework to four small-scale single-cell perturbation datasets and one large-scale screening experiment, demonstrating that this representation can inherently generalize to previously unseen cell types. Furthermore, our method outperforms the state-of-the-art methods in predicting the post-perturbation response in unobserved cell types.",200.0,,,"Saudi Arabia,Sweden",ChemBERTa,,,,,2025-05-01 10:42,,,,,,"Academia,Academia",,,,,,,"Academia,Academia",,,,,,,,,
Llama 3.1-405B,Language,Language modeling/generation,Meta AI,"Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,
Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie
Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen
Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux,
Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang
Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle
Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino,
Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip
Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Gregoire
Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov,
Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert,
Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong,
Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe
Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala,
Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer,
Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence
Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat,
Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas,
Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar
Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji,
Olivier Duchenne, Onur Ã‡elebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng,
Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong,
Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta
Raileanu, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor,
Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun
Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan,
Shruti Bhosale, Shun Zhang, Simon Vandenhende, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin
Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas
Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta,
Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vladan Petrovic, Weiwei
Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaoqing Ellen Tan,
Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen
Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, and Zoe
Papakipos.
(core contributors)",2024-07-23,The Llama 3 Herd of Models,https://ai.meta.com/research/publications/the-llama-3-herd-of-models/,,"SOTA improvement,Training cost","High training compute, exceeds 4o and Claude 3.5 on some benchmarks:

https://ai.meta.com/blog/meta-llama-3-1/ ",405000000000.0,405B,3.8e+25,"Stated in paper.

Also, 6 * 405B * 15.6T training tokens = 3.8e25",Llama 3 dataset,"Knowledge cutoff date is December 2023, according to https://huggingface.co/meta-llama/Llama-3.1-405B.",15600000000000.0,15.6T tokens,2142.0,"Trained on 30.84M GPU hours (https://huggingface.co/blog/llama31) and used ""up to 16K H100 GPU[s]"" so training took at least
30.84M / 16k = 1927.5 hours or ~80 days. 

Section 3.3.4 gives reliability details over a 54 day period during training, for which they had ""higher than 90% effective training time""
1927.5 / 0.9 = 2142 hours

Probably, full training time is somewhat longer, since it sounds like there were periods where not all 16k H100s were running.",NVIDIA H100 SXM5 80GB,,Confident,"Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",1.0,,Open weights (restricted use),United States of America,,,,16384.0,0.4042,2025-06-10 15:16,,,,16000000.0,,Industry,,,,,Open (restricted use),"Llama 3.1 model license:

https://huggingface.co/meta-llama/Meta-Llama-3.1-8B/blob/main/LICENSE 

must seek separate license if over 700m monthly users, acceptable use restrictions

training code here: https://github.com/meta-llama/llama-recipes/blob/main/src/llama_recipes/utils/train_utils.py#L70 
",Industry,,MFU ranges between 0.38 and 0.43 depending on the specific parallelism used; I assume the geometric mean: sqrt(0.38 * 0.43) = 0.4042,BF16,22622532.159299143,"Reported,Operation counting",meta-llama,9.4e+22,"Section 4 gives detail about the post-training process. They do 6 rounds of post-training, using the model from the previous iteration in each successive round. In each round, they fine-tune a copy of the language model into a reward model (RM) using preference data, then use the reward model to do rejection sampling on human annotation prompts. Next they do supervised fine-tuning (SFT) on the rejection sampled data along with some synthetic data (8.5k to 9k steps per round). Next, they do Direct Preference Optimization (DPO) over the most recent generation of preference data. Finally, they do model averaging over experiments using different samples of data or hyperparameters in each of the RM, SFT, and DPO steps. 

We can get a very rough estimate of total post-training compute:
- RM section doesn't give any detail about batch size or training length. In Llama 2, they trained the RM for one epoch over their 1.7B tokens of preference data. We aren't told how many examples they have in Llama 3, though we see that examples are somewhat longer, apparently because they contain more turns on average. Assuming preference data scaled up by a similar amount to pre-training data (2T to 15T tokens from Llama 2 to 3), that suggests around 1.7B * 15T / 2T = 12.75B tokens, which corresponds to 6 * 12.75B * 405B = 3.098e22 FLOPs, if they used the 405b variant for their reward model.
- SFT uses 8.5k-9k steps in each round. Batch size is not given here, but in Llama 2 they used batches of 64 sequences of 4096 = 262,144 tokens. Assuming they scaled up SFT batches by a similar ratio as they did pretraining (4M in Llama 2 -> 16M for most of Llama 3 pretraining), Llama 3's SFT would have used around 1M tokens per batch. That suggests 6 * 9k * 1M * 405B = 2.187e22 FLOP per round of SFT. 
- DPO section gives no detail about quantity of examples or training length. Llama 2 used PPO for 200-400 iterations per model; each iteration was a batch of 512 examples, and took ~330 seconds. Our current estimate is that Llama 2-70B used 1000 A100s, which would suggest 330 * 3.12e14 * 1000 = 4.118e22 FLOP. DPO uses less compute, but presumably they've also scaled up their data here; without further info let's speculate these roughly cancel out.
So total fine-tuning compute is about 3.1e22 + 2.2e22 + 4.1e22 = 9.4E22 FLOPs",3
OutfitAnyone,"Image generation,Vision",Image generation,"Alibaba,University of Science and Technology of China (USTC)","Ke Sun, Jian Cao, Qi Wang, Linrui Tian, Xindi Zhang, Lian Zhuo, Bang Zhang, Liefeng Bo, Wenbo Zhou, Weiming Zhang, Daiheng Gao",2024-07-23,OutfitAnyone: Ultra-high Quality Virtual Try-On for Any Clothing and Any Person,https://arxiv.org/abs/2407.16224,,,,,,,,Unspecified unreleased,,,,,,,,Unknown,"Virtual Try-On (VTON) has become a transformative technology, empowering users to experiment with fashion without ever having to physically try on clothing. However, existing methods often struggle with generating high-fidelity and detail-consistent results. While diffusion models, such as Stable Diffusion series, have shown their capability in creating high-quality and photorealistic images, they encounter formidable challenges in conditional generation scenarios like VTON. Specifically, these models struggle to maintain a balance between control and consistency when generating images for virtual clothing trials. OutfitAnyone addresses these limitations by leveraging a two-stream conditional diffusion model, enabling it to adeptly handle garment deformation for more lifelike results. It distinguishes itself with scalability-modulating factors such as pose, body shape and broad applicability, extending from anime to in-the-wild images. OutfitAnyone's performance in diverse scenarios underscores its utility and readiness for real-world deployment. For more details and animated results, please see \url{this https URL}.",,,Hosted access (no API),"China,China",,,,,,2025-05-19 16:16,,,,,,"Industry,Academia",,,,,Unreleased,https://huggingface.co/spaces/HumanAIGC/OutfitAnyone,"Industry,Academia",,,,,,,,,
DCLM 7B,Language,"Language modeling/generation,Question answering,Quantitative reasoning",Apple,,2024-07-20,Model Card for DCLM-Baseline-7B,https://huggingface.co/apple/DCLM-7B,,,,7000000000.0,7B,1.05e+23,6 FLOP / parameter / token * 7 * 10^9 parameters * 2.5 * 10^12 tokens = 1.05e+23 FLOP,"StarCoder,Proofpile 2,DCLM-baseline","""To ensure our trained model is broadly useful, including for math and coding tasks, we combine our 3.8T DCLM-BASELINE with the StarCoder and ProofPile2 data to arrive at a 4.1T token dataset.""",2500000000000.0,Total Training Tokens: 2.5T,,,NVIDIA H100 SXM5 80GB,,Confident,"DCLM-Baseline-7B is a 7 billion parameter language model trained on the DCLM-Baseline dataset, which was curated as part of the DataComp for Language Models (DCLM) benchmark. This model is designed to showcase the effectiveness of systematic data curation techniques for improving language model performance.",,,Open weights (restricted use),United States of America,,,,,,2025-05-30 14:29,,,,,,Industry,,,,,Unreleased,"Apple Sample Code license (no patent rights, copyright-only)
https://huggingface.co/apple/DCLM-7B",Industry,,,,,Operation counting,apple,,,
Athene-70B,Language,"Language modeling/generation,Question answering",Nexusflow,"Evan Frick, Peter Jin, Tianle Li, Karthik Ganesan, Jian Zhang, Jiantao Jiao, Banghua Zhu",2024-07-19,Athene-70B: Redefining the Boundaries of Post-Training for Open Models,https://nexusflow.ai/blogs/athene,,,,70000000000.0,70B,,,Unspecified unreleased,,,,,,,,Confident,"We are excited to announce the release of Athene-Llama3-70B by Nexusflow, a strong open-weights chat model fine-tuned from Meta AIâ€™s Llama-3-70B. Athene-70B has achieved an impressive Arena-Hard-Auto score of 77.8%, placing it close to leading proprietary models such as GPT-4o (79.2%) and Claude-3.5-Sonnet (79.3%). This represents a significant leap from its predecessor, Llama-3-70B-Instruct, which scored 46.6%. Athene-70B is now under public testing on Chatbot Arena. The improvement comes from Nexusflowâ€™s targeted post-training pipeline to enhance desired model behaviors.",,,Open weights (non-commercial),United States of America,Llama 3-70B,,,,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,"Nexus Research License ""for personal use only"" + llama license applies",Industry,,,,,,Nexusflow,,,
Mistral NeMo,Language,"Language modeling/generation,Code generation",Mistral AI,"Albert Jiang, Alexandre Sablayrolles, Alexis Tacnet, Alok Kothari, Antoine Roux, Arthur Mensch, Audrey Herblin-Stoop, Augustin Garreau, Austin Birky, Bam4d, Baptiste Bout, Baudouin de Monicault, Blanche Savary, Carole Rambaud, Caroline Feldman, Devendra Singh Chaplot, Diego de las Casas, Eleonore Arcelin, Emma Bou Hanna, Etienne Metzger, Gaspard Blanchet, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Harizo Rajaona, Henri Roussez, Hichem Sattouf, Ian Mack, Jean-Malo Delignon, Jessica Chudnovsky, Justus Murke, Kartik Khandelwal, Lawrence Stewart, Louis Martin, Louis Ternon, Lucile Saulnier, LÃ©lio Renard Lavaud, Margaret Jennings, Marie Pellat, Marie Torelli, Marie-Anne Lachaux, Marjorie Janiewicz, MickaÃ«l Seznec, Nicolas Schuhl, Niklas Muhs, Olivier de Garrigues, Patrick von Platen, Paul Jacob, Pauline Buche, Pavan Kumar Reddy, Perry Savas, Pierre Stock, Romain Sauvestre, Sagar Vaze, Sandeep Subramanian, Saurabh Garg, Sophia Yang, Szymon Antoniak, Teven Le Scao, Thibault Schueller, Thibaut Lavril, Thomas Wang, ThÃ©ophile Gervet, TimothÃ©e Lacroix, Valera Nemychnikova, Wendy Shang, William El Sayed, William Marshall",2024-07-18,Mistral NeMo,https://mistral.ai/news/mistral-nemo/,,,,12000000000.0,12b,,,,,,,,,,Self-supervised learning,Confident,"Mistral NeMo: our new best small model. A state-of-the-art 12B model with 128k context length, built in collaboration with NVIDIA, and released under the Apache 2.0 license.",,,Open weights (unrestricted),France,,,,,,2025-05-09 11:32,,,,,,Industry,,,,,,,Industry,,,,,,mistralai,,,
GPT-4o mini,"Language,Multimodal,Vision","Chat,Language modeling/generation,Code generation,Visual question answering",OpenAI,"Pre-training leads
Aidan Clark, Alex Paino, Jacob Menick

Post-training leads
Liam Fedus, Luke Metz

Architecture leads
Clemens Winter, Lia Guy

Optimization leads
Sam Schoenholz, Daniel Levy

Long-context lead
Nitish Keskar

Pre-training Data leads
Alex Carney, Alex Paino, Ian Sohl, Qiming Yuan

Tokenizer lead
Reimar Leike

Human data leads
Arka Dhar, Brydon Eastman, Mia Glaese

Eval lead
Ben Sokolowsky

Data flywheel lead
Andrew Kondrich

Inference lead
Felipe Petroski Such

Inference Productionization lead
Henrique Ponde de Oliveira Pinto

Post-training infrastructure leads
Jiayi Weng, Randall Lin, Youlong Cheng

Pre-training organization lead
Nick Ryder

Pre-training program lead
Lauren Itow

Post-training organization leads
Barret Zoph, John Schulman

Post-training program lead
Mianna Chen

Core contributors
Adam Lerer, Adam P. Goucher, Adam Perelman, Akila Welihinda, Alec Radford, Alex Borzunov, Alex Carney, Alex Chow, Alex Paino, Alex Renzin, Alex Tachard Passos, Alexi Christakis, Ali Kamali, Allison Moyer, Allison Tam, Amadou Crookes, Amin Tootoonchian, Ananya Kumar, Andrej Karpathy, Andrey Mishchenko, Andrew Cann, Andrew Kondrich, Andrew Tulloch, Angela Jiang, Antoine Pelisse, Antonia Woodford, Anuj Gosalia, Avi Nayak, Avital Oliver, Behrooz Ghorbani, Ben Leimberger, Ben Wang, Beth Hoover, Blake Samic, Brian Guarraci, Brydon Eastman, Camillo Lugaresi, Chak Li, Charlotte Barette, Chelsea Voss, Chen Ding, Chong Zhang, Chris Beaumont, Chris Hallacy, Chris Koch, Christian Gibson, Christine Choi, Christopher Hesse, Colin Wei, Daniel Kappler, Daniel Levin, Daniel Levy, David Farhi, David Mely, David Sasaki, Dimitris Tsipras, Doug Li, Duc Phong Nguyen, Duncan Findlay, Edmund Wong, Ehsan Asdar, Elizabeth Proehl, Elizabeth Yang, Eric Peterson, Eric Sigler, Eugene Brevdo, Farzad Khorasani, Francis Zhang, Gene Oden, Geoff Salmon, Hadi Salman, Haiming Bao, Heather Schmidt, Hongyu Ren, Hyung Won Chung, Ian Kivlichan, Ian O'Connell, Ian Osband, Ibrahim Okuyucu, Ilya Kostrikov, Ingmar Kanitscheider, Jacob Coxon, James Crooks, James Lennon, Jane Park, Jason Teplitz, Jason Wei, Jason Wolfe, Jay Chen, Jeff Harris, Jiayi Weng, Jie Tang, Joanne Jang, Jonathan Ward, Jonathan McKay, Jong Wook Kim, Josh Gross, Josh Kaplan, Joy Jiao, Joyce Lee, Juntang Zhuang, Kai Fricke, Kavin Karthik, Kenny Hsu, Kiel Howe, Kyle Luther, Larry Kai, Lauren Itow, Leo Chen, Lia Guy, Lien Mamitsuka, Lilian Weng, Long Ouyang, Louis Feuvrier, Lukas Kondraciuk, Lukasz Kaiser, Lyric Doshi, Mada Aflak, Maddie Simens, Madeleine Thompson, Marat Dukhan, Marvin Zhang, Mateusz Litwin, Matthew Zeng, Max Johnson, Mayank Gupta, Mia Glaese, Michael Janner, Michael Petrov, Michael Wu, Michelle Fradin, Michelle Pokrass, Miguel Oom Temudo de Castro, Mikhail Pavlov, Minal Khan, Mo Bavarian, Murat Yesildal, Natalia Gimelshein, Natalie Staudacher, Nick Stathas, Nik Tezak, Nithanth Kudige, Noel Bundick, Ofir Nachum, Oleg Boiko, Oleg Murk, Olivier Godement, Owen Campbell-Moore, Philip Pronin, Philippe Tillet, Rachel Lim, Rajan Troll, Randall Lin, Rapha gontijo lopes, Raul Puri, Reah Miyara, Reimar Leike, Renaud Gaubert, Reza Zamani, Rob Honsby, Rohit Ramchandani, Rory Carmichael, Ruslan Nigmatullin, Ryan Cheu, Sara Culver, Scott Gray, Sean Grove, Sean Metzger, Shantanu Jain, Shengjia Zhao, Sherwin Wu, Shuaiqi (Tony) Xia, Sonia Phene, Spencer Papay, Steve Coffey, Steve Lee, Steve Lee, Stewart Hall, Suchir Balaji, Tal Broda, Tal Stramer, Tarun Gogineni, Ted Sanders, Thomas Cunninghman, Thomas Dimson, Thomas Raoux, Tianhao Zheng, Christina Kim, Todd Underwood, Tristan Heywood, Valerie Qi, Vinnie Monaco, Vlad Fomenko, Weiyi Zheng, Wenda Zhou, Wojciech Zaremba, Yash Patil, Yilei, Qian, Yongjik Kim, Youlong Cheng, Yuchen He, Yuchen Zhang, Yujia Jin, Yunxing Dai, Yury Malkov",2024-07-18,GPT-4o mini: advancing cost-efficient intelligence,https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/,,Significant use,"No public breakdown of GPT-4o mini users, but as of late 2024, it is one of the few main models available in ChatGPT and OpenAI's cheapest model. OpenAI has hundreds of millions of users.",,,7.36001e+24,"Training compute estimated from benchmark scores.

90% CI [3.23e+24, 2.05e+25]",Unspecified unreleased,"This has a knowledge cutoff date of October 2023 - which I assume means October 1, 2023 - according to https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models?tabs=global-standard%2Cstandard-chat-completions#gpt-4o-and-gpt-4-turbo.",,,,,,,Speculative,"OpenAI is committed to making intelligence as broadly accessible as possible. Today, we're announcing GPT-4o mini, our most cost-efficient small model. We expect GPT-4o mini will significantly expand the range of applications built with AI by making intelligence much more affordable. GPT-4o mini scores 82% on MMLU and currently outperforms GPT-41 on chat preferences in LMSYS leaderboard(opens in a new window). It is priced at 15 cents per million input tokens and 60 cents per million output tokens, an order of magnitude more affordable than previous frontier models and more than 60% cheaper than GPT-3.5 Turbo.",,,API access,United States of America,,,,,,2025-05-12 19:08,,,,,,Industry,,,2.050001e+25,,Unreleased,,Industry,,,,,Benchmarks,,,,15
Cygnet,Language,"Chat,Language modeling/generation",Gray Swan,,2024-07-16,Embrace the Future of Secure AI with Cygnet,"https://www.grayswan.ai/product/cygnet
https://www.grayswan.ai/news/gray-swan-launch",,,,8000000000.0,,,Trained from Llama 3-8B which used 7.2e23 FLOPs. Unclear what kind of training they did on top of this. ,,,,,,,,,Confident,,,,API access,United States of America,Llama 3-8B,,,,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
Codestral Mamba,Language,"Code generation,Code autocompletion",Mistral AI,"Albert Jiang, Alexandre Sablayrolles, Alexis Tacnet, Alok Kothari, Antoine Roux, Arthur Mensch, Audrey Herblin-Stoop, Augustin Garreau, Austin Birky, Bam4d, Baptiste Bout, Baudouin de Monicault, Blanche Savary, Carole Rambaud, Caroline Feldman, Devendra Singh Chaplot, Diego de las Casas, Eleonore Arcelin, Emma Bou Hanna, Etienne Metzger, Gaspard Blanchet, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Harizo Rajaona, Henri Roussez, Hichem Sattouf, Ian Mack, Jean-Malo Delignon, Jessica Chudnovsky, Justus Murke, Kartik Khandelwal, Lawrence Stewart, Louis Martin, Louis Ternon, Lucile Saulnier, LÃ©lio Renard Lavaud, Margaret Jennings, Marie Pellat, Marie Torelli, Marie-Anne Lachaux, Marjorie Janiewicz, MickaÃ«l Seznec, Nicolas Schuhl, Niklas Muhs, Olivier de Garrigues, Patrick von Platen, Paul Jacob, Pauline Buche, Pavan Kumar Reddy, Perry Savas, Pierre Stock, Romain Sauvestre, Sagar Vaze, Sandeep Subramanian, Saurabh Garg, Sophia Yang, Szymon Antoniak, Teven Le Scao, Thibault Schueller, Thibaut Lavril, Thomas Wang, ThÃ©ophile Gervet, TimothÃ©e Lacroix, Valera Nemychnikova, Wendy Shang, William El Sayed, William Marshall",2024-07-16,"As a tribute to Cleopatra, whose glorious destiny ended in tragic snake circumstances, we are proud to release Codestral Mamba, a Mamba2 language model specialised in code generation, available under an Apache 2.0 license.",https://mistral.ai/news/codestral-mamba/,,,,7285403648.0,"""This is an instructed model, with 7,285,403,648 parameters.""",,,Unspecified unreleased,,,,,,,,Confident,"Following the publishing of the Mixtral family, Codestral Mamba is another step in our effort to study and provide new architectures. It is available for free use, modification, and distribution, and we hope it will open new perspectives in architecture research. Codestral Mamba was designed with help from Albert Gu and Tri Dao.

Unlike Transformer models, Mamba models offer the advantage of linear time inference and the theoretical ability to model sequences of infinite length. It allows users to engage with the model extensively with quick responses, irrespective of the input length. This efficiency is especially relevant for code productivity use casesâ€”this is why we trained this model with advanced code and reasoning capabilities, enabling it to perform on par with SOTA transformer-based models.",,,Open weights (unrestricted),France,,,,,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,"Apache 2

https://huggingface.co/mistralai/Mamba-Codestral-7B-v0.1",Industry,,,,,,,,,
DeepL LLM,Language,Translation,DeepL,DeepL,2024-07-16,"DeepL's next-gen LLM outperforms ChatGPT-4, Google, and Microsoft for translation quality",https://www.deepl.com/en/blog/next-gen-language-model,,Significant use,"DeepL has ~10M monthly users: https://www.forbes.com/sites/rashishrivastava/2023/08/08/deepl-is-trying-to-take-on-google-translate-and-chatgpt/#:~:text=DeepL%20boasts%20more%20than%2010,struggle%20with%20a%20language%20barrier.= ",,,,,Unspecified unreleased,,,,,,,Supervised,Unknown,"Our next-generation (â€œnext-genâ€) language model outperforms Google Translate, ChatGPT-4, and Microsoft for translation quality
The new LLM's translations require fewer edits, with Google needing 2x and ChatGPT-4 needing 3x more edits to achieve the same quality
Built using our own groundbreaking, specialized LLM technology and proprietary training data, designed specifically for translation
The same enterprise-level security youâ€™re used to for Pro customers",,,Hosted access (no API),Germany,,,,,,2025-05-09 11:32,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
Mathstral,"Mathematics,Language","Mathematical reasoning,Language modeling/generation,Quantitative reasoning",Mistral AI,"Albert Jiang, Alexandre Sablayrolles, Alexis Tacnet, Alok Kothari, Antoine Roux, Arthur Mensch, Audrey Herblin-Stoop, Augustin Garreau, Austin Birky, Bam4d, Baptiste Bout, Baudouin de Monicault, Blanche Savary, Carole Rambaud, Caroline Feldman, Devendra Singh Chaplot, Diego de las Casas, Eleonore Arcelin, Emma Bou Hanna, Etienne Metzger, Gaspard Blanchet, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Harizo Rajaona, Henri Roussez, Hichem Sattouf, Ian Mack, Jean-Malo Delignon, Jessica Chudnovsky, Justus Murke, Kartik Khandelwal, Lawrence Stewart, Louis Martin, Louis Ternon, Lucile Saulnier, LÃ©lio Renard Lavaud, Margaret Jennings, Marie Pellat, Marie Torelli, Marie-Anne Lachaux, Marjorie Janiewicz, MickaÃ«l Seznec, Nicolas Schuhl, Niklas Muhs, Olivier de Garrigues, Patrick von Platen, Paul Jacob, Pauline Buche, Pavan Kumar Reddy, Perry Savas, Pierre Stock, Romain Sauvestre, Sagar Vaze, Sandeep Subramanian, Saurabh Garg, Sophia Yang, Szymon Antoniak, Teven Le Scao, Thibault Schueller, Thibaut Lavril, Thomas Wang, ThÃ©ophile Gervet, TimothÃ©e Lacroix, Valera Nemychnikova, Wendy Shang, William El Sayed, William Marshall",2024-07-16,Mathstral-7B-v0.1 ,"https://huggingface.co/mistralai/Mathstral-7B-v0.1
https://mistral.ai/news/mathstral",,SOTA improvement,SOTA on MathOdyssey and AMC 2023 benchmarks.,7000000000.0,,,,,,,,,,,,Confident,"We're contributing Mathstral to the science community to bolster efforts in advanced mathematical problems requiring complex, multi-step logical reasoning. The Mathstral release is part of our broader effort to support academic projectsâ€”it was produced in the context of our collaboration with Project Numina.

Akin to Isaac Newton in his time, Mathstral stands on the shoulders of Mistral 7B and specializes in STEM subjects. It achieves state-of-the-art reasoning capacities in its size category across various industry-standard benchmarks. In particular, it achieves 56.6% on MATH and 63.47% on MMLU, with the following MMLU performance difference by subject between Mathstral 7B and Mistral 7B.",,,Open weights (unrestricted),France,Mistral 7B,,,,,2025-06-01 16:25,,,,,,Industry,,,,,Unreleased,"Apache 2.0 but ""You need to agree to share your contact information to access this model""
https://huggingface.co/mistralai/Mathstral-7B-v0.1",Industry,,,,,,mistralai,,,
LLaVA-NeXT-32B-Qwen,"Multimodal,Language","Chat,Language modeling/generation,Visual question answering,Image captioning",LMMs-Lab,,2024-07-16,Github: lmms-lab/llava-next-qwen-32b,https://huggingface.co/lmms-lab/llava-next-qwen-32b,,,,32000000000.0,,,,,,,"Training Data
[Pretrain] 558K filtered image-text pairs from LAION/CC/SBU, captioned by BLIP.
158K GPT-generated multimodal instruction-following data.
500K academic-task-oriented VQA data mixture.
50K GPT-4V data mixture.
40K ShareGPT data.
20K COCO Caption data.

",35.0,The training cost is 30-40 hours on 8 x 8 NVIDIA A100-SXM4-80GB (may vary due to hardware differences).,NVIDIA A100 SXM4 80 GB,,Confident,"Model type: LLaVA is an open-source chatbot trained by fine-tuning LLM on multimodal instruction-following data. It is an auto-regressive language model, based on the transformer architecture.",,,Open weights (restricted use),,Qwen1.5-32B,754790400000000000000,"312000000000000 FLOP / GPU / sec [A100 80GB reported, bf 16 assumed] * 64 GPUs * 35 hours [""30-40 hours"" reported] * 3600 sec / hour * 0.3 [assumed utilization] = 7.547904e+20 FLOP",64.0,,2025-06-09 12:38,,,,,,Research collective,,,,,Unreleased,"https://huggingface.co/lmms-lab/llava-next-qwen-32b

""This project utilizes certain datasets and checkpoints that are subject to their respective original licenses. Users must comply with all terms and conditions of these original licenses, including but not limited to the OpenAI Terms of Use for the dataset and the specific licenses for base language models for checkpoints trained using the dataset (e.g. Llama-1/2 community license for LLaMA-2 and Vicuna-v1.5, Tongyi Qianwen LICENSE AGREEMENT and META LLAMA 3 COMMUNITY LICENSE AGREEMENT). This project does not impose any additional constraints beyond those stipulated in the original licenses""",Research collective,,,,50504.59589543689,Hardware,lmms-lab,,,
SmolLM-1.7B,Language,"Chat,Question answering,Language modeling/generation",,"Loubna Ben Allal, Anton Lozhkov, Elie Bakouch",2024-07-16,SmolLM - blazingly fast and remarkably powerful,https://huggingface.co/blog/smollm,,,,1710000000.0,"The image here, https://huggingface.co/blog/smollm#hyperparameters-choice, shows that SmolLM-1.7B has 1.71B parameters.",1.03e+22,"""Therefore, we decided to train the 1.7B model on 1 trillion tokens"" (https://huggingface.co/blog/smollm#experiments). The model doesn't have any CNNs or RNNs (https://huggingface.co/blog/smollm#hyperparameters-choice), so I will approximate it to be dense. Then assuming the model saw 1 trillion tokens during training, the 6ND approximation yields
Training compute
= # of active parameters / forward pass * # of tokens * 6 FLOPs / token
= 1.71e9 parameters * 1e12 tokens * 6 FLOPs / token
~= 1.03e22 FLOPS

""We also instruction tuned the models using publicly available permissive instruction datasets. We trained all three models for one epoch on the permissive subset of the WebInstructSub dataset, combined with StarCoder2-Self-OSS-Instruct. Following this, we performed DPO (Direct Preference Optimization) for one epoch: using HelpSteer for the 135M and 1.7B models... We followed the training parameters from the Zephyr-Gemma recipe in the alignment handbook, but adjusted the SFT (Supervised Fine-Tuning) learning rate to 3e-4"" (https://huggingface.co/blog/smollm#evaluation).",SmolLM Corpus,See full details here: https://huggingface.co/blog/smollm#data-curation,1000000000000.0,Pretraining tokens: 1T,,,NVIDIA H100 PCIe,,Confident,"This blog post introduces SmolLM, a family of state-of-the-art small models with 135M, 360M, and 1.7B parameters, trained on a new high-quality dataset. It covers data curation, model evaluation, and usage.",,,Open weights (unrestricted),,,,,64.0,,2025-06-09 12:24,,,,2000000.0,,,,,,,Unreleased,"Apache 2.0
https://huggingface.co/HuggingFaceTB/SmolLM-1.7B",,,,BF16,44191.52140850728,Operation counting,HuggingFaceTB,,,
Qwen2-Audio,"Multimodal,Language,Audio","Speech recognition,Speech synthesis,Translation",Alibaba,"Yunfei Chu, Jin Xu, Qian Yang, Haojie Wei, Xipin Wei, Zhifang Guo, Yichong Leng, Yuanjun Lv, Jinzheng He, Junyang Lin, Chang Zhou, Jingren Zhou",2024-07-15,Qwen2-Audio Technical Report,https://arxiv.org/abs/2407.10759,,,,8200000000.0,"""The total parameters of Qwen2-Audio is 8.2B parameters.""",,,Unspecified unreleased,,43200000000.0,"Figure 3: Statistics (hours) of pre-training dataset
Speech - 370k hours
Sound - 10K hours
Music - 100k hours

""each frame of the encoder output approximately corresponds to a 40ms
segment of the original audio signal.""

480000*60*60*1000/40 = 43200000000 tokens",,,,,Likely,"We introduce the latest progress of Qwen-Audio, a large-scale audio-language model called Qwen2-Audio, which is capable of accepting various audio signal inputs and performing audio analysis or direct textual responses with regard to speech instructions. In contrast to complex hierarchical tags, we have simplified the pre-training process by utilizing natural language prompts for different data and tasks, and have further expanded the data volume. We have boosted the instruction-following capability of Qwen2-Audio and implemented two distinct audio interaction modes for voice chat and audio analysis. In the voice chat mode, users can freely engage in voice interactions with Qwen2-Audio without text input. In the audio analysis mode, users could provide audio and text instructions for analysis during the interaction. Note that we do not use any system prompts to switch between voice chat and audio analysis modes. Qwen2-Audio is capable of intelligently comprehending the content within audio and following voice commands to respond appropriately. For instance, in an audio segment that simultaneously contains sounds, multi-speaker conversations, and a voice command, Qwen2-Audio can directly understand the command and provide an interpretation and response to the audio. Additionally, DPO has optimized the model's performance in terms of factuality and adherence to desired behavior. According to the evaluation results from AIR-Bench, Qwen2-Audio outperformed previous SOTAs, such as Gemini-1.5-pro, in tests focused on audio-centric instruction-following capabilities. Qwen2-Audio is open-sourced with the aim of fostering the advancement of the multi-modal language community.",,,Open weights (unrestricted),China,Qwen-7B,2,6ND = 6*8200000000*43200000000=2.12544e+21,,,2025-05-01 10:42,,,,,,Industry,,,,,,"https://github.com/QwenLM/Qwen2-Audio
",Industry,,,,,Operation counting,,,,
OmniGenome,Biology,Protein or nucleotide language model (pLM/nLM),University of Exeter,"Heng Yang, Ke Li",2024-07-15,OmniGenome: Aligning RNA Sequences with Secondary Structures in Genomic Foundation Models,https://arxiv.org/abs/2407.11242,1.0,,,186000000.0,,3.3900690258459335e+20,"1. Hardware setup: 8x NVIDIA RTX 4090 GPUs (3.30Ã—10Â¹â´ FLOP/s per GPU)

2. Training duration: 3 weeks (directly provided)
   = 1,814,400 seconds (3 weeks Ã— 7 days Ã— 24 hours Ã— 3600 seconds)

3. Utilization rate: 40%

4. Final calculation:
   3.30Ã—10Â¹â´ FLOP/s Ã— 8 GPUs Ã— 1,814,400 seconds Ã— 0.4 = 1.9Ã—10Â²Â¹ FLOPs

Alternative:
6*54.2B*186M=60487200000000000000

Geometric Mean: 339006902584593356472",,,54200000001.0,"54.2 billion tokens = 54.2 x 10^9 = 54,200,000,000 tokens",,,NVIDIA GeForce RTX 4090,,Likely,"The alignment between RNA sequences and structures in foundation models (FMs) has yet to be thoroughly investigated. Existing FMs have struggled to establish sequence-structure alignment, hindering the free flow of genomic information between RNA sequences and structures. In this study, we introduce OmniGenome, an RNA FM trained to align RNA sequences with respect to secondary structures based on structure-contextualised modelling. The alignment enables free and bidirectional mappings between sequences and structures by utilising the flexible RNA modelling paradigm that supports versatile input and output modalities, i.e., sequence and/or structure as input/output. We implement RNA design and zero-shot secondary structure prediction as case studies to evaluate the Seq2Str and Str2Seq mapping capacity of OmniGenome. Results on the EternaV2 benchmark show that OmniGenome solved 74% of puzzles, whereas existing FMs only solved up to 3% of the puzzles due to the oversight of sequence-structure alignment. We leverage four comprehensive in-silico genome modelling benchmarks to evaluate performance across a diverse set of genome downstream tasks, where the results show that OmniGenome achieves state-of-the-art performance on RNA and DNA benchmarks, even without any training on DNA genomes.",,,,United Kingdom of Great Britain and Northern Ireland,,,,8.0,,2025-05-01 10:42,,,,,,Academia,,,,,,,Academia,,,,7102.366961281501,Hardware,,,,
PARM,Biology,"Protein or nucleotide language model (pLM/nLM),Nucleotide generation","Oncode Institute,UMC Utrecht,Netherlands Cancer Institute,University of Groningen,Radboud University Medical Center","LucÃ­a Barbadilla-MartÃ­nez, Noud Klaassen, VinÃ­cius H. Franceschini-Santos, JÃ©rÃ©mie Breda, Miguel Hernandez-Quiles, Tijs van Lieshout, Carlos G. Urzua TraslaviÃ±a, Hatice YÃ¼cel, Minh Chau Luong Boi, Celia Hermana-Garcia-Agullo, Sebastian Gregoricchio, Wilbert Zwart, Emile Voest, Lude Franke, Michiel Vermeulen, Jeroen de Ridder, Bas van Steensel",2024-07-15,The regulatory grammar of human promoters uncovered by MPRA-trained deep learning,https://www.biorxiv.org/content/10.1101/2024.07.09.602649v2,,,,,,,,,,,,,,NVIDIA RTX 6000 Ada Generation,,Unknown,"One of the major challenges in genomics is to build computational models that accurately predict genome-wide gene expression from the sequences of regulatory elements. At the heart of gene regulation are promoters, yet their regulatory logic is still incompletely understood. Here, we report PARM, a cell-type specific deep learning model trained on specially designed massively parallel reporter assays that query human promoter sequences. PARM requires âˆ¼1,000 times less computational power than state-of-the-art technology, and reliably predicts autonomous promoter activity throughout the genome from DNA sequence alone, in multiple cell types. PARM can even design purely synthetic strong promoters. We leveraged PARM to systematically identify binding sites of transcription factors (TFs) that are likely to contribute to the activity of each natural human promoter. We uncovered and experimentally confirmed striking positional preferences of TFs that differ between activating and repressive regulatory functions, as well as a complex grammar of motif-motif interactions. For example, many, but not all, TFs act as repressors when their binding motif is located near or just downstream of the transcription start site. Our approach lays the foundation towards a deep understanding of the regulation of human promoters by TFs.",10.0,,,Netherlands,,,,1.0,,2025-05-01 10:42,,,,,,Academia,,,,,,,Academia,,,,,,,,,
HelixProtX,Biology,Protein generation,Baidu,"Zhiyuan Chen, Tianhao Chen, Chenggang Xie, Yang Xue, Xiaonan Zhang, Jingbo Zhou, Xiaomin Fang",2024-07-12,"Unifying Sequences, Structures, and Descriptions for Any-to-Any Protein Generation with the Large Multimodal Model HelixProtX",https://arxiv.org/abs/2407.09274,,,,,,,,,,888216576.0,"2168498 instances, 80% training data -> 1734798
estimated tokens per instance 512
1734798*512=888216576",,,NVIDIA A100 SXM4 40 GB,,Confident,"Proteins are fundamental components of biological systems and can be represented through various modalities, including sequences, structures, and textual descriptions. Despite the advances in deep learning and scientific large language models (LLMs) for protein research, current methodologies predominantly focus on limited specialized tasks â€“ often predicting one protein modality from another. These approaches restrict the understanding and generation of multimodal protein data. In contrast, large multimodal models have demonstrated potential capabilities in generating any-to-any content like text, images, and videos, thus enriching user interactions across various domains. Integrating these multimodal model technologies into protein research offers significant promise by potentially transforming how proteins are studied. To this end, we introduce HelixProtX, a system built upon the large multimodal model, aiming to offer a comprehensive solution to protein research by supporting any-to-any protein modality generation. Unlike existing methods, it allows for the transformation of any input protein modality into any desired protein modality. The experimental results affirm the advanced capabilities of HelixProtX, not only in generating functional descriptions from amino acid sequences but also in executing critical tasks such as designing protein sequences and structures from textual descriptions. Preliminary findings indicate that HelixProtX consistently achieves superior accuracy across a range of protein-related tasks, outperforming existing state-of-the-art models. By integrating multimodal large models into protein research, HelixProtX opens new avenues for understanding protein biology, thereby promising to accelerate scientific discovery.",52.0,,Unreleased,China,,,,8.0,,2025-05-01 10:42,,,,,,Industry,,,,,,,Industry,,,BF16,6313.636864775396,Hardware,,,,
Deep learning linking mechanistic models to single-cell transcriptomics data reveals transcriptional bursting in response to DNA damage,Biology,Transcriptomic prediction,"Sun Yat-sen University,University of California Irvine,Guangdong Provincial People's Hospital,Guangdong Academy of Medical Sciences","Zhiwei Huang, Songhao Luo, Zihao Wang, Zhenquan Zhang, Benyuan Jiang, Qing Nie, Jiajun Zhang",2024-07-12,Deep learning linking mechanistic models to single-cell transcriptomics data reveals transcriptional bursting in response to DNA damage,https://www.biorxiv.org/content/10.1101/2024.07.10.602845v1,,,,2176.0,"""Firstly, the input layer contains 5 neurons ""
""Therefore, we identified a single hidden layer of 128 neurons 
and an output layer of 12 neurons as the appropriate model architecture""

Parameter: 5*128+128*12=2176",39168000000.0,2*2176*3*15000*200=39168000000,,,15000.0,,,,,,Confident,"Cells must adopt flexible regulatory strategies to make decisions regarding their fate, including differentiation, apoptosis, or survival in the face of various external stimuli. One key cellular strategy that enables these functions is stochastic gene expression programs. However, understanding how transcriptional bursting, and consequently, cell fate, responds to DNA damage on a genome-wide scale poses a challenge. In this study, we propose an interpretable and scalable inference framework, DeepTX, that leverages deep learning methods to connect mechanistic models and scRNA-seq data, thereby revealing genome-wide transcriptional burst kinetics. This framework enables rapid and accurate solutions to transcription models and the inference of transcriptional burst kinetics from scRNA-seq data. Applying this framework to several scRNA-seq datasets of DNA-damaging drug treatments, we observed that fluctuations in transcriptional bursting induced by different drugs could lead to distinct fate decisions: IdU treatment induces differentiation in mouse embryonic stem cells by increasing the burst size of gene expression, while 5FU treatment with low and high dose increases the burst frequency of gene expression to induce cell apoptosis and survival in human colon cancer cells. Together, these results show that DeepTX can be used to analyze single-cell transcriptomics data and can provide mechanistic insights into cell fate decisions.",200.0,,,China,,,,,,2025-05-01 10:42,,,,,,Academia,,,,,,,Academia,,,,,,,,,
PaliGemma,Vision,Visual question answering,Google DeepMind,"Lucas Beyer, Andreas Steiner, AndrÃ© Susano Pinto, Alexander Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neumann, Ibrahim Alabdulmohsin, Michael Tschannen, Emanuele Bugliarello, Thomas Unterthiner, Daniel Keysers, Skanda Koppula, Fangyu Liu, Adam Grycner, Alexey Gritsenko, Neil Houlsby, Manoj Kumar, Keran Rong, Julian Eisenschlos, Rishabh Kabra, Matthias Bauer, Matko BoÅ¡njak, Xi Chen, Matthias Minderer, Paul Voigtlaender, Ioana Bica, Ivana Balazevic, Joan Puigcerver, Pinelopi Papalampidi, Olivier Henaff, Xi Xiong, Radu Soricut, Jeremiah Harmsen, Xiaohua Zhai",2024-07-10,PaliGemma: A versatile 3B VLM for transfer,https://arxiv.org/abs/2407.07726v1,,,,3000000000.0,"Model Components:
Vision Encoder: SigLIP-So400m, a 400M parameter model pretrained with a contrastive objective.
Language Model: Gemma-2B, a 2B parameter decoder-only model from the Gemma family of LLMs.",1.0652844e+22,197000000000000 FLOP/s *256 GPUs *102 hours *3600 s / hour *0.55 [reported utilization] + 123000000000000 FLOP/s *32 GPUs *60 hours [for transfer] *3600 s / hour *0.55 [reported utilization]= 1.0652844e+22 FLOP,"WebLI,Conceptual Captions (CC3M),Unspecified unreleased","""some of the pretraining datasets remain private""

PaliGemma is pre-trained on the following mixture of datasets:

WebLI: WebLI (Web Language Image) is a web-scale multilingual image-text dataset built from the public web. A wide range of WebLI splits are used to acquire versatile model capabilities, such as visual semantic understanding, object localization, visually-situated text understanding, multilinguality, etc.
CC3M-35L: Curated English image-alt_text pairs from webpages (Sharma et al., 2018). We used the Google Cloud Translation API to translate into 34 additional languages.
VQÂ²A-CC3M-35L/VQG-CC3M-35L: A subset of VQ2A-CC3M (Changpinyo et al., 2022a), translated into the same additional 34 languages as CC3M-35L, using the Google Cloud Translation API.
OpenImages: Detection and object-aware questions and answers (Piergiovanni et al. 2022) generated by handcrafted rules on the OpenImages dataset.
WIT: Images and texts collected from Wikipedia (Srinivasan et al., 2021).",487782604800.0,"""We train Stage1 at resolution 224px (hence, ð‘img = 256 image tokens) and sequence length ð‘txt = 128 for a total of 1 billion examples.""

""For resolution 448, we train for an additional 50 M examples, and for resolution 896, we add another 10 M examples. <..> we also increase the text sequence length to ð‘txt = 512 tokens.""

""Stage1 sees slightly less than 350 B tokens, and both Stage2 combined about 90 B tokens.""

5189 tokens/second/device (reported) * 102*3600*256 = 487782604800 tokens",162.0,"""One training run of the final PaliGemma model using TPUv5e-256 takes slightly less than 3 days for Stage1 and 15h for each Stage2. Stage1 sees
slightly less than 350 B tokens, and both Stage2 combined about 90 B tokens. Transfers take between 20min and 10h on TPUv3-32, depending on the task""

""we transfer the pretrained models on more than 30 academic benchmarks via fine-tuning""

3*24+15*2 = 102 (stage 1 and stage 2 without transfers)
assuming(!) 2 hours on average for transfer, 2*30 = 60 hours","Google TPU v5e,Google TPU v3",,Confident,"PaliGemma is an open Vision-Language Model (VLM) that is based on the SigLIP-So400m vision encoder and the Gemma-2B language model. It is trained to be a versatile and broadly knowledgeable base model that is effective to transfer. It achieves strong performance on a wide variety of open-world tasks. We evaluate PaliGemma on almost 40 diverse tasks including standard VLM benchmarks, but also more specialized tasks such as remote-sensing and segmentation.",,,Open weights (restricted use),"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France",,,,256.0,0.55,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,https://www.kaggle.com/models/google/paligemma,Industry,,"""We measured a model FLOPS utilization (MFU) of 55%, resulting in 5189 tokens/second/device.""",,,Hardware,,,,
OpenDiLoCo 150M,Language,Language modeling/generation,Prime Intellect,"Sami Jaghouar, Jack Min Ong, Johannes Hagemann",2024-07-10,"OpenDiLoCo: An Open-Source Framework for Globally Distributed
Low-Communication Training",https://arxiv.org/abs/2407.07852,,,,150000000.0,"""We conduct various experiments using a model with 150 million parameters on a language modeling task using the C4 dataset (Raffel et al., 2019). """,4.152361e+19,6ND = 6*150*10^6*46137344000 = 4.152361e+19,C4,,46137344000.0,"""The hyperparameters are consistent with DiLoCoacross experiments: an inner learning rate of 4eâˆ’4, 1,000 warm-up steps, 0.1 weight decay, a batch size of 512, a sequence length of 1,024, a learning rate for the Nesterov outer optimizer of 0.7, and Nesterov momentum of 0.9. Similarly, we run the experiments for a total of 88,000 steps.""

512*1024*88000=46137344000",,,,,Confident,"OpenDiLoCo is an open-source implementation and replication of the Distributed Low-Communication (DiLoCo) training method for large language models. We provide a reproducible implementation of the DiLoCo experiments, offering it within a scalable, decentralized training framework using the Hivemind library. We demonstrate its effectiveness by training a model across two continents and three countries, while maintaining 90-95% compute utilization. Additionally, we conduct ablations studies focusing on the algorithm's compute efficiency, scalability in the number of workers and show that its gradients can be all-reduced using FP16 without any performance degradation. Furthermore, we scale OpenDiLoCo to 3x the size of the original work, demonstrating its effectiveness for billion parameter models.",,,,United States of America,,,,,,2025-05-01 10:42,,,,,,Industry,,,,,Open source,"https://github.com/PrimeIntellect-ai/OpenDiLoCo
Apache 2.0",Industry,,"""We demonstrate its effectiveness by training a model across two continents and three countries, while maintaining 90-95% compute utilization.""",,,Operation counting,,,,
OpenDiLoCo 1.1B,Language,Language modeling/generation,Prime Intellect,"Sami Jaghouar, Jack Min Ong, Johannes Hagemann",2024-07-10,"OpenDiLoCo: An Open-Source Framework for Globally Distributed
Low-Communication Training",https://arxiv.org/abs/2407.07852,,,,1100000000.0,"""We adopt the same hyperparameters as TinyLlama (Zhang et al., 2024), employing a model with 1.1B parameters, a learning rate of 4eâˆ’4 and a batch size of 2048.""",6.0901294e+20,6ND = 6*1.1*10^9*92274688000 = 6.0901294e+20,C4,,92274688000.0,"44000 steps

""For the scaled-up 1.1B parameter experiment, we limit it to 44, 000 steps because of the 4Ã— larger batch size. "" 
batch size 2048

sequence length 1024

2048*1024*44000 = 92274688000",,,,,Confident,"OpenDiLoCo is an open-source implementation and replication of the Distributed Low-Communication (DiLoCo) training method for large language models. We provide a reproducible implementation of the DiLoCo experiments, offering it within a scalable, decentralized training framework using the Hivemind library. We demonstrate its effectiveness by training a model across two continents and three countries, while maintaining 90-95% compute utilization. Additionally, we conduct ablations studies focusing on the algorithm's compute efficiency, scalability in the number of workers and show that its gradients can be all-reduced using FP16 without any performance degradation. Furthermore, we scale OpenDiLoCo to 3x the size of the original work, demonstrating its effectiveness for billion parameter models.",,,,United States of America,,,,,,2025-05-01 10:42,,,,,,Industry,,,,,Open source,"https://github.com/PrimeIntellect-ai/OpenDiLoCo
Apache 2.0",Industry,,"""We demonstrate its effectiveness by training a model across two continents and three countries, while maintaining 90-95% compute utilization.""",,,Operation counting,,,,
MAP-Neo,Language,"Language modeling/generation,Question answering,Quantitative reasoning,Translation,Code generation","University of Waterloo,01.AI,Wuhan University","Ge Zhang, Scott Qu, Jiaheng Liu, Chenchen Zhang, Chenghua Lin, Chou Leuang Yu, Danny Pan, Esther Cheng, Jie Liu, Qunshu Lin, Raven Yuan, Tuney Zheng, Wei Pang, Xinrun Du, Yiming Liang, Yinghao Ma, Yizhi Li, Ziyang Ma, Bill Lin, Emmanouil Benetos, Huan Yang, Junting Zhou, Kaijing Ma, Minghao Liu, Morry Niu, Noah Wang, Quehry Que, Ruibo Liu, Sine Liu, Shawn Guo, Soren Gao, Wangchunshu Zhou, Xinyue Zhang, Yizhi Zhou, Yubo Wang, Yuelin Bai, Yuhan Zhang, Yuxiang Zhang, Zenith Wang, Zhenzhu Yang, Zijian Zhao, Jiajun Zhang, Wanli Ouyang, Wenhao Huang, Wenhu Chen",2024-07-10,MAP-Neo: Highly Capable and Transparent Bilingual Large Language Model Series,https://arxiv.org/abs/2405.19327,,,,7000000000.0,7B,1.89e+23,6 FLOP / parameter / token * 7 * 10^9 parameters * 4.5 * 10^12 tokens = 1.89e+23 FLOP,Matrix,"""we introduce Matrix, a bilingual pre-training corpus of 4.5T tokens. Upon its release, Matrix could be the largest transparent LLM pre-training corpus to our best knowledge""",4500000000000.0,4.5T high-quality tokens,,,NVIDIA H800 SXM5,,Confident,"Large Language Models (LLMs) have made great strides in recent years to achieve unprecedented performance across different tasks. However, due to commercial interest, the most competitive models like GPT, Gemini, and Claude have been gated behind proprietary interfaces without disclosing the training details. Recently, many institutions have open-sourced several strong LLMs like LLaMA-3, comparable to existing closed-source LLMs. However, only the model's weights are provided with most details (e.g., intermediate checkpoints, pre-training corpus, and training code, etc.) being undisclosed. To improve the transparency of LLMs, the research community has formed to open-source truly open LLMs (e.g., Pythia, Amber, OLMo), where more details (e.g., pre-training corpus and training code) are being provided. These models have greatly advanced the scientific study of these large models including their strengths, weaknesses, biases and risks. However, we observe that the existing truly open LLMs on reasoning, knowledge, and coding tasks are still inferior to existing state-of-the-art LLMs with similar model sizes. To this end, we open-source MAP-Neo, a highly capable and transparent bilingual language model with 7B parameters trained from scratch on 4.5T high-quality tokens. Our MAP-Neo is the first fully open-sourced bilingual LLM with comparable performance compared to existing state-of-the-art LLMs. Moreover, we open-source all details to reproduce our MAP-Neo, where the cleaned pre-training corpus, data cleaning pipeline, checkpoints, and well-optimized training/evaluation framework are provided. Finally, we hope our MAP-Neo will enhance and strengthen the open research community and inspire more innovations and creativities to facilitate the further improvements of LLMs.",,,Open weights (unrestricted),"Canada,China,China",,,,512.0,,2025-05-30 14:29,,,,,,"Academia,Industry,Academia",,,,,Open source,"Apache 2.0
https://huggingface.co/m-a-p/neo_7b

MIT license for code
https://github.com/multimodal-art-projection/MAP-NEO","Academia,Industry,Academia",,,,707158.8241182694,Operation counting,m-a-p,,,
ColPali,Vision,Character recognition,"Illuin Technology,Equall.ai,University Paris-Saclay,ETH Zurich","Manuel Faysse, Hugues Sibille, Tony Wu, Bilel Omrani, Gautier Viaud, CÃ©line Hudelot, Pierre Colombo",2024-07-05,ColPali: Efficient Document Retrieval with Vision Language Models,https://arxiv.org/abs/2407.01449,,,,3000000000.0,,,,,"Our training dataset of 127,460 query-page pairs is comprised of train sets of openly available academic datasets (63%) and a synthetic dataset made up of pages from web-crawled PDF documents and augmented with VLM-generated (Claude-3 Sonnet) pseudo-questions (37%). Our training set is fully English by design, enabling us to study zero-shot generalization to non-English languages. We explicitly verify no multi-page PDF document is used both ViDoRe and in the train set to prevent evaluation contamination. ",,,,"""a training run represents about
40 hours of Mi250x AMD GPUs. Our experiments, in total, represent 1405 Mi250x GPU hours
from highly efficient compute clusters running on
low-carbon nuclear energy, representing a total of
around 15kg CO2 eq""

""We train on an 8 GPU setup with data parallelism",AMD Radeon Instinct MI250X,,Confident,"Documents are visually rich structures that convey information through text, as well as tables, figures, page layouts, or fonts. While modern document retrieval systems exhibit strong performance on query-to-text matching, they struggle to exploit visual cues efficiently, hindering their performance on practical document retrieval applications such as Retrieval Augmented Generation. To benchmark current systems on visually rich document retrieval, we introduce the Visual Document Retrieval Benchmark ViDoRe, composed of various page-level retrieving tasks spanning multiple domains, languages, and settings. The inherent shortcomings of modern systems motivate the introduction of a new retrieval model architecture, ColPali, which leverages the document understanding capabilities of recent Vision Language Models to produce high-quality contextualized embeddings solely from images of document pages. Combined with a late interaction matching mechanism, ColPali largely outperforms modern document retrieval pipelines while being drastically faster and end-to-end trainable.",1.0,,Open weights (restricted use),"France,United States of America,France,Switzerland",PaliGemma,132364800000000000000,383000000000000.0000 * 8 * 40 * 3600 * 0.3 = 1.323648e+20,8.0,,2025-05-01 10:42,,,,,,"Industry,Industry,Academia,Academia",,,,40.0,Open source,"MIT license + ColPali's vision language backbone model (PaliGemma) is under gemma license as specified in its model card. 
https://huggingface.co/vidore/colpali-v1.2
https://github.com/illuin-tech/colpali","Industry,Industry,Academia,Academia",,,BF16,7893.276433193946,Hardware,,,,
DualNetGO,Biology,Protein function prediction,Hong Kong University of Science and Technology (HKUST),"Zhuoyang Chen,  Qiong Luo",2024-07-03,DualNetGO: A Dual Network Model for Protein Function Prediction via Effective Feature Selection,https://www.biorxiv.org/content/10.1101/2023.11.29.569192v2,0.0,,,82000000.0,,,,,,,,,,NVIDIA GeForce RTX 3090,,Confident,"Motivation Protein-protein Interaction (PPI) networks are crucial for automatically annotating protein functions. As multiple PPI networks exist for the same set of proteins that capture properties from different aspects, it is a challenging task to effectively utilize these heterogeneous networks. Recently, several deep learning models have combined PPI networks from all evidence, or concatenated all graph embeddings for protein function prediction. However, the lack of a judicious selection procedure prevents the effective harness of information from different PPI networks, as these networks vary in densities, structures, and noise levels. Consequently, combining protein features indiscriminately could increase the noise level, leading to decreased model performance.

Results We develop DualNetGO, a dual network model comprised of a classifier and a selector, to predict protein functions by effectively selecting features from different sources including graph embeddings of PPI networks, protein domain and subcellular location information. Evaluation of DualNetGO on human and mouse datasets in comparison with other network-based models show at least 4.5%, 6.2% and 14.2% improvement on Fmax in BP, MF and CC Gene Ontology categories respectively for human, and 3.3%, 10.6% and 7.7% improvement on Fmax for mouse. We demonstrate the generalization capability of our model by training and testing on the CAFA3 data, and show its versatility by incorporating Esm2 embeddings. We further show that our model is insensitive to the choice of graph embedding method and is time- and memory-saving. These results demonstrate that combining a subset of features including PPI networks and protein attributes selected by our model is more effective in utilizing PPI network information than only using one kind of or concatenating graph embeddings from all kinds of PPI networks.

Availability and implementation The source code of DualNetGO and some of the experiment data are available at: https://github.com/georgedashen/DualNetGO.",,,,"Hong Kong,China",,,,1.0,,2025-05-01 10:42,,,,,,Academia,,,,,,,Academia,,,,379.5013460399013,,,,,
Llama-SEA-LION-v2-8B,Language,"Question answering,Chat",AI Singapore,AI Singapore,2024-07-01,Llama-SEA-LION-v2-8B,https://huggingface.co/aisingapore/Llama-SEA-LION-v2-8B,,,,8000000000.0,"""We performed continued pre-training in English and SEA languages on Meta-Llama-3-8B-Instruct, a decoder model using the Llama 3 architecture, to create Llama-SEA-LION-v2-8B"" according to https://huggingface.co/aisingapore/Llama-SEA-LION-v2-8B.",,,"Dolma,Wikipedia",Complete list here: https://huggingface.co/aisingapore/Llama-SEA-LION-v2-8B#data,,"48B tokens but no other, more specific information: https://huggingface.co/aisingapore/Llama-SEA-LION-v2-8B#data",48.0,Training utilized 8 instances of AWS EC2 p5d.24xlarge - 64 Nvidia H100 80GB GPUs - and took 2 days (https://huggingface.co/aisingapore/Llama-SEA-LION-v2-8B).,NVIDIA H100 SXM5 80GB,,Confident,"SEA-LION is a collection of Large Language Models (LLMs) which have been pretrained and instruct-tuned for the Southeast Asia (SEA) region.

Llama-SEA-LION-v2-8B is a multilingual model which has undergone continued pre-training on approximately 48B tokens across 5 SEA languages: English, Indonesia, Tamil, Thai and Vietnamese.

SEA-LION stands for Southeast Asian Languages In One Network.",,,Open weights (restricted use),Singapore,Llama 3-8B,7,"The tensor type is BF16. Training utilized 8 instances of AWS EC2 p5d.24xlarge - 64 Nvidia H100 80GB GPUs - and took 2 days (https://huggingface.co/aisingapore/Llama-SEA-LION-v2-8B).

The Nvidia H100 80GB is the H100 SXM. BFLOAT16 Tensor Core peak FLOPS with sparsity is 1,979 teraFLOPS (https://resources.nvidia.com/en-us-tensor-core/nvidia-tensor-core-gpu-datasheet?ncid=no-ncid).

Assuming 33% utilization rate, training compute is
= 0.33 * 64 GPUs * 1979e12 FLOPS / GPU * 2 days * 86400 s / day
~= 7.22e21 FLOPS",64.0,,2025-05-26 15:48,,,,,,Government,,,,,Open source,"Llama 3 license (branding requirements, size cap 700M MAU)
https://huggingface.co/aisingapore/Llama-SEA-LION-v2-8B

https://github.com/aisingapore/sealion",Government,,,,88412.57127051479,,aisingapore,,,
ChatBit,Language,"Chat,Language modeling/generation","Beijing Institute of Technology,Academy of Military Science,Minzu University of China","Zhang Huaping, Li Chunjin, Wei Shunping, Geng Guotong, Li Weiwei, Li Yugang",2024-06-28,Large language model-driven open-source intelligence cognition,https://d.wanfangdata.com.cn/periodical/gfkj202403008,,,,13000000000.0,Assuming as in LLaMA-13 since it is its finetune,,,,,100000000.0,"""researchers noted that its model incorporated only 100,000 military dialogue records, a relatively small number compared with other LLMs.""

""å…¶ä»–è®¾ç½®ï¼šè®­ç»ƒçš„epochä¸º3ï¼Œæœ€å¤§é•¿åº¦ max_lengthä¸º2 048ï¼Œbatch_sizeä¸º8ï¼Œä½¿ç”¨8ä¸ª A100 GPUè®­ç»ƒï¼Œæ¢¯åº¦ç§¯ç´¯ä¸º4ï¼Œwarm_upä¸ºè®­ ç»ƒæ€»æ­¥æ•°çš„10%ï¼Œé‡‡ç”¨AdamWä¼˜åŒ–å™¨ï¼Œï¢ 1 = 0.90ã€ï¢ 2 = 0.95ï¼Œå­¦ä¹ çŽ‡ä¸º1e-5"" 

which is translated as ""Other settings: 3 epochs for training, max_length of 2 048, batch_size of 8, 8 A100 GPUs for training, gradient accumulation of 4, warm_up of 10% of the total number of training steps, AdamW optimizer, ï¢ 1 = 0.90, ï¢ 2 = 0.95, and learning rate of 1e-5.""

Assuming (!) average dialogue is 1000 tokens (the example in Table 2 has little bit less than 500 characters and they mention maximum sequence length as 2048) then total dataset token count is
100000*1000 = 100000000 tokens",,,NVIDIA A100,,Likely,"Three academic papers and several analysts have confirmed the information, with ChatBIT using Metaâ€™s Llama 13B large language model (LLM). This LLM has been modified for intelligence gathering and processing, allowing military planners to use it for operational decision-making.

With the extensive application of open-source intelligence in the military field, the demand for cognition and analysis of relevant intelligence is growing. However, the large language models currently used by researchers are prone to severe hallucination, rendering the information generated unreliable address the present study collected approximately 100,000 dialogue records online and constructed an open-source military intelligence dataset. Subsequently, a new model, ChatBIT, which is specifically optimized for dialogue and question answering tasks in the military field, This study further compared the military knowledge question answering capabilities of the ChatBIT model with those of the Vicuna-13B model. ChatBIT was found to outperform Vicuna-13B in a series of standardized evaluation metrics including the BLEU score, ROUGE-1, ROUGE and ROUGE-L scores were respectively 3.2079,2.2562, and 1.5939 points higher than those of Vicuna-13B.These results indicate that the ChatBIT model provides more accurate and reliable information when dealing with military dialogue and questions ans. Specifically, ChatBIT's BLEU score was 2.3909 higher than that of Vicuna-13B. Furthermore, ChatBIT's ROUGE-1, ROUGE-2, and ROUGE-L scores were respectively 3.2079,2.2562, Specifically, ChatBIT's BLEU score was 2.3909 higher than that of Vicuna-13B. Furthermore, ChatBIT's ROUGE-1, ROUGE-2, and ROUGE-L scores were respectively 3.2079,2.2562",3.0,,Unreleased,"China,China,China",LLaMA-13B,23400000000000000000,6ND = 6*13*10^9*100000000*3 = 2.34e+19,8.0,,2025-05-01 10:42,,,,16384.0,"max_length of 2 048, batch_size of 8","Academia,Government,Academia",,,,,Unreleased,,"Academia,Government,Academia",,,,6315.605581782233,Operation counting,,,,
Molecular Diffusion Models with Virtual Receptors,Biology,Drug discovery,Verily Research,"Matan Halfon, Eyal Rozenberg, Ehud Rivlin, Daniel Freedman",2024-06-26,Molecular Diffusion Models with Virtual Receptors,https://arxiv.org/abs/2406.18330,0.0,,,,,,,,,132863.0,"""The refined datasets are of size 132,863 (train) and 63,929 (validation).""",73.0,,,,Confident,"Machine learning approaches to Structure-Based Drug Design (SBDD) have proven quite fertile over the last few years. In particular, diffusion-based approaches to SBDD have shown great promise. We present a technique which expands on this diffusion approach in two crucial ways. First, we address the size disparity between the drug molecule and the target/receptor, which makes learning more challenging and inference slower. We do so through the notion of a Virtual Receptor, which is a compressed version of the receptor; it is learned so as to preserve key aspects of the structural information of the original receptor, while respecting the relevant group equivariance. Second, we incorporate a protein language embedding used originally in the context of protein folding. We experimentally demonstrate the contributions of both the virtual receptors and the protein embeddings: in practice, they lead to both better performance, as well as significantly faster computations.",1000.0,,,United States of America,,,,,,2025-05-01 10:42,,,,,,Industry,,,,,,,Industry,,,,,,,,,
ESM3 (98B),Biology,Protein generation,"EvolutionaryScale,University of California (UC) Berkeley","Thomas Hayes, Roshan Rao, Halil Akin, Nicholas James Sofroniew, Deniz Oktay, Zeming Lin, Robert Verkuil, Vincent Quy Tran, Jonathan Deaton, Marius Wiggert, Rohil Badkundri, Irhum Shafkat, Jun Gong, Alexander Derry, Raul Santiago Molina, Neil Thomas, Yousuf Khan, Chetan Mishra, Carolyn Kim, Liam J Bartie, Patrick D Hsu, Tom Sercu, Salvatore Candido, Alexander Rives",2024-06-25,ESM3: Simulating 500 million years of evolution with a language model,https://www.evolutionaryscale.ai/blog/esm3-release ,,Historical significance,"Largest (in compute) biology and protein model to date, was able to discover novel green fluorescent proteins",98500000000.0,98.5 billion (Table S1),1.07e+24,"""ESM3 at its largest scale was trained with 1.07Ã—10^24 FLOPs on 2.78 billion proteins and 771 billion unique tokens, and has 98 billion parameters.""

per Table 1, trained 98B model on 1.8T training tokens. 98 billion * 1800 billion * 6 = 1.06e24. Likely some rounding, so will go with developer's reported count.",ESM3 Dataset,,771000000000.0, 771 billion tokens,,,,,Confident,"More than three billion years of evolution have
produced an image of biology encoded into the
space of natural proteins. Here we show that language models trained on tokens generated by evolution can act as evolutionary simulators to generate functional proteins that are far away from
known proteins. We present ESM3, a frontier
multimodal generative language model that reasons over the sequence, structure, and function
of proteins. ESM3 can follow complex prompts
combining its modalities and is highly responsive
to biological alignment. We have prompted ESM3
to generate fluorescent proteins with a chain of
thought. Among the generations that we synthesized, we found a bright fluorescent protein at far
distance (58% identity) from known fluorescent
proteins. Similarly distant natural fluorescent proteins are separated by over five hundred million
years of evolution,

(from paper preview: https://evolutionaryscale-public.s3.us-east-2.amazonaws.com/research/esm3.pdf )",2.3,,Unreleased,"United States of America,United States of America",,,,,,2025-05-01 10:42,,,,4194304.0,Table S1,"Industry,Academia",,,,,Unreleased,only small version released,"Industry,Academia",,,,,Reported,,,,
ESM3-open-small,Biology,Protein generation,"EvolutionaryScale,University of California (UC) Berkeley","Thomas Hayes, Roshan Rao, Halil Akin, Nicholas James Sofroniew, Deniz Oktay, Zeming Lin, Robert Verkuil, Vincent Quy Tran, Jonathan Deaton, Marius Wiggert, Rohil Badkundri, Irhum Shafkat, Jun Gong, Alexander Derry, Raul Santiago Molina, Neil Thomas, Yousuf Khan, Chetan Mishra, Carolyn Kim, Liam J Bartie, Patrick D Hsu, Tom Sercu, Salvatore Candido, Alexander Rives",2024-06-25,ESM3: Simulating 500 million years of evolution with a language model,https://www.evolutionaryscale.ai/blog/esm3-release ,,,,1400000000.0,1.4 billion,2.7e+21,"see Table S1. Not clear which 1.4B model is the one that they released, could be either 6.7e20 or 2.7e21.",,,320000000000.0,Table S1,,,,,Confident,"More than three billion years of evolution have
produced an image of biology encoded into the
space of natural proteins. Here we show that language models trained on tokens generated by evolution can act as evolutionary simulators to generate functional proteins that are far away from
known proteins. We present ESM3, a frontier
multimodal generative language model that reasons over the sequence, structure, and function
of proteins. ESM3 can follow complex prompts
combining its modalities and is highly responsive
to biological alignment. We have prompted ESM3
to generate fluorescent proteins with a chain of
thought. Among the generations that we synthesized, we found a bright fluorescent protein at far
distance (58% identity) from known fluorescent
proteins. Similarly distant natural fluorescent proteins are separated by over five hundred million
years of evolution,

(from paper preview: https://evolutionaryscale-public.s3.us-east-2.amazonaws.com/research/esm3.pdf )",,,Open weights (non-commercial),"United States of America,United States of America",,,,,,2025-05-01 10:42,,,,,,"Industry,Academia",,,,,Unreleased,"open code and weights with non-commercial license. probably just inference code
https://github.com/evolutionaryscale/esm/tree/main ","Industry,Academia",,,,,Reported,,,,
Flexi-JEST++,Vision,Image classification,Google DeepMind,"Talfan Evans, Nikhil Parthasarathy, Hamza Merzic, Olivier J. Henaff",2024-06-25,Data curation via joint example selection further accelerates multimodal learning,https://arxiv.org/abs/2406.17711v1,,,,,,1.26e+21,"1.26*10^21 from the figure 1

there is slight disrepancy (within confidence interval) between sigclip/flexi-jest++ training compute ratio if we compare epoch's estimated compute and table 1",WebLI,"""We train the learner model in all JEST experiments on the WebLI dataset, specifically a billion-scale subset of English image-text pairs loosely filtered with image-text
alignment ""

""We additionally explore scaling data curation (JEST++/FlexiJEST++) using
reference models trained on â€œWebLI-curated++â€ which adds approximately 600M additional webscraped image-text pairs filtered with the same strong curation pipeline.""",1280000000000.0,"4b training examples seen (figure 1)

The vision encoder takes images resized to (256 x 256) and the text-encoder tokenizes text with the sentencepiece tokenizer [26] trained on the English C4 dataset [39]. We crop the text to the first 64 tokens.

visual encoder: ViT-B/16

4*10^9*((256/16)^2 + 64) = 1.28e+12 tokens (text and image)",,,Google TPU v5e,,Confident,"Data curation is an essential component of large-scale pretraining. In this work, we demonstrate that jointly selecting batches of data is more effective for learning than selecting examples independently. Multimodal contrastive objectives expose the dependencies between data and thus naturally yield criteria for measuring the joint learnability of a batch. We derive a simple and tractable algorithm for selecting such batches, which significantly accelerate training beyond individually-prioritized data points. As performance improves by selecting from larger super-batches, we also leverage recent advances in model approximation to reduce the associated computational overhead. As a result, our approach--multimodal contrastive learning with joint example selection (JEST)--surpasses state-of-the-art models with up to 13Ã— fewer iterations and 10Ã— less computation. Essential to the performance of JEST is the ability to steer the data selection process towards the distribution of smaller, well-curated datasets via pretrained reference models, exposing the level of data curation as a new dimension for neural scaling laws.",,,Unreleased,"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France",,,,256.0,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,,Industry,,,,70739.50833122246,Other,,,,
JEST++,Vision,Image classification,Google DeepMind,"Talfan Evans, Nikhil Parthasarathy, Hamza Merzic, Olivier J. Henaff",2024-06-25,Data curation via joint example selection further accelerates multimodal learning,https://arxiv.org/abs/2406.17711v1,,,,,,1.9e+21,"1.9*10^21 from figure 1 and  table 1

there is slight disrepancy (within confidence interval) between sigclip/jest++ training compute ratio if we compare epoch's estimated compute and table 1",WebLI,"""We train the learner model in all JEST experiments on the WebLI dataset, specifically a billion-scale subset of English image-text pairs loosely filtered with image-text
alignment ""

""We additionally explore scaling data curation (JEST++/FlexiJEST++) using
reference models trained on â€œWebLI-curated++â€ which adds approximately 600M additional webscraped image-text pairs filtered with the same strong curation pipeline.""",1280000000000.0,"4b training examples seen (figure 1)

The vision encoder takes images resized to (256 x 256) and the text-encoder tokenizes text with the sentencepiece tokenizer [26] trained on the English C4 dataset [39]. We crop the text to the first 64 tokens.

visual encoder: ViT-B/16

4*10^9*((256/16)^2 + 64) = 1.28e+12 tokens (text and image)",,,Google TPU v5e,,Confident,"Data curation is an essential component of large-scale pretraining. In this work, we demonstrate that jointly selecting batches of data is more effective for learning than selecting examples independently. Multimodal contrastive objectives expose the dependencies between data and thus naturally yield criteria for measuring the joint learnability of a batch. We derive a simple and tractable algorithm for selecting such batches, which significantly accelerate training beyond individually-prioritized data points. As performance improves by selecting from larger super-batches, we also leverage recent advances in model approximation to reduce the associated computational overhead. As a result, our approach--multimodal contrastive learning with joint example selection (JEST)--surpasses state-of-the-art models with up to 13Ã— fewer iterations and 10Ã— less computation. Essential to the performance of JEST is the ability to steer the data selection process towards the distribution of smaller, well-curated datasets via pretrained reference models, exposing the level of data curation as a new dimension for neural scaling laws.",,,Unreleased,"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France",,,,256.0,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,,Industry,,,,70739.50833122246,Other,,,,
JEST-L++,Vision,Image classification,DeepMind,"Talfan Evans, Nikhil Parthasarathy, Hamza Merzic, Olivier J. Henaff",2024-06-25,Data curation via joint example selection further accelerates multimodal learning,https://arxiv.org/abs/2406.17711v1,18.0,,,,,2e+21,See the right subfigure of Figure 1 on page 2 of https://arxiv.org/pdf/2406.17711v1.,WebLI,"""We train the learner model in all JEST experiments on the WebLI dataset... To train reference models, we use smaller high-quality datasets. JEST/Flexi-JEST reference models are trained on a strongly filtered 100M scale subset of WebLI filtered for high text and image quality and image-text alignment, which we refer to as â€œWebLI-curatedâ€. We additionally explore scaling data curation (JEST++/FlexiJEST++) using reference models trained on â€œWebLI-curated++â€ which adds approximately 600M additional webscraped image-text pairs filtered with the same strong curation pipeline,"" according to pages 4 and 5 of https://arxiv.org/pdf/2406.17711v1.",3000000000.0,"""Our default training configuration follows that of SigLIP [54], with a ViT-B/16 and Bert-B image-text dual encoder, training on WebLI for 3 billion examples with a batch size of 32k and the sigmoid-contrastive loss...  We split training across 256 TPUv5e chips,"" according to page 14 of https://arxiv.org/pdf/2406.17711v1.",,,Google TPU v5e,,Confident,"Data curation is an essential component of large-scale pretraining. In this work, we demonstrate that jointly selecting batches of data is more effective for learning than selecting examples independently. Multimodal contrastive objectives expose the dependencies between data and thus naturally yield criteria for measuring the joint learnability of a batch. We derive a simple and tractable algorithm for selecting such batches, which significantly accelerate training beyond individually-prioritized data points. As performance improves by selecting from larger super-batches, we also leverage recent advances in model approximation to reduce the associated computational overhead. As a result, our approach--multimodal contrastive learning with joint example selection (JEST)--surpasses state-of-the-art models with up to 13Ã— fewer iterations and 10Ã— less computation. Essential to the performance of JEST is the ability to steer the data selection process towards the distribution of smaller, well-curated datasets via pretrained reference models, exposing the level of data curation as a new dimension for neural scaling laws.",,,Unreleased,United Kingdom of Great Britain and Northern Ireland,,,,256.0,,2025-06-16 10:56,,,,32000.0,,Industry,,,,,Unreleased,"(a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [No] We do not explicitly provide the code for our experiments, however we detail in pseudocode the main components of our method (see algorithms 1 and A.1). Our model implementations and base experimental configuration were adopted from the open source big_vision codebase [3]. We do not provide the reference or downstream training datasets, but we provide ablations (see Experiments Section) on the open source LAION dataset.

(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] We describe and analyse experimentally the hyperparameters that relate specifically to our methods. We provide pseudocode for our method in the main text and Appendix, and describe all our hyperparameters. All other parameters are adopted from SigLIP pretraining in the big_vision codebase [3, 52].",Industry,,,,70739.50833122246,,,,,
Gemma 2 9B,Language,"Language modeling/generation,Chat,Code generation,Question answering,Quantitative reasoning",Google DeepMind,"Gemma Team, Google DeepMind",2024-06-24,"Gemma 2 offers best-in-class performance, runs at incredible speed across different hardware and easily integrates with other AI tools.",https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf,,,,9000000000.0,,4.32e+23,"""For the 9B model, we train on an 8x16x32 configuration of TPUv4, totaling 4096 chips""

6ND = 6 FLOP / token / parameter * 9000000000 parameters * 8000000000000 tokens = 4.32e+23 FLOP",Unspecified unreleased,"Web Documents: A diverse collection of web text ensures the model is exposed to a broad range of linguistic styles, topics, and vocabulary. Primarily English-language content.
Code: Exposing the model to code helps it to learn the syntax and patterns of programming languages, which improves its ability to generate code or understand code-related questions.
Mathematics: Training on mathematical text helps the model learn logical reasoning, symbolic representation, and to address mathematical queries.",8000000000000.0,"""the 9B model on 8 trillion tokens""",,,Google TPU v4,,Confident,"Now weâ€™re officially releasing Gemma 2 to researchers and developers globally. Available in both 9 billion (9B) and 27 billion (27B) parameter sizes, Gemma 2 is higher-performing and more efficient at inference than the first generation, with significant safety advancements built in. In fact, at 27B, it offers competitive alternatives to models more than twice its size, delivering the kind of performance that was only possible with proprietary models as recently as December. And thatâ€™s now achievable on a single NVIDIA H100 Tensor Core GPU or TPU host, significantly reducing deployment costs.",,,Open weights (restricted use),"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France",,,,4096.0,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,"Gemma 2 is available under our 
commercially-friendly Gemma license, giving developers and researchers the ability to share and commercialize their innovations.",Industry,,,,1374398.1970781134,Operation counting,google,,,
Gemma 2 27B,Language,"Language modeling/generation,Chat,Code generation,Question answering,Quantitative reasoning",Google DeepMind,"Gemma Team, Google DeepMind",2024-06-24,"Gemma 2 offers best-in-class performance, runs at incredible speed across different hardware and easily integrates with other AI tools.",https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf,,,,27000000000.0,,2.106e+24,"""For the 27B model, we train on an 8x24x32 configuration of
TPUv5p, totaling 6144 chips""

trained on 13T tokens

6ND = 6*27000000000*13000000000000=2.106e+24",Unspecified unreleased,"Web Documents: A diverse collection of web text ensures the model is exposed to a broad range of linguistic styles, topics, and vocabulary. Primarily English-language content.
Code: Exposing the model to code helps it to learn the syntax and patterns of programming languages, which improves its ability to generate code or understand code-related questions.
Mathematics: Training on mathematical text helps the model learn logical reasoning, symbolic representation, and to address mathematical queries.",13000000000000.0,"""We train Gemma 2 27B on 13 trillion tokens of primarily-English data""",,,Google TPU v5p,,Confident,"Now weâ€™re officially releasing Gemma 2 to researchers and developers globally. Available in both 9 billion (9B) and 27 billion (27B) parameter sizes, Gemma 2 is higher-performing and more efficient at inference than the first generation, with significant safety advancements built in. In fact, at 27B, it offers competitive alternatives to models more than twice its size, delivering the kind of performance that was only possible with proprietary models as recently as December. And thatâ€™s now achievable on a single NVIDIA H100 Tensor Core GPU or TPU host, significantly reducing deployment costs.",,,Open weights (restricted use),"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France",,,,6144.0,,2025-05-01 20:09,,,,,,Industry,,,,,Unreleased,"Gemma 2 is available under our 
commercially-friendly Gemma license, giving developers and researchers the ability to share and commercialize their innovations.",Industry,,,,,Operation counting,google,,,
Gemma 2 2B,Language,"Language modeling/generation,Chat,Code generation,Question answering",Google DeepMind,"Gemma Team, Google DeepMind",2024-06-24,"Gemma 2 offers best-in-class performance, runs at incredible speed across different hardware and easily integrates with other AI tools.",https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf,,,,2600000000.0,,3.12e+22,"""For the 2.6B model, we train on a 2x16x16 configuration of TPUv5e, totaling 512 chips""

6ND = 6 FLOP / token / parameter * 2600000000 parameters * 2000000000000 tokens = 3.12e+22 FLOP",Unspecified unreleased,"Web Documents: A diverse collection of web text ensures the model is exposed to a broad range of linguistic styles, topics, and vocabulary. Primarily English-language content.
Code: Exposing the model to code helps it to learn the syntax and patterns of programming languages, which improves its ability to generate code or understand code-related questions.
Mathematics: Training on mathematical text helps the model learn logical reasoning, symbolic representation, and to address mathematical queries.",2000000000000.0,"""the 2.6B on 2 trillion tokens""",,,Google TPU v5e,,Confident,"Now weâ€™re officially releasing Gemma 2 to researchers and developers globally. Available in both 9 billion (9B) and 27 billion (27B) parameter sizes, Gemma 2 is higher-performing and more efficient at inference than the first generation, with significant safety advancements built in. In fact, at 27B, it offers competitive alternatives to models more than twice its size, delivering the kind of performance that was only possible with proprietary models as recently as December. And thatâ€™s now achievable on a single NVIDIA H100 Tensor Core GPU or TPU host, significantly reducing deployment costs.",,,Open weights (restricted use),"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France",,,,512.0,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,"https://huggingface.co/google/gemma-2-2b

Gemma 2 is available under our 
commercially-friendly Gemma license, giving developers and researchers the ability to share and commercialize their innovations.",Industry,,,,141482.16734627637,Operation counting,,,,
BADGER,Biology,Drug discovery,"NVIDIA,University of California (UC) Berkeley","Yue Jian, Curtis Wu, Danny Reidenbach, Aditi S. Krishnapriyan",2024-06-24,General Binding Affinity Guidance for Diffusion Models in Structure-Based Drug Design,https://arxiv.org/abs/2406.16821,1.0,,,2900000.0,2.9M transformer model,,,,,100001.0,"100,000 (training) + 100 (testing) = 100,100 unique datapoints
20 epochs mentioned but only unique datapoints counted
Final estimate: 1.0e5",,,NVIDIA RTX 6000 Ada Generation,,Likely,"Structure-Based Drug Design (SBDD) focuses on generating valid ligands that strongly and specifically bind to a designated protein pocket. Several methods use machine learning for SBDD to generate these ligands in 3D space, conditioned on the structure of a desired protein pocket. Recently, diffusion models have shown success here by modeling the underlying distributions of atomic positions and types. While these methods are effective in considering the structural details of the protein pocket, they often fail to explicitly consider the binding affinity. Binding affinity characterizes how tightly the ligand binds to the protein pocket, and is measured by the change in free energy associated with the binding process. It is one of the most crucial metrics for benchmarking the effectiveness of the interaction between a ligand and protein pocket. To address this, we propose BADGER: Binding Affinity Diffusion Guidance with Enhanced Refinement. BADGER is a general guidance method to steer the diffusion sampling process towards improved protein-ligand binding, allowing us to adjust the distribution of the binding affinity between ligands and proteins. Our method is enabled by using a neural network (NN) to model the energy function, which is commonly approximated by AutoDock Vina (ADV). ADVâ€™s energy function is non-differentiable, and estimates the affinity based on the interactions between a ligand and target protein receptor. By using a NN as a differentiable energy function proxy, we utilize the gradient of our learned energy function as a guidance method on top of any trained diffusion model. We show that our method improves the binding affinity of generated ligands to their protein receptors by up to 60%, significantly surpassing previous machine learning methods. We also show that our guidance method is flexible and can be easily applied to other diffusion-based SBDD frameworks.",20.0,,,"United States of America,United States of America",,,,1.0,,2025-05-01 10:42,,,,,,"Industry,Academia",,,,,,,"Industry,Academia",,,,,,,,,
Cambrian-1-34B,"Multimodal,Vision,Language","Image captioning,Visual question answering,Character recognition",New York University (NYU),"Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, Ziteng Wang, Rob Fergus, Yann LeCun, Saining Xie",2024-06-24,"Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs
",https://arxiv.org/abs/2406.16860,288.0,SOTA improvement,"Cambrian-1 outperforms other open source models and achieves competitive performance on a number of benchmarks, compared to proprietary models such as GPT-4V, Gemini, and Grok-1.5. ",34000000000.0,,,,"Cambrian-Alignment,Cambrian-10M","links to datasets: 
https://huggingface.co/datasets/nyu-visionx/Cambrian-Alignment
https://huggingface.co/datasets/nyu-visionx/Cambrian-10M",,,,Our final Cambrian-1 models are trained in less than 4 days on a TPU-V4-512.,Google TPU v4,,Confident,"We introduce Cambrian-1, a family of multimodal LLMs (MLLMs) designed with a vision-centric approach. While stronger language models can enhance multimodal capabilities, the design choices for vision components are often insufficiently explored and disconnected from visual representation learning research. This gap hinders accurate sensory grounding in real-world scenarios. Our study uses LLMs and visual instruction tuning as an interface to evaluate various visual representations, offering new insights into different models and architectures -- self-supervised, strongly supervised, or combinations thereof -- based on experiments with over 20 vision encoders. We critically examine existing MLLM benchmarks, address the difficulties involved in consolidating and interpreting results from various tasks, and introduce a new vision-centric benchmark, CV-Bench. To further improve visual grounding, we propose the Spatial Vision Aggregator (SVA), a dynamic and spatially-aware connector that integrates high-resolution vision features with LLMs while reducing the number of tokens. Additionally, we discuss the curation of high-quality visual instruction-tuning data from publicly available sources, emphasizing the importance of data source balancing and distribution ratio. Collectively, Cambrian-1 not only achieves state-of-the-art performance but also serves as a comprehensive, open cookbook for instruction-tuned MLLMs. We provide model weights, code, supporting tools, datasets, and detailed instruction-tuning and evaluation recipes. We hope our release will inspire and accelerate advancements in multimodal systems and visual representation learning.",,,Open weights (unrestricted),United States of America,Yi-34B,,,512.0,,2025-06-01 16:42,,,,,,Academia,,,,,Open source,"Apache 2.0
https://huggingface.co/nyu-visionx/cambrian-34b

Apache 2.0
https://github.com/cambrian-mllm/cambrian",Academia,,,FP32,171799.77463476418,,yu-visionx,,,
Code Droid,Language,"Code generation,Code autocompletion",Factory,,2024-06-22,Code Droid: A Technical Report,https://www.factory.ai/news/code-droid-technical-report,,,,,,,,,,,,,,,,Unknown,"Code Droid is designed to execute coding tasks based on natural language instructions. For our customers, its primary function is to automate rote, tedious programming tasks. Common use cases include codebase modernization, feature development, proof-of-concept creation, and building integrations. Here we outline some of the core functionality that weâ€™ve developed to enable it to perform reliably in production environments.",,,Hosted access (no API),United States of America,,,,,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
Claude 3.5 Sonnet,"Multimodal,Language,Vision","Chat,Image captioning,Code generation,Language modeling/generation",Anthropic,,2024-06-20,Claude 3.5 Sonnet,https://www-cdn.anthropic.com/fed9cc193a14b84131812372d8d5857f8f304c52/Model_Card_Claude_3_Addendum.pdf,,"Significant use,SOTA improvement","""It also sets new performance standards in evaluations of graduate
level science knowledge (GPQA) [1], general reasoning (MMLU) [2], and coding proficiency (HumanEval)
[3].""",,,2.700000000000001e+25,"Blog post by Dario Amodei includes some info on 3.5 Sonnet compute: https://darioamodei.com/on-deepseek-and-export-controls
""Claude 3.5 Sonnet is a mid-sized model that cost a few $10M's to train (I won't give an exact number). Also, 3.5 Sonnet was not trained in any way that involved a larger or more expensive model (contrary to some rumors).""

Using assumptions about GPU pricing, this lets us estimate compute. https://docs.google.com/spreadsheets/d/1-p-ab6t6dkUM6T7GwnFp85ePTMpZMW7LFY7fW2t8POs/",Unspecified unreleased,Training data cutoff Apr 2024,,,,,,,Speculative,,,,API access,United States of America,,,,,,2025-05-16 10:30,,,,,,Industry,,,,,Unreleased,,Industry,,,,,Cost,,,,3
RNA-FrameFlow,Biology,RNA design,"National University of Singapore,Prescient Design,University of Missouri,University of Cambridge","Rishabh Anand, Chaitanya K. Joshi, Alex Morehead, Arian R. Jamasb, Charles Harris, Simon V. Mathis, Kieran Didi, Bryan Hooi, Pietro LiÃ²",2024-06-19,RNA-FrameFlow: Flow Matching for de novo 3D RNA Backbone Design,https://arxiv.org/abs/2406.13839,1.0,,,16800000.0,,3.6889344e+18,4*35580000000000*0.4*18hours=3688934400000000000,,,638001.0,"6,382 sequences = 5,319 (primary) + 1,063 (augmented)
638,200 datapoints = 6,382 sequences Ã— 100 nucleotides/sequence

Final estimate: 638,200 datapoints",18.0,,NVIDIA GeForce RTX 3090,,Confident,,,,,"Singapore,United States of America,United States of America,United Kingdom of Great Britain and Northern Ireland",,,,4.0,,2025-05-01 10:42,,,,,,"Academia,Industry,Academia,Academia",,,,,,,"Academia,Industry,Academia,Academia",,,,2763.6312855065025,Hardware,,,,
MPNNsol,Biology,Protein generation,"Ecole Polytechnique FÂ´edÂ´erale de Lausanne (EPFL),University at Buffalo,University of Washington,Massachusetts Institute of Technology (MIT)","Casper A. Goverde, Martin Pacesa, Nicolas Goldbach, Lars J. Dornfeld, Petra E. M. Balbi, Sandrine Georgeon, StÃ©phane Rosset, Srajan Kapoor, Jagrity Choudhury, Justas Dauparas, Christian Schellhaas, Simon Kozlov, David Baker, Sergey Ovchinnikov, Alex J. Vecchio, Bruno E. Correia",2024-06-19,Computational design of soluble and functional membrane protein analogues,https://www.nature.com/articles/s41586-024-07601-y#Sec30,,,,,,,,PDB (Protein Data Bank),"The MPNNsol model was trained on protein assemblies in the PDB 
(as of 2 August 2021) determined by X-ray crystallography or cryo-EM to a resolution of better than 3.5 Ã… and with fewer than 10,000 residue",,,,,,,Unknown,"De novo design of complex protein folds using solely computational means remains a substantial challenge1. Here we use a robust deep learning pipeline to design complex folds and soluble analogues of integral membrane proteins. Unique membrane topologies, such as those from G-protein-coupled receptors2, are not found in the soluble proteome, and we demonstrate that their structural features can be recapitulated in solution. Biophysical analyses demonstrate the high thermal stability of the designs, and experimental structures show remarkable design accuracy. The soluble analogues were functionalized with native structural motifs, as a proof of concept for bringing membrane protein functions to the soluble proteome, potentially enabling new approaches in drug discovery. In summary, we have designed complex protein topologies and enriched them with functionalities from membrane proteins, with high experimental success rates, leading to a de facto expansion of the functional soluble fold space.",,,Open weights (unrestricted),"Switzerland,United States of America,United States of America,United States of America","ProteinMPNN,AlphaFold 2",,,,,2025-05-01 10:42,,,,,,"Academia,Academia,Academia,Academia",,,,,Unreleased,"https://github.com/dauparas/ProteinMPNN/tree/main/soluble_model_weights

MIT license","Academia,Academia,Academia,Academia",,,,,,,,,
GLM-4V-9B,"Language,Multimodal,Vision","Language modeling/generation,Code generation,Question answering,Translation,Visual question answering","Zhipu AI,Tsinghua University","Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Diego Rojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, Hao Yu, Hongning Wang, Jiadai Sun, Jiajie Zhang, Jiale Cheng, Jiayi Gui, Jie Tang, Jing Zhang, Juanzi Li, Lei Zhao, Lindong Wu, Lucen Zhong, Mingdao Liu, Minlie Huang, Peng Zhang, Qinkai Zheng, Rui Lu, Shuaiqi Duan, Shudan Zhang, Shulin Cao, Shuxun Yang, Weng Lam Tam, Wenyi Zhao, Xiao Liu, Xiao Xia, Xiaohan Zhang, Xiaotao Gu, Xin Lv, Xinghan Liu, Xinyi Liu, Xinyue Yang, Xixuan Song, Xunkai Zhang, Yifan An, Yifan Xu, Yilin Niu, Yuantao Yang, Yueyan Li, Yushi Bai, Yuxiao Dong, Zehan Qi, Zhaoyu Wang, Zhen Yang, Zhengxiao Du, Zhenyu Hou, Zihan Wang",2024-06-18,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools,https://arxiv.org/abs/2406.12793,,,,9000000000.0,9B,,"6ND = 6*9000000000*10000000000000=54*10^(9+13)=5.4*10^23

""GLM-4-9B is pre-trained on approximately ten trillion tokens of multilingual corpus with a context length of 8192 (8K) and post-trained with the same pipeline and data used for GLM-4 (0520)"".
",,"""To date, the GLM-4 models are
pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage""",10000000000000.0,"""GLM-4-9B is pre-trained on approximately ten trillion tokens of multilingual corpus with a context length of 8192 (8K) and post-trained with the same pipeline and data used for GLM-4 (0520)"".",,,,,Likely,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",,,Open weights (non-commercial),"China,China",,,,,,2025-05-09 15:56,,,,,,"Industry,Academia",,5.42,,,,https://github.com/THUDM/GLM-4/blob/main/README_en.md,"Industry,Academia",,,,,Operation counting,,,,
DeepSeek-Coder-V2 236B,Language,"Code generation,Code autocompletion",DeepSeek,"Qihao Zhu, Daya Guo, Zhihong Shao, Dejian Yang, Peiyi Wang, Runxin Xu, Y. Wu, Yukun Li, Huazuo Gao, Shirong Ma, Wangding Zeng, Xiao Bi, Zihui Gu, Hanwei Xu, Damai Dai, Kai Dong, Liyue Zhang, Yishi Piao, Zhibin Gou, Zhenda Xie, Zhewen Hao, Bingxuan Wang, Junxiao Song, Deli Chen, Xin Xie, Kang Guan, Yuxiang You, Aixin Liu, Qiushi Du, Wenjun Gao, Xuan Lu, Qinyu Chen, Yaohui Wang, Chengqi Deng, Jiashi Li, Chenggang Zhao, Chong Ruan, Fuli Luo, Wenfeng Liang",2024-06-17,DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence,https://github.com/deepseek-ai/DeepSeek-Coder-V2,,SOTA improvement,"New SOTA on Aider, AIME 2024, and Math Odyssey benchmarks (including against proprietary models such as Claude 3 Opus, GPT-4o and GPT-4 Turbo).
Note that Figure 1 appears to show the new model getting SOTA for several other benchmarks, but omits results from GPT-4o which wins in most cases.",236000000000.0,Mixture of experts model. 21B parameters activated per token.,1.2852e+24,"Trained on a total of 10.2T tokens
6NC: 6 * 10.2T * 21B active parameters = 1.285e24","GitHub,Common Crawl","See Section 2. ""In the pre-training phase, the dataset of DeepSeek-Coder-V2 is created with a composition of 60% source code, 10% math corpus, and 30% natural language corpus""",3191000000000.0,"""In the pre-training phase, the dataset of DeepSeek-Coder-V2 is created with a composition of 60% source code, 10% math corpus, and 30% natural language corpus ... The source code consists of 1,170B code-related tokens sourced from GitHub and CommonCrawl... For the math corpus, we collect 221B math-related tokens sourced from CommonCrawl... In total, DeepSeek-Coder-V2 has been exposed to 10.2T training tokens, where 4.2 trillion tokens originate from the DeepSeek V2 dataset, while the remaining 6 trillion tokens come from the DeepSeek-Coder-V2 dataset""

Total of 1.391T tokens in the new data.

From the DeepSeek-V2 paper: ""our tokenized pretraining corpus contains 8.1T tokens""

So some tokens are trained over for multiple epochs:
- 6T * 0.6 / 1.17T = 3.1 epochs on the code corpus
- 6T * 0.1 / 221B = 2.7 epochs on the math corpus
- 6T * 0.3 / 8.1T = 0.22 epochs on the natural language corpus

Total unique tokens seen is likely 1.17T + 221B + (6T*0.3) = 3.191T",,,,Self-supervised learning,Confident,"We present DeepSeek-Coder-V2, an open-source Mixture-of-Experts (MoE) code language model that achieves performance comparable to GPT4-Turbo in code-specific tasks. Specifically, DeepSeek-Coder-V2 is further pre-trained from an intermediate checkpoint of DeepSeek-V2 with additional 6 trillion tokens. Through this continued pre-training, DeepSeek-Coder-V2 substantially enhances the coding and mathematical reasoning capabilities of DeepSeek-V2, while maintaining comparable performance in general language tasks. Compared to DeepSeekCoder-33B, DeepSeek-Coder-V2 demonstrates significant advancements in various aspects of code-related tasks, as well as reasoning and general capabilities. Additionally, DeepSeek-CoderV2 expands its support for programming languages from 86 to 338, while extending the context length from 16K to 128K. In standard benchmark evaluations, DeepSeek-Coder-V2 achieves superior performance compared to closed-source models such as GPT4-Turbo, Claude 3 Opus, and Gemini 1.5 Pro in coding and math benchmarks.",,,Open weights (restricted use),China,DeepSeek-V2 (MoE-236B),,,,,2025-05-09 11:32,,"The paper does not mention any hardware, GPUs or any information regarding the hardware used.",,36864000.0,"Most training is done at batch size of 36,864. They do long context training: ""In the first stage, we utilize a sequence length of 32K and a batch size of 1152 for 1000 steps. In the second stage, we train the model for an additional 1000 steps, employing a sequence length of 128K and a batch size of 288 sequences"" 128k*288 = 36,864,000",Industry,,,,,Unreleased,"license has some harmful use restrictions: https://github.com/deepseek-ai/DeepSeek-Coder-V2/blob/main/LICENSE-MODEL 

no training code",Industry,,,,,Operation counting,deepseek-ai,,,
Gen-3 Alpha,"Video,Vision","Video generation,Text-to-video,Image-to-video",Runway,,2024-06-17,"Gen-3 Alpha is the first of upcoming models that offers improvement in fidelity, consistency, motion and speed over previous generations of models. ","https://runwayml.com/research/introducing-gen-3-alpha

https://help.runwayml.com/hc/en-us/articles/30266515017875-Creating-with-Text-Image-to-Video-on-Gen-3-Alpha-and-Turbo",,,,,,,,Unspecified unreleased,,,,,,,,Unknown,"Gen-3 Alpha is the first of the next generation of foundation models trained by Runway on a new infrastructure built for large-scale multimodal training. It is a major improvement in fidelity, consistency, and motion over Gen-2, and a step towards building General World Models.",,,API access,United States of America,,,,,,2025-05-29 14:58,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
Ovis-7B,"Multimodal,Language,Vision","Visual question answering,Language modeling/generation,Question answering","Alibaba,Nanjing University","Shiyin Lu, Yang Li, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, Han-Jia Ye",2024-06-17,"Ovis: Structural Embedding Alignment for Multimodal Large Language Model
",https://arxiv.org/abs/2405.20797,12.0,,,7000000000.0,,1.7e+23,"Fine tune: 989500000000000*128*0.3*15*60*60=2.0518272e+21
QWEN 1.5 training FLOP: 1.68e+23
Total: 1.7e+23",,15M datapoints (image-text pairs),,,15.0,,NVIDIA H100 SXM5 80GB,,Confident,"We propose Ovis, a novel MLLM architecture designed to structurally align visual and textual embeddings. Ovis integrates an additional learnable visual embedding table into the visual encoderâ€™s process. To capture rich visual semantics, each image patch indexes the visual embedding table multiple times, resulting in a final visual embedding that is a probabilistic combination of the indexed embeddings.",1.0,,Open weights (unrestricted),"China,China",Qwen1.5-7B,,,128.0,,2025-05-23 12:04,,,,,,"Industry,Academia",,,,,Unreleased,"Apache 2.0
https://huggingface.co/AIDC-AI/Ovis-Clip-Qwen1_5-7B

I don't see a training script for this particular model here: https://github.com/AIDC-AI/Ovis","Industry,Academia",,,,176880.2801222388,,AIDC-AI,,,
MindEye2,"Medicine,Vision,Image generation","Image captioning,Image generation","Stability AI,Medical AI Research Center (MedARC),Princeton University,University of Minnesota,University of Sydney,University of Waterloo","Paul S. Scotti, Mihir Tripathy, Cesar Kadir Torrico Villanueva, Reese Kneeland, Tong Chen, Ashutosh Narang, Charan Santhirasegaran, Jonathan Xu, Thomas Naselaris, Kenneth A. Norman, Tanishq Mathew Abraham",2024-06-15,MindEye2: Shared-Subject Models Enable fMRI-To-Image With 1 Hour of Data,https://arxiv.org/abs/2403.11207,,,,,,,,Natural Scenes Dataset (NSD) ,,,,,"from their note ""If you are training MindEye2 on a single GPU on the full 40 sessions, expect that pre-training and fine-tuning both take approximately 1 day to complete.""  I am assuming with ""Likely"" confidence that they trained for 24 chip-hours",NVIDIA A100 SXM4 80 GB,,Likely,"Reconstructions of visual perception from brain activity have improved tremendously, but the practical utility of such methods has been limited. This is because such models are trained independently per subject where each subject requires dozens of hours of expensive fMRI training data to attain high-quality results. The present work showcases high-quality reconstructions using only 1 hour of fMRI training data. We pretrain our model across 7 subjects and then fine-tune on minimal data from a new subject. Our novel functional alignment procedure linearly maps all brain data to a shared-subject latent space, followed by a shared non-linear mapping to CLIP image space. We then map from CLIP space to pixel space by fine-tuning Stable Diffusion XL to accept CLIP latents as inputs instead of text. This approach improves out-of-subject generalization with limited training data and also attains state-of-the-art image retrieval and reconstruction metrics compared to single-subject approaches. MindEye2 demonstrates how accurate reconstructions of perception are possible from a single visit to the MRI facility. All code is available on GitHub.",150.0,,Open weights (unrestricted),"United Kingdom of Great Britain and Northern Ireland,United States of America,United States of America,United States of America,Australia,Canada",Stable Diffusion XL (SDXL),8087040000000000000,312000000000000*24*3600*0.3 = 8.08704e+18,8.0,,2025-05-01 10:42,,,,,,"Industry,Research collective,Academia,Academia,Academia,Academia",,,,24.0,Open source,"MIT License
https://github.com/MedARC-AI/MindEyeV2/tree/main","Industry,Research collective,Academia,Academia,Academia,Academia",,,,6317.434225819239,Hardware,,,,
Nemotron-4 340B,Language,"Language modeling/generation,Chat,Question answering",NVIDIA,"Bo Adler, Niket Agarwal, Ashwath Aithal, Dong H. Anh, Pallab Bhattacharya, Annika Brundyn, Jared Casper, Bryan Catanzaro, Sharon Clay, Jonathan Cohen, Sirshak Das, Ayush Dattagupta, Olivier Delalleau, Leon Derczynski, Yi Dong, Daniel Egert, Ellie Evans, Aleksander Ficek, Denys Fridman, Shaona Ghosh, Boris Ginsburg, Igor Gitman, Tomasz Grzegorzek, Robert Hero, Jining Huang, Vibhu Jawa, Joseph Jennings, Aastha Jhunjhunwala, John Kamalu, Sadaf Khan, Oleksii Kuchaiev, Patrick LeGresley, Hui Li, Jiwei Liu, Zihan Liu, Eileen Long, Ameya Sunil Mahabaleshwarkar, Somshubra Majumdar, James Maki, Miguel Martinez, Maer Rodrigues de Melo, Ivan Moshkov, Deepak Narayanan, Sean Narenthiran, Jesus Navarro, Phong Nguyen, Osvald Nitski, Vahid Noroozi, Guruprasad Nutheti, Christopher Parisien, Jupinder Parmar, Mostofa Patwary, Krzysztof Pawelec, Wei Ping, Shrimai Prabhumoye, Rajarshi Roy, Trisha Saar, Vasanth Rao Naik Sabavat, Sanjeev Satheesh, Jane Polak Scowcroft, Jason Sewall, Pavel Shamis, Gerald Shen, Mohammad Shoeybi, Dave Sizer, Misha Smelyanskiy, Felipe Soares, Makesh Narsimhan Sreedhar, Dan Su, Sandeep Subramanian, Shengyang Sun, Shubham Toshniwal, Hao Wang, Zhilin Wang, Jiaxuan You, Jiaqi Zeng, Jimmy Zhang, Jing Zhang, Vivienne Zhang, Yian Zhang, Chen Zhu",2024-06-14,NVIDIA Releases Open Synthetic Data Generation Pipeline for Training Large Language Models,https://blogs.nvidia.com/blog/nemotron-4-synthetic-data-generation-llm-training/ ,,Training cost,"~2e25 FLOP, so high training cost, likely >5M",340000000000.0,340B,1.8000000000000005e+25,"9 trillion tokens for training
6 * 340B * 9T = 1.8E25

alternatively, can do a hardware estimate with a few extra steps:

According to the technical report, Nemotron-4 340B was trained using up to 6144 H100 GPUs. Helpfully, they also report the model FLOP utilization (MFU), which was 41-42% (Table 2). This is the ratio of the actual output of their GPUs, in FLOP used for training, relative to their theoretical max of 989 teraFLOP/s per GPU. 
Unfortunately, the report omits the last ingredient, which is the duration of the training run. However, in Table 2 they report some relevant data that we can use to infer the training time. 
Nemotron-4 was trained in several stages, but the largest stage used all 6144 GPUs with a batch size of 2304 and an iteration time (time per batch) of 8.0 seconds. This stage involved 7.6T tokens, so it makes up the majority of training. 
A batch size of 2304 means that each batch consists of 2304 sequences, and they report that the sequence length used for training was 4096 tokens. This means that each batch contained 4096 * 2304 = 9,437,184 tokens. 
So, during this stage, it took 8 seconds to train the model on 9.4m tokens. Extrapolating to the entire 9T token dataset, this implies the training run would have taken 7,659,574 seconds, or 89 days. (it actually took longer because they didn't use all their GPUs for the whole run) 
Multiplying 7,659,574 seconds by 41% MFU, 989 peak teraFLOP/s for each H100, and 6144 H100s, we get ~1.9e25 FLOP. This is very close to our first estimate. 
",Unspecified unreleased,"The technical report for the 340B model cites the report for the 15B version (https://arxiv.org/pdf/2402.16819 )

from that paper:

""We train Nemotron-4 15B on a pre-training dataset consisting of 8 trillion tokens. At a high-level,
the data blend is split into three different types of data: English natural language data (70%), multilingual
natural language data (15%), and source-code data (15%).
The English corpus consists of curated documents from a variety of sources and domains including web
documents, news articles, scientific papers, books, etc and the distribution used in our pre-training set is
highlighted in Figure 2. The code and multilingual data consists of a diverse set of natural and programming
languages. We find that appropriately sampling tokens from these languages is key to strong accuracies in
these domains. We share the distributions used for both code and multilingual tokens in our pre-training
dataset in Figure 3 and Figure 4 respectively.
In constructing the pre-training corpus, we remove any possible duplicates via document-level exact and
near-deduplication (Jennings et al., 2023). We additionally applied document-level quality filtering across
our corpus using a language-model based filtering approach similar to (Wenzek et al., 2019) in addition to a
series of heuristic filters as described in (Rae et al., 2022) and (Raffel et al., 2020).""",6750000000000.0,"9T training tokens.

They first train on an 8T token dataset and then an additional 1T tokens, it's slightly unclear if that's more data or a partial second epoch

6.75T words using 1 token = 0.75 words",2200.0,"see training compute notes, this is an inferred estimate",NVIDIA H100 SXM5 80GB,,Confident,"We release the Nemotron-4 340B model family, including Nemotron-4-340B-Base, Nemotron-4-
340B-Instruct, and Nemotron-4-340B-Reward. Our models are open access under the NVIDIA Open
Model License Agreement, a permissive model license that allows distribution, modification, and use of
the models and its outputs. These models perform competitively to open access models on a wide range
of evaluation benchmarks, and were sized to fit on a single DGX H100 with 8 GPUs when deployed in
FP8 precision. We believe that the community can benefit from these models in various research studies
and commercial applications, especially for generating synthetic data to train smaller language models.
Notably, over 98% of data used in our model alignment process is synthetically generated, showcasing
the effectiveness of these models in generating synthetic data. To further support open research and
facilitate model development, we are also open-sourcing the synthetic data generation pipeline used in
our model alignment process.

(from technical report: https://d1qx31qr3h6wln.cloudfront.net/publications/Nemotron_4_340B_8T_0.pdf )",,,Open weights (unrestricted),United States of America,,,,6144.0,0.4107,2025-05-28 15:58,,,,,,Industry,,,,,Unreleased,Permissive commercial license: https://developer.download.nvidia.com/licenses/nvidia-open-model-license-agreement-june-2024.pdf ,Industry,,"Table 2 indicates MFU at different stages of training.

((42.4% * 200B) + (42.3% * 200B) + (41.0% * 7600B)) / (200B + 200B + 7600B) = 0.410675, averaged over training.",BF16,8490820.682633882,"Operation counting,Hardware",nvidia,,,5
DigiRL,"Vision,Language",Visual question answering,"University of California (UC) Berkeley,University of Illinois Urbana-Champaign (UIUC),Google DeepMind","Hao Bai, Yifei Zhou, Mert Cemri, Jiayi Pan, Alane Suhr, Sergey Levine, Aviral Kumar",2024-06-14,DigiRL: Training In-The-Wild Device-Control Agents with Autonomous Reinforcement Learning,https://arxiv.org/abs/2406.11896,,,"""In this paper, we propose a novel autonomous RL approach, DigiRL, for training in-the-wild, multi-
modal, device-control agents that establish a new state-of-the-art performance on a number of Android
control tasks from Android-in-the-Wild dataset [31 ].""",1300000000.0,,,,,,,,,,,,Confident,"Training corpuses for vision language models (VLMs) typically lack sufficient amounts of decision-centric data. This renders off-the-shelf VLMs sub-optimal for decision-making tasks such as in-the-wild device control through graphical user interfaces (GUIs). While training with static demonstrations has shown some promise, we show that such methods fall short for controlling real GUIs due to their failure to deal with real-world stochasticity and non-stationarity not captured in static observational data. This paper introduces a novel autonomous RL approach, called DigiRL, for training in-the-wild device control agents through fine-tuning a pre-trained VLM in two stages: offline RL to initialize the model, followed by offline-to-online RL. To do this, we build a scalable and parallelizable Android learning environment equipped with a VLM-based evaluator and develop a simple yet effective RL approach for learning in this domain. Our approach runs advantage-weighted RL with advantage estimators enhanced to account for stochasticity along with an automatic curriculum for deriving maximal learning signal. We demonstrate the effectiveness of DigiRL using the Android-in-the-Wild (AitW) dataset, where our 1.3B VLM trained with RL achieves a 49.5% absolute improvement -- from 17.7 to 67.2% success rate -- over supervised fine-tuning with static human demonstration data. These results significantly surpass not only the prior best agents, including AppAgent with GPT-4V (8.3% success rate) and the 17B CogAgent trained with AitW data (38.5%), but also the prior best autonomous RL approach based on filtered behavior cloning (57.8%), thereby establishing a new state-of-the-art for digital agents for in-the-wild device control.",,,Open weights (unrestricted),"United States of America,United States of America,United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France",,,,,,2025-05-01 10:42,,,,,,"Academia,Academia,Industry",,,,,Open source,"Apache 2 

https://github.com/DigiRL-agent/digirl","Academia,Academia,Industry",,,,,,,,,
OpenVLA,"Robotics,Vision,Language",Robotic manipulation,"Stanford University,University of California (UC) Berkeley,Toyota Research Institute,Google DeepMind,Massachusetts Institute of Technology (MIT),Physical Intelligence","Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, Quan Vuong, Thomas Kollar, Benjamin Burchfiel, Russ Tedrake, Dorsa Sadigh, Sergey Levine, Percy Liang, Chelsea Finn",2024-06-13,OpenVLA: An Open-Source Vision-Language-Action Mode,"https://openvla.github.io/
https://arxiv.org/abs/2406.09246",73.0,SOTA improvement,"""OpenVLA outperforms the 55B-parameter RT-2-X model [1, 7], the prior state-of-the-art VLA, by 16.5% absolute success rate across 29 evaluation tasks on the WidowX and Google Robot embodiments.""

Top10 recent paper from Sebastian Sartor 2025-05-14",7188100000.0,"Based on a Prismatic-7B VLM backbone, which itself is comprised of 600M parameter vision encoder (DinoV2 + SigLIP) plus Llama-2 7B. Table 1 indicates 7.1881 billion trainable parameters",1.1e+23,"Majority of compute is from VLA pre-training embedded in Prismatic-7B and it's constituent models. 

The fine-tuning compute used in this paper is ""64 A100 GPUs for 14 days, or a total of 21,500 A100-hours""
21500 * 3600 * 3.12e14 * 0.4 = 9.66e21

Prismatic-7B training took ""less than 9 hours"" on 8 A100s: 9 * 3600 * 8 * 3.12e14 * 0.4 = 3.23e19

Add in the pre-trained components:
- DinoV2 = 7.42e21, per our database
- The SigLIP model in question is SoViT-400m/14 from the cited Alabdulmohsin et al., 2023) and ""is pretrained on 40 billion examples, which amounts to 9T GFLOPs and 230K TPUv3 core-hours"" = 9e21
- Llama 2-7B = 8.4e22, per our database

Total
9.66e21 + 3.23e19 + 7.42e21 + 9e21 + 8.4e22 = 1.10e23",Open X-Embodiment,"""The full OpenX dataset, at the time of writing, consists of more than 70 individual robot datasets, with more than 2M robot trajectories [...] we apply multiple steps of data curation to the raw dataset.""",970000.0,"""OpenVLA consists of a pretrained visually-conditioned language model backbone that captures visual features at multiple granularities, fine-tuned on a large, diverse dataset of 970k robot manipulation trajectories from the Open-X Embodiment [1] dataset""
Filtered from 2M total in OpenX.",336.0,"""The final OpenVLA model is trained on a cluster of 64 A100 GPUs for 14 days""
14 days * 24 hr/day = 336 hours",NVIDIA A100,Supervised,Confident,"Large policies pretrained on a combination of Internet-scale vision-language data and diverse robot demonstrations have the potential to change how we teach robots new skills: rather than training new behaviors from scratch, we can fine-tune such vision-language-action (VLA) models to obtain robust, generalizable policies for visuomotor control. Yet, widespread adoption of VLAs for robotics has been challenging as 1) existing VLAs are largely closed and inaccessible to the public, and 2) prior work fails to explore methods for efficiently fine-tuning VLAs for new tasks, a key component for adoption. Addressing these challenges, we introduce OpenVLA, a 7B-parameter open-source VLA trained on a diverse collection of 970k real-world robot demonstrations. OpenVLA builds on a Llama 2 language model combined with a visual encoder that fuses pretrained features from DINOv2 and SigLIP. As a product of the added data diversity and new model components, OpenVLA demonstrates strong results for generalist manipulation, outperforming closed models such as RT-2-X (55B) by 16.5% in absolute task success rate across 29 tasks and multiple robot embodiments, with 7x fewer parameters. We further show that we can effectively fine-tune OpenVLA for new settings, with especially strong generalization results in multi-task environments involving multiple objects and strong language grounding abilities, and outperform expressive from-scratch imitation learning methods such as Diffusion Policy by 20.4%. We also explore compute efficiency; as a separate contribution, we show that OpenVLA can be fine-tuned on consumer GPUs via modern low-rank adaptation methods and served efficiently via quantization without a hit to downstream success rate. Finally, we release model checkpoints, fine-tuning notebooks, and our PyTorch codebase with built-in support for training VLAs at scale on Open X-Embodiment datasets.",27.0,,Open weights (unrestricted),"United States of America,United States of America,United States of America,United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France,United States of America,United States of America",Llama 2-7B,9,"""64 A100 GPUs for 14 days, or a total of 21,500 A100-hours""
21500 * 3600 * 3.12e14 * 0.4 = 9.66e21",64.0,,2025-06-11 21:39,,,,2048.0,,"Academia,Academia,Industry,Industry,Academia,Industry",,,,,Open source,"""OpenVLA uses multiple pretrained model components: SigLIP [9] and DinoV2 [25] vision encoders and a Llama 2 [10] language model backbone. For all three models, weights are open, but not their training data or code. We release training data, code and model weights for reproducing OpenVLA on top of these components.""
All published material is on an MIT license.

train code: https://github.com/openvla/openvla/blob/main/scripts/pretrain.py ","Academia,Academia,Industry,Industry,Academia,Industry",,,BF16,50541.724821294745,Hardware,,,,
Animate Anyone,Video,"Video generation,Image-to-video",Alibaba,"Li Hu, Xin Gao, Peng Zhang, Ke Sun, Bang Zhang, Liefeng Bo",2024-06-13,Animate Anyone: Consistent and Controllable Image-to-Video Synthesis for Character Animation,https://arxiv.org/abs/2311.17117,,,,,,,,Unspecified unreleased,,,"""we collect 5K character video
clips from the internet to train our model""

"" In the first training stage, individual video frames are sampled, resized, and centercropped to a resolution of 768Ã—768. Training is conducted for 30,000 steps with a batch size of 64. In the second
training stage, we train the temporal layer for 10,000 steps
with 24-frame video sequences and a batch size of 4""",,,NVIDIA A100,,Unknown,"Character Animation aims to generating character videos from still images through driving signals. Currently, diffusion models have become the mainstream in visual generation research, owing to their robust generative capabilities. However, challenges persist in the realm of image-to-video, especially in character animation, where temporally maintaining consistency with detailed information from character remains a formidable problem. In this paper, we leverage the power of diffusion models and propose a novel framework tailored for character animation. To preserve consistency of intricate appearance features from reference image, we design ReferenceNet to merge detail features via spatial attention. To ensure controllability and continuity, we introduce an efficient pose guider to direct character's movements and employ an effective temporal modeling approach to ensure smooth inter-frame transitions between video frames. By expanding the training data, our approach can animate arbitrary characters, yielding superior results in character animation compared to other image-to-video methods. Furthermore, we evaluate our method on benchmarks for fashion video and human dance synthesis, achieving state-of-the-art results.",,,Unreleased,China,,,,4.0,,2025-05-19 15:43,,,,,,Industry,,,,,Unreleased,,Industry,,,,3158.8578013309216,,,,,
Mamba2-Hybrid,Language,"Language modeling/generation,Question answering",NVIDIA,"Roger Waleffe, Wonmin Byeon, Duncan Riach, Brandon Norick, Vijay Korthikanti, Tri Dao, Albert Gu, Ali Hatamizadeh, Sudhakar Singh, Deepak Narayanan, Garvit Kulshreshtha, Vartika Singh, Jared Casper, Jan Kautz, Mohammad Shoeybi, Bryan Catanzaro",2024-06-12,An Empirical Study of Mamba-based Language Models,https://arxiv.org/abs/2406.07887,,,,8660000000.0,Table 6,1.8186e+23,"6ND = 6*8660000000.00 parameters * 3500000000000 tokens = 1.8186 Ã— 10^23

",Unspecified unreleased,"""We train the models discussed in this report on 1.1T and 3.5T token datasets. Both datasets are predecessors of the dataset used to train Nemotron-4 and are comprised of 70% English, 15% non-English, and 15% code.""

",3500000000000.0,,,,NVIDIA H100 SXM5 80GB,,Likely,"Selective state-space models (SSMs) like Mamba overcome some of the shortcomings of Transformers, such as quadratic computational complexity with sequence length and large inference-time memory requirements from the key-value cache. Moreover, recent studies have shown that SSMs can match or exceed the language modeling capabilities of Transformers, making them an attractive alternative. In a controlled setting (e.g., same data), however, studies so far have only presented small scale experiments comparing SSMs to Transformers. To understand the strengths and weaknesses of these architectures at larger scales, we present a direct comparison between 8B-parameter Mamba, Mamba-2, and Transformer models trained on the same datasets of up to 3.5T tokens. We also compare these models to a hybrid architecture consisting of 43% Mamba-2, 7% attention, and 50% MLP layers (Mamba-2-Hybrid). Using a diverse set of tasks, we answer the question of whether Mamba models can match Transformers at larger training budgets. Our results show that while pure SSMs match or exceed Transformers on many tasks, they lag behind Transformers on tasks which require strong copying or in-context learning abilities (e.g., 5-shot MMLU, Phonebook) or long-context reasoning. In contrast, we find that the 8B Mamba-2-Hybrid exceeds the 8B Transformer on all 12 standard tasks we evaluated (+2.65 points on average) and is predicted to be up to 8x faster when generating tokens at inference time. To validate long-context capabilities, we provide additional experiments evaluating variants of the Mamba-2-Hybrid and Transformer extended to support 16K, 32K, and 128K sequences. On an additional 23 long-context tasks, the hybrid model continues to closely match or exceed the Transformer on average. To enable further study, we release the checkpoints as well as the code used to train our models as part of NVIDIA's Megatron-LM project.",,,Open weights (unrestricted),United States of America,,,,1024.0,0.299,2025-05-16 10:30,,,,4194304.0,"""On the larger dataset we increase the batch size to 1024"" seq length 4096",Industry,,,,,Open source,"https://github.com/NVIDIA/Megatron-LM/tree/ssm/examples/mamba
Apache 2.0

train script: https://github.com/NVIDIA/Megatron-LM/blob/ssm/examples/mamba/train.sh ",Industry,,"""When training on NVIDIA H100 GPUs (NVIDIA 2023), with a tensor-parallel
size of four and data-parallel size of 256 (1024 total GPUs) (micro batch size 4, global batch size 1024), our Mamba-2-Hybrid achieves an MFU of 29.9%.""

Model Flop Utilization (MFU)",,1415199.8102553426,Operation counting,,,,
ProteinReDiff,Biology,Protein design,"FPT Software AI Center,University of Chicago,Indiana State University","Viet Thanh Duy Nguyen, Nhan D. Nguyen,  Truong Son Hy",2024-06-12,Complex-based Ligand-Binding Proteins Redesign by Equivariant Diffusion-based Generative Models,https://www.biorxiv.org/content/10.1101/2024.04.17.589997v3.abstract,1.0,,,,,,,,,8640001.0,"Total Samples: 9,430 (PDBBind) + 15,261 (CATH) = 24,691 samples
Tokens per Sample: 300 (protein) + 50 (SMILES) = 350 tokens
Total Data Points: 24,691 Ã— 350 = 8,641,850",,,,,Confident,"Proteins, serving as the fundamental architects of biological processes, interact with ligands to perform a myriad of functions essential for life. Designing functional ligand-binding proteins is pivotal for advancing drug development and enhancing therapeutic efficacy. In this study, we introduce ProteinReDiff, an efficient computational framework targeting the redesign of ligand-binding proteins. Using equivariant diffusion-based generative models, ProteinReDiff enables the creation of high-affinity ligand-binding proteins without the need for detailed structural information, leveraging instead the potential of initial protein sequences and ligand SMILES strings. Our evaluations across sequence diversity, structural preservation, and ligand binding affinity underscore ProteinReDiffâ€™s potential to advance computational drug discovery and protein engineering. Our source code is publicly available at https: //github.com/HySonLab/Protein_Redesign.",100.0,,Open weights (unrestricted),"Vietnam,United States of America,United States of America",,,,,,2025-06-16 12:58,,,,,,"Industry,Academia",,,,,Open source,"MIT license for weights, training and inference code
https://github.com/HySonLab/Protein_Redesign","Industry,Academia",,,,,,,,,
Stable Diffusion 3 Medium,Image generation,"Image generation,Text-to-image",Stability AI,,2024-06-12,"Announcing the Open Release of Stable Diffusion 3 Medium, Our Most Sophisticated Image Generation Model to Date ","https://stability.ai/news/stable-diffusion-3-medium
https://huggingface.co/stabilityai/stable-diffusion-3-medium",,,,2500000000.0,2.5B,,,Unspecified unreleased,"We used synthetic data and filtered publicly available data to train our models. The model was pre-trained on 1 billion images. The fine-tuning data includes 30M high-quality aesthetic images focused on specific visual content and style, as well as 3M preference data images.

from the april paper: ""In general, we pretrain all of our models on low-resolution images of size 256^2 pixels. Next, we finetune our models on higher resolutions with mixed aspect ratios""",,,,,,,Confident,"Stable Diffusion 3 Medium is Stability AIâ€™s most advanced text-to-image open model yet.

The small size of this model makes it perfect for running on consumer PCs and laptops as well as enterprise-tier GPUs. It is suitably sized to become the next standard in text-to-image models.

The weights are now available under an open Community License. For large-scale commercial use, please contact us for licensing details.

To try Stable Diffusion 3 models, try using the API on the Stability Platform, sign up for a free three-day trial on Stable Assistant, and try Stable Artisan via Discord.",,,Open weights (restricted use),United Kingdom of Great Britain and Northern Ireland,,,,,,2025-02-14 13:51,,,,,,Industry,,,,,Unreleased,"Community License: Free for research, non-commercial, and commercial use for organisations or individuals with less than $1M annual revenue. You only need a paid Enterprise license if your yearly revenues exceed USD$1M and you use Stability AI models in commercial products or services.",Industry,,,,,,stabilityai,,,
Luma Dream Machine,"Video,Vision","Video generation,Text-to-video,Image-to-video",LumaLabs,,2024-06-12,,https://lumalabs.ai/dream-machine,,,,,,,,Unspecified unreleased,,,,,"""To build Dream Machine we worked with Amazon Web Services (AWS) and their best-in-class H100 training infrastructure SageMaker HyperPod. """,NVIDIA H100 SXM5 80GB,,Unknown,"Dream Machine is an AI model that makes high quality, realistic videos fast from text and images.
It is a highly scalable and efficient transformer model trained directly on videos making it capable of generating physically accurate, consistent and eventful shots. Dream Machine is our first step towards building a universal imagination engine and it is available to everyone now!",,,API access,United States of America,,,,,,2025-05-19 12:44,Amazon Web Services,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
Samba 3.8B,Language,Language modeling/generation,"Microsoft,University of Illinois Urbana-Champaign (UIUC)","Liliang Ren, Yang Liu, Yadong Lu, Yelong Shen, Chen Liang, Weizhu Chen",2024-06-11,Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling,https://arxiv.org/abs/2406.07522,,,"not formally notable

New architecture (Mamba hybrid) with strongest performance for an open model of its size:

""In particular, the largest 3.8B base model pre-trained with 3.2T tokens achieves a 71.2 score for MMLU [HBB+21], 54.9 for
HumanEval [CTJ+21], and 69.6 for GSM8K [CKB+21], substantially outperforming strong open source language models up to 8B parameters, as detailed in Table 1.",3800000000.0,"3.8B, table 10",7.3e+22,"""We scale SAMBA with 421M, 1.3B, 1.7B and up to 3.8B parameters. In particular, the largest 3.8B base model pre-trained with 3.2T tokens achieves a 71.2 score for MMLU [HBB+21], 54.9 for HumanEval [CTJ+21], and 69.6 for GSM8K""

3.8B * 3.2T * 6 = 7.3e22",Phi-3 Dataset,Table 10,3200000000000.0,3.2T tokens,,,,,Confident,,1.0,,Unreleased,"United States of America,Multinational,India,Belgium,United States of America",,,,,,2025-05-30 14:29,,,,,,"Industry,Academia",,,,,Open source,training code: https://github.com/microsoft/Samba ,"Industry,Academia",,,,,Operation counting,,,,
TiTok-L,Image generation,Image generation,"ByteDance,Technical University of Munich","Qihang Yu, Mark Weber, Xueqing Deng, Xiaohui Shen, Daniel Cremers, Liang-Chieh Chen",2024-06-11,An Image is Worth 32 Tokens for Reconstruction and Generation,https://arxiv.org/abs/2406.07550,,,,307000000.0,"""For TiTok variants, we primarily investigate three model sizesâ€”small, base, and large (i.e., TiTok-S, TiTok-B, TiTok-L)â€”comprising 22M, 86M, and 307M parameters for encoder and decoder, respectively""",1.7252352e+21,312000000000000*5120*3600*0.3 = 1.7252352e+21,ImageNet-1k,,,"Tokenizer:
""longer training to 200 epochs and decoder fine-tuning, all other hyper-parameters remain the same. We use patch size 16 for all vision transformers at resolution 256 Ã— 256 and increase it to 32 for resolution 512 Ã— 512 to ensure a computation efficiency.""

""the training duration is extended to 1M iterations (200 epochs)""

Generator:
"" The generative models are trained with a batch size of 2048 and 500k iterations to improve training efficiency""

""ImageNet 256 Ã— 256""",,"""Moreover, the tokenizer training takes 64 A100-40G for 74 hours
(TiTok-L-32)""

""The generator training takes 32 A100-40G for 12 hours (TiTok-L-32)""

74*64+12*32 = 5120 chip-hours",NVIDIA A100 SXM4 80 GB,,Confident,"Recent advancements in generative models have highlighted the crucial role of image tokenization in the efficient synthesis of high-resolution images. Tokenization, which transforms images into latent representations, reduces computational demands compared to directly processing pixels and enhances the effectiveness and efficiency of the generation process. Prior methods, such as VQGAN, typically utilize 2D latent grids with fixed downsampling factors. However, these 2D tokenizations face challenges in managing the inherent redundancies present in images, where adjacent regions frequently display similarities. To overcome this issue, we introduce Transformer-based 1-Dimensional Tokenizer (TiTok), an innovative approach that tokenizes images into 1D latent sequences. TiTok provides a more compact latent representation, yielding substantially more efficient and effective representations than conventional techniques. For example, a 256 x 256 x 3 image can be reduced to just 32 discrete tokens, a significant reduction from the 256 or 1024 tokens obtained by prior methods. Despite its compact nature, TiTok achieves competitive performance to state-of-the-art approaches. Specifically, using the same generator framework, TiTok attains 1.97 gFID, outperforming MaskGIT baseline significantly by 4.21 at ImageNet 256 x 256 benchmark. The advantages of TiTok become even more significant when it comes to higher resolution. At ImageNet 512 x 512 benchmark, TiTok not only outperforms state-of-the-art diffusion model DiT-XL/2 (gFID 2.74 vs. 3.04), but also reduces the image tokens by 64x, leading to 410x faster generation process. Our best-performing variant can significantly surpasses DiT-XL/2 (gFID 2.13 vs. 3.04) while still generating high-quality samples 74x faster.",,,Open weights (unrestricted),"China,Germany",,,,,,2025-05-01 10:42,,,,,,"Industry,Academia",,,,5120.0,Open source,"apache 2
https://huggingface.co/fun-research/TiTok

https://github.com/bytedance/1d-tokenizer","Industry,Academia",,,,,Hardware,,,,
Kling,"Video,Vision","Video generation,Image-to-video,Text-to-video",Kuaishou Technology,,2024-06-10,"Kling is a video generation model developed by the Kuaishou model team, which has powerful video generation capabilities and allows users to easily and efficiently complete artistic video creation.","https://klingai.io/
https://ir.kuaishou.com/news-releases/news-release-details/kuaishou-unveils-comprehensive-ai-models-reshaping-content-and",,,,,,,,,,,,,,,,Unknown,"Large-scale reasonable movement
Kling uses a 3D spatiotemporal joint attention mechanism to better model complex spatiotemporal movement, generate video content with large-scale movement, and conform to the laws of movement.

Video generation up to 2 minutes
Thanks to efficient training infrastructure, extreme reasoning optimization and scalable infrastructure, Kling's large model can generate videos up to 2 minutes long with a frame rate of 30fps.

Simulate physical world characteristics
Based on the powerful modeling capabilities inspired by the self-developed model architecture and Scaling Law, Kling can simulate the physical characteristics of the real world and generate videos that conform to the laws of physics.

Powerful concept combination capabilities
Based on a deep understanding of text-video semantics and the powerful capabilities of the Diffusion Transformer architecture, Kling can transform users' rich imagination into concrete pictures and fictional scenes that will not appear in the real world.

Movie-level image generation
Based on the self-developed 3D VAE, Keling can generate movie-level videos with 1080p resolution, which can vividly present both the vast and magnificent grand scenes and the delicate close-up shots.

Supports free output video aspect ratio
Keling adopts a variable resolution training strategy, which can output a variety of video aspect ratios for the same content during the inference process, meeting the needs of using video materials in richer scenes.

Expression and body drive
Based on the self-developed 3D face and body reconstruction technology, combined with background stability and redirection modules, the expression and body full drive technology is realized. With only a full-body photo, you can experience the vivid ""singing and dancing"" gameplay.",,,Hosted access (no API),China,,,,,,2025-05-19 12:45,,,,,,Industry,,,,,Unreleased,"""Yes, KLING AI is accessible as a public demo in China, allowing users to experience its capabilities firsthand.""",Industry,,,,,,,,,
Qwen2-72B,Language,"Chat,Language modeling/generation",Alibaba,Qwen Team,2024-06-07,Hello Qwen2,"https://qwenlm.github.io/blog/qwen2/ 
https://arxiv.org/abs/2407.10671 ",,Training cost,"SOTA claims are against open source models within a parameter class.

Possibly high training cost, at 3e24 FLOP seems borderline.",72710000000.0,"72.71B parameters in total, of which 70.21B are non-embedding parameters",3.02e+24,"72 billion params, 7 trillion tokens

6 * 72 billion * 7 trillion ~= 3.02e24",Unspecified unreleased,"""All models were pre-trained on a high-quality, large-scale dataset comprising over 7 trillion tokens,
covering a wide range of domains and languages. Compared to previous editions of Qwen, Qwen2
includes a broader spectrum of linguistic data, enhancing the quantity and quality of code and mathematics content. """,7000000000000.0,"""All models were pre-trained on a high-quality, large-scale dataset comprising over 7 trillion tokens, covering a wide range of domains and languages.""",,,,Self-supervised learning,Confident,"After months of efforts, we are pleased to announce the evolution from Qwen1.5 to Qwen2. This time, we bring to you:
- Pretrained and instruction-tuned models of 5 sizes, including Qwen2-0.5B, Qwen2-1.5B, Qwen2-7B, Qwen2-57B-A14B, and Qwen2-72B;
- Having been trained on data in 27 additional languages besides English and Chinese;
- State-of-the-art performance in a large number of benchmark evaluations;
- Significantly improved performance in coding and mathematics;
- Extended context length support up to 128K tokens with Qwen2-7B-Instruct and Qwen2-72B-Instruct.

(Technical report to follow)",,,Open weights (unrestricted),China,,,,,,2025-05-28 15:58,,"There is no paper to reference, no information about hardware used for training found in media.",,,,Industry,,,5.000010000000001e+24,,Unreleased,Apache 2.0,Industry,,,BF16,,Operation counting,Qwen,,,18
Qwen2-7B,Language,"Chat,Language modeling/generation",Alibaba,Qwen Team,2024-06-07,Hello Qwen2,"https://qwenlm.github.io/blog/qwen2/ 
https://arxiv.org/abs/2407.10671 ",,,,7000000000.0,7B parameters (table 1),2.9400000000001e+23,"7 billion params, 7 trillion tokens

6 FLOP * 7 billion * 7 trillion ~= 2.94e23 FLOP",Unspecified unreleased,"""All models were pre-trained on a high-quality, large-scale dataset comprising over 7 trillion tokens, covering a wide range of domains and languages. Compared to previous editions of Qwen, Qwen2 includes a broader spectrum of linguistic data, enhancing the quantity and quality of code and mathematics content.""",7000000000000.0,"""All models were pre-trained on a high-quality, large-scale dataset comprising over 7 trillion tokens, covering a wide range of domains and languages.""",,,,Self-supervised learning,Confident,"This report introduces the Qwen2 series, the latest addition to our large language models and large multimodal models. We release a comprehensive suite of
foundational and instruction-tuned language models, encompassing a parameter
range from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts
model. Qwen2 surpasses most prior open-weight models, including its predecessor
Qwen1.5, and exhibits competitive performance relative to proprietary models
across diverse benchmarks on language understanding, generation, multilingual
proficiency, coding, mathematics, and reasoning.
The flagship model, Qwen2-72B, showcases remarkable performance: 84.2 on
MMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as
a base language model. The instruction-tuned variant, Qwen2-72B-Instruct, attains
9.1 on MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench. Moreover,
Qwen2 demonstrates robust multilingual capabilities, proficient in approximately
30 languages, spanning English, Chinese, Spanish, French, German, Arabic, Russian, Korean, Japanese, Thai, Vietnamese, and more, underscoring its versatility
and global reach.
To foster community innovation and accessibility, we have made the Qwen2 model
weights openly available on Hugging Face1 and ModelScope2, and the supplementary materials including example code on GitHub3. These platforms also include
resources for quantization, fine-tuning, and deployment, facilitating a wide range
of applications and research endeavors.",,,Open weights (unrestricted),China,,,,,,2025-05-09 11:32,,"There is no paper to reference, no information about hardware used for training found in media.",,,,Industry,,,,,Unreleased,Apache 2.0,Industry,,,,,Operation counting,Qwen,,,
Qwen2-57B-A14B,Language,"Chat,Language modeling/generation",Alibaba,Qwen Team,2024-06-07,Hello Qwen2,"https://qwenlm.github.io/blog/qwen2/ 
https://arxiv.org/abs/2407.10671 ",,,,57000000000.0,57B parameters (table 1),3.7800000000001e+23,"""For MoE models, 57B-A14B denotes that the model has 57B parameters in total and for each token 14B parameters are active"" (page 5)

C ~= 6 FLOP * 14e9 * 4.5e12 = 3.78e23",Unspecified unreleased,"""All models were pre-trained on a high-quality, large-scale dataset comprising over 7 trillion tokens, covering a wide range of domains and languages. Compared to previous editions of Qwen, Qwen2 includes a broader spectrum of linguistic data, enhancing the quantity and quality of code and mathematics content.""",4500000000000.0,"""All models were pre-trained on a high-quality, large-scale dataset comprising over 7 trillion tokens, covering a wide range of domains and languages.""

57B-A14B model was trained with a 4.5T subset of the 7T overall dataset. (table 1)",,,,Self-supervised learning,Confident,"This report introduces the Qwen2 series, the latest addition to our large language models and large multimodal models. We release a comprehensive suite of
foundational and instruction-tuned language models, encompassing a parameter
range from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts
model. Qwen2 surpasses most prior open-weight models, including its predecessor
Qwen1.5, and exhibits competitive performance relative to proprietary models
across diverse benchmarks on language understanding, generation, multilingual
proficiency, coding, mathematics, and reasoning.
The flagship model, Qwen2-72B, showcases remarkable performance: 84.2 on
MMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as
a base language model. The instruction-tuned variant, Qwen2-72B-Instruct, attains
9.1 on MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench. Moreover,
Qwen2 demonstrates robust multilingual capabilities, proficient in approximately
30 languages, spanning English, Chinese, Spanish, French, German, Arabic, Russian, Korean, Japanese, Thai, Vietnamese, and more, underscoring its versatility
and global reach.
To foster community innovation and accessibility, we have made the Qwen2 model
weights openly available on Hugging Face1 and ModelScope2, and the supplementary materials including example code on GitHub3. These platforms also include
resources for quantization, fine-tuning, and deployment, facilitating a wide range
of applications and research endeavors.",,,Open weights (unrestricted),China,,,,,,2025-05-09 11:32,,,,,,Industry,,,,,Unreleased,Apache 2.0,Industry,,,,,Operation counting,Qwen,,,
Audioseal,"Audio,Speech","Audio classification,Audio generation",Facebook AI Research,"Robin San Roman, Pierre Fernandez, Alexandre DÃ©fossez, Teddy Furon, Tuan Tran, Hady Elsahar",2024-06-06,Proactive Detection of Voice Cloning with Localized Watermarking,https://arxiv.org/abs/2401.17264,,,,,,,,VoxPopuli,"Our watermark generator and detector are trained on a 4.5K hours subset from the VoxPopuli (Wang et al., 2021) dataset.",61560000.0,"4,500 hours * 13,680 English words per hour = 61560000 words",,,,,Confident,"In the rapidly evolving field of speech generative models, there is a pressing need to ensure audio authenticity against the risks of voice cloning. We present AudioSeal, the first audio watermarking technique designed specifically for localized detection of AI-generated speech. AudioSeal employs a generator/detector architecture trained jointly with a localization loss to enable localized watermark detection up to the sample level, and a novel perceptual loss inspired by auditory masking, that enables AudioSeal to achieve better imperceptibility. AudioSeal achieves state-of-the-art performance in terms of robustness to real life audio manipulations and imperceptibility based on automatic and human evaluation metrics. Additionally, AudioSeal is designed with a fast, single-pass detector, that significantly surpasses existing models in speed - achieving detection up to two orders of magnitude faster, making it ideal for large-scale and real-time applications.",,,,United States of America,,,,,,2025-05-01 10:42,,,,,,Industry,,,,,Open source,"The code in this repository is released under the MIT license
https://github.com/facebookresearch/audioseal

training code: https://github.com/facebookresearch/audioseal/blob/main/docs/TRAINING.md 

looks like training scripts are in this repo: 
https://github.com/facebookresearch/audiocraft/blob/main/audiocraft/solvers/watermark.py ",Industry,,,,,,,,,
ProTrek,Biology,Protein or nucleotide language model (pLM/nLM),Westlake University,"Jin Su, Xibin Zhou, Xuting Zhang, Fajie Yuan",2024-06-03,"ProTrek: Navigating the Protein Universe through
Tri-Modal Contrastive Learning",https://www.biorxiv.org/content/10.1101/2024.05.30.596740v2.abstract,1.0,,,930000000.0,,,,,,45000000001.0,"40M examples
Step 1: 14M (Swiss-Prot) + 25M (UniRef50) â‰ˆ 40M examples

Step 2: 512 (sequence) + 512 (structure) + 100 (text) = 1,124 tokens per example

Step 3: 40,000,000 Ã— 1,124 = 4.496 Ã— 10Â¹â° tokens

Final result: 4.496 Ã— 10Â¹â° tokens",,,NVIDIA A100,,Confident,"ProTrek redefines protein exploration by seamlessly fusing sequence, structure, and natural language function (SSF) into an advanced tri-modal language model. Through contrastive learning, ProTrek bridges the gap between protein data and human understanding, enabling lightning-fast searches across nine SSF pairwise modality combinations. Trained on vastly larger datasets, ProTrek demonstrates quantum leaps in performance: (1) Elevating protein sequence-function interconversion by 30-60 fold; (2) Surpassing current alignment tools (i.e., Foldseek and MMseqs2) in both speed (100-fold acceleration) and accuracy, identifying functionally similar proteins with diverse structures; and (3) Outperforming ESM-2 in 9 of 11 downstream prediction tasks, setting new benchmarks in protein intelligence. These results suggest that ProTrek will become a core tool for protein searching, understanding, and analysis.",,,,China,ESM2-650M,,,20.0,,2025-05-01 10:42,,,,,,Academia,,,,,,,Academia,,,,15797.806687181936,,,,,
Prot2Token,Biology,Protein or nucleotide language model (pLM/nLM),"University of Missouri,Politecnico di Milano","Mahdi Pourmirzaei, Farzaneh Esmaili, Mohammadreza Pourmirzaei, Duolin Wang, Dong Xu",2024-06-03,Prot2Token: A multi-task framework for protein language processing using autoregressive language modeling,https://www.biorxiv.org/content/10.1101/2024.05.31.596915v1.abstract,1.0,,,650000000.0,,,, AlphaFold database (AFDB),"Many datasets were used, see Table 8",310000001.0,"1,024,055 = 8,678 + 53,571 + 21,446 + 15,550 + 29,898 + 12,312 + 29,215 + 23,604 + 35,669 + 8,716 + 300,700 + 6,391 + 16,436 + 22,841 + 10,400 + 428,628

307,216,500 = 1,024,055 Ã— 300

Final estimate: 3.1 Ã— 10â¸",,,,,Confident,"This paper proposes a versatile tokenization method and introduces Prot2Token, a model that combines autoregressive language modeling with protein language models (PLMs) to tackle various protein prediction tasks using protein sequences. Leveraging our tokenization method, Prot2Token adapts existing PLMs for multiple tasks such as protein-level prediction, residue-level prediction, and protein-protein interaction prediction through next-token prediction of tokenized target label sequences. By incorporating prompt tokens into the decoder, Prot2Token enables multi-task training in a single end-to-end session. Our results demonstrate that Prot2Token not only matches the performance of specialized models across various tasks but also paves the way for integrating protein tasks with large language models (LLMs), representing an important step towards creating general-purpose PLMs for advanced protein language processing (PLP). Additionally, we use Prot2Token to develop S-ESM, a structure-aware version of the ESM model, which achieves competitive performance with state-of-the-art methods in 3D structure-related tasks using only protein sequences. Code is available at: https://github.com/mahdip72/prot2token.",16.0,,,"United States of America,Italy",ESM2-650M,,,,,2025-05-01 10:42,,,,,,Academia,,,,,,,Academia,,,,,,,,,
MiniCPM-2.4B,Language,"Language modeling/generation,Code generation,Translation","Tsinghua University,ModelBest","Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, Xinrong Zhang, Zheng Leng Thai, Kaihuo Zhang, Chongyi Wang, Yuan Yao, Chenyang Zhao, Jie Zhou, Jie Cai, Zhongwu Zhai, Ning Ding, Chao Jia, Guoyang Zeng, Dahai Li, Zhiyuan Liu, Maosong Sun",2024-06-03,MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies,https://arxiv.org/abs/2404.06395,,,,2442057984.0,Table 2,1.584e+22,6 FLOP / parameter / token * 2.4*10^9 parameters * 1.1*10^12 tokens = 1.584e+22 FLOP,"Common Crawl,Dolma,C4,The Pile","The models were trained on a mixture of datasets including:

CommonCrawl Chinese (25%)
Code Pretrain (25%)
Dolma (24%)
C4 (15%)
Pile (8%)
Other smaller datasets including peS2o, Arxiv, and Open Web Math (3% combined)",1100000000000.0,1.1T tokens (Table 2),,,,,Confident,"The burgeoning interest in developing Large Language Models (LLMs) with up to trillion parameters has been met with concerns regarding resource efficiency and practical expense, particularly given the immense cost of experimentation. This scenario underscores the importance of exploring the potential of Small Language Models (SLMs) as a resource-efficient alternative. In this context, we introduce MiniCPM, specifically the 1.2B and 2.4B non-embedding parameter variants, not only excel in their respective categories but also demonstrate capabilities on par with 7B-13B LLMs. While focusing on SLMs, our approach exhibits scalability in both model and data dimensions for future LLM research. Regarding model scaling, we employ extensive model wind tunnel experiments for stable and optimal scaling. For data scaling, we introduce a Warmup-Stable-Decay (WSD) learning rate scheduler (LRS), conducive to continuous training and domain adaptation. We present an in-depth analysis of the intriguing training dynamics that occurred in the WSD LRS. With WSD LRS, we are now able to efficiently study data-model scaling law without extensive retraining experiments on both axes of model and data, from which we derive the much higher compute optimal data-model ratio than Chinchilla Optimal. Additionally, we introduce MiniCPM family, including MiniCPM-DPO, MiniCPM-MoE and MiniCPM-128K, whose excellent performance further cementing MiniCPM's foundation in diverse SLM applications. MiniCPM models are available publicly at this https URL .",,,Open weights (restricted use),"China,China",,,,,,2025-05-01 10:42,,,,4000000.0,,"Academia,Industry",,,,,Unreleased,"https://huggingface.co/openbmb/MiniCPM-2B-sft-bf16

This repository is released under the Apache-2.0 License.

The usage of MiniCPM model weights must strictly follow the General Model License (GML).

The models and weights of MiniCPM are completely free for academic research.

If you intend to utilize the model for commercial purposes, please reach out to cpm@modelbest.cn to obtain the certificate of authorization.","Academia,Industry",,,,,Operation counting,,,,
MiniCPM-3-4B,Language,"Language modeling/generation,Code generation,Translation","Tsinghua University,ModelBest","Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, Xinrong Zhang, Zheng Leng Thai, Kaihuo Zhang, Chongyi Wang, Yuan Yao, Chenyang Zhao, Jie Zhou, Jie Cai, Zhongwu Zhai, Ning Ding, Chao Jia, Guoyang Zeng, Dahai Li, Zhiyuan Liu, Maosong Sun",2024-06-03,MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies,https://huggingface.co/openbmb/MiniCPM3-4B,,,,4000000000.0,4b,,,,,,,,,,,Confident,"MiniCPM3-4B is the 3rd generation of MiniCPM series. The overall performance of MiniCPM3-4B surpasses Phi-3.5-mini-Instruct and GPT-3.5-Turbo-0125, being comparable with many recent 7B~9B models.

Compared to MiniCPM1.0/MiniCPM2.0, MiniCPM3-4B has a more powerful and versatile skill set to enable more general usage. MiniCPM3-4B supports function call, along with code interpreter. Please refer to Advanced Features for usage guidelines.

MiniCPM3-4B has a 32k context window. Equipped with LLMxMapReduce, MiniCPM3-4B can handle infinite context theoretically, without requiring huge amount of memory.",,,Open weights (restricted use),"China,China",,,,,,2025-05-01 10:42,,,,,,"Academia,Industry",,,,,Unreleased,"This repository is released under the Apache-2.0 License.
The usage of MiniCPM3-4B model weights must strictly follow MiniCPM Model License.md.
The models and weights of MiniCPM3-4B are completely free for academic research. after filling out a ""questionnaire"" for registration, are also available for free commercial use.","Academia,Industry",,,,,,,,,
MiniCPM-1.2B,Language,"Language modeling/generation,Code generation,Translation","Tsinghua University,ModelBest","Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, Xinrong Zhang, Zheng Leng Thai, Kaihuo Zhang, Chongyi Wang, Yuan Yao, Chenyang Zhao, Jie Zhou, Jie Cai, Zhongwu Zhai, Ning Ding, Chao Jia, Guoyang Zeng, Dahai Li, Zhiyuan Liu, Maosong Sun",2024-06-03,MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies,https://arxiv.org/abs/2404.06395,,,,1247442432.0,Table 2,7.92e+21,6 FLOP / parameter / token * 1.2*10^9 parameters * 1.1*10^12 tokens = 7.92e+21 FLOP,"Common Crawl,Dolma,C4,The Pile","The models were trained on a mixture of datasets including:

CommonCrawl Chinese (25%)
Code Pretrain (25%)
Dolma (24%)
C4 (15%)
Pile (8%)
Other smaller datasets including peS2o, Arxiv, and Open Web Math (3% combined)",1100000000000.0,1.1T tokens (Table 2),,,,,Confident,"The burgeoning interest in developing Large Language Models (LLMs) with up to trillion parameters has been met with concerns regarding resource efficiency and practical expense, particularly given the immense cost of experimentation. This scenario underscores the importance of exploring the potential of Small Language Models (SLMs) as a resource-efficient alternative. In this context, we introduce MiniCPM, specifically the 1.2B and 2.4B non-embedding parameter variants, not only excel in their respective categories but also demonstrate capabilities on par with 7B-13B LLMs. While focusing on SLMs, our approach exhibits scalability in both model and data dimensions for future LLM research. Regarding model scaling, we employ extensive model wind tunnel experiments for stable and optimal scaling. For data scaling, we introduce a Warmup-Stable-Decay (WSD) learning rate scheduler (LRS), conducive to continuous training and domain adaptation. We present an in-depth analysis of the intriguing training dynamics that occurred in the WSD LRS. With WSD LRS, we are now able to efficiently study data-model scaling law without extensive retraining experiments on both axes of model and data, from which we derive the much higher compute optimal data-model ratio than Chinchilla Optimal. Additionally, we introduce MiniCPM family, including MiniCPM-DPO, MiniCPM-MoE and MiniCPM-128K, whose excellent performance further cementing MiniCPM's foundation in diverse SLM applications. MiniCPM models are available publicly at this https URL .",,,Open weights (restricted use),"China,China",,,,,,2025-05-01 10:42,,,,4000000.0,,"Academia,Industry",,,,,Unreleased,"https://huggingface.co/openbmb/MiniCPM-1B-sft-bf16

This repository is released under the Apache-2.0 License.

The usage of MiniCPM model weights must strictly follow the General Model License (GML).

The models and weights of MiniCPM are completely free for academic research.

If you intend to utilize the model for commercial purposes, please reach out to cpm@modelbest.cn to obtain the certificate of authorization.","Academia,Industry",,,,,Operation counting,,,,
MULAN,Biology,Protein or nucleotide language model (pLM/nLM),"AIRI Artificial Intelligence Research Institute,Skolkovo Institute of Science and Technology,Belozersky Institute of Physio-Chemical Biology,Ligand Pro","Daria Frolova, Marina A. Pak, Anna Litvin, Ilya Sharov, Dmitry N. Ivankov, Ivan Oseledets",2024-06-02,MULAN: Multimodal Protein Language Model for Sequence and Structure Encoding,https://www.biorxiv.org/content/10.1101/2024.05.30.596565v1,0.0,,,35000000.0,,5.1777408000000007e+20,"Finetune: 989500000000000*9days*0.4=307774080000000000000

Base model: 209999999999999970000.00

Total: 209999999999999970000.00+307774080000000000000=517774080000000000000", AlphaFold database (AFDB),,5100000000.0,"""It contains 17M AlphaFold protein structures not shorter
than 30 residues.""

Assuming 300 tokens / amino acids per structure
17M*300=5100000000",,,NVIDIA H100 SXM5 80GB,,Confident,"Most protein language models (PLMs), which are used to produce high-quality protein representations, use only protein sequences during training. However, the known protein structure is crucial in many protein property prediction tasks, so there is a growing interest in incorporating the knowledge about the protein structure into a PLM. In this study, we propose MULAN, a MULtimodal PLM for both sequence and ANgle-based structure encoding. MULAN has a pre-trained sequence encoder and an introduced Structure Adapter, which are then fused and trained together. According to the evaluation on 7 downstream tasks of various nature, both small and medium-sized MULAN models show consistent improvement in quality compared to both sequence-only ESM-2 and structure-aware SaProt. Importantly, our model offers a cheap increase in the structural awareness of the protein representations due to finetuning of existing PLMs instead of training from scratch. We perform a detailed analysis of the proposed model and demonstrate its awareness of the protein structure. The implementation, training data and model checkpoints are available at https://github.com/DFrolova/MULAN.",15.0,,Open weights (unrestricted),"Russia,Russia,Russia,Russia",ESM2-35M,,,1.0,,2025-06-09 15:13,,,,,,"Research collective,Academia,Academia,Industry",,,,,Open source,"MIT license for code
https://github.com/DFrolova/MULAN

MIT license for weights
https://huggingface.co/DFrolova/MULAN-ESM2-35M","Research collective,Academia,Academia,Industry",,,,759.5268508808502,Hardware,DFrolova,,,
DRGN-AI,Biology,Cryo-EM image reconstruction,"Stanford University,SLAC National Laboratory,Princeton University,Columbia University","Axel Levy, Michal Grzadkowski, FrÃ©dÃ©ric Poitevin, Francesca Vallese, Oliver Biggs Clarke, Gordon Wetzstein, Ellen D. Zhong",2024-06-02,Revealing biomolecular structure and motion with neural ab initio cryo-EM reconstruction,https://www.biorxiv.org/content/10.1101/2024.05.30.596729v1,,,,,,6.469e+19,48*60*60*4*312000000000000*0.3=6.469632e+19,,,710437.0,"710,437 Table S1",48.0,Table S1,NVIDIA A100,,Confident,"Proteins and other biomolecules form dynamic macromolecular machines that are tightly orchestrated to move, bind, and perform chemistry. Cryo-electron microscopy (cryo-EM) can access the intrinsic heterogeneity of these complexes and is therefore a key tool for understanding mechanism and function. However, 3D reconstruction of the resulting imaging data presents a challenging computational problem, especially without any starting information, a setting termed ab initio reconstruction. Here, we introduce a method, DRGN-AI, for ab initio heterogeneous cryo-EM reconstruction. With a two-step hybrid approach combining search and gradient-based optimization, DRGN-AI can reconstruct dynamic protein complexes from scratch without input poses or initial models. Using DRGN-AI, we reconstruct the compositional and conformational variability contained in a variety of benchmark datasets, process an unfiltered dataset of the DSL1/SNARE complex fully ab initio, and reveal a new â€œsupercomplexâ€ state of the human erythrocyte ankyrin-1 complex. With this expressive and scalable model for structure determination, we hope to unlock the full potential of cryo-EM as a high-throughput tool for structural biology and discovery.",,,,"United States of America,United States of America,United States of America",,,,4.0,,2025-05-01 10:42,,,,,,"Academia,Academia,Academia",,,,,,,"Academia,Academia,Academia",,,,3159.631699664337,,,,,
KeTu (Kolors),Image generation,"Image generation,Text-to-image",Kuaishou Technology,Kolors Team,2024-05-31,Kolors: Effective Training of Diffusion Model for Photorealistic Text-to-Image Synthesis,"https://github.com/Kwai-Kolors/Kolors/blob/master/imgs/Kolors_paper.pdf

https://ir.kuaishou.com/news-releases/news-release-details/kuaishou-unveils-comprehensive-ai-models-reshaping-content-and",,,,2600000000.0,2.6B (Table 4),,,"LAION,JourneyDB,Unspecified unreleased","The training of Kolors is divided into two distinct phases: the concept learning phase and the quality improvement phase. During the concept learning phase, the model primarily acquires comprehensive knowledge and concepts from a large-scale dataset comprising billions of image-text pairs. The data
for this phase is sourced from public datasets (e.g. , LAION [35], DataComp [11], JourneyDB [37]) as well as proprietary datasets.

In the quality improvement phase, the focus shifts to enhancing image details and aesthetics at high resolutions. This methodology ultimately yields millions of Level 5 high-aesthetic images",,,,,,,Confident,"We present Kolors, a latent diffusion model for text-to-image synthesis, characterized by its profound understanding of both English and Chinese, as well as an impressive degree of photorealism. There are three key insights contributing to the development of Kolors. Firstly, unlike large language model T5 used in Imagen and Stable Diffusion 3, Kolors is built upon the General Language Model (GLM), which enhances its comprehension capabilities in both English and Chinese.
Moreover, we employ a multimodal large language model to recaption the extensive training dataset for fine-grained text understanding. These strategies significantly improve Kolorsâ€™ ability to comprehend intricate semantics, particularly those involving multiple entities, and enable its advanced text rendering capabilities.
Secondly, we divide the training of Kolors into two phases: the concept learning phase with broad knowledge and the quality improvement phase with specifically curated high-aesthetic data. Furthermore, we investigate the critical role of the noise schedule and introduce a novel schedule to optimize high-resolution image generation. These strategies collectively enhance the visual appeal of the generated high-resolution images. Lastly, we propose a category-balanced benchmark KolorsPrompts, which serves as a guide for the training and evaluation of Kolors. Consequently, even when employing the commonly used U-Net backbone, Kolors has demonstrated remarkable performance in human evaluations, surpassing the existing open-source models and achieving Midjourney-v6 level performance, especially in terms of visual appeal. We will release the code and weights of Kolors at https://github.com/Kwai-Kolors/Kolors, and hope that it will benefit future research and applications in the visual generation community.",,,Open weights (restricted use),China,ChatGLM3-6B,,,,,2025-05-14 11:16,,,,,,Industry,,,,,Open source,"Apache 2.0 though they put restrictions on commercial use: 
""Kolors are fully open-sourced for academic research. For commercial use, please fill out this questionnaire and sent it to kwai-kolors@kuaishou.com for registration.""

https://github.com/Kwai-Kolors/Kolors

https://huggingface.co/Kwai-Kolors/Kolors",Industry,,,,,,,,,
"Mamba 2, 2.7B",Language,"Language modeling/generation,Question answering","Princeton University,Carnegie Mellon University (CMU)","Tri Dao, Albert Gu",2024-05-31,Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality,https://arxiv.org/abs/2405.21060,,,,2700000000.0,2.7B,4.86e+21,6ND = 6*2.7B*300B = 4.86Ã—10^21,The Pile,,300000000000.0,300B tokens,,,,,Confident,"While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.",,,Open weights (unrestricted),"United States of America,United States of America",,,,,,2025-05-01 10:42,,,,,,"Academia,Academia",,,,,,"Apache 2

https://github.com/state-spaces/mamba
https://huggingface.co/state-spaces/mamba2-2.7b","Academia,Academia",,,,,Operation counting,,,,
Granite 20B,Language,Language modeling/generation,IBM Research,IBM Research,2024-05-31,Granite Foundation Models,https://www.ibm.com/downloads/documents/us-en/10a99803c92fdb35,,,Unclear how many users,20000000000.0,,3.0000000000001e+23,6*2500000000000*20000000000=3e+23,"Stack Exchange,Common Crawl,Wikimedia","""For English and code, we used Wikimedia, Stack Exchange, and commoncrawl. For multilingual data, we used portions of commoncrawl.""",2500000000000.0,"For pre-training, we used 0.5 trillion English, 0.4 trillion multilingual (es, fr, de, pt), and 1.6 trillion code tokens.",,,,,Confident,"We introduce the Granite series of decoder-only foundation models for generative artificial intelligence (AI) tasks that are ready for enterprise use. We report on the architecture, capabilities, underlying data and data governance, training algorithms, compute infrastructure, energy and carbon footprint, testing and evaluation, socio-technical harms and mitigations, and usage policies.",,,Open weights (unrestricted),"United States of America,Multinational,Ireland,United Kingdom of Great Britain and Northern Ireland,Brazil,Switzerland",,,,,,2025-05-26 18:11,,,,,,Industry,,,,,Unreleased,"Apache 2.0 
https://huggingface.co/ibm-granite/granite-20b-code-base-8k

no pretraining code here, inference and fine-tuning only
https://github.com/ibm-granite/granite-code-models",Industry,,,,,Operation counting,ibm-granite,,,
sgRNAGen,Biology,RNA design,Beijing Institute of Technology,"Yan Xia, Zeyu Liang, Xiaowen Du, Dengtian Cao, Jing Li, Lichao Sun, Yi-Xin Huo, Shuyuan Guo",2024-05-31,Design nonrepetitive and diverse activity single-guide RNA by deep learning,https://www.biorxiv.org/content/10.1101/2024.05.30.596019v1,,,,14158450.0,""" with our model comprising 8 layers and 12 heads. ""
""The vocabulary of sgRNAGen encompasses383
seven tokens: A, T, C, G, and special tokens such as [PAD], [MASK], and [UNK]""
""The hidden size386
was set as 384""

8 layers, 12 heads, vocab of 7, d_model of 384
d_head=384/12=32
Attention parameters: 12*(384*(2*32+32)+32*384)=589824
MLP = 384*4*384*2= 1179648

Total: 7*382+8*(589824+1179648)=14158450

",,,,,2440000.0,"""40,000 CRISPR-related129
RNA sequences have been utilized for training the sgRNAGen model""
Generated test cases are 61 tokens, assuming this also applies to training data

40000*61=2440000",,,,,Confident,"Multiplex and precise control of the gene expression based on CRISPR/Cas9 is important to metabolic regulation in synthetic biology. However, employing single guide RNAs (sgRNAs) that possess repetitive DNA sequences and exhibit uniform activity could detrimentally affect the editing process, undermining both its stability and regulatory potential. In this study, we developed a deep generative model based on a decoder-only Transformer architecture (sgRNAGen) for the de novo generation of a series of nonrepetitive and diverse sgRNAs with activity. To assess the quality of sgRNAs generated by sgRNAGen, we evaluated their activity by targeting essential genes, with the results indicating that 98% of the generated sgRNAs were active in Bacillus subtilis. The generated sgRNAs were further validated for applications in single-gene editing, large fragment knockouts, and multiplex editing. Notably, the efficiency of knocking out long fragments up to 169.5 kb reached 100%, and targeting multiple sites allowed for the creation of strains with various combinations of mutations in a single editing. Furthermore, we developed a CRISPRi system utilizing the designed sgRNAs to regulate gene expression with desired strength and high precision. SgRNAGen offers a method for devising nonrepetitive and diverse activity sgRNAs, enhancing metabolic control and advancing applications within synthetic biology.",,,,China,,,,,,2025-05-01 10:42,,,,,,Academia,,,,,,,Academia,,,,,,,,,
FoldFlow2,Biology,"Protein generation,Protein design","Dreamfold,University of Montreal / UniversitÃ© de MontrÃ©al,McGill University,University of Oxford","Guillaume Huguet, James Vuckovic, Kilian Fatras, Eric Thibodeau-Laufer, Pablo Lemos, Riashat Islam, Cheng-Hao Liu, Jarrid Rector-Brooks, Tara Akhound-Sadegh, Michael Bronstein, Alexander Tong, Avishek Joey Bose",2024-05-30,Sequence-Augmented SE(3)-Flow Matching For Conditional Protein Backbone Generation,https://arxiv.org/abs/2405.20313,8.0,,,,,7.646000000001e+21,"1. Hardware setup: 2x NVIDIA A100 40GB GPUs (3.12 x 10^14 FLOP/s per GPU)
2. Training duration: 4 days (directly provided) = 345,600 seconds
3. Utilization rate: 40%
4. Calculation: 2 GPUs Ã— 3.12Ã—10^14 FLOP/s Ã— 345,600s Ã— 0.4 = 8.6Ã—10^19 FLOPs

Base model: 7.560000000001e+21
Total: 7646000000001000000000",,,35520000.0,"Average protein length = (60 + 384) / 2 = 222 residues
Total datapoints = 160,000 structures Ã— 222 residues = 35,520,000
",96.0,,NVIDIA A100,,Confident,"Proteins are essential for almost all biological processes and derive their diverse functions from complex 3D structures, which are in turn determined by their amino acid sequences. In this paper, we exploit the rich biological inductive bias of amino acid sequences and introduce FoldFlow-2, a novel sequence-conditioned SE(3)-equivariant flow matching model for protein structure generation. FoldFlow-2 presents substantial new architectural features over the previous FoldFlow family of models including a protein large language model to encode sequence, a new multi-modal fusion trunk that combines structure and sequence representations, and a geometric transformer based decoder. To increase diversity and novelty of generated samples -- crucial for de-novo drug design -- we train FoldFlow-2 at scale on a new dataset that is an order of magnitude larger than PDB datasets of prior works, containing both known proteins in PDB and high-quality synthetic structures achieved through filtering. We further demonstrate the ability to align FoldFlow-2 to arbitrary rewards, e.g. increasing secondary structures diversity, by introducing a Reinforced Finetuning (ReFT) objective. We empirically observe that FoldFlow-2 outperforms previous state-of-the-art protein structure-based generative models, improving over RFDiffusion in terms of unconditional generation across all metrics including designability, diversity, and novelty across all protein lengths, as well as exhibiting generalization on the task of equilibrium conformation sampling. Finally, we demonstrate that a fine-tuned FoldFlow-2 makes progress on challenging conditional design tasks such as designing scaffolds for the VHH nanobody.",,,Open weights (non-commercial),"Canada,Canada,Canada,United Kingdom of Great Britain and Northern Ireland",ESM2-650M,,,2.0,,2025-06-16 11:06,,,,,,"Industry,Academia,Academia,Academia",,,,,Open (non-commercial),"CC NC 4.0
https://github.com/DreamFold/FoldFlow
weights
https://github.com/DreamFold/FoldFlow/releases/tag/0.2.0","Industry,Academia,Academia,Academia",,,,1579.9213978749815,Hardware,,,,
Codestral,Language,"Code generation,Code autocompletion",Mistral AI,"Albert Jiang, Alexandre Sablayrolles, Alexis Tacnet, Antoine Roux, Arthur Mensch, Audrey Herblin-Stoop, Baptiste Bout, Baudouin de Monicault, Blanche Savary, Bam4d, Caroline Feldman, Devendra Singh Chaplot, Diego de las Casas, Eleonore Arcelin, Emma Bou Hanna, Etienne Metzger, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Harizo Rajaona, Henri Roussez, Jean-Malo Delignon, Jia Li, Justus Murke, Kartik Khandelwal, Lawrence Stewart, Louis Martin, Louis Ternon, Lucile Saulnier, LÃ©lio Renard Lavaud, Margaret Jennings, Marie Pellat, Marie Torelli, Marie-Anne Lachaux, Marjorie Janiewicz, Mickael Seznec, Nicolas Schuhl, Patrick von Platen, Romain Sauvestre, Pierre Stock, Sandeep Subramanian, Saurabh Garg, Sophia Yang, Szymon Antoniak, Teven Le Scao, Thibaut Lavril, Thibault Schueller, TimothÃ©e Lacroix, ThÃ©ophile Gervet, Thomas Wang, Valera Nemychnikova, Wendy Shang, William El Sayed, William Marshall",2024-05-29,,https://mistral.ai/news/codestral/,,,,22200000000.0,22.2B from hugging face model card,,,Unspecified unreleased,"Codestral is trained on a diverse dataset of 80+ programming languages, including the most popular ones, such as Python, Java, C, C++, JavaScript, and Bash. It also performs well on more specific ones like Swift and Fortran. ",,,,,,,Confident,"We introduce Codestral, our first-ever code model. Codestral is an open-weight generative AI model explicitly designed for code generation tasks. It helps developers write and interact with code through a shared instruction and completion API endpoint. As it masters code and English, it can be used to design advanced AI applications for software developers.

A model fluent in 80+ programming languages
Codestral is trained on a diverse dataset of 80+ programming languages, including the most popular ones, such as Python, Java, C, C++, JavaScript, and Bash. It also performs well on more specific ones like Swift and Fortran. This broad language base ensures Codestral can assist developers in various coding environments and projects.

Codestral saves developers time and effort: it can complete coding functions, write tests, and complete any partial code using a fill-in-the-middle mechanism. Interacting with Codestral will help level up the developerâ€™s coding game and reduce the risk of errors and bugs.",,,Open weights (non-commercial),France,,,,,,2025-04-09 10:56,,,,,,Industry,,,,,Unreleased,"Codestral is a 22B open-weight model licensed under the new Mistral AI Non-Production License, which means that you can use it for research and testing purposes. Codestral can be downloaded on HuggingFace.",Industry,,,,,,,,,
Aurora,Earth science,Weather forecasting,Microsoft Research,"Cristian Bodnar, Wessel P. Bruinsma, Ana Lucic, Megan Stanley, Johannes Brandstetter, Patrick Garvan, Maik Riechert, Jonathan Weyn, Haiyu Dong, Anna Vaughan, Jayesh K. Gupta, Kit Tambiratnam, Alex Archibald, Elizabeth Heider, Max Welling, Richard E. Turner, Paris Perdikaris",2024-05-28,Aurora: A Foundation Model of the Atmosphere,https://arxiv.org/abs/2405.13063,,,,1300000000.0,,4.5287424e+21,"""The model is trained using bf16 mixed
precision.""

312000000000000*420*32*3600*.3 =  4528742400000000000000 FLOP

",,,700000000000.0,"700B tokens (see Figure 4)
Total 1,219.91 TB 11,023,730 frames

""All models are pretrained for 150 k steps on 32 GPUs, with a batch size of one per GPU.""",420.0,"""32 A100 GPUs, which corresponds to approximately two and a half weeks of training""

17,5 days * 24 hours = 420",NVIDIA A100,,Confident,"Deep learning foundation models are revolutionizing many facets of science by leveraging vast amounts of data to learn general-purpose representations that can be adapted to tackle diverse downstream tasks. Foundation models hold the promise to also transform our ability to model our planet and its subsystems by exploiting the vast expanse of Earth system data. Here we introduce Aurora, a large-scale foundation model of the atmosphere trained on over a million hours of diverse weather and climate data. Aurora leverages the strengths of the foundation modelling approach to produce operational forecasts for a wide variety of atmospheric prediction problems, including those with limited training data, heterogeneous variables, and extreme events. In under a minute, Aurora produces 5-day global air pollution predictions and 10-day high-resolution weather forecasts that outperform state-of-the-art classical simulation tools and the best specialized deep learning models. Taken together, these results indicate that foundation models can transform environmental forecasting.",,,Unreleased,"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational",,,,32.0,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,,Industry,,,BF16,25279.86827447104,Hardware,,,,
TimeGPT-1,Other,,Nixtla,"Azul Garza, Cristian Challu, Max Mergenthaler-Canseco",2024-05-27,TimeGPT-1,https://arxiv.org/abs/2310.03589,,,,,,,,,"""training set incorporates time series from a broad array of domains, including finance, economics, demographics, healthcare, weather, IoT sensor data, energy, web traffic, sales, transport, and banking.""",100000000000.0,"""TimeGPT was trained on, to our knowledge, the largest collection of publicly available time series,collectively encompassing over 100 billion data points.""",,"""TimeGPT underwent a multi-day training period on a cluster of NVIDIA A10G GPUs.""",NVIDIA A10G,,Confident,"In this paper, we introduce TimeGPT, the first foundation model for time series, capable of generating accurate predictions for diverse datasets not seen during training. We evaluate our pre-trained model against established statistical, machine learning, and deep learning methods, demonstrating that TimeGPT zero-shot inference excels in performance, efficiency, and simplicity. Our study provides compelling evidence that insights from other domains of artificial intelligence can be effectively applied to time series analysis. We conclude that large-scale time series models offer an exciting opportunity to democratize access to precise predictions and reduce uncertainty by leveraging the capabilities of contemporary advancements in deep learning.",,,,United States of America,,,,,,2025-05-01 10:42,,,,,,Industry,,,,,,,Industry,,,,,,,,,
NV-Embed-v1,Language,"Language modeling/generation,Question answering",NVIDIA,"Chankyu Lee, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad Shoeybi, Bryan Catanzaro, Wei Ping",2024-05-27,NV-Embed: Improved Techniques for Training LLMs as Generalist Embedding Models,https://arxiv.org/abs/2405.17428,,,,7000000000.0,,,,,,2490368000.0,"""The model is finetuned with 128 batch
size, where each batch is composed of a query paired with 1 positive and 7 hard negative documents.
We train using Bfloat16, and set the maximum sequence length as 512 tokens.""

First stage - 20k
Second stage - 18k

128*512*38000=2490368000",,,,,Confident,"Decoder-only large language model (LLM)-based embedding models are beginning to outperform BERT or T5-based embedding models in general-purpose text embedding tasks, including dense vector-based retrieval. In this work, we introduce the NV-Embed model with a variety of architectural designs and training procedures to significantly enhance the performance of LLM as a versatile embedding model, while maintaining its simplicity and reproducibility. For model architecture, we propose a latent attention layer to obtain pooled embeddings, which consistently improves retrieval and downstream task accuracy compared to mean pooling or using the last <EOS> token embedding from LLMs. To enhance representation learning, we remove the causal attention mask of LLMs during contrastive training. For model training, we introduce a two-stage contrastive instruction-tuning method. It first applies contrastive training with instructions on retrieval datasets, utilizing in-batch negatives and curated hard negative examples. At stage-2, it blends various non-retrieval datasets into instruction tuning, which not only enhances non-retrieval task accuracy but also improves retrieval performance. Combining these techniques, our NV-Embed model, using only publicly available data, has achieved a record-high score of 69.32, ranking No. 1 on the Massive Text Embedding Benchmark (MTEB) (as of May 24, 2024), with 56 tasks, encompassing retrieval, reranking, classification, clustering, and semantic textual similarity tasks. Notably, our model also attains the highest score of 59.36 on 15 retrieval tasks in the MTEB benchmark (also known as BEIR). We will open-source the model at: this https URL.",,,Open weights (non-commercial),United States of America,Mistral 7B,104595460000000000000,6ND = 6*7*10^9*2490368000 = 1.0459546e+20,,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,"Creative Commons Attribution Non Commercial 4.0

https://huggingface.co/nvidia/NV-Embed-v1",Industry,,,,,Operation counting,,,,
Genie 2 (bio),Biology,Protein generation,"Columbia University,Rutgers University","Yeqing Lin, Minji Lee, Zhao Zhang, Mohammed AlQuraishi",2024-05-24,"Out of Many, One: Designing and Scaffolding Proteins at the Scale of the Structural Universe with Genie 2",https://arxiv.org/abs/2405.15489,,,,15700000.0,Table 2,3.234816e+20,"""We train Genie 2 using data parallelism on 8 Nvidia A100 GPUs with an effective batch size of 48. We train the model for 40 epochs (âˆ¼5 days) for a total of âˆ¼960 GPU hours.""

312000000000000 peak FLOP/s * 960 GPU hours * 3600 s * 0.3 assumed utilization = 3.234816e+20 ", AlphaFold database (AFDB),"we train Genie 2 using the AlphaFold database (AFDB) [39], which consists of approximately 214M AlphaFold 2 predictions spanning nearly the entirety of UniProt [14].",214000000.0,,120.0,,NVIDIA A100,,Confident,"Protein diffusion models have emerged as a promising approach for protein design. One such pioneering model is Genie, a method that asymmetrically represents protein structures during the forward and backward processes, using simple Gaussian noising for the former and expressive SE(3)-equivariant attention for the latter. In this work we introduce Genie 2, extending Genie to capture a larger and more diverse protein structure space through architectural innovations and massive data augmentation. Genie 2 adds motif scaffolding capabilities via a novel multi-motif framework that designs co-occurring motifs with unspecified inter-motif positions and orientations. This makes possible complex protein designs that engage multiple interaction partners and perform multiple functions. On both unconditional and conditional generation, Genie 2 achieves state-of-the-art performance, outperforming all known methods on key design metrics including designability, diversity, and novelty. Genie 2 also solves more motif scaffolding problems than other methods and does so with more unique and varied solutions. Taken together, these advances set a new standard for structure-based protein design. Genie 2 inference and training code, as well as model weights, are freely available at: this https URL.",40.0,,Open weights (unrestricted),"United States of America,United States of America",,,,8.0,,2025-05-01 10:42,,,,48.0,,"Academia,Academia",,,,960.0,Open source,"Apache 2
https://github.com/aqlaboratory/genie2","Academia,Academia",,,,6320.530060464735,Hardware,,,,
LLPS,Biology,Protein or nucleotide language model (pLM/nLM),InstaDeep,"Benoit Gaujac, JÃ©rÃ©mie DonÃ , Liviu Copoiu, Timothy Atkinson, Thomas Pierrot, Thomas D. Barrett",2024-05-24,Learning the Language of Protein Structure,https://arxiv.org/abs/2405.15840,4.0,,,344000000.0,,,,PDB (Protein Data Bank),,70000001.0,"70 million total structural tokens 
= 310,000 protein structures x ~225 tokens/structure
= 7.0 x 10^7 tokens",,,Google TPU v4,,Confident,"Representation learning and \emph{de novo} generation of proteins are pivotal computational biology tasks. Whilst natural language processing (NLP) techniques have proven highly effective for protein sequence modelling, structure modelling presents a complex challenge, primarily due to its continuous and three-dimensional nature. Motivated by this discrepancy, we introduce an approach using a vector-quantized autoencoder that effectively tokenizes protein structures into discrete representations. This method transforms the continuous, complex space of protein structures into a manageable, discrete format with a codebook ranging from 4096 to 64000 tokens, achieving high-fidelity reconstructions with backbone root mean square deviations (RMSD) of approximately 1-5 Ã…. To demonstrate the efficacy of our learned representations, we show that a simple GPT model trained on our codebooks can generate novel, diverse, and designable protein structures. Our approach not only provides representations of protein structure, but also mitigates the challenges of disparate modal representations and sets a foundation for seamless, multi-modal integration, enhancing the capabilities of computational methods in protein design.",250.0,,,United Kingdom of Great Britain and Northern Ireland,,,,128.0,,2025-05-01 10:42,,,,,,Industry,,,,,,,Industry,,,,42979.6044111602,,,,,
YOLOv10-X,Vision,Object detection,Tsinghua University,"Ao Wang, Hui Chen, Lihao Liu, Kai Chen, Zijia Lin, Jungong Han, Guiguang Ding",2024-05-23,YOLOv10: Real-Time End-to-End Object Detection,https://arxiv.org/abs/2405.14458,,,,29500000.0,29.5M,1.478888e+17,"6ND = 6*29500000.00*118000*500 = 1.0443e+16 (likely an underestimation)

(speculative confidence because I don't know the amount of tokens per image)

assuming(!) batch size 64 -> number of updates per epoch = 118000/64 = 1844

160.4 Gigaflops * 1844 * 500 = 1.478888e+17",COCO,,118000.0,size of COCO 2017,, All models are trained on 8 NVIDIA 3090 GPUs,NVIDIA GeForce RTX 3090 Ti,,Likely,YOLOv10 is created by researchers from Tsinghua University using the Ultralytics Python package. This version provides real-time object detection advancements by introducing an End-to-End head that eliminates Non-Maximum Suppression (NMS) requirements.,500.0,,Open weights (unrestricted),China,,,,8.0,,2025-06-11 18:00,,,,,,Academia,,,,,Open source,"https://github.com/THU-MIG/yolov10

AGPL-3.0 license",Academia,,,,7110.754668295396,Operation counting,,,,
360Zhinao-7B,Language,"Question answering,Language modeling/generation",360 Security Technology,360zhinao,2024-05-22,360Zhinao Technical Report,https://arxiv.org/abs/2405.13386,0.0,,,7000000000.0,,1.4279999999999998e+23,6*3400000000000*7000000000=1.428e+23,,,3400000000000.0,3.4 trillion tokens,,,,,Confident,"We present 360Zhinao models with 7B parameter size and context lengths spanning 4K, 32K and 360K, all available at this https URL. For rapid development in pretraining, we establish a stable and sensitive ablation environment to evaluate and compare experiment runs with minimal model size. Under such guidance, we perfect our data cleaning and composition strategies to pretrain 360Zhinao-7B-Base on 3.4T tokens. We also mainly emphasize data during alignment, where we strive to balance quantity and quality with filtering and reformatting. With tailored data, 360Zhinao-7B's context window is easily extended to 32K and 360K. RMs and RLHF are trained following SFT and credibly applied to specific tasks. All together these contributions lead to 360Zhinao-7B's competitive performance among models of similar size.",,,Open weights (unrestricted),China,,,,,,2025-05-23 10:28,,,,,,Industry,,,,,Open source,"Apache 2.0
https://huggingface.co/qihoo360/360Zhinao-7B-Base

https://github.com/Qihoo360/360zhinao?tab=readme-ov-file",Industry,,,,,,qihoo360,,,
ProtT3,Biology,Protein question answering,"National University of Singapore,University of Science and Technology of China (USTC),Hokkaido University","Zhiyuan Liu, An Zhang, Hao Fei, Enzhi Zhang, Xiang Wang, Kenji Kawaguchi, Tat-Seng Chua",2024-05-21,ProtT3: Protein-to-Text Generation for Text-based Protein Understanding,https://arxiv.org/abs/2405.12564,8.0,,,1600000000.0,Table 12,,,"SwissProt,ProteinKG25",,1362012358.0,"430,595 + 422,315 + 3,360,000 = 4,212,910 datapoints
4,212,910 Ã— 300 = 1,263,873,000 tokens
Final estimate: 1.3 billion tokens
Exact numbers from Table 2
430595*(336+48)+422315*(338+101)+3359693*(291+10)=1362012358",,,"NVIDIA A100,NVIDIA V100",,Likely,"Language Models (LMs) excel in understanding textual descriptions of proteins, as evident in biomedical question-answering tasks. However, their capability falters with raw protein data, such as amino acid sequences, due to a deficit in pretraining on such data. Conversely, Protein Language Models (PLMs) can understand and convert protein data into high-quality representations, but struggle to process texts. To address their limitations, we introduce ProtT3, a framework for Protein-to-Text Generation for Text-based Protein Understanding. ProtT3 empowers an LM to understand protein sequences of amino acids by incorporating a PLM as its protein understanding module, enabling effective protein-to-text generation. This collaboration between PLM and LM is facilitated by a cross-modal projector (i.e., Q-Former) that bridges the modality gap between the PLM's representation space and the LM's input space. Unlike previous studies focusing on protein property prediction and protein-text retrieval, we delve into the largely unexplored field of protein-to-text generation. To facilitate comprehensive benchmarks and promote future research, we establish quantitative evaluations for protein-text modeling tasks, including protein captioning, protein question-answering, and protein-text retrieval. Our experiments show that ProtT3 substantially surpasses current baselines, with ablation studies further highlighting the efficacy of its core components. Our code is available at this https URL.",60.0,,,"Singapore,China,Japan",,,,6.0,,2025-05-01 10:42,,,,,,"Academia,Academia,Academia",,,,,,,"Academia,Academia,Academia",,,,,,,,,
"ALLaM adapted13B
",Language,"Language modeling/generation,Translation,Question answering",Saudi Data and Artificial Intelligence Authority,Saudi Data and Artificial Intelligence Authority,2024-05-21,"ALLaM: Large Language Models for Arabic and English
","https://arxiv.org/abs/2407.15390
https://huggingface.co/ALLaM-AI/ALLaM-7B-Instruct-previehttps://www.middleeastainews.com/p/sdaias-allam-arabic-llm-live-on-watsonxw",9.0,SOTA improvement,"SOTA at Arabic MMLU benchmarks, all results in this paper: https://openreview.net/pdf?id=MscdsFVZrN",13000000000.0,,1.716e+23,"Finetuning compute: 
6*13000000000*1200000000000=9.36e+22

Llama 13B training FLOP: 7.8e+22

Total: 1.716e+23",,"""Starting from Llama-2 pretrained model weights, we continue pretraining
the ALLaM-7B and ALLaM-13B models on 1.2T tokens""",1200000000000.0,"3,431,217,579(4.3B) total documents, with a total of 4,587,781,981,546(4.5T) words, and 5.2T tokens.",,,NVIDIA A100,,Confident,"We present ALLaM: Arabic Large Language Model, a series of large language models to support the ecosystem of Arabic Language Technologies (ALT). ALLaM is carefully trained considering the values of language alignment and knowledge transfer at scale. Our autoregressive decoder-only architecture models demonstrate how second-language acquisition via vocabulary expansion and pretraining on a mixture of Arabic and English text can steer a model towards a new language (Arabic) without any catastrophic forgetting in the original language (English). Furthermore, we highlight the effectiveness of using parallel/translated data to aid the process of knowledge alignment between languages. Finally, we show that extensive alignment with human preferences can significantly enhance the performance of a language model compared to models of a larger scale with lower quality alignment. ALLaM achieves state-of-the-art performance in various Arabic benchmarks, including MMLU Arabic, ACVA, and Arabic Exams. Our aligned models improve both in Arabic and English from their base aligned models.",1.0,,API access,Saudi Arabia,LLaMA-13B,,,,,2025-06-02 11:56,,,,,,"Industry,Government",,,,,Unreleased,"on IBM watsonx platform
https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-models.html?context=wx","Industry,Government",,,BF16,,,,,,
ALLaMÂ adapted 70B,Language,"Language modeling/generation,Translation,Question answering",Saudi Data and Artificial Intelligence Authority,Saudi Data and Artificial Intelligence Authority,2024-05-21,"ALLaM: Large Language Models for Arabic and English
",https://arxiv.org/abs/2407.15390,9.0,SOTA improvement,"SOTA at Arabic MMLU benchmarks, all results in this paper: https://openreview.net/pdf?id=MscdsFVZrN",70000000000.0,,1.062e+24,"Llama 70B: 8.1e+23
Finetune: 6*70000000000*600000000000=252000000000000000000000
Total: 1062000000000000000000000",,,600000000000.0,"""For the ALLaM-70B model, we only train up to 600B tokens""",,,NVIDIA A100,,Confident,"We present ALLaM: Arabic Large Language Model, a series of large language models to support the ecosystem of Arabic Language Technologies (ALT). ALLaM is carefully trained considering the values of language alignment and knowledge transfer at scale. Our autoregressive decoder-only architecture models demonstrate how second-language acquisition via vocabulary expansion and pretraining on a mixture of Arabic and English text can steer a model towards a new language (Arabic) without any catastrophic forgetting in the original language (English). Furthermore, we highlight the effectiveness of using parallel/translated data to aid the process of knowledge alignment between languages. Finally, we show that extensive alignment with human preferences can significantly enhance the performance of a language model compared to models of a larger scale with lower quality alignment. ALLaM achieves state-of-the-art performance in various Arabic benchmarks, including MMLU Arabic, ACVA, and Arabic Exams. Our aligned models improve both in Arabic and English from their base aligned models.",1.0,,Unreleased,Saudi Arabia,Llama 2-70B,,,,,2025-06-02 11:56,,,,,,"Industry,Government",,,,,Unreleased,not yet released,"Industry,Government",,,BF16,,,,,,
ALLaM 7B,Language,"Language modeling/generation,Translation,Question answering",Saudi Data and Artificial Intelligence Authority,Saudi Data and Artificial Intelligence Authority,2024-05-21,"ALLaM: Large Language Models for Arabic and English
",https://arxiv.org/abs/2407.15390,9.0,SOTA improvement,"SOTA at Arabic MMLU benchmarks, all results in this paper: https://openreview.net/pdf?id=MscdsFVZrN",7000000000.0,,9.04e+22,"FT: 6*7000000000*1200000000000=5.04e+22

Llama 7B compute: 4e+22

Total: 9.04e+22",,,1200000000000.0,"""Starting from Llama-2 pretrained model weights, we continue pretraining
the ALLaM-7B and ALLaM-13B models on 1.2T tokens""",,,NVIDIA A100,,Confident,"We present ALLaM: Arabic Large Language Model, a series of large language models to support the ecosystem of Arabic Language Technologies (ALT). ALLaM is carefully trained considering the values of language alignment and knowledge transfer at scale. Our autoregressive decoder-only architecture models demonstrate how second-language acquisition via vocabulary expansion and pretraining on a mixture of Arabic and English text can steer a model towards a new language (Arabic) without any catastrophic forgetting in the original language (English). Furthermore, we highlight the effectiveness of using parallel/translated data to aid the process of knowledge alignment between languages. Finally, we show that extensive alignment with human preferences can significantly enhance the performance of a language model compared to models of a larger scale with lower quality alignment. ALLaM achieves state-of-the-art performance in various Arabic benchmarks, including MMLU Arabic, ACVA, and Arabic Exams. Our aligned models improve both in Arabic and English from their base aligned models.",1.0,,Open weights (unrestricted),Saudi Arabia,LLaMA-7B,,,,,2025-06-02 11:57,,,,,,"Industry,Government",,,,,Unreleased,"Apache 2.0
https://huggingface.co/ALLaM-AI/ALLaM-7B-Instruct-preview","Industry,Government",,,BF16,,,,,,
ALLaM 34B,Language,"Language modeling/generation,Translation,Question answering",Saudi Data and Artificial Intelligence Authority,Saudi Data and Artificial Intelligence Authority,2024-05-21,AI Models for Arabic and English,https://openreview.net/pdf?id=MscdsFVZrN,9.0,SOTA improvement,"SOTA at Arabic MMLU benchmarks, all results in this paper: https://openreview.net/pdf?id=MscdsFVZrN",34000000000.0,,1.0608e+24,6*34000000000*5200000000000=1.060800e+24,,,5200000000000.0,"3,431,217,579(4.3B) total documents, with a total of 4,587,781,981,546(4.5T) words, and 5.2T tokens.",,,NVIDIA A100,,Confident,,1.0,,Unreleased,Saudi Arabia,,,,,,2025-06-02 11:59,,,,,,"Industry,Government",,,,,Unreleased,"34b is not in the HF repo yet
https://huggingface.co/ALLaM-AI","Industry,Government",,,BF16,,,,,,
GLM-4 (0520),Language,"Language modeling/generation,Question answering,Code generation,Quantitative reasoning,Translation",Zhipu AI,"Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Diego Rojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, Hao Yu, Hongning Wang, Jiadai Sun, Jiajie Zhang, Jiale Cheng, Jiayi Gui, Jie Tang, Jing Zhang, Juanzi Li, Lei Zhao, Lindong Wu, Lucen Zhong, Mingdao Liu, Minlie Huang, Peng Zhang, Qinkai Zheng, Rui Lu, Shuaiqi Duan, Shudan Zhang, Shulin Cao, Shuxun Yang, Weng Lam Tam, Wenyi Zhao, Xiao Liu, Xiao Xia, Xiaohan Zhang, Xiaotao Gu, Xin Lv, Xinghan Liu, Xinyi Liu, Xinyue Yang, Xixuan Song, Xunkai Zhang, Yifan An, Yifan Xu, Yilin Niu, Yuantao Yang, Yueyan Li, Yushi Bai, Yuxiao Dong, Zehan Qi, Zhaoyu Wang, Zhen Yang, Zhengxiao Du, Zhenyu Hou, Zihan Wang",2024-05-20,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools,https://arxiv.org/abs/2406.12793,,Training cost,"Trained on 10T tokens with similar architecture to GPT-4, probably >$1M compute cost.",,,,"'- â€œthe GLM-4 models are pre-trained on ten trillions of tokensâ€
- I did not find any information about parameters or compute. Here they speculatively estimate GLM-4 to be 200B parameters (which seems plausible), though no source provided: https://lifearchitect.ai/models-table/
- â€œGLM-4 gets close to the state-of-the-art models (GPT-4-Turbo, Gemini 1.5 Pro, and Claude 3 Opus)â€  none of these models has parameters disclosed or compute estimation.

6*10000000000000*200000000000 = 1.2e+25 FLOPs with â€œLikelyâ€ confidence (+/- 1 OOM)",,"""To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage""",10000000000000.0,,,,,,Likely,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",,,API access,China,GLM-4 (0116),,,,,2025-06-06 12:56,,,,,,Industry,,,,,Unreleased,"the GLM-4 API at
https://bigmodel.cn

repo with models from the same series but not exactly this one: https://github.com/THUDM/GLM-4",Industry,,,BF16,,Operation counting,,,,
Diamond,Games,Atari,"University of Geneva,University of Edinburgh,Microsoft Research","Eloi Alonso, Adam Jelley, Vincent Micheli, Anssi Kanervisto, Amos Storkey, Tim Pearce, FranÃ§ois Fleuret",2024-05-20,Diffusion for World Modeling: Visual Details Matter in Atari,https://arxiv.org/abs/2405.12399,,,,,,8.0490594e+20,82600000000000*9022.8*3600*0.3=8.0490594e+20,,"Training loop
Number of epochs 1000
Training steps per epoch 400
Batch size 32
Environment steps per epoch 100

Image observation dimensions 64Ã—64Ã—3",,,9022.8,"""We trained DIAMOND from scratch for 5 random seeds on each game. Each run utilized around 12GB of VRAM and took approximately 2.9 days on a single Nvidia RTX 4090 (1.03 GPU years in total).""

1.03*365*24 = 9022.8",NVIDIA GeForce RTX 4090,,Likely,"World models constitute a promising approach for training reinforcement learning agents in a safe and sample-efficient manner. Recent world models predominantly operate on sequences of discrete latent variables to model environment dynamics. However, this compression into a compact discrete representation may ignore visual details that are important for reinforcement learning. Concurrently, diffusion models have become a dominant approach for image generation, challenging well-established methods modeling discrete latents. Motivated by this paradigm shift, we introduce DIAMOND (DIffusion As a Model Of eNvironment Dreams), a reinforcement learning agent trained in a diffusion world model. We analyze the key design choices that are required to make diffusion suitable for world modeling, and demonstrate how improved visual details can lead to improved agent performance. DIAMOND achieves a mean human normalized score of 1.46 on the competitive Atari 100k benchmark; a new best for agents trained entirely within a world model. To foster future research on diffusion for world modeling, we release our code, agents and playable world models at this https URL.",,,Open weights (unrestricted),"Switzerland,United Kingdom of Great Britain and Northern Ireland,United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational",,,,,,2025-05-01 10:42,,,,,,"Academia,Academia,Industry",,,,,,"MIT license
https://github.com/eloialonso/diamond?tab=MIT-1-ov-file","Academia,Academia,Industry",,,,,Hardware,,,,
Octo-Base,Robotics,Robotic manipulation,"University of California (UC) Berkeley,Stanford University,Carnegie Mellon University (CMU),DeepMind","Octo Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Tobias Kreiman, Charles Xu, Jianlan Luo, You Liang Tan, Lawrence Yunliang Chen, Pannag Sanketi, Quan Vuong, Ted Xiao, Dorsa Sadigh, Chelsea Finn, Sergey Levine",2024-05-20,Octo: An Open-Source Generalist Robot Policy,https://arxiv.org/abs/2405.12213 ,248.0,SOTA improvement,"Octoâ€™s ability to control multiple robots out-of-the-box to the best openly available generalist robot policy, RT-1-X. RT-1-X had success rates of 0.2, ~0.38, and 0.6 on WidowX, UR5, and RT-1 Robot. Octo-Base outperformed RT-1-X on all tasks, with success rates of approximately 0.5, 0.7, and 0.8 on WidowX, UR5, and RT-1 Robot, respectively. (Source: https://arxiv.org/abs/2405.12213)",93000000.0,Source: https://arxiv.org/abs/2405.12213 ,5.85e+20,"Training compute 
= peak FLOPs * utilization rate * training time
~= 128 TPUs * 275e12 FLOPs / TPU * 0.33 * 14 hours * 3600 s / hour
~= 585446400e12 FLOPs
= 5.85e20 FLOPs,
assuming utilization rate = 0.33.",Open X-Embodiment,Octo was trained on a mixture of 25 datasets from the Open X-Embodiment Dataset. (Source: https://arxiv.org/abs/2405.12213),,"Octo was pre-trained on 800k robot episodes from the Open X-Embodiment dataset. The authors describe this dataset as ""heterogeneous,"" so it's unclear how to compute the size of the dataset. (Source: https://arxiv.org/abs/2405.12213)",14.0,"â€œWe trained two variants of our model: Octo-Small with a transformer backbone that mirrors the size of a ViT-S, andOcto-Base with a transformer backbone that mirrors the size of a ViT-B. The ViT-B was trained for 300k steps with a batch size of 2048 using a TPU v4-128 pod, which took 14 hours."" (Source: https://arxiv.org/abs/2405.12213)
",Google TPU v4,,Confident,"Large policies pretrained on diverse robot datasets have the potential to transform robotic learning: instead of training new policies from scratch, such generalist robot policies may be finetuned with only a little in-domain data, yet generalize broadly. However, to be widely applicable across a range of robotic learning scenarios, environments, and tasks, such policies need to handle diverse sensors and action spaces, accommodate a variety of commonly used robotic platforms, and finetune readily and efficiently to new domains. In this work, we aim to lay the groundwork for developing open-source, widely applicable, generalist policies for robotic manipulation. As a first step, we introduce Octo, a large transformer-based policy trained on 800k trajectories from the Open X-Embodiment dataset, the largest robot manipulation dataset to date. It can be instructed via language commands or goal images and can be effectively finetuned to robot setups with new sensory inputs and action spaces within a few hours on standard consumer GPUs. In experiments across 9 robotic platforms, we demonstrate that Octo serves as a versatile policy initialization that can be effectively finetuned to new observation and action spaces. We also perform detailed ablations of design decisions for the Octo model, from architecture to training data, to guide future research on building generalist robot models.",,,Open weights (unrestricted),"United States of America,United States of America,United States of America,United Kingdom of Great Britain and Northern Ireland",,,,128.0,,2025-06-06 11:22,,,,,,"Academia,Academia,Academia,Industry",,,,,Open source,"MIT license
https://huggingface.co/rail-berkeley/octo-base

https://github.com/octo-models/octo","Academia,Academia,Academia,Industry",,,BF16,42983.43309675412,Hardware,rail-berkeley,,,
HelixFold,Biology,Protein folding prediction,Baidu,"Xiaomin Fang, Jie Gao, Jing Hu, Lihang Liu, Yang Xue, Xiaonan Zhang, Kunrui Zhu",2024-05-17,HelixFold-Multimer: Elevating Protein Complex Structure Prediction to New Heights,https://arxiv.org/abs/2404.10260v2,4.0,,,,,,,,,,,122.9,"""The complete training time are optimized from 11 days to 5.12 days""",,,Unknown,"While monomer protein structure prediction tools boast impressive accuracy, the prediction of protein complex structures remains a daunting challenge in the field. This challenge is particularly pronounced in scenarios involving complexes with protein chains from different species, such as antigen-antibody interactions, where accuracy often falls short. Limited by the accuracy of complex prediction, tasks based on precise protein-protein interaction analysis also face obstacles. In this report, we highlight the ongoing advancements of our protein complex structure prediction model, HelixFold-Multimer, underscoring its enhanced performance. HelixFold-Multimer provides precise predictions for diverse protein complex structures, especially in therapeutic protein interactions. Notably, HelixFold-Multimer achieves remarkable success in antigen-antibody and peptide-protein structure prediction, greatly surpassing AlphaFold 3. HelixFold-Multimer is now available for public use on the PaddleHelix platform, offering both a general version and an antigen-antibody version. Researchers can conveniently access and utilize this service for their development needs.",,,Unreleased,China,,,,,,2025-06-11 17:58,,,,,,Industry,,,,,Open (non-commercial),"CC BY-NC-SA 4.0
https://github.com/PaddlePaddle/PaddleHelix",Industry,,,,,,,,,
ProSST,Biology,Protein or nucleotide language model (pLM/nLM),"Shanghai Jiao Tong University,Shanghai AI Lab,East China University of Science and Technology","Mingchen Li, Pan Tan, Xinzhu Ma, Bozitao Zhong, Huiqun Yu, Ziyi Zhou, Wanli Ouyang, Bingxin Zhou, Liang Hong, Yang Tan",2024-05-17,ProSST: Protein Language Modeling with Quantized Structure and Disentangled Attention,https://www.biorxiv.org/content/10.1101/2024.04.15.589672v3.abstract,9.0,,,110000000.0,Explicitly denoted in Table 2,6.46714368e+20,"""All ProSST models is trained on a DGX-A800 GPU (8Ã—80G) server in BF16 precision for about a month."" 

8*77970000000000*0.4*1month=646714368000000000000",,,5600000001.0,18.8M structures Ã— 300 residues per structure = 5.64B data points â‰ˆ 5.6B data points,720.0,"""All ProSST models is trained [...] for about a month.""",NVIDIA A800 PCIe 40 GB,,Confident,"Protein language models (PLMs) have shown remarkable capabilities in various protein function prediction tasks. However, while protein function is intricately tied to structure, most existing PLMs do not incorporate protein structure information. To address this issue, we introduce ProSST, a Transformer-based protein language model that seamlessly integrates both protein sequences and structures. ProSST incorporates a structure quantization module and a Transformer architecture with disentangled attention. The structure quantization module translates a 3D protein structure into a sequence of discrete tokens by first serializing the protein structure into residue-level local structures and then embeds them into dense vector space. These vectors are then quantized into discrete structure tokens by a pre-trained clustering model. These tokens serve as an effective protein structure representation. Furthermore, ProSST explicitly learns the relationship between protein residue token sequences and structure token sequences through the sequence-structure disentangled attention. We pre-train ProSST on millions of protein structures using a masked language model objective, enabling it to learn comprehensive contextual representations of proteins. To evaluate the proposed ProSST, we conduct extensive experiments on the zero-shot mutation effect prediction and several supervised downstream tasks, where ProSST achieves the state-of-the-art performance among all baselines. Our code and pretrained models are publicly available 2.",,,,"China,China,China",,,,8.0,,2025-05-01 10:42,,,,,,"Academia,Academia,Academia",,,,,,,"Academia,Academia,Academia",,,,3950.9471355488704,Hardware,,,,
Chameleon-34B,"Multimodal,Image generation,Language,Vision","Language modeling/generation,Vision-language generation,Visual question answering,Text-to-image",Facebook AI Research,"Srinivasan Iyer, Bernie Huang, Lili Yu, Arun Babu, Chunting Zhou, Kushal Tirumala, Xi Victoria Lin, Hu Xu, Xian Li, Akshat Shrivastava, Omer Levy, Armen Aghajanyan, Ram Pasunuru, Andrew Cohen, Aram H. Markosyan, Koustuv Sinha, Xiaoqing Ellen Tan, Ivan Evtimov, Ping Yu, Tianlu Wang, Olga Golovneva, Asli Celikyilmaz, Pedro Rodriguez, Leonid Shamis, Vasu Sharma, Christine Jou, Karthik Padthe, Ching-Feng Yeh, Mingda Chen, Bapi Akula, Jacob Kahn, Daniel Li, Scott Yih, Barlas Oguz, Morteza Behrooz, Benjamin Muller, Carleigh Wood, Mary Williamson, Ramya Raghavendra, Barbara Usher, William Ngan, Nikolay Bashlykov, Lukas Blecher, Sony Theakanath, Ammar Rizvi, Gargi Ghosh, Luke Zettlemoyer",2024-05-16,Chameleon: Mixed-Modal Early-Fusion Foundation Models,https://arxiv.org/abs/2405.09818v1,,,"They claim SOTA on some vision-text tasks, but it seems like this is only against other general purpose vision-language models. Specialized models win by a fair margin on COCO image captioning and Flickr30k.",34000000000.0,,1.6453571041e+24,"GPU method:
Table 2 shows that 34B model pre-training uses 4282407 GPU-hours, trained across 3072 A100s.
3.12e14 * 4282407 * 3600 * 0.3 =  1.44e24

Parameter-token method:
Pre-training goes over 9.2T tokens, post-training only goes over 1.1B tokens (sum of tokens column in Table 3).
6 * 34B *  9.2T = 1.88e24

Geometric mean: sqrt(1.44e24 * 1.88e24) = 1.65e24",Unspecified unreleased,"Pre-training:
- 2.9 trillion tokens of pure text
- 1.4 billion text-image pairs, which produces 1.5 trillion text-image tokens
	- Since each image is 1024 tokens, implies 1.43 trillion image tokens and 0.07 trillion text tokens
- 400 billion tokens of image-text interleaved documents
	- Difficult to estimate image-to-text ratio, but references OBELIKS paper which had 141 million web pages, 353 million associated images, and 115 billion text tokens.
	- 353 million * 1024 = 361.5 billion image tokens, so 75.9% of the tokens would be from images
	- Implies 303.5 billion image tokens and 96.5 text tokens
- There is a second stage of ""higher quality"" pre-training tokens which are not described in detail


Post-training:
- 940 million tokens of text
- 1.1 million tokens of code
- 19.4 million tokens of visual chats
	- Featuring 16.7 thousand images, so 17.1M image tokens and 2.3M text tokens
- 68 million tokens of image generations
	- Featuring 64.3 thousand images, so 65.8M image tokens and 2.2M text tokens
- 35.8 million tokens of interleaved generations
	- Featuring 30.7 thousand images, so 31.4M image tokens and 4.4M text tokens
- 38.6 million tokens of safety training
	- Featuring 1.6 thousand images, so 1.6M image tokens and 37M text tokens",4400000000000.0,"Slightly conflicting info. Pre-training data details describe different types of data that sum to 4.8 trillion tokens, but Table 1 indicates 4.4T. Using table values as this agrees with other statements about epochs and total tokens seen.",1394.0,"34B model pre-training uses 4282407 GPU-hours, trained across 3072 A100s
4282407 / 3072 = 1394",NVIDIA A100 SXM4 80 GB,Self-supervised learning,Confident,"We present Chameleon, a family of early-fusion token-based mixed-modal models capable of understanding and generating images and text in any arbitrary sequence. We outline a stable training approach from inception, an alignment recipe, and an architectural parameterization tailored for the early-fusion, token-based, mixed-modal setting. The models are evaluated on a comprehensive range of tasks, including visual question answering, image captioning, text generation, image generation, and long-form mixed modal generation. Chameleon demonstrates broad and general capabilities, including state-of-the-art performance in image captioning tasks, outperforms Llama-2 in text-only tasks while being competitive with models such as Mixtral 8x7B and Gemini-Pro, and performs non-trivial image generation, all in a single model. It also matches or exceeds the performance of much larger models, including Gemini Pro and GPT-4V, according to human judgments on a new long-form mixed-modal generation evaluation, where either the prompt or outputs contain mixed sequences of both images and text. Chameleon marks a significant step forward in a unified modeling of full multimodal documents.",2.1,,Open weights (non-commercial),United States of America,,,"Not enough info to estimate. GPU time given for pretraining, and while we know # of fine-tuning tokens we don't know # of epochs.",3072.0,,2025-06-06 10:38,,,,,,Industry,,,,,Unreleased,"https://ai.meta.com/resources/models-and-libraries/chameleon-downloads/?gk_enable=chameleon_web_flow_is_live

""The models weâ€™re releasing today were safety tuned and support mixed-modal inputs and text-only output to be used for research purposes. While weâ€™ve taken steps to develop these models responsibly, we recognize that risks remain. At this time, we are not releasing the Chameleon image generation model.""",Industry,,,,2427515.9787339047,"Hardware,Operation counting",,,,
LBSTER,Biology,Protein or nucleotide language model (pLM/nLM),"Prescient Design,Genentech","Nathan C. Frey, Taylor Joren, Aya Abdelsalam Ismail, Allen Goodman, Richard Bonneau, Kyunghyun Cho, Vladimir GligorijeviÄ‡",2024-05-15,Cramming Protein Language Model Training in 24 GPU Hours,https://www.biorxiv.org/content/10.1101/2024.05.14.594108v1.abstract,1.0,,,67000000.0,"""we are able to train a 67 million parameter model""",1.078272e+19,"1 day on 1 A100

312000000000000*1day*0.4=10782720000000000000",UniRef50,"While listing rules of general challenge the paper is trying to complete, they list ""The training, validation, and test data splits are from UniRef50 [...]""",13000000001.0,"UniRef50 dataset size calculation:
43,000,000 sequences Ã— 300 tokens/sequence = 1.29 Ã— 10Â¹â° tokens

Training process calculation: 
1,048,576 tokens/step Ã— 50,000 steps = 5.24288 Ã— 10Â¹â° total tokens

Epochs calculation:
5.24288 Ã— 10Â¹â° / 1.29 Ã— 10Â¹â° = 4.06 epochs

Final unique tokens (first epoch only): 1.3 Ã— 10Â¹â° tokens",24.0,"""we are able to train a 67 million parameter model in a single day",NVIDIA A100 SXM4 80 GB,,Confident,"Protein language models (pLMs) are ubiquitous across biological machine learning research, but state-of-the-art models like ESM2 take hundreds of thousands of GPU hours to pre-train on the vast protein universe. Resource requirements for scaling up pLMs prevent fundamental investigations into how optimal modeling choices might differ from those used in natural language. Here, we define a â€œcrammingâ€ challenge for pLMs and train performant models in 24 hours on a single GPU. By re-examining many aspects of pLM training, we are able to train a 67 million parameter model in a single day that achieves comparable performance on downstream protein fitness landscape inference tasks to ESM-3B, a model trained for over 15, 000Ã— more GPU hours than ours. We open source our library1 for training and inference, LBSTER: Language models for Biological Sequence Transformation and Evolutionary Representation.",4.06,,,"United States of America,United States of America",,,,1.0,,2025-05-01 10:42,,,,,,"Industry,Industry",,,,,,,"Industry,Industry",,,,434.1893527126348,Hardware,,,,
Doubao-lite,Language,"Language modeling/generation,Question answering",ByteDance,,2024-05-15,Doubao General Model Lite (Doubao-lite),https://www.volcengine.com/docs/6360/1264663,,,,,,,,Unspecified unreleased,,,,,,,,Unknown,"A lightweight version of the LLM, offering lower token costs and latency compared to the Pro version. 
",,,API access,China,,,,,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
Doubao Role-Playing Model,Language,"Language modeling/generation,Question answering",ByteDance,,2024-05-15,Doubao Role-Playing Model,https://www.volcengine.com/docs/6360/1264663,,,,,,,,Unspecified unreleased,,,,,,,,Unknown,"This model is designed for personalized character creation. 
",,,API access,China,,,,,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
Doubao Text-to-Speech Model,Speech,Text-to-speech,ByteDance,,2024-05-15,Doubao Text-to-Speech Model,https://www.volcengine.com/docs/6360/1264663,,,,,,,,Unspecified unreleased,,,,,,,,Unknown,This model is capable of generating natural and expressive speech. ,,,API access,China,,,,,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
Doubao Voice Cloning Model,Speech,"Audio generation,Speech synthesis,Speech recognition",ByteDance,,2024-05-15,Doubao Voice Cloning Model,https://www.volcengine.com/docs/6360/1264663,,,,,,,,Unspecified unreleased,,,,,,,,Unknown,"This model can accurately clone a voice within just 5 seconds, preserving timbre similarity and naturalness. It also supports cross-lingual voice transfer. 
",,,API access,China,,,,,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
Doubao Speech Recognition Model,Speech,Speech recognition,ByteDance,,2024-05-15,Doubao Speech Recognition Model,https://www.volcengine.com/docs/6360/1264663,,,,,,,,Unspecified unreleased,,,,,,,,Unknown,"This model supports multi-language speech recognition, with high accuracy, sensitivity, and low latency. ",,,API access,China,,,,,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
Doubao Text-to-Image Model,Image generation,"Text-to-image,Image generation",ByteDance,,2024-05-15,Doubao Text-to-Image Model,https://www.volcengine.com/docs/6360/1264663,,,,,,,,Unspecified unreleased,,,,,,,,Unknown,"With precise text understanding and accurate image-text matching, this model produces aesthetically pleasing images, especially those related to Chinese cultural elements. 
",,,API access,China,,,,,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
Doubao Function Call Model,Language,Language modeling/generation,ByteDance,,2024-05-15,Doubao Function Call Model,https://www.volcengine.com/docs/6360/1264663,,,,,,,,Unspecified unreleased,,,,,,,,Unknown,"Designed for complex tool invocation, this model excels at accurately identifying functions and extracting parameters. 
",,,API access,China,,,,,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
Doubao Vectorization Model,Language,Language modeling/generation,ByteDance,,2024-05-15,Doubao Vectorization Model,https://www.volcengine.com/docs/6360/1264663,,,,,,,,Unspecified unreleased,,,,,,,,Unknown,"Focused on vector search applications, this model provides core understanding capabilities for LLM knowledge bases and supports multiple languages. 
",,,API access,China,,,,,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
Doubao Image-to-Image generation model,"Image generation,Vision",Image generation,ByteDance,,2024-05-15,Doubao Image-to-Image generation model,https://www.volcengine.com/product/doubao,,,,,,,,Unspecified unreleased,,,,,,,,Unknown,,,,API access,China,,,,,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
Doubao Real-Time translation model,"Language,Speech","Translation,Speech synthesis,Speech recognition",ByteDance,,2024-05-15,Doubao Real-Time translation model,https://www.volcengine.com/product/doubao,,,,,,,,Unspecified unreleased,,,,,,,,Unknown,,,,API access,China,,,,,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
Doubao Video Generation model,Video,Video generation,ByteDance,,2024-05-15,Doubao Video Generation model,https://www.volcengine.com/product/doubao,,,,,,,,Unspecified unreleased,,,,,,,,Unknown,,,,API access,China,,,,,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
VILA1.5-13B,Multimodal,"Chat,Visual question answering,Image captioning,Language modeling/generation","NVIDIA,Massachusetts Institute of Technology (MIT)","Ji Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz, Mohammad Shoeybi, Song Han",2024-05-15,VILA: On Pre-training for Visual Language Models,https://arxiv.org/abs/2312.07533,384.0,SOTA improvement,"Table5. Comparison with state-of-the-art methods on 12 visual-language benchmarks. Our models consistently outperform LLaVA-1.5 under
a head-to-head comparison, using the same prompts and the same base LLM (Vicuna-1.5 is based on Llama-2), showing the effectiveness of
visual-language pre-training",13000000000.0,,,,,,32430000000.0,"Table 2
MMC4: 25M images with 576+122.5 tokens each
COYO: 25M images with 576+22.7 tokens each
25M*(576+122.5+576+22.7)=32430000000",,,NVIDIA A100,,Confident,"Visual language models (VLMs) rapidly progressed with
the recent success of large language models. There have
been growing efforts on visual instruction tuning to extend
the LLM with visual inputs, but lacks an in-depth study of
the visual language pre-training process, where the model
learns to perform joint modeling on both modalities. In this
work, we examine the design options for VLM pre-training
by augmenting LLM towards VLM through step-by-step controllable comparisons. We introduce three main findings:
(1) freezing LLMs during pre-training can achieve decent
zero-shot performance, but lack in-context learning capability, which requires unfreezing the LLM; (2) interleaved pretraining data is beneficial whereas image-text pairs alone
are not optimal; (3) re-blending text-only instruction data
to image-text data during instruction fine-tuning not only
remedies the degradation of text-only tasks, but also boosts
VLM task accuracy. With an enhanced pre-training recipe
we build VILA, a Visual Language model family that consistently outperforms the state-of-the-art models, e.g., LLaVA1.5, across main benchmarks without bells and whistles.
Multi-modal pre-training also helps unveil appealing properties of VILA, including multi-image reasoning, enhanced
in-context learning, and better world knowledge. VILA is
also deployable on Jetson Orin for on-device VLM.
",,,Open weights (non-commercial),"United States of America,United States of America",Llama 2-13B,,,128.0,,2025-06-06 12:04,,,,,,"Industry,Academia",,,,,Open source,"Apache 2.0 for code, CC by NC for weights
https://github.com/NVlabs/VILA

https://huggingface.co/Efficient-Large-Model/VILA1.5-13b","Industry,Academia",,,,101148.7516079354,,Efficient-Large-Model,,,
Veo,"Video,Vision","Video generation,Text-to-video,Image-to-video",Google DeepMind,"Abhishek Sharma, Adams Yu, Ali Razavi, Andeep Toor, Andrew Pierson, Ankush Gupta, Austin Waters, AÃ¤ron van den Oord, Daniel Tanis, Dumitru Erhan, Eric Lau, Eleni Shaw, Gabe Barth-Maron, Greg Shaw, Han Zhang, Henna Nandwani, Hernan Moraldo, Hyunjik Kim, Irina Blok, Jakob Bauer, Jeff Donahue, Junyoung Chung, Kory Mathewson, Kurtis David, Lasse Espeholt, Marc van Zee, Matt McGill, Medhini Narasimhan, Miaosen Wang, MikoÅ‚aj BiÅ„kowski, Mohammad Babaeizadeh, Mohammad Taghi Saffar, Nando de Freitas, Nick Pezzotti, Pieter-Jan Kindermans, Poorva Rane, Rachel Hornung, Robert Riachi, Ruben Villegas, Rui Qian, Sander Dieleman, Serena Zhang, Serkan Cabi, Shixin Luo, Shlomi Fruchter, Signe NÃ¸rly, Srivatsan Srinivasan, Tobias Pfaff, Tom Hume, Vikas Verma, Weizhe Hua, William Zhu, Xinchen Yan, Xinyu Wang, Yelin Kim, Yuqing Du and Yutian Chen.",2024-05-14,"Weâ€™re introducing Veo, our most capable model for generating high-definition video",https://deepmind.google/technologies/veo/,,,,,,,,Unspecified unreleased,,,,,,,,Unknown,"Veo is our most capable video generation model to date. It generates high-quality, 1080p resolution videos that can go beyond a minute, in a wide range of cinematic and visual styles.",,,Hosted access (no API),"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France",,,,,,2025-05-27 10:02,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
Imagen 3,Image generation,"Image generation,Text-to-image",Google DeepMind,"Jason Baldridge, Jakob Bauer, Mukul Bhutani, Nicole Brichtova, Andrew Bunner, Kelvin Chan, Sergio GÃ³mez Colmenarejo, Sander Dieleman, Yuqing Du, Zach Eaton-Rosen, Hongliang Fei, Yilin Gao, Evgeny Gladchenko, Mandy Guo, Alex Haig, Will Hawkins, Hexiang (Frank) Hu, Huilian Huang, Tobenna Peter Igwe, Christos Kaplanis, Siavash Khodadadeh, Ksenia Konyushkova, Karol Langner, Eric Lau, Shixin Luo, SoÅˆa MokrÃ¡, Henna Nandwani, Yasumasa Onoe, AÃ¤ron van den Oord, Zarana Parekh, Jordi Pont-Tuset, Hang Qi, Rui Qian, Deepak Ramachandran, Ali Razavi, Robert Riachi, Hansa Srinivasan, Srivatsan Srinivasan, Robin Strudel, Benigno Uria, Oliver Wang, Su Wang, Austin Waters, Chris Wolff, Auriel Wright, Zhisheng Xiao, Keyang Xu, Marc van Zee, Junlin Zhang, Wenlei Zhou and Konrad Zoln.",2024-05-14,Imagen 3: our highest quality text-to-image model,https://deepmind.google/technologies/imagen-3/,,,,,,,,Unspecified unreleased,"""Our model is trained on a large dataset comprising images, text and associated annotations. To ensure quality and safety standards, we employ a multi-stage filtering process. This process begins by removing unsafe, violent, or low-quality images. We then eliminate AI-generated images to prevent
the model from learning artifacts or biases commonly found in such images. Additionally, we use deduplication pipelines and down-weight similar images to minimize the risk of outputs overfitting particular elements of training data.""",,,,,"Google TPU v4,Google TPU v5e",,Unknown,"Imagen 3 is our highest quality text-to-image model, capable of generating images with even better detail, richer lighting and fewer distracting artifacts than our previous models.",,,Hosted access (no API),"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France",,,,,,2025-05-01 10:42,,,,,,Industry,,,,,,,Industry,,,,,,,,,
LearnLM-Tutor,Language,"Language modeling/generation,Chat,Question answering","Google Research,Google DeepMind,Google,Arizona State University,Lund University,University of Oxford","Irina Jurenka, Markus Kunesch, Kevin R. McKee, Daniel Gillick, Shaojian Zhu, Sara Wiltberger, Shubham Milind Phal, Katherine Hermann, Daniel Kasenberg, Avishkar Bhoopchand, Ankit Anand, Miruna PÃ®slar, Stephanie Chan, Lisa WangÂ§, Jennifer She, Parsa Mahmoudieh, Aliya Rysbek, Wei-Jen Ko, Andrea Huber, Brett Wiltshire, Gal Elidanâ€¡, Roni Rabin, Jasmin Rubinovitzâ€ , Amit Pitaru, Mac McAllister, Julia Wilkowski, David Choi, Roee Engelberg, Lidan Hackmon, Adva Levin, Rachel Griffin, Michael Sears, Filip Bar, Mia Mesar, Mana Jabbour, Arslan Chaudhry, James Cohan, Sridhar Thiagarajan, Nir Levine, Ben Brown, Dilan GorurÂ§, Svetlana Grant, Rachel Hashimshoni, Laura Weidinger, Jieru Hu, Dawn Chen, Kuba Dolecki, Canfer Akbulut, Maxwell Bileschi, Laura Culp, Wen-Xin Dong, Nahema Marchal, Kelsie Van Deman, Hema Bajaj Misra, Michael Duah, Moran Ambar, Avi Caciularu, Sandra Lefdal, Chris Summerfield, James An, Pierre-Alexandre Kamienny, Abhinit Mohdi, Theofilos Strinopoulous, Annie Hale, Wayne Anderson, Luis C. Cobo, Niv Efronâ€ , Muktha Ananda, Shakir Mohamed, Maureen Heymans, Zoubin Ghahramani, Yossi Matias, Ben Gomes, Lila Ibrahim.",2024-05-14,Towards Responsible Development of Generative AI for Education: An Evaluation-Driven Approach,https://storage.googleapis.com/deepmind-media/LearnLM/LearnLM_paper.pdf,,,,,,,,"GSM8K,Unspecified unreleased","Human Tutoring Data: Conversations between educators and learners.
Gen AI Role-Play Data: Simulated tutor-learner interactions.
GSM8k Dialogue: Adapted word-problem datasets converted into conversational format.
Golden Conversations: Handcrafted, high-quality dialogues by educators.
Safety Dataset: Specially designed for safety-related fine-tuning.",,,,,,,Unknown,"A major challenge facing the world is the provision of equitable and universal access to quality education. Recent advances in generative AI (gen AI) have created excitement about the potential of new technologies to offer a personal tutor for every learner and a teaching assistant for every teacher. The full extent of this dream, however, has not yet materialised. We argue that this is primarily due to the difficulties with verbalising pedagogical intuitions into gen AI prompts and the lack of good evaluation practices, reinforced by the challenges in defining excellent pedagogy. Here we present our work collaborating with learners and educators to translate high level principles from learning science into a pragmatic set of seven diverse educational benchmarks, spanning quantitative, qualitative, automatic and human
evaluations; and to develop a new set of fine-tuning datasets to improve the pedagogical capabilities of Gemini, introducing LearnLM-Tutor. Our evaluations show that LearnLM-Tutor is consistently preferred over a prompt tuned Gemini by educators and learners on a number of pedagogical dimensions. We hope that this work can serve as a first step towards developing a comprehensive educational evaluation framework, and that this can enable rapid progress within the AI and EdTech communities towards
maximising the positive impact of gen AI in education.",,,Hosted access (no API),"Multinational,United States of America,Canada,Switzerland,United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France,United States of America,United States of America,Sweden,United Kingdom of Great Britain and Northern Ireland",Gemini 1.0 Ultra,,,,,2025-05-01 10:42,,,,,,"Industry,Industry,Industry,Academia,Academia,Academia",,,,,Unreleased,,"Industry,Industry,Industry,Academia,Academia,Academia",,,,,,,,,
Hunyuan-DiT,Image generation,"Image generation,Text-to-image",Tencent,"Zhimin Li, Jianwei Zhang, Qin Lin, Jiangfeng Xiong, Yanxin Long, Xinchi Deng, Yingfang Zhang, Xingchao Liu, Minbin Huang, Zedong Xiao, Dayou Chen, Jiajun He, Jiahao Li, Wenyue Li, Chen Zhang, Rongwei Quan, Jianxiang Lu, Jiabin Huang, Xiaoyan Yuan, Xiaoxiao Zheng, Yixuan Li, Jihong Zhang, Chao Zhang, Meng Chen, Jie Liu, Zheng Fang, Weiyan Wang, Jinbao Xue, Yangyu Tao, Jianchen Zhu, Kai Liu, Sihuan Lin, Yifu Sun, Yun Li, Dongdong Wang, Mingtao Chen, Zhichao Hu, Xiao Xiao, Yan Chen, Yuhong Liu, Wei Liu, Di Wang, Yong Yang, Jie Jiang, Qinglin Lu",2024-05-14,Hunyuan-DiT: A Powerful Multi-Resolution Diffusion Transformer with Fine-Grained Chinese Understanding,https://arxiv.org/abs/2405.08748,,,,1500000000.0,1.5B,,,Unspecified unreleased,,,,,,,,Confident,"We present Hunyuan-DiT, a text-to-image diffusion transformer with fine-grained understanding of both English and Chinese. To construct Hunyuan-DiT, we carefully design the transformer structure, text encoder, and positional encoding. We also build from scratch a whole data pipeline to update and evaluate data for iterative model optimization. For fine-grained language understanding, we train a Multimodal Large Language Model to refine the captions of the images. Finally, Hunyuan-DiT can perform multi-turn multimodal dialogue with users, generating and refining images according to the context. Through our holistic human evaluation protocol with more than 50 professional human evaluators, Hunyuan-DiT sets a new state-of-the-art in Chinese-to-image generation compared with other open-source models. Code and pretrained models are publicly available at this http URL",,,Open weights (restricted use),China,,,,,,2025-05-01 10:42,,,,,,Industry,,,,,Open (restricted use),"https://huggingface.co/Tencent-Hunyuan/HunyuanDiT
requires additional licensing in case of 100M+ monthly users
+ the license doesn't apply for EU ",Industry,,,,,,Tencent-Hunyuan,,,
Yi-Large,Language,"Chat,Language modeling/generation",01.AI,"Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming Yang, Tao Yu, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, Zonghong Dai",2024-05-13,,,,Training cost,,100000000000.0,"""Yi-Large is a software over-the-air-driven closed-source large model with a parameter of over 100 billion tokens."" from https://www.chinadaily.com.cn/a/202405/13/WS6641abd1a31082fc043c6ccd.html",1.7999999999999996e+24,"6ND = 6*100000000000*3000000000000=1.8e+24

(speculative confidence because training dataset size is very uncertain)",,,3000000000000.0,"3T tokens for previous Yi models: ""Targeted as a bilingual language model and trained on 3T multilingual corpus, the Yi series models become one of the strongest LLM worldwide, showing promise in language understanding, commonsense reasoning, reading comprehension, and more.""
",,,,,Speculative,,,,API access,China,,,,,,2025-06-10 15:34,,"There is no paper to reference, no information about hardware used for training found in media.",,,,Industry,,,5.6920997883e+25,,Unreleased,,Industry,,,FP8,,Operation counting,,,,
GPT-4o,"Multimodal,Language,Audio,Speech,Vision","Chat,Image generation,Audio generation,Vision-language generation,Table tasks,Language modeling/generation,Question answering,Speech recognition",OpenAI,"Aidan Clark, Alex Paino, Jacob Menick, Liam Fedus, Luke Metz, Clemens Winter, Lia Guy, Sam Schoenholz, Daniel Levy, Nitish Keskar, Alex Carney, Alex Paino, Ian Sohl, Qiming Yuan, Reimar Leike, Arka Dhar, Brydon Eastman, Mia Glaese, Ben Sokolowsky, Andrew Kondrich, Felipe Petroski Such, Henrique Ponde de Oliveira Pinto, Jiayi Weng, Randall Lin, Youlong Cheng, Nick Ryder, Lauren Itow, Barret Zoph, John Schulman, Mianna Chen, Adam Lerer, Adam P. Goucher, Adam Perelman, Akila Welihinda, Alec Radford, Alex Borzunov, Alex Carney, Alex Chow, Alex Paino, Alex Renzin, Alex Tachard Passos, Alexi Christakis, Ali Kamali, Allison Moyer, Allison Tam, Amin Tootoonchian, Ananya Kumar, Andrej Karpathy, Andrey Mishchenko, Andrew Cann, Andrew Kondrich, Andrew Tulloch, Angela Jiang, Antoine Pelisse, Anuj Gosalia, Avi Nayak, Avital Oliver, Behrooz Ghorbani, Ben Leimberger, Ben Wang, Blake Samic, Brian Guarraci, Brydon Eastman, Camillo Lugaresi, Chak Li, Charlotte Barette, Chelsea Voss, Chong Zhang, Chris Beaumont, Chris Hallacy, Chris Koch, Christian Gibson, Christopher Hesse, Colin Wei, Daniel Kappler, Daniel Levin, Daniel Levy, David Farhi, David Mely, David Sasaki, Dimitris Tsipras, Doug Li, Duc Phong Nguyen, Duncan Findlay, Edmund Wong, Ehsan Asdar, Elizabeth Proehl, Elizabeth Yang, Eric Peterson, Eric Sigler, Eugene Brevdo, Farzad Khorasani, Francis Zhang, Gene Oden, Geoff Salmon, Hadi Salman, Haiming Bao, Heather Schmidt, Hongyu Ren, Hyung Won Chung, Ian Kivlichan, Ian O'Connell, Ian Osband, Ilya Kostrikov, Ingmar Kanitscheider, Jacob Coxon, James Crooks, James Lennon, Jason Teplitz, Jason Wei, Jason Wolfe, Jay Chen, Jeff Harris, Jiayi Weng, Jie Tang, Joanne Jang, Jonathan Ward, Jonathan McKay, Jong Wook Kim, Josh Gross, Josh Kaplan, Joy Jiao, Joyce Lee, Juntang Zhuang, Kai Fricke, Kavin Karthik, Kenny Hsu, Kiel Howe, Kyle Luther, Larry Kai, Lauren Itow, Leo Chen, Lia Guy, Lien Mamitsuka, Lilian Weng, Long Ouyang, Louis Feuvrier, Lukas Kondraciuk, Lukasz Kaiser, Lyric Doshi, Mada Aflak, Maddie Simens, Madeleine Thompson, Marat Dukhan, Marvin Zhang, Mateusz Litwin, Max Johnson, Mayank Gupta, Mia Glaese, Michael Janner, Michael Petrov, Michael Wu, Michelle Fradin, Michelle Pokrass, Miguel Oom Temudo de Castro, Mikhail Pavlov, Minal Khan, Mo Bavarian, Natalia Gimelshein, Natalie Staudacher, Nick Stathas, Nik Tezak, Nithanth Kudige, Noel Bundick, Ofir Nachum, Oleg Boiko, Oleg Murk, Olivier Godement, Owen Campbell-Moore, Philip Pronin, Philippe Tillet, Rachel Lim, Rajan Troll, Randall Lin, Rapha gontijo lopes, Raul Puri, Reah Miyara, Reimar Leike, Renaud Gaubert, Reza Zamani, Rob Honsby, Rohit Ramchandani, Rory Carmichael, Ruslan Nigmatullin, Ryan Cheu, Scott Gray, Sean Grove, Sean Metzger, Shantanu Jain, Shengjia Zhao, Sherwin Wu, Shuaiqi (Tony) Xia, Sonia Phene, Spencer Papay, Steve Coffey, Steve Lee, Steve Lee, Stewart Hall, Suchir Balaji, Tal Broda, Tal Stramer, Tarun Gogineni, Ted Sanders, Thomas Cunninghman, Thomas Dimson, Thomas Raoux, Tianhao Zheng, Tina Kim, Todd Underwood, Tristan Heywood, Valerie Qi, Vinnie Monaco, Vlad Fomenko, Weiyi Zheng, Wenda Zhou, Wojciech Zaremba, Yash Patil, Yilei, Qian, Yongjik Kim, Youlong Cheng, Yuchen He, Yuchen Zhang, Yujia Jin, Yunxing Dai, Yury Malkov",2024-05-13,Hello GPT-4o,"https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",,"SOTA improvement,Significant use","Outperforms GPT-4 Turbo and other models on text and especially on multimodal benchmarks, such as MMLU, GPQA, HumanEval, MMMU, etc See Model Evaluations: https://openai.com/index/hello-gpt-4o/ 

GPT-4o is now the default model in ChatGPT, so it's one of the most widely used models.",,"Not known.

Inference costs in the API are 2x cheaper than GPT-4 Turbo",3.810001e+25,Training compute estimated from benchmark scores.,Unspecified unreleased,"""With GPT-4o, we trained a single new model end-to-end across text, vision, and audio.""
This has a knowledge cutoff date of October 2023 - which I assume means October 1, 2023 - according to https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models?tabs=global-standard%2Cstandard-chat-completions#gpt-4o-and-gpt-4-turbo.",,,,,,,Speculative,"Weâ€™re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (â€œoâ€ for â€œomniâ€) is a step towards much more natural human-computer interactionâ€”it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.",,,API access,United States of America,,,"Definitely a new model, not a GPT-4 finetune",,,2025-05-12 19:06,,,,,,Industry,,,,,Unreleased,,Industry,,,,,Benchmarks,,,,2
Yi-1.5-34B,Language,"Chat,Language modeling/generation,Translation,Code generation",01.AI,,2024-05-13,"Yi-1.5 is an upgraded version of Yi, delivering stronger performance in coding, math, reasoning, and instruction-following capability.",https://huggingface.co/01-ai/Yi-1.5-34B,,,,34000000000.0,34b,7.344e+23,6 FLOP / parameter / token * 34*10^9 parameters * 3.6*10^12 tokens = 7.344e+23 FLOP,Unspecified unreleased,assuming same as Yi 34 - Chinese and English dataset,3600000000000.0,"3.6T
""Yi-1.5 is an upgraded version of Yi. It is continuously pre-trained on Yi with a high-quality corpus of 500B tokens and fine-tuned on 3M diverse fine-tuning samples.""

3.6T total pre-trained tokens ",,,,,Confident,"Yi-1.5 is an upgraded version of Yi. It is continuously pre-trained on Yi with a high-quality corpus of 500B tokens and fine-tuned on 3M diverse fine-tuning samples.

Compared with Yi, Yi-1.5 delivers stronger performance in coding, math, reasoning, and instruction-following capability, while still maintaining excellent capabilities in language understanding, commonsense reasoning, and reading comprehension.

Yi-1.5 comes in 3 model sizes: 34B, 9B, and 6B. For model details and benchmarks, see Model Card.",,,Open weights (restricted use),China,,,,,,2025-05-01 10:42,,,Yi-34B,,,Industry,,,,,Unreleased,"no training code

the model https://huggingface.co/01-ai/Yi-1.5-34B Apache 2.0

""If you create derivative works based on this model, please include the following attribution in your derivative works:""",Industry,,,,,Operation counting,01-ai,,,
Yi-1.5-9B,Language,"Language modeling/generation,Question answering",01.AI,,2024-05-13,Yi-1.5 is an upgraded version of Yi.,https://huggingface.co/01-ai/Yi-1.5-9B,,,,8830000000.0,8.83B (safetensors),1.90728e+23,6 FLOP / parameter / token * 8.83*10^9 parameters * 3.6*10^12 tokens = 1.90728e+23 FLOP,Unspecified unreleased,,3600000000000.0,3.6T pre-trained tokens,,,,,Confident,"Yi-1.5 is an upgraded version of Yi. It is continuously pre-trained on Yi with a high-quality corpus of 500B tokens and fine-tuned on 3M diverse fine-tuning samples.

Compared with Yi, Yi-1.5 delivers stronger performance in coding, math, reasoning, and instruction-following capability, while still maintaining excellent capabilities in language understanding, commonsense reasoning, and reading comprehension.",,,Open weights (unrestricted),China,,,,,,2025-05-16 10:30,,,,,,Industry,,,,,Unreleased,"https://huggingface.co/01-ai/Yi-1.5-9B
Apache 2.0",Industry,,,,,Operation counting,01-ai,,,
llama-3-airoboros-70b-3.3,Language,"Language modeling/generation,Question answering",,Jon Durbin,2024-05-12,airoboros: using large language models to fine-tune large language models,https://huggingface.co/jondurbin/airoboros-70b-3.3,,,,70000000000.0,same as llama 3 70B,,,airoboros datasets,"The fine-tuning data was mostly generated by OpenAI API calls to gpt-4, via airoboros",,,,,NVIDIA RTX A6000,,Confident,"This is my take on implementing the Self-Instruct paper. The approach is quite heavily modified, and does not use any human-generated seeds.

This updated implementation supports either the /v1/completions endpoint or /v1/chat/completions, which is particularly useful in that it supports gpt-4 and gpt-3.5-turbo (which is 1/10 the cost of text-davinci-003).

Huge thank you to the folks over at a16z for sponsoring the costs associated with building models and associated tools!",,,Open weights (restricted use),,Llama 3-70B,,,2.0,,2025-05-01 10:42,,,,,,,,,,,Open source,"""I am purposingly leaving this license ambiguous (other than the fact you must comply with the Meta original license for llama-2) because I am not a lawyer and refuse to attempt to interpret all of the terms accordingly.

Your best bet is probably to avoid using this commercially due to the OpenAI API usage.

Either way, by using this model, you agree to completely indemnify me.

You must also agree to all of the terms in the origina llama-3 license.""
https://huggingface.co/jondurbin/airoboros-70b-3.3

training code Apache 2.0
https://github.com/jondurbin/airoboros",,,,,1185.416125675559,,jondurbin,,,
MoLeR,Biology,Drug discovery,"Microsoft Research,Novartis","Krzysztof Maziarz, Henry Jackson-Flux, Pashmina Cameron, Finton Sirockin, Nadine Schneider, Nikolaus Stiefl, Marwin Segler, Marc Brockschmidt",2024-05-12,Learning to Extend Molecular Scaffolds with Structural Motifs,https://arxiv.org/abs/2103.03864,,,,,,2.1062592e+18,"training speed 95.2 molecules/sec (Table 1)

1.5*10^6 molecules / 95.2 = 15756 seconds = 4 hours (1 epoch)

""a few GPU days"" - let's assume (!) it was 10 GPU-days 

10*24*3600*8126000000000*0.3 = 2.1062592e+18",GuacaMol,"We use training data from GuacaMol (Brown et al., 2019), which released a curated set of â‰ˆ1.5M drug-like molecules, divided into train, validation and test sets.",,,,"""Training MoLeR requires first preprocessing the data, which takes up to one CPU day for GuacaMol, followed by training itself, which takes up to a few GPU days.""",NVIDIA Tesla K80,,Speculative,"Recent advancements in deep learning-based modeling of molecules promise to accelerate in silico drug discovery. A plethora of generative models is available, building molecules either atom-by-atom and bond-by-bond or fragment-by-fragment. However, many drug discovery projects require a fixed scaffold to be present in the generated molecule, and incorporating that constraint has only recently been explored. Here, we propose MoLeR, a graph-based model that naturally supports scaffolds as initial seed of the generative procedure, which is possible because it is not conditioned on the generation history. Our experiments show that MoLeR performs comparably to state-of-the-art methods on unconstrained molecular optimization tasks, and outperforms them on scaffold-based tasks, while being an order of magnitude faster to train and sample from than existing approaches. Furthermore, we show the influence of a number of seemingly minor design choices on the overall performance.",,,Open weights (unrestricted),"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Switzerland",,,,,,2025-05-01 10:42,,,,,,"Industry,Industry",,,,,Open source,https://github.com/microsoft/molecule-generation,"Industry,Industry",,,,,Hardware,,,,
Fugaku-LLM,Language,"Language modeling,Translation,Japanese language modeling,Language modeling/generation,Question answering","Tohoku University,CyberAgent,Tokyo Institute of Technology,Fujitsu,RIKEN,Nagoya University,Kotoba Technologies",Fugaku Team,2024-05-10,Release of â€œFugaku-LLMâ€ â€“ a large language model trained on the supercomputer â€œFugakuâ€,https://www.fujitsu.com/global/about/resources/news/press-releases/2024/0510-01.html,,,,13000000000.0,"""Fugaku-LLM has 13 billion parameters (2)""",2.9640000000001e+22,https://www.wolframalpha.com/input?i=6+FLOP+*+13+billion+*+380+billion,Unspecified unreleased,"""Fugaku-LLM was trained on proprietary Japanese data collected by CyberAgent, along with English data, and other data.""",380000000000.0,"""Fugaku-LLM was trained on 380 billion tokens using 13,824 nodes of Fugaku, with about 60% of the training data being Japanese, combined with English, mathematics, and code.""",,,,,Confident,"A team of researchers in Japan released Fugaku-LLM, a large language model (1) with enhanced Japanese language capability, using the RIKEN supercomputer Fugaku. The team is led by Professor Rio Yokota of Tokyo Institute of Technology, Associate Professor Keisuke Sakaguchi of Tohoku University, Koichi Shirahata of Fujitsu Limited, Team Leader Mohamed Wahib of RIKEN, Associate Professor Koji Nishiguchi of Nagoya University, Shota Sasaki of CyberAgent, Inc, and Noriyuki Kojima of Kotoba Technologies Inc.

To train large language models on Fugaku, the researchers developed distributed training methods, including porting the deep learning framework Megatron-DeepSpeed to Fugaku in order to optimize the performance of Transformers on Fugaku. They accelerated the dense matrix multiplication library for Transformers, and optimized communication performance for Fugaku by combining three types of parallelization techniques and accelerated the collective communication library on the Tofu interconnect D.

Fugaku-LLM has 13 billion parameters (2) and is larger than the 7-billion-parameter models that have been developed widely in Japan. Fugaku-LLM has enhanced Japanese capabilities, with an average score of 5.5 on the Japanese MT-Bench (3), the highest performance among open models that are trained using original data produced in Japan. In particular, the benchmark performance for humanities and social sciences tasks reached a remarkably high score of 9.18.

Fugaku-LLM was trained on proprietary Japanese data collected by CyberAgent, along with English data, and other data. The source code of Fugaku-LLM is available on GitHub (4) and the model is available on Hugging Face (5). Fugaku-LLM can be used for research and commercial purposes as long as users comply with the license.

In the future, as more researchers and engineers participate in improving the models and their applications, the efficiency of training will be improved, leading to next-generation innovative research and business applications, such as the linkage of scientific simulation and generative AI, and social simulation of virtual communities with thousands of AIs.",,,Open weights (unrestricted),"Japan,Japan,Japan,Japan,Japan,Japan,Japan",,,,,,2024-12-16 14:49,,,,,,"Academia,Industry,Academia,Industry,Academia,Industry",,,,,Unreleased,,"Academia,Industry,Academia,Industry,Academia,Industry",,,,,Operation counting,,,,
Gemini 1.5 Flash,"Multimodal,Language,Vision,Audio","Chat,Image captioning,Visual question answering,Translation,Language modeling/generation,Question answering,Speech recognition",Google DeepMind,Gemini Team,2024-05-10,Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context,https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf,,,,,,,"""Gemini 1.5 Flash is a dense Transformer based model that is online distilled [...] from Gemini 1.5 Pro.""",,"""Our pre-training dataset includes data sourced across many different domains, including web documents and code, and incorporates image, audio, and video content. For the instruction tuning phase we finetuned Gemini 1.5 models on a collection of multimodal data (containing paired instructions and appropriate responses), with further tuning based on human preference data. We refer readers to the Gemini 1.0 Technical Report (Gemini-Team et al., 2023) for further information.""
",,,,"""Gemini 1.5 models are trained on multiple 4096-chip pods of Googleâ€™s TPUv4 accelerators, distributed across multiple datacenters""",Google TPU v4,,Unknown,"In this report, we introduce the Gemini 1.5 family of models, representing the next generation of highly compute-efficient multimodal models capable of recalling and reasoning over fine-grained information from millions of tokens of context, including multiple long documents and hours of video and audio. The family includes two new models: (1) an updated Gemini 1.5 Pro, which exceeds the February version on the great majority of capabilities and benchmarks; (2) Gemini 1.5 Flash, a more lightweight variant designed for efficiency with minimal regression in quality. Gemini 1.5 models achieve near-perfect recall on long-context retrieval tasks across modalities, improve the state-of-the-art in long-document QA, long-video QA and long-context ASR, and match or surpass Gemini 1.0 Ultraâ€™s state-of-the-art performance across a broad set of benchmarks. Studying the limits of Gemini 1.5â€™s long-context ability, we find continued improvement in next-token prediction and near-perfect retrieval (>99%) up to at least 10M tokens, a generational leap over existing models such as Claude 3.0 (200k) and GPT-4 Turbo (128k). Finally, we highlight real-world use cases, such as Gemini 1.5 collaborating with professions on their completing their tasks achieving 26 to 75% time savings across 10 different job categories, as well as surprising new capabilities of large language models at the frontier; when given a grammar manual for Kalamang, a language with fewer than 200 speakers worldwide, the model learns to translate English to Kalamang at a similar level to a person who learned from the same content.
",,,API access,"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France",,,,,,2025-06-18 01:07,,,,,,Industry,,,,,Unreleased,API access: https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models,Industry,,,,,,,,,
Gemini 1.5 Flash 8B,"Multimodal,Language,Vision,Audio","Chat,Image captioning,Visual question answering,Translation,Language modeling/generation,Question answering,Speech recognition",Google DeepMind,Gemini Team,2024-05-10,Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context,https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf,,,,8000000000.0,8B,,,,"""Our pre-training dataset includes data sourced across many different domains, including web documents and code, and incorporates image, audio, and video content. For the instruction tuning phase we finetuned Gemini 1.5 models on a collection of multimodal data (containing paired instructions and appropriate responses), with further tuning based on human preference data. We refer readers to the Gemini 1.0 Technical Report (Gemini-Team et al., 2023) for further information.""
",,,,"""Gemini 1.5 models are trained on multiple 4096-chip pods of Googleâ€™s TPUv4 accelerators, distributed across multiple datacenters""",Google TPU v4,,Confident,"In this report, we introduce the Gemini 1.5 family of models, representing the next generation of highly compute-efficient multimodal models capable of recalling and reasoning over fine-grained information from millions of tokens of context, including multiple long documents and hours of video and audio. The family includes two new models: (1) an updated Gemini 1.5 Pro, which exceeds the February version on the great majority of capabilities and benchmarks; (2) Gemini 1.5 Flash, a more lightweight variant designed for efficiency with minimal regression in quality. Gemini 1.5 models achieve near-perfect recall on long-context retrieval tasks across modalities, improve the state-of-the-art in long-document QA, long-video QA and long-context ASR, and match or surpass Gemini 1.0 Ultraâ€™s state-of-the-art performance across a broad set of benchmarks. Studying the limits of Gemini 1.5â€™s long-context ability, we find continued improvement in next-token prediction and near-perfect retrieval (>99%) up to at least 10M tokens, a generational leap over existing models such as Claude 3.0 (200k) and GPT-4 Turbo (128k). Finally, we highlight real-world use cases, such as Gemini 1.5 collaborating with professions on their completing their tasks achieving 26 to 75% time savings across 10 different job categories, as well as surprising new capabilities of large language models at the frontier; when given a grammar manual for Kalamang, a language with fewer than 200 speakers worldwide, the model learns to translate English to Kalamang at a similar level to a person who learned from the same content.
",,,API access,"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France",,,,,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,API access: https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models,Industry,,,,,,,,,
MatterSim (M3GNet - MatterSim-v1.0.0-5M),Materials science,"Atomistic simulations,Molecular simulation",Microsoft Research AI for Science,"Han Yang, Chenxi Hu, Yichi Zhou, Xixian Liu, Yu Shi, Jielan Li, Guanzhi Li, Zekun Chen, Shuizhou Chen, Claudio Zeni, Matthew Horton, Robert Pinsler, Andrew Fowler, Daniel ZÃ¼gner, Tian Xie, Jake Smith, Lixin Sun, Qian Wang, Lingyu Kong, Chang Liu, Hongxia Hao, Ziheng Lu",2024-05-10,"MatterSim: A Deep Learning Atomistic Model Across Elements, Temperatures and Pressures",https://arxiv.org/abs/2405.04967,,,,4500000.0,"""As the training data size increases up to 3M, the the total number of parameters in M3GNet increase accordingly from 880K to 4.5M.""",1.62e+16,"Speculative confidence because I am unsure how to calculate gradient updates/tokens

6ND = 6*4500000*3000000*200 = 1.62e+16",,,3000000.0,"""As the training data size increases up to 3M, the the total number of parameters in M3GNet increase accordingly from 880K to 4.5M.""",,,NVIDIA A100,,Speculative,"Accurate and fast prediction of materials properties is central to the digital transformation of materials design. However, the vast design space and diverse operating conditions pose significant challenges for accurately modeling arbitrary material candidates and forecasting their properties. We present MatterSim, a deep learning model actively learned from large-scale first-principles computations, for efficient atomistic simulations at first-principles level and accurate prediction of broad material properties across the periodic table, spanning temperatures from 0 to 5000 K and pressures up to 1000 GPa. Out-of-the-box, the model serves as a machine learning force field, and shows remarkable capabilities not only in predicting ground-state material structures and energetics, but also in simulating their behavior under realistic temperatures and pressures, signifying an up to ten-fold enhancement in precision compared to the prior best-in-class. This enables MatterSim to compute materials' lattice dynamics, mechanical and thermodynamic properties, and beyond, to an accuracy comparable with first-principles methods. Specifically, MatterSim predicts Gibbs free energies for a wide range of inorganic solids with near-first-principles accuracy and achieves a 15 meV/atom resolution for temperatures up to 1000K compared with experiments. This opens an opportunity to predict experimental phase diagrams of materials at minimal computational cost. Moreover, MatterSim also serves as a platform for continuous learning and customization by integrating domain-specific data. The model can be fine-tuned for atomistic simulations at a desired level of theory or for direct structure-to-property predictions, achieving high data efficiency with a reduction in data requirements by up to 97%.",200.0,,Open weights (unrestricted),"United States of America,United Kingdom of Great Britain and Northern Ireland,China,Netherlands,Germany",,,,8.0,,2025-05-01 10:42,,,,,,Industry,,,,,,"MIT license 
https://github.com/microsoft/mattersim",Industry,,,,6322.500926906498,Operation counting,,,,
MatterSim (Grpaphomer),Materials science,"Atomistic simulations,Molecular simulation",Microsoft Research AI for Science,"Han Yang, Chenxi Hu, Yichi Zhou, Xixian Liu, Yu Shi, Jielan Li, Guanzhi Li, Zekun Chen, Shuizhou Chen, Claudio Zeni, Matthew Horton, Robert Pinsler, Andrew Fowler, Daniel ZÃ¼gner, Tian Xie, Jake Smith, Lixin Sun, Qian Wang, Lingyu Kong, Chang Liu, Hongxia Hao, Ziheng Lu",2024-05-10,"MatterSim: A Deep Learning Atomistic Model Across Elements, Temperatures and Pressures",https://arxiv.org/abs/2405.04967,,,,182000000.0,"""The total parameters of Graphormer is 182M.""",1.118208e+20,"Speculative confidence because I am unsure how to calculate gradient updates/tokens

6*1,562,500*256*256*182000000=1.118208e+20",,,,"""The model is trained for a total of 1,562,500 steps""
""The batch size for training is set to 256""
""the maximum number of expanded atoms is capped at 256""

",,,NVIDIA A100,,Speculative,"Accurate and fast prediction of materials properties is central to the digital transformation of materials design. However, the vast design space and diverse operating conditions pose significant challenges for accurately modeling arbitrary material candidates and forecasting their properties. We present MatterSim, a deep learning model actively learned from large-scale first-principles computations, for efficient atomistic simulations at first-principles level and accurate prediction of broad material properties across the periodic table, spanning temperatures from 0 to 5000 K and pressures up to 1000 GPa. Out-of-the-box, the model serves as a machine learning force field, and shows remarkable capabilities not only in predicting ground-state material structures and energetics, but also in simulating their behavior under realistic temperatures and pressures, signifying an up to ten-fold enhancement in precision compared to the prior best-in-class. This enables MatterSim to compute materials' lattice dynamics, mechanical and thermodynamic properties, and beyond, to an accuracy comparable with first-principles methods. Specifically, MatterSim predicts Gibbs free energies for a wide range of inorganic solids with near-first-principles accuracy and achieves a 15 meV/atom resolution for temperatures up to 1000K compared with experiments. This opens an opportunity to predict experimental phase diagrams of materials at minimal computational cost. Moreover, MatterSim also serves as a platform for continuous learning and customization by integrating domain-specific data. The model can be fine-tuned for atomistic simulations at a desired level of theory or for direct structure-to-property predictions, achieving high data efficiency with a reduction in data requirements by up to 97%.",,,,"United States of America,United Kingdom of Great Britain and Northern Ireland,China,Netherlands,Germany",,,,64.0,,2025-05-01 10:42,,,,,,Industry,,,,,,,Industry,,,,50580.00741525198,Operation counting,,,,
Falcon 2 11B,Language,Language modeling/generation,Technology Innovation Institute,,2024-05-09,Falcon2-11B,https://huggingface.co/tiiuae/falcon-11B ,,,,11000000000.0,11B,3.6e+23,"trained on 5.5T tokens

6 * 11B * 5.5T = 3.6e23",RefinedWeb,"""Falcon2-11B was trained over 5,000B tokens of RefinedWeb, a high-quality filtered and deduplicated web dataset which we enhanced with curated corpora. It followed a four stage training strategy. The first three stages were focused on increasing the context length, from to 2048 to 4096 and finally to 8192 tokens. The last stage aimed to further enhance performance using only high quality data.""

Possibly an updated version of RefinedWeb, which only had 3.5T tokens when Falcon 1 was released? not clear.",5500000000000.0,5.5T tokens: https://falconllm.tii.ae/falcon-2.html ,1400.0,"roughly two months: https://huggingface.co/tiiuae/falcon-11B 

so ~1400 days",NVIDIA A100 SXM4 40 GB,,Confident,"Falcon2-11B is an 11B parameters causal decoder-only model built by TII and trained on over 5,000B tokens of RefinedWeb enhanced with curated corpora. The model is made available under the TII Falcon License 2.0, the permissive Apache 2.0-based software license which includes an acceptable use policy that promotes the responsible use of AI.",,,Open weights (restricted use),United Arab Emirates,,,,,,2025-06-10 15:56,,,,,,Government,,,,,Unreleased,"Open but has an acceptable use policy: https://falconllm-staging.tii.ae/falcon-2-acceptable-use-policy.html 

https://huggingface.co/tiiuae/falcon-11B",Government,,,,,Operation counting,tiiuae,,,
AlphaFold 3,Biology,Protein folding prediction,"Google DeepMind,Isomorphic Labs","Josh Abramson, Jonas Adler, Jack Dunger, Richard Evans, Tim Green, Alexander Pritzel, Olaf Ronneberger, Lindsay Willmore, Andrew J. Ballard, Joshua Bambrick, Sebastian W. Bodenstein, David A. Evans, Chia-Chun Hung, Michael Oâ€™Neill, David Reiman, Kathryn Tunyasuvunakool, Zachary Wu, AkvilÄ— Å½emgulytÄ—, Eirini Arvaniti, Charles Beattie, Ottavia Bertolli, Alex Bridgland, Alexey Cherepanov, Miles Congreve, Alexander I. Cowen-Rivers, Andrew Cowie, Michael Figurnov, Fabian B. Fuchs, Hannah Gladman, Rishub Jain, Yousuf A. Khan, Caroline M. R. Low, Kuba Perlin, Anna Potapenko, Pascal Savy, Sukhdeep Singh, Adrian Stecula, Ashok Thillaisundaram, Catherine Tong, Sergei Yakneen, Ellen D. Zhong, Michal Zielinski, Augustin Å½Ã­dek, Victor Bapst, Pushmeet Kohli, Max Jaderberg, Demis Hassabis, John M. Jumper",2024-05-08,Accurate structure prediction of biomolecular interactions with AlphaFold 3,https://www.nature.com/articles/s41586-024-07487-w,,,,,,4.1405645e+22,256 GPUs * 480 hours [see training time notes] * 3600 sec / hour * 312000000000000 FLOP / GPU / sec * 0.3 [assumed utilization] = 4.1405645e+22 FLOP,PDB (Protein Data Bank),,29491200000.0,"from https://www.biorxiv.org/content/10.1101/2024.11.19.624167v2.full.pdf
""As a comparison AlphaFol3 trained a similar architecture for nearly 150k steps with a batch size of 256""

from supplementary materials ""The model is trained with a batch size of 256""

150000 steps * 256 sequences per batch * 384 tokens per batch [at initial training stage, Table 6, supplementary materials] = 14 745 600 000 tokens

from supplementary materials Table 6 fine-tuning took exactly the same amount of GPU-hours -> the entire amount of training tokens is 14 745 600 000 * 2 = 29491200000
",480.0,"supplementary materials Table 6
20 days *24 hours = 480 hours",NVIDIA A100,,Confident,"The introduction of AlphaFoldâ€‰2 has spurred a revolution in modelling the structure of proteins and their interactions, enabling a huge range of applications in protein modelling and design Here we describe our AlphaFoldâ€‰3 model with a substantially updated diffusion-based architecture that is capable of predicting the joint structure of complexes including proteins, nucleic acids, small molecules, ions and modified residues. The new AlphaFold model demonstrates substantially improved accuracy over many previous specialized tools: far greater accuracy for proteinâ€“ligand interactions compared with state-of-the-art docking tools, much higher accuracy for proteinâ€“nucleic acid interactions compared with nucleic-acid-specific predictors and substantially higher antibodyâ€“antigen prediction accuracy compared with AlphaFold-Multimer. Together, these results show that high-accuracy modelling across biomolecular space is possible within a single unified deep-learning framework.",1.0,,Open weights (non-commercial),"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France,United Kingdom of Great Britain and Northern Ireland",,,,256.0,,2025-05-06 15:22,,,,,,"Industry,Industry",,,,,Unreleased,"Inference code CC-BY-NC-SA 4.0
weights - permissions given after approval (only non-commercial use)","Industry,Industry",,,,202329.0409413959,Hardware,,,,
BiosimDock,"Medicine,Biology","Drug discovery,Protein-ligand binding affinity prediction",DeepOrigin,"Garik Petrosyan, Garegin Papoian, Natalie Ma, Tigran Abramyan",2024-05-08,We Spill the Beans: Deep Origin's AI- and Physics-Based Models for Drug Discovery,https://www.deeporigin.com/blog/we-spill-the-beans-deep-origins-ai-and-physics-based-models-for-drug-discovery,,,,,,,,PDBbind,,,,,,,,Unknown,"We outperform other models on accuracy of binding affinity and binding pose prediction
Docking and virtual screening tools are meaningful only if they are able to make fast, accurate, and useful predictions. A good model can filter true binders from a broad pool of potential molecules, including false positives that may be similar in chemical properties. In the hit identification stage of drug discovery it can give drug hunting teams a chemically-diverse set of potential hit molecules to evaluate with experimental assays or further computational analysis, helping them narrow the path to a lead candidate. In contrast, a bad model returns many false positives, costing a team money and effort â€“ and potentially leading drug hunters off-course for months or years.

To benchmark, we tested the BiosimDock model on the PDBbind core dataset and the DEKOIS 2.0 dataset.1, 2, 3, 4 The PDBbind core dataset contains 285 experimental structures of protein-ligand bound complexes across different protein classes. It remains a standard due to widespread use in benchmarking, facilitating comparison between models. It also enables accuracy prediction based on binding poses. DEKOIS 2.0 (Demanding Evaluation Kits for Objective In silico Screening) is an extensively-curated dataset of 81 targets across protein classes, including proteases, kinases, transferases, oxido-reductases, nuclear receptors, and hydrolases. Each target has an accompanying library of true binders and decoys, which have similar physical and chemical properties but do not interact with the target protein. This enables rigorous benchmarking of models for enrichment of true binders over false positives.4, 5, 6 ",,,Hosted access (no API),"Armenia,United States of America",,,,,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
Emu2,"Multimodal,Language,Vision,Image generation","Language modeling/generation,Question answering,Visual question answering,Image generation,Text-to-image","Beijing Academy of Artificial Intelligence / BAAI,Tsinghua University,Peking University","Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Zhengxiong Luo, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, Xinlong Wang",2024-05-08,Generative Multimodal Models are In-Context Learners,https://arxiv.org/abs/2312.13286,,,,37000000000.0,"37B

""we leverage pretrained EVA-02-CLIP-E-plus [70], LLaMA-33B [73] and SDXL [58] to initialize the Visual Encoder, Multimodal Modeling, and Visual Decoder, respectively""",,,"LAION-2B,CapsFusion-120M,WebVid-10M,MMC4 / Multimodal C4,YT-Storyboard-1B,GRIT,The Pile","""The pretraining data for Emu2 comprises several publicly accessible datasets, including image-text pairs from LAION-2B [65] and CapsFusion-120M [87], video-text pairs from WebVid-10M [8], interleaved image-text data from Multimodal-C4 (MMC4) [95], interleaved videotext data from YT-Storyboard-1B [71], grounded imagetext pairs from GRIT-20M introduced by Kosmos-2 [57]
and CapsFusion-grounded-100M curated by CapsFusion120M. Additionally, language-only data from Pile [26] is included to retain textual reasoning capability.""",,"1. only captioning loss on the text tokens:
""The input images are resized to 224Ã—224. <..> We pretrain Emu2 on 162 million imagetext samples and 7 million video-text samples for 35,200 iterations. The global batch size is 6,144 for the image-text pairs and 768 for video-text pairs. The training process is then restarted at a higher 448-pixel resolution for an additional 4,000 iterations.""

2. freeze the Visual Encoder and only optimize the linear projection layer and Multimodel Modeling with both text classification loss and image regression loss.:
"" All images are resized to 448 Ã— 448, and the maximum learning
rate is 1 \times 10^{-5} . We use a global batch size of 12,800 for
image-text pair data, 6,400 for video-text pair data, 3,200
for image-text and video-text interleaved data, and 800 for
language-only data. The training process spans 20,350 iterations and consumes about 160 million samples of imagetext data and 3.8B tokens of language-only data.""

3. Visual decoding",,,,,Confident,"The human ability to easily solve multimodal tasks in context (i.e., with only a few demonstrations or simple instructions), is what current multimodal systems have largely struggled to imitate. In this work, we demonstrate that the task-agnostic in-context learning capabilities of large multimodal models can be significantly enhanced by effective scaling-up. We introduce Emu2, a generative multimodal model with 37 billion parameters, trained on large-scale multimodal sequences with a unified autoregressive objective. Emu2 exhibits strong multimodal in-context learning abilities, even emerging to solve tasks that require on-the-fly reasoning, such as visual prompting and object-grounded generation. The model sets a new record on multiple multimodal understanding tasks in few-shot settings. When instruction-tuned to follow specific instructions, Emu2 further achieves new state-of-the-art on challenging tasks such as question answering benchmarks for large multimodal models and open-ended subject-driven generation. These achievements demonstrate that Emu2 can serve as a base model and general-purpose interface for a wide range of multimodal tasks. Code and models are publicly available to facilitate future research.",,,Open weights (non-commercial),"China,China,China","EVA-CLIP (EVA-02-CLIP-E/14+),LLaMA-33B,Stable Diffusion XL (SDXL)",,,,,2025-06-12 13:03,,,,,,"Academia,Academia,Academia",,,,,Unreleased,"Apache 2.0
https://github.com/baaivision/Emu/tree/main/Emu2

License: Non-commercial license (mentioned in github readme)
https://huggingface.co/BAAI/Emu2","Academia,Academia,Academia",,,BF16,,,BAAI,,,
DeepSeek-V2 (MoE-236B),Language,"Language modeling/generation,Chat,Code generation",DeepSeek,"DeepSeek-AI, Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Hanwei Xu, Hao Yang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J.L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jin Chen, Jingyang Yuan, Junjie Qiu, Junxiao Song, Kai Dong, Kaige Gao, Kang Guan, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qihao Zhu, Qinyu Chen, Qiushi Du, R.J. Chen, R.L. Jin, Ruiqi Ge, Ruizhe Pan, Runxin Xu, Ruyi Chen, S.S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Size Zheng, T. Wang, Tian Pei, Tian Yuan, Tianyu Sun, W.L. Xiao, Wangding Zeng, Wei An, Wen Liu, Wenfeng Liang, Wenjun Gao, Wentao Zhang, X.Q. Li, Xiangyue Jin, Xianzu Wang, Xiao Bi, Xiaodong Liu, Xiaohan Wang, Xiaojin Shen, Xiaokang Chen, Xiaosha Chen, Xiaotao Nie, Xiaowen Sun",2024-05-07,"DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model","https://arxiv.org/abs/2405.04434 
https://github.com/deepseek-ai/DeepSeek-V2 ",,,as of July 2024 the strongest open model in lmsys: https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard ,236000000000.0,"21B active params, 236B total",1.02e+24,21b active params * 8.1 trillion * 6 = 1.02e24,Unspecified unreleased,"""We construct a high-quality and multi-source pre-training corpus consisting of 8.1T tokens.
Compared with the corpus used in DeepSeek 67B (our previous release) (DeepSeek-AI, 2024), this
corpus features an extended amount of data, especially Chinese data, and higher data quality. We
first pretrain DeepSeek-V2 on the full pre-training corpus""",8100000000000.0,8.1 Trillion,,"172.8K GPU hours, wall time not stated",NVIDIA H800 SXM5,,Confident,"We present DeepSeek-V2, a strong Mixture-of-Experts (MoE) language model characterized by economical training and efficient inference. It comprises 236B total parameters, of which 21B are activated for each token, and supports a context length of 128K tokens. DeepSeek-V2 adopts innovative architectures including Multi-head Latent Attention (MLA) and DeepSeekMoE. MLA guarantees efficient inference through significantly compressing the Key-Value (KV) cache into a latent vector, while DeepSeekMoE enables training strong models at an economical cost through sparse computation. Compared with DeepSeek 67B, DeepSeek-V2 achieves significantly stronger performance, and meanwhile saves 42.5% of training costs, reduces the KV cache by 93.3%, and boosts the maximum generation throughput to 5.76 times. We pretrain DeepSeek-V2 on a high-quality and multi-source corpus consisting of 8.1T tokens, and further perform Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) to fully unlock its potential. Evaluation results show that, even with only 21B activated parameters, DeepSeek-V2 and its chat versions still achieve top-tier performance among open-source models.",,,Open weights (restricted use),China,,,,,,2025-05-01 10:42,,Paper on DeepSeek-V3,,18432000.0,"During main training: ""the batch size is gradually increased from 2304 to 9216 in the training of the first 225B tokens, and then keeps 9216 in the remaining training. We set the maximum sequence length to 4K."" So for most of training it is 9216*4k = 36,864. They then do long context training ""with a sequence length of 32K and a batch size of 576 sequences"" so 32K * 576 = 18,432,000",Industry,,,,172800.0,Unreleased,open weights with harmful use restrictions: https://github.com/deepseek-ai/DeepSeek-V2/blob/main/LICENSE-MODEL ,Industry,,,,,Operation counting,deepseek-ai,,,
xLSTM 1.4B,Language,,Johannes Kepler University Linz,"Maximilian Beck, Korbinian PÃ¶ppel, Markus Spanring, Andreas Auer, Oleksandra Prudnikova, Michael Kopp, GÃ¼nter Klambauer, Johannes Brandstetter, Sepp Hochreiter",2024-05-07,xLSTM: Extended Long Short-Term Memory,https://arxiv.org/abs/2405.04517,,,,1422600000.0,,2.56e+18,"""We developed and trained all our models and baselines over the course of
three months on a cluster with 128 nodes of eight NVIDIA A100 GPUs each."" 1024 

6*300000000*1422600000=2.56068e+18",SlimPajama,"""We therefore increase the amount of training data and train on 300B tokens
from SlimPajama. """,300000000.0,"""We therefore increase the amount of training data and train on 300B tokens
from SlimPajama. """,,,NVIDIA A100,,Confident,"In the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories, in particular they constituted the first Large Language Models (LLMs). However, the advent of the Transformer technology with parallelizable self-attention at its core marked the dawn of a new era, outpacing LSTMs at scale. We now raise a simple question: How far do we get in language modeling when scaling LSTMs to billions of parameters, leveraging the latest techniques from modern LLMs, but mitigating known limitations of LSTMs? Firstly, we introduce exponential gating with appropriate normalization and stabilization techniques. Secondly, we modify the LSTM memory structure, obtaining: (i) sLSTM with a scalar memory, a scalar update, and new memory mixing, (ii) mLSTM that is fully parallelizable with a matrix memory and a covariance update rule. Integrating these LSTM extensions into residual block backbones yields xLSTM blocks that are then residually stacked into xLSTM architectures. Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling. Code available at: https://github.com/NX-AI/xlstm",1.0,,,Austria,,,,1024.0,,2025-05-01 10:42,,,,,,Academia,,,,,,,Academia,,,,809334.1869283952,,,,,
Med-Gemini-2D,"Medicine,Vision","Visual question answering,Medical diagnosis","Google DeepMind,Google Research","Lin Yang, Shawn Xu, Andrew Sellergren, Timo Kohlberger, Yuchen Zhou, Ira Ktena, Atilla Kiraly, Faruk Ahmed, Farhad Hormozdiari, Tiam Jaroensri, Eric Wang, Ellery Wulczyn, Fayaz Jamil, Theo Guidroz, Chuck Lau, Siyuan Qiao, Yun Liu, Akshay Goel, Kendall Park, Arnav Agharwal, Nick George, Yang Wang, Ryutaro Tanno, David G. T. Barrett, Wei-Hung Weng, S. Sara Mahdavi, Khaled Saab, Tao Tu, Sreenivasa Raju Kalidindi, Mozziyar Etemadi, Jorge Cuadros, Gregory Sorensen, Yossi Matias, Katherine Chou, Greg Corrado, Joelle Barral, Shravya Shetty, David Fleet, S. M. Ali Eslami, Daniel Tse, Shruthi Prabhakara, Cory McLean, Dave Steiner, Rory Pilgrim, Christopher Kelly, Shekoofeh Azizi, Daniel Golden",2024-05-06,Advancing Multimodal Medical Capabilities of Gemini,https://arxiv.org/abs/2405.03162,,,,,,,,"Slake-VQA,MIMIC-CXR,PAD-UFES-20,Path-VQA,CXR-US2,PubMed Central,UK Biobank","""More than 7 million data samples from 3.7 million medical images and cases is used for fine-tuning and further instruction-tuning of Gemini for medical applications in Med-Gemini. This includes diverse set of modalities including 2D and 3D radiology images, pathology,
ophthalmology, and genomic data. These datasets includes mostly free text paired with medical data, which eliminates the need for expensive expert labeling of the training data.""",,,,"""Like its predecessor Gemini 1.5 Pro and all other Gemini models, Med-Gemini was trained on
large-scale Google TPUv4 accelerator pods spread across multiple data-centers.""",Google TPU v4,,Unknown,"Many clinical tasks require an understanding of specialized data, such as medical images and genomics, which is not typically found in general-purpose large multimodal models. Building upon Gemini's multimodal models, we develop several models within the new Med-Gemini family that inherit core capabilities of Gemini and are optimized for medical use via fine-tuning with 2D and 3D radiology, histopathology, ophthalmology, dermatology and genomic data. Med-Gemini-2D sets a new standard for AI-based chest X-ray (CXR) report generation based on expert evaluation, exceeding previous best results across two separate datasets by an absolute margin of 1% and 12%, where 57% and 96% of AI reports on normal cases, and 43% and 65% on abnormal cases, are evaluated as ""equivalent or better"" than the original radiologists' reports. We demonstrate the first ever large multimodal model-based report generation for 3D computed tomography (CT) volumes using Med-Gemini-3D, with 53% of AI reports considered clinically acceptable, although additional research is needed to meet expert radiologist reporting quality. Beyond report generation, Med-Gemini-2D surpasses the previous best performance in CXR visual question answering (VQA) and performs well in CXR classification and radiology VQA, exceeding SoTA or baselines on 17 of 20 tasks. In histopathology, ophthalmology, and dermatology image classification, Med-Gemini-2D surpasses baselines across 18 out of 20 tasks and approaches task-specific model performance. Beyond imaging, Med-Gemini-Polygenic outperforms the standard linear polygenic risk score-based approach for disease risk prediction and generalizes to genetically correlated diseases for which it has never been trained. Although further development and evaluation are necessary in the safety-critical medical domain, our results highlight the potential of Med-Gemini across a wide range of medical tasks.
",,,Unreleased,"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France,Multinational,United States of America,Canada,Switzerland",Gemini 1.5 Pro,,,,,2025-05-01 10:42,,,,,,"Industry,Industry",,,,,Unreleased,,"Industry,Industry",,,,,,,,,
Med-Gemini-3D,"Medicine,Vision","Visual question answering,Medical diagnosis","Google DeepMind,Google Research","Lin Yang, Shawn Xu, Andrew Sellergren, Timo Kohlberger, Yuchen Zhou, Ira Ktena, Atilla Kiraly, Faruk Ahmed, Farhad Hormozdiari, Tiam Jaroensri, Eric Wang, Ellery Wulczyn, Fayaz Jamil, Theo Guidroz, Chuck Lau, Siyuan Qiao, Yun Liu, Akshay Goel, Kendall Park, Arnav Agharwal, Nick George, Yang Wang, Ryutaro Tanno, David G. T. Barrett, Wei-Hung Weng, S. Sara Mahdavi, Khaled Saab, Tao Tu, Sreenivasa Raju Kalidindi, Mozziyar Etemadi, Jorge Cuadros, Gregory Sorensen, Yossi Matias, Katherine Chou, Greg Corrado, Joelle Barral, Shravya Shetty, David Fleet, S. M. Ali Eslami, Daniel Tse, Shruthi Prabhakara, Cory McLean, Dave Steiner, Rory Pilgrim, Christopher Kelly, Shekoofeh Azizi, Daniel Golden",2024-05-06,Advancing Multimodal Medical Capabilities of Gemini,https://arxiv.org/abs/2405.03162,,,,,,,,CT-US1,"""More than 7 million data samples from 3.7 million medical images and cases is used for fine-tuning and further instruction-tuning of Gemini for medical applications in Med-Gemini. This includes diverse set of modalities including 2D and 3D radiology images, pathology,
ophthalmology, and genomic data. These datasets includes mostly free text paired with medical data, which eliminates the need for expensive expert labeling of the training data.""",,,,"""Like its predecessor Gemini 1.5 Pro and all other Gemini models, Med-Gemini was trained on
large-scale Google TPUv4 accelerator pods spread across multiple data-centers.""",Google TPU v4,,Unknown,"Many clinical tasks require an understanding of specialized data, such as medical images and genomics, which is not typically found in general-purpose large multimodal models. Building upon Gemini's multimodal models, we develop several models within the new Med-Gemini family that inherit core capabilities of Gemini and are optimized for medical use via fine-tuning with 2D and 3D radiology, histopathology, ophthalmology, dermatology and genomic data. Med-Gemini-2D sets a new standard for AI-based chest X-ray (CXR) report generation based on expert evaluation, exceeding previous best results across two separate datasets by an absolute margin of 1% and 12%, where 57% and 96% of AI reports on normal cases, and 43% and 65% on abnormal cases, are evaluated as ""equivalent or better"" than the original radiologists' reports. We demonstrate the first ever large multimodal model-based report generation for 3D computed tomography (CT) volumes using Med-Gemini-3D, with 53% of AI reports considered clinically acceptable, although additional research is needed to meet expert radiologist reporting quality. Beyond report generation, Med-Gemini-2D surpasses the previous best performance in CXR visual question answering (VQA) and performs well in CXR classification and radiology VQA, exceeding SoTA or baselines on 17 of 20 tasks. In histopathology, ophthalmology, and dermatology image classification, Med-Gemini-2D surpasses baselines across 18 out of 20 tasks and approaches task-specific model performance. Beyond imaging, Med-Gemini-Polygenic outperforms the standard linear polygenic risk score-based approach for disease risk prediction and generalizes to genetically correlated diseases for which it has never been trained. Although further development and evaluation are necessary in the safety-critical medical domain, our results highlight the potential of Med-Gemini across a wide range of medical tasks.
",,,Unreleased,"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France,Multinational,United States of America,Canada,Switzerland",Gemini 1.5 Pro,,,,,2025-05-01 10:42,,,,,,"Industry,Industry",,,,,Unreleased,,"Industry,Industry",,,,,,,,,
Microsoft MAI-1,Language,"Language modeling,Language modeling/generation",Microsoft,,2024-05-06,"Meet MAI-1: Microsoft Readies New AI Model to Compete With Google, OpenAI","https://www.theinformation.com/articles/meet-mai-1-microsoft-readies-new-ai-model-to-compete-with-google-openai

https://em360tech.com/tech-articles/what-mai-1-deep-dive-microsofts-gpt-4-rival",,,,500000000000.0,500B ,,,Unspecified unreleased,,,,,,,Supervised,Speculative,"""MAI-1 is a new large language model (LLM) being developed by Microsoft thatâ€™s reportedly as powerful as other leading LLMs including OpenAIâ€™s GPT-4 and Googleâ€™s Gemini Ultra.

The new model is being overseen by recently hired Mustafa Suleyman â€“ co-founder of Google DeepMind and former CEO of AI startup Inflection â€“  who joined Microsoft in March along with most of the startupâ€™s employees through a deal worth $625 million. 

Microsoft may reportedly use training data and certain other assets from Inflection AI to train MAI-1. The modelâ€™s training dataset is also said to include other types of textual data as well, including text generated by GPT-4 and content scraped from the web. Microsoft is reportedly carrying out the development process using a â€œlarge cluster of serversâ€ equipped with Nvidia Corp. graphics cards.

The exact purpose of the MAI-1 is not yet known and will depend on how well the model performs. 

But with a reported 500 parameters, the model is reportedly ""far larger"" than the smaller, open-source models Microsoft had previously trained.""",,,Unreleased,"United States of America,Multinational,India,Belgium",,,,,,2025-05-09 11:32,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
Soccer Robot,Robotics,"Animal (human/non-human) imitation,Sports","Google DeepMind,University College London (UCL)","Dhruva Tirumala, Markus Wulfmeier, Ben Moran, Sandy Huang, Jan Humplik, Guy Lever, Tuomas Haarnoja, Leonard Hasenclever, Arunkumar Byravan, Nathan Batchelor, Neil Sreendra, Kushal Patel, Marlon Gwira, Francesco Nori, Martin Riedmiller, Nicolas Heess",2024-05-03,Learning Robot Soccer from Egocentric Vision with Deep Reinforcement Learning,https://arxiv.org/abs/2405.02425,,,,,,,,,,,,,"""We use a distributed training setup with 64 V100 GPU actors and a
TPU v2 learner pod arranged in a 2 Ã— 2 topology.""","NVIDIA V100,Google TPU v2",,Unknown,"We apply multi-agent deep reinforcement learning (RL) to train end-to-end robot soccer policies with fully onboard computation and sensing via egocentric RGB vision. This setting reflects many challenges of real-world robotics, including active perception, agile full-body control, and long-horizon planning in a dynamic, partially-observable, multi-agent domain. We rely on large-scale, simulation-based data generation to obtain complex behaviors from egocentric vision which can be successfully transferred to physical robots using low-cost sensors. To achieve adequate visual realism, our simulation combines rigid-body physics with learned, realistic rendering via multiple Neural Radiance Fields (NeRFs). We combine teacher-based multi-agent RL and cross-experiment data reuse to enable the discovery of sophisticated soccer strategies. We analyze active-perception behaviors including object tracking and ball seeking that emerge when simply optimizing perception-agnostic soccer play. The agents display equivalent levels of performance and agility as policies with access to privileged, ground-truth state. To our knowledge, this paper constitutes a first demonstration of end-to-end training for multi-agent robot soccer, mapping raw pixel observations to joint-level actions, that can be deployed in the real world. Videos of the game-play and analyses can be seen on our website this https URL .",,,Unreleased,"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France,United Kingdom of Great Britain and Northern Ireland",,,,64.0,,2025-05-01 10:42,,,,,,"Industry,Academia",,,,,Unreleased,,"Industry,Academia",,,,,,,,,
MetaMath 70B,Language,Quantitative reasoning,"University of Cambridge,Southern University of Science and Technology (SUSTech),Hong Kong University of Science and Technology (HKUST),Huawei Noah's Ark Lab,Alan Turing Institute,Max Planck Institute for Intelligent Systems","Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T. Kwok, Zhenguo Li, Adrian Weller, Weiyang Liu",2024-05-03,MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models,https://arxiv.org/abs/2309.12284,,,,70000000000.0,70B,,,"GSM8K,MATH",,105600000.0,"160K examples 
396 MB

396 MB * 200000 english words per MB * 4/3 tokens per english word = 105600000 tokens",,,,,Likely,"Large language models (LLMs) have pushed the limits of natural language understanding and exhibited excellent problem-solving ability. Despite the great success, most existing open-source LLMs (e.g., LLaMA-2) are still far away from satisfactory for solving mathematical problem due to the complex reasoning procedures. To bridge this gap, we propose MetaMath, a fine-tuned language model that specializes in mathematical reasoning. Specifically, we start by bootstrapping mathematical questions by rewriting the question from multiple perspectives without extra knowledge, which results in a new dataset called MetaMathQA. Then we fine-tune the LLaMA-2 models on MetaMathQA. Experimental results on two popular benchmarks (i.e., GSM8K and MATH) for mathematical reasoning demonstrate that MetaMath outperforms a suite of open-source LLMs by a significant margin. Our MetaMath-7B model achieves 66.4% on GSM8K and 19.4% on MATH, exceeding the state-of-the-art models of the same size by 11.5% and 8.7%. Particularly, MetaMath-70B achieves an accuracy of 82.3% on GSM8K, slightly better than GPT-3.5-Turbo. We release all the MetaMathQA dataset, the MetaMath models with different model sizes and the training code for public use.",3.0,,Open weights (restricted use),"United Kingdom of Great Britain and Northern Ireland,China,Hong Kong,China,China,United Kingdom of Great Britain and Northern Ireland,Germany",Llama 2-70B,133056000000000000000,"Likely confidence because I am not sure about the epochs

6*70*10^9*105600000*3 = 1.33056 Ã— 10^20
",,,2025-05-01 10:42,,,,,,"Academia,Academia,Academia,Industry,Government,Academia",,,,,,"llama 2 license

https://huggingface.co/meta-math/MetaMath-70B-V1.0","Academia,Academia,Academia,Industry,Government,Academia",,,,,Operation counting,,,,
MetaMath 7B (Mistral finetune),Language,Quantitative reasoning,"University of Cambridge,Southern University of Science and Technology (SUSTech),Hong Kong University of Science and Technology (HKUST),Huawei Noah's Ark Lab,Alan Turing Institute,Max Planck Institute for Intelligent Systems","Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T. Kwok, Zhenguo Li, Adrian Weller, Weiyang Liu",2024-05-03,MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models,https://arxiv.org/abs/2309.12284,,,,7000000000.0,7B,,,"GSM8K,MATH",,105600000.0,"160K examples 
396 MB

396 MB * 200000 english words per MB * 4/3 tokens per english word = 105600000 tokens",,,NVIDIA A100,,Confident,"Large language models (LLMs) have pushed the limits of natural language understanding and exhibited excellent problem-solving ability. Despite the great success, most existing open-source LLMs (e.g., LLaMA-2) are still far away from satisfactory for solving mathematical problem due to the complex reasoning procedures. To bridge this gap, we propose MetaMath, a fine-tuned language model that specializes in mathematical reasoning. Specifically, we start by bootstrapping mathematical questions by rewriting the question from multiple perspectives without extra knowledge, which results in a new dataset called MetaMathQA. Then we fine-tune the LLaMA-2 models on MetaMathQA. Experimental results on two popular benchmarks (i.e., GSM8K and MATH) for mathematical reasoning demonstrate that MetaMath outperforms a suite of open-source LLMs by a significant margin. Our MetaMath-7B model achieves 66.4% on GSM8K and 19.4% on MATH, exceeding the state-of-the-art models of the same size by 11.5% and 8.7%. Particularly, MetaMath-70B achieves an accuracy of 82.3% on GSM8K, slightly better than GPT-3.5-Turbo. We release all the MetaMathQA dataset, the MetaMath models with different model sizes and the training code for public use.",3.0,,Open weights (unrestricted),"United Kingdom of Great Britain and Northern Ireland,China,Hong Kong,China,China,United Kingdom of Great Britain and Northern Ireland,Germany",Mistral 7B,13305600000000000000,6*7*10^9*105600000*3 = 1.33056 Ã— 10^19,8.0,,2025-05-01 10:42,,,,,,"Academia,Academia,Academia,Industry,Government,Academia",,,,,,"apache 2 license
https://huggingface.co/meta-math/MetaMath-Mistral-7B","Academia,Academia,Academia,Industry,Government,Academia",,,,6323.486590573598,Operation counting,meta-math,,,
Idefics2,"Multimodal,Language,Vision","Language modeling,Image captioning,Visual question answering","Hugging Face,Sorbonne University","Hugo LaurenÃ§on, LÃ©o Tronchon, Matthieu Cord, Victor Sanh",2024-05-03,What matters when building vision-language models?,https://arxiv.org/abs/2405.02246,,,,8000000000.0,8B,,,"OBELICS,LAION-COCO,OCR-IDL,PDFA,Public Multimodal Dataset (PMD)","Idefics2 was trained on a mixture of openly available datasets for the pretraining: Interleaved webdocuments (Wikipedia,OBELICS), image-caption pairs (Public Multimodal Dataset, LAION-COCO), OCR data (PDFA (en), IDL and Rendered-text, and image-to-code data (WebSight)).",321000000000.0,"""The resulting visual features are mapped (and optionally pooled) to the LLM input space to get the visual tokens (64 in our standard configuration).""

""We use a learning rate of 10âˆ’4 and do around 2 epochs on our training data. It corresponds to approximately 1.5 billion images and 225 billion text tokens. We note that this is orders of magnitude more training data than other open VLMs.""

225*10^9+64*1.5*10^9 = 321000000000 = 321B tokens

+ they used similar sized dataset for fine-tuning",,,,,Confident,"The growing interest in vision-language models (VLMs) has been driven by improvements in large language models and vision transformers. Despite the abundance of literature on this subject, we observe that critical decisions regarding the design of VLMs are often not justified. We argue that these unsupported decisions impede progress in the field by making it difficult to identify which choices improve model performance. To address this issue, we conduct extensive experiments around pre-trained models, architecture choice, data, and training methods. Our consolidation of findings includes the development of Idefics2, an efficient foundational VLM of 8 billion parameters. Idefics2 achieves state-of-the-art performance within its size category across various multimodal benchmarks, and is often on par with models four times its size. We release the model (base, instructed, and chat) along with the datasets created for its training.",,,Open weights (unrestricted),"Multinational,United States of America,France","Mistral 7B,SigLIP 400M",1,6ND = 6*321*10^9*8*10^9 = 1.5408e+22,,,2025-05-01 10:42,,,,,,"Industry,Academia",,,,,,"apache 2.0
https://huggingface.co/HuggingFaceM4/idefics2-8b","Industry,Academia",,,,,Operation counting,HuggingFaceM4,,,
OpenELM-1.1B,Language,"Language modeling/generation,Code generation,Question answering",Apple,"Sachin Mehta, Mohammad Hossein Sekhavat, Qingqing Cao, Maxwell Horton, Yanzi Jin, Chenfan Sun, Iman Mirzadeh, Mahyar Najibi, Dmitry Belenko, Peter Zatloukal, Mohammad Rastegari",2024-05-02,OpenELM: An Efficient Language Model Family with Open Training and Inference Framework,https://arxiv.org/abs/2404.14619,,,,1080000000.0,1.08B (Table 4a),1.0520327e+22,"details from Table 4 and Table 9

6 FLOP / token / parameter * 1.08*10^9 parameters * 1.5*10^12 tokens = 9.72e+21 FLOP

312000000000000 FLOP / sec / GPU [bf16 assumed] * 128 GPUs * 11 days * 24 hours / day * 3600 sec / hour * 0.3 [assumed utilization] = 1.1386552e+22 FLOP

sqrt(9.72e+21*1.1386552e+22) = 1.0520327e+22","RefinedWeb,RedPajama,The Pile,Dolma","""OpenELM was pretrained on public datasets. Specifically, our pre-training dataset contains RefinedWeb, PILE, a subset of RedPajama, and a subset of Dolma v1.6."" (from github)",1500000000000.0,"1.5T (Table 4a)

Table 9:
Batch size (tokens) approx. 4M 
Training steps 350,000",240.0,11 days (Table 9) = 240 hours,NVIDIA A100 SXM4 80 GB,,Confident,"The reproducibility and transparency of large language models are crucial for advancing open research, ensuring the trustworthiness of results, and enabling investigations into data and model biases, as well as potential risks. To this end, we release OpenELM, a state-of-the-art open language model. OpenELM uses a layer-wise scaling strategy to efficiently allocate parameters within each layer of the transformer model, leading to enhanced accuracy. For example, with a parameter budget of approximately one billion parameters, OpenELM exhibits a 2.36% improvement in accuracy compared to OLMo while requiring 2Ã— fewer pre-training tokens.
Diverging from prior practices that only provide model weights and inference code, and pre-train on private datasets, our release includes the complete framework for training and evaluation of the language model on publicly available datasets, including training logs, multiple checkpoints, and pre-training configurations. We also release code to convert models to MLX library for inference and fine-tuning on Apple devices. This comprehensive release aims to empower and strengthen the open research community, paving the way for future open research endeavors.
Our source code along with pre-trained model weights and training recipes is available at \url{this https URL}. Additionally, \model models can be found on HuggingFace at: \url{this https URL}.",1.0,,Open weights (non-commercial),United States of America,,,,128.0,,2025-05-01 10:42,,,,4000000.0,Table 9,Industry,,,,,,"https://github.com/apple/corenet/tree/main/projects/openelm
https://huggingface.co/apple/OpenELM-1_1B
apple research license",Industry,,,,101178.0385953958,"Operation counting,Hardware",apple,,,
OpenELM-3B,Language,"Language modeling/generation,Code generation,Question answering",Apple,"Sachin Mehta, Mohammad Hossein Sekhavat, Qingqing Cao, Maxwell Horton, Yanzi Jin, Chenfan Sun, Iman Mirzadeh, Mahyar Najibi, Dmitry Belenko, Peter Zatloukal, Mohammad Rastegari",2024-05-02,OpenELM: An Efficient Language Model Family with Open Training and Inference Framework,https://arxiv.org/abs/2404.14619,,,,3040000000.0,3.04B (Table 4a),3.417119e+22,"details from Table 4 and Table 9

6 FLOP / token / parameter * 3.04*10^9 parameters * 1.5*10^12 tokens = 2.736e+22 FLOP

989500000000000 FLOP / sec / GPU [bf16 assumed] *  128 GPUs * 13 days * 24 hours / day * 3600 sec / hour * 0.3 [assumed utilization] = 4.2678006e+22 FLOP

sqrt(2.736e+22*4.2678006e+22) = 3.417119e+22","RefinedWeb,RedPajama,The Pile,Dolma","""OpenELM was pretrained on public datasets. Specifically, our pre-training dataset contains RefinedWeb, PILE, a subset of RedPajama, and a subset of Dolma v1.6."" (from github)",1500000000000.0,"1.5T (Table 4a)

Table 9:
Batch size (tokens) approx. 4M 
Training steps 350,000",288.0,13 days (Table 9) = 288 hours,NVIDIA H100 SXM5 80GB,,Confident,"The reproducibility and transparency of large language models are crucial for advancing open research, ensuring the trustworthiness of results, and enabling investigations into data and model biases, as well as potential risks. To this end, we release OpenELM, a state-of-the-art open language model. OpenELM uses a layer-wise scaling strategy to efficiently allocate parameters within each layer of the transformer model, leading to enhanced accuracy. For example, with a parameter budget of approximately one billion parameters, OpenELM exhibits a 2.36% improvement in accuracy compared to OLMo while requiring 2Ã— fewer pre-training tokens.
Diverging from prior practices that only provide model weights and inference code, and pre-train on private datasets, our release includes the complete framework for training and evaluation of the language model on publicly available datasets, including training logs, multiple checkpoints, and pre-training configurations. We also release code to convert models to MLX library for inference and fine-tuning on Apple devices. This comprehensive release aims to empower and strengthen the open research community, paving the way for future open research endeavors.
Our source code along with pre-trained model weights and training recipes is available at \url{this https URL}. Additionally, \model models can be found on HuggingFace at: \url{this https URL}.",1.0,,Open weights (non-commercial),United States of America,,,,128.0,,2025-05-01 10:42,,,,4000000.0,Table 9,Industry,,,,,,"https://github.com/apple/corenet/tree/main/projects/openelm
https://huggingface.co/apple/OpenELM-3B
apple research license",Industry,,,,177061.56754194264,"Operation counting,Hardware",apple,,,
OpenELM-450M,Language,"Language modeling/generation,Code generation,Question answering",Apple,"Sachin Mehta, Mohammad Hossein Sekhavat, Qingqing Cao, Maxwell Horton, Yanzi Jin, Chenfan Sun, Iman Mirzadeh, Mahyar Najibi, Dmitry Belenko, Peter Zatloukal, Mohammad Rastegari",2024-05-02,OpenELM: An Efficient Language Model Family with Open Training and Inference Framework,https://arxiv.org/abs/2404.14619,,,,450000000.0,0.45B (Table 4a),6.3156568e+21,"details from Table 4 and Table 9

6 FLOP / token / parameter * 0.45*10^9 parameters * 1.5*10^12 tokens = 4.05e+21 FLOP

989500000000000 FLOP / sec / GPU [bf16 assumed] * 128 GPUs * 3 days * 24 hours / day * 3600 sec / hour * 0.3 [assumed utilization] = 9.8487706e+21 FLOP

sqrt(4.05e+21*9.8487706e+21) = 6.3156568e+21","RefinedWeb,RedPajama,The Pile,Dolma","""OpenELM was pretrained on public datasets. Specifically, our pre-training dataset contains RefinedWeb, PILE, a subset of RedPajama, and a subset of Dolma v1.6."" (from github)",1500000000000.0,"1.5T (Table 4a)

Table 9:
Batch size (tokens) approx. 4M 
Training steps 350,000",72.0,3 days (Table 9) = 72 hours,NVIDIA H100 SXM5 80GB,,Confident,"The reproducibility and transparency of large language models are crucial for advancing open research, ensuring the trustworthiness of results, and enabling investigations into data and model biases, as well as potential risks. To this end, we release OpenELM, a state-of-the-art open language model. OpenELM uses a layer-wise scaling strategy to efficiently allocate parameters within each layer of the transformer model, leading to enhanced accuracy. For example, with a parameter budget of approximately one billion parameters, OpenELM exhibits a 2.36% improvement in accuracy compared to OLMo while requiring 2Ã— fewer pre-training tokens.
Diverging from prior practices that only provide model weights and inference code, and pre-train on private datasets, our release includes the complete framework for training and evaluation of the language model on publicly available datasets, including training logs, multiple checkpoints, and pre-training configurations. We also release code to convert models to MLX library for inference and fine-tuning on Apple devices. This comprehensive release aims to empower and strengthen the open research community, paving the way for future open research endeavors.
Our source code along with pre-trained model weights and training recipes is available at \url{this https URL}. Additionally, \model models can be found on HuggingFace at: \url{this https URL}.",1.0,,Open weights (non-commercial),United States of America,,,,128.0,,2025-05-01 10:42,,,,4000000.0,Table 9,Industry,,,,,,"https://github.com/apple/corenet/tree/main/projects/openelm
https://huggingface.co/apple/OpenELM-450M
apple research license",Industry,,,,177061.56754194264,"Operation counting,Hardware",apple,,,
OpenELM-270M,Language,"Language modeling/generation,Code generation,Question answering",Apple,"Sachin Mehta, Mohammad Hossein Sekhavat, Qingqing Cao, Maxwell Horton, Yanzi Jin, Chenfan Sun, Iman Mirzadeh, Mahyar Najibi, Dmitry Belenko, Peter Zatloukal, Mohammad Rastegari",2024-05-02,OpenELM: An Efficient Language Model Family with Open Training and Inference Framework,https://arxiv.org/abs/2404.14619,,,,270000000.0,0.27B (Table 4a),2.7470309e+21,"details from Table 4 and Table 9

6 FLOP / token / parameter * 0.27*10^9 parameters * 1.5*10^12 tokens = 2.43e+21 FLOP

312000000000000 FLOP / sec / GPU [bf16 assumed] * 128 GPUs * 3 days * 24 hours / day * 3600 sec / hour * 0.3 [assumed utilization]  = 3.1054234e+21 FLOP

sqrt(2.43e+21*3.1054234e+21) = 2.7470309e+21","RefinedWeb,RedPajama,The Pile,Dolma","""OpenELM was pretrained on public datasets. Specifically, our pre-training dataset contains RefinedWeb, PILE, a subset of RedPajama, and a subset of Dolma v1.6."" (from github)",1500000000000.0,"1.5T (Table 4a)

Table 9:
Batch size (tokens) approx. 4M 
Training steps 350,000",72.0,3 days (Table 9) = 72 hours,NVIDIA A100 SXM4 80 GB,,Confident,"The reproducibility and transparency of large language models are crucial for advancing open research, ensuring the trustworthiness of results, and enabling investigations into data and model biases, as well as potential risks. To this end, we release OpenELM, a state-of-the-art open language model. OpenELM uses a layer-wise scaling strategy to efficiently allocate parameters within each layer of the transformer model, leading to enhanced accuracy. For example, with a parameter budget of approximately one billion parameters, OpenELM exhibits a 2.36% improvement in accuracy compared to OLMo while requiring 2Ã— fewer pre-training tokens.
Diverging from prior practices that only provide model weights and inference code, and pre-train on private datasets, our release includes the complete framework for training and evaluation of the language model on publicly available datasets, including training logs, multiple checkpoints, and pre-training configurations. We also release code to convert models to MLX library for inference and fine-tuning on Apple devices. This comprehensive release aims to empower and strengthen the open research community, paving the way for future open research endeavors.
Our source code along with pre-trained model weights and training recipes is available at \url{this https URL}. Additionally, \model models can be found on HuggingFace at: \url{this https URL}.",1.0,,Open weights (non-commercial),United States of America,,,,128.0,,2025-05-01 10:42,,,,4000000.0,Table 9,Industry,,,,,,"https://github.com/apple/corenet/tree/main/projects/openelm
https://huggingface.co/apple/OpenELM-270M
apple research license",Industry,,,,101178.0385953958,"Operation counting,Hardware",apple,,,
Med-Gemini-M 1.5,"Medicine,Vision,Video,Language,Multimodal","Medical diagnosis,Question answering,Visual question answering,Mortality prediction,Video description,Text summarization","Google DeepMind,Google Research","Khaled Saab, Tao Tu, Wei-Hung Weng, Ryutaro Tanno, David Stutz, Ellery Wulczyn, Fan Zhang, Tim Strother, Chunjong Park, Elahe Vedadi, Juanma Zambrano Chaves, Szu-Yeu Hu, Mike Schaekermann, Aishwarya Kamath, Yong Cheng, David G.T. Barrett, Cathy Cheung, Basil Mustafa, Anil Palepu, Daniel McDuff, Le Hou, Tomer Golany, Luyang Liu, Jean-baptiste Alayrac, Neil Houlsby, Nenad Tomasev, Jan Freyberg, Charles Lau, Jonas Kemp, Jeremy Lai, Shekoofeh Azizi, Kimberly Kanada, SiWai Man, Kavita Kulkarni, Ruoxi Sun, Siamak Shakeri, Luheng He, Ben Caine, Albert Webson, Natasha Latysheva, Melvin Johnson, Philip Mansfield, Jian Lu, Ehud Rivlin, Jesper Anderson, Bradley Green, Renee Wong, Jonathan Krause, Jonathon Shlens, Ewa Dominowska, S. M. Ali Eslami, Katherine Chou, Claire Cui, Oriol Vinyals, Koray Kavukcuoglu, James Manyika, Jeff Dean, Demis Hassabis, Yossi Matias, Dale Webster, Joelle Barral, Greg Corrado, Christopher Semturs, S. Sara Mahdavi, Juraj Gottweis, Alan Karthikesalingam, Vivek Natarajan",2024-05-01,Capabilities of Gemini Models in Medicine,https://arxiv.org/abs/2404.18416,,,,,,,,"Slake-VQA,Path-VQA,ROCO (Radiology Objects in COntext),PAD-UFES-20,MIMIC-CXR","Med-Gemini-M 1.5 uses the following datasets for training:

Slake-VQA: Visual Question Answering (VQA) for radiology images, containing 9,849 samples.
Path-VQA: VQA for pathology images, with 19,755 samples.
ROCO (Radiology Objects in COntext): VQA tasks across multiple imaging modalities
PAD-UFES-20: Dermatology image classification dataset
MIMIC-CXR: Radiology dataset with chest X-rays and corresponding reports",,,,,Google TPU v4,,Unknown,"Excellence in a wide variety of medical applications poses considerable challenges for AI, requiring advanced reasoning, access to up-to-date medical knowledge and understanding of complex multimodal data. Gemini models, with strong general capabilities in multimodal and long-context reasoning, offer exciting possibilities in medicine. Building on these core strengths of Gemini, we introduce Med-Gemini, a family of highly capable multimodal models that are specialized in medicine with the ability to seamlessly use web search, and that can be efficiently tailored to novel modalities using custom encoders. We evaluate Med-Gemini on 14 medical benchmarks, establishing new state-of-the-art (SoTA) performance on 10 of them, and surpass the GPT-4 model family on every benchmark where a direct comparison is viable, often by a wide margin. On the popular MedQA (USMLE) benchmark, our best-performing Med-Gemini model achieves SoTA performance of 91.1% accuracy, using a novel uncertainty-guided search strategy. On 7 multimodal benchmarks including NEJM Image Challenges and MMMU (health & medicine), Med-Gemini improves over GPT-4V by an average relative margin of 44.5%. We demonstrate the effectiveness of Med-Gemini's long-context capabilities through SoTA performance on a needle-in-a-haystack retrieval task from long de-identified health records and medical video question answering, surpassing prior bespoke methods using only in-context learning. Finally, Med-Gemini's performance suggests real-world utility by surpassing human experts on tasks such as medical text summarization, alongside demonstrations of promising potential for multimodal medical dialogue, medical research and education. Taken together, our results offer compelling evidence for Med-Gemini's potential, although further rigorous evaluation will be crucial before real-world deployment in this safety-critical domain.",,,Unreleased,"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France,Multinational,United States of America,Canada,Switzerland",Gemini 1.5 Pro,,,,,2025-05-01 10:42,,,,,,"Industry,Industry",,,,,Unreleased,,"Industry,Industry",,,,,,,,,
GenCast,Earth science,Weather forecasting,Google DeepMind,"Ilan Price, Alvaro Sanchez-Gonzalez, Ferran Alet, Tom R. Andersson, Andrew El-Kadi, Dominic Masters, Timo Ewalds, Jacklynn Stott, Shakir Mohamed, Peter Battaglia, Remi Lam, Matthew Willson",2024-05-01,GenCast: Diffusion-based ensemble forecasting for medium-range weather,https://arxiv.org/abs/2312.15796,,,,,,8.169984e+20,"The model was trained for 120 hours on 32 TPUv5. 
Assuming it was TPUv5e, bf16 precision:

197000000000000 FLOP/s * 120 hours * 3600 s/hour * 32 instances * 0.3 [assumed utilization] = 8.169984e+20 FLOP",ERA5,"""We trained GenCast on a dataset built from the ECMWFâ€™s ERA5 archive (Hersbach et al., 2020), a large corpus of global reanalysis data.""",,"Stage 1:  2 million training steps. batch size 32 
Stage 2: 64 000 further training steps.  batch size 32 ",,"Stage 1: ""This training stage takes a little over 3.5 days using 32 TPUv5 instances.""
Stage 2: ""This takes just under 1.5 days using 32 TPUv5 instances.""
5*24 = 120 hours",Google TPU v5e,,Confident,"Weather forecasts are fundamentally uncertain, so predicting the range of probable weather scenarios is crucial for important decisions, from warning the public about hazardous weather, to planning renewable energy use. Here, we introduce GenCast, a probabilistic weather model with greater skill and speed than the top operational medium-range weather forecast in the world, the European Centre for Medium-Range Forecasts (ECMWF)'s ensemble forecast, ENS. Unlike traditional approaches, which are based on numerical weather prediction (NWP), GenCast is a machine learning weather prediction (MLWP) method, trained on decades of reanalysis data. GenCast generates an ensemble of stochastic 15-day global forecasts, at 12-hour steps and 0.25 degree latitude-longitude resolution, for over 80 surface and atmospheric variables, in 8 minutes. It has greater skill than ENS on 97.4% of 1320 targets we evaluated, and better predicts extreme weather, tropical cyclones, and wind power production. This work helps open the next chapter in operational weather forecasting, where critical weather-dependent decisions are made with greater accuracy and efficiency.",,,Open weights (non-commercial),"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France",,,,32.0,,2025-06-16 14:57,,,,,,Industry,,,,120.0,Open source,"Apache 2.0 for training and inference code
https://github.com/google-deepmind/graphcast

CC BY-NC-SA 4.0 for weights",Industry,,,,8853.275531781688,Hardware,,,,
Multi-Token Prediction 7B,Language,Code generation,Facebook AI Research,"Fabian Gloeckle, Badr Youbi Idrissi, Baptiste RoziÃ¨re, David Lopez-Paz, Gabriel Synnaeve",2024-04-30,Better & Faster Large Language Models via Multi-token Prediction,https://arxiv.org/abs/2404.19737,,,,6700000000.0,6.7B (â€œ7Bâ€),3.841092e+23,"""training all models reported in the paper required around 500K GPU hours of computation on hardware of type A100-80GB and H100.""
A100-80 GB peak FLOP/s [assumed fp16 precision]: 77970000000000
H100 peak FLOP/s [assumed SXM5 TensorCore]: 989000000000000
assuming 50/50 usage:

(77970000000000+989000000000000)*0.5*500000hours*3600s*0.3=2.880819e+23 for ALL models in the paper

assuming this model has taken around 40% of all used compute (https://docs.google.com/spreadsheets/d/1Yc-HAdYgn6e9SUIliMaQtvQscIsEZomLdXbWyYl_jfQ/edit?usp=sharing)

then it's assumed compute is 3.841092e+23 FLOPs","CodeContests,Unspecified unreleased",,250000000000.0,1T total tokens over 4 epochs (Table 1),,,"NVIDIA A100 SXM4 80 GB,NVIDIA H100 SXM5 80GB",,Likely,"Large language models such as GPT and Llama are trained with a next-token prediction loss. In this work, we suggest that training language models to predict multiple future tokens at once results in higher sample efficiency. More specifically, at each position in the training corpus, we ask the model to predict the following n tokens using n independent output heads, operating on top of a shared model trunk. Considering multi-token prediction as an auxiliary training task, we measure improved downstream capabilities with no overhead in training time for both code and natural language models. The method is increasingly useful for larger model sizes, and keeps its appeal when training for multiple epochs. Gains are especially pronounced on generative benchmarks like coding, where our models consistently outperform strong baselines by several percentage points. Our 13B parameter models solves 12 % more problems on HumanEval and 17 % more on MBPP than comparable next-token models. Experiments on small algorithmic tasks demonstrate that multi-token prediction is favorable for the development of induction heads and algorithmic reasoning capabilities. As an additional benefit, models trained with 4-token prediction are up to 3 times faster at inference, even with large batch sizes.",4.0,,Open weights (non-commercial),United States of America,,,,,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,"https://huggingface.co/facebook/multi-token-prediction

""weâ€™re releasing the pre-trained models for code completion under a non-commercial/research-only license.""",Industry,,,,,Hardware,,,,
Multi-Token Prediction 13B,Language,Code generation,Facebook AI Research,"Fabian Gloeckle, Badr Youbi Idrissi, Baptiste RoziÃ¨re, David Lopez-Paz, Gabriel Synnaeve",2024-04-30,Better & Faster Large Language Models via Multi-token Prediction,https://arxiv.org/abs/2404.19737,,,,13000000000.0,13B (Figure 1),1.5364368e+23,"""training all models reported in the paper required around 500K GPU hours of computation on hardware of type A100-80GB and H100.""
A100-80 GB peak FLOP/s [assumed fp16 precision]: 77970000000000
H100 peak FLOP/s [assumed SXM5 TensorCore]: 989000000000000
assuming 50/50 usage:

(77970000000000+989000000000000)*0.5*500000hours*3600s*0.3=2.880819e+23 for ALL models in the paper

assuming this model has taken around 16% of all used compute (https://docs.google.com/spreadsheets/d/1Yc-HAdYgn6e9SUIliMaQtvQscIsEZomLdXbWyYl_jfQ/edit?usp=sharing)

then it's assumed compute is 1.5364368e+23 FLOPs",,,209700000000.0, 209.7B (Table S13),,,"NVIDIA A100 SXM4 80 GB,NVIDIA H100 SXM5 80GB",,Likely,"Large language models such as GPT and Llama are trained with a next-token prediction loss. In this work, we suggest that training language models to predict multiple future tokens at once results in higher sample efficiency. More specifically, at each position in the training corpus, we ask the model to predict the following n tokens using n independent output heads, operating on top of a shared model trunk. Considering multi-token prediction as an auxiliary training task, we measure improved downstream capabilities with no overhead in training time for both code and natural language models. The method is increasingly useful for larger model sizes, and keeps its appeal when training for multiple epochs. Gains are especially pronounced on generative benchmarks like coding, where our models consistently outperform strong baselines by several percentage points. Our 13B parameter models solves 12 % more problems on HumanEval and 17 % more on MBPP than comparable next-token models. Experiments on small algorithmic tasks demonstrate that multi-token prediction is favorable for the development of induction heads and algorithmic reasoning capabilities. As an additional benefit, models trained with 4-token prediction are up to 3 times faster at inference, even with large batch sizes.",,,Unreleased,United States of America,,,,,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,,Industry,,,,,Hardware,,,,
DiffPepBuilder,Biology,Protein generation,Peking University,"Fanhao Wang, Yuzhe Wang, Laiyi Feng, Changsheng Zhang, Luhua Lai",2024-04-30,Target-Specific De Novo Peptide Binder Design with DiffPepBuilder,https://arxiv.org/abs/2405.00128,0.0,,,104000000.0,,7.667785728001e+21,"1. Hardware: 8x NVIDIA A800 80GB GPUs (7.80e13 FLOP/s per GPU)
2. Training duration: 5 days = 432,000 seconds (directly provided)
3. Utilization: 40%
4. Calculation: 
   7.80e13 FLOP/s/GPU Ã— 8 GPUs Ã— 432,000s Ã— 0.40 = 1.08e20 FLOP
   (7.80e13 Ã— 8 = 6.24e14 FLOP/s total â†’ Ã— 432,000s = 2.70e20 â†’ Ã— 0.40 = 1.08e20)

Base model: 7.560000000001e+21

Total: 7667785728001000000000",,,283001.0,"Step-by-step calculation:
Complexes = 14,897
Average length = (8 + 30)/2 = 19
Total datapoints = 14,897 Ã— 19 = 283,043 â‰ˆ 2.83Ã—10âµ",120.0,,NVIDIA A800 PCIe 40 GB,,Confident,"Despite the exciting progress in target-specific de novo protein binder design, peptide binder design remains challenging due to the flexibility of peptide structures and the scarcity of protein-peptide complex structure data. In this study, we curated a large synthetic dataset, referred to as PepPC-F, from the abundant protein-protein interface data and developed DiffPepBuilder, a de novo target-specific peptide binder generation method that utilizes an SE(3)-equivariant diffusion model trained on PepPC-F to co-design peptide sequences and structures. DiffPepBuilder also introduces disulfide bonds to stabilize the generated peptide structures. We tested DiffPepBuilder on 30 experimentally verified strong peptide binders with available protein-peptide complex structures. DiffPepBuilder was able to effectively recall the native structures and sequences of the peptide ligands and to generate novel peptide binders with improved binding free energy. We subsequently conducted de novo generation case studies on three targets. In both the regeneration test and case studies, DiffPepBuilder outperformed AfDesign and RFdiffusion coupled with ProteinMPNN, in terms of sequence and structure recall, interface quality, and structural diversity. Molecular dynamics simulations confirmed that the introduction of disulfide bonds enhanced the structural rigidity and binding performance of the generated peptides. As a general peptide binder de novo design tool, DiffPepBuilder can be used to design peptide binders for given protein targets with three dimensional and binding site information.",,,,China,ESM2-650M,,,8.0,,2025-05-01 10:42,,,,,,Academia,,,,,,,Academia,,,,3952.443165561074,Hardware,,,,
Amazon Q Developer,Language,"Code generation,Code autocompletion",Amazon,,2024-04-30,The most capable generative AIâ€“powered assistant for software development,https://aws.amazon.com/blogs/aws/amazon-q-developer-now-generally-available-includes-new-capabilities-to-reimagine-developer-experience/,,,,,,,,Unspecified unreleased,,,,,,,,Unknown,"Amazon Q Developer has agents that can generate real-time code suggestions based on your comments and existing code, bootstrap new projects from a single prompt (/dev), automate the process of upgrading and transforming legacy Java applications with the Amazon Q Developer transformation capability (/transform), generate customized code recommendations from your private repositories securely, and quickly understand what resources are running in your AWS account with a simple prompt.

Today [Dec 03, 2024], weâ€™re expanding Amazon Q Developer agent capabilities for: 1) enhanced documentation in codebases (/doc), 2) supporting code reviews to detect and resolve security and code quality issues (/review), and 3) generating unit tests automatically and improving test coverage (/test) across the software development lifecycle in your preferred IDE or GitLab Duo with Amazon Q (in preview), which is one of the most popular enterprise DevOps platforms",,,API access,United States of America,,,,,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
InternVL1.5,"Multimodal,Vision,Language","Visual question answering,Image captioning,Object detection,Character recognition,Language modeling/generation,Translation","Shanghai AI Lab,SenseTime,Tsinghua University,Nanjing University,Fudan University,Chinese University of Hong Kong (CUHK)","Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, Ji Ma, Jiaqi Wang, Xiaoyi Dong, Hang Yan, Hewei Guo, Conghui He, Botian Shi, Zhenjiang Jin, Chao Xu, Bin Wang, Xingjian Wei, Wei Li, Wenjian Zhang, Bo Zhang, Pinlong Cai, Licheng Wen, Xiangchao Yan, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, Wenhai Wang",2024-04-29,How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites,https://arxiv.org/abs/2404.16821,,,,25500000000.0,25.5B,,,"LAION,COYO-700M,GRIT,TextCaps",,,"""in our model,
a 448Ã—448 image is represented by 256 visual tokens.""",,,,,Confident,"In this report, we introduce InternVL 1.5, an open-source multimodal large language model (MLLM) to bridge the capability gap between open-source and proprietary commercial models in multimodal understanding. We introduce three simple improvements: (1) Strong Vision Encoder: we explored a continuous learning strategy for the large-scale vision foundation model -- InternViT-6B, boosting its visual understanding capabilities, and making it can be transferred and reused in different LLMs. (2) Dynamic High-Resolution: we divide images into tiles ranging from 1 to 40 of 448Ã—448 pixels according to the aspect ratio and resolution of the input images, which supports up to 4K resolution input. (3) High-Quality Bilingual Dataset: we carefully collected a high-quality bilingual dataset that covers common scenes, document images, and annotated them with English and Chinese question-answer pairs, significantly enhancing performance in OCR- and Chinese-related tasks. We evaluate InternVL 1.5 through a series of benchmarks and comparative studies. Compared to both open-source and proprietary models, InternVL 1.5 shows competitive performance, achieving state-of-the-art results in 8 of 18 benchmarks. Code has been released at this https URL.",,,Open weights (unrestricted),"China,Hong Kong,China,China,China,China,Hong Kong,China","InternViT-6B,InternLM2-20B",,,,,2025-02-07 20:50,,,,,,"Academia,Industry,Academia,Academia,Academia,Academia",,,,,,"MIT license

https://huggingface.co/collections/OpenGVLab/internvl15-6675ae031d45e5a07007f260","Academia,Industry,Academia,Academia,Academia,Academia",,,,,,OpenGVLab,,,
Swallow,Language,"Language modeling/generation,Chat,Translation",Tokyo Institute of Technology,"Kazuki Fujii, Taishi Nakamura, Mengsay Loem, Hiroki Iida, Masanari Ohi, Kakeru Hattori, Hirai Shota, Sakae Mizuki, Rio Yokota, Naoaki Okazaki",2024-04-27,Continual Pre-Training for Cross-Lingual LLM Adaptation: Enhancing Japanese Language Capabilities,https://arxiv.org/abs/2404.17790,,,,70000000.0,,,,"Wikipedia (ja),The Pile,RefinedWeb","The corpus used for continual pre-training includes the Swallow Corpus, which is explained in Appendix A, Japanese Wikipediaâ€¡, and for English, the RefinedWeb (Penedo et al., 2023) and The Pile (Gao et al., 2020). From these corpora, approximately 100B tokens were sampled for the training data of continual pre-training. The sampling was configured so that 5% of the English text comes from RefinedWeb, another 5% from English arXiv paper texts within The Pile, and the remaining 90% from Japanese texts. The Japanese text comprises about 1.6B tokens from Japanese Wikipedia, with the rest from the Swallow Corpus. The ratio of Japanese to English data in the training set was decided based on preliminary experiments (see Appendix B for details).",100000000000.0,100B,,,NVIDIA A100,,Confident,"Cross-lingual continual pre-training of large language models (LLMs) initially trained on English corpus allows us to leverage the vast amount of English language resources and reduce the pre-training cost. In this study, we constructed Swallow, an LLM with enhanced Japanese capability, by extending the vocabulary of Llama 2 to include Japanese characters and conducting continual pre-training on a large Japanese web corpus. Experimental results confirmed that the performance on Japanese tasks drastically improved through continual pre-training, and the performance monotonically increased with the amount of training data up to 100B tokens. Consequently, Swallow achieved superior performance compared to other LLMs that were trained from scratch in English and Japanese. An analysis of the effects of continual pre-training revealed that it was particularly effective for Japanese question answering tasks. Furthermore, to elucidate effective methodologies for cross-lingual continual pre-training from English to Japanese, we investigated the impact of vocabulary expansion and the effectiveness of incorporating parallel corpora. The results showed that the efficiency gained through vocabulary expansion had no negative impact on performance, except for the summarization task, and that the combined use of parallel corpora enhanced translation ability.",,,Open weights (restricted use),Japan,Llama 2-70B,5,"Mentioned directly in the paper: The training of Swallow 7b, 13b, and 70b required approximately 5.0 Ã— 1021 FLOPS, 9.4 Ã— 1021 FLOPS, 5.0 Ã— 1022 FLOPS of computation, respectively.

6ND = 6*70B*100B=4.2*10^22 FLOPs

We utilized the AI Bridging Cloud Infrastructure (ABCI) of the National Institute of Advanced Industrial Science and Technology, Japan for training. We employed mixed precision (bfloat16) and used multiple NVIDIA A100 nodes for distributed parallel training. Each node is equipped with eight NVIDIA A100 40GB GPUs, and the nodes are interconnected via InfiniBand HDR.",,0.506,2024-12-02 10:25,,,,,,Academia,,,,,,"https://huggingface.co/tokyotech-llm/Swallow-70b-instruct-hf
license: llama2",Academia,,Table 7,,,"Reported,Operation counting",,,,
Vidu,Video,"Video generation,Text-to-video","Tsinghua University,ShengShu","Fan Bao, Chendong Xiang, Gang Yue, Guande He, Hongzhou Zhu, Kaiwen Zheng, Min Zhao, Shilong Liu, Yaole Wang, Jun Zhu",2024-04-27,"æˆ‘å›½è‡ªç ”è§†é¢‘å¤§æ¨¡åž‹é¢å‘å…¨çƒä¸Šçº¿

Vidu: a Highly Consistent, Dynamic and Skilled Text-to-Video Generator with Diffusion Models","https://www.stdaily.com/index/kejixinwen/202408/927f046f4fb743f994726fb08c7e3cbf.shtml

https://arxiv.org/abs/2405.04233",,,,,,,,,,,,,,,,Unknown,"We introduce Vidu, a high-performance text-to-video generator that is capable of producing 1080p videos up to 16 seconds in a single generation. Vidu is a diffusion model with U-ViT as its backbone, which unlocks the scalability and the capability for handling long videos. Vidu exhibits strong coherence and dynamism, and is capable of generating both realistic and imaginative videos, as well as understanding some professional photography techniques, on par with Sora -- the most powerful reported text-to-video generator. Finally, we perform initial experiments on other controllable video generation, including canny-to-video generation, video prediction and subject-driven generation, which demonstrate promising results.",,,API access,"China,China",,,,,,2025-05-19 12:47,,,,,,"Academia,Industry",,,,,,,"Academia,Industry",,,,,,,,,
Qwen1.5-110B,Language,"Chat,Language modeling/generation,Quantitative reasoning,Code generation,Translation",Alibaba,Qwen Team,2024-04-25,Qwen1.5-110B: The First 100B+ Model of the Qwen1.5 Series,https://qwenlm.github.io/blog/qwen1.5-110b/?ref=upstract.com,,,,110000000000.0,"110B
",,lower bound is taken from Qwen1.5 72B training compute estimation,Unspecified unreleased,"We pretrained the models with a large amount of data, and we post-trained the models with both supervised finetuning and direct preference optimization.",,"A Qwen developer gave token counts for other models in the series at this github issue: https://github.com/QwenLM/Qwen2/issues/97
110B was asked but got no response.
7B, 14B, and 72B got 4T, 4T, and 3T tokens respectively.

In another issue from Qwen2: ""We are not authorized to share the details right now but the rough number is over 3T tokens for Qwen1.5 and over 7T tokens for Qwen2."" https://github.com/QwenLM/Qwen2/issues/562",,,,,Confident,"The Qwen1.5-110B is the largest model in the Qwen1.5 series, and it is also the first one with over 100 billion parameters in the series. It demonstrates competitive performance against the very recently released SOTA model Llama-3-70B and it is significantly better than the 72B model. This tells us that there is still a lot of room in model size scaling for better performance. While the releease of Llama-3 indicates the significance of data scaling to an extremely large scale, we believe we can get the best of both worlds by scaling both data and model size in our future release. Stay tuned for Qwen2!",,,Open weights (unrestricted),China,,,,,,2025-05-01 10:42,,,,,,Industry,,1.3e+24,,,Unreleased,https://huggingface.co/Qwen/Qwen1.5-110B,Industry,,,,,,Qwen,,,
Beyond ESM2: Graph-Enhanced Protein Sequence Modeling with Efficient,Biology,Protein or nucleotide language model (pLM/nLM),"Huazhong University of Science and Technology,Fudan University,Northwestern Polytechnical University","Shujian Jiao, Bingxuan Li, Lei Wang, Xiaojin Zhang, Wei Chen, Jiajie Peng, Zhongyu Wei",2024-04-24,Beyond ESM2: Graph-Enhanced Protein Sequence Modeling with Efficient Clustering,https://arxiv.org/abs/2404.15805,,,,35800000.0,,,,UniProtKB,,198405973.0,Table 2: 540601*367.01=198405973,,,,,Confident,"Proteins are essential to life's processes, underpinning evolution and diversity. Advances in sequencing technology have revealed millions of proteins, underscoring the need for sophisticated pre-trained protein models for biological analysis and AI development. Facebook's ESM2, the most advanced protein language model to date, leverages a masked prediction task for unsupervised learning, crafting amino acid representations with notable biochemical accuracy. Yet, it lacks in delivering functional protein insights, signaling an opportunity for enhancing representation this http URL study addresses this gap by incorporating protein family classification into ESM2's this http URL approach, augmented with Community Propagation-Based Clustering Algorithm, improves global protein representations, while a contextual prediction task fine-tunes local amino acid accuracy. Significantly, our model achieved state-of-the-art results in several downstream experiments, demonstrating the power of combining global and local methodologies to substantially boost protein representation quality.",,,,"China,China,China",ESM2-35M,,,,,2025-05-01 10:42,,,,,,"Academia,Academia,Academia",,,,,,,"Academia,Academia,Academia",,,,,,,,,
Arctic,Language,"Language modeling/generation,Question answering,Code generation,Quantitative reasoning",Snowflake,Snowflake AI Research,2024-04-24,"Snowflake Arctic: The Best LLM for Enterprise AI â€” Efficiently Intelligent, Truly Open",https://www.snowflake.com/en/blog/arctic-open-efficient-foundation-language-models-snowflake/,,,,480000000000.0,""" It combines a 10B dense transformer model with a residual 128x3.66B MoE MLP resulting in 480B total and 17B active parameters chosen using a top-2 gating.""",3.8347175e+23,"from the graph:
1x - Arctic
1.9X - Llama 3 8B (7.2Ã—10^23) ~ = Yi 34B (6.1e23) -> x = 3.2105263e+23
3X - Code Llama 70B (1.26e+24) -> x = 4.2e+23
17.5X - Llama 3 70B (7.861e24) -> x =4.492e+23

= 3.7975893e+23

Operation counting (17B active parameters): 
6ND = 6 FLOP / parameter / token * 17*10^9 parameters * 3.5*10^12 tokens = 3.57e+23 FLOP

geometric mean:(3.2105263e+23*4.492e+23*4.2e+23*3.57e+23)^(1/4) = 3.8347175e+23",,,3500000000000.0,"""Arctic was trained with a three-stage curriculum each with a different data composition focusing on generic skills in the first phase (1T Tokens), and enterprise-focused skills in the latter two phases (1.5T and 1T tokens). ""

1+1.5+1 = 3.5",,"""less than 3K GPU weeks""",,,Confident,"Built by Snowflake, Arctic is a family of enterprise-grade LLMs with leading performance in enterprise intelligence and breakthrough efficiency. Snowflake Arctic is a truly open, Apache 2.0 licensed model.",,,Open weights (unrestricted),United States of America,,,,,,2025-05-01 10:42,AWS,,,,,Industry,,,,504000.0,Open source,Apache 2.0 license with ungated access to weights and code paired with open data recipe and research insights.,Industry,$2000000.00,,,,"Other,Operation counting",,,,
phi-3-medium 14B,Language,"Chat,Language modeling/generation",Microsoft,"Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, SÃ©bastien Bubeck, Martin Cai, Caio CÃ©sar Teodoro Mendes, Weizhu Chen, Vishrav Chaudhary, Parul Chopra, Allie Del Giorno, Gustavo de Rosa, Matthew Dixon, Ronen Eldan, Dan Iter, Amit Garg, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J. Hewett, Jamie Huynh, Mojan Javaheripi, Xin Jin, Piero Kauffmann, Nikos Karampatziakis, Dongwoo Kim, Mahoud Khademi, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li, Chen Liang, Weishung Liu, Eric Lin, Zeqi Lin, Piyush Madan, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang Qin, Marko Radmilac, Corby Rosset, Sambudha Roy, Olatunji Ruwase, Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Xia Song, Masahiro Tanaka, Xin Wang, Rachel Ward, Guanhua Wang, Philipp Witte, Michael Wyatt, Can Xu, Jiahang Xu, Sonali Yadav, Fan Yang, Ziyi Yang, Donghan Yu, Chengruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, Xiren Zhou",2024-04-23,Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone,https://arxiv.org/abs/2404.14219,,,,14000000000.0,14B,4.032e+23,counting operations: 6Ã—4.8Ã—10^12 tokens Ã— 14Ã—10^9 parameters â‰ˆ 4.032Ã—10^23 FLOPS,Phi-3 Dataset,"we also trained phi-3-medium, a model with 14B parameters using the same tokenizer and architecture of phi-3-mini, and trained on the same data for slightly more epochs (4.8T tokens total as for phi-3-small)
Knowledge cutoff date is October 2023, according to https://console.cloud.google.com/vertex-ai/publishers/microsoft/model-garden/phi3?hl=ja&inv=1&invt=AbxQQg.",4800000000000.0,,,,,,Likely,"We introduce phi-3-mini, a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both academic benchmarks and internal testing, rivals that of models such as Mixtral 8x7B and GPT-3.5 (e.g., phi-3-mini achieves 69% on MMLU and 8.38 on MT-bench), despite being small enough to be deployed on a phone. The innovation lies entirely in our dataset for training, a scaled-up version of the one used for phi-2, composed of heavily filtered web data and synthetic data. The model is also further aligned for robustness, safety, and chat format. We also provide some initial parameter-scaling results with a 7B and 14B models trained for 4.8T tokens, called phi-3-small and phi-3-medium, both significantly more capable than phi-3-mini (e.g., respectively 75% and 78% on MMLU, and 8.7 and 8.9 on MT-bench).",,,Open weights (unrestricted),"United States of America,Multinational,India,Belgium",,,,,,2025-05-13 00:53,,,,,,Industry,,,,,Unreleased,"MIT license for weights:
https://huggingface.co/microsoft/Phi-3-medium-128k-instruct ",Industry,,,,,Operation counting,microsoft,,,
SenseChat 5.0,Language,"Chat,Language modeling/generation",SenseTime,SenseNova Team,2024-04-23,,https://zhidx.com/p/421866.html,,,,600000000000.0,"This article claims the model is a 600B MoE:
https://www.sensetime.com/cn/news-detail/51168158?categoryId=72",,,,,1670000000000.0,"Words per gygabyte for Mandarin Chinese: 167M

10000*167000000 = 1670000000000

""is trained based on more than 10TB of tokens, covers a large amount of synthetic data""

However, a later news article from SenseTime itself said: ""In terms of data, SenseChat V5 uses a new generation of data production pipelines to produce 10T tokens of high-quality training data."" (It also says ""10T tokens"" in the original untranslated version).",,,,,Likely,"SenseTime has newly upgraded its ""SenseNova 5.0"" large model system , and its comprehensive capabilities are fully comparable to GPT-4 Turbo .",,,Hosted access (no API),"Hong Kong,China",,,,,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
phi-3-small 7.4B,Language,"Chat,Language modeling/generation",Microsoft,"Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, SÃ©bastien Bubeck, Martin Cai, Caio CÃ©sar Teodoro Mendes, Weizhu Chen, Vishrav Chaudhary, Parul Chopra, Allie Del Giorno, Gustavo de Rosa, Matthew Dixon, Ronen Eldan, Dan Iter, Amit Garg, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J. Hewett, Jamie Huynh, Mojan Javaheripi, Xin Jin, Piero Kauffmann, Nikos Karampatziakis, Dongwoo Kim, Mahoud Khademi, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li, Chen Liang, Weishung Liu, Eric Lin, Zeqi Lin, Piyush Madan, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang Qin, Marko Radmilac, Corby Rosset, Sambudha Roy, Olatunji Ruwase, Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Xia Song, Masahiro Tanaka, Xin Wang, Rachel Ward, Guanhua Wang, Philipp Witte, Michael Wyatt, Can Xu, Jiahang Xu, Sonali Yadav, Fan Yang, Ziyi Yang, Donghan Yu, Chengruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, Xiren Zhou",2024-04-23,Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone,https://arxiv.org/abs/2404.14219,,,,7400000000.0,7.4B,2.1312000000000003e+23,6ND = 6 FLOP / parameter / token * 7.4B parameters * 4.8T tokens = 2.1312e+23 FLOP,,"""4.8T tokens total as for phi-3-small""
Knowledge cutoff date is October 2023, according to https://console.cloud.google.com/vertex-ai/publishers/microsoft/model-garden/phi3?hl=ja&inv=1&invt=AbxQQg.",4800000000000.0,,,,,,Confident,"We introduce phi-3-mini, a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both academic benchmarks and internal testing, rivals that of models such as Mixtral 8x7B and GPT-3.5 (e.g., phi-3-mini achieves 69% on MMLU and 8.38 on MT-bench), despite being small enough to be deployed on a phone. The innovation lies entirely in our dataset for training, a scaled-up version of the one used for phi-2, composed of heavily filtered web data and synthetic data. The model is also further aligned for robustness, safety, and chat format. We also provide some initial parameter-scaling results with a 7B and 14B models trained for 4.8T tokens, called phi-3-small and phi-3-medium, both significantly more capable than phi-3-mini (e.g., respectively 75% and 78% on MMLU, and 8.7 and 8.9 on MT-bench).",,,Open weights (unrestricted),"United States of America,Multinational,India,Belgium",,,,,,2025-06-10 17:14,,,,,,Industry,,,,,Unreleased,"MIT license

https://huggingface.co/microsoft/Phi-3-small-128k-instruct",Industry,,,,,Operation counting,microsoft,,,
phi-3.5-mini,Language,"Language modeling/generation,Question answering,Translation",Microsoft,"Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, SÃ©bastien Bubeck, Martin Cai, Caio CÃ©sar Teodoro Mendes, Weizhu Chen, Vishrav Chaudhary, Parul Chopra, Allie Del Giorno, Gustavo de Rosa, Matthew Dixon, Ronen Eldan, Dan Iter, Amit Garg, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J. Hewett, Jamie Huynh, Mojan Javaheripi, Xin Jin, Piero Kauffmann, Nikos Karampatziakis, Dongwoo Kim, Mahoud Khademi, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li, Chen Liang, Weishung Liu, Eric Lin, Zeqi Lin, Piyush Madan, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang Qin, Marko Radmilac, Corby Rosset, Sambudha Roy, Olatunji Ruwase, Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Xia Song, Masahiro Tanaka, Xin Wang, Rachel Ward, Guanhua Wang, Philipp Witte, Michael Wyatt, Can Xu, Jiahang Xu, Sonali Yadav, Fan Yang, Ziyi Yang, Donghan Yu, Chengruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, Xiren Zhou",2024-04-23,Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone,https://arxiv.org/abs/2404.14219,,,,3800000000.0,3.8B,3.7101154e+22,"6ND = 6*3800000000.00 parameters *3400000000000 tokens  = 7.752e+22

512 GPUs *133800000000000 FLOP/s *240 hours *3600 sec/hour *0.3 [assumed utilization] = 1.7756652e+22

geometric mean sqrt (7.752e+22*1.7756652e+22) = 3.7101154e+22",,"Status: This is a static model trained on an offline dataset with cutoff date October 2023 for publicly available data. Future versions of the tuned models may be released as we improve models.
Supported languages: Arabic, Chinese, Czech, Danish, Dutch, English, Finnish, French, German, Hebrew, Hungarian, Italian, Japanese, Korean, Norwegian, Polish, Portuguese, Russian, Spanish, Swedish, Thai, Turkish, Ukrainian",3400000000000.0,Training data: 3.4T tokens,240.0,"GPUs: 512 H100-80G
Training time: 10 days
https://huggingface.co/microsoft/Phi-3.5-mini-instruct",NVIDIA H100 SXM5 80GB,,Confident,"We introduce phi-3-mini, a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both academic benchmarks and internal testing, rivals that of models such as Mixtral 8x7B and GPT-3.5 (e.g., phi-3-mini achieves 69% on MMLU and 8.38 on MT-bench), despite being small enough to be deployed on a phone. The innovation lies entirely in our dataset for training, a scaled-up version of the one used for phi-2, composed of heavily filtered web data and synthetic data. The model is also further aligned for robustness, safety, and chat format. We also provide some initial parameter-scaling results with a 7B and 14B models trained for 4.8T tokens, called phi-3-small and phi-3-medium, both significantly more capable than phi-3-mini (e.g., respectively 75% and 78% on MMLU, and 8.7 and 8.9 on MT-bench).",,,Open weights (unrestricted),"United States of America,Multinational,India,Belgium",,,,512.0,,2025-05-13 00:53,,,,,,Industry,,,,,Unreleased,"https://huggingface.co/microsoft/Phi-3.5-mini-instruct

The model is licensed under the MIT license.",Industry,,,,708388.2341861207,"Hardware,Operation counting",microsoft,,,
phi-3.5-Vision,Vision,Visual question answering,Microsoft,"Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, SÃ©bastien Bubeck, Martin Cai, Caio CÃ©sar Teodoro Mendes, Weizhu Chen, Vishrav Chaudhary, Parul Chopra, Allie Del Giorno, Gustavo de Rosa, Matthew Dixon, Ronen Eldan, Dan Iter, Amit Garg, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J. Hewett, Jamie Huynh, Mojan Javaheripi, Xin Jin, Piero Kauffmann, Nikos Karampatziakis, Dongwoo Kim, Mahoud Khademi, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li, Chen Liang, Weishung Liu, Eric Lin, Zeqi Lin, Piyush Madan, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang Qin, Marko Radmilac, Corby Rosset, Sambudha Roy, Olatunji Ruwase, Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Xia Song, Masahiro Tanaka, Xin Wang, Rachel Ward, Guanhua Wang, Philipp Witte, Michael Wyatt, Can Xu, Jiahang Xu, Sonali Yadav, Fan Yang, Ziyi Yang, Donghan Yu, Chengruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, Xiren Zhou",2024-04-23,Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone,https://arxiv.org/abs/2404.14219,,,,4200000000.0,4.2B,8.784e+22,"Base model: 7.524e+22
Finetune: 1.260000e+22
Total: 87840000000000000000000",,"This was released together with the Phi-3 family and Phi-3.5 Mini and MOE, so I assume it has the same knowledge cutoff date of October 2023, which is given in https://console.cloud.google.com/vertex-ai/publishers/microsoft/model-garden/phi3?hl=ja&inv=1&invt=AbxQQg.",500000000000.0,Training data: 500B tokens (vision tokens + text tokens),144.0,"GPUs: 256 A100-80G
Training time: 6 days
https://huggingface.co/microsoft/Phi-3.5-vision-instruct",NVIDIA H100 SXM5 80GB,,Confident,"We introduce phi-3-mini, a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both academic benchmarks and internal testing, rivals that of models such as Mixtral 8x7B and GPT-3.5 (e.g., phi-3-mini achieves 69% on MMLU and 8.38 on MT-bench), despite being small enough to be deployed on a phone. The innovation lies entirely in our dataset for training, a scaled-up version of the one used for phi-2, composed of heavily filtered web data and synthetic data. The model is also further aligned for robustness, safety, and chat format. We also provide some initial parameter-scaling results with a 7B and 14B models trained for 4.8T tokens, called phi-3-small and phi-3-medium, both significantly more capable than phi-3-mini (e.g., respectively 75% and 78% on MMLU, and 8.7 and 8.9 on MT-bench).",,,Open weights (unrestricted),"United States of America,Multinational,India,Belgium",phi-3-mini 3.8B,8,"6ND = 6*4200000000.00 parameters *500000000000 tokens = 1.26e+22

256 GPUs *133800000000000 FLOP/s*144 hours *3600 sec/hour *0.3 [assumed utilization]= 5.3269955e+21

geometric mean sqrt(5.3269955e+21*1.26e+22) = 8.1926884e+21",256.0,,2025-05-30 14:27,,,,,,Industry,,,,,Unreleased,"https://huggingface.co/microsoft/Phi-3.5-vision-instruct

The model is licensed under the MIT license.",Industry,,,,354194.11709306034,"Hardware,Operation counting",microsoft,,,
Phi-3.5-MoE,Language,"Language modeling/generation,Translation,Question answering,Code generation,Quantitative reasoning",Microsoft,"Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, SÃ©bastien Bubeck, Martin Cai, Caio CÃ©sar Teodoro Mendes, Weizhu Chen, Vishrav Chaudhary, Parul Chopra, Allie Del Giorno, Gustavo de Rosa, Matthew Dixon, Ronen Eldan, Dan Iter, Amit Garg, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J. Hewett, Jamie Huynh, Mojan Javaheripi, Xin Jin, Piero Kauffmann, Nikos Karampatziakis, Dongwoo Kim, Mahoud Khademi, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li, Chen Liang, Weishung Liu, Eric Lin, Zeqi Lin, Piyush Madan, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang Qin, Marko Radmilac, Corby Rosset, Sambudha Roy, Olatunji Ruwase, Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Xia Song, Masahiro Tanaka, Xin Wang, Rachel Ward, Guanhua Wang, Philipp Witte, Michael Wyatt, Can Xu, Jiahang Xu, Sonali Yadav, Fan Yang, Ziyi Yang, Donghan Yu, Chengruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, Xiren Zhou",2024-04-23,Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone,https://arxiv.org/abs/2404.14219,,,,60800000000.0,"""Phi-3.5-MoE has 16x3.8B parameters with 6.6B active parameters when using 2 experts. The model is a mixture-of-expert decoder-only Transformer model using the tokenizer with vocabulary size of 32,064.""",3.0202896e+23,"512 GPUs * 989500000000000 FLOP / sec * 552 hours * 3600 sec / hour * 0.3 [assumed utilization] = 3.0202896e+23 FLOP

6 FLOP / token / parameter * 4900000000000 tokens * 6.6*10^9 active parameters = 1.9404e+23 FLOP (slightly less confidence than hardware estimation since the 6ND formula is less accurate for MoE)",Unspecified unreleased,"""Our training data includes a wide variety of sources, totaling 4.9 trillion tokens (including 10% multilingual), and is a combination of

publicly available documents filtered rigorously for quality, selected high-quality educational data, and code;
newly created synthetic, â€œtextbook-likeâ€ data for the purpose of teaching math, coding, common sense reasoning, general knowledge of the world (science, daily activities, theory of mind, etc.);
high quality chat format supervised data covering various topics to reflect human preferences on different aspects such as instruct-following, truthfulness, honesty and helpfulness.
We are focusing on the quality of data that could potentially improve the reasoning ability for the""
Knowledge cutoff date is October 2023, according to https://console.cloud.google.com/vertex-ai/publishers/microsoft/model-garden/phi3?hl=ja&inv=1&invt=AbxQQg.",4900000000000.0,Training data: 4.9T tokens,552.0,"GPUs: 512 H100-80G
Training time: 23 days

23 days * 24 hours / day = 552 hours",NVIDIA H100 SXM5 80GB,,Confident,"We introduce phi-3-mini, a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both academic benchmarks and internal testing, rivals that of models such as Mixtral 8x7B and GPT-3.5 (e.g., phi-3-mini achieves 69% on MMLU and 8.38 on MT-bench), despite being small enough to be deployed on a phone. Our training dataset is a scaled-up version of the one used for phi-2, composed of heavily filtered publicly available web data and synthetic data. The model is also further aligned for robustness, safety, and chat format. We also provide parameter-scaling results with a 7B, 14B models trained for 4.8T tokens, called phi-3-small, phi-3-medium, both significantly more capable than phi-3-mini (e.g., respectively 75%, 78% on MMLU, and 8.7, 8.9 on MT-bench). To enhance multilingual, multimodal, and long-context capabilities, we introduce three models in the phi-3.5 series: phi-3.5-mini, phi-3.5-MoE, and phi-3.5-Vision. The phi-3.5-MoE, a 16 x 3.8B MoE model with 6.6 billion active parameters, achieves superior performance in language reasoning, math, and code tasks compared to other open-source models of similar scale, such as Llama 3.1 and the Mixtral series, and on par with Gemini-1.5-Flash and GPT-4o-mini. Meanwhile, phi-3.5-Vision, a 4.2 billion parameter model derived from phi-3.5-mini, excels in reasoning tasks and is adept at handling both single-image and text prompts, as well as multi-image and text prompts.",,,Open weights (unrestricted),"United States of America,Multinational,India,Belgium",,,,512.0,,2025-05-16 10:30,,,,,,Industry,,,,,Unreleased,"https://huggingface.co/microsoft/Phi-3.5-MoE-instruct

MIT license (instruct model)",Industry,,,,708388.2341861207,"Hardware,Operation counting",microsoft,,,
VISTA-2D,"Biology,Vision",Cell segmentation,NVIDIA,"Vishwesh Nath, Andriy Myronenko, Harry Clifford",2024-04-22,Advancing Cell Segmentation and Morphology Analysis with NVIDIA AI Foundation Model VISTA-2D,https://developer.nvidia.com/blog/advancing-cell-segmentation-and-morphology-analysis-with-nvidia-ai-foundation-model-vista-2d/,,,,100000000.0,"""VISTA-2D has a network architecture with ~100 million training hyperparameters""
Presumably the quote is incorrect and they meant 100 million parameters.",,,,,,"""A total of ~15,000 annotated cell images were collected to train the generalist VISTA-2D model.""",,,,,Confident,"Model highlights
Robust deep learning algorithm based on transformers
Generalist model as compared to specialist models
Multiple dataset sources and file formats supported
Multiple modalities of imaging data collectively supported 
Multi-GPU and multinode training support",,,Hosted access (no API),United States of America,Segment Anything Model,,,,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
InstructPLM,Biology,"Protein generation,Protein folding prediction","Zhejiang Lab,Zhejiang University (ZJU),Nanjing University,Tsinghua University,Alibaba,Chinese University of Hong Kong (CUHK)","Jiezhong Qiu, Junde Xu, Jie Hu, Hanqun Cao, Liya Hou, Zijun Gao, Xinyi Zhou, Anni Li, Xiujuan Li, Bin Cui, Fei Yang, Shuang Peng, Ning Sun, Fangyu Wang, Aimin Pan, Jie Tang, Jieping Ye, Junyang Lin, Jin Tang, Xingxu Huang, Pheng Ann Heng, Guangyong Chen",2024-04-20,InstructPLM: Aligning Protein Language Models to Follow Protein Structure Instructions,https://www.biorxiv.org/content/10.1101/2024.04.17.589642v1.abstract,3.0,,,89100000.0,Table 2,,,CATH,,5400001.0,"InstructPLM Training Data Points:
- Training Proteins: 18,024
- Avg Sequence Length: 300
- Total Tokens = 18,024 Ã— 300 = 5,407,200 â‰ˆ 5.4 Ã— 10^6 data points",,,NVIDIA A100,,Confident,"Large language models are renowned for their efficacy in capturing intricate patterns, including co-evolutionary relationships, and underlying protein languages. However, current methodologies often fall short in illustrating the emergence of genomic insertions, duplications, and insertion/deletions (indels), which account for approximately 14% of human pathogenic mutations. Given that structure dictates function, mutated proteins with similar structures are more likely to persist throughout biological evolution. Motivated by this, we leverage crossmodality alignment and instruct fine-tuning techniques inspired by large language models to align a generative protein language model with protein structure instructions. Specifically, we present a method for generating variable-length and diverse proteins to explore and simulate the complex evolution of life, thereby expanding the repertoire of options for protein engineering. Our proposed protein LM-based approach, InstructPLM, demonstrates significant performance enhancements both in silico and in vitro. On native protein backbones, it achieves a perplexity of 2.68 and a sequence recovery rate of 57.51, surpassing Protein-MPNN by 39.2% and 25.1%, respectively. Furthermore, we validate the efficacy of our model by redesigning PETase and L-MDH. For PETase, all fifteen designed variable-length PETase exhibit depolymerization activity, with eleven surpassing the activity levels of the wild type. Regarding L-MDH, an enzyme lacking an experimentally determined structure, InstructPLM is able to design functional enzymes with an AF2-predicted structure. Code and model weights of InstructPLM are publicly available*.",200.0,,,"China,China,China,China,China,Hong Kong,China",,,,4.0,,2025-05-01 10:42,,,,,,"Academia,Academia,Academia,Industry,Academia",,,,,,,"Academia,Academia,Academia,Industry,Academia",,,,3162.658758253668,,,,,
SaProt,Biology,Protein or nucleotide language model (pLM/nLM),"Zhejiang University (ZJU),Westlake University","Jin Su, Chenchen Han, Yuyang Zhou, Junjie Shan, Xibin Zhou, Fajie Yuan",2024-04-19,SaProt: Protein Language Modeling with Structure-aware Vocabulary,https://www.biorxiv.org/content/10.1101/2023.10.01.560349v5.abstract,60.0,,,650000000.0,"""To our knowledge, SaProt stands out as the PLM currently trained with the largest number of protein structures, containing 650 million parameters"" Assume 40% utilization, FP16 tensor precision",6.21084672e+22,"""Its training lasted 3 months and utilized 64 NVIDIA 80G A100 GPUs,""
64*312000000000000*0.4*3months=6.210847e+22",,,12000000001.0,4.0 Ã— 10â· sequences Ã— 3.0 Ã— 10Â² tokens/sequence = 1.2 Ã— 10Â¹â° tokens,2160.0,"""Its training lasted 3 months """,NVIDIA A100,,Confident,"Large-scale protein language models (PLMs), such as the ESM family, have achieved remarkable performance in various downstream tasks related to protein structure and function by undergoing unsupervised training on residue sequences. They have become essential tools for researchers and practitioners in biology. However, a limitation of vanilla PLMs is their lack of explicit consideration for protein structure information, which suggests the potential for further improvement. Motivated by this, we introduce the concept of a â€œstructure-aware vocabularyâ€ that integrates residue tokens with structure tokens. The structure tokens are derived by encoding the 3D structure of proteins using Foldseek. We then propose SaProt, a large-scale general-purpose PLM trained on an extensive dataset comprising approximately 40 million protein sequences and structures. Through extensive evaluation, our SaProt model surpasses well-established and renowned baselines across 10 significant downstream tasks, demonstrating its exceptional capacity and broad applicability. We have made the code1, pre-trained model, and all relevant materials available at https://github.com/westlake-repl/SaProt.",,,,"China,China",,,,64.0,,2025-05-01 10:42,,,,,,"Academia,Academia",,,,,,,"Academia,Academia",,,,50603.66703135999,Hardware,,,,
Llama 3-70B,Language,"Chat,Language modeling/generation,Code generation",Meta AI,Aaditya Singh; Aaron Grattafiori; Abhimanyu Dubey; Abhinav Jauhri; Abhinav Pandey; Abhishek Kadian; Adam Kelsey; Adi Gangidi; Ahmad Al-Dahle; Amit Sangani; Ahuva Goldstand; Aiesha Letman; Ajay Menon; Akhil Mathur; Alan Schelten; Alex Vaughan; Amy Yang; Andrei Lupu; Andres Alvarado; Andrew Gallagher; Andrew Gu; Andrew Ho; Andrew Poulton; Andrew Ryan; Angela Fan; Ankit Ramchandani; Anthony Hartshorn; Archi Mitra; Archie Sravankumar; Artem Korenev; Arun Rao; Ashley Gabriel; Ashwin Bharambe; Assaf Eisenman; Aston Zhang; Ash JJhaveri; Aurelien Rodriguez; Austen Gregerson; Ava Spataru; Baptiste Roziere; Ben Maurer; Benjamin Leonhardi; Bernie Huang; Bhargavi Paranjape; Bing Liu; Binh Tang; Bobbie Chern; Brani Stojkovic; Brian Fuller; Catalina Mejia Arenas; Chao Zhou; Charlotte Caucheteux; Chaya Nayak; Ching-Hsiang Chu; Chloe Bi; Chris Cai; Chris Cox; Chris Marra; Chris McConnell; Christian Keller; Christoph Feichtenhofer; Christophe Touret; Chunyang Wu; Corinne Wong; Cristian Canton Ferrer; Damien Allonsius; Daniel Kreymer; Daniel Haziza; Daniel Li; Danielle Pintz; Danny Livshits; Danny Wyatt; David Adkins; David Esiobu; David Xu; Davide Testuggine; Delia David; Devi Parikh; Dhruv Choudhary; Dhruv Mahajan; Diana Liskovich; Diego Garcia-Olano; Diego Perino; Dieuwke Hupkes; Dingkang Wang; Dustin Holland; Egor Lakomkin; Elina Lobanova; Xiaoqing Ellen Tan; Emily Dinan; Eric Smith; Erik Brinkman; Esteban Arcaute; Filip Radenovic; Firat Ozgenel; Francesco Caggioni; Frank Seide; Frank Zhang; Gabriel Synnaeve; Gabriella Schwarz; Gabrielle Lee; Gada Badeer; Georgia Anderson; Graeme Nail; Gregoire Mialon; Guan Pang; Guillem Cucurell; Hailey Nguyen; Hamid Shojanazeri; Hannah Korevaar; Hannah Wang; Haroun Habeeb; Harrison Rudolph; Henry Aspegren; Hu Xu; Hugo Touvron; Iga Kozlowska; Igor Molybog; Igor Tufanov; Iliyan Zarov; Imanol Arrieta Ibarra; Irina-Elena Veliche; Isabel Kloumann; Ishan Misra; Ivan Evtimov; Jacob Xu; Jade Copet; Jake Weissman; Jan Geffert; Jana Vranes; Japhet Asher; Jason Park; Jay Mahadeokar; Jean-Baptiste Gaya; Jeet Shah; Jelmer van der Linde; Jennifer Chan; Jenny Hong; Jenya Lee; Jeremy Fu; Jeremy Teboul; Jianfeng Chi; Jianyu Huang; Jie Wang; Jiecao Yu; Joanna Bitton; Joe Spisak; Joelle Pineau; Jon Carvill; Jongsoo Park; Joseph Rocca; Joshua Johnstun; Junteng Jia; Kalyan Vasuden Alwala; Kam Hou U; Kate Plawiak; Kartikeya Upasani; Kaushik Veeraraghavan; Ke Li; Kenneth Heafield; Kevin Stone; Khalid El-Arini; Krithika Iyer; Kshitiz Malik; Kuenley Chiu; Kunal Bhalla; Kyle Huang; Lakshya Garg; Lauren Rantala-Yeary; Laurens van der Maaten; Lawrence Chen; Leandro Silva; Lee Bell; Lei Zhang; Liang Tan; Louis Martin; Lovish Madaan; Luca Wehrstedt; Lukas Blecher; Luke de Oliveira; Madeline Muzzi; Madian Khabsa; Manav Avlani; Mannat Singh; Manohar Paluri; Mark Zuckerberg; Marcin Kardas; Martynas Mankus; Mathew Oldham; Mathieu Rita; Matthew Lennie; Maya Pavlova; Meghan Keneally; Melanie Kambadur; Mihir Patel; Mikayel Samvelyan; Mike Clark; Mike Lewis; Min Si; Mitesh Kumar Singh; Mo Metanat; Mona Hassan; Naman Goyal; Narjes Torabi; Nicolas Usunier; Nikolay Bashlykov; Nikolay Bogoychev; Niladri Chatterji; Ning Dong; Oliver Aobo Yang; Olivier Duchenne; Onur Celebi; Parth Parekh; Patrick Alrassy; Paul Saab; Pavan Balaji; Pedro Rittner; Pengchuan Zhang; Pengwei Li; Petar Vasic; Peter Weng; Polina Zvyagina; Prajjwal Bhargava; Pratik Dubal; Praveen Krishnan; Punit Singh Koura; Qing He; Rachel Rodriguez; Ragavan Srinivasan; Rahul Mitra; Ramon Calderer; Raymond Li; Robert Stojnic; Roberta Raileanu; Robin Battey; Rocky Wang; Rohit Girdhar; Rohit Patel; Romain Sauvestre; Ronnie Polidoro; Roshan Sumbaly; Ross Taylor; Ruan Silva; Rui Hou; Rui Wang; Russ Howes; Ruty Rinott; Saghar Hosseini; Sai Jayesh Bondu; Samyak Datta; Sanjay Singh; Sara Chugh; Sargun Dhillon; Satadru Pan; Sean Bell; Sergey Edunov; Shaoliang Nie; Sharan Narang; Sharath Raparthy; Shaun Lindsay; Sheng Feng; Sheng Shen; Shenghao Lin; Shiva Shankar; Shruti Bhosale; Shun Zhang; Simon Vandenhende; Sinong Wang; Seohyun Sonia Kim; Soumya Batra; Sten Sootla; Steve Kehoe; Suchin Gururangan; Sumit Gupta; Sunny Virk; Sydney Borodinsky; Tamar Glaser; Tamar Herman; Tamara Best; Tara Fowler; Thomas Georgiou; Thomas Scialom; Tianhe Li; Todor Mihaylov; Tong Xiao; Ujjwal Karn; Vedanuj Goswami; Vibhor Gupta; Vignesh Ramanathan; Viktor Kerkez; Vinay Satish Kumar; Vincent Gonguet; Vish Vogeti; Vlad Poenaru; Vlad Tiberiu Mihailescu; Vladan Petrovic; Vladimir Ivanov; Wei Li; Weiwei Chu; Wenhan Xiong; Wenyin Fu; Wes Bouaziz; Whitney Meers; Will Constable; Xavier Martinet; Xiaojian Wu; Xinbo Gao; Xinfeng Xie; Xuchao Jia; Yaelle Goldschlag; Yann LeCun; Yashesh Gaur; Yasmine Babaei; Ye Qi; Yenda Li; Yi Wen; Yiwen Song; Youngjin Nam; Yuchen Hao; Yuchen Zhang; Yun Wang; Yuning Mao; Yuzi He; Zacharie Delpierre Coudert; Zachary DeVito; Zahra Hankir; Zhaoduo Wen; Zheng Yan; Zhengxing Chen; Zhenyu Yang; Zoe Papakipos,2024-04-18,Introducing Meta Llama 3: The most capable openly available LLM to date,https://ai.meta.com/blog/meta-llama-3/,,Significant use,"Will almost certainly be very influential and widely used in the open access AI industry, as with the previous Llama generations.",70000000000.0,,7.860999999999999e+24,"Arithmetic calculation:
6 * 15T tokens * 70B parameters = 6.3e24

GPU calculation:
https://huggingface.co/meta-llama/Meta-Llama-3-70B indicates training took 6.4M GPU-hours
We also know their larger scale training runs for 405B were getting between 0.38-0.41 MFU. Presumably the 70B model gets at least 0.43 utilization (405B has to be split across two nodes, while 70B should fit on one).
990 TFLOPS per GPU * 6.4 million GPU hours * 3600s * 0.43 = 9.808e24

Geometric mean: sqrt(6.3e24 * 9.808e24) = 7.861e24",Llama 3 dataset,"""Llama 3 is pretrained on over 15T tokens that were all collected from publicly available sources. Our training dataset is seven times larger than that used for Llama 2, and it includes four times more code. To prepare for upcoming multilingual use cases, over 5% of the Llama 3 pretraining dataset consists of high-quality non-English data that covers over 30 languages.""
Knowledge cutoff date is December 2023, according to https://huggingface.co/meta-llama/Meta-Llama-3-70B. ",15000000000000.0,,,,NVIDIA H100 SXM5 80GB,,Confident,,,,Open weights (restricted use),United States of America,,,,,,2025-05-29 10:37,,,,,,Industry,,,9.799999999999998e+24,6400000.0,Unreleased,"https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md

License A custom commercial license is available at: https://llama.meta.com/llama3/license",Industry,,"Although they describe utilization, this is implied to only be for the largest model, presumably the 405B.

We can impute utilization from comparison between arithmetic and hardware methods.

Arithmetic calculation:
6 * 15T tokens * 70B parameters = 6.3e24

GPU calculation:
990 TFLOPS per GPU * 6.4 million GPU hours * 3600s = 2.281e25 at full utilization

Implies 6.3e24 / 2.281e25 = 0.2762 utilization. 
This seems unlikely â€“ they report utilization during the training of 405B (which was big enough to need to be split across 2 nodes of 8 H100s) between 0.38-0.43.",BF16,,"Operation counting,Hardware",meta-llama,,,11
FRED-T5-XL,Language,"Chat,Language modeling/generation",Sber,"Dmitry Zmitrovich, Alexander Abramov, Andrey Kalmykov, Maria Tikhonova, Ekaterina Taktasheva, Danil Astafurov, Mark Baushenko, Artem Snegirev, Vitalii Kadulin, Sergey Markov, Tatiana Shavrina, Vladislav Mikhailov, Alena Fenogenova",2024-04-18,A Family of Pretrained Transformer Language Models for Russian,https://arxiv.org/abs/2309.10931,,,,1740000000.0,1.74B,2.5264222803000006e+22,"6*1500000000000*1740000000=1.566e+22
or
312000000000000 FLOPs/s * 112 GPUs * 45 days * 24 h * 3600 s * 0.3  = 4.076e22
geometric mean: 2.5e22","Wikipedia,Corus,C4,OpenSubtitles","In general, different domains and sizes of the subcorpora are included in the resulting pretraining corpora of our LMs, which range from 30GB (ruBERT) to 450GB (ruGPT-3).",1500000000000.0,"450GB *200M words per GB = 90000000000 words
In general, different domains and sizes of the subcorpora are included in the resulting pretraining corpora of our LMs, which range from 30GB (ruBERT) to 450GB (ruGPT-3).

""Collectively, the model saw about 1.5 trillion tokens."" from https://habr.com/ru/companies/sberdevices/articles/730088/",1920.0,"""The FRED-T5-large and FREDT5-XL models are pretrained with a total batch size of 2048 for 35 days on 160 V100 GPUs, followed by 5 days on 80 A100 GPUs, and for 45 days on 112 A100 GPUs, respectively.""

Total hardware-time: 264960 chip-hours
https://www.wolframalpha.com/input?i=%2835*160%2B5*80%2B45*112%29*24.","NVIDIA A100,NVIDIA Tesla V100 SXM2",,Confident,"Transformer language models (LMs) are fundamental to NLP research methodologies and applications in various languages. However, developing such models specifically for the Russian language has received little attention. This paper introduces a collection of 13 Russian Transformer LMs, which spans encoder (ruBERT, ruRoBERTa, ruELECTRA), decoder (ruGPT-3), and encoder-decoder (ruT5, FRED-T5) architectures. We provide a report on the model architecture design and pretraining, and the results of evaluating their generalization abilities on Russian language understanding and generation datasets and benchmarks. By pretraining and releasing these specialized Transformer LMs, we aim to broaden the scope of the NLP research directions and enable the development of industrial solutions for the Russian language.",,,Open weights (unrestricted),Russia,,,,160.0,,2025-04-30 11:35,,,,2048.0,,"Industry,Government",,,,264960.0,Unreleased,"https://huggingface.co/ai-forever/FRED-T5-1.7B
License:
apache-2.0
","Industry,Government",,,,,"Operation counting,Hardware",ai-forever,,,
Mixtral 8x22B,Language,"Language modeling/generation,Code generation,Translation,Quantitative reasoning,Question answering",Mistral AI,"Albert Jiang, Alexandre Sablayrolles, Alexis Tacnet, Antoine Roux, Arthur Mensch, Audrey Herblin-Stoop, Baptiste Bout, Baudouin de Monicault,Blanche Savary, Bam4d, Caroline Feldman, Devendra Singh Chaplot, Diego de las Casas, Eleonore Arcelin, Emma Bou Hanna, Etienne Metzger, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Harizo Rajaona, Jean-Malo Delignon, Jia Li, Justus Murke, Louis Martin, Louis Ternon, Lucile Saulnier, LÃ©lio Renard Lavaud, Margaret Jennings, Marie Pellat, Marie Torelli, Marie-Anne Lachaux, Nicolas Schuhl, Patrick von Platen, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Thibaut Lavril, TimothÃ©e Lacroix, ThÃ©ophile Gervet, Thomas Wang, Valera Nemychnikova, William El Sayed, William Marshall",2024-04-17,Mixtral 8x22B,https://mistral.ai/news/mixtral-8x22b/,,,Previous release (Mixtral 8x7B) seems to have significant usage. Appears to be SOTA in several benchmarks (MMLU) for open source models,141000000000.0,"141B params, 39B active: https://mistral.ai/news/mixtral-8x22b/ ",2.34e+24,"Assuming the model was trained on ~1-10 trillions of tokens (same OOM as the models from the comparison in Figure 1. Llama 2 was trained on 2T tokens) + Mistral Small 3 was trained on 8T of tokens, we can estimate training compute with ""speculative"" confidence:

6 FLOP / token / parameter * 39 * 10^9 active parameters * 10*10^12 tokens [speculatively] = 2.34e+24 FLOP",Unspecified unreleased,,,,,,,Self-supervised learning,Speculative,"Mixtral 8x22B is our latest open model. It sets a new standard for performance and efficiency within the AI community. It is a sparse Mixture-of-Experts (SMoE) model that uses only 39B active parameters out of 141B, offering unparalleled cost efficiency for its size.

Mixtral 8x22B comes with the following strengths:

- It is fluent in English, French, Italian, German, and Spanish
- It has strong mathematics and coding capabilities
- It is natively capable of function calling; along with the constrained output mode implemented on la Plateforme, this enables application development and tech stack modernisation at scale
- Its 64K tokens context window allows precise information recall from large documents
",,,Open weights (unrestricted),France,,,,,,2025-05-30 16:23,,,,,,Industry,,,,,Unreleased,Apache 2.0 license,Industry,,,,,Operation counting,mistralai,,,
SIMA,"Games,Video","Action recognition,Video description,Video",Google DeepMind,"SIMA Team, Maria Abi Raad, Arun Ahuja, Catarina Barros, Frederic Besse, Andrew Bolt, Adrian Bolton, Bethanie Brownfield, Gavin Buttimore, Max Cant, Sarah Chakera, Stephanie C. Y. Chan, Jeff Clune, Adrian Collister, Vikki Copeman, Alex Cullum, Ishita Dasgupta, Dario de Cesare, Julia Di Trapani, Yani Donchev, Emma Dunleavy, Martin Engelcke, Ryan Faulkner, Frankie Garcia, Charles Gbadamosi, Zhitao Gong, Lucy Gonzales, Kshitij Gupta, Karol Gregor, Arne Olav Hallingstad, Tim Harley, Sam Haves, Felix Hill, Ed Hirst, Drew A. Hudson, Jony Hudson, Steph Hughes-Fitt, Danilo J. Rezende, Mimi Jasarevic, Laura Kampis, Rosemary Ke, Thomas Keck, Junkyung Kim, Oscar Knagg, Kavya Kopparapu, Andrew Lampinen, Shane Legg, Alexander Lerchner, Marjorie Limont, Yulan Liu, Maria Loks-Thompson, Joseph Marino, Kathryn Martin Cussons, Loic Matthey, Siobhan Mcloughlin, Piermaria Mendolicchio, Hamza Merzic, Anna Mitenkova, Alexandre Moufarek, Valeria Oliveira, Yanko Oliveira, Hannah Openshaw, Renke Pan, Aneesh Pappu, Alex Platonov, Ollie Purkiss, David Reichert, John Reid, Pierre Harvey Richemond, Tyson Roberts, Giles Ruscoe, Jaume Sanchez Elias, Tasha Sandars, Daniel P. Sawyer, Tim Scholtes, Guy Simmons, Daniel Slater, Hubert Soyer, Heiko Strathmann, Peter Stys, Allison C. Tam, Denis Teplyashin, Tayfun Terzi, Davide Vercelli, Bojan Vujatovic, Marcus Wainwright, Jane X. Wang, Zhengdong Wang, Daan Wierstra, Duncan Williams, Nathaniel Wong, Sarah York, Nick Young",2024-04-17,Scaling Instructable Agents Across Many Simulated Worlds,https://arxiv.org/abs/2404.10179,,,,,,,,,,,,,,,,Unknown,"Building embodied AI systems that can follow arbitrary language instructions in any 3D environment is a key challenge for creating general AI. Accomplishing this goal requires learning to ground language in perception and embodied actions, in order to accomplish complex tasks. The Scalable, Instructable, Multiworld Agent (SIMA) project tackles this by training agents to follow free-form instructions across a diverse range of virtual 3D environments, including curated research environments as well as open-ended, commercial video games. Our goal is to develop an instructable agent that can accomplish anything a human can do in any simulated 3D environment. Our approach focuses on language-driven generality while imposing minimal assumptions. Our agents interact with environments in real-time using a generic, human-like interface: the inputs are image observations and language instructions and the outputs are keyboard-and-mouse actions. This general approach is challenging, but it allows agents to ground language across many visually complex and semantically rich environments while also allowing us to readily run agents in new environments. In this paper we describe our motivation and goal, the initial progress we have made, and promising preliminary results on several diverse research environments and a variety of commercial video games.",,,Unreleased,"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France",,,,,,2025-04-15 05:08,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
METL-Global,Biology,"Protein or nucleotide language model (pLM/nLM),Protein design","University of Wisconsin Madison,Morgridge Institute for Research","Sam Gelman, Bryce Johnson, Chase Freschlin, Sameer Dâ€™Costa, Anthony Gitter, Philip A. Romero",2024-04-17,Biophysics-based protein language models for protein engineering,https://www.biorxiv.org/content/10.1101/2024.03.15.585128v1.abstract,3.0,,,50000000.0,,,,,,6000000000.0,"New estimate, Global only:
30M * 200 residues = 6000000000

Combined data
50M variants (20M + 30M) Ã— 200 residues = 10 billion (1.0e10) tokens

20M from METL-Local
30M from METL-Global (148 proteins Ã— 200k variants)
Average sequence length: 200 residues

Final estimate: 1.0e10 tokens",,,,,Likely,"Protein language models trained on evolutionary data have emerged as powerful tools for predictive problems involving protein sequence, structure, and function. However, these models overlook decades of research into biophysical factors governing protein function. We propose Mutational Effect Transfer Learning (METL), a protein language model framework that unites advanced machine learning and biophysical modeling. Using the METL framework, we pretrain transformer-based neural networks on biophysical simulation data to capture fundamental relationships between protein sequence, structure, and energetics. We finetune METL on experimental sequence-function data to harness these biophysical signals and apply them when predicting protein properties like thermostability, catalytic activity, and fluorescence. METL excels in challenging protein engineering tasks like generalizing from small training sets and position extrapolation, although existing methods that train on evolutionary signals remain powerful for many types of experimental assays. We demonstrate METLâ€™s ability to design functional green fluorescent protein variants when trained on only 64 examples, showcasing the potential of biophysics-based protein language models for protein engineering.",30.0,,,"United States of America,United States of America",,,,4.0,,2025-05-01 10:42,,,,,,"Academia,Academia",,,,,,,"Academia,Academia",,,,,,,,,
GRITLM 7B,Language,"Language modeling/generation,Question answering,Quantitative reasoning","Contextual AI,The University of Hong Kong,Microsoft","Niklas Muennighoff, Hongjin Su, Liang Wang, Nan Yang, Furu Wei, Tao Yu, Amanpreet Singh, Douwe Kiela",2024-04-17,Generative Representational Instruction Tuning,https://arxiv.org/abs/2402.09906,,,,7240000000.0,7.24B,,,"E5,Tulu 2","""We finetune our final models from Mistral 7B [68] and Mixtral 8x7B [69] using adaptations of E5 [160] and the TÃ¼lu 2 data [64]. For E5, we adapt it by adding S2ORC [91] to increase its scientific data (â€œE5Sâ€), while for TÃ¼lu 2 we filter out their custom prompts that contain answers related to the origin of their model.""",,"""For GRITLM 7B, we use a batch size of 2048 for embedding data and 256 for generative data and we train the model for a total of 1253 steps corresponding to one epoch on the generative data and 1.36 epochs on the embedding data.""",48.0,"""For the training of GRITLM 7B, we used 8 nodes with 8 NVIDIA A100 80GB GPUs each for 48 hours corresponding to 3,072 GPU hours. """,NVIDIA A100 SXM4 80 GB,,Confident,"7B parameter model that uses bidirectional attention for embedding and causal attention for generation. It is finetuned from Mistral-7B

All text-based language problems can be reduced to either generation or embedding. Current models only perform well at one or the other. We introduce generative representational instruction tuning (GRIT) whereby a large language model is trained to handle both generative and embedding tasks by distinguishing between them through instructions. Compared to other open models, our resulting GritLM 7B sets a new state of the art on the Massive Text Embedding Benchmark (MTEB) and outperforms all models up to its size on a range of generative tasks. By scaling up further, GritLM 8x7B outperforms all open generative language models that we tried while still being among the best embedding models. Notably, we find that GRIT matches training on only generative or embedding data, thus we can unify both at no performance loss. Among other benefits, the unification via GRIT speeds up Retrieval-Augmented Generation (RAG) by > 60% for long documents, by no longer requiring separate retrieval and generation models. Models, code, etc. are freely available at this https URL.",,,Open weights (unrestricted),"United States of America,Hong Kong,China,United States of America,Multinational,India,Belgium",Mistral 7B,1,312000000000000 FLOP / GPU / hour * 3072 GLU-hours * 3600 sec / hour * 0.3 [assumed utilization] = 1.0351411e+21 FLOP,8.0,,2025-05-27 10:34,,,,,,"Industry,Academia,Industry",,,,3072.0,Open source,"MIT License

https://github.com/ContextualAI/gritlm","Industry,Academia,Industry",,,BF16,6325.740113156251,Hardware,,,,
GRITLM 8x7B,Language,"Language modeling/generation,Question answering,Quantitative reasoning","Contextual AI,The University of Hong Kong,Microsoft","Niklas Muennighoff, Hongjin Su, Liang Wang, Nan Yang, Furu Wei, Tao Yu, Amanpreet Singh, Douwe Kiela",2024-04-17,Generative Representational Instruction Tuning,https://arxiv.org/abs/2402.09906,,,,46700000000.0,46.7B,,,"E5,Tulu 2","""We finetune our final models from Mistral 7B [68] and Mixtral 8x7B [69] using adaptations of E5 [160] and the TÃ¼lu 2 data [64]. For E5, we adapt it by adding S2ORC [91] to increase its scientific data (â€œE5Sâ€), while for TÃ¼lu 2 we filter out their custom prompts that contain answers related to the origin of their model.""",,"""For GRITLM 8X7B, the embedding batch size is 256 due to compute limitations. """,80.0,"""for GRITLM 8X7B, we used 32 nodes with 8 NVIDIA H100 80GB GPUs each for 80 hours corresponding to 20,480 GPU hours.""",NVIDIA H100 SXM5 80GB,,Confident,"8x7B parameter model that uses bidirectional attention for embedding and causal attention for generation. It is finetuned from Mistral-8x7B

All text-based language problems can be reduced to either generation or embedding. Current models only perform well at one or the other. We introduce generative representational instruction tuning (GRIT) whereby a large language model is trained to handle both generative and embedding tasks by distinguishing between them through instructions. Compared to other open models, our resulting GritLM 7B sets a new state of the art on the Massive Text Embedding Benchmark (MTEB) and outperforms all models up to its size on a range of generative tasks. By scaling up further, GritLM 8x7B outperforms all open generative language models that we tried while still being among the best embedding models. Notably, we find that GRIT matches training on only generative or embedding data, thus we can unify both at no performance loss. Among other benefits, the unification via GRIT speeds up Retrieval-Augmented Generation (RAG) by > 60% for long documents, by no longer requiring separate retrieval and generation models. Models, code, etc. are freely available at this https URL.",,,Open weights (unrestricted),"United States of America,Hong Kong,China,United States of America,Multinational,India,Belgium",Mixtral 8x7B,2,989500000000000 FLOP / GPU / sec * 20480 GPU-hours * 3600 sec / hour * 0.3 [assumed utilization] = 2.1886157e+22 FLOP,8.0,,2025-05-27 10:33,,,,,,"Industry,Academia,Industry",,,,20480.0,Open source,"MIT License

https://github.com/ContextualAI/gritlm","Industry,Academia,Industry",,,BF16,11070.04519802344,Hardware,,,,
abab6.5,Language,Language modeling/generation,MiniMax,,2024-04-17,The General Large Language Model abab6.5 Series,https://www.minimaxi.com/en/news/abab65-series,,,,1000000000000.0,,,,Unspecified unreleased,,,,,,,,Confident,"The abab6.5 series includes two models: abab6.5 and abab6.5s. Abab6.5 features a trillion parameters and supports a context length of 200k tokens; abab6.5s uses the same training techniques and data as abab6.5 but is more efficient, supporting a 200k token context length and capable of processing nearly 30,000 words within a second. With significant cost advantages, an industry-leading context length and fast speed, MiniMax's abab6.5 series of LLMs offer unique value propositions.",,,API access,China,,,,,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
Reka Core,"Multimodal,Language,Vision","Chat,Language modeling/generation,Image captioning,Code generation,Code autocompletion",Reka AI,"Aitor Ormazabal Che Zheng Cyprien de Masson dâ€™Autume Dani Yogatama
Deyu Fu Donovan Ong Eric Chen Eugenie Lamprecht Hai Pham Isaac Ong
Kaloyan Aleksiev Lei Li Matthew Henderson Max Bain Mikel Artetxe
Nishant Relan Piotr Padlewski Qi Liu Ren Chen Samuel Phua
Yazheng Yang Yi Tay Yuqi Wang Zhongkai Zhu Zhihui Xie",2024-04-15,"Reka Core, Flash, and Edge: A Series of Powerful
Multimodal Language Models",https://publications.reka.ai/reka-core-tech-report.pdf,,Training cost,,67000000000.0,,8.400010000000001e+24,"No direct information about Reka Core model (""Reka Core has not finished training and is still improving."")

The smaller dense model Reka Flash has 21B parameters and was trained on 5 trillion language tokens.

There is information about compute: ""Our setup comprises of clusters from a mixture of vendors with our peak compute being approximately
2.5K H100s and 2.5K A100s.""

If we assume 2 months of training with 2.5k H100s and 2.5k A100s at utilization 0.5 we get 8.4e24 FLOP (2500*9.9e14+2500*3.12e14)*60*60*24*60*0.5.","Wikipedia,Unspecified unreleased","The training data comprises a mixture of publicly available and proprietary/licensed datasets with a dataset knowledge cutoff of November 2023. The dataset ingested by our model comprises of text, images, videos, and audio clips. Reka Flash and Reka Edge were trained on approximately 5 trillion and 4.5 trillion extensively deduplicated and filtered language tokens, respectively. While the classification of corpora is not strictly defined to one class or category, approximately 25% of our pretraining data is code related, and 30% are STEM related. Approximately 25% of the data is web crawl. About 10% of our data has some relation to math.",,,,,"NVIDIA A100,NVIDIA H100 SXM5 80GB",,Speculative,,,,API access,United States of America,,,,,,2025-05-28 16:02,,,,,,Industry,,6.3e+23,2.6563132345e+26,,Unreleased,,Industry,,,BF16,,Hardware,,,,10
WizardLM-2 8x22B,Language,"Language modeling/generation,Question answering",Microsoft,,2024-04-15,,https://wizardlm.github.io/WizardLM2/,,,,141000000000.0,Parameters: 141B,,,,,,,,,,,Confident,"We introduce and opensource WizardLM-2, our next generation state-of-the-art large language models, which have improved performance on complex chat, multilingual, reasoning and agent. New family includes three cutting-edge models: WizardLM-2 8x22B, WizardLM-2 70B, and WizardLM-2 7B.

WizardLM-2 is the latest milestone in our effort in scaling up LLM post-training. One year ago, we have been iterating on training of Wizard series since our first work on Empowering Large Language Models to Follow Complex Instructions, then we accelerate the evolution to code and math reasoning scenarios. Since then, Evol-Instruct and Instruction&Process Supervised Reinforcement Learning (RLEIF) have become fundamental technologies for GenAI community. Recently, we have further optimized our methods and data quality, resulting in significant performance improvements, the outcome is WizardLM-2.

WizardLM-2 8x22B is our most advanced model, and the best opensource LLM in our internal evaluation on highly complex tasks. WizardLM-2 70B reaches top-tier reasoning capabilities and is the first choice in the same size. WizardLM-2 7B is the fastest and achieves comparable performance with existing 10x larger opensource leading models.
",,,Open weights (unrestricted),"United States of America,Multinational,India,Belgium",Mixtral 8x22B,,,,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,"The License of WizardLM-2 8x22B and WizardLM-2 7B is Apache2.0.

https://huggingface.co/alpindale/WizardLM-2-8x22B",Industry,,,,,,alpindale,,,
WizardLM-2 70B,Language,"Language modeling/generation,Question answering",Microsoft,,2024-04-15,,https://wizardlm.github.io/WizardLM2/,,,,70000000000.0,70b,,,,,,,,,,,Confident,"We introduce and opensource WizardLM-2, our next generation state-of-the-art large language models, which have improved performance on complex chat, multilingual, reasoning and agent. New family includes three cutting-edge models: WizardLM-2 8x22B, WizardLM-2 70B, and WizardLM-2 7B.

WizardLM-2 is the latest milestone in our effort in scaling up LLM post-training. One year ago, we have been iterating on training of Wizard series since our first work on Empowering Large Language Models to Follow Complex Instructions, then we accelerate the evolution to code and math reasoning scenarios. Since then, Evol-Instruct and Instruction&Process Supervised Reinforcement Learning (RLEIF) have become fundamental technologies for GenAI community. Recently, we have further optimized our methods and data quality, resulting in significant performance improvements, the outcome is WizardLM-2.

WizardLM-2 8x22B is our most advanced model, and the best opensource LLM in our internal evaluation on highly complex tasks. WizardLM-2 70B reaches top-tier reasoning capabilities and is the first choice in the same size. WizardLM-2 7B is the fastest and achieves comparable performance with existing 10x larger opensource leading models.
",,,Unreleased,"United States of America,Multinational,India,Belgium",Llama 2-70B,,,,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,"The License of WizardLM-2 70B is Llama-2-Community.

I haven't found a repository though",Industry,,,,,,,,,
WizardLM-2 7B,Language,"Language modeling/generation,Question answering",Microsoft,,2024-04-15,,https://wizardlm.github.io/WizardLM2/,,,,7000000000.0,7b,,,,,,,,,,,Confident,"We introduce and opensource WizardLM-2, our next generation state-of-the-art large language models, which have improved performance on complex chat, multilingual, reasoning and agent. New family includes three cutting-edge models: WizardLM-2 8x22B, WizardLM-2 70B, and WizardLM-2 7B.

WizardLM-2 is the latest milestone in our effort in scaling up LLM post-training. One year ago, we have been iterating on training of Wizard series since our first work on Empowering Large Language Models to Follow Complex Instructions, then we accelerate the evolution to code and math reasoning scenarios. Since then, Evol-Instruct and Instruction&Process Supervised Reinforcement Learning (RLEIF) have become fundamental technologies for GenAI community. Recently, we have further optimized our methods and data quality, resulting in significant performance improvements, the outcome is WizardLM-2.

WizardLM-2 8x22B is our most advanced model, and the best opensource LLM in our internal evaluation on highly complex tasks. WizardLM-2 70B reaches top-tier reasoning capabilities and is the first choice in the same size. WizardLM-2 7B is the fastest and achieves comparable performance with existing 10x larger opensource leading models.
",,,Open weights (unrestricted),"United States of America,Multinational,India,Belgium",Mistral 7B,,,,,2025-05-12 15:41,,,,,,Industry,,,,,Unreleased,"The License of WizardLM-2 8x22B and WizardLM-2 7B is Apache2.0.

https://huggingface.co/dreamgen/WizardLM-2-7B",Industry,,,,,,dreamgen,,,
DDPM,Biology,Gene expression profile generation,"University Paris-Saclay,Radboud University Medical Center","Alice Lacan, Romain AndrÃ©, Michele Sebag, Blaise Hanczar",2024-04-13,In Silico Generation of Gene Expression profiles using Diffusion Models,https://www.biorxiv.org/content/10.1101/2024.04.10.588825v1.abstract,0.0,,,,,9.000000000000006e+17,"1. Hardware setup: 1x NVIDIA A40 GPU (1.50Ã—10Â¹â´ FLOP/s per GPU)

2. Training duration: 15,000 seconds (directly provided - sum of TCGA training: 3,780s and GTEx training: 11,220s)

3. Utilization rate: 40%

4. Calculation: 1.50Ã—10Â¹â´ FLOP/s Ã— 1 GPU Ã— 15,000s Ã— 0.4 = 9.0Ã—10Â¹â· FLOPs",,,16000001.0,"TCGA: 6,499 Ã— 978 = 6,356,022
GTEx: 9,796 Ã— 974 = 9,541,304
Total: 6,356,022 + 9,541,304 = 15,897,326 â‰ˆ 1.6e7 datapoints",,,NVIDIA A40 PCIe,,Confident,"Motivation RNA-seq data is used for precision medicine (e.g., cancer predictions), which benefits from deep learning approaches to analyze complex gene expression data. However, transcriptomics datasets often have few samples compared to deep learning standards. Synthetic data generation is thus being explored to address this data scarcity. So far, only deep generative models such as Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs) have been used for this aim. Considering the recent success of diffusion models (DM) in image generation, we propose the first generation pipeline that leverages the power of said diffusion models.

Results This paper presents two state-of-the-art diffusion models (DDPM and DDIM) and achieves their adaptation in the transcriptomics field. DM-generated data of L1000 landmark genes show better predictive performance over TCGA and GTEx datasets. We also compare linear and nonlinear reconstruction methods to recover the complete transcriptome. Results show that such reconstruction methods can boost the performances of diffusion models, as well as VAEs and GANs. Overall, the extensive comparison of various generative models using data quality indicators shows that diffusion models perform best and second-best, making them promising synthetic transcriptomics generators.",,,,"France,Netherlands",,,,1.0,,2025-05-01 10:42,,,,,,"Academia,Academia",,,,,,,"Academia,Academia",,,,325.87415621262454,Hardware,,,,
DDIM,Biology,Gene expression profile generation,"University Paris-Saclay,Radboud University Medical Center","Alice Lacan, Romain AndrÃ©, Michele Sebag, Blaise Hanczar",2024-04-13,In Silico Generation of Gene Expression profiles using Diffusion Models,https://www.biorxiv.org/content/10.1101/2024.04.10.588825v1.abstract,0.0,,,,,9.000000000000006e+17,"1. Hardware setup: 1x NVIDIA A40 GPU (1.50Ã—10Â¹â´ FLOP/s per GPU)

2. Training duration: 15,000 seconds (directly provided - sum of TCGA training: 3,780s and GTEx training: 11,220s)

3. Utilization rate: 40%

4. Calculation: 1.50Ã—10Â¹â´ FLOP/s Ã— 1 GPU Ã— 15,000s Ã— 0.4 = 9.0Ã—10Â¹â· FLOPs",,,16000001.0,"TCGA: 6,499 Ã— 978 = 6,356,022
GTEx: 9,796 Ã— 974 = 9,541,304
Total: 6,356,022 + 9,541,304 = 15,897,326 â‰ˆ 1.6e7 datapoints",,,,,Confident,"Motivation: RNA-seq data is used for precision medicine (e.g., cancer predictions), which benefits from deep learning approaches to analyze complex gene expression data. However, transcriptomics datasets often have few samples compared to deep learning standards. Synthetic data generation is thus being explored to address this data scarcity. So far, only deep generative models such as Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs) have been used for this aim. Considering the recent success of diffusion models (DM) in image generation, we propose the first generation pipeline that leverages the power of said diffusion models. Results: This paper presents two state-of-the-art diffusion models (DDPM and DDIM) and achieves their adaptation in the transcriptomics field. DM-generated data of L1000 landmark genes show better predictive performance over TCGA and GTEx datasets. We also compare linear and nonlinear reconstruction methods to recover the complete transcriptome. Results show that such reconstruction methods can boost the performances of diffusion models, as well as VAEs and GANs. Overall, the extensive comparison of various generative models using data quality indicators shows that diffusion models perform best and second-best, making them promising synthetic transcriptomics generators.",,,,"France,Netherlands",,,,,,2025-05-01 10:42,,,,,,"Academia,Academia",,,,,,,"Academia,Academia",,,,,Hardware,,,,
tsuzumi 7B upgrade 2024,"Language,Multimodal,Vision","Chat,Language modeling/generation,Document classification",NTT Communication Science Laboratories,Kyosuke Nishida,2024-04-11,"NTT's Large Language Model ""tsuzumi"" is Here!",https://ntt-research.com/2024-upgrade-reality-tsuzumi/,,,,7000000000.0,7B,4.2e+22,"6ND = 6 * 7000000000 * 1000000000000 = 4.2e+22
(unknown number of epochs -> 'likely' confidence)",,,1000000000000.0,"1T+ from https://ntt-research.com/2024-upgrade-reality-tsuzumi/
5:55",,,,,Likely,"tsuzumi is a large-scale language model created by NTT Laboratories. The name is inspired by the traditional Japanese drum â€œé¼“â€ and the model reflects the instrumentâ€™s compact and efficient design. Our vision for the future involves tackling societal challenges through the collaborative intelligence of a network of smaller, specialized LLMs like tsuzumi. 

In this presentation, Kyosuke Nishida, Senior Distinguished Researcher in the NTT Human Informatics Laboratories, demonstrates the tsuzumi-7B model, which was developed from scratch and features 7 billion parameters and over one trillion Japanese and English tokens. A vision-and-language model using tsuzumi for visual document understanding is also showcased.",,,,Japan,,,,,,2025-02-14 14:23,,,,,,Industry,,,,,,,Industry,,,,,Operation counting,,,,
HGRN2 3B,Language,"Language modeling/generation,Question answering","Shanghai AI Lab,Massachusetts Institute of Technology (MIT),Taptap","Zhen Qin, Songlin Yang, Weixuan Sun, Xuyang Shen, Dong Li, Weigao Sun, Yiran Zhong",2024-04-11,HGRN2: Gated Linear RNNs with State Expansion,https://arxiv.org/abs/2404.07904v1,,,,2900000000.0,2.9B,1.74e+21,6ND = 6* 2.9*10^9*100*10^9 = 1.74e+21,"The Pile,C4,Wikipedia",,100000000000.0,100B Table 6,,,,,Confident,"Hierarchically gated linear RNN (HGRN,Qin et al. 2023) has demonstrated competitive training speed and performance in language modeling, while offering efficient inference. However, the recurrent state size of HGRN remains relatively small, which limits its this http URL address this issue, inspired by linear attention, we introduce a simple outer-product-based state expansion mechanism so that the recurrent state size can be significantly enlarged without introducing any additional parameters. The linear attention form also allows for hardware-efficient this http URL extensive experiments verify the advantage of HGRN2 over HGRN1 in language modeling, image classification, and Long Range this http URL largest 3B HGRN2 model slightly outperforms Mamba and LLaMa Architecture Transformer for language modeling in a controlled experiment setting; and performs competitively with many open-source 3B models in downstream evaluation while using much fewer total training tokens.

""HGRN2 is comparable to the state-of-the-art (SOTA)
methods at 1 billion and better than SOTA methods at 3 billion size.""",,,,"China,United States of America,Spain",,,,,,2025-05-01 10:42,,,,,,"Academia,Academia,Industry",,,,,,,"Academia,Academia,Industry",,,,,Operation counting,,,,
HGRN2 1B,Language,"Language modeling/generation,Question answering","Shanghai AI Lab,Massachusetts Institute of Technology (MIT),Taptap","Zhen Qin, Songlin Yang, Weixuan Sun, Xuyang Shen, Dong Li, Weigao Sun, Yiran Zhong",2024-04-11,HGRN2: Gated Linear RNNs with State Expansion,https://arxiv.org/abs/2404.07904v1,,,,1000000000.0,1B,6.000000001e+21,6ND = 6*10^9 * 100*10^9 = 6*10^21,"The Pile,C4,Wikipedia",,100000000000.0,100B Table 6,,,,,Confident,"Hierarchically gated linear RNN (HGRN,Qin et al. 2023) has demonstrated competitive training speed and performance in language modeling, while offering efficient inference. However, the recurrent state size of HGRN remains relatively small, which limits its this http URL address this issue, inspired by linear attention, we introduce a simple outer-product-based state expansion mechanism so that the recurrent state size can be significantly enlarged without introducing any additional parameters. The linear attention form also allows for hardware-efficient this http URL extensive experiments verify the advantage of HGRN2 over HGRN1 in language modeling, image classification, and Long Range this http URL largest 3B HGRN2 model slightly outperforms Mamba and LLaMa Architecture Transformer for language modeling in a controlled experiment setting; and performs competitively with many open-source 3B models in downstream evaluation while using much fewer total training tokens.

""HGRN2 is comparable to the state-of-the-art (SOTA)
methods at 1 billion and better than SOTA methods at 3 billion size.""",,,,"China,United States of America,Spain",,,,,,2025-05-01 10:42,,,,,,"Academia,Academia,Industry",,,,,,,"Academia,Academia,Industry",,,,,Operation counting,,,,
Zephyr 141B-A39B																																						,Language,"Language modeling/generation,Question answering","Hugging Face,Korea Advanced Institute of Science and Technology (KAIST),Argilla","Alvaro Bartolome, Jiwoo Hong, Noah Lee, Kashif Rasul, Lewis Tunstall",2024-04-10,Model Card for Zephyr 141B-A39B,https://huggingface.co/HuggingFaceH4/zephyr-orpo-141b-A35b-v0.1,,,,141000000000.0,"""A Mixture of Experts (MoE) model with 141B total parameters and 39B active parameters.""",,,Capybara-DPO 7K binarized,,,,1.3,"""1.3 hours on 4 nodes of 8 x H100s""",NVIDIA H100 SXM5 80GB,,Confident,"Zephyr is a series of language models that are trained to act as helpful assistants. Zephyr 141B-A39B is the latest model in the series, and is a fine-tuned version of mistral-community/Mixtral-8x22B-v0.1 that was trained using a novel alignment algorithm called Odds Ratio Preference Optimization (ORPO) with 7k instances for 1.3 hours on 4 nodes of 8 x H100s. ORPO does not require an SFT step to achieve high performance and is thus much more computationally efficient than methods like DPO and PPO. To train Zephyr-141B-A39B, we used the argilla/distilabel-capybara-dpo-7k-binarized preference dataset, which consists of synthetic, high-quality, multi-turn preferences that have been scored via LLMs.",3.0,,Open weights (unrestricted),"Multinational,United States of America,Korea (Republic of),Spain",Mixtral 8x22B,44456256000000000000,989500000000000*32*1.3*3600*0.3 = 4.4456256e+19,32.0,,2024-12-02 10:55,,,,,,"Industry,Academia,Industry",,,,,Open source,"Apache 2.0

https://github.com/huggingface/alignment-handbook/tree/main/recipes/zephyr-141b-A35b
https://huggingface.co/HuggingFaceH4/zephyr-orpo-141b-A35b-v0.1","Industry,Academia,Industry",,,,44287.08397263639,Hardware,,,,
Stable LM 2 12B,Language,"Language modeling/generation,Translation",Stability AI,,2024-04-08,Introducing Stable LM 2 12B,"https://stability.ai/news/introducing-stable-lm-2-12b
https://huggingface.co/stabilityai/stablelm-2-12b",,,,12143605760.0,Precise number given in HF model card,2.91e+23,"2* 12143605760 params * 3* 2T tokens * 2 epochs = 2.91e23. 
Trained on 384 H100s (AWS P5 instances).","RefinedWeb,RedPajama-Data,The Pile,StarCoder,CulturaX","The dataset is comprised of a filtered mixture of open-source large-scale datasets available on the HuggingFace Hub: Falcon RefinedWeb extract (Penedo et al., 2023), RedPajama-Data (Together Computer., 2023) and The Pile (Gao et al., 2020) both without the Books3 subset, and StarCoder (Li et al., 2023). We further supplement our training with multi-lingual data from CulturaX (Nguyen et al., 2023) and, in particular, from its OSCAR corpora, as well as restructured data in the style of Yuan & Liu (2022).",2000000000000.0,2T tokens,,,NVIDIA H100 SXM5 80GB,Self-supervised learning,Confident,"Introducing the latest additions to our Stable LM 2 language model series: a 12 billion parameter base model and an instruction-tuned variant, trained on 2 trillion tokens in seven languages: English, Spanish, German, Italian, French, Portuguese, and Dutch. This medium-sized model balances strong performance, efficiency, memory requirements, and speed, following our established Stable LM 2 1.6B framework as detailed in our previously released technical report. With this release, weâ€™re extending our model range, offering a transparent and powerful tool for developers to innovate in AI language technology. Soon, we plan to introduce a long-context variant of these models which will be available on Hugging Face upon release.

From Hugging Face:
Stable LM 2 12B is a 12.1 billion parameter decoder-only language model pre-trained on 2 trillion tokens of diverse multilingual and code datasets for two epochs.",2.0,,Open weights (restricted use),United Kingdom of Great Britain and Northern Ireland,,,,,,2025-06-11 07:40,,,,,,Industry,,,,,Open source,"Requires Stability AI Membership. Free for non-commercial use, $20/month for commercial use if less than $1M in annual revenue, $1M in institutional funding, and 1M monthly active users. 

Apache 2.0 license for repo, which includes detailed hyperparams and training details: https://github.com/Stability-AI/StableLM/blob/main/LICENSE  ",Industry,,,BF16,,Operation counting,,,,
YaART,Image generation,"Text-to-image,Image generation",Yandex,"Sergey Kastryulin, Artem Konev, Alexander Shishenya, Eugene Lyapustin, Artem Khurshudov, Alexander Tselousov, Nikita Vinokurov, Denis Kuznedelev, Alexander Markovich, Grigoriy Livshits, Alexey Kirillov, Anastasiia Tabisheva, Liubov Chubarova, Marina Kaminskaia, Alexander Ustyuzhanin, Artemii Shvetsov, Daniil Shlenskii, Valerii Startsev, Dmitrii Kornilov, Mikhail Romanov, Artem Babenko, Sergei Ovcharenko, Valentin Khrulkov",2024-04-08,YaART: Yet Another ART Rendering Technology,https://arxiv.org/abs/2404.05666,,,,2300000000.0,,8.21376e+19,"There is cascade of 3 models:

GEN64
Parameters: 2.3 billion
Input: Textual prompt
Output: 64x64 image
Batch Size: 4800
Iterations: 1.1 Ã— 10^6 

Compute = 6 * 2.3 billion * 4800 * 1.1 Ã— 10^6 = 7.2864e+19

SR256
Parameters: 700 million
Input: 64x64 image
Output: 256x256 image
Batch Size: 960
Iterations: 1.5 Ã— 10^6 

Compute = 6 * 700 million * 960 * 1.5 Ã— 10^6 = 6.048e+18

SR1024
Parameters: 700 million
Input: 256x256 image
Output: 1024x1024 image
Batch Size: 512
Iterations: 1.5 Ã— 10^6 

Compute = 6 * 700 million * 512 * 1.5 Ã— 10^6 = 3.2256e+18

Total compute = 7.2864e+19 + 6.048e+18 + 3.2256e+18 = 8.21376e+19",,,,,,,NVIDIA A100 SXM4 80 GB,,Speculative,"In the rapidly progressing field of generative models, the development of efficient and high-fidelity text-to-image diffusion systems represents a significant frontier. This study introduces YaART, a novel production-grade text-to-image cascaded diffusion model aligned to human preferences using Reinforcement Learning from Human Feedback (RLHF). During the development of YaART, we especially focus on the choices of the model and training dataset sizes, the aspects that were not systematically investigated for text-to-image cascaded diffusion models before. In particular, we comprehensively analyze how these choices affect both the efficiency of the training process and the quality of the generated images, which are highly important in practice. Furthermore, we demonstrate that models trained on smaller datasets of higher-quality images can successfully compete with those trained on larger datasets, establishing a more efficient scenario of diffusion models training. From the quality perspective, YaART is consistently preferred by users over many existing state-of-the-art models.",,,API access,Russia,,,,160.0,,2025-02-14 14:24,,,,4800.0,,Industry,,,,,,,Industry,,,,126540.16145027014,Operation counting,,,,
SambaLingo-Thai-Chat (7B),Language,Chat,"""SambaNova Systems, Inc""","Zoltan Csaki, Bo Li, Jonathan Li, Qiantong Xu, Pian Pawakapan, Leon Zhang, Yun Du, Hengyu Zhao, Changran Hu, Urmish Thakker",2024-04-08,SambaLingo: Teaching Large Language Models New Languages,https://arxiv.org/abs/2404.05829,,,,6950000000.0,https://huggingface.co/sambanovasystems/SambaLingo-Thai-Chat,8.569999999999999e+22,"""Continuous Pre-training: We pack the pretraining mixture into sequences of length 4096 and pretrain with document attention as described in Section 3.2 of Iyer et al. (2022) to ensure we only attend to tokens in the context of the corresponding text document. We train with a global batch size of 1024, sequence length of 4096, maximum learning rate of 1e-4 with cosine decay, warm-up ratio of 0.01 and a weight decay of 0.1. Each expert is trained for a maximum of 4 epochs, following (Muennighoff et al., 2023). Notably, we train all model parameters, foregoing use of PEFT methods such as LoRA (Hu et al., 2022), which are known to be inferior to
full parameter training (Zhao et al., 2024a)(Sun et al., 2023)"" (https://arxiv.org/pdf/2404.05829, page 16). 

""The base model adapts Llama-2-7b to Thai by training on 38 billion tokens from the Thai split of the Cultura-X dataset"" (https://huggingface.co/sambanovasystems/SambaLingo-Thai-Chat).  


Fine-tune: 7.0e+9*4.0e+10*6=1.7e+21
Base model: 8.4e+22
Total: 8.4e+22+1.7e+21=8.6e+22",CulturaX,"""The base model adapts Llama-2-7b to Thai by training on 38 billion tokens from the Thai split of the Cultura-X dataset"" (https://huggingface.co/sambanovasystems/SambaLingo-Thai-Chat).  ",10000000000.0,,,,,,Confident,"Despite the widespread availability of LLMs, there remains a substantial gap in their capabilities and availability across diverse languages. One approach to address these issues has been to take an existing pre-trained LLM and continue to train it on new languages. While prior works have experimented with language adaptation, many questions around best practices and methodology have not been covered. In this paper, we present a comprehensive investigation into the adaptation of LLMs to new languages. Our study covers the key components in this process, including vocabulary extension, direct preference optimization and the data scarcity problem for human alignment in low-resource languages. We scale these experiments across 9 languages and 2 parameter scales (7B and 70B). We compare our models against Llama 2, Aya-101, XGLM, BLOOM and existing language experts, outperforming all prior published baselines. Additionally, all evaluation code and checkpoints are made public to facilitate future research.",4.0,,Open weights (unrestricted),United States of America,Llama 2-7B,,"""The alignment phase follows the recipe for Zephyr-7B, and comprises two stages: supervised fine-tuning (SFT) and Direct Performance Optimization (DPO).

The SFT phase was done on the ultrachat_200k dataset mixed with the Google translated version of the ultrachat_200k dataset. It was trained for one epoch with global batch size 512 and max sequence length 2048 tokens. We used a linear decay learning rate of 2e-5 and 10% warmup.

The DPO phase was done on the ultrafeedback dataset and cai-conversation-harmless dataset, mixed with 10% of the data Google translated. It was trained with global batch size 32 and for three epochs. We used a linear decay learning rate of 5e-7, 10% warmup and Î²=0.1 as the regularization factor for DPO""  (https://huggingface.co/sambanovasystems/SambaLingo-Thai-Chat).",,,2025-05-01 10:42,,,,1024.0,"""Continuous Pre-training: We pack the pretraining mixture into sequences of length 4096 and pretrain with document attention as described in Section 3.2 of Iyer et al. (2022) to ensure we only attend to tokens in the context of the corresponding text document. We train with a global batch size of 1024, sequence length of 4096, maximum learning rate of 1e-4 with cosine decay, warm-up ratio of 0.01 and a weight decay of 0.1. Each expert is trained for a maximum of 4 epochs, following (Muennighoff et al., 2023). Notably, we train all model parameters, foregoing use of PEFT methods such as LoRA (Hu et al., 2022), which are known to be inferior to
full parameter training (Zhao et al., 2024a)(Sun et al., 2023)"" (https://arxiv.org/pdf/2404.05829, page 16). ",Industry,,,,,,,Industry,,,,,,,,,
SambaLingo-Thai-Chat-70B,Language,Chat,"""SambaNova Systems, Inc""","Zoltan Csaki, Bo Li, Jonathan Li, Qiantong Xu, Pian Pawakapan, Leon Zhang, Yun Du, Hengyu Zhao, Changran Hu, Urmish Thakker",2024-04-08,SambaLingo: Teaching Large Language Models New Languages,https://arxiv.org/abs/2404.05829,,,,70000000000.0,https://huggingface.co/sambanovasystems/SambaLingo-Thai-Chat-70B,8.1168e+23,"""Continuous Pre-training: We pack the pretraining mixture into sequences of length 4096 and pretrain with document attention as described in Section 3.2 of Iyer et al. (2022) to ensure we only attend to tokens in the context of the corresponding text document. We train with a global batch size of 1024, sequence length of 4096, maximum learning rate of 1e-4 with cosine decay, warm-up ratio of 0.01 and a weight decay of 0.1. Each expert is trained for a maximum of 4 epochs, following (Muennighoff et al., 2023). Notably, we train all model parameters, foregoing use of PEFT methods such as LoRA (Hu et al., 2022), which are known to be inferior to
full parameter training (Zhao et al., 2024a)(Sun et al., 2023)"" (https://arxiv.org/pdf/2404.05829, page 16). 

""The base model adapts Llama-2-70b to Thai by training on 26 billion tokens from the Thai split of the Cultura-X dataset"" (https://huggingface.co/sambanovasystems/SambaLingo-Thai-Chat).  

Since Llama is a dense model, assuming SambaLingo-Thai-Chat was trained for 4 epochs, the 6ND approximation yields
Pre-training compute
= # of active parameters / forward pass * # of tokens * 6 FLOPs / token * # of epochs
~= 7e10 parameters * 2.6e10 tokens * 6 FLOPs / token * 4 epochs
~= 4.37e22 FLOPs

Finetune: 6*7.0e+10*4.0e+9=1.7e+21
Base model: 8.1e+23
Total: 8.116800e+23",CulturaX,"""The base model adapts Llama-2-7b to Thai by training on 38 billion tokens from the Thai split of the Cultura-X dataset"" (https://huggingface.co/sambanovasystems/SambaLingo-Thai-Chat).  ",10000000000.0,,,,,,Confident,"Despite the widespread availability of LLMs, there remains a substantial gap in their capabilities and availability across diverse languages. One approach to address these issues has been to take an existing pre-trained LLM and continue to train it on new languages. While prior works have experimented with language adaptation, many questions around best practices and methodology have not been covered. In this paper, we present a comprehensive investigation into the adaptation of LLMs to new languages. Our study covers the key components in this process, including vocabulary extension, direct preference optimization and the data scarcity problem for human alignment in low-resource languages. We scale these experiments across 9 languages and 2 parameter scales (7B and 70B). We compare our models against Llama 2, Aya-101, XGLM, BLOOM and existing language experts, outperforming all prior published baselines. Additionally, all evaluation code and checkpoints are made public to facilitate future research.",0.4,,Open weights (restricted use),United States of America,Llama 2-70B,1,"""The alignment phase follows the recipe for Zephyr-7B, and comprises two stages: supervised fine-tuning (SFT) and Direct Performance Optimization (DPO).

The SFT phase was done on the ultrachat_200k dataset mixed with the Google translated version of the ultrachat_200k dataset. It was trained for one epoch with global batch size 512 and max sequence length 2048 tokens. We used a linear decay learning rate of 2e-5 and 10% warmup.

The DPO phase was done on the ultrafeedback dataset and cai-conversation-harmless dataset, mixed with 10% of the data Google translated. It was trained with global batch size 32 and for three epochs. We used a linear decay learning rate of 5e-7, 10% warmup and Î²=0.1 as the regularization factor for DPO.""  (https://huggingface.co/sambanovasystems/SambaLingo-Thai-Chat-70B).",,,2025-05-23 16:32,,,,32.0,,Industry,,,,,Unreleased,"Llama 2 license
""You will not use the Llama Materials or any output or results of the 
Llama Materials to improve any other large language model""
""Meta may terminate this Agreement if you are in breach of any term or 
condition of this Agreement""
[If MAU] ""is greater than 700 million monthly active users in the 
preceding calendar month, you must request a license from Meta""

https://huggingface.co/sambanovasystems/SambaLingo-Thai-Chat-70B",Industry,,,,,,sambanovasystems,,,
ESM-AA,Biology,"Protein folding prediction,Protein or nucleotide language model (pLM/nLM),Proteins","Peking University,Nanjing University,Tsinghua University,PharMolix","Kangjie Zheng, Siyu Long, Tianyu Lu, Junwei Yang, Xinyu Dai, Ming Zhang, Zaiqing Nie, Wei-Ying Ma, Hao Zhou",2024-04-05,ESM All-Atom: Multi-scale Protein Language Model for Unified Molecular Modeling,https://arxiv.org/abs/2403.12995,2.0,,,35000000.0,,7.28e+20,"1. Hardware: 16x NVIDIA A100 GPUs (3.12e14 FLOP/s per GPU)
2. Training duration: 3 days directly reported = 259,200 seconds
3. Utilization: 40% assumed
4. Calculation: 3.12e14 FLOP/s Ã— 16 GPUs Ã— 259,200s Ã— 0.40 = 5.18e20 FLOPs

Base model: 209999999999999970000
Total: 209999999999999970000+5.18e20=728000000000000000000",,,23300000000.0,"
Assuming 300 tokens per protein and 100 token per molecule
8M*300+209M*100=23300000000
",72.0,,NVIDIA A100,,Confident,"Protein language models have demonstrated significant potential in the field of protein engineering. However, current protein language models primarily operate at the residue scale, which limits their ability to provide information at the atom level. This limitation prevents us from fully exploiting the capabilities of protein language models for applications involving both proteins and small molecules. In this paper, we propose ESM-AA (ESM All-Atom), a novel approach that enables atom-scale and residue-scale unified molecular modeling. ESM-AA achieves this by pre-training on multi-scale code-switch protein sequences and utilizing a multi-scale position encoding to capture relationships among residues and atoms. Experimental results indicate that ESM-AA surpasses previous methods in protein-molecule tasks, demonstrating the full utilization of protein language models. Further investigations reveal that through unified molecular modeling, ESM-AA not only gains molecular knowledge but also retains its understanding of proteins. The source codes of ESM-AA are publicly released at this https URL.",,,,"China,China,China,China",ESM2-35M,,,16.0,,2025-05-01 10:42,,,,,,"Academia,Academia,Academia,Industry",,,,,,,"Academia,Academia,Academia,Industry",,,,12654.861564218108,Hardware,,,,
Viking,Language,"Language modeling/generation,Language generation,Translation","Silo AI,University of Turku",,2024-04-04,"Viking 33B is a 33B parameter decoder-only transformer pretrained on Finnish, English, Swedish, Danish, Norwegian, Icelandic and code. It is being trained on 2 trillion tokens (700B billion as of this release). Viking 33B is a fully open source model and is made available under the Apache 2.0 License.",https://huggingface.co/LumiOpen/Viking-33B,,,,33000000000.0,33B,2.574e+23,"Plan is to train on 2 trillion tokens, but most recent release is at 1.3T
6 * 33B * 1.3 trillion = 2.574E23",Unspecified unreleased,,2000000000000.0,"Viking is being trained on a 2 trillion token mixed dataset of English, Finnish, Swedish, Danish, Norwegian, Icelandic and code.",,,AMD Radeon Instinct MI250X,,Confident,,,,Open weights (unrestricted),"Finland,Finland",,,,1024.0,,2025-05-14 15:36,,LUMI supercomputer (Finland),,,,"Industry,Academia",,,,,Open source,code here: https://github.com/LumiOpen/Megatron-DeepSpeed/blob/main/pretrain_viking_33B.sh ,"Industry,Academia",,,,1012411.470653334,Operation counting,,,,
Sailor-7B-Chat,Language,Chat,"Sea AI Lab,Singapore University of Technology & Design","Longxu Dou, Qian Liu, Guangtao Zeng, Jia Guo, Jiahui Zhou, Wei Lu, Min Lin",2024-04-04,Sailor: Open Language Models for South-East Asia,https://arxiv.org/abs/2404.03608,,,,7720000000.0,https://huggingface.co/sail/Sailor-7B-Chat,1.7726e+23,"""The final pre-training corpus, SailCraft, is composed of approximately 200B tokens, integrating both SEA tokens and replay tokens, as elaborated in Section 4.4. We use the batch size of 4M tokens and the learning rate of 1e-4. Following a warmup period of 500 steps, the learning rate remains constant. This scheduling strategy encourages more transferable conclusions from simulations and allows for easier recovery from interrupted training
sessions. Generally Sailor models consume around 200B tokens, completing a full pass through the SailCraft corpus once"" (https://arxiv.org/pdf/2404.03608, page 15).

Qwen1.5 is a dense model (https://huggingface.co/Qwen/Qwen1.5-7B), so assuming Sailor-7B-Chat was trained on SailCraft for 1 epoch, the 6ND approximation yields
Pre-training compute
= # of active parameters / forward pass * # of tokens * 6 FLOPs / token
~= 7.72e9 parameters * 2e11 tokens * 6 FLOPs / token
~= 9.26e21 FLOPS

Base model: 1.68e23
Total: 9.26e21+1.68e23=1.8e+23
","CC100,MADLAD-400,Wikipedia,OPUS,SlimPajama,SkyPile",See pages 7-8 of https://arxiv.org/pdf/2404.03608 for full list and details.,200000000000.0,"""The final pre-training corpus, SailCraft, is composed of approximately 200B tokens, integrating both SEA tokens and replay tokens, as elaborated in Section 4.4"" (https://arxiv.org/pdf/2404.03608, page 15). Assuming all Asian languages have a word per token ratio of 1 (as the ones in https://docs.google.com/document/d/1XWLyMzcVfDv4eFQX3yPgM8MZ3_Q1phtIFz9GKv4_KaM/edit?tab=t.0#heading=h.ieihc08p8dn0), SailCraft contains approximately 200B words. Since this is a text generation model, the dataset size is measured in number of words.",,,NVIDIA A100 SXM4 40 GB,,Confident,"We present Sailor, a family of open language models ranging from 0.5B to 7B parameters, tailored for South-East Asian (SEA) languages. These models are continually pre-trained from Qwen1.5, a great language model for multilingual use cases. From Qwen1.5, Sailor models accept 200B to 400B tokens, primarily covering the languages of English, Chinese, Vietnamese, Thai, Indonesian, Malay, and Lao. The training leverages several techniques, including BPE dropout for improving the model robustness, aggressive data cleaning and deduplication, and small proxy models to optimize data mixture. Experimental results on four typical tasks indicate that Sailor models demonstrate strong performance across different benchmarks, including commonsense reasoning, question answering, reading comprehension and examination. Embracing the open-source spirit, we share our insights through this report to spark a wider interest in developing large language models for multilingual use cases.",1.0,,Open weights (unrestricted),"Singapore,Singapore",Qwen1.5-7B,,,64.0,,2025-05-23 16:35,,,,4000000.0,,Academia,,,,,Unreleased,"Apache 2.0

https://huggingface.co/sail/Sailor-7B-Chat

 only evaluation and inference code in the repo https://github.com/sail-sg/sailor-llm",Academia,,,,50620.5735326667,,sail,,,
Universal-1,Speech,Speech recognition,AssemblyAI,"Francis McCann (lead), Luka Chkhetiani (lead), Andrew Ehrenberg, Robert McHardy, Rami Botros, Yash Khare, Andrea Vanzo, Taufiquzzaman Peyash, Gabriel Oexle, Michael Liang, Ilya Sklyar, Ahmed Etefy, Daniel McCrystal, William Pipsico Ferreira, Ruben Bousbib, Ben Gotthold, Soheyl Bahadoori, Enver Fakhan, Rahul Bagai, Mez Rashid, James He, Takuya Yoshioka, Travis Kupsche, Domenic Donato, Marco Ramponi, Ryan Oâ€™Connor, Sam Flamini, 
Sergio Ramirez Martin (lead), Rajpreet Thethy, Lee Vaughn, Martin Schweiger, Anita Miller, Prachie Banthia, Audy Mulia, Amanda Lee, Christy Roach, Domenic Donato, Dylan Duan, Matt Lawler, Ryan Oâ€™Connor, and Whitney DeGraaf, Francis McCann, Sergio Ramirez Martin, Jaime Lorenzo-Trueba, Ruben Bousbib, Michelle Asuamah, Marco Ramponi, Gabriel Oexle, Rami Botros, Ilya Skylar, Patrick Loeber, Dillon Pulliam, Michael Chinen",2024-04-03,Robust and accurate multilingual speech-to-text,https://www.assemblyai.com/research/universal-1,,,,600000000.0,"""Training on more than 12.5 million hours of diverse multilingual audio data, 600M-parameter Conformer RNN-T based Universal-1 achieves remarkable robustness""
",,,Unspecified unreleased,,,"""Trained on over 12.5 million hours of multilingual audio data""


+ fine-tuning 150K hours

""A batch size of 2048 is used.""",,,Google TPU v5e,,Confident,"We are excited to introduce Universal-1, our latest and most powerful speech recognition model. Trained on over 12.5 million hours of multilingual audio data, Universal-1 achieves best-in-class speech-to-text accuracy across four major languages: English, Spanish, French, and German.",,,API access,,Conformer,,,,0.54,2025-05-12 15:06,,,,,,Industry,,,,,Unreleased,,Industry,,"""Pre-training was performed on a cluster of v5e TPU chips, operating with 54% Model Flops Utilization (MFU), thanks to our optimized JAX implementation.""",,,,,,,
Mixture-of-Depths,Language,Language modeling/generation,"Google DeepMind,McGill University,Mila - Quebec AI (originally Montreal Institute for Learning Algorithms)","David Raposo, Sam Ritter, Blake Richards, Timothy Lillicrap, Peter Conway Humphreys, Adam Santoro",2024-04-02,Mixture-of-Depths: Dynamically allocating compute in transformer-based language models,https://arxiv.org/abs/2404.02258,35.0,,"Improvements on transformer loss for a given compute budget, or can match loss with >60% fewer FLOPs per training step. See Figure 3.",3000000000.0,"Figure 4: ""We used the 12.5% capacity MoD variant to perform an isoFLOP analysis for 6e18, 2e19, and 1e20 FLOPs, training models varying in size from 60M to 3B parameters""",1e+20,"Figure 4: ""We used the 12.5% capacity MoD variant to perform an isoFLOP analysis for 6e18, 2e19, and 1e20 FLOPs, training models varying in size from 60M to 3B parameters""",,Training set not described.,,,,,,,Confident,"Transformer-based language models spread FLOPs uniformly across input sequences. In this work we demonstrate that transformers can instead learn to dynamically allocate FLOPs (or compute) to specific positions in a sequence, optimising the allocation along the sequence for different layers across the model depth. Our method enforces a total compute budget by capping the number of tokens (k) that can participate in the self-attention and MLP computations at a given layer. The tokens to be processed are determined by the network using a top-k routing mechanism. Since k is defined a priori, this simple procedure uses a static computation graph with known tensor sizes, unlike other conditional computation techniques. Nevertheless, since the identities of the k tokens are fluid, this method can expend FLOPs non-uniformly across the time and model depth dimensions. Thus, compute expenditure is entirely predictable in sum total, but dynamic and context-sensitive at the token-level. Not only do models trained in this way learn to dynamically allocate compute, they do so efficiently. These models match baseline performance for equivalent FLOPS and wall-clock times to train, but require a fraction of the FLOPs per forward pass, and can be upwards of 50\% faster to step during post-training sampling.",,,Unreleased,"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France,Canada,Canada",,,,,,2025-02-19 13:35,,,,,,"Industry,Academia,Academia",,,,,Unreleased,,"Industry,Academia,Academia",,,,,Reported,,,,
POKEÂ´LLMON,"Games,Language","Language modeling/generation,Pokemon battles",Georgia Institute of Technology,"Sihao Hu, Tiansheng Huang, Ling Liu",2024-04-02,PokeLLMon: A Human-Parity Agent for Pokemon Battles with Large Language Models,https://arxiv.org/abs/2402.01118,,,,,,,,,,,,,,,,Unknown,"We introduce PokeLLMon, the first LLM-embodied agent that achieves human-parity performance in tactical battle games, as demonstrated in Pokemon battles. The design of PokeLLMon incorporates three key strategies: (i) In-context reinforcement learning that instantly consumes text-based feedback derived from battles to iteratively refine the policy; (ii) Knowledge-augmented generation that retrieves external knowledge to counteract hallucination and enables the agent to act timely and properly; (iii) Consistent action generation to mitigate the panic switching phenomenon when the agent faces a powerful opponent and wants to elude the battle. We show that online battles against human demonstrates PokeLLMon's human-like battle strategies and just-in-time decision making, achieving 49% of win rate in the Ladder competitions and 56% of win rate in the invited battles. Our implementation and playable battle logs are available at: this https URL.",,,Hosted access (no API),United States of America,GPT-4,,,,,2025-04-17 03:28,,,,,,Academia,,,,,Open source,"https://play.pokemonshowdown.com/
https://github.com/git-disl/PokeLLMon",Academia,,,,,,,,,
AutoDiff,Biology,Drug discovery,"Galixir Technologies,Rensselaer Polytechnic Institute,Massachusetts Institute of Technology (MIT)","Xinze Li, Penglei Wang, Tianfan Fu, Wenhao Gao, Chengtao Li, Leilei Shi, Junhong Liu",2024-04-02,AUTODIFF: Autoregressive Diffusion Modeling for Structure-based Drug Design,https://arxiv.org/abs/2404.02003,1.0,,,,,,,,,22500001.0,"22.5M poses = 22.5M datapoints
= 22,500,000 datapoints

Final estimate = 22.5M",,,,,Confident,"Structure-based drug design (SBDD), which aims to generate molecules that can bind tightly to the target protein, is an essential problem in drug discovery, and previous approaches have achieved initial success. However, most existing methods still suffer from invalid local structure or unrealistic conformation issues, which are mainly due to the poor leaning of bond angles or torsional angles. To alleviate these problems, we propose AUTODIFF, a diffusion-based fragment-wise autoregressive generation model. Specifically, we design a novel molecule assembly strategy named conformal motif that preserves the conformation of local structures of molecules first, then we encode the interaction of the protein-ligand complex with an SE(3)-equivariant convolutional network and generate molecules motif-by-motif with diffusion modeling. In addition, we also improve the evaluation framework of SBDD by constraining the molecular weights of the generated molecules in the same range, together with some new metrics, which make the evaluation more fair and practical. Extensive experiments on CrossDocked2020 demonstrate that our approach outperforms the existing models in generating realistic molecules with valid structures and conformations while maintaining high binding affinity.",,,Unreleased,"China,United States of America,United States of America",,,,,,2025-06-16 13:13,,,,,,"Industry,Academia,Academia",,,,,Unreleased,,"Industry,Academia,Academia",,,,,,,,,
MobileCLIP-B (LT),Vision,"Image captioning,Image classification",Apple,"Pavan Kumar Anasosalu Vasu, Hadi Pouransari, Fartash Faghri, Raviteja Vemulapalli, Oncel Tuzel",2024-04-01,MobileCLIP: Fast Image-Text Models through Multi-Modal Reinforced Training,https://arxiv.org/abs/2311.17049,,,,149700000.0,"Table 7 
Params (M) (img+txt)
86.3 + 63.4",1.3423599e+22,"6 FLOP / token / parameter * 490*10^9 tokens [see dataset size notes] * 30.50 epochs * 149700000.00 parameters = 1.3423599e+22 FLOP

'Likely' confidence because I am not sure about dataset size",DataCompDR-1B,"""One seen sample for DataCompDR is a triplet of one randomly augmented
image, one ground-truth caption, and one randomly picked synthetic caption.""",490000000000.0,"140TB, 1.28B datapoints - size of the dataset

""We train using 64 nodes with 8xA100-80GB GPUs and a
per-GPU batch size of either 128 or 256.""

39B - seen samples

LR schedule Const(300k, 65k, 2k) + Cosine(300k, 65k, 2k) 39B

600k total steps(for LT = long training model)

39/1.28 ~ 30.5 epochs

assumming image resolution 256*256, patch size 14*14, caption length 50 text tokens

then total size of the dataset is

1.28*10^9*((256/14)^2+50) = 491990204082 tokens ~490B tokens",,"""We train using 64 nodes with 8xA100-80GB GPUs and a
per-GPU batch size of either 128 or 256.""",NVIDIA A100 SXM4 80 GB,,Likely,"Contrastive pretraining of image-text foundation models, such as CLIP, demonstrated excellent zero-shot performance and improved robustness on a wide range of downstream tasks. However, these models utilize large transformer-based encoders with significant memory and latency overhead which pose challenges for deployment on mobile devices. In this work, we introduce MobileCLIP -- a new family of efficient image-text models optimized for runtime performance along with a novel and efficient training approach, namely multi-modal reinforced training. The proposed training approach leverages knowledge transfer from an image captioning model and an ensemble of strong CLIP encoders to improve the accuracy of efficient models. Our approach avoids train-time compute overhead by storing the additional knowledge in a reinforced dataset. MobileCLIP sets a new state-of-the-art latency-accuracy tradeoff for zero-shot classification and retrieval tasks on several datasets. Our MobileCLIP-S2 variant is 2.3Ã— faster while more accurate compared to previous best CLIP model based on ViT-B/16. We further demonstrate the effectiveness of our multi-modal reinforced training by training a CLIP model based on ViT-B/16 image backbone and achieving +2.9% average performance improvement on 38 evaluation benchmarks compared to the previous best. Moreover, we show that the proposed approach achieves 10Ã—-1000Ã— improved learning efficiency when compared with non-reinforced CLIP training. Code and models are available at this https URL .",30.5,,Open weights (unrestricted),United States of America,,,,256.0,,2025-05-22 15:09,,,,,,Industry,,,,,Open source,MIT License,Industry,,,,202495.82204270296,Operation counting,,,,
TeleChat-7B,Language,"Language modeling/generation,Chat,Question answering,Text summarization,Code generation,Translation",China Telecom,"Zhongjiang He, Zihan Wang, Xinzhang Liu, Shixuan Liu, Yitong Yao, Yuyao Huang, Xuelong Li, Yongxiang Li, Zhonghao Che, Zhaoxi Zhang, Yan Wang, Xin Wang, Luwen Pu, Huinan Xu, Ruiyu Fang, Yu Zhao, Jie Zhang, Xiaomeng Huang, Zhilong Lu, Jiaxin Peng, Wenjun Zheng, Shiquan Wang, Bingkai Yang, Xuewei he, Zhuoru Jiang, Qiyi Xie, Yanhan Zhang, Zhongqiu Li, Lingling Shi, Weiwei Fu, Yin Zhang, Zilu Huang, Sishi Xiong, Yuxiang Zhang, Chao Wang, Shuangyong Song",2024-04-01,TELECHAT TECHNICAL REPORT,https://arxiv.org/abs/2401.03804,,,,7000000000.0,,4.2e+22,"80 nodes, each having 8 Nvidia A100 Sxm 40GB GPUs

6 FLOP / token / parameter * 7*10^9 parameters * 1 * 10^12 tokens = 4.2e+22 FLOP",Telechat - PTD,,1000000000000.0,Table 3,,"""Training TeleChat took one month (including downtime).""",NVIDIA A100 SXM4 40 GB,,Confident,"In this technical report, we present TeleChat, a collection of large language models (LLMs) with parameters of 3 billion, 7 billion and 12 billion. It includes pretrained language models as well as fine-tuned chat models that is aligned with human preferences. TeleChat is initially pretrained on an extensive corpus containing a diverse collection of texts from both English and Chinese languages, including trillions of tokens. Subsequently, the model undergoes fine-tuning to align with human preferences, following a detailed methodology that we describe. We evaluate the performance of TeleChat on various tasks, including language understanding, mathematics, reasoning, code generation, and knowledge-based question answering. Our findings indicate that TeleChat achieves comparable performance to other open-source models of similar size across a wide range of public benchmarks. To support future research and applications utilizing LLMs, we release the fine-tuned model checkpoints of TeleChat's 7B and 12B variant, along with code and a portion of our pretraining data, to the public community.",,,Open weights (restricted use),China,,,,640.0,,2025-06-09 11:43,,,,,,Industry,,,,,Open (restricted use),"Apache 2.0 
weights:
https://huggingface.co/Tele-AI/telechat-7B

inference code:
https://github.com/Tele-AI/Telechat

""Community use TeleChat model needs to follow ã€Š TeleChat model community license agreement ã€‹. The TeleChat model supports commercial use. If you plan to use the TeleChat model or its derivatives for commercial purposes, you need to contact the mailbox below tele_ai@chinatelecom.cn , Submit the application materials required by the ã€ŠTeleChat model community license agreementã€‹. After the review is approved, you will be granted a non-exclusive, global, non-transferable, non-relicable, revocable commercial copyright license.""",Industry,,,,506239.5551067574,Operation counting,Tele-AI,,,
TeleChat-3B,Language,"Language modeling/generation,Chat,Question answering,Text summarization,Code generation,Translation",China Telecom,"Zhongjiang He, Zihan Wang, Xinzhang Liu, Shixuan Liu, Yitong Yao, Yuyao Huang, Xuelong Li, Yongxiang Li, Zhonghao Che, Zhaoxi Zhang, Yan Wang, Xin Wang, Luwen Pu, Huinan Xu, Ruiyu Fang, Yu Zhao, Jie Zhang, Xiaomeng Huang, Zhilong Lu, Jiaxin Peng, Wenjun Zheng, Shiquan Wang, Bingkai Yang, Xuewei he, Zhuoru Jiang, Qiyi Xie, Yanhan Zhang, Zhongqiu Li, Lingling Shi, Weiwei Fu, Yin Zhang, Zilu Huang, Sishi Xiong, Yuxiang Zhang, Chao Wang, Shuangyong Song",2024-04-01,TELECHAT TECHNICAL REPORT,https://arxiv.org/abs/2401.03804,,,,3000000000.0,,1.44e+22,"80 nodes, each having 8 Nvidia A100 Sxm 40GB GPUs

6 FLOP / token / parameter * 3*10^9 parameters * 0.8 * 10^12 tokens = 1.44e+22 FLOP",Telechat - PTD,,800000000000.0,Table 3,,"""Training TeleChat took one month (including downtime).""",NVIDIA A100 SXM4 40 GB,,Confident,"In this technical report, we present TeleChat, a collection of large language models (LLMs) with parameters of 3 billion, 7 billion and 12 billion. It includes pretrained language models as well as fine-tuned chat models that is aligned with human preferences. TeleChat is initially pretrained on an extensive corpus containing a diverse collection of texts from both English and Chinese languages, including trillions of tokens. Subsequently, the model undergoes fine-tuning to align with human preferences, following a detailed methodology that we describe. We evaluate the performance of TeleChat on various tasks, including language understanding, mathematics, reasoning, code generation, and knowledge-based question answering. Our findings indicate that TeleChat achieves comparable performance to other open-source models of similar size across a wide range of public benchmarks. To support future research and applications utilizing LLMs, we release the fine-tuned model checkpoints of TeleChat's 7B and 12B variant, along with code and a portion of our pretraining data, to the public community.",,,Unreleased,China,,,,640.0,,2025-06-09 11:43,,,,,,Industry,,,,,Unreleased,,Industry,,,,506239.5551067574,Operation counting,,,,
TeleChat-12B,Language,"Language modeling/generation,Chat,Question answering,Text summarization,Code generation,Translation",China Telecom,"Zhongjiang He, Zihan Wang, Xinzhang Liu, Shixuan Liu, Yitong Yao, Yuyao Huang, Xuelong Li, Yongxiang Li, Zhonghao Che, Zhaoxi Zhang, Yan Wang, Xin Wang, Luwen Pu, Huinan Xu, Ruiyu Fang, Yu Zhao, Jie Zhang, Xiaomeng Huang, Zhilong Lu, Jiaxin Peng, Wenjun Zheng, Shiquan Wang, Bingkai Yang, Xuewei he, Zhuoru Jiang, Qiyi Xie, Yanhan Zhang, Zhongqiu Li, Lingling Shi, Weiwei Fu, Yin Zhang, Zilu Huang, Sishi Xiong, Yuxiang Zhang, Chao Wang, Shuangyong Song",2024-04-01,TELECHAT TECHNICAL REPORT,https://arxiv.org/abs/2401.03804,10.0,,,12000000000.0,,8.64e+22,"80 nodes, each having 8 Nvidia A100 Sxm 40GB GPUs

6 FLOP / token / parameter * 12*10^9 parameters * 1.2 * 10^12 tokens = 8.64e+22 FLOP",Telechat - PTD,,1200000000000.0,Table 3,,"""Training TeleChat took one month (including downtime).""",NVIDIA A100 SXM4 40 GB,,Confident,"In this technical report, we present TeleChat, a collection of large language models (LLMs) with parameters of 3 billion, 7 billion and 12 billion. It includes pretrained language models as well as fine-tuned chat models that is aligned with human preferences. TeleChat is initially pretrained on an extensive corpus containing a diverse collection of texts from both English and Chinese languages, including trillions of tokens. Subsequently, the model undergoes fine-tuning to align with human preferences, following a detailed methodology that we describe. We evaluate the performance of TeleChat on various tasks, including language understanding, mathematics, reasoning, code generation, and knowledge-based question answering. Our findings indicate that TeleChat achieves comparable performance to other open-source models of similar size across a wide range of public benchmarks. To support future research and applications utilizing LLMs, we release the fine-tuned model checkpoints of TeleChat's 7B and 12B variant, along with code and a portion of our pretraining data, to the public community.",,,Open weights (restricted use),China,,,,640.0,,2025-06-09 11:43,,,,,,Industry,,,,,Open (restricted use),"Apache 2.0
https://huggingface.co/Tele-AI/TeleChat-12B

""Community use TeleChat model needs to follow ã€Š TeleChat model community license agreement ã€‹. The TeleChat model supports commercial use. If you plan to use the TeleChat model or its derivatives for commercial purposes, you need to contact the mailbox below tele_ai@chinatelecom.cn , Submit the application materials required by the ã€ŠTeleChat model community license agreementã€‹. After the review is approved, you will be granted a non-exclusive, global, non-transferable, non-relicable, revocable commercial copyright license.""",Industry,,,,506239.5551067574,Operation counting,Tele-AI,,,
ReALM,Language,"Named entity recognition,Language modeling,Part-of-speech tagging",Apple,"Joel Ruben Antony Moniz, Soundarya Krishnan, Melis Ozyildirim, Prathamesh Saraf, Halim Cagri Ates, Yuan Zhang, Hong Yu, Nidhi Rajshree",2024-03-29,ReALM: Reference Resolution As Language Modeling,https://arxiv.org/abs/2403.20329,,SOTA improvement,"""We show that ReaLM outperforms previous approaches, and performs roughly as well as the state-of-the-art LLM today, GPT-4, despite consisting of far fewer parameters.""

""We also benchmark against GPT-3.5 and GPT-4, with our smallest model achieving performance comparable to that of GPT-4, and our larger models substantially outperforming it.""",3000000000.0,Fine-tuned FLAN-T5 models ranging from 80M to 3B,,Fine-tuned from FLAN-T5,,"Mix of synthetic and manually annotated data. 

""Each data point contains the user query and a list of entities, along with the ground-truth entity (or set of entities) that are relevant to the corresponding user query. Each entity, in turn, contains information about its type and other properties such as the name and other textual details associated with the entity (the label and time of an alarm, for example)""",16300.0,2300 training examples from conversation; 3900 synthetically generated training examples; 10100 training examples using context from a phone screen.,,,,Supervised,Confident,"Reference resolution is an important problem, one that is essential to understand and successfully handle context of different kinds. This context includes both previous turns and context that pertains to non-conversational entities, such as entities on the user's screen or those running in the background. While LLMs have been shown to be extremely powerful for a variety of tasks, their use in reference resolution, particularly for non-conversational entities, remains underutilized. This paper demonstrates how LLMs can be used to create an extremely effective system to resolve references of various types, by showing how reference resolution can be converted into a language modeling problem, despite involving forms of entities like those on screen that are not traditionally conducive to being reduced to a text-only modality. We demonstrate large improvements over an existing system with similar functionality across different types of references, with our smallest model obtaining absolute gains of over 5% for on-screen references. We also benchmark against GPT-3.5 and GPT-4, with our smallest model achieving performance comparable to that of GPT-4, and our larger models substantially outperforming it.",,,Unreleased,United States of America,Flan-T5 11B,,No training details given,,,2025-06-04 17:15,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
Voice Engine,"Audio,Speech",Audio generation,OpenAI,,2024-03-29,Navigating the Challenges and Opportunities of Synthetic Voices,https://openai.com/blog/navigating-the-challenges-and-opportunities-of-synthetic-voices,,,,,,,,,,,,,,,,Unknown,,,,,United States of America,,,,,,2024-09-05 14:08,,,,,,Industry,,,,,,,Industry,,,,,,,,,
Grok-1.5,Language,"Language modeling,Chat",xAI,,2024-03-28,"Introducing Grok-1.5, our latest model capable of long context understanding and advanced reasoning. Grok-1.5 will be available to our early testers and existing Grok users on the ð• platform in the coming days.",https://x.ai/blog/grok-1.5,,,,,,9.26e+24,"Lower bound is taken from Grok-1 estimation
Upper bound is taken from Grok-2 estimation

geometric mean: 
sqrt(2.90000000001*29.6)*10^24 = 9.26e+24",Unspecified unreleased,,,,,,,,Speculative,,,,Hosted access (no API),United States of America,,,,,,2025-03-24 15:02,Oracle,,,,,Industry,,2.90000000001e+24,2.9599999999999996e+25,,Unreleased,"Musk noted that Grok-1.5 will power xAIâ€™s ChatGPT-challenging chatbot on the X platform, while Grok-2, the successor of the new model, is still in the training phase",Industry,,,,,,,,,
Grok-1.5V,"Multimodal,Language,Vision","Language modeling,Chat,Image captioning,Code autocompletion,Code generation,Visual question answering",xAI,,2024-03-28,"Introducing Grok-1.5V, our first-generation multimodal model. In addition to its strong text capabilities, Grok can now process a wide variety of visual information, including documents, diagrams, charts, screenshots, and photographs. Grok-1.5V will be available soon to our early testers and existing Grok users.",https://x.ai/blog/grok-1.5v,,,,,,,Lower bound is taken from Grok-1 estimation,Unspecified unreleased,,,,,,,,Speculative,,,,Hosted access (no API),United States of America,,,,,,2025-03-24 15:00,Oracle,,,,,Industry,,2.90000000001e+24,,,Unreleased,,Industry,,,,,,,,,
YandexGPT 3,Language,"Language modeling/generation,Chat,Question answering,Text summarization,Table tasks",Yandex,,2024-03-28,,https://ya.ru/ai/gpt-3,,,,,,,,,,,,,,,,Unknown,,,,API access,Russia,,,,,,2024-09-24 11:52,,,,,,Industry,,,,,,,Industry,,,,,,,,,
Jamba,Language,"Language modeling/generation,Chat",AI21 Labs,"Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedigos, Erez Safahi, Shaked Meirom, Yonatan Belinkov, Shai Shalev-Shwartz, Omri Abend, Raz Alon, Tomer Asida, Amir Bergman, Roman Glozman, Michael Gokhman, Avashalom Manevich, Nir Ratner, Noam Rozen, Erez Shwartz, Mor Zusman, Yoav Shoham",2024-03-28,Jamba: A Hybrid Transformer-Mamba Language Model,https://arxiv.org/abs/2403.19887,,,,51600000000.0,"51.6B from https://huggingface.co/ai21labs/Jamba-v0.1
(12B active parameters, 52B total available parameters)",,,Unspecified unreleased,"Jamba is trained on an in-house dataset that contains text data from the Web, books, and code, with the last update in March 2024.",,,,,NVIDIA H100 SXM5 80GB,,Confident,"We present Jamba, a new base large language model based on a novel hybrid Transformer-Mamba mixture-of-experts (MoE) architecture. Specifically, Jamba interleaves blocks of Transformer and Mamba layers, enjoying the benefits of both model families. MoE is added in some of these layers to increase model capacity while keeping active parameter usage manageable. This flexible architecture allows resource- and objective-specific configurations. In the particular configuration we have implemented, we end up with a powerful model that fits in a single 80GB GPU. Built at large scale, Jamba provides high throughput and small memory footprint compared to vanilla Transformers, and at the same time state-of-the-art performance on standard language model benchmarks and long-context evaluations. Remarkably, the model presents strong results for up to 256K tokens context length. We study various architectural decisions, such as how to combine Transformer and Mamba layers, and how to mix experts, and show that some of them are crucial in large scale modeling. We also describe several interesting properties of these architectures which the training and evaluation of Jamba have revealed, and plan to release checkpoints from various ablation runs, to encourage further exploration of this novel architecture. We make the weights of our implementation of Jamba publicly available under a permissive license.",,,Open weights (unrestricted),Israel,,,,,,2025-02-05 18:13,,,,,,Industry,,,,,Unreleased,Released with open weights under Apache 2.0,Industry,,,,,,,,,
DBRX,Language,"Chat,Code generation",Databricks,Mosaic Research Team,2024-03-27,Introducing DBRX: A New State-of-the-Art Open LLM,https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm,,Training cost,,132000000000.0,132B mixture of experts. 36B parameters active per inference,2.6e+24,"Mixture of Experts (MoE)

36 billion active params * 12 trillion tokens * 6 ~= 2.6e24
https://www.wolframalpha.com/input?i=6+FLOP+*+36+billion+*+12+trillion

also, it was trained on 3072 NVIDIA H100s, but with an unclear timeframe (end-end process was three months, including evals and red-teaming).",,"12T tokens, text and code

""It was pre-trained on 12T tokens of text and code data...

DBRX was pretrained on 12T tokens of carefully curated data and a maximum context length of 32k tokens. We estimate that this data is at least 2x better token-for-token than the data we used to pretrain the MPT family of models""

from HF: https://huggingface.co/databricks/dbrx-base

The training mix used for DBRX contains both natural-language and code examples. The vast majority of our training data is in the English language",9000000000000.0,"12T tokens is equivalent to 9T words. Though it includes code data, so not very literally 9T words",,,NVIDIA H100 SXM5 80GB,,Confident,"Today, we are excited to introduce DBRX, an open, general-purpose LLM created by Databricks. Across a range of standard benchmarks, DBRX sets a new state-of-the-art for established open LLMs. Moreover, it provides the open community and enterprises building their own LLMs with capabilities that were previously limited to closed model APIs; according to our measurements, it surpasses GPT-3.5, and it is competitive with Gemini 1.0 Pro. It is an especially capable code model, surpassing specialized models like CodeLLaMA-70B on programming, in addition to its strength as a general-purpose LLM.",1.0,,Open weights (restricted use),United States of America,,,,,,2025-05-28 16:02,,,,,,Industry,,,,,Unreleased,"license: https://www.databricks.com/legal/open-model-license
conditions based on monthly users",Industry,,,BF16,,Operation counting,databricks,,,16
ProstT5,Biology,Protein or nucleotide language model (pLM/nLM),"Technical University of Munich,Seoul National University,Institute for Advanced Study,TUM School of Life Sciences Weihenstephan","Michael Heinzinger, Konstantin Weissenow, Joaquin Gomez Sanchez, Adrian Henkel, Milot Mirdita, Martin Steinegger, Burkhard Rost",2024-03-24,Bilingual Language Model for Protein Sequence and Structure,https://www.biorxiv.org/content/10.1101/2023.07.23.550085v2.abstract,50.0,,,3000000000.0,,3.0999999999999906e+21,"1. Hardware setup: 8x NVIDIA A100 GPUs (3.12e14 FLOP/s per GPU)

2. Training duration: Directly provided - 36 days total (10 days pre-training + 26 days translation training) = 3,110,400 seconds

3. Utilization rate: 40%

4. Final calculation:
3.12e14 FLOP/s Ã— 8 GPUs Ã— 3,110,400s Ã— 0.4 = 3.1e21 FLOPs",,,8100000001.0,"Number of samples = 34M
Tokens per sequence = 238
Total = 34,000,000 Ã— 238 = 8,092,000,000 tokens â‰ˆ 8.1B",864.0,,NVIDIA A100,,Confident,"Adapting large language models (LLMs) to protein sequences spawned the development of powerful
protein language models (pLMs). Concurrently, AlphaFold2 broke through in protein structure prediction.
Now we can systematically and comprehensively explore the dual nature of proteins that act and exist
as three-dimensional (3D) machines and evolve as linear strings of one-dimensional (1D) sequences.
Here, we leverage pLMs to simultaneously model both modalities by combining 1D sequences with 3D
structure in a single model. We encode protein structures as token sequences using the 3Di-alphabet
introduced by the 3D-alignment method Foldseek. This new foundation pLM extracts the features and
patterns of the resulting â€œstructure-sequenceâ€ representation. Toward this end, we built a non-redundant
dataset from AlphaFoldDB and fine-tuned an existing pLM (ProtT5) to translate between 3Di and amino
acid sequences. As a proof-of-concept for our novel approach, dubbed Protein structure-sequence T5
(ProstT5), we showed improved performance for subsequent prediction tasks, and for â€œinverse foldingâ€,
namely the generation of novel protein sequences adopting a given structural scaffold (â€œfoldâ€). Our work
showcased the potential of pLMs to tap into the information-rich protein structure revolution fueled by
AlphaFold2. ProstT5 paves the way to develop new tools integrating the vast resource of 3D predictions,
and opens new research avenues in the post-AlphaFold2 era. Our model is freely available for all at
https://github.com/mheinzinger/ProstT5.",,,,"Germany,Korea (Republic of),United States of America,Germany",,,,8.0,,2025-05-01 10:42,,,,,,"Academia,Academia,Academia,Academia",,,,,,,"Academia,Academia,Academia,Academia",,,,6329.121902923846,Hardware,,,,
Suno v3,Audio,Audio generation,Suno,,2024-03-21,"Create full, two-minute songs in seconds with v3",https://suno.com/blog/v3,,,,,,,,,,,,,,,,Unknown,"At Suno, we are building a future where anyone can make music. You can make a song for any moment in any major language with just a few short words. Award-winning artists use Suno, but our core user base consists of everyday people making music â€” often for the first time.

Today, we are excited to introduce v3, our first model capable of producing radio-quality music. v3 enables you to make full, two-minute songs in seconds and is now available to all users at https://app.suno.ai. Make your own song with v3 today!",,,Hosted access (no API),United States of America,,,,,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
MiniGPT4 + LRV-Instruction,"Language,Vision,Multimodal","Language modeling/generation,Chat,Visual question answering,Image captioning","University of Maryland,Microsoft","Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, Lijuan Wang",2024-03-19,Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning,https://arxiv.org/abs/2306.14565,,,,7000000000.0,7B (Table 3),,,,"We build LRV-Instruction, a large and diverse dataset containing 400k visual instructions, with 16 vision and language tasks and negative instructions in different semantic levels and styles.

we leverage GPT4, instead of human workers, to build LRV-Instruction",4840000.0,"Figure 4:
400k instructions
Ave Instruction Length 12.1

12.1*400000=4840000",,,NVIDIA Quadro RTX 8000,,Speculative,"Despite the promising progress in multi-modal tasks, current large multi-modal models (LMMs) are prone to hallucinating inconsistent descriptions with respect to the associated image and human instructions. This paper addresses this issue by introducing the first large and diverse visual instruction tuning dataset, named Large-scale Robust Visual (LRV)-Instruction. Our dataset comprises 400k visual instructions generated by GPT4, covering 16 vision-and-language tasks with open-ended instructions and answers. Unlike existing studies that primarily focus on positive instruction samples, we design LRV-Instruction to include both positive and negative instructions for more robust visual instruction tuning. Our negative instructions are designed at three semantic levels: (i) Nonexistent Object Manipulation, (ii) Existent Object Manipulation and (iii) Knowledge Manipulation. To efficiently measure the hallucination generated by LMMs, we propose GPT4-Assisted Visual Instruction Evaluation (GAVIE), a stable approach to evaluate visual instruction tuning like human experts. GAVIE does not require human-annotated groundtruth answers and can adapt to diverse instruction formats. We conduct comprehensive experiments to investigate the hallucination of LMMs. Our results demonstrate existing LMMs exhibit significant hallucinations when presented with our negative instructions, particularly Existent Object and Knowledge Manipulation instructions. Moreover, we successfully mitigate hallucination by finetuning MiniGPT4 and mPLUG-Owl on LRV-Instruction while improving performance on several public datasets compared to state-of-the-art methods. Additionally, we observed that a balanced ratio of positive and negative instances in the training data leads to a more robust model. Code and data are available at this https URL.",,,Open weights (unrestricted),"United States of America,United States of America,Multinational,India,Belgium",MiniGPT4 (Vicuna finetune),200000000000000000,6ND = 6*7B*4840000=203280000*10^9=2*10^17,,,2024-12-02 10:24,,,,,,"Academia,Industry",,,,,Open source,"https://github.com/FuxiaoLiu/LRV-Instruction
BSD 3-Clause","Academia,Industry",,,,,Operation counting,,,,
JetFire (GPT2-LARGE),Language,Language modeling/generation,Tsinghua University,"Haocheng Xi, Yuxiang Chen, Kang Zhao, Kaijun Zheng, Jianfei Chen, Jun Zhu",2024-03-19,Jetfire: Efficient and Accurate Transformer Pretraining with INT8 Data Flow and Per-Block Quantization,https://arxiv.org/abs/2403.12422v1,,,,774000000.0,Table 4.,,,OPENWEBTEXT,"We evaluate our method by training three GPT2 (Radford et al., 2019) models with different sizes: GPT2-base for 300k steps, GPT2-medium for 200k steps, and GPT2-large for 120k steps on the OpenWebText (Peterson et al., 2019) dataset based on NanoGPT3 (Hyperparameters: Learning Rate = 1.5 Ã— 10âˆ’4, Weight Decay = 10âˆ’1). We report the validation loss and the fine tuning average accuracy on the GLUE (Wang et al., 2018a) dataset over 3 seeds. The results are shown in Table 4.",,,,,NVIDIA GeForce RTX 4090,,Confident,"Pretraining transformers are generally time-consuming. Fully quantized training (FQT) is a promising approach to speed up pretraining. However, most FQT methods adopt a quantize-compute-dequantize procedure, which often leads to suboptimal speedup and significant performance degradation when used in transformers due to the high memory access overheads and low-precision computations. In this work, we propose Jetfire, an efficient and accurate INT8 training method specific to transformers. Our method features an INT8 data flow to optimize memory access and a per-block quantization method to maintain the accuracy of pretrained transformers. Extensive experiments demonstrate that our INT8 FQT method achieves comparable accuracy to the FP16 training baseline and outperforms the existing INT8 training works for transformers. Moreover, for a standard transformer block, our method offers an end-to-end training speedup of 1.42x and a 1.49x memory reduction compared to the FP16 baseline.",,,,China,,,,,,2025-05-01 10:42,,,,,,Academia,,,,,,,Academia,,,,,,,,,
Stable Video 3D (SV3D),"Vision,3D modeling",3D reconstruction,Stability AI,"Vikram Voleti, Chun-Han Yao, Mark Boss, Adam Letts, David Pankratz, Dmitry Tochilkin, Christian Laforte, Robin Rombach, Varun Jampani",2024-03-18,SV3D: Novel Multi-view Synthesis and 3D Generation from a Single Image using Latent Video Diffusion,https://arxiv.org/abs/2403.12008,,,,,,5.1757056e+20,4*8*144*3600*312000000000000*0.3 / 3 models = 5.1757056e+20,Objaverse,"Training Dataset
We use renders from the Objaverse dataset, utilizing our enhanced rendering method that more closely replicate the distribution of images found in the real world, significantly improving our modelâ€™s ability to generalize. We selected a carefully curated subset of the Objaverse dataset for the training data, which is available under the CC-BY license.",,"""All three models (SV3Du, SV3Dc, SV3Dp) are trained for 105k iterations in
total (SV3Dp is trained unconditionally for 55k iterations and conditionally for 50k iterations), with an effective batch size of 64 on 4 nodes of 8 80GB A100 GPUs for around 6 days.""",144.0,"""on 4 nodes of 8 80GB A100 GPUs for around 6 days.""

6*24 = 144 hours",NVIDIA A100 SXM4 80 GB,,Confident,"We present Stable Video 3D (SV3D) -- a latent video diffusion model for high-resolution, image-to-multi-view generation of orbital videos around a 3D object. Recent work on 3D generation propose techniques to adapt 2D generative models for novel view synthesis (NVS) and 3D optimization. However, these methods have several disadvantages due to either limited views or inconsistent NVS, thereby affecting the performance of 3D object generation. In this work, we propose SV3D that adapts image-to-video diffusion model for novel multi-view synthesis and 3D generation, thereby leveraging the generalization and multi-view consistency of the video models, while further adding explicit camera control for NVS. We also propose improved 3D optimization techniques to use SV3D and its NVS outputs for image-to-3D generation. Extensive experimental results on multiple datasets with 2D and 3D metrics as well as user study demonstrate SV3D's state-of-the-art performance on NVS as well as 3D reconstruction compared to prior works.",,,Open weights (non-commercial),United Kingdom of Great Britain and Northern Ireland,,,,,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,"License: Stability AI Community License.
https://huggingface.co/stabilityai/sv3d",Industry,,,,,Hardware,,,,
ERNIE-RNA,Biology,Protein or nucleotide language model (pLM/nLM),"Microsoft Research,Syngentech,Tsinghua University","Weijie Yin, Zhaoyu Zhang, Liang He, Rui Jiang, Shuo Zhang, Gan Liu, Xuegong Zhang, Tao Qin, Zhen Xie",2024-03-17,ERNIE-RNA: An RNA Language Model with Structure-enhanced Representations,https://www.biorxiv.org/content/10.1101/2024.03.17.585376v1.abstract,3.0,,,86000000.0,,2.1000000000000016e+21,"1. Hardware: 24x V100 32GB GPUs (1.30e+14 FLOPs/s per GPU)
2. Training duration: 20 days (directly provided) = 1.728e+6 seconds
3. Utilization: 40%
4. Calculation: 
   24 GPUs Ã— 1.30e+14 FLOPs/s Ã— 1.728e+6 seconds Ã— 0.4 utilization = 2.1e+21 FLOPs",,,6120000000.0,"Number of sequences (20.4M) Ã— Maximum sequence length (1024)
Average assumed length ~300
20.4M*300=6120000000",480.0,,NVIDIA Tesla V100 DGXS 32 GB,,Confident,"With large amounts of unlabeled RNA sequences data produced by high-throughput sequencing technologies, pre-trained RNA language models have been developed to estimate semantic space of RNA molecules, which facilities the understanding of grammar of RNA language. However, existing RNA language models overlook the impact of structure when modeling the RNA semantic space, resulting in incomplete feature extraction and suboptimal performance across various downstream tasks. In this study, we developed a RNA pre-trained language model named ERNIE-RNA (Enhanced Representations with base-pairing restriction for RNA modeling) based on a modified BERT (Bidirectional Encoder Representations from Transformers) by incorporating base-pairing restriction with no MSA (Multiple Sequence Alignment) information. We found that the attention maps from ERNIE-RNA with no fine-tuning are able to capture RNA structure in the zero-shot experiment more precisely than conventional methods such as fine-tuned RNAfold and RNAstructure, suggesting that the ERNIE-RNA can provide comprehensive RNA structural representations. Furthermore, ERNIE-RNA achieved SOTA (state-of-the-art) performance after fine-tuning for various downstream tasks, including RNA structural and functional predictions. In summary, our ERNIE-RNA model provides general features which can be widely and effectively applied in various subsequent research tasks. Our results indicate that introducing key knowledge-based prior information in the BERT framework may be a useful strategy to enhance the performance of other language models.",,,,"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,China,China",,,,24.0,,2025-05-01 10:42,,,,,,"Industry,Industry,Academia",,,,,,,"Industry,Industry,Academia",,,,11868.95362272414,Hardware,,,,
MM1-30B,"Multimodal,Language,Vision","Chat,Image captioning",Apple,"Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Floris Weers, Anton Belyi, Haotian Zhang, Karanjeet Singh, Doug Kang, Ankur Jain, Hongyu HÃ¨, Max Schwarzer, Tom Gunter, Xiang Kong, Aonan Zhang, Jianyu Wang, Chong Wang, Nan Du, Tao Lei, Sam Wiseman, Guoli Yin, Mark Lee, Zirui Wang, Ruoming Pang, Peter Grasch, Alexander Toshev, Yinfei Yang",2024-03-14,"MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training",https://arxiv.org/abs/2403.09611,122.0,SOTA improvement,""" In particular, the pretrained model MM1 is SOTA, performing better than Emu2 [105], Flamingo [3],
and IDEFICS [47] on captioning and visual question answering (VQA) tasks
in few-shot settings, both in small and large size regimes""

Table 4: outperforms Gemini and GPT-4V on VQA",30000000000.0,30B,4.86e+23,"Pre-trained on ~2B image-text pairs and 2T tokens (Table 2). Each image is 144 tokens, so the images are ~300B tokens.
Then additional multimodal training for 400B tokens, for a total of ~2.7T tokens.

This is the final training recipe: ""We initialize both the image encoder and the underlying LLM decoder weights for MM1 from in-house pre-trained models2. We then perform multimodal pre-training on the above data mix for 200k steps (approx. 400B tokens).""

Compute  = 6ND = 6 * 2.7 trillion * 30 billion = 4.86e23

maybe the size of the visual connector is relevant","Conceptual Captions (CC3M),Conceptual Captions 12M (CC12M),COYO-700M,Unspecified unreleased,OBELICS","Text, captioned images. See Table 2",1500000000000.0,at least 2T tokens,,,,,Likely,"In this work, we discuss building performant Multimodal Large Language Models (MLLMs). In particular, we study the importance of various architecture components and data choices. Through careful and comprehensive ablations of the image encoder, the vision language connector, and various pre-training data choices, we identified several crucial design lessons. For example, we demonstrate that for large-scale multimodal pre-training using a careful mix of image-caption, interleaved image-text, and text-only data is crucial for achieving state-of-the-art (SOTA) few-shot results across multiple benchmarks, compared to other published pre-training results. Further, we show that the image encoder together with image resolution and the image token count has substantial impact, while the vision-language connector design is of comparatively negligible importance. By scaling up the presented recipe, we build MM1, a family of multimodal models up to 30B parameters, including both dense models and mixture-of-experts (MoE) variants, that are SOTA in pre-training metrics and achieve competitive performance after supervised fine-tuning on a range of established multimodal benchmarks. Thanks to large-scale pre-training, MM1 enjoys appealing properties such as enhanced in-context learning, and multi-image reasoning, enabling few-shot chain-of-thought prompting.",,,Unreleased,United States of America,,,,,,2025-02-14 14:36,,,,,,Industry,,,,,Unreleased,,Industry,,,,,Operation counting,,,,
ManiGaussian,"Robotics,Vision,Video",Robotic manipulation,"Tsinghua University,Nanyang Technological University,Carnegie Mellon University (CMU)","Guanxing Lu, Shiyi Zhang, Ziwei Wang, Changliu Liu, Jiwen Lu, Yansong Tang",2024-03-13,ManiGaussian: Dynamic Gaussian Splatting for Multi-task Robotic Manipulation,"https://arxiv.org/abs/2403.08321
https://guanxinglu.github.io/ManiGaussian/",53.0,SOTA improvement,"1 Introduction: ""We evaluate our ManiGaussian method on the RLBench dataset [27] with 10 tasks and 166 variants, where our method outperforms the state-of-the-art multi-task robotic manipulation methods by 13.1% in the average task success rate""",,,,,,,,,,"""All the compared methods are trained on two NVIDIA RTX 4090 GPUs for 100k iterations with a batch size of 2.""

https://github.com/GuanxingLu/ManiGaussian
Training: ""We train our ManiGaussian on two NVIDIA RTX 4090 GPUs for <2 days.""

Paper didn't specify precision, but GitHub code/weights might confirm - I gave a cursory look over the repo but didn't see anything obvious.

Assume BF16 -> 3.3e14 FLOP/s/GPU 
Assume <2 days = <48 hr  ->  42 hr = 151200 s
Assume 0.3 utilization
0.3 * 2 GPU * 3.3e14 FLOP/s/GPU * 151200 s ~= 2.99e19 FLOP


 ",NVIDIA GeForce RTX 4090,,Confident,"Performing language-conditioned robotic manipulation tasks in unstructured environments is highly demanded for general intelligent robots. Conventional robotic manipulation methods usually learn semantic representation of the observation for action prediction, which ignores the scene-level spatiotemporal dynamics for human goal completion. In this paper, we propose a dynamic Gaussian Splatting method named ManiGaussian for multi-task robotic manipulation, which mines scene dynamics via future scene reconstruction. Specifically, we first formulate the dynamic Gaussian Splatting framework that infers the semantics propagation in the Gaussian embedding space, where the semantic representation is leveraged to predict the optimal robot action. Then, we build a Gaussian world model to parameterize the distribution in our dynamic Gaussian Splatting framework, which provides informative supervision in the interactive environment via future scene reconstruction. We evaluate our ManiGaussian on 10 RLBench tasks with 166 variations, and the results demonstrate our framework can outperform the state-of-the-art methods by 13.1\% in average success rate. Project page: this https URL.",,,Open weights (unrestricted),"China,Singapore,United States of America",,,,2.0,,2025-06-02 13:51,,,,,,"Academia,Academia,Academia",,,,,Open source,"MIT license
https://github.com/GuanxingLu/ManiGaussian","Academia,Academia,Academia",,,,1780.5016389530874,,,,,
Command R,Language,"Language modeling/generation,Language generation,Translation,Code autocompletion","Cohere,Cohere for AI",,2024-03-11,,https://cohere.com/blog/command-r,,,,35000000000.0,https://huggingface.co/CohereForAI/c4ai-command-r-v01,,,Unspecified unreleased,,,,,,,,Confident,"Today, we are introducing Command R, a new LLM aimed at large-scale production workloads. Command R targets the emerging â€œscalableâ€ category of models that balance high efficiency with strong accuracy, enabling companies to move beyond proof of concept, and into production.

Command R is a generative model optimized for long context tasks such as retrieval-augmented generation (RAG) and using external APIs and tools. It is designed to work in concert with our industry-leading Embed and Rerank models to provide best-in-class integration for RAG applications and excel at enterprise use cases. As a model built for companies to implement at scale, Command R boasts: 
- Strong accuracy on RAG and Tool Use
- Low latency, and high throughput
- Longer 128k context and lower pricing
- Strong capabilities across 10 key languages
- Model weights available on HuggingFace for research and evaluation",,,Open weights (non-commercial),"Canada,Multinational,Canada",,,,,,2025-05-01 10:42,,,,,,"Industry,Industry",checked,,,,Unreleased,Not 100% sure that open weight release is identical to the API version. Weights are CC-BY-NC 4.0.,"Industry,Industry",,,,,,,,,
RFM-1,"Robotics,Vision,Video","Robotic manipulation,Image captioning,Video description",Covariant,"Andrew Sohn, Anusha Nagabandi, Carlos Florensa, Daniel Adelberg, Di Wu, Hassan Farooq, Ignasi Clavera, Jeremy Welborn, Juyue Chen, Nikhil Mishra, Peter Chen, Peter Qian, Pieter Abbeel, Rocky Duan, Varun Vijay, Yang Liu",2024-03-11,Introducing RFM-1: Giving robots human-like reasoning capabilities,https://covariant.ai/insights/introducing-rfm-1-giving-robots-human-like-reasoning-capabilities/,,,,8000000000.0,8b,2.4e+20,"6 FLOP / token / parameter * 5000000000 tokens * 8*10^9 parameters = 2.4e+20 FLOP

I am not confident about amount of epochs and whether the model is dense",,,5000000000.0,"from here https://covariant.ai/insights/rfm-1-update-higher-quality-grasp-accuracy/

5*10^9 tokens",,,,,Likely,"What is RFM-1
Set up as a multimodal any-to-any sequence model, RFM-1 is an 8 billion parameter transformer trained on text, images, videos, robot actions, and a range of numerical sensor readings.

By tokenizing all modalities into a common space and performing autoregressive next-token prediction, RFM-1 uses its broad range of input and output modalities to enable diverse applications.

For example, it can perform image-to-image learning for scene analysis tasks like segmentation and identification. It can combine text instructions with image observations to generate desired grasp actions or motion sequences. It can pair a scene image with a targeted grasp image to predict outcomes as videos or simulate the numerical sensor readings that would occur along the way.",,,Unreleased,United States of America,,,,,,2025-06-13 15:29,,,,,,Industry,,,,,Unreleased,,Industry,,,,,Operation counting,,,,
HAM-TTS,Speech,"Text-to-speech,Speech synthesis","Geely Automobile Research Institute (Ningbo) Company,National Institute of Informatics,Shanghai Jiao Tong University","Chunhui Wang, Chang Zeng, Bowen Zhang, Ziyang Ma, Yefan Zhu, Zifeng Cai, Jian Zhao, Zhonglin Jiang, Yong Chen",2024-03-09,HAM-TTS: Hierarchical Acoustic Modeling for Token-Based Zero-Shot Text-to-Speech with Model and Data Scaling,https://arxiv.org/abs/2403.05989,,,,800000000.0,,," training was carried out over a total of
400k steps",,,,,,,NVIDIA A100,,Confident,"Token-based text-to-speech (TTS) models have emerged as a promising avenue for generating natural and realistic speech, yet they grapple with low pronunciation accuracy, speaking style and timbre inconsistency, and a substantial need for diverse training data. In response, we introduce a novel hierarchical acoustic modeling approach complemented by a tailored data augmentation strategy and train it on the combination of real and synthetic data, scaling the data size up to 650k hours, leading to the zero-shot TTS model with 0.8B parameters. Specifically, our method incorporates a latent variable sequence containing supplementary acoustic information based on refined self-supervised learning (SSL) discrete units into the TTS model by a predictor. This significantly mitigates pronunciation errors and style mutations in synthesized speech. During training, we strategically replace and duplicate segments of the data to enhance timbre uniformity. Moreover, a pretrained few-shot voice conversion model is utilized to generate a plethora of voices with identical content yet varied timbres. This facilitates the explicit learning of utterance-level one-to-many mappings, enriching speech diversity and also ensuring consistency in timbre. Comparative experiments (Demo page: this https URL our model's superiority over VALL-E in pronunciation precision and maintaining speaking style, as well as timbre continuity.",,,Unreleased,"China,Japan,China",,,,512.0,,2025-06-16 13:15,,,,,,"Industry,Academia",,,,,Unreleased,demo page: https://anonymous.4open.science/w/ham-tts/,"Industry,Academia",,,,405199.1321316636,,,,,
Derm Foundational Model,"Vision,Medicine","Image embedding,Medical diagnosis,Cancer diagnosis,Image classification",Google Research,"Dave Steiner, Rory Pilgrim",2024-03-08,Health-specific embedding tools for dermatology and pathology,https://research.google/blog/health-specific-embedding-tools-for-dermatology-and-pathology/,,,,928000000.0,"""The model is a BiT-M ResNet101x3""

BiT-M has 928M parameters, I assume same here",,,Unspecified unreleased,"""It is pre-trained on large amounts of labeled skin images to produce 6144 dimensional embeddings that capture dense features relevant for analyzing these images""

""Base model (pre-training): A large number of health-related image-text pairs from the public web
SFT (supervised fine-tuned) model: tele-dermatology datasets from the United States and Colombia, a skin cancer dataset from Australia, and additional public images. The images come from a mix of device types, including images from smartphone cameras, other cameras, and dermatoscopes. The images also have a mix of image takers; images may have been taken by clinicians during consultations or self-captured by patients.""",,,,,,,Likely,"Derm Foundation is a machine learning (ML) model that produces embeddings based on dermatology images. The embeddings can be used to efficiently build AI models for dermatology image related tasks, requiring less data and less compute than having to fully train a model without the embeddings or the pretrained model.

Trained on large scale datasets, Derm Foundation helps businesses and institutions in healthcare and life sciences do more with their dermatology data with less data, accelerating their ability to build AI models for dermatology image analysis.",,,Open weights (restricted use),"Multinational,United States of America,Canada,Switzerland",Big Transfer (BiT-M),,,,,2025-05-01 10:42,,,,,,Industry,,,,,Open source,"https://huggingface.co/google/derm-foundation
not for clinical use

https://github.com/Google-Health/derm-foundation
source code under Apache 2.0 but no training dataset",Industry,,,,,,google,,,
Inflection-2.5,Language,Chat,Inflection AI,,2024-03-07,Inflection-2.5: meet the world's best personal AI,https://inflection.ai/inflection-2-5,,Significant use,one million daily users; six million monthly,,,1.0001e+25,"""Inflection-1 used approximately 4% the training FLOPs of GPT-4 and, on average, performed at approximately 72% GPT-4 level on a diverse range of IQ-oriented tasks. Inflection-2.5, now powering Pi, achieves more than 94% the average performance of GPT-4 despite using only 40% the training FLOPs.""

This is a weird one - we estimated GPT-4 at 2.1e25 FLOP (which could be off somewhat, or Inflection could believe a different number). 40% of that is ~8e24. But Inflection 2, the previous model, was trained on ~1e25 FLOP per Inflection. Inflection-2.5 also does better on benchmarks than 2. Intuitively Inflection-2.5 would be trained on appreciably more compute. 

1e25 seems like a rough, perhaps conservative guess given all this.",,,,,,,NVIDIA H100 SXM5 80GB,,Speculative,"At Inflection, our mission is to create a personal AI for everyone. Last May, we released Piâ€”a personal AI, designed to be empathetic, helpful, and safe. In November we announced a new major foundation model, Inflection-2, the second best LLM in the world at the time.

Now we are adding IQ to Piâ€™s exceptional EQ.

We are launching Inflection-2.5, our upgraded in-house model that is competitive with all the world's leading LLMs like GPT-4 and Gemini. It couples raw capability with our signature personality and unique empathetic fine-tuning. Inflection-2.5 is available to all Pi's users today, at pi.ai, on iOS, on Android, or our new desktop app.",,,Hosted access (no API),United States of America,,,,,,2025-02-19 13:35,,,,,,Industry,,,,,Unreleased,,Industry,,,,,Comparison with other models,,,,9
BaseFold,Biology,Protein folding prediction,Basecamp Research,"Geraldene Munsamy, Tanggis Bohnuud, Philipp Lorenz",2024-03-06,IMPROVING ALPHAFOLD2 PERFORMANCE WITH A GLOBAL METAGENOMIC & BIOLOGICAL DATA SUPPLY CHAIN,https://www.biorxiv.org/content/10.1101/2024.03.06.583325v1,,,,,,,,BRD (Basecamp Research Data),,300000000001.0,"Number of Sequences (1 x 10â¹) Ã— Average Residues per Sequence (300) = 3 Ã— 10Â¹Â¹ datapoints

Final estimate: 3 Ã— 10Â¹Â¹",,,NVIDIA A100,,Unknown,"Scaling laws suggest that more than a trillion species inhabit our planet but only a miniscule and unrepresentative fraction (less than 0.00001%) have been studied or sequenced to date. Deep learning models, including those applied to tasks in the life sciences, depend on the quality and size of training or reference datasets. Given the large knowledge gap we experience when it comes to life on earth, we present a data-centric approach to improving deep learning models in Biology: We built partnerships with nature parks and biodiversity stakeholders across 5 continents covering 50% of global biomes, establishing a global metagenomics and biological data supply chain. With higher protein sequence diversity captured in this dataset compared to existing public data, we apply this data advantage to the protein folding problem by MSA supplementation during inference of AlphaFold2. Our model, BaseFold, exceeds traditional AlphaFold2 performance across targets from the CASP15 and CAMEO, 60% of which show improved pLDDT scores and RMSD values being reduced by up to 80%. On top of this, the improved quality of the predicted structures can yield better docking results. By sharing benefits with the stakeholders this data originates from, we present a way of simultaneously improving deep learning models for biology and incentivising protection of our planetâ€™s biodiversity.",,,Unreleased,United Kingdom of Great Britain and Northern Ireland,AlphaFold 2,,,,,2025-06-11 18:22,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
GroundingGPT,"Multimodal,Language,Video,Vision,Speech","Chat,Language modeling/generation,Image embedding,Image captioning,Video description,Speech recognition","ByteDance,Fudan University","Zhaowei Li, Qi Xu, Dong Zhang, Hang Song, Yiqing Cai, Qi Qi, Ran Zhou, Junting Pan, Zefeng Li, Van Tu Vu, Zhida Huang, Tao Wang",2024-03-05,GroundingGPT:Language Enhanced Multi-modal Grounding Model,https://arxiv.org/abs/2401.06071v5,,,,7000000000.0,,,,"LLaVA-Pretrain-595k,Valley-Pretrain-703k,Wavcaps,COCO,OCR-VQA,TextVQA,VisualGenome,Flickr30K Entities,DiDeMo,VGGSS,VCR ,Activitynet Captions,Clotho","Stage1
Image LLaVA-Pretrain-595k
Video Valley-Pretrain-703k
Audio Wavcaps
Stage2
Image RefCOCO, RefCOCOg, RefCOCO+, Visual Genome
Video DiDeMo, HiREST, Charades-STA, Didemo
Audio VGGSS
Stage3
Image LLava-1.5-mix665k, Flickr30k Entities, VCR
Video Valley-Instruct-73k, Videochat-Instruct-11k, Activitynet Captions
Audio Clotho",,can probably be extracted from their github dataset preparation guide,,,NVIDIA A100,,Confident,"Multi-modal large language models have demonstrated impressive performance across various tasks in different modalities. However, existing multi-modal models primarily emphasize capturing global information within each modality while neglecting the importance of perceiving local information across modalities. Consequently, these models lack the ability to effectively understand the fine-grained details of input data, limiting their performance in tasks that require a more nuanced understanding. To address this limitation, there is a compelling need to develop models that enable fine-grained understanding across multiple modalities, thereby enhancing their applicability to a wide range of tasks. In this paper, we propose GroundingGPT, a language enhanced multi-modal grounding model. Beyond capturing global information like other multi-modal models, our proposed model excels at tasks demanding a detailed understanding of local information within the input. It demonstrates precise identification and localization of specific regions in images or moments in videos. To achieve this objective, we design a diversified dataset construction pipeline, resulting in a multi-modal, multi-granularity dataset for model training. The code, dataset, and demo of our model can be found at https: //github.com/lzw-lzw/GroundingGPT.",1.0,,Open weights (unrestricted),"China,China",Vicuna-7B v0,,,8.0,,2025-03-27 09:43,,,,,,"Industry,Academia",,,,,Open source,"https: //github.com/lzw-lzw/GroundingGPT

 Apache-2.0 license","Industry,Academia",,,,6331.800435296151,,,,,
Claude 3 Sonnet,"Multimodal,Language,Vision","Chat,Image captioning,Code generation,Language modeling/generation",Anthropic,,2024-03-04,"The Claude 3 Model Family: Opus, Sonnet, Haiku",https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf,,Training cost,"Based on leaks, Claude 3 Opus and Sonnet probably cost >$1M to train.",,,,,Unspecified unreleased,"Claude 3 models are trained on a proprietary mix of publicly available information on the Internet as of August 2023, as well as non-public data from third parties, data provided by data labeling services and paid contractors, and data we generate internally. We employ several data cleaning and filtering methods, including deduplication and classification. The Claude 3 suite of models have not been trained on any user prompt or output data submitted to us by users or customers, including free users, Claude Pro users, and API customers.
This has a knowledge cutoff date of August 2023, according to https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf, which I assume means August 1, 2023.",,,,"Like its predecessors, Claude 3 models employ various training methods, such as unsupervised learning and Constitutional AI [6]. These models were trained using hardware from Amazon Web Services (AWS) and Google Cloud Platform (GCP)",,,Unknown,"We introduce Claude 3, a new family of large multimodal models â€“ Claude 3 Opus, our most capable offering, Claude 3 Sonnet, which provides a combination of skills and speed, and Claude 3 Haiku, our fastest and least expensive model. All new models have vision capabilities that enable them to process and analyze image data. The Claude 3 family demonstrates strong performance across benchmark evaluations and sets a new standard on
measures of reasoning, math, and coding. Claude 3 Opus achieves state-of-the-art results on evaluations like GPQA [1], MMLU [2], MMMU [3] and many more. Claude 3 Haiku performs as well or better than Claude 2 [4] on most pure-text tasks, while Sonnet and Opus significantly outperform it. Additionally, these models exhibit improved fluency in non-English languages, making them more versatile for a global audience. In this report, we provide an in-depth analysis of our evaluations, focusing on core capabilities, safety, societal impacts, and the catastrophic risk assessments we committed to in our Responsible Scaling Policy [5].
",,,API access,United States of America,,,,,,2025-05-12 19:54,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
Claude 3 Opus,"Multimodal,Language,Vision","Chat,Image captioning,Code generation,Language modeling/generation",Anthropic,,2024-03-04,"The Claude 3 Model Family: Opus, Sonnet, Haiku",https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf,,"SOTA improvement,Training cost","Based on leaks, Claude 3 Opus and Sonnet probably cost >$1M to train.",,,1.6400009999999998e+25,Training compute estimated from benchmark scores.,Unspecified unreleased,"Claude 3 models are trained on a proprietary mix of publicly available information on the Internet as of August 2023, as well as non-public data from third parties, data provided by data labeling services and paid contractors, and data we generate internally. We employ several data cleaning and filtering methods, including deduplication and classification. The Claude 3 suite of models have not been trained on any user prompt or output data submitted to us by users or customers, including free users, Claude Pro users, and API customers.
This has a knowledge cutoff date of August 2023, according to https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf, which I assume means August 1, 2023.",,,,"Like its predecessors, Claude 3 models employ various training methods, such as unsupervised learning and Constitutional AI [6]. These models were trained using hardware from Amazon Web Services (AWS) and Google Cloud Platform (GCP)",,,Speculative,"We introduce Claude 3, a new family of large multimodal models â€“ Claude 3 Opus, our most capable offering, Claude 3 Sonnet, which provides a combination of skills and speed, and Claude 3 Haiku, our fastest and least expensive model. All new models have vision capabilities that enable them to process and analyze image data. The Claude 3 family demonstrates strong performance across benchmark evaluations and sets a new standard on
measures of reasoning, math, and coding. Claude 3 Opus achieves state-of-the-art results on evaluations like GPQA [1], MMLU [2], MMMU [3] and many more. Claude 3 Haiku performs as well or better than Claude 2 [4] on most pure-text tasks, while Sonnet and Opus significantly outperform it. Additionally, these models exhibit improved fluency in non-English languages, making them more versatile for a global audience. In this report, we provide an in-depth analysis of our evaluations, focusing on core capabilities, safety, societal impacts, and the catastrophic risk assessments we committed to in our Responsible Scaling Policy [5].
",,,API access,United States of America,,,,,,2025-05-12 19:55,,,,,,Industry,,,,,Unreleased,,Industry,,,,,Benchmarks,,,,4
Aramco Metabrain AI,Language,Language modeling/generation,Saudi Aramco,Saudi Aramco,2024-03-04,Saudi Aramco unveils industryâ€™s first generative AI model,https://www.offshore-technology.com/news/saudi-aramco-unveils-industry-first-generative-ai-model/,,Training cost,,250000000000.0,"""It has 250 billion parameters that are adjustable during training to generate outputs or make predictions.""",1.05e+25,6*250B*7T=1.05e+25,,"""The AI was trained using seven trillion data points, collecting more than 90 years of company history.""",7000000000000.0,,,,,Supervised,Likely,,,,Unreleased,Saudi Arabia,,,,,,2025-05-26 18:11,,,,,,"Industry,Government",,,,,Unreleased,,"Industry,Government",,,,,Operation counting,,,,7
MACE-MP-0,Materials science,Molecular simulation,"University of Cambridge,Federal Institute of Materials Research and Testing (BAM),""NERSC, Lawrence Berkeley National Laboratory"",University of British Columbia (UBC),Friedrich Schiller University Jena,University of Bayreuth,Fritz Haber Institute of the Max Planck Society,U. S. Naval Research Laboratory,Chemix,Daresbury Laboratory,BASF,University of South Carolina,University of Stuttgart,Uppsala University,Newcastle University,Technical University of Denmark,Aix-Marseille UniversitÃ©,University of Warwick,University of California Los Angeles (UCLA),InstaDeep,University of California (UC) Berkeley","Ilyes Batatia, Philipp Benner, Yuan Chiang, Alin M. Elena, DÃ¡vid P. KovÃ¡cs, Janosh Riebesell, Xavier R. Advincula, Mark Asta, Matthew Avaylon, William J. Baldwin, Fabian Berger, Noam Bernstein, Arghya Bhowmik, Samuel M. Blau, Vlad CÄƒrare, James P. Darby, Sandip De, Flaviano Della Pia, Volker L. Deringer, Rokas ElijoÅ¡ius, Zakariya El-Machachi, Fabio Falcioni, Edvin Fako, Andrea C. Ferrari, Annalena Genreith-Schriever, Janine George, Rhys E. A. Goodall, Clare P. Grey, Petr Grigorev, Shuang Han, Will Handley, Hendrik H. Heenen, Kersti Hermansson, Christian Holm, Jad Jaafar, Stephan Hofmann, Konstantin S. Jakob, Hyunwook Jung, Venkat Kapil, Aaron D. Kaplan, Nima Karimitari, James R. Kermode, Namu Kroupa, Jolla Kullgren, Matthew C. Kuner, Domantas Kuryla, Guoda Liepuoniute, Johannes T. Margraf, Ioan-Bogdan MagdÄƒu, Angelos Michaelides, J. Harry Moore, Aakash A. Naik, Samuel P. Niblett, Sam Walton Norwood, Niamh O'Neill, Christoph Ortner, Kristin A. Persson, Karsten Reuter, Andrew S. Rosen, Lars L. Schaaf, Christoph Schran, Benjamin X. Shi, Eric Sivonxay, TamÃ¡s K. Stenczel, Viktor Svahn, Christopher Sutton, Thomas D. Swinburne, Jules Tilly, Cas van der Oord, Eszter Varga-Umbrich, Tejs Vegge, Martin VondrÃ¡k, Yangshuai Wang, William C. Witt, Fabian Zills, GÃ¡bor CsÃ¡nyi",2024-03-01,A foundation model for atomistic materials chemistry,https://arxiv.org/abs/2401.00096,,,,,,8.760959999999999e+20,312000000000000*2600*3600*0.3 = 8.76096e+20,Materials Project,"""public database of 150k inorganic crystals""

""The MACE-MP-0 model was trained on the MPtrj dataset which was compiled originally for CHGNet (24). This dataset consists of a large number of static calculations and structural optimization trajectories from the Materials Project (MP) (19). These include approx. 1.5M configurations
(roughly ten times the approx. 150k unique MP structures)""",,,,"""Models are trained for 200 epochs on 40â€“80 NVIDIA A100 GPUs across 10â€“20 nodes. Training the medium-sized model took approx. 2,600 GPU hours.""",NVIDIA A100,,Confident,"Machine-learned force fields have transformed the atomistic modelling of materials by enabling simulations of ab initio quality on unprecedented time and length scales. However, they are currently limited by: (i) the significant computational and human effort that must go into development and validation of potentials for each particular system of interest; and (ii) a general lack of transferability from one chemical system to the next. Here, using the state-of-the-art MACE architecture we introduce a single general-purpose ML model, trained on a public database of 150k inorganic crystals, that is capable of running stable molecular dynamics on molecules and materials. We demonstrate the power of the MACE-MP-0 model - and its qualitative and at times quantitative accuracy - on a diverse set problems in the physical sciences, including the properties of solids, liquids, gases, chemical reactions, interfaces and even the dynamics of a small protein. The model can be applied out of the box and as a starting or ""foundation model"" for any atomistic system of interest and is thus a step towards democratising the revolution of ML force fields by lowering the barriers to entry.",200.0,,Open weights (unrestricted),"United Kingdom of Great Britain and Northern Ireland,Germany,United States of America,Canada,Germany,Germany,Germany,United States of America,United States of America,United Kingdom of Great Britain and Northern Ireland,Germany,United States of America,Germany,Sweden,United Kingdom of Great Britain and Northern Ireland,Denmark,France,United Kingdom of Great Britain and Northern Ireland,United States of America,United Kingdom of Great Britain and Northern Ireland,United States of America",,,,80.0,,2025-05-01 10:42,,,,,,"Academia,Academia,Government,Academia,Academia,Academia,Academia,Government,Industry,Academia,Industry,Academia,Academia,Academia,Academia,Academia,Academia,Academia,Academia,Industry,Academia",,,,2600.0,Open source,"MIT License

https://github.com/ACEsuit/mace-mp
https://github.com/ACEsuit/mace/","Academia,Academia,Government,Academia,Academia,Academia,Academia,Government,Industry,Academia,Industry,Academia,Academia,Academia,Academia,Academia,Academia,Academia,Academia,Industry,Academia",,,,63323.64481276617,Hardware,,,,
StarCoder 2 15B,Language,"Code generation,Code autocompletion","Hugging Face,ServiceNow,NVIDIA,BigCode","Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, Tianyang Liu, Max Tian, Denis Kocetkov, Arthur Zucker, Younes Belkada, Zijian Wang, Qian Liu, Dmitry Abulkhanov, Indraneil Paul, Zhuang Li, Wen-Ding Li, Megan Risdal, Jia Li, Jian Zhu, Terry Yue Zhuo, Evgenii Zheltonozhskii, Nii Osae Osae Dade, Wenhao Yu, Lucas KrauÃŸ, Naman Jain, Yixuan Su, Xuanli He, Manan Dey, Edoardo Abati, Yekun Chai, Niklas Muennighoff, Xiangru Tang, Muhtasham Oblokulov, Christopher Akiki, Marc Marone, Chenghao Mou, Mayank Mishra, Alex Gu, Binyuan Hui, Tri Dao, Armel Zebaze, Olivier Dehaene, Nicolas Patry, Canwen Xu, Julian McAuley, Han Hu, Torsten Scholak, Sebastien Paquet, Jennifer Robinson, Carolyn Jane Anderson, Nicolas Chapados, Mostofa Patwary, Nima Tajbakhsh, Yacine Jernite, Carlos MuÃ±oz Ferrandis, Lingming Zhang, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, Harm de Vries",2024-02-29,StarCoder 2 and The Stack v2: The Next Generation,https://arxiv.org/abs/2402.19173,,,,15000000000.0,15B,3.87e+23,estimation is given in Table 6 ,The Stack v2,See Table 4. The Stack V2 plus some extras. Created from repositorites from Github with permissive licences.,913230000000.0,from Table 4,,,,,Confident,"The BigCode project, an open-scientific collaboration focused on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder2. In partnership with Software Heritage (SWH), we build The Stack v2 on top of the digital commons of their source code archive. Alongside the SWH repositories spanning 619 programming languages, we carefully select other high-quality data sources, such as GitHub pull requests, Kaggle notebooks, and code documentation. This results in a training set that is 4x larger than the first StarCoder dataset. We train StarCoder2 models with 3B, 7B, and 15B parameters on 3.3 to 4.3 trillion tokens and thoroughly evaluate them on a comprehensive set of Code LLM benchmarks. We find that our small model, StarCoder2-3B, outperforms other Code LLMs of similar size on most benchmarks, and also outperforms StarCoderBase-15B. Our large model, StarCoder2- 15B, significantly outperforms other models of comparable size. In addition, it matches or outperforms CodeLlama-34B, a model more than twice its size. Although DeepSeekCoder- 33B is the best-performing model at code completion for high-resource languages, we find that StarCoder2-15B outperforms it on math and code reasoning benchmarks, as well as several low-resource languages. We make the model weights available under an OpenRAIL license and ensure full transparency regarding the training data by releasing the SoftWare Heritage persistent IDentifiers (SWHIDs) of the source code data. ",4.49,,Open weights (restricted use),"Multinational,United States of America,United States of America,United States of America",,,,,,2025-02-17 15:38,,,,4100000.0,Table 7,"Industry,Industry,Industry",,,,,Unreleased,"commercial use allowed, but various use cases restricted: https://huggingface.co/spaces/bigcode/bigcode-model-license-agreement

code is fine-tune only: https://github.com/bigcode-project/starcoder2?tab=readme-ov-file#fine-tuning ","Industry,Industry,Industry",,,,,Reported,,,,
Griffin,Language,"Language modeling/generation,Chat",Google DeepMind,"Soham De, Samuel L. Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, Guillaume Desjardins, Arnaud Doucet, David Budden, Yee Whye Teh, Razvan Pascanu, Nando De Freitas, Caglar Gulcehre",2024-02-29,Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models,https://arxiv.org/abs/2402.19427,,,,14000000000.0,,1.5848931924611137e+22,"Figure 1.a
10^(1/5)*10^22 = 15848931924611134852021.0137339150701326944213382503906831629",,,300000000000.0,,,,Google TPU v3,,Confident,"Recurrent neural networks (RNNs) have fast inference and scale efficiently on long sequences, but they are difficult to train and hard to scale. We propose Hawk, an RNN with gated linear recurrences, and Griffin, a hybrid model that mixes gated linear recurrences with local attention. Hawk exceeds the reported performance of Mamba on downstream tasks, while Griffin matches the performance of Llama-2 despite being trained on over 6 times fewer tokens. We also show that Griffin can extrapolate on sequences significantly longer than those seen during training. Our models match the hardware efficiency of Transformers during training, and during inference they have lower latency and significantly higher throughput. We scale Griffin up to 14B parameters, and explain how to shard our models for efficient distributed training.",,,Unreleased,"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France",,,,,,2025-06-16 14:59,,,,,,Industry,,,,,Unreleased,,Industry,,,,,Reported,,,,
Hawk,Language,"Language modeling/generation,Chat",Google DeepMind,"Soham De, Samuel L. Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, Guillaume Desjardins, Arnaud Doucet, David Budden, Yee Whye Teh, Razvan Pascanu, Nando De Freitas, Caglar Gulcehre",2024-02-29,Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models,https://arxiv.org/abs/2402.19427,,,,7000000000.0,"""All three model families are trained at a range of model scales from 100M to 7B parameters, with an additional Griffin model with 14 billion parameters.""",3.95e+21,"Figure 1.a
Digitized and found 3.95e21 for biggest Hawk model",,,300000000000.0,Table 1,,,Google TPU v3,,Confident,"Recurrent neural networks (RNNs) have fast inference and scale efficiently on long sequences, but they are difficult to train and hard to scale. We propose Hawk, an RNN with gated linear recurrences, and Griffin, a hybrid model that mixes gated linear recurrences with local attention. Hawk exceeds the reported performance of Mamba on downstream tasks, while Griffin matches the performance of Llama-2 despite being trained on over 6 times fewer tokens. We also show that Griffin can extrapolate on sequences significantly longer than those seen during training. Our models match the hardware efficiency of Transformers during training, and during inference they have lower latency and significantly higher throughput. We scale Griffin up to 14B parameters, and explain how to shard our models for efficient distributed training.",,,Unreleased,"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France",,,,,,2025-06-16 15:00,,,,,,Industry,,,,,Unreleased,,Industry,,,,,Reported,,,,
Humanoid Locomotion,Robotics,Animal (human/non-human) imitation,University of California (UC) Berkeley,"Ilija Radosavovic, Bike Zhang, Baifeng Shi, Jathushan Rajasegaran, Sarthak Kamat, Trevor Darrell, Koushil Sreenath, Jitendra Malik",2024-02-29,Humanoid Locomotion as Next Token Prediction,https://arxiv.org/abs/2402.19469,28.0,,"No SOTA tag because there is no recognized benchmark, but does appear to improve on SOTA for metrics like trajectory adherence, positional tracking error (see figures 5-8).",8000000.0,"Largest model trained has 8M params. Note actual model used in real-world experiments appears to be 2M, to improve latency. See sections 5.1 and 5.9.",,,,"4 sources. First, they use a neural network policy pretrained with RL, and collect 10k trajectories of 10s each using the Agility Roboticsâ€™ simulator on flat ground, without domain randomization. Second, 10k trajectories of 10s each from the model-based controller developed by Agility Robotics. Third, a 1k trajectory subset of motion capture (MoCap) recordings of humans from the KIT dataset. Fourth, a collection of YouTube videos which are converted to trajectories with inverse kinematics (no details about how many).",,,,,,,Likely,"We cast real-world humanoid control as a next token prediction problem, akin to predicting the next word in language. Our model is a causal transformer trained via autoregressive prediction of sensorimotor trajectories. To account for the multi-modal nature of the data, we perform prediction in a modality-aligned way, and for each input token predict the next token from the same modality. This general formulation enables us to leverage data with missing modalities, like video trajectories without actions. We train our model on a collection of simulated trajectories coming from prior neural network policies, model-based controllers, motion capture data, and YouTube videos of humans. We show that our model enables a full-sized humanoid to walk in San Francisco zero-shot. Our model can transfer to the real world even when trained on only 27 hours of walking data, and can generalize to commands not seen during training like walking backward. These findings suggest a promising path toward learning challenging real-world control tasks by generative modeling of sensorimotor trajectories.",,,Unreleased,United States of America,,,,,,2024-11-01 10:03,,,,,,Academia,,,,,,,Academia,,,,,,,,,
YOLOv9-E,Vision,Object detection,"Academia Sinica,National Taipei University of Technology,Chung Yuan Christian University","Chien-Yao Wang, I-Hau Yeh, Hong-Yuan Mark Liao",2024-02-29,YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information,https://arxiv.org/abs/2402.13616,,,,57300000.0,57.3M,1.74258e+17,"6ND = 6*57300000.00 parameters *118000 images *500 epochs = 2.02842e+16 (likely an underestimation)

(speculative confidence because I don't know the amount of tokens per image)

assuming(!) batch size 64 -> number of updates per epoch = 118000/64 = 1844

 189.0 Gigaflops * 1844 * 500 = 1.74258e+17",COCO,,118000.0,"Training set: Approximately 118,000 images (COCO-2017), no information about about tokenization / patching / batching",,,,,Likely,"Today's deep learning methods focus on how to design the most appropriate objective functions so that the prediction results of the model can be closest to the ground truth. Meanwhile, an appropriate architecture that can facilitate acquisition of enough information for prediction has to be designed. Existing methods ignore a fact that when input data undergoes layer-by-layer feature extraction and spatial transformation, large amount of information will be lost. This paper will delve into the important issues of data loss when data is transmitted through deep networks, namely information bottleneck and reversible functions. We proposed the concept of programmable gradient information (PGI) to cope with the various changes required by deep networks to achieve multiple objectives. PGI can provide complete input information for the target task to calculate objective function, so that reliable gradient information can be obtained to update network weights. In addition, a new lightweight network architecture -- Generalized Efficient Layer Aggregation Network (GELAN), based on gradient path planning is designed. GELAN's architecture confirms that PGI has gained superior results on lightweight models. We verified the proposed GELAN and PGI on MS COCO dataset based object detection. The results show that GELAN only uses conventional convolution operators to achieve better parameter utilization than the state-of-the-art methods developed based on depth-wise convolution. PGI can be used for variety of models from lightweight to large. It can be used to obtain complete information, so that train-from-scratch models can achieve better results than state-of-the-art models pre-trained using large datasets, the comparison results are shown in Figure 1. The source codes are at: this https URL.",500.0,,Open weights (unrestricted),"Taiwan,Taiwan,Taiwan",,,,,,2025-05-01 10:42,,,,,,"Academia,Academia,Academia",,,,,Open source,"GPL-3.0 license (copyleft). train and validate code:
https://github.com/WongKinYiu/yolov9?tab=GPL-3.0-1-ov-file","Academia,Academia,Academia",,,,,Operation counting,,,,
RiNALMo,Biology,"RNA structure prediction,RNA splice-site prediction,Mean ribosome load prediction","University of Zagreb,Genome Institute of Singapore,Bioinformatics Institute","Rafael Josip PeniÄ‡, Tin VlaÅ¡iÄ‡, Roland G. Huber, Yue Wan, Mile Å ikiÄ‡",2024-02-29,RiNALMo: General-Purpose RNA Language Models Can Generalize Well on Structure Prediction Tasks,https://arxiv.org/abs/2403.00043,16.0,,,650000000.0,,1.0500000000000004e+21,"1. Hardware: 7x NVIDIA A100 GPUs (3.12e14 FLOP/s per GPU)
2. Training duration: 2 weeks (directly provided) = 1,209,600 seconds
3. Utilization: 40% (0.4)
4. Calculation: 3.12e14 FLOP/s Ã— 7 GPUs Ã— 1,209,600s Ã— 0.4 = 1.05e21 FLOPs",,,,"17,000,000 RNA sequences",336.0,,NVIDIA A100,,Confident,"Ribonucleic acid (RNA) plays a variety of crucial roles in fundamental biological processes. Recently, RNA has become an interesting drug target, emphasizing the need to improve our understanding of its structures and functions. Over the years, sequencing technologies have produced an enormous amount of unlabeled RNA data, which hides important knowledge and potential. Motivated by the successes of protein language models, we introduce RiboNucleic Acid Language Model (RiNALMo) to help unveil the hidden code of RNA. RiNALMo is the largest RNA language model to date with 650 million parameters pre-trained on 36 million non-coding RNA sequences from several available databases. RiNALMo is able to extract hidden knowledge and capture the underlying structure information implicitly embedded within the RNA sequences. RiNALMo achieves state-of-the-art results on several downstream tasks. Notably, we show that its generalization capabilities can overcome the inability of other deep learning methods for secondary structure prediction to generalize on unseen RNA families. The code has been made publicly available on this https URL.",,,,"Croatia,Singapore,Singapore",,,,7.0,,2025-05-01 10:42,,,,,,"Academia,Academia,Government",,,,,,,"Academia,Academia,Government",,,,5540.942313044689,Hardware,,,,
PTM-Mamba,Biology,Protein or nucleotide language model (pLM/nLM),Duke University,"Zhangzhi Peng, Benjamin Schussheim, Pranam Chatterjee",2024-02-29,PTM-Mamba: A PTM-Aware Protein Language Model with Bidirectional Gated Mamba Blocks,https://www.biorxiv.org/content/10.1101/2024.02.28.581983v1,12.0,,,,,,"
",,,32000001.0,"79,707 sequences Ã— 400 tokens/sequence = 31,882,800 tokens â‰ˆ 3.2 Ã— 10â· tokens",,,NVIDIA A100,,Likely,"Proteins serve as the workhorses of living organisms, orchestrating a wide array of vital functions. Post-translational modifications (PTMs) of their amino acids greatly influence the structural and functional diversity of different protein types and uphold proteostasis, allowing cells to swiftly respond to environmental changes and intricately regulate complex biological processes. To this point, efforts to model the complex features of proteins have involved the training of large and expressive protein language models (pLMs) such as ESM-2 and ProtT5, which accurately encode structural, functional, and physicochemical properties of input protein sequences. However, the over 200 million sequences that these pLMs were trained on merely scratch the surface of proteomic diversity, as they neither input nor account for the effects of PTMs. In this work, we fill this major gap in protein sequence modeling by introducing PTM tokens into the pLM training regime. We then leverage recent advancements in structured state space models (SSMs), specifically Mamba, which utilizes efficient hardware-aware primitives to overcome the quadratic time complexities of Transformers. After adding a comprehensive set of PTM tokens to the model vocabulary, we train bidirectional Mamba blocks whose outputs are fused with state-of-the-art ESM-2 embeddings via a novel gating mechanism. We demonstrate that our resultant PTM-aware pLM, PTM-Mamba, improves upon ESM-2â€™s performance on various PTM-specific tasks. PTM-Mamba is the first and only pLM that can uniquely input and represent both wild-type and PTM sequences, motivating downstream modeling and design applications specific to post-translationally modified proteins. To facilitate PTM-aware protein language modeling applications, we have made our model available at: https://huggingface.co/ChatterjeeLab/PTM-Mamba.",,,Open weights (non-commercial),United States of America,ESM2-650M,,,8.0,,2025-06-16 11:32,,,,,,Academia,,,,,Open source,"CC-BY-NC-ND-4.0 for weights
https://huggingface.co/ChatterjeeLab/PTM-Mamba

Apache 2.0 for inference and training code
https://github.com/programmablebio/ptm-mamba?tab=readme-ov-file",Academia,,,,6332.505500622502,Hardware,ChatterjeeLab,,,
Protllm,Biology,Protein or nucleotide language model (pLM/nLM),"Beijing Institute of Technology,Beihang University,Peking University,Smart Grid Research Institute,Shanghai AI Lab","Le Zhuo, Zewen Chi, Minghao Xu, Heyan Huang, Heqi Zheng, Conghui He, Xian-Ling Mao, Wentao Zhang",2024-02-28,ProtLLM: An Interleaved Protein-Language LLM with Protein-as-Word Pre-Training,https://arxiv.org/abs/2403.07920,8.0,,,,,,"
","UniProtKB,Mol instructions",,1310720000.0,"Summary of calculations:

PubMed Articles: 165,206 Ã— 10,000 = 1.65206 Ã— 10^9
UniProt Annotations: 64,634 Ã— 100 = 6.4634 Ã— 10^6
STRING Annotations: 25,682 Ã— 100 = 2.5682 Ã— 10^6
Mol-Instructions: 173,973 Ã— 500 = 86.9865 Ã— 10^6

Dataset total: 1.65206 Ã— 10^9 + 6.4634 Ã— 10^6 + 2.5682 Ã— 10^6 + 86.9865 Ã— 10^6 = 1.7475 Ã— 10^9

Training tokens: 10,000 Ã— 256 Ã— 512 = 1.31072 Ã— 10^9

Final estimate: 1.31 Ã— 10^9 data points

Training step based estimate: 
10000 steps with batch size 256 and length 512 (Table 4)
10000*256*512=1310720000

",,,NVIDIA A100,,Confident,"We propose ProtLLM, a versatile cross-modal large language model (LLM) for both protein-centric and protein-language tasks. ProtLLM features a unique dynamic protein mounting mechanism, enabling it to handle complex inputs where the natural language text is interspersed with an arbitrary number of proteins. Besides, we propose the protein-as-word language modeling approach to train ProtLLM. By developing a specialized protein vocabulary, we equip the model with the capability to predict not just natural language but also proteins from a vast pool of candidates. Additionally, we construct a large-scale interleaved protein-text dataset, named InterPT, for pre-training. This dataset comprehensively encompasses both (1) structured data sources like protein annotations and (2) unstructured data sources like biological research papers, thereby endowing ProtLLM with crucial knowledge for understanding proteins. We evaluate ProtLLM on classic supervised protein-centric tasks and explore its novel protein-language applications. Experimental results demonstrate that ProtLLM not only achieves superior performance against protein-specialized baselines on protein-centric tasks but also induces zero-shot and in-context learning capabilities on protein-language tasks.",,,Unreleased,"China,China,China,China,China",LLaMA-7B,,,4.0,,2025-06-12 13:36,,,,,,"Academia,Academia,Academia,Academia",,,,,Open (non-commercial),"no clear license for training and inference code
https://github.com/ProtLLM/ProtLLM/tree/main

weights are announced but not released
https://huggingface.co/ProtLLM","Academia,Academia,Academia,Academia",,,,3166.3232615544166,Operation counting,,,,
Evo,Biology,Protein or nucleotide language model (pLM/nLM),"Stanford University,University of California (UC) Berkeley,Together","Eric Nguyen, Michael Poli, Matthew G. Durrant, Armin W. Thomas, Brian Kang, Jeremy Sullivan, Madelena Y. Ng, Ashley Lewis, Aman Patel, Aaron Lou, Stefano Ermon, Stephen A. Baccus, Tina Hernandez-Boussard, Christopher RÃ©, Patrick D. Hsu, Brian L. Hie",2024-02-27,Sequence modeling and design from molecular to genome scale with Evo,"https://arcinstitute.org/news/blog/evo
https://www.biorxiv.org/content/10.1101/2024.02.27.582234v1",44.0,,Competitive with SOTA protein language models,7000000000.0,"Based on a StripedHyena architecture, context length of 131 kilobases",2.0000000001e+22,"""In total, Evo was trained on approximately 340B tokens, using approximately 2e22 FLOPS""",OpenGenome,"""We're also open-sourcing a large 300B token training dataset we compiled, which we call OpenGenome, consisting of 2.7M publicly available prokaryotic and phage genomes""",300000000000.0,"300 billion nucleotides. Since these are tokenized at the single-nucleotide level, 300B tokens.",2968.0,"Trained for first stage on 64 H100s, then for second stage on 128 A100s. Trained primarily with BF16 precision; FP32 used for ""long convolutional parameters"". I use BF16 performance for simplicity.

This gives a plausible range for completing 2e22 FLOPs.

If all 2e22 FLOPs were performed on H100s:
2e22 FLOP / (64 GPUs * 1.3e14 FLOP/GPU-sec) * 1/0.3 efficiency *  1/3600 hour/sec  = 2226 hours

If all 2e22 FLOPs were performed on A100s:
2e22 FLOP / (128 GPUs * 3.9e13 FLOP/GPU-sec) * 1/0.3 efficiency * 1/3600 hour/sec = 3710 hours

Assume each stage is roughly half the total time, so:
(2226+3710)/2 = 2968 hours","NVIDIA A100,NVIDIA H100 SXM5 80GB",Self-supervised learning,Confident,"The genome is a sequence that completely encodes the DNA, RNA, and proteins that orchestrate the function of a whole organism. Advances in machine learning combined with massive datasets of whole genomes could enable a biological foundation model that accelerates the mechanistic understanding and generative design of complex molecular interactions. We report Evo, a genomic foundation model that enables prediction and generation tasks from the molecular to genome scale. Using an architecture based on advances in deep signal processing, we scale Evo to 7 billion parameters with a context length of 131 kilobases (kb) at single-nucleotide, byte resolution. Trained on whole prokaryotic genomes, Evo can generalize across the three fundamental modalities of the central dogma of molecular biology to perform zero-shot function prediction that is competitive with, or outperforms, leading domain-specific language models. Evo also excels at multielement generation tasks, which we demonstrate by generating synthetic CRISPR-Cas molecular complexes and entire transposable systems for the first time. Using information learned over whole genomes, Evo can also predict gene essentiality at nucleotide resolution and can generate coding-rich sequences up to 650 kb in length, orders of magnitude longer than previous methods. Advances in multi-modal and multi-scale learning with Evo provides a promising path toward improving our understanding and control of biology across multiple levels of complexity.",1.0,,Open weights (unrestricted),"United States of America,United States of America,United States of America",,,,,,2025-05-09 11:32,,,,,,"Academia,Academia,Industry",,,,,,Apache License 2.0,"Academia,Academia,Industry",,,,,Reported,,,,
BitNet b1.58,Language,"Language modeling/generation,Question answering","University of Chinese Academy of Sciences,Microsoft Research","Shuming Ma, Hongyu Wang, Lingxiao Ma, Lei Wang, Wenhui Wang, Shaohan Huang, Li Dong, Ruiping Wang, Jilong Xue, Furu Wei",2024-02-27,The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits,https://arxiv.org/abs/2402.17764,,,,70000000000.0,,2.8735486e+22,"6ND = 6*70*10^9*100*10^9 = 42000000000000000000000 (4.2e+22)

Figure 3 suggests it 41.2 times more energy efficient than LLAMA 70B (which is estimated 8.1e+23 FLOPS  -> 1.9660194e+22 FLOPs 

geometric mean -> 2.8735486e+22",RedPajama,,100000000000.0,"""we pre-trained the models on the RedPajama dataset [Com23] for 100 billion tokens.""",,,,,Confident,"Recent research, such as BitNet, is paving the way for a new era of 1-bit Large Language Models (LLMs). In this work, we introduce a 1-bit LLM variant, namely BitNet b1.58, in which every single parameter (or weight) of the LLM is ternary {-1, 0, 1}. It matches the full-precision (i.e., FP16 or BF16) Transformer LLM with the same model size and training tokens in terms of both perplexity and end-task performance, while being significantly more cost-effective in terms of latency, memory, throughput, and energy consumption. More profoundly, the 1.58-bit LLM defines a new scaling law and recipe for training new generations of LLMs that are both high-performance and cost-effective. Furthermore, it enables a new computation paradigm and opens the door for designing specific hardware optimized for 1-bit LLMs.",,,,"China,United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational",,,,,,2025-05-01 10:42,,,,,,"Academia,Industry",,,,,,,"Academia,Industry",,,,,"Operation counting,Comparison with other models",,,,
Nemotron-4 15B,Language,"Language modeling/generation,Code generation,Question answering,Translation,Quantitative reasoning",NVIDIA,"Jupinder Parmar, Shrimai Prabhumoye, Joseph Jennings, Mostofa Patwary, Sandeep Subramanian, Dan Su, Chen Zhu, Deepak Narayanan, Aastha Jhunjhunwala, Ayush Dattagupta, Vibhu Jawa, Jiwei Liu, Ameya Mahabaleshwarkar, Osvald Nitski, Annika Brundyn, James Maki, Miguel Martinez, Jiaxuan You, John Kamalu, Patrick LeGresley, Denys Fridman, Jared Casper, Ashwath Aithal, Oleksii Kuchaiev, Mohammad Shoeybi, Jonathan Cohen, Bryan Catanzaro",2024-02-27,Nemotron-4 15B Technical Report,https://arxiv.org/abs/2402.16819,,,,15000000000.0,15b,7.5005116e+23,"6ND = 6 FLOP/token/parameter * 15*10^9 parameters * 8*10^12 tokens = 7.2e+23 FLOP

""Nemotron-4 was trained using 384 DGX H100 nodes; each node contains 8 H100 80GB SXM5 GPUs based on the NVIDIA Hopper architecture (NVIDIA, 2022). Each H100 GPU has a peak throughput of 989 teraFLOP/s when doing 16-bit floating point (bfloat16) arithmetic without sparsity.

Table 2 reports more detailed training schedule:

989*10^12 FLOP/sec * 3600 sec/hour * 24 hours * (768 gpus * 0.343 [reported utilization] * 0.8 days +  1536 gpus * 0.333 [reported utilization] * 0.4 days + 2304 gpus * 0.305 [reported utilization] * 11.9 days) = 7.5005116e+23 FLOP",Unspecified unreleased,"""At a high-level, the data blend is split into three different types of data: English natural language data (70%), multilingual natural language data (15%), and source-code data (15%).""",8000000000000.0,"""15-billion-parameter large multilingual language model trained on 8 trillion text tokens""",312.0,"""Training was completed in approximately 13 calendar days.""",NVIDIA H100 SXM5 80GB,,Confident,"We introduce Nemotron-4 15B, a 15-billion-parameter large multilingual language model trained on 8 trillion text tokens. Nemotron-4 15B demonstrates strong performance when assessed on English, multilingual, and coding tasks: it outperforms all existing similarly-sized open models on 4 out of 7 downstream evaluation areas and achieves competitive performance to the leading open models in the remaining ones. Specifically, Nemotron-4 15B exhibits the best multilingual capabilities of all similarly-sized models, even outperforming models over four times larger and those explicitly specialized for multilingual tasks.",,,Unreleased,United States of America,,,,3072.0,0.305,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,,Industry,,,BF16,4255633.23275038,"Operation counting,Hardware",,,,
Palmyra Vision,"Vision,Language","Visual question answering,Video description,Image captioning",Writer,,2024-02-27,"Meet Palmyra Vision, our multimodal LLM with vision capabilities",https://writer.com/blog/palmyra-vision/,,,,,,,,Unspecified unreleased,,,,,,,,Unknown,"Palmyra Vision is a multimodal large language model (LLM) with vision capabilities developed by Writer that can analyze and generate text based on images.
It excels in tasks such as extracting handwritten text, classifying objects, analyzing graphs and charts, and answering specific questions based on visual inputs. Palmyra Vision achieved a score of 84.4% on VQAv2 benchmark, outperforming other prominent multimodal models.
Palmyra Vision offers a range of practical applications in the enterprise, including product description generation, interpreting charts and graphs, compliance detection, improving accessibility by creating ALT descriptions, and text extraction from handwritten reports.",,,Hosted access (no API),United States of America,,,,,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
Mistral Large,Language,Chat,Mistral AI,,2024-02-26,"Mistral Large, our new flagship model",https://mistral.ai/news/mistral-large/,,Training cost,"~$20M training cost: https://www.wsj.com/tech/ai/the-9-month-old-ai-startup-challenging-silicon-valleys-giants-ee2e4c48
https://x.com/EMostaque/status/1762152740938031484?s=20 ",,,1.1199999999999999e+25,"https://www.wsj.com/tech/ai/the-9-month-old-ai-startup-challenging-silicon-valleys-giants-ee2e4c48

Mistral spent <20 million euro (meaning approximately 20 million?) to train Mistral Large:

https://x.com/EMostaque/status/1762152740938031484?s=20
""assuming this is on H100s with @Scaleway who are â‚¬1.9/hour => 10m H100 hours (c 30m A100 hrs), 3 months at 4k H100s :timer_clock:"" -Emad Mostaque

Assuming bf16 or fp16, H100 SXM performance is 989 TFLOPS
At 1.9 euro per H100-hour and 30% utilization, spending 20M euro produces 1.12*10^25 FLOP.
https://www.wolframalpha.com/input?i=20+million+%2F+%281.9%2Fhour%29+*+989+TFLOPS+*+0.30 ",,,,,2500.0,Speculation by Emad Mostaque: 20M euro spent at Scaleway (1.9 euro per H100-hour) would be around 3 months on 4000 H100s.,NVIDIA H100 SXM5 80GB,,Likely,,,,API access,France,,,,,,2025-02-14 14:38,,,,,,Industry,,,,,Unreleased,,Industry,,,,,Cost,,,,5
ProLLaMA,Biology,Protein question answering,"Peking University,Peng Cheng Laboratory","Liuzhenghao Lv, Zongying Lin, Hao Li, Yuyang Liu, Jiaxi Cui, Calvin Yu-Chian Chen, Li Yuan, Yonghong Tian",2024-02-26,ProLLaMA: A Protein Language Model for Multi-Task Protein Language Processing,https://arxiv.org/abs/2402.16445,13.0,,,7000000000.0,,8.412e+22,"1. Hardware setup: 8x NVIDIA RTX A6000 GPUs (3.87e13 FLOP/s per GPU)

2. Training duration: Provided directly - Stage 1: 6 days, Stage 2: 5 days (Total: 11 days = 950,400 seconds)

3. Utilization rate: 40%

4. Calculation:
(8 GPUs Ã— 3.87e13 FLOP/s/GPU) Ã— 950,400 seconds Ã— 0.4 utilization = 1.2e20 FLOPs
(Stage 1: 1.603e20 + Stage 2: 1.338e20) Ã— 0.4 = 1.2e20 FLOPs

Base model: 8.4e+22
total : 84120000000000000000000",UniRef50,,16000000001.0,"52,807,283 sequences Ã— 300 residues/sequence = 15,842,184,900 tokens â‰ˆ 1.6 Ã— 10^10 tokens",264.0,,NVIDIA RTX A6000,,Confident,"Large Language Models (LLMs) have achieved remarkable performance in multiple Natural Language Processing (NLP) tasks. Under the premise that protein sequences constitute the protein language, Protein Language Models(PLMs) have advanced the field of protein engineering. However, as of now, unlike LLMs in NLP, PLMs cannot handle the protein understanding task and the protein generation task simultaneously in the Protein Language Processing (PLP) field. This prompts us to delineate the inherent limitations in current PLMs: (i) the lack of natural language capabilities, (ii) insufficient instruction understanding, and (iii) high training resource demands. To address these challenges, we introduce a training framework to transform any general LLM into a PLM capable of handling multiple PLP tasks. To improve training efficiency, we propose Protein Vocabulary Pruning (PVP) for general LLMs. We construct a multi-task instruction dataset containing 13 million samples with superfamily information, facilitating better modeling of protein sequence-function landscapes. Through these methods, we develop the ProLLaMA model, the first known PLM to handle multiple PLP tasks simultaneously. Experiments show that ProLLaMA achieves state-of-the-art results in the unconditional protein sequence generation task. In the controllable protein sequence generation task, ProLLaMA can design novel proteins with desired functionalities. As for the protein understanding task, ProLLaMA achieves a 62\% exact match rate in superfamily prediction. Codes, model weights, and datasets are available at \url{this https URL} and \url{this https URL}.",,,,"China,China",Llama 2-7B,,,8.0,,2025-05-01 10:42,,,,,,"Academia,Academia",,,,,,,"Academia,Academia",,,,4749.696433127339,Hardware,,,,
DecompDiff,Biology,Drug discovery,"University of Illinois Urbana-Champaign (UIUC),ByteDance,University of Chinese Academy of Sciences,Chinese Academy of Sciences,Tsinghua University","Jiaqi Guan, Xiangxin Zhou, Yuwei Yang, Yu Bao, Jian Peng, Jianzhu Ma, Qiang Liu, Liang Wang, Quanquan Gu",2024-02-26,DecompDiff: Diffusion Models with Decomposed Priors for Structure-Based Drug Design,https://arxiv.org/abs/2403.07902,41.0,,,,"The key/value/query embedding is obtained through a 2-layer MLP with LayerNorm and ReLU activation. Stacking these three layers as a block, our model consists of 6 blocks with hidden dim=128 and n heads=16.",1.9e+19,"1. Hardware setup: 1x NVIDIA A100 GPU (3.12e14 FLOPs/s for FP16)
2. Training duration: 41.7 hours (provided directly) = 150,120 seconds
3. Utilization rate: 40% (assumed)
4. Calculation: 3.12e14 FLOPs/s Ã— 1 GPU Ã— 150,120s Ã— 0.4 = 1.873e19 FLOPs",,,100001.0,"""100,000 complexes are selected for training"" (diffusion based model)",41.7,,NVIDIA A100,,Confident,"Designing 3D ligands within a target binding site is a fundamental task in drug discovery. Existing structured-based drug design methods treat all ligand atoms equally, which ignores different roles of atoms in the ligand for drug design and can be less efficient for exploring the large drug-like molecule space. In this paper, inspired by the convention in pharmaceutical practice, we decompose the ligand molecule into two parts, namely arms and scaffold, and propose a new diffusion model, DECOMPDIFF, with decomposed priors over arms and scaffold. In order to facilitate the decomposed generation and improve the properties of the generated molecules, we incorporate both bond diffusion in the model and additional validity guidance in the sampling phase. Extensive experiments on CrossDocked2020 show that our approach achieves state-of-the-art performance in generating high-affinity molecules while maintaining proper molecular properties and conformational stability, with up to âˆ’8.39 Avg. Vina Dock score and 24.5% Success Rate. The code is provided at https://github. com/bytedance/DecompDiff",,,,"United States of America,China,China,China,China",,,,1.0,,2025-05-01 10:42,,,,,,"Academia,Industry,Academia,Academia,Academia",,,,,,,"Academia,Industry,Academia,Academia,Academia",,,,434.9538858175219,Hardware,,,,
Gemma 1.1 7B Instruct,Language,"Language modeling/generation,Question answering",Google,"Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Laurent Sifre, Morgane RiviÃ¨re, Mihir Sanjay Kale, Juliette Love, Pouya Tafti, LÃ©onard Hussenot and et al.",2024-02-24,,https://huggingface.co/google/gemma-1.1-7b-it,,,,8540000000.0,"Safetensors 
Model size 8.54B params",3.0744e+23,6ND = 6*6000000000000*8540000000=3.0744e+23,Unspecified unreleased,"""These models were trained on a dataset of text data that includes a wide variety of sources, totaling 6 trillion tokens. Here are the key components:

Web Documents: A diverse collection of web text ensures the model is exposed to a broad range of linguistic styles, topics, and vocabulary. Primarily English-language content.
Code: Exposing the model to code helps it to learn the syntax and patterns of programming languages, which improves its ability to generate code or understand code-related questions.
Mathematics: Training on mathematical text helps the model learn logical reasoning, symbolic representation, and to address mathematical queries.""",6000000000000.0,"""These models were trained on a dataset of text data that includes a wide variety of sources, totaling 6 trillion tokens. """,,,Google TPU v5e,,Confident,"This is Gemma 1.1 7B (IT), an update over the original instruction-tuned Gemma release.

Gemma 1.1 was trained using a novel RLHF method, leading to substantial gains on quality, coding capabilities, factuality, instruction following and multi-turn conversation quality. We also fixed a bug in multi-turn conversations, and made sure that model responses don't always start with ""Sure,"".

We believe this release represents an improvement for most use cases, but we encourage users to test in their particular applications. The previous model will continue to be available in the same repo. We appreciate the enthusiastic adoption of Gemma, and we continue to welcome all feedback from the community.",,,Open weights (restricted use),United States of America,,,,,,2025-05-16 10:30,,,,,,Industry,,,,,Unreleased,"https://huggingface.co/google/gemma-1.1-7b-it

""This repository is publicly accessible, but you have to accept the conditions to access its files and content.""",Industry,,,,,Operation counting,,,,
MegaScale (Production),Language,Language modeling/generation,"ByteDance,Peking University","Ziheng Jiang, Haibin Lin, Yinmin Zhong, Qi Huang, Yangrui Chen, Zhi Zhang, Yanghua Peng, Xiang Li, Cong Xie, Shibiao Nong, Yulu Jia, Sun He, Hongmin Chen, Zhihao Bai, Qi Hou, Shipeng Yan, Ding Zhou, Yiyao Sheng, Zhuo Jiang, Haohan Xu, Haoran Wei, Zhang Zhang, Pengfei Nie, Leqi Zou, Sida Zhao, Liang Xiang, Zherui Liu, Zhe Li, Xiaoying Jia, Jianxi Ye, Xin Jin, Xin Liu",2024-02-23,"MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs",https://arxiv.org/abs/2402.15627,40.0,SOTA improvement,Improves SOTA in FLOP utilization for distributed LLM training by 1.34X.,530000000000.0,"Production run is stated to have ""hundreds of billions of parameters"". Since the authors also do a number of experiments with a 530B model, I speculate they've used 530B for the production model.",3.9e+24,"Speculative. The model is stated to have trained for ""several weeks"". Assuming 530B parameters and ""several"" = 3, compute can be estimated from the 175B model's stated PFLOP/sec:
2166.3 aggregate PFlops/sec * 3 weeks * 7 days/week * 24 hours/day * 3600 seconds/hour = 3.9e+24.
As an upper bound, say 8e+24. ",,,,"Speculative. Authors note production system was trained on ""multi-trillions of tokens"". This could refer to training for multiple epochs on the same 300B tokens used to train the 175B and 530B models outlined in more detail in the paper. Alternatively, it could refer to a larger dataset of perhaps 3-9 trillion tokens.",504.0,"Speculative. Authors state ""several weeks"". For analysis, I've assumed this means around 3 weeks.",NVIDIA A100,Self-supervised learning,Speculative,"We present the design, implementation and engineering experience in building and deploying MegaScale, a production system for training large language models (LLMs) at the scale of more than 10,000 GPUs. Training LLMs at this scale brings unprecedented challenges to training efficiency and stability. We take a full-stack approach that co-designs the algorithmic and system components across model block and optimizer design, computation and communication overlapping, operator optimization, data pipeline, and network performance tuning. Maintaining high efficiency throughout the training process (i.e., stability) is an important consideration in production given the long extent of LLM training jobs. Many hard stability issues only emerge at large scale, and in-depth observability is the key to address them. We develop a set of diagnosis tools to monitor system components and events deep in the stack, identify root causes, and derive effective techniques to achieve fault tolerance and mitigate stragglers. MegaScale achieves 55.2% Model FLOPs Utilization (MFU) when training a 175B LLM model on 12,288 GPUs, improving the MFU by 1.34x compared to Megatron-LM. We share our operational experience in identifying and fixing failures and stragglers. We hope by articulating the problems and sharing our experience from a systems perspective, this work can inspire future LLM systems research.",,,Unreleased,"China,China",,,,12288.0,0.48,2025-05-09 11:32,,ByteDance MegaScale training cluster,,,,"Industry,Academia",,,8.01e+24,,Unreleased,"Code for MegaScale (also called veScale) training system are released under Apache Licence: https://github.com/volcengine/vescale
The model itself is unreleased.","Industry,Academia",,"Figure 12 shows MFU, which is around 0.484 during stable periods, with short spikes to as low as 0.4. It seems like the overall average is likely minimally affected by the spikes, but I'll round down to 0.48",,9728028.18454986,Other,,,,8
Genie,"Video,Games","Video generation,Image-to-video,Text-to-video",Google DeepMind,"Jake Bruce, Michael Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes, Matthew Lai, Aditi Mavalankar, Richie Steigerwald, Chris Apps, Yusuf Aytar, Sarah Bechtle, Feryal Behbahani, Stephanie Chan, Nicolas Heess, Lucy Gonzalez, Simon Osindero, Sherjil Ozair, Scott Reed, Jingwei Zhang, Konrad Zolna, Jeff Clune, Nando de Freitas, Satinder Singh, Tim RocktÃ¤schel",2024-02-23,Genie: Generative Interactive Environments,https://arxiv.org/abs/2402.15391,,,,10700000000.0,"When combined with the tokenizer and action
model this brings the total to 10.7B parameters,
trained on 942B tokens, which we refer to as the
Genie model. ",6.6e+22,Table 12,,"Our approach, Genie, is trained from a large dataset of over 200,000 hours of publicly available Internet gaming videos

We train Genie on a filtered set of 30,000 hours of Internet gameplay videos from hundreds of 2D platformer games, producing a foundation world model for this setting.

The final dataset contains 6.8M 16s video clips (30k hours)",942000000000.0,"When combined with the tokenizer and action model this brings the total to 10.7B parameters, trained on 942B tokens, which we refer to as the Genie model.",,,Google TPU v5p,,Confident,"We introduce Genie, the first generative interactive environment trained in an unsupervised manner from unlabelled Internet videos. The model can be prompted to generate an endless variety of action-controllable virtual worlds described through text, synthetic images, photographs, and even sketches. At 11B parameters, Genie can be considered a foundation world model. It is comprised of a spatiotemporal video tokenizer, an autoregressive dynamics model, and a simple and scalable latent action model. Genie enables users to act in the generated environments on a frame-by-frame basis despite training without any ground-truth action labels or other domain-specific requirements typically found in the world model literature. Further the resulting learned latent action space facilitates training agents to imitate behaviors from unseen videos, opening the path for training generalist agents of the future.",,,Unreleased,"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France",,,,256.0,,2025-05-19 12:47,,,,512.0,"As a result, for our final model, we train a 10.1B dynamics model with a batch size of 512, for a total of 125k steps, using 256 TPUv5p",Industry,,,,,Unreleased,"We have chosen not to release the trained model checkpoints, the
modelâ€™s training dataset, or examples from that data to accompany this paper or the website. We would like to have the opportunity to further engage with the research (and video game) community and to ensure that any future such releases are respectful, safe and responsible.",Industry,,,,,Reported,,,,
Stable Diffusion 3,Image generation,"Image generation,Text-to-image",Stability AI,"Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas MÃ¼ller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, Robin Rombach",2024-02-22,Scaling Rectified Flow Transformers for High-Resolution Image Synthesis,"https://arxiv.org/abs/2403.03206
https://stability.ai/news/stable-diffusion-3",,,,8000000000.0,,5e+22,"Finally, we performed
a scaling study of this combination up to a model size of
8B parameters and 5 Ã— 1022 training FLOPs.","ImageNet,Conceptual Captions 12M (CC12M)","We use two datasets to account for the missing of a standard text-to-image benchmark. As a widely used dataset, we convert the ImageNet dataset (Russakovsky et al., 2014) into a dataset suitable for text-to-image models by adding captions of the form â€œa photo of a âŒ©class nameâŒªâ€ to images, where âŒ©class nameâŒª is randomly chosen from one of the provided names for the imageâ€™s class label. As a more realistic text-to-image dataset, we use the CC12M dataset (Changpinyo et al., 2021) for training.",,,,,,,Confident,"Diffusion models create data from noise by inverting the forward paths of data towards noise and have emerged as a powerful generative modeling technique for high-dimensional, perceptual data such as images and videos. Rectified flow is a recent generative model formulation that connects data and noise in a straight line. Despite its better theoretical properties and conceptual simplicity, it is not yet decisively established as standard practice. In this work, we improve existing noise sampling techniques for training rectified flow models by biasing them towards perceptually relevant scales. Through a large-scale study, we demonstrate the superior performance of this approach compared to established diffusion formulations for high-resolution text-to-image synthesis. Additionally, we present a novel transformer-based architecture for text-to-image generation that uses separate weights for the two modalities and enables a bidirectional flow of information between image and text tokens, improving text comprehension, typography, and human preference ratings. We demonstrate that this architecture follows predictable scaling trends and correlates lower validation loss to improved text-to-image synthesis as measured by various metrics and human evaluations. Our largest models outperform state-of-the-art models, and we will make our experimental data, code, and model weights publicly available.",,,API access,United Kingdom of Great Britain and Northern Ireland,,,,,,2024-11-06 17:13,,,,,,Industry,,,,,Unreleased,"Apr 2014
We have partnered with Fireworks AI, the fastest and most reliable API platform in the market, to deliver Stable Diffusion 3 and Stable Diffusion 3 Turbo.

In keeping with our commitment to open generative AI, we aim to make the model weights available for self-hosting with a Stability AI Membership in the near future.",Industry,,,,,Reported,,,,
Gemma 7B,Language,"Language modeling/generation,Chat,Code generation,Question answering,Quantitative reasoning",Google DeepMind,"Gemma Team, Google DeepMind",2024-02-21,Gemma: Open Models Based on Gemini Research and Technology,https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf,,,,8538074112.0,"Table 2, sum of embedding and non-embedding parameters",3.07e+23,"6ND aproximation 6*8.54B*6T = 3.07e23
""Gemma 2B and 7B are trained on 2T and 6T tokens respectively of primarily-English data from web documents, mathematics, and code.""

As confirmation: ""We estimate the carbon emissions from pretraining the Gemma models to be âˆ¼ 131 ð‘¡ð¶ð‘‚2ð‘’ð‘ž. ""

U.S. avg CO2 per kWh is ~0.87lbs 
131 tCO2 * 2000 lb/t * (1 kWh/0.87lb) = 3.01e5 kWh

Per SemiAnalysis TPU v5e uses ~ 5x less power than H100, so ~140 W TDP
3.01e5 kWh * 1000 W/kW * 1 TPUv5e/140 W  = 2.15e6 TPUv5e-hours

In bf16 precision, TPUv5e has peak performance of 197 TF/s, so:
2.15e6 * 3600 * 197e12 * 0.3 = 4.57e23",Unspecified unreleased,"""Gemma 2B and 7B are trained on 2T and 6T tokens respectively of primarily-English data from web documents, mathematics, and code.""",6000000000000.0,"""Gemma 2B and 7B are trained on 2T and 6T tokens respectively of primarily-English data from web documents, mathematics, and code.""
Not explicitly stated that this doesn't involve multiple epochs, but I expect it does not.",,,Google TPU v5e,,Confident,,,,Open weights (restricted use),"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France",,,,4096.0,,2025-05-28 23:01,,,,,,Industry,,,,,Unreleased,"https://ai.google.dev/gemma/terms

no illegal use or abuse",Industry,,,,1134987.171306518,"Operation counting,Hardware",,,,
Re-Dock,Biology,Protein-ligand contact prediction,"Zhejiang University (ZJU),Westlake University,University of Washington","Yufei Huang, Odin Zhang, Lirong Wu, Cheng Tan, Haitao Lin, Zhangyang Gao, Siyuan Li, Stan. Z. Li",2024-02-21,Re-Dock: Towards Flexible and Realistic Molecular Docking with Diffusion Bridge,https://arxiv.org/abs/2402.11459,2.0,,,,,7.500000000000002e+19,"1. Hardware setup: 1x NVIDIA A100 GPU (3.12e14 FLOP/s)

2. Training duration: 7 days (directly provided) = 604,800 seconds

3. Utilization rate: 40%

4. Final calculation:
1 GPU Ã— 3.12e14 FLOP/s Ã— 604,800s Ã— 0.4 = 7.55e19 FLOPs",PDBbind,,23496.0,23496 examples in PDBbind 2020,168.0,,NVIDIA A100,,Likely,"Accurate prediction of protein-ligand binding structures, a task known as molecular docking is crucial for drug design but remains challenging. While deep learning has shown promise, existing methods often depend on holo-protein structures (docked, and not accessible in realistic tasks) or neglect pocket sidechain conformations, leading to limited practical utility and unrealistic conformation predictions. To fill these gaps, we introduce an under-explored task, named flexible docking to predict poses of ligand and pocket sidechains simultaneously and introduce Re-Dock, a novel diffusion bridge generative model extended to geometric manifolds. Specifically, we propose energy-to-geometry mapping inspired by the Newton-Euler equation to co-model the binding energy and conformations for reflecting the energy-constrained docking generative process. Comprehensive experiments on designed benchmark datasets including apo-dock and cross-dock demonstrate our model's superior effectiveness and efficiency over current methods.",,,,"China,China,United States of America",,,,1.0,,2025-05-01 10:42,,,,,,"Academia,Academia,Academia",,,,,,,"Academia,Academia,Academia",,,,435.0023192617824,Hardware,,,,
PepGLAD,Biology,Protein design,"Tsinghua University,Renmin University of China","Xiangzhe Kong, Yinjun Jia, Wenbing Huang, Yang Liu",2024-02-21,Full-Atom Peptide Design with Geometric Latent Diffusion,https://arxiv.org/abs/2402.13555,,,,,,,"
",PDB (Protein Data Bank),,196601.0,"
PepBench Training: 4,157 entries Ã— 10 residues = 41,570 residues
PepBDB Training: 8,434 entries Ã— 10 residues = 84,340 residues
ProtFrag: 70,645 monomers Ã— 1 residue = 70,645 residues

Total: 41,570 + 84,340 + 70,645 = 196,555 residues",,,,,Confident,"Peptide design plays a pivotal role in therapeutics, allowing brand new possibility to leverage target binding sites that are previously undruggable. Most existing methods are either inefficient or only concerned with the target-agnostic design of 1D sequences. In this paper, we propose a generative model for full-atom \textbf{Pep}tide design with \textbf{G}eometric \textbf{LA}tent \textbf{D}iffusion (PepGLAD) given the binding site. We first establish a benchmark consisting of both 1D sequences and 3D structures from Protein Data Bank (PDB) and literature for systematic evaluation. We then identify two major challenges of leveraging current diffusion-based models for peptide design: the full-atom geometry and the variable binding geometry. To tackle the first challenge, PepGLAD derives a variational autoencoder that first encodes full-atom residues of variable size into fixed-dimensional latent representations, and then decodes back to the residue space after conducting the diffusion process in the latent space. For the second issue, PepGLAD explores a receptor-specific affine transformation to convert the 3D coordinates into a shared standard space, enabling better generalization ability across different binding shapes. Experimental Results show that our method not only improves diversity and binding affinity significantly in the task of sequence-structure co-design, but also excels at recovering reference structures for binding conformation generation.",560.0,,,"China,China",,,,1.0,,2025-05-01 10:42,,,,,,"Academia,Academia",,,,,,,"Academia,Academia",,,,,Hardware,,,,
Gemma 2B,Language,"Language modeling/generation,Chat,Code generation,Question answering,Quantitative reasoning",Google DeepMind,"Gemma Team, Google DeepMind",2024-02-21,Gemma: Open Models Based on Gemini Research and Technology,https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf,,,,2506434560.0,"Table 2, sum of embedding and non-embedding parameters:

2B 524,550,144 + 1,981,884,416 = 2506434560",4.5115822e+22,"6ND = 6*2506434560.00 parameters * 3*10^12 training tokens = 4.5115822e+22

(assuming 1 epoch)",Unspecified unreleased,"""Gemma 2B and 7B are trained on 2T and 6T tokens respectively of primarily-English data from web documents, mathematics, and code.""",3000000000000.0,"""Gemma 2B and 7B are trained on 3T and 6T tokens respectively of primarily-English data from web documents, mathematics, and code.""
Not explicitly stated that this doesn't involve multiple epochs, but I expect it does not.",,,Google TPU v5e,,Confident,,,,Open weights (restricted use),"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France",,,,512.0,,2025-05-28 23:01,,,,,,Industry,,,,,Unreleased,"https://ai.google.dev/gemma/terms

no illegal use or abuse",Industry,,,,141873.39641331477,Operation counting,,,,
Sora,"Video,Vision","Video generation,Text-to-video,Image-to-video,Video-to-video",OpenAI,,2024-02-15,Video generation models as world simulators,https://openai.com/index/video-generation-models-as-world-simulators/,,SOTA improvement,,,,,,Unspecified unreleased,"Sora was trained on diverse datasets, including a mix of publicly available data, proprietary data accessed through partnerships, and custom datasets developed in-house. These consist of:

Select publicly available data, mostly collected from industry-standard machine learning datasets and web crawls.

Proprietary data from data partnerships. We form partnerships to access non-publicly available data. For example, we partnered with Shutterstockâ  Pond5 on building and delivering AI-generated images. We also partner to commission and create datasets fit for our needs.

Human data: Feedback from AI trainers, red teamers, and employees. ",,,,,,,Unknown,"Sora is OpenAIâ€™s video generation model, designed to take text, image, and video inputs and generate a new video as an output. Users can create videos up to 1080p resolution (20 seconds max) in various formats, generate new content from text, or enhance, remix, and blend their own assets. Users will be able to explore the Featured and Recent feeds which showcase community creations and offer inspiration for new ideas. Sora builds on learnings from DALLÂ·E and GPT models, and is designed to give people expanded tools for storytelling and creative expression. 

Sora is a diffusion model, which generates a video by starting off with a base video that looks like static noise and gradually transforms it by removing the noise over many steps. By giving the model foresight of many frames at a time, weâ€™ve solved a challenging problem of making sure a subject stays the same even when it goes out of view temporarily. Similar to GPT models, Sora uses a transformer architecture, unlocking superior scaling performance. ",,,Unreleased,United States of America,,,,,,2025-05-19 12:48,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
Gemini 1.5 Pro,"Language,Multimodal","Language modeling,Visual question answering",Google DeepMind,Gemini Team,2024-02-15,Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context,https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf,,Significant use,"Google DeepMind's current best public model, being used for their products.",,MoE architecture,1.580001e+25,Training compute imputed from benchmark scores.,Unspecified unreleased,,,,,,Google TPU v4,,Speculative,,,,API access,"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France",,,,,,2025-05-02 11:29,,,,,,Industry,,,,,Unreleased,API access: https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models,Industry,,,,,Benchmarks,,,,4
ProtChatGPT,Biology,Protein question answering,"University of Technology Sydney,Zhejiang University (ZJU)","Chao Wang, Hehe Fan, Ruijie Quan, Yi Yang",2024-02-15,ProtChatGPT: Towards Understanding Proteins with Large Language Models,https://arxiv.org/abs/2402.09649,9.0,,,8000000000.0,,7.200353201209069e+23,"GPU hours: 86400s*9.9e+14*2*0.4=6.8e+19

6ND: 6*8000000000*380000001=18240000048000000000

GMean(1.8e+19,6.8e+19)=3.5e+19

Base model: 7.2e+23

Total: 720035320120906900000000",,,380000001.0,"Stage 1:
- Proteins: 549,000 Ã— 500 = 2.745 Ã— 10â¸
- Descriptions: 549,000 Ã— 50 = 2.745 Ã— 10â·
- Total: 2.745 Ã— 10â¸ + 2.745 Ã— 10â· = 3.0195 Ã— 10â¸

Stage 2:
- Proteins: 143,508 Ã— 500 = 7.1754 Ã— 10â·
- Descriptions: 143,508 Ã— 50 = 7.1754 Ã— 10â¶
- Total: 7.1754 Ã— 10â· + 7.1754 Ã— 10â¶ = 7.892 Ã— 10â·

Final Total: 3.0195 Ã— 10â¸ + 7.892 Ã— 10â· = 3.8087 Ã— 10â¸",24.0,,NVIDIA H100 SXM5 80GB,,Confident,"Protein research is crucial in various fundamental disciplines, but understanding their intricate structure-function relationships remains challenging. Recent Large Language Models (LLMs) have made significant strides in comprehending task-specific knowledge, suggesting the potential for ChatGPT-like systems specialized in protein to facilitate basic research. In this work, we introduce ProtChatGPT, which aims at learning and understanding protein structures via natural languages. ProtChatGPT enables users to upload proteins, ask questions, and engage in interactive conversations to produce comprehensive answers. The system comprises protein encoders, a Protein-Language Pertaining Transformer (PLP-former), a projection adapter, and an LLM. The protein first undergoes protein encoders and PLP-former to produce protein embeddings, which are then projected by the adapter to conform with the LLM. The LLM finally combines user questions with projected embeddings to generate informative answers. Experiments show that ProtChatGPT can produce promising responses to proteins and their corresponding questions. We hope that ProtChatGPT could form the basis for further exploration and application in protein research. Code and our pre-trained model will be publicly available.",,,Unreleased,"Australia,China",Llama 3-8B,35000000000000000000,,2.0,,2025-05-23 13:06,,,,,,"Academia,Academia",,,,,Unreleased,"""Code and our pre-trained model will be publicly available""
as of May'25 I cannot find it","Academia,Academia",,,,2771.3350442941023,Hardware,,,,
Gemini 1.0 Pro Vision,"Vision,Language,Video","Language modeling,Visual question answering,Chat,Translation,Video description,Question answering",Google DeepMind,Gemini Team,2024-02-15,The best performing image and video understanding model to handle a broad range of applications. ,,,,,,,,,Unspecified unreleased,,,,,,,,Unknown,"Gemini 1.0 Pro Vision supports text, image, and video as inputs.

Max input tokens: 16,384
Max output tokens: 2,048
Max images per prompt: 16
Max video length: 2 minutes
Max videos per prompt: 1
Training data: Up to February 2023",,,API access,"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France",,,,,,2025-06-17 01:44,,,,,,Industry,,,,,Unreleased,API Access: https://console.cloud.google.com/vertex-ai/generative/multimodal/create/text?model=gemini-1.0-pro-vision-001,Industry,,,,,,,,,
Sora Turbo,"Video,Vision","Video generation,Text-to-video",OpenAI,,2024-02-15,Sora is here,https://openai.com/index/sora-is-here/,,Significant use,,,,,,Unspecified unreleased,"Sora was trained on diverse datasets, including a mix of publicly available data, proprietary data accessed through partnerships, and custom datasets developed in-house. These consist of:

Select publicly available data, mostly collected from industry-standard machine learning datasets and web crawls.

Proprietary data from data partnerships. We form partnerships to access non-publicly available data. For example, we partnered with Shutterstockâ  Pond5 on building and delivering AI-generated images. We also partner to commission and create datasets fit for our needs.

Human data: Feedback from AI trainers, red teamers, and employees. ",,,,,,,Unknown,"Our video generation model is rolling out at sora.comâ .

Earlier this year, we introduced Soraâ , our model that can create realistic videos from text, and shared our initial research progressâ  on world simulation. Sora serves as a foundation for AI that understands and simulates realityâ€”an important step towards developing models that can interact with the physical world.

We developed a new version of Soraâ€”Sora Turboâ€”that is significantly faster than the model we previewed in February. Weâ€™re releasing it today as a standalone product at Sora.com to ChatGPT Plus and Pro users.",,,Unreleased,United States of America,,,,,,2025-05-19 12:48,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
KwaiYii 175B,Language,Language modeling/generation,Kuaishou Technology,,2024-02-14,,https://blog.csdn.net/kuaishoutech/article/details/140542568,,,,175000000000.0,175B,,,,,,,,,,,Confident,"In June 2024, quick-handed NLP experts reported on the â€œKwaiYiiâ€ model at the Global Artificial Intelligence Technology Conference. The model was developed in early 2023, and the 175B model was released at the end of February 2024. Many capabilities are close to the latest version of GPT-4. Introduced eight key technological innovations and landing practices in scenes such as AI Xiaofu, including solutions to various challenges, which will continue to be iterated in the future.",,,,China,,,,,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
Aya,Language,"Language modeling/generation,Chat,Translation","Cohere for AI,Brown University,Cohere,Carnegie Mellon University (CMU),Massachusetts Institute of Technology (MIT)","Ahmet ÃœstÃ¼n, Viraat Aryabumi, Zheng-Xin Yong, Wei-Yin Ko, Daniel D'souza, Gbemileke Onilude, Neel Bhandari, Shivalika Singh, Hui-Lee Ooi, Amr Kayid, Freddie Vargus, Phil Blunsom, Shayne Longpre, Niklas Muennighoff, Marzieh Fadaee, Julia Kreutzer, Sara Hooker",2024-02-12,Aya Model: An Instruction Finetuned Open-Access Multilingual Language Model,https://arxiv.org/abs/2402.07827,114.0,SOTA improvement,"from abstract ""We introduce extensive new evaluation suites that broaden the state-of-art for multilingual eval across 99
language""",13000000000.0,13B  - fine tune of mT5 - from last page - model card ,,"13B parameters, batch size = 256, sequence length = 1024 (for both input and output), 30K updates
- aproximation 6ND = 6 * 13B * 2 * 1024 * 30K * 256= 1226833920000000000000 = 1.22683392e+21
""We finetune mT5 models using the Adafactor optimizer [Shazeer & Stern, 2018] with a learning rate of 3 Ã— 10âˆ’4 and a batch size of 256. We find that using a smaller learning rate compared to 1 Ã— 10âˆ’3 leads to a better downstream performance, which is potentially due to the diverse nature of our IFT mixture. Both input and target sequence length are set to 1024.""
""We train all the models for 30,000 update steps with data packing enabled.16 This results in a training budget of 25M samples. """,,"""Expansion of Language Coverage We significantly expand the size of available training data to directly address the linguistic inequality of recent NLP development. "" from the paper
""Datasets: xP3x, Aya Dataset, Aya Collection, DataProvenance collection, ShareGPT-Command."" from https://huggingface.co/CohereForAI/aya-101and https://huggingface.co/CohereForAI/aya-101#data-sources",,"at least 835 GB + size of ShareGPT-command + size of DataProvenance collection
https://huggingface.co/CohereForAI/aya-101#data-sourcesxP3x  - 680GB - from 
https://huggingface.co/datasets/CohereForAI/xP3x

aya_dataset - 138MB - https://huggingface.co/datasets/CohereForAI/aya_dataset

aya collection - 155GB - https://huggingface.co/datasets/CohereForAI/aya_collection",,,Google TPU v4,,Speculative,"Recent breakthroughs in large language models (LLMs) have centered around a handful of data-rich languages. What does it take to broaden access to breakthroughs beyond first-class citizen languages? Our work introduces Aya, a massively multilingual generative language model that follows instructions in 101 languages of which over 50% are considered as lower-resourced. Aya outperforms mT0 and BLOOMZ on the majority of tasks while covering double the number of languages. We introduce extensive new evaluation suites that broaden the state-of-art for multilingual eval across 99 languages -- including discriminative and generative tasks, human evaluation, and simulated win rates that cover both held-out tasks and in-distribution performance. Furthermore, we conduct detailed investigations on the optimal finetuning mixture composition, data pruning, as well as the toxicity, bias, and safety of our models. We open-source our instruction datasets and our model at this https://huggingface.co/CohereForAI/aya-101",,,Open weights (unrestricted),"Multinational,Canada,United States of America,Canada,United States of America,United States of America",mT5-XXL,1,"13B parameters, batch size = 256, sequence length = 1024 (for both input and output), 30K updates
- aproximation 6ND = 6 * 13B * 2 * 1024 * 30K * 256= 1226833920000000000000 = 1.22683392e+21
""We finetune mT5 models using the Adafactor optimizer [Shazeer & Stern, 2018] with a learning rate of 3 Ã— 10âˆ’4 and a batch size of 256. We find that using a smaller learning rate compared to 1 Ã— 10âˆ’3 leads to a better downstream performance, which is potentially due to the diverse nature of our IFT mixture. Both input and target sequence length are set to 1024.""
""We train all the models for 30,000 update steps with data packing enabled.16 This results in a training budget of 25M samples. """,128.0,,2024-11-01 10:04,,,,,,"Industry,Academia,Industry,Academia,Academia",,,,,Unreleased,Apache 2.0,"Industry,Academia,Industry,Academia,Academia",,,,43077.3425083266,,,,,
PLAPT,Biology,Protein-ligand binding affinity prediction,"Wolfram Research,ASC27,Newport High School,Sanskriti School","Tyler Rose, NicolÃ² Monti, Navvye Anand, Tianyu Shen",2024-02-12,PLAPT: Protein-Ligand Binding Affinity Prediction Using Pretrained Transformers,https://www.biorxiv.org/content/10.1101/2024.02.08.575577v3.abstract,1.0,,,1474624.0,"""Figure 3: The prediction module takes a 1792x1 feature vector as its input, which is then partitioned into two streams: The first stream processes the first 1024 indices of the feature vector through a 512-node protein-specific linear layer, followed by a ReLU activation function. Concurrently, the second stream processes the latter 768 feature indices through a similar 512-node moleculespecific linear layer, also followed by a ReLU activation.  Outputs from both streams are concatenated into a single vector of 1024 elements. This combined vector is passed through a batch normalization layer with a momentum of 0.9 and epsilon of 0.001. The vector is then fed through the 512-node Linear Layer 1 and ReLU activation. A dropout layer with a probability of 20% is then applied to mitigate overfitting.  Following the dropout layer, the prediction module continues to reduce the feature space with the 64-node Linear Layer 2 with ReLU activation, before reaching the single node Linear Layer 3. This layer outputs the predicted scalar value representing the normalized negative log10 affinity value, which is then un-normalized.""  (512 * 1024) + (512 * 768) + (512 * 1024) + (64 * 512) + 64 = 1474624",3.900846548437782e+22,"1474624 connections; 90,000 training examples; table a4 mentions 60 training rounds

6*1474624*90000*60=47777817600000

Including base models:
8465436600000000000 + 3.9e+22 + 47777817600000 = 39008465484377820000000",,,313000001.0,"Training set: 90,000 samples
Tokens per sample: 3,200 (protein) + 278 (ligand) = 3,478
Total tokens = 90,000 Ã— 3,478 = 313,020,000 (3.13e8) tokens",,,NVIDIA GeForce RTX 4060 Ti,,Confident,"Predicting protein-ligand binding affinity is crucial for drug discovery, as it enables efficient identification of drug candidates. We introduce PLAPT, a novel model utilizing transfer learning from pre-trained transformers like ProtBERT and ChemBERTa to predict binding affinities with high accuracy. Our method processes one-dimensional protein and ligand sequences, leveraging a branching neural network architecture for feature integration and affinity estimation. We demonstrate PLAPTâ€™s superior performance through validation on multiple datasets, achieving state-of-the-art results while requiring significantly less computational resources for training compared to existing models. Our findings indicate that PLAPT offers a highly effective and accessible approach for accelerating drug discovery efforts.",60.0,,,"United States of America,Italy,United States of America,India","ChemBERTa,ProtBERT-BFD",,,1.0,,2025-05-01 10:42,,,,,,"Industry,Industry,Academia",,,,,,,"Industry,Industry,Academia",,,,174.03580522109968,Operation counting,,,,
DiscDiff,Biology,Protein or nucleotide language model (pLM/nLM),Imperial College London,"Zehui Li, Yuhao Ni, William A V Beardall, Guoxuan Xia, Akashaditya Das, Guy-Bart Stan, Yiren Zhao",2024-02-08,DiscDiff: Latent Diffusion Model for DNA Sequence Generation,https://arxiv.org/abs/2402.06079,2.0,,,,,3.399999999999988e+19,"1. Hardware setup:
- VAE stage: 1x NVIDIA RTX A6000 (3.87e13 FLOP/s)
- UNet stage: 1x NVIDIA A100 40GB (3.12e14 FLOP/s)

2. Training duration (provided directly):
- VAE: 24 GPU-hours (86,400 seconds)
- UNet: 72 GPU-hours (259,200 seconds)

3. Utilization rate: 40% for both stages

4. Calculation:
VAE: 3.87e13 FLOP/s Ã— 86,400s Ã— 0.4 = 1.34e18 FLOPs
UNet: 3.12e14 FLOP/s Ã— 259,200s Ã— 0.4 = 3.24e19 FLOPs
Total = 1.34e18 + 3.24e19 = 3.4e19 FLOPs",,,327680000.0,"160000 examples with 2048 length (Table 1)
1.6e+5*2.0e+3=3.3e+8

",,,"NVIDIA RTX A6000,NVIDIA A100",,Confident,"This paper introduces a novel framework for DNA sequence generation, comprising two key components: DiscDiff, a Latent Diffusion Model (LDM) tailored for generating discrete DNA sequences, and Absorb-Escape, a post-training algorithm designed to refine these sequences. Absorb-Escape enhances the realism of the generated sequences by correcting â€˜round errorsâ€™ inherent in the conversion process between latent and input spaces. Our approach not only sets new standards in DNA sequence generation but also demonstrates superior performance over existing diffusion models, in generating both short and long DNA sequences. Additionally, we introduce EPD-GenDNA, the first comprehensive, multi-species dataset for DNA generation, encompassing 160,000 unique sequences from 15 species. We hope this study will advance the generative modelling of DNA, with potential implications for gene therapy and protein production.",,,,United Kingdom of Great Britain and Northern Ireland,,,,2.0,,2025-05-01 10:42,,,,,,Academia,,,,,,,Academia,,,,,Hardware,,,,
Distilled Grandmaster,Games,Chess,DeepMind,"Anian Ruoss, GrÃ©goire DelÃ©tang, Sourabh Medapati, Jordi Grau-Moya, Li Kevin Wenliang, Elliot Catt, John Reid, Tim Genewein",2024-02-07,Grandmaster-Level Chess Without Search,https://arxiv.org/abs/2402.04494,10.0,,,270000000.0,"""Our largest model has roughly 270 million parameters.""",1.035671832e+22,"10356718320000000065536 FLOP
""Board states ð‘  are encoded as FEN strings which we convert to fixed-length strings of 77 characters where the ASCII-code of each character is one token."" so 77 tokens for board + 1 token for action ""For our largest training dataset, based on 10M games, this results in 15.32B action-value estimates""
so input is 78 tokens for each action-value
number of tokens = 1194960000000.0
The model is dense transformer
"" We train for 10 million
steps, which corresponds to 2.67 epochs for a batch
size of 4096 with 15.32B data points "", but in appendix A.2 there is mention of 5.35 epochs
I have used higher value from 5.35 and 2.67,
Probably final they trained model for 5.35 epochs and used checkpoint from 2.67 as final model.
aproximation 6ND for 5.35 epochs = 6*270e6*1194960000000.0 * 5.35 =  10356718320000000065536
",,"custom
""To get a large corpus of â€œground-truthâ€ action-values we
use Stockfish 16 as an oracle to annotate millions of
board states obtained from randomly drawn games on
lichess.org, which are mostly played by humans vary-
ing significantly in playing strength.""",15320000000.0,"15.32B examples * 78 tokens per example = 1.19e12
Training is supervised. I count each action-value (board state, action and numeric evaluation of state from Stockfish 16) as 1 data point.""For our largest training dataset, based on 10M games, this results in 15.32B action-value estimates""",,,,Supervised,Confident,"The recent breakthrough successes in machine learning are mainly attributed to scale: namely large-scale attention-based architectures and datasets of unprecedented scale. This paper investigates the impact of training at scale for chess. Unlike traditional chess engines that rely on complex heuristics, explicit search, or a combination of both, we train a 270M parameter transformer model with supervised learning on a dataset of 10 million chess games. We annotate each board in the dataset with action-values provided by the powerful Stockfish 16 engine, leading to roughly 15 billion data points. Our largest model reaches a Lichess blitz Elo of 2895 against humans, and successfully solves a series of challenging chess puzzles, without any domain-specific tweaks or explicit search algorithms. We also show that our model outperforms AlphaZero's policy and value networks (without MCTS) and GPT-3.5-turbo-instruct. A systematic investigation of model and dataset size shows that strong chess performance only arises at sufficient scale. To validate our results, we perform an extensive series of ablations of design choices and hyperparameters. ",,,Open weights (unrestricted),United Kingdom of Great Britain and Northern Ireland,,,,,,2025-06-16 10:50,,,,,,Industry,,,,,Open source,"All software is licensed under the Apache License, Version 2.0 (Apache 2.0)
The model weights are licensed under Creative Commons Attribution 4.0 (CC-BY). 
Some portions of the dataset are in the public domain by a Creative Commons CC0 license from lichess.org. The remainder of the dataset is licensed under Creative Commons Attribution 4.0 (CC-BY). 

https://github.com/google-deepmind/searchless_chess",Industry,,,,,Operation counting,,,,
Structure-Informed Protein Language Model,Biology,Protein or nucleotide language model (pLM/nLM),"Mila - Quebec AI (originally Montreal Institute for Learning Algorithms),University of Montreal / UniversitÃ© de MontrÃ©al,IBM Research,HEC Montreal,CIFAR AI Research","Zuobai Zhang, Jiarui Lu, Vijil Chenthamarakshan, AurÃ©lie Lozano, Payel Das, Jian Tang",2024-02-07,Structure-Informed Protein Language Model,https://arxiv.org/abs/2402.05856,3.0,,,650000000.0,,,,,,3700000.0,"Fine-tuning:
12,312 proteins Ã— 300 residues = 3.7 Ã— 10^6 tokens",,,,,Likely,"Protein language models are a powerful tool for learning protein representations through pre-training on vast protein sequence datasets. However, traditional protein language models lack explicit structural supervision, despite its relevance to protein function. To address this issue, we introduce the integration of remote homology detection to distill structural information into protein language models without requiring explicit protein structures as input. We evaluate the impact of this structure-informed training on downstream protein function prediction tasks. Experimental results reveal consistent improvements in function annotation accuracy for EC number and GO term prediction. Performance on mutant datasets, however, varies based on the relationship between targeted properties and protein structures. This underscores the importance of considering this relationship when applying structure-aware training to protein function prediction tasks. Code and model weights are available at this https URL.",50.0,,,"Canada,Canada,United States of America,Multinational,Ireland,United Kingdom of Great Britain and Northern Ireland,Brazil,Switzerland,Canada,Canada",ESM2-650M,,,,,2025-05-01 10:42,,,,,,"Academia,Academia,Industry,Academia,Research collective",,,,,,,"Academia,Academia,Industry,Academia,Research collective",,,,,,,,,
SenseChat-Medical V4,"Language,Vision,Multimodal","Medical diagnosis,Language modeling/generation,Question answering,Character recognition",SenseTime,,2024-02-06,"SenseTime Unveils SenseNova 4.0, Bringing Novel AI experience
","https://www.sensetime.com/en/news-detail/51167493?categoryId=1072

https://www.sensetime.com/cn/product-detail?categoryId=51134395&gioNav=1",,,,,,,,,,30000000.0,30 billion tokens of medical data (fine-tune of SenseChat),,,,,Confident,"SenseChat-Medical V4, SenseTime's upgraded medical LLM, boasts stronger capabilities for multi-round dialogues, context understanding, and tool invocation. It can handle professional medical Q&A, complex inference for medical tasks, intelligent interpretation, and interactive Q&A of multimodal medical documents. SenseChat-Medical V4's overall performance closely matches that of GPT-4 and ranks second in two authoritative industry evaluations: the 2023 Pharmacist Licensure Examination LLM evaluation and the Chinese medical LLM evaluation benchmark MedBench. In the former evaluation, it even outperforms GPT-4 in two categories.",,,API access,"Hong Kong,China",SenseChat,,,,,2025-06-11 17:08,,,,,,Industry,,,,,Unreleased,API: https://platform.sensenova.cn/,Industry,,,,,,,,,
Qwen Plus,Language,"Language modeling/generation,Code generation,Question answering,Translation,Quantitative reasoning",Alibaba,,2024-02-06,"Qwen-Plus: Balanced capabilities with performance, cost, and speed positioned between Qwen-Max and Qwen-Turbo. Suitable for tasks of moderate complexity.",https://help.aliyun.com/zh/model-studio/models?scm=20140722.S_help%40%40%E6%96%87%E6%A1%A3%40%402840914.S_BB1%40bl%2BBB2%40bl%2BRQW%40ag0%2Bos0.ID_2840914-RL_plus-LOC_doc%7EUND%7Eab-OR_ser-PAR1_2102029c17437872399601371d08f3-V_4-P0_0-P1_0&spm=a2c4g.11186623.help-search.i20,,,,,,,,Unspecified unreleased,,,,,,,,Unknown,,,,API access,China,,,,,,2025-06-03 06:56,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
Qwen Turbo,Language,"Language modeling/generation,Code generation,Question answering,Translation,Quantitative reasoning",Alibaba,,2024-02-06,"Qwen-Turbo: The fastest and lowest-cost model in the Qwen series, suitable for simple tasks.",https://help.aliyun.com/zh/model-studio/models?scm=20140722.S_help%40%40%E6%96%87%E6%A1%A3%40%402840914.S_BB1%40bl%2BBB2%40bl%2BRQW%40ag0%2Bos0.ID_2840914-RL_plus-LOC_doc%7EUND%7Eab-OR_ser-PAR1_2102029c17437872399601371d08f3-V_4-P0_0-P1_0&spm=a2c4g.11186623.help-search.i20,,,,,,,,Unspecified unreleased,,,,,,,,Unknown,,,,API access,China,,,,,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
Qwen1.5-32B,Language,"Chat,Language modeling/generation,Quantitative reasoning,Code generation,Translation",Alibaba,Qwen Team,2024-02-05,Introducing Qwen1.5,https://qwenlm.github.io/blog/qwen1.5/,,,,32000000000.0,32B,,"upper bound is taken from Qwen1.5 72B training compute estimation

lower bound is taken from Qwen1.5 14B training compute estimation",Unspecified unreleased,,,,,,,,Confident,"Qwen1.5 is the beta version of Qwen2, a transformer-based decoder-only language model pretrained on a large amount of data. In comparison with the previous released Qwen, the improvements include:

8 model sizes, including 0.5B, 1.8B, 4B, 7B, 14B, 32B and 72B dense models, and an MoE model of 14B with 2.7B activated;
Significant performance improvement in human preference for chat models;
Multilingual support of both base and chat models;
Stable support of 32K context length for models of all sizes
No need of trust_remote_code.",,,Open weights (restricted use),China,,,,,,2025-05-29 00:21,,,,,,Industry,,3.36e+23,1.3e+24,,Unreleased,"https://huggingface.co/Qwen/Qwen1.5-32B
""If you are commercially using the Materials, and your product or service has more than 100 million monthly active users, You shall request a license from Us""",Industry,,,,,Operation counting,Qwen,,,
DeepSeekMath 7B,Language,Quantitative reasoning,"DeepSeek,Tsinghua University,Peking University","Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y.K. Li, Y. Wu, Daya Guo",2024-02-05,DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models,https://arxiv.org/abs/2402.03300,,,,7000000000.0,"""Our model is initialized with DeepSeek-Coder-Base-v1.5 7B (Guo et al., 2024) and trained for 500B tokens.""",1.014e+23,8.04e+22 (base model) + 2.1e+22 (fine-tune) = 1.014e+23,"DeepSeekMath Corpus,arXiv,GitHub,Common Crawl","""By implementing a meticulously designed data selection pipeline, we successfully construct the DeepSeekMath Corpus, a high-quality dataset of 120B tokens from web pages filtered for mathematical content, which is almost 7 times the size of the math web pages used by Minerva (Lewkowycz et al., 2022a) and 9 times the size of the recently released OpenWebMath
(Paster et al., 2023).""

""The distribution of the data is as follows: 56% is from the DeepSeekMath Corpus, 4% from AlgebraicStack, 10% from arXiv, 20% is Github code, and the remaining 10% is natural language data from Common Crawl in both English and Chinese.""",500000000000.0,"""Our model is initialized with DeepSeek-Coder-Base-v1.5 7B (Guo et al., 2024) and trained for 500B tokens.""",,,,,Confident,"Mathematical reasoning poses a significant challenge for language models due to its complex and structured nature. In this paper, we introduce DeepSeekMath 7B, which continues pre-training DeepSeek-Coder-Base-v1.5 7B with 120B math-related tokens sourced from Common Crawl, together with natural language and code data. DeepSeekMath 7B has achieved an impressive score of 51.7% on the competition-level MATH benchmark without relying on external toolkits and voting techniques, approaching the performance level of Gemini-Ultra and GPT-4. Self-consistency over 64 samples from DeepSeekMath 7B achieves 60.9% on MATH. The mathematical reasoning capability of DeepSeekMath is attributed to two key factors: First, we harness the significant potential of publicly available web data through a meticulously engineered data selection pipeline. Second, we introduce Group Relative Policy Optimization (GRPO), a variant of Proximal Policy Optimization (PPO), that enhances mathematical reasoning abilities while concurrently optimizing the memory usage of PPO.",,,Open weights (restricted use),"China,China,China",DeepSeek Coder 6.7B,2,6 FLOP / token / parameter * 7B parameters * 500B tokens = 2.1e+22 FLOP,,,2025-05-16 10:30,,,,,,"Industry,Academia,Academia",,,,,Unreleased,"deepseek license
https://huggingface.co/deepseek-ai/deepseek-math-7b-base","Industry,Academia,Academia",,,,,Operation counting,deepseek-ai,,,
Qwen1.5-72B,Language,"Chat,Language modeling/generation,Quantitative reasoning,Code generation,Translation",Alibaba,Qwen Team,2024-02-04,Introducing Qwen1.5,https://qwenlm.github.io/blog/qwen1.5/,,SOTA improvement,"#1 in C-Eval (84.1, better than Qwen-72B. https://qwenlm.github.io/blog/qwen1.5/, https://cevalbenchmark.com/static/leaderboard.html)",72000000000.0,72B,1.3e+24,"3T training tokens: https://github.com/QwenLM/Qwen2/issues/97 

6 * 72 billion * 3 trillion = ~1.3e24",Unspecified unreleased,"""We pretrained the models with a large amount of data, and we post-trained the models with both supervised finetuning and direct preference optimization.""",3000000000000.0,3 trillion tokens from this response https://github.com/QwenLM/Qwen2/issues/97,,,,,Confident,"In recent months, our focus has been on developing a â€œgoodâ€ model while optimizing the developer experience. As we progress towards Qwen1.5, the next iteration in our Qwen series, this update arrives just before the Chinese New Year. With Qwen1.5, we are open-sourcing base and chat models across six sizes: 0.5B, 1.8B, 4B, 7B, 14B, and 72B. In line with tradition, weâ€™re also providing quantized models, including Int4 and Int8 GPTQ models, as well as AWQ and GGUF quantized models. To enhance the developer experience, weâ€™ve merged Qwen1.5â€™s code into Hugging Face transformers, making it accessible with transformers>=4.37.0 without needing trust_remote_code.",,,Open weights (restricted use),China,,,,,,2025-05-01 10:42,,"There is no paper to reference, no information about hardware used for training found in media.",,,,Industry,,,,,Unreleased,"restriction on >100m monthly users:

https://huggingface.co/Qwen/Qwen1.5-72B/blob/main/LICENSE",Industry,,,,,Operation counting,Qwen,,,14
Qwen1.5-7B,Language,"Chat,Language modeling/generation,Quantitative reasoning,Code generation,Translation",Alibaba,Qwen Team,2024-02-04,Introducing Qwen1.5,https://huggingface.co/Qwen/Qwen1.5-7B,,,,7000000000.0,7B,1.68e+23,6 FLOP / parameter / token * 7*10^9 parameters * 4*10^12 tokens =  1.68e+23 FLOP,Unspecified unreleased,"""We pretrained the models with a large amount of data, and we post-trained the models with both supervised finetuning and direct preference optimization.""",4000000000000.0,4 trillion tokens from this response https://github.com/QwenLM/Qwen2/issues/97,,,,,Confident,"Qwen1.5 is the beta version of Qwen2, a transformer-based decoder-only language model pretrained on a large amount of data. In comparison with the previous released Qwen, the improvements include:

8 model sizes, including 0.5B, 1.8B, 4B, 7B, 14B, 32B and 72B dense models, and an MoE model of 14B with 2.7B activated;
Significant performance improvement in human preference for chat models;
Multilingual support of both base and chat models;
Stable support of 32K context length for models of all sizes
No need of trust_remote_code.",,,Open weights (unrestricted),China,,,,,,2025-05-29 00:56,,,,,,Industry,,,,,Unreleased,https://huggingface.co/Qwen/Qwen1.5-7B,Industry,,,,,Operation counting,Qwen,,,
Qwen1.5-14B,Language,"Chat,Language modeling/generation,Quantitative reasoning,Code generation,Translation",Alibaba,Qwen Team,2024-02-04,Introducing Qwen1.5,https://huggingface.co/Qwen/Qwen1.5-14B,,,,14000000000.0,14B,3.36e+23,6 FLOP / parameter / token * 14*10^9 parameters * 4*10^12 tokens =  3.36e+23 FLOP,Unspecified unreleased,"""We pretrained the models with a large amount of data, and we post-trained the models with both supervised finetuning and direct preference optimization.""",4000000000000.0,4 trillion tokens from this response https://github.com/QwenLM/Qwen2/issues/97,,,,,Confident,"Qwen1.5 is the beta version of Qwen2, a transformer-based decoder-only language model pretrained on a large amount of data. In comparison with the previous released Qwen, the improvements include:

8 model sizes, including 0.5B, 1.8B, 4B, 7B, 14B, 32B and 72B dense models, and an MoE model of 14B with 2.7B activated;
Significant performance improvement in human preference for chat models;
Multilingual support of both base and chat models;
Stable support of 32K context length for models of all sizes
No need of trust_remote_code.",,,Open weights (unrestricted),China,,,,,,2025-05-29 00:23,,,,,,Industry,,,,,Unreleased,https://huggingface.co/Qwen/Qwen1.5-14B,Industry,,,,,Operation counting,Qwen,,,
OLMo-7B,Language,"Language modeling/generation,Chat","Allen Institute for AI,University of Washington","Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkinson, Russell Authur, Khyathi Raghavi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel, Tushar Khot, William Merrill, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Valentina Pyatkin, Abhilasha Ravichander, Dustin Schwenk, Saurabh Shah, Will Smith, Emma Strubell, Nishant Subramani, Mitchell Wortsman, Pradeep Dasigi, Nathan Lambert, Kyle Richardson, Luke Zettlemoyer, Jesse Dodge, Kyle Lo, Luca Soldaini, Noah A. Smith, Hannaneh Hajishirzi",2024-02-01,OLMo: Accelerating the Science of Language Models,https://arxiv.org/abs/2402.00838v1,,,,7000000000.0,,1.0332e+23,"direct calculation:
6*7B*2.46trillion=1.0332 Ã— 10^23

(calculation also reporoduced by the developers in https://arxiv.org/pdf/2501.00656)",Dolma,,2000000000000.0,"""We built our training dataset out of a 2T-token sample from our open dataset, Dolma [...] All of our released models have been trained to at least 2T tokens (a single epoch over our training data), and some have been trained beyond that by starting a second epoch over the data with a different shuffling order"" 

Table 1 indicates total tokens seen are 2.46T for the 7B parameter model, though note that a later release in July 2024 has been trained to 2.75T tokens: https://github.com/allenai/OLMo?tab=readme-ov-file",,,"AMD Radeon Instinct MI250X,NVIDIA A100",,Confident,"Language models (LMs) have become ubiquitous in both NLP research and in commercial product offerings. As their commercial importance has surged, the most powerful models have become closed off, gated behind proprietary interfaces, with important details of their training data, architectures, and development undisclosed. Given the importance of these details in scientifically studying these models, including their biases and potential risks, we believe it is essential for the research community to have access to powerful, truly open LMs. To this end, this technical report details the first release of OLMo, a state-of-the-art, truly Open Language Model and its framework to build and study the science of language modeling. Unlike most prior efforts that have only released model weights and inference code, we release OLMo and the whole framework, including training data and training and evaluation code. We hope this release will empower and strengthen the open research community and inspire a new wave of innovation.",1.23,,Open weights (unrestricted),"United States of America,United States of America",,,,,,2025-01-03 15:51,,"LUMI supercomputer,  MosaicML (Databricks)",,4000000.0,"For OLMo-1B and -7B models, we use a constant global batch size of 
approximately 4M tokens (2048 instances, each with a sequence length of 2048 tokens).","Research collective,Academia",,,,,Open source,"License: The code and model are released under Apache 2.0.

Weights https://huggingface.co/allenai/OLMo-7B
Code https://github.com/allenai/OLMo","Research collective,Academia",,,,,Operation counting,,,,
LLaVA-NeXT-34B (LLaVA-1.6),"Multimodal,Language,Vision","Visual question answering,Chat,Question answering","University of Wisconsin Madison,ByteDance,Nanyang Technological University,University of California (UC) Berkeley","Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, Yong Jae Lee",2024-01-30,"LLaVA-NeXT: Improved reasoning, OCR, and world knowledge","https://llava-vl.github.io/blog/2024-01-30-llava-next/, https://huggingface.co/liuhaotian/llava-v1.6-34b",,,"""SoTA Performance! LLaVA-NeXT achieves the best performance compared with open-source LMMs such as CogVLM or Yi-VL. Compared with commercial ones, it catches up to Gemini Pro and outperforms Qwen-VL-Plus on selected benchmarks.""",34750000000.0,34.75B,2.5878528e+20,"2.6e20 = 32 * 312e12 * 0.3 * 24* 3600 = num gpus * peak flops * assumed utilization rate * time in seconds
""The largest 34B variant finishes training in ~1 day with 32 A100s.""",,"""Data: Coming soon.""",,"'1318K"" image-text pairs",24.0,"""The largest 34B variant finishes training in ~1 day with 32 A100s.""",NVIDIA A100,,Likely,,,,Open weights (unrestricted),"United States of America,China,Singapore,United States of America",,,,32.0,,2025-02-11 17:04,,,,,,"Academia,Industry,Academia,Academia",,,,768.0,,Apache 2.0,"Academia,Industry,Academia,Academia",,,,25346.95016648552,Hardware,,,,
BGE-M3 Embedding,Language,"Language modeling/generation,Translation",Beijing Academy of Artificial Intelligence / BAAI,"Jianlv Chen, Shitao Xiao, Peitian Zhan, Kun Luo, Defu Lian, Zheng Liu",2024-01-30,"BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation",https://github.com/FlagOpen/FlagEmbedding/blob/master/FlagEmbedding/BGE_M3/BGE_M3.pdf,,,,335000000.0,335M params (from Hugging Face model card),,"I'm not sure that 6ND works since the architecture might not be dense.

Pre-training is conducted on 32 A100(40GB) GPUs for 20,000 steps.
The second stage is conducted on 96 A800(80GB) GPUs. This training process takes 25,000 steps.","MTP,mC4,CC-News,xP3,Wikipedia,NLLB,S2ORC,CCMatrix",Table 1,1200041400.0,"From Table 1
Pre-training data: 1.2B tokens
Fine-tuning data: 41.4K tokens",,,"NVIDIA A100,NVIDIA A800 PCIe 40 GB",,Likely,"In this paper, we present a new embedding
model, called M3-Embedding, which is distinguished
for its versatility in Multi-Linguality,
Multi-Functionality, and Multi-Granularity. It
can support more than 100 working languages,
leading to new state-of-the-art performances
on multi-lingual and cross-lingual retrieval
tasks. It can simultaneously perform the three
common retrieval functionalities of embedding
model: dense retrieval, multi-vector retrieval,
and sparse retrieval, which provides a unified
model foundation for real-world IR applications.
It is able to process inputs of different
granularities, spanning from short sentences to
long documents of up to 8192 tokens. The effective
training of M3-Embedding involves the
following technical contributions. We propose
a novel self-knowledge distillation approach,
where the relevance scores from different retrieval
functionalities can be integrated as the
teacher signal to enhance the training quality.
We also optimize the batching strategy,
enabling a large batch size and high training
throughput to ensure the discriminativeness of
embeddings. To the best of our knowledge,
M3-Embedding is the first embedding model
which realizes such a strong versatility. The
model and code will be publicly available at
https://github.com/FlagOpen/FlagEmbedding.",,,Open weights (unrestricted),China,XLM-RoBERTa,,,,,2024-09-05 14:08,,,,,,Academia,,,,,Open source,FlagEmbedding is licensed under the MIT License.,Academia,,,,,,,,,
Code Llama-70B,Language,Code generation,Meta AI,"Baptiste RoziÃ¨re, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Ellen Tan, Yossef (Yossi) Adi, Jingyu Liu, Tal Remez, JÃ©rÃ©my Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Defossez, Jade Copet, Faisal Azhar, Hugo Touvron, Gabriel Synnaeve, Louis Martin, Nicolas Usunier, Thomas Scialom",2024-01-29,Code Llama: Open Foundation Models for Code,"https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/
https://arxiv.org/abs/2308.12950",1297.0,,"""In our own benchmark testing, Code Llama outperformed state-of-the-art publicly available LLMs on code tasks""",70000000000.0,70B,1.26e+24,"Base model saw 2T tokens, Code Llama-70B was trained on an additional 1T. 6NC:
6 * 3T * 70B = 1.26e24",Unspecified unreleased,"We are releasing four sizes of Code Llama with 7B, 13B, 34B, and 70B parameters respectively. Each of these models is trained with 500B tokens of code and code-related data, apart from 70B, which is trained on 1T tokens.",1000000000000.0,Llama 70B training dataset was 2 trillion tokens. Code Llama finetuning dataset was 1 trillion tokens of code.,6480.0,"Assuming Code Llama 70B training continued on same hardware as Llama 2 70B.
Llama 2 70B used 1720320 A100 hours. Training all Code Llama models took 1.4M A100 hours (Table 26). Based on model sizes and number of tokens seen, 70B model used about 985k A100 hours to fine tune (see utilization notes). Total GPU hours is thus around 2.7M",NVIDIA A100 SXM4 80 GB,,Confident,"We release Code Llama, a family of large language models for code based on Llama 2 providing state-of-the-art performance among open models, infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks. We provide multiple flavors to cover a wide range of applications: foundation models (Code Llama), Python specializations (Code Llama - Python), and instruction-following models (Code Llama - Instruct) with 7B, 13B and 34B parameters each. All models are trained on sequences of 16k tokens and show improvements on inputs with up to 100k tokens. 7B and 13B Code Llama and Code Llama - Instruct variants support infilling based on surrounding content. Code Llama reaches state-of-the-art performance among open models on several code benchmarks, with scores of up to 53% and 55% on HumanEval and MBPP, respectively. Notably, Code Llama - Python 7B outperforms Llama 2 70B on HumanEval and MBPP, and all our models outperform every other publicly available model on MultiPL-E. We release Code Llama under a permissive license that allows for both research and commercial use.",1.0,,Open weights (restricted use),United States of America,Llama 2-70B,4,"Fine tuning from base model uses 1T tokens.
70B * 1T * 6 = 4.2E23",400.0,0.3821,2024-11-25 15:23,,,,4000000.0,"Llama 2 pretraining used 4M batches. I believe the sentence below refers to the training from Llama 2 -> Code Llama-base. 

""We use a batch size of 4M tokens which are presented as sequences of 4,096 tokens each.""

Subsequent fine-tuning batch sizes are 500k-1M. 

""For Code Llama - Instruct, we train with a batch size of 524,288 tokens and on approx. 5B tokens in total... For long context fine-tuning (LCFT)... the batch size is set to 2M tokens for model sizes 7B and 13B and to 1M tokens for model size 34B, respectively. Training lasts for 10,000 gradient steps by default."" ",Industry,,,,2705320.0,Unreleased,"Llama 2 license. can't use outputs to train models.

https://github.com/meta-llama/llama/blob/main/LICENSE",Industry,,"Based on 6NC estimate, fine tuning required around 4.2e23 FLOP. Table 26 indicates 1400k A100 GPU hours used to train all twelve models, i.e. 1400k * 3600 * 3.12e14 = 1.57e24 FLOP. 70B model trained on 1T tokens while others used 500B. Python finetuning added another 100B tokens to each. 70B's share of total GPU hours was around:
(70 billion * 1.1 trillion) / (((7 + 13 + 34) billion * 600 billion) + (70 billion * 1.1 trillion)) = 0.7038
Implies actual GPU-hours for 70B model was: 0.7038 * 1400k = 985k
(Early version of paper without the 70B models indicated only 400k GPU-hours, which also indicates training 70B models took around 1M GPU-hours)
Total compute at peak throughput: 985k * 3600 * 3.12e14 = 1.107e24
Utilization: 4.23e23 / 1.107e24 = 0.3821",,316843.9329176827,Operation counting,,,,
Karakuri LM,Language,"Language modeling/generation,Chat",KARAKURI Inc.,,2024-01-26,KARAKURI LM,https://huggingface.co/karakuri-ai/karakuri-lm-70b-v0.1,,,,70000000000.0,70B,,,,"Uses cc100 (~0.6T tokens), mC4 (6.3T tokens), and RedPajama-Data-1T (1.2T tokens).

Seems likely these contain considerable overlap.",,"If no overlap, full dataset being trained on contains as many as 8.1T tokens.",,,,,Confident,,,,Open weights (restricted use),Japan,Llama 2-70B,,,,,2024-09-05 14:08,,,,,,Industry,,,,,,borrows Llama 2 license,Industry,,,,,,,,,
ProteinStructureTransformer,Biology,Protein or nucleotide language model (pLM/nLM),Max Planck Institute of Biochemistry,"Dexiong Chen, Philip Hartout, Paolo Pellizzoni, Carlos Oliver, Karsten Borgwardt",2024-01-26,ENDOWING PROTEIN LANGUAGE MODELS WITH STRUCTURAL KNOWLEDGE,https://arxiv.org/abs/2401.14819,7.0,,,1137000000.0,,7.616995200001001e+21,"GPU hour estimate
4*36000s*9.9e+14*0.4=5.7e+19

Base model: 7.560000000001e+21
Combined: 7616995200001001000000",,,160000001.0,"542,378 proteins Ã— 300 residues/protein = 162,713,400 tokens (1.627e8)

Final estimate: 1.6e8 datapoints",10.0,,NVIDIA H100 SXM5 80GB,,Confident,"Understanding the relationships between protein sequence, structure and function is a long-standing biological challenge with manifold implications from drug design to our understanding of evolution. Recently, protein language models have emerged as the preferred method for this challenge, thanks to their ability to harness large sequence databases. Yet, their reliance on expansive sequence data and parameter sets limits their flexibility and practicality in real-world scenarios. Concurrently, the recent surge in computationally predicted protein structures unlocks new opportunities in protein representation learning. While promising, the computational burden carried by such complex data still hinders widely-adopted practical applications. To address these limitations, we introduce a novel framework that enhances protein language models by integrating protein structural data. Drawing from recent advances in graph transformers, our approach refines the self-attention mechanisms of pretrained language transformers by integrating structural information with structure extractor modules. This refined model, termed Protein Structure Transformer (PST), is further pretrained on a small protein structure database, using the same masked language modeling objective as traditional protein language models. Empirical evaluations of PST demonstrate its superior parameter efficiency relative to protein language models, despite being pretrained on a dataset comprising only 542K structures. Notably, PST consistently outperforms the state-of-the-art foundation model for protein sequences, ESM-2, setting a new benchmark in protein function prediction. Our findings underscore the potential of integrating structural information into protein language models, paving the way for more effective and efficient protein modeling Code and pretrained models are available at this https URL.",100.0,,,Germany,ESM2-650M,56995200000000000000,,4.0,,2025-05-01 10:42,,,,,,Academia,,,,,,,Academia,,,,5545.139273980643,Hardware,,,,
DeepSeek Coder 33B,Language,Code generation,"DeepSeek,Peking University","Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y. Wu, Y.K. Li, Fuli Luo, Yingfei Xiong, Wenfeng Liang",2024-01-25,DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence,https://arxiv.org/abs/2401.14196,,,"SOTA among open-source: ""For coding capabilities, DeepSeek Coder achieves state-of-the-art performance among open-source code models on multiple programming languages and various benchmarks.""",33000000000.0,33B,3.96e+23,"""Step 1: Initially pre-trained with a dataset consisting of 87% code, 10% code-related language (Github Markdown and StackExchange), and 3% non-code-related Chinese language. Models are pre-trained using 1.8T tokens and a 4K window size in this step.
Step 2: Further Pre-training using an extended 16K window size on an additional 200B tokens, resulting in foundational models (DeepSeek-Coder-Base).
Step 3: Instruction Fine-tuning on 2B tokens of instruction data, resulting in instruction-tuned models (DeepSeek-Coder-Instruct).""

This means it was trained on 2T tokens. 2T * 33B * 6 = 3.96e23",,"""Trained from scratch on 2T tokens, including 87% code and 13% linguistic data in both English and Chinese languages.""",2000000000000.0,"""Trained from scratch on 2T tokens, including 87% code and 13% linguistic data in both English and Chinese languages.""

""The total data volume is 798 GB with 603 million files.""",,,,,Likely,"The rapid development of large language models has revolutionized code intelligence in software development. However, the predominance of closed-source models has restricted extensive research and development. To address this, we introduce the DeepSeek-Coder series, a range of open-source code models with sizes from 1.3B to 33B, trained from scratch on 2 trillion tokens. These models are pre-trained on a high-quality project-level code corpus and employ a fill-in-the-blank task with a 16K window to enhance code generation and infilling. Our extensive evaluations demonstrate that DeepSeek-Coder not only achieves state-of-the-art performance among open-source code models across multiple benchmarks but also surpasses existing closed-source models like Codex and GPT-3.5. Furthermore, DeepSeek-Coder models are under a permissive license that allows for both research and unrestricted commercial use.",,,Open weights (restricted use),"China,China",,,,,,2025-05-16 10:30,,,,,,"Industry,Academia",,,,,Unreleased,"code doesn't seem to be training code.

deepseek license:
https://github.com/deepseek-ai/DeepSeek-Coder/blob/main/LICENSE-MODEL","Industry,Academia",,,,,Operation counting,deepseek-ai,,,
Qwen-VL-Max,"Multimodal,Language,Vision","Chat,Image captioning,Face recognition,Visual question answering",Alibaba,,2024-01-25,Introducing Qwen-VL,https://qwenlm.github.io/blog/qwen-vl/,,SOTA improvement,"""Notably, Qwen-VL-Max outperforms both GPT-4V from OpenAI and Gemini from Google in tasks on Chinese question answering and Chinese text comprehension""",7000000000.0,"Not stated. Qwen-VL (less capable, presumably smaller version) is 9.6B

Upd: 7B parameters mentioned here
https://github.com/QwenLM/Qwen-VL#qwen-vl-plus",,,Unspecified unreleased,,,,,,,,Confident,"Along with the rapid development of our large language model Qwen, we leveraged Qwenâ€™s capabilities and unified multimodal pretraining to address the limitations of multimodal models in generalization, and we opensourced multimodal model Qwen-VL in Sep. 2023. Recently, the Qwen-VL series has undergone a significant upgrade with the launch of two enhanced versions, Qwen-VL-Plus and Qwen-VL-Max. The key technical advancements in these versions include:

Substantially boost in image-related reasoning capabilities;
Considerable enhancement in recognizing, extracting, and analyzing details within images and texts contained therein;
Support for high-definition images with resolutions above one million pixels and images of various aspect ratios.",,,API access,China,,,,,,2025-06-06 12:41,,,,,,Industry,,,,,Unreleased,https://help.aliyun.com/zh/dashscope/developer-reference/tongyi-qianwen-vl-plus-api,Industry,,,,,,,,,
text-embedding-3-small,Language,Language modeling,OpenAI,,2024-01-25,New embedding models and API updates,https://openai.com/index/new-embedding-models-and-api-updates/,,,,,,,,,,,,,,,,Unknown,,,,API access,United States of America,,,,,,2025-05-01 10:42,,,,,,Industry,,,,,,,Industry,,,,,,,,,
text-embedding-3-large,Language,Language modeling,OpenAI,,2024-01-25,New embedding models and API updates,https://openai.com/index/new-embedding-models-and-api-updates/,,,,,,,,,,,,,,,,Unknown,,,,API access,United States of America,,,,,,2025-05-01 10:42,,,,,,Industry,,,,,,,Industry,,,,,,,,,
Fuyu-Heavy,"Multimodal,Language,Vision","Chat,Language modeling/generation,Visual question answering,System control",Adept,,2024-01-24,Adept Fuyu-Heavy: A new multimodal model,https://www.adept.ai/blog/adept-fuyu-heavy,,,"According to Adept: ""Fuyu-Heavy is the worldâ€™s third-most-capable multimodal model, behind only GPT4-V and Gemini Ultra, which are 10-20 times bigger.""

""Fuyu-Heavy performs roughly on par with Gemini Pro on standard text-only evaluations, outperforming it on the commonly used MMLU benchmark.""",100000000000.0,"""Fuyu-Heavy is the worldâ€™s third-most-capable multimodal model, behind only GPT4-V and Gemini Ultra, which are 10-20 times bigger""

So possibly around ~100B params, though GPT-4/Gemini params aren't public",,Nvidia hardware,,"curated/generated image data:

""high-quality image pre-training data is scarce, weâ€™ve devoted a lot of effort to collecting, curating, and even creating this data. Thereâ€™s also a delicate balance between text and image tasks â€” we had to develop recipes for striking this balance at scale""",,,,,,,Speculative,"Weâ€™re excited to introduce Adept Fuyu-Heavy, a new multimodal model designed specifically for digital agents. Fuyu-Heavy is the worldâ€™s third-most-capable multimodal model, behind only GPT4-V and Gemini Ultra, which are 10-20 times bigger. Weâ€™re excited about this model because:

It excels at multimodal reasoning. To us the killer feature is UI understanding, but it also performs well on more traditional multimodal benchmarks. In particular, Fuyu-Heavy scores higher on the MMMU benchmark than even Gemini Pro.
On standard text-based benchmarks, it matches or exceeds the performance of models in the same compute class despite having to devote some of its capacity to image modeling.
It demonstrates that (with some modifications) we can scale up the Fuyu architecture and reap all of the associated benefits, including handling arbitrary size/shape images and efficiently re-using existing transformer optimizations.",,,Hosted access (no API),United States of America,,,,,,2025-06-09 14:58,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
Lumiere,"Multimodal,Video,Vision","Video generation,Text-to-video,Image-to-video","Google Research,Weizmann Institute of Science,Tel Aviv University,Technion - Israel Institute of Technology","Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Herrmann, Roni Paiss, Shiran Zada, Ariel Ephrat, Junhwa Hur, Guanghui Liu, Amit Raj, Yuanzhen Li, Michael Rubinstein, Tomer Michaeli, Oliver Wang, Deqing Sun, Tali Dekel, Inbar Mosseri",2024-01-23,Lumiere: A Space-Time Diffusion Model for Video Generation,https://arxiv.org/abs/2401.12945,115.0,,"Authors claim to attain state-of-the-art text-to-video generation results, though quantitative results in Table 1 indicate that Lumiere is did not set records for either metric (though it is at the Pareto frontier of these metrics). Figure 10 indicates qualitative improvements over baseline systems, but the baseline appears to be an average of all other systems.",,"The authors ""inflate"" a pre-trained T2I Imagen model (3B parameters). These pre-trained weights are frozen. An overview of the inflated architecture is given in Figure 4, but is not sufficient to calculate total trainable parameters.",,Actual # of newly trained parameters is unclear. The 3B Imagen weights are frozen.,,,2400000000.0,"We train our T2V model on a dataset containing 30M videos along with their text caption. The videos are 80 frames long at 16 fps (5 seconds). The base model is trained at 128Ã—128 and the SSR outputs 1024 Ã— 1024 frames.

30M videos * 80 frame/video = 2.4B",,,,Self-supervised learning,Confident,"We introduce Lumiere -- a text-to-video diffusion model designed for synthesizing videos that portray realistic, diverse and coherent motion -- a pivotal challenge in video synthesis. To this end, we introduce a Space-Time U-Net architecture that generates the entire temporal duration of the video at once, through a single pass in the model. This is in contrast to existing video models which synthesize distant keyframes followed by temporal super-resolution -- an approach that inherently makes global temporal consistency difficult to achieve. By deploying both spatial and (importantly) temporal down- and up-sampling and leveraging a pre-trained text-to-image diffusion model, our model learns to directly generate a full-frame-rate, low-resolution video by processing it in multiple space-time scales. We demonstrate state-of-the-art text-to-video generation results, and show that our design easily facilitates a wide range of content creation tasks and video editing applications, including image-to-video, video inpainting, and stylized generation.",200.0,,Unreleased,"Multinational,United States of America,Canada,Switzerland,Israel,Israel,Israel",Imagen,,,,,2025-05-19 12:49,,,,,,"Industry,Academia,Academia,Academia",,,,,,,"Industry,Academia,Academia,Academia",,,,,,,,,
Yi-VL-34B,"Vision,Language,Multimodal","Visual question answering,Language modeling/generation",01.AI,,2024-01-23,"Yi Vision Language Model
Better Bilingual Multimodal Model",https://huggingface.co/01-ai/Yi-VL-34B,,,,34000000000.0,34b,1.85174e+22,"989500000000000*240*3600*128*0.3 = 3.2829235e+22

6*34B*(100*10^6*224Ã—224/14Ã—14 + 25*10^6*448Ã—448/14Ã—14) =  1.04448 Ã— 10^22 

sqrt(3.2829235e+22*1.04448 Ã— 10^22 ) = 1.85174... Ã— 10^22","LAION-400M,LLaVAR,Flickr2K,VQAv2,RefCOCO",,,"""Stage 1: The parameters of ViT and the projection module are trained using an image resolution of 224Ã—224. The LLM weights are frozen. The training leverages an image caption dataset comprising 100 million image-text pairs from LAION-400M. ""

""Stage 2: The image resolution of ViT is scaled up to 448Ã—448, and the parameters of ViT and the projection module are trained. It aims to further boost the model's capability for discerning intricate visual details. The dataset used in this stage includes about 25 million image-text pairs, such as LAION-400M, CLLaVA, LLaVAR, Flickr, VQAv2, RefCOCO, Visual7w and so on.""",240.0,"""The total training time amounted to approximately 10 days for Yi-VL-34B""",NVIDIA H100 SXM5 80GB,,Confident,"Yi Vision Language (Yi-VL) model is the open-source, multimodal version of the Yi Large Language Model (LLM) series, enabling content comprehension, recognition, and multi-round conversations about images.

Yi-VL demonstrates exceptional performance, ranking first among all existing open-source models in the latest benchmarks including MMMU in English and CMMMU in Chinese (based on data available up to January 2024).

Yi-VL-34B is the first open-source 34B vision language model worldwide.",1.0,,Open weights (restricted use),China,"Yi-34B,CLIP ViT-H/14 - LAION-2B",,,128.0,,2025-02-10 15:20,,,,,,Industry,,,,,Unreleased,"""All usage must adhere to the Apache 2.0 license.

For free commercial use, you only need to send an email to get official commercial permission.""",Industry,,,,177456.311892851,"Hardware,Operation counting",01-ai,,,
Prothyena,Biology,Protein or nucleotide language model (pLM/nLM),Tokyo Institute of Technology,"Yiming Zhang, Manabu Okumura",2024-01-22,ProtHyena: A fast and efficient foundation protein language model at single amino acid Resolution,https://www.biorxiv.org/content/10.1101/2024.01.18.576206v1.abstract,2.0,,,4300000.0,"""This model employs a BPE tokenizer with
a vocabulary size of 10k, which consequentially expands the
model parameters to 4.3 million""",,,Pfam,,3000000001.0,"Data Estimate Summary:
10,000,000 sequences Ã— 300 amino acids = 3,000,000,000 tokens (3 billion)",,,,,Confident,"The emergence of self-supervised deep language models has revolutionized natural language processing tasks and has recently extended its applications to biological sequence analysis. Traditional models, primarily based on the Transformer and BERT architectures, demonstrate substantial effectiveness in various applications. However, these models are inherently constrained by the attention mechanismâ€™s quadratic computational complexity O(L2), limiting their efficiency and the length of context they can process. Addressing these limitations, we introduce ProtHyena, a novel approach that leverages the Hyena operator. This innovative methodology circumvents the constraints imposed by attention mechanisms, thereby reducing the time complexity to a subquadratic, enabling the modeling of extra-long protein sequences at the single amino acid level without the need to compress data. ProtHyena is able to achieve, and in many cases exceed, state-of-the-art results in various downstream tasks with only 10% of the parameters typically required by attention-based models. The architecture of ProtHyena presents a highly efficient solution for training protein predictors, offering a promising avenue for fast and efficient analysis of biological sequences.",,,,Japan,,,,,,2025-05-01 10:42,,,,,,Academia,,,,,,,Academia,,,,,,,,,
StableLM-2-1.6B,Language,Language modeling/generation,Stability AI,Stability AI Language Team,2024-01-18,Stable LM 2 1.6B,https://huggingface.co/stabilityai/stablelm-2-1_6b,,,,1644417024.0,Table under Model Architecture gives exact parameter count,1.92e+22,"6 * 1.6B * 2T = 19200000000000000000000
","RefinedWeb,RedPajama-Data,The Pile,StarCoder","""The dataset is comprised of a filtered mixture of open-source large-scale datasets available on the HuggingFace Hub: Falcon RefinedWeb extract (Penedo et al., 2023), RedPajama-Data (Together Computer., 2023) and The Pile (Gao et al., 2020) both without the Books3 subset, and StarCoder (Li et al., 2023). We further supplement our training with multi-lingual data from CulturaX (Nguyen et al., 2023) and, in particular, from its OSCAR corpora, as well as restructured data in the style of Yuan & Liu (2022).""",2000000000000.0,"""model pre-trained on 2 trillion tokens of diverse multilingual and code datasets for two epochs.""",,,NVIDIA A100 SXM4 40 GB,Self-supervised learning,Confident,,,,Open weights (non-commercial),United Kingdom of Great Britain and Northern Ireland,,,,512.0,,2025-05-27 10:35,,,,,,Industry,,,,,,"non-commercial:

https://huggingface.co/stabilityai/stablelm-2-1_6b/blob/main/LICENSE",Industry,,,,405659.59358954936,Operation counting,stabilityai,,,
AlphaGeometry,Mathematics,"Quantitative reasoning,Geometry","Google DeepMind,New York University (NYU)","Trieu H. Trinh, Yuhuai Wu, Quoc V. Le, He He, Thang Luong",2024-01-17,Solving olympiad geometry without human demonstrations,https://www.nature.com/articles/s41586-023-06747-5,186.0,SOTA improvement,"""On a test set of 30 latest olympiad-level problems, AlphaGeometry solves 25, outperforming the previous best method that only solves ten problems and approaching the performance of an average International Mathematical Olympiad (IMO) gold medallist.""",151000000.0,"""Overall, the transformer has 151 million parameters, excluding embedding layers at its input and output heads.""",,"Training details. Don't think there's enough info for a FLOP estimate.

""Our customized tokenizer is trained with â€˜wordâ€™ mode using
SentencePiece36 and has a vocabulary size of 757. We limit the maximum context length to 1,024 tokens and use T5-style relative position embedding37. Sequence packing38,39 is also used because more
than 90% of our sequences are under 200 in length. During training, a
dropout40 rate of 5% is applied pre-attention and post-dense. A 4â€‰Ã—â€‰4 slice of TPUv3 (ref.â€‰41) is used as its hardware accelerator. For pretraining, we train the transformer with a batch size of 16 per core
and a cosine learning-rate schedule that decays from 0.01 to 0.001
in 10,000,000 steps. For fine-tuning, we maintain the final learning rate of 0.001 for another 1,000,000 steps""",,Synthetic dataset of geometry proofs,,"100m examples of theorem-proofs

""By using existing symbolic engines on a diverse set of random theorem premises, we extracted 100 million synthetic theorems and their
proofs, many with more than 200 proof steps, four times longer than
the average proof length of olympiad theorems.""",,,Google TPU v3,,Confident,"Proving mathematical theorems at the olympiad level represents a notable milestone in human-level automated reasoning1,2,3,4, owing to their reputed difficulty among the worldâ€™s best talents in pre-university mathematics. Current machine-learning approaches, however, are not applicable to most mathematical domains owing to the high cost of translating human proofs into machine-verifiable format. The problem is even worse for geometry because of its unique translation challenges1,5, resulting in severe scarcity of training data. We propose AlphaGeometry, a theorem prover for Euclidean plane geometry that sidesteps the need for human demonstrations by synthesizing millions of theorems and proofs across different levels of complexity. AlphaGeometry is a neuro-symbolic system that uses a neural language model, trained from scratch on our large-scale synthetic data, to guide a symbolic deduction engine through infinite branching points in challenging problems. On a test set of 30 latest olympiad-level problems, AlphaGeometry solves 25, outperforming the previous best method that only solves ten problems and approaching the performance of an average International Mathematical Olympiad (IMO) gold medallist. Notably, AlphaGeometry produces human-readable proofs, solves all geometry problems in the IMO 2000 and 2015 under human expert evaluation and discovers a generalized version of a translated IMO theorem in 2004.",,,Open weights (unrestricted),"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France,United States of America",,,,,,2024-11-01 10:05,,,,,,"Industry,Academia",,,,,Open source,"Apache 2.0: https://github.com/google-deepmind/alphageometry 

Data is synthetic so can be reproduced using open code","Industry,Academia",,,,,,,,,
GLM-4,"Language,Multimodal,Image generation","Language modeling/generation,Question answering,Code generation,Text-to-image,Image generation",Zhipu AI,"Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Diego Rojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, Hao Yu, Hongning Wang, Jiadai Sun, Jiajie Zhang, Jiale Cheng, Jiayi Gui, Jie Tang, Jing Zhang, Juanzi Li, Lei Zhao, Lindong Wu, Lucen Zhong, Mingdao Liu, Minlie Huang, Peng Zhang, Qinkai Zheng, Rui Lu, Shuaiqi Duan, Shudan Zhang, Shulin Cao, Shuxun Yang, Weng Lam Tam, Wenyi Zhao, Xiao Liu, Xiao Xia, Xiaohan Zhang, Xiaotao Gu, Xin Lv, Xinghan Liu, Xinyi Liu, Xinyue Yang, Xixuan Song, Xunkai Zhang, Yifan An, Yifan Xu, Yilin Niu, Yuantao Yang, Yueyan Li, Yushi Bai, Yuxiao Dong, Zehan Qi, Zhaoyu Wang, Zhen Yang, Zhengxiao Du, Zhenyu Hou, Zihan Wang",2024-01-17,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools,https://arxiv.org/abs/2406.12793,,,,,,,,,"""To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage""",10000000000000.0,,,,,,Confident,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",,,Hosted access (no API),China,,,,,,2024-12-16 14:42,,,,,,Industry,,,,,,GLM-4 All Tools is accessible via the website https://chatglm.cn,Industry,,,,,,,,,
VideoCrafter2,"Video,Vision","Video generation,Text-to-video,Image-to-video",Tencent,"Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, Ying Shan",2024-01-17,VideoCrafter2: Overcoming Data Limitations for High-Quality Video Diffusion Models,https://arxiv.org/abs/2401.09047,,,,,,,,"WebVid-10M,LAION-COCO,JourneyDB","""For joint image and video training,
we utilize the low-quality WebVid-10M and LAION-COCO
datasets.""",,"""The training resolution is set at 512 Ã— 320. <..> The models are trained on 32 NVIDIA A100 GPUs for 270K iterations with a batch size of 128.  <..> The finetuning is conducted on 8 A100 GPUs for 30K iterations with a batch size of 256. Given that the images from JDB have a square resolution, we adjust the finetuning resolution to 512 Ã— 512.""",,,NVIDIA A100,,Unknown,"Text-to-video generation aims to produce a video based on a given prompt. Recently, several commercial video models have been able to generate plausible videos with minimal noise, excellent details, and high aesthetic scores. However, these models rely on large-scale, well-filtered, high-quality videos that are not accessible to the community. Many existing research works, which train models using the low-quality WebVid-10M dataset, struggle to generate high-quality videos because the models are optimized to fit WebVid-10M. In this work, we explore the training scheme of video models extended from Stable Diffusion and investigate the feasibility of leveraging low-quality videos and synthesized high-quality images to obtain a high-quality video model. We first analyze the connection between the spatial and temporal modules of video models and the distribution shift to low-quality videos. We observe that full training of all modules results in a stronger coupling between spatial and temporal modules than only training temporal modules. Based on this stronger coupling, we shift the distribution to higher quality without motion degradation by finetuning spatial modules with high-quality images, resulting in a generic high-quality video model. Evaluations are conducted to demonstrate the superiority of the proposed method, particularly in picture quality, motion, and concept composition.",,,Open weights (unrestricted),China,,,,32.0,,2025-05-19 12:51,,,,,,Industry,,,,,Open source,"""code, models and data are distributed under Apache 2.0 License.""

https://github.com/AILab-CVC/VideoCrafter?tab=readme-ov-file",Industry,,,,25354.289217139973,,,,,
DeciCoder-6B,Language,"Chat,Code autocompletion",Deci AI,DeciAI Research Team,2024-01-15, Model Card for DeciCoder-6B,https://huggingface.co/Deci/DeciCoder-6B,,,,6000000000.0,6B,7.56e+21,"Python, Java, Javascript, Rust, C++, C, and C# subset of Starcoder Training Dataset

From the Starcoder paper, that's 7.9% + 11.3% + 8.4% + 1.2% + 6.4% + 7% + 5.8% of the total, so 48% of 815 GB, say 410 GB of code. If we assume they trained for 2 epochs, 820 GB.

820 GB * 200M word per GB = 1.6e11 words
1.6e11 / 0.75 = 2.1e11 tokens

C = 6ND = 6 * 2.1e11 * 6e9 = 7.6e21",StarCoder,,,,,,,,Likely,,,,Open weights (unrestricted),Israel,,,,,,2025-02-14 15:49,,,,,,Industry,,,,,,Apache 2.0,Industry,,,,,Operation counting,,,,
OmniNA,Biology,Protein or nucleotide language model (pLM/nLM),Tianjin Medical University,"Xilin Shen, Xiangchun Li",2024-01-15,OmniNA: A foundation model for nucleotide sequences,https://www.biorxiv.org/content/10.1101/2024.01.14.575543v1.abstract,3.0,,,1700000000.0,,2.51092992e+21,"200k steps, 2048batch, 601 sequence length

200k*2048*601=246169600000

Compute: 6*1.7e+9*2.5e+11=2.5e+21

Epoch: 246169600000/1076462666666=0.22",,,1076462666666.0,"From Nucleotide Sequences: 1,076,200,000,000 tokens
From Text Annotations: 197,000,000 words (estimated at 197000000/0.75=262666666 tokens
Total: 1076462666666",,,NVIDIA A100 SXM4 80 GB,,Likely,"Foundation models have demonstrated exceptional efficacy across diverse downstream tasks. However, within the realms of genomics and transcriptomics, a notable gap persists in the availability of models that afford a comprehensive understanding of nucleotide sequence principles across various species. Here, we present OmniNA, a foundation generative model designed for comprehensive nucleotide sequence learning. The model was pre-trained on 91.7 million nucleotide sequences and the corresponding annotations encompassing 1076.2 billion bases and 197 million words spanning a multitude of species. We demonstrated OmniNA gains the capacity to understand the semantics of the nucleotide sequence and textual annotations by analyzing the learned representation of the pre-trained model. OmniNA can be fine-tuned to align multiple nucleotide learning tasks with natural language paradigms. We demonstrate OmniNA-1.7B surpasses or rivals state-of-the art methods in 17 nucleotide tasks, encompassing nucleotide sequences detection and species classification. The modelâ€™s understanding of nucleotide grammars enhances its capability to reveal the mutation effect of nucleotide sequence on DNA and RNA processing. We hereby release the OmniNA-1.7B model as an open-source contribution to the research community. This foundation model signifies a step toward advancing our comprehension of nucleotide sequences across diverse species and holds substantial promise to facilitating genomics and transcriptomics research.",0.22,,,China,,,,8.0,,2025-05-01 10:42,,,,,,Academia,,,,,,,Academia,,,,6338.854622611998,Operation counting,,,,
InternViT-6B,"Vision,Language",Visual question answering,"Shanghai AI Lab,Nanjing University,The University of Hong Kong,Tsinghua University,SenseTime,University of Science and Technology of China (USTC)","Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, Jifeng Dai",2024-01-15,InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks,https://arxiv.org/abs/2312.14238,,,,6000000000.0,6B,,,,,,,,,NVIDIA A100,,Confident,"The exponential growth of large language models (LLMs) has opened up numerous possibilities for multimodal AGI systems. However, the progress in vision and vision-language foundation models, which are also critical elements of multi-modal AGI, has not kept pace with LLMs. In this work, we design a large-scale vision-language foundation model (InternVL), which scales up the vision foundation model to 6 billion parameters and progressively aligns it with the LLM, using web-scale image-text data from various sources. This model can be broadly applied to and achieve state-of-the-art performance on 32 generic visual-linguistic benchmarks including visual perception tasks such as image-level or pixel-level recognition, vision-language tasks such as zero-shot image/video classification, zero-shot image/video-text retrieval, and link with LLMs to create multi-modal dialogue systems. It has powerful visual capabilities and can be a good alternative to the ViT-22B. We hope that our research could contribute to the development of multi-modal large models. Code and models are available at this https URL.",,,,"China,China,Hong Kong,China,China,Hong Kong,China,China",LLaMA-7B,,,640.0,,2025-05-01 10:42,,,,,,"Academia,Academia,Academia,Academia,Industry,Academia",,,,,,,"Academia,Academia,Academia,Academia,Industry,Academia",,,,507108.36980895983,,,,,
Yiye Qingzhou-0.7B,"Multimodal,Language,Vision,Speech,Video","Language modeling/generation,Question answering,Visual question answering,Speech recognition,Video description",EFFYIC (è¯†å› æ™ºèƒ½),,2024-01-15,"36æ°ªé¦–å‘ | ä¸“æ³¨ä¼ä¸šçº§å¤§æ¨¡åž‹è½åœ°ï¼Œã€Œè¯†å› æ™ºèƒ½ã€å®Œæˆæ•°åƒä¸‡å…ƒé¦–è½®èžèµ„
","https://36kr.com/p/3062982772761984
https://www.effyic.com/technical_core",,,,700000000.0,0.7B,,,Unspecified unreleased,,,,,,,,Confident,,,,Hosted access (no API),China,,,,,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
Yiye Qingzhou-45B,"Multimodal,Language,Vision,Speech,Video","Language modeling/generation,Question answering,Visual question answering,Speech recognition,Video description",EFFYIC (è¯†å› æ™ºèƒ½),,2024-01-15,"36æ°ªé¦–å‘ | ä¸“æ³¨ä¼ä¸šçº§å¤§æ¨¡åž‹è½åœ°ï¼Œã€Œè¯†å› æ™ºèƒ½ã€å®Œæˆæ•°åƒä¸‡å…ƒé¦–è½®èžèµ„
","https://36kr.com/p/3062982772761984
https://www.effyic.com/technical_core",,,,45000000000.0,45B,,,Unspecified unreleased,,,,,,,,Confident,,,,Hosted access (no API),China,,,,,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
 InternVL,"Vision,Language","Visual question answering,Image classification,Image captioning","Shanghai AI Lab,Nanjing University,The University of Hong Kong,Tsinghua University,SenseTime,University of Science and Technology of China (USTC)","Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, Jifeng Dai",2024-01-15,InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks,https://arxiv.org/abs/2312.14238,,,,14000000000.0,"14B
",1.744956e+23,"trainable / total parameters 
stage 1: 13B / 13B 
stage 2: 1B / 14B

training tokens: 
stage 1: (28.7-0.5)*0.5*(196/16)^2 + 0.5*(224/16)^2  = 2213B
stage 2: 1.6*(224/16)^2 = 313.6 B

6*13B*2213B + 6*1B*313.6 B = 174495.6 *10^18 = 1.744956 Ã— 1023","LAION-COCO,COYO-700M,SBU,Conceptual Captions (CC3M),Conceptual Captions 12M (CC12M),Wukong,LAION",,2527000000000.0,"Stage 1

"" The training involves a total batch size of 164K across 640 A100 GPUs, extending over 175K iterations to process about 28.7 billion samples. To enhance efficiency, we initially train at a 196Ã—196 resolution, masking 50% of image tokens [87], and later switch to 224Ã—224 resolution without masking for the final 0.5 billion samples.""

Stage 2

1.6B samples
""The input images are processed at a resolution of 224Ã—224. For optimization, the AdamW optimizer [98] is employed with Î²1 = 0.9, Î²2 = 0.98, weight decay set at 0.05, and a total batch size of 20K. The training extends over 80K steps across 160 A100 GPUs""

28.7 billion / 4.98 = 5.76 epochs

""As summarized in Table 2, the original dataset contains 6.03 billion image-text pairs, and 4.98 billion remains after cleaning.""

patch size is 14

(28.7-0.5)*0.5*(196/16)^2 + 0.5*(224/16)^2 + 1.6*(224/16)^2 = 2 527 000 000 000 training tokens",800.0,1.744956e+23/(312000000000000*640*3600*0.3) = 809 hours ~ 1 month,NVIDIA A100 SXM4 80 GB,,Likely,"The exponential growth of large language models (LLMs) has opened up numerous possibilities for multimodal AGI systems. However, the progress in vision and vision-language foundation models, which are also critical elements of multi-modal AGI, has not kept pace with LLMs. In this work, we design a large-scale vision-language foundation model (InternVL), which scales up the vision foundation model to 6 billion parameters and progressively aligns it with the LLM, using web-scale image-text data from various sources. This model can be broadly applied to and achieve state-of-the-art performance on 32 generic visual-linguistic benchmarks including visual perception tasks such as image-level or pixel-level recognition, vision-language tasks such as zero-shot image/video classification, zero-shot image/video-text retrieval, and link with LLMs to create multi-modal dialogue systems. It has powerful visual capabilities and can be a good alternative to the ViT-22B. We hope that our research could contribute to the development of multi-modal large models. Code and models are available at this https URL.",,,Open weights (unrestricted),"China,China,Hong Kong,China,China,Hong Kong,China,China","InternViT-6B,LLaMA-7B",,,640.0,,2025-05-26 16:10,,,,164000.0,,"Academia,Academia,Academia,Academia,Industry,Academia",,,,,Unreleased,"MIT license
https://huggingface.co/OpenGVLab/InternVL-14B-224px

I cannot find training code for this model here https://github.com/OpenGVLab/InternVL","Academia,Academia,Academia,Academia,Industry,Academia",,,BF16,507108.36980895983,Operation counting,OpenGVLab,,,
InternLM2-20B,Language,"Chat,Language modeling/generation,Question answering","Shanghai AI Lab,SenseTime,Chinese University of Hong Kong (CUHK),Fudan University","Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, Xiaoyi Dong, Haodong Duan, Qi Fan, Zhaoye Fei, Yang Gao, Jiaye Ge, Chenya Gu, Yuzhe Gu, Tao Gui, Aijia Guo, Qipeng Guo, Conghui He, Yingfan Hu, Ting Huang, Tao Jiang, Penglong Jiao, Zhenjiang Jin, Zhikai Lei, Jiaxing Li, Jingwen Li, Linyang Li, Shuaibin Li, Wei Li, Yining Li, Hongwei Liu, Jiangning Liu, Jiawei Hong, Kaiwen Liu, Kuikun Liu, Xiaoran Liu, Chengqi Lv, Haijun Lv, Kai Lv, Li Ma, Runyuan Ma, Zerun Ma, Wenchang Ning, Linke Ouyang, Jiantao Qiu, Yuan Qu, Fukai Shang, Yunfan Shao, Demin Song, Zifan Song, Zhihao Sui, Peng Sun, Yu Sun, Huanze Tang, Bin Wang, Guoteng Wang, Jiaqi Wang, Jiayu Wang, Rui Wang, Yudong Wang, Ziyi Wang, Xingjian Wei, Qizhen Weng, Fan Wu, Yingtong Xiong, Chao Xu, Ruiliang Xu, Hang Yan, Yirong Yan, Xiaogui Yang, Haochen Ye, Huaiyuan Ying, Jia Yu, Jing Yu, Yuhang Zang, Chuyu Zhang, Li Zhang, Pan Zhang, Peng Zhang, Ruijie Zhang, Shuo Zhang, Songyang Zhang, Wenjian Zhang, Wenwei Zhang, Xingcheng Zhang, Xinyue Zhang, Hui Zhao, Qian Zhao, Xiaomeng Zhao, Fengzhe Zhou, Zaida Zhou, Jingming Zhuo, Yicheng Zou, Xipeng Qiu, Yu Qiao, Dahua Lin",2024-01-12,InternLM2 Technical Report,https://arxiv.org/abs/2403.17297,,,,20000000000.0,20B,3.12e+23,6ND = 6 * 2600000000000 * 20000000000 = 3.12e+23,Unspecified unreleased,"""The text data in our pre-training dataset can be categorized by source into web pages,
papers, patents, and books. To transform these sources into a pre-training dataset, we first
standardize all data into a specified format, categorize them by type and language, and
store them in JSON Lines (jsonl) format. Then, for all data, we apply several processing
steps including rule-based filtering, data deduplication, safety filtering, and quality filtering.
This results in a rich, safe, and high-quality text dataset.""",2600000000000.0,"""The total number of tokens used for pre-training the 1.8B, 7B, and 20B models ranges from 2.0T to 2.6T, and the pre-training process consists of three distinct phases. """,,,,,Confident,"The evolution of Large Language Models (LLMs) like ChatGPT and GPT-4 has sparked discussions on the advent of Artificial General Intelligence (AGI). However, replicating such advancements in open-source models has been challenging. This paper introduces InternLM2, an open-source LLM that outperforms its predecessors in comprehensive evaluations across 6 dimensions and 30 benchmarks, long-context modeling, and open-ended subjective evaluations through innovative pre-training and optimization techniques. The pre-training process of InternLM2 is meticulously detailed, highlighting the preparation of diverse data types including text, code, and long-context data. InternLM2 efficiently captures long-term dependencies, initially trained on 4k tokens before advancing to 32k tokens in pre-training and fine-tuning stages, exhibiting remarkable performance on the 200k ``Needle-in-a-Haystack"" test. InternLM2 is further aligned using Supervised Fine-Tuning (SFT) and a novel Conditional Online Reinforcement Learning from Human Feedback (COOL RLHF) strategy that addresses conflicting human preferences and reward hacking. By releasing InternLM2 models in different training stages and model sizes, we provide the community with insights into the model's evolution.",1.0,,Open weights (restricted use),"China,Hong Kong,China,Hong Kong,China,China",,,,,,2025-02-07 13:56,,,,5000000.0,Table 3. ,"Academia,Industry,Academia,Academia",,,,,Unreleased,need to apply for commercial license. there's a repo but doesn't look like there's pretraining code.  https://github.com/InternLM/InternLM ,"Academia,Industry,Academia,Academia",,,,,Operation counting,internlm,,,
DeepSeekMoE-16B,Language,Chat,DeepSeek,"Damai Dai, Chengqi Deng, Chenggang Zhao, R.X. Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Y. Wu, Zhenda Xie, Y.K. Li, Panpan Huang, Fuli Luo, Chong Ruan, Zhifang Sui, Wenfeng Liang",2024-01-11,DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models,https://arxiv.org/abs/2401.06066,101.0,,"""In addition, DeepSeekMoE 2B nearly approaches the performance of its dense counterpart with the same number of total parameters, which set the upper bound of MoE models. Subsequently, we scale up DeepSeekMoE to 16B parameters and show that it achieves comparable performance with LLaMA2 7B, with only about 40% of computations""",16000000000.0,"16B (total, but it's sparse)",3.4e+22,"""With the DeepSeekMoE architecture, we scale up our MoE model to a larger scale with 16B total parameters and train it on 2T tokens""

""Evaluation results reveal that with only about 40% of computations, DeepSeekMoE 16B achieves comparable performance with DeepSeek 7B (DeepSeek-AI, 2024), a dense model trained on the same 2T corpus""

40% * 7B = 2.8B, so 2.8B effective parameters (see also Table 4)

2.8B * 2T * 6 ~= 3.4e22

Table 4 shows 74.4T FLOPs per 4k tokens. Over 2T tokens, this is 3.72e22 FLOPs (unclear whether this is FLOPs for training or inference)",,"""Our training data is sampled from a large-scale multilingual corpus created by DeepSeek-AI. The corpus primarily focuses on English and Chinese but also encompasses other languages. It is derived from diverse sources, including web text, mathematical material, coding scripts, published literature, and various other textual materials. For the purpose of validation experiments, we sample a subset containing 100B tokens from the corpus to train our models""",2000000000000.0,"""Leveraging our architecture, we subsequently scale up the model parameters to 16B and train DeepSeekMoE 16B on a large-scale corpus with 2T tokens.""
Not actually clear that the dataset size is 2T, since 2T appears to be the number of tokens trained over, possibly for multiple epochs.",,,"NVIDIA A100,NVIDIA H800 SXM5",,Likely,"In the era of large language models, Mixture-of-Experts (MoE) is a promising architecture for managing computational costs when scaling up model parameters. However, conventional MoE architectures like GShard, which activate the top-K out of N experts, face challenges in ensuring expert specialization, i.e. each expert acquires non-overlapping and focused knowledge. In response, we propose the DeepSeekMoE architecture towards ultimate expert specialization. It involves two principal strategies: (1) finely segmenting the experts into mN ones and activating mK from them, allowing for a more flexible combination of activated experts; (2) isolating Ks experts as shared ones, aiming at capturing common knowledge and mitigating redundancy in routed experts. Starting from a modest scale with 2B parameters, we demonstrate that DeepSeekMoE 2B achieves comparable performance with GShard 2.9B, which has 1.5 times the expert parameters and computation. In addition, DeepSeekMoE 2B nearly approaches the performance of its dense counterpart with the same number of total parameters, which set the upper bound of MoE models. Subsequently, we scale up DeepSeekMoE to 16B parameters and show that it achieves comparable performance with LLaMA2 7B, with only about 40% of computations. Further, our preliminary efforts to scale up DeepSeekMoE to 145B parameters consistently validate its substantial advantages over the GShard architecture, and show its performance comparable with DeepSeek 67B, using only 28.5% (maybe even 18.2%) of computations.",1.0,,Open weights (restricted use),China,,,,,,2024-11-10 17:26,,,,,,Industry,,,,,,abuse restrictions: https://github.com/deepseek-ai/DeepSeek-MoE/blob/main/LICENSE-MODEL,Industry,,,,,Operation counting,,,,
MAGNeT,Audio,Audio generation,"Meta AI,Hebrew University of Jerusalem,Kyutai","Alon Ziv, Itai Gat, Gael Le Lan, Tal Remez, Felix Kreuk, Alexandre DÃ©fossez, Jade Copet, Gabriel Synnaeve, Yossi Adi",2024-01-09,Masked Audio Generation using a Single Non-Autoregressive Transformer,"https://arxiv.org/abs/2401.04577, https://github.com/facebookresearch/audiocraft/blob/main/docs/MAGNET.md",,,"Mainly efficiency benefits: ""The proposed approach is comparable to the evaluated baselines, while being significantly faster (x7 faster than the autoregressive baseline).""",1500000000.0,"""We train non-autoregressive transformer models using 300M (MAGNET-small) and 1.5B
(MAGNET-large) parameters.""",,"""We train the models for
1M steps with the AdamW optimizer (Loshchilov & Hutter, 2017), a batch size of 192 examples... We train the models using respectively 32 GPUs for the small model and 64 GPUs for the large ones with float16 precision.""","ShutterStock,Pond5","""We follow the same setup as in Copet et al. (2023) and use 20K hours of licensed music to train
MAGNET.  Specifically, we rely on the same 10K high-quality music tracks, the ShutterStock,
and Pond5 music data collections as used in Copet et al. (2023)
8 with respectively 25K and 365K
instrument-only music tracks""

Copet et al is the MusicGen paper",,20k hours,,,,,Likely,"We introduce MAGNeT, a masked generative sequence modeling method that operates directly over several streams of audio tokens. Unlike prior work, MAGNeT is comprised of a single-stage, non-autoregressive transformer. During training, we predict spans of masked tokens obtained from a masking scheduler, while during inference we gradually construct the output sequence using several decoding steps. To further enhance the quality of the generated audio, we introduce a novel rescoring method in which, we leverage an external pre-trained model to rescore and rank predictions from MAGNeT, which will be then used for later decoding steps. Lastly, we explore a hybrid version of MAGNeT, in which we fuse between autoregressive and non-autoregressive models to generate the first few seconds in an autoregressive manner while the rest of the sequence is being decoded in parallel. We demonstrate the efficiency of MAGNeT for the task of text-to-music and text-to-audio generation and conduct an extensive empirical evaluation, considering both objective metrics and human studies. The proposed approach is comparable to the evaluated baselines, while being significantly faster (x7 faster than the autoregressive baseline). Through ablation studies and analysis, we shed light on the importance of each of the components comprising MAGNeT, together with pointing to the trade-offs between autoregressive and non-autoregressive modeling, considering latency, throughput, and generation quality. Samples are available on our demo page this https URL.",,,Open weights (unrestricted),"United States of America,Israel,France",,,,,,2024-09-05 14:08,,,,,,"Industry,Academia,Industry",,,,,,MIT,"Industry,Academia,Industry",,,,,,,,,
Stable Code 3B,Language,"Language modeling/generation,Code generation",Stability AI,"Pinnaparaju, Nikhil and Adithyan, Reshinth and Phung, Duy and Tow, Jonathan and Baicoianu, James and  and Cooper, Nathan",2024-01-09,Stable Code 3B,https://huggingface.co/stabilityai/stable-code-3b,,,,2796431360.0,"2796431360 from https://huggingface.co/stabilityai/stable-code-3b#model-architecture
""stable-code-3b is a 2.7B billion parameter decoder-only language model pre-trained on 1.3 trillion tokens of diverse textual and code datasets. """,2.106e+22,"6ND = 2.7e9 * 1.3e12 * 6 = 2,106E+22
""stable-code-3b is a 2.7B billion parameter decoder-only language model pre-trained on 1.3 trillion tokens of diverse textual and code datasets. ""","RefinedWeb,GitHub,StarCoder","""Training Dataset

The dataset is comprised of a filtered mixture of open-source large-scale datasets available on the HuggingFace Hub: Falcon RefinedWeb extract (Penedo et al., 2023), along with CommitPackFT and Github Issues (BigCode., 2023), and StarCoder (Li et al., 2023). We further supplement our training with data from mathematical domains (Azerbayev, Zhangir, et al., 2023 and, Yu, Longhui, et al., 2023). """,,"1.3T tokens
""stable-code-3b is a 2.7B billion parameter decoder-only language model pre-trained on 1.3 trillion tokens of diverse textual and code datasets. """,,,NVIDIA A100 SXM4 40 GB,,Likely,,,,Open weights (non-commercial),United Kingdom of Great Britain and Northern Ireland,,,,256.0,,2024-10-26 12:37,,,,,,Industry,,,,,,"non-commercial by default. looks like they charge for commercial licenses? 
https://stability.ai/news/introducing-stability-ai-membership",Industry,,,,202870.4528973297,Operation counting,,,,
FABind,Biology,Protein-ligand contact prediction,"Renmin University of China,Huazhong University of Science and Technology,Microsoft Research AI for Science,University of Science and Technology of China (USTC)","Qizhi Pei, Kaiyuan Gao, Lijun Wu, Jinhua Zhu, Yingce Xia, Shufang Xie, Tao Qin, Kun He, Tie-Yan Liu, Rui Yan",2024-01-09,FABind: Fast and Accurate Protein-Ligand Binding,https://arxiv.org/abs/2310.06763v5,,,,,,,,PDBbind,"After the filtration, we used 17, 299 complexes that were recorded before 2019 for training
purposes

Estimating a protein-ligand pair at 500 tokens

17299*500=8649500",8649500.0,,,,NVIDIA V100,,Likely,"Modeling the interaction between proteins and ligands and accurately predicting their binding structures is a critical yet challenging task in drug discovery. Recent advancements in deep learning have shown promise in addressing this challenge, with sampling-based and regression-based methods emerging as two prominent approaches. However, these methods have notable limitations. Sampling-based methods often suffer from low efficiency due to the need for generating multiple candidate structures for selection. On the other hand, regression-based methods offer fast predictions but may experience decreased accuracy. Additionally, the variation in protein sizes often requires external modules for selecting suitable binding pockets, further impacting efficiency. In this work, we propose FABind, an end-to-end model that combines pocket prediction and docking to achieve accurate and fast protein-ligand binding. FABind incorporates a unique ligand-informed pocket prediction module, which is also leveraged for docking pose estimation. The model further enhances the docking process by incrementally integrating the predicted pocket to optimize protein-ligand binding, reducing discrepancies between training and inference. Through extensive experiments on benchmark datasets, our proposed FABind demonstrates strong advantages in terms of effectiveness and efficiency compared to existing methods. Our code is available at this https URL",500.0,,,"China,China,United States of America,United Kingdom of Great Britain and Northern Ireland,China,Netherlands,Germany,China",,,,8.0,,2025-05-01 10:42,,,,,,"Academia,Academia,Industry,Academia",,,,,,,"Academia,Academia,Industry,Academia",,,,4754.776239781165,,,,,
Improved motif-scaffolding with SE(3) flow matching,Biology,Protein design,"University of Oxford,Massachusetts Institute of Technology (MIT),Microsoft Research AI for Science","Jason Yim, Andrew Campbell, Emile Mathieu, Andrew Y. K. Foong, Michael Gastegger, JosÃ© JimÃ©nez-Luna, Sarah Lewis, Victor Garcia Satorras, Bastiaan S. Veeling, Frank NoÃ©, Regina Barzilay, Tommi S. Jaakkola",2024-01-08,Improved motif-scaffolding with SE(3) flow matching,https://arxiv.org/abs/2401.04082,,,,16800000.0,,1.6000000000000008e+19,"1. Hardware: 2x NVIDIA RTX A6000 (3.87e13 FLOP/s per GPU)
2. Training duration: 6 days = 518,400 seconds (directly provided)
3. Utilization: 40%
4. Calculation: 
   2 GPUs Ã— 3.87e13 FLOP/s Ã— 518,400s Ã— 0.40 = 1.6e19 FLOPs",,,,,144.0,,NVIDIA RTX A6000,,Confident,"Protein design often begins with the knowledge of a desired function from a motif which motif-scaffolding aims to construct a functional protein around. Recently, generative models have achieved breakthrough success in designing scaffolds for a range of motifs. However, generated scaffolds tend to lack structural diversity, which can hinder success in wet-lab validation. In this work, we extend FrameFlow, an SE(3) flow matching model for protein backbone generation, to perform motif-scaffolding with two complementary approaches. The first is motif amortization, in which FrameFlow is trained with the motif as input using a data augmentation strategy. The second is motif guidance, which performs scaffolding using an estimate of the conditional score from FrameFlow without additional training. On a benchmark of 24 biologically meaningful motifs, we show our method achieves 2.5 times more designable and unique motif-scaffolds compared to state-of-the-art. Code: this https URL",,,,"United Kingdom of Great Britain and Northern Ireland,United States of America,United States of America,United Kingdom of Great Britain and Northern Ireland,China,Netherlands,Germany",,,,2.0,,2025-05-01 10:42,,,,,,"Academia,Academia,Industry",,,,,,,"Academia,Academia,Industry",,,,1188.7205317093983,Hardware,,,,
DeepSeek LLM 67B,Language,"Chat,Language modeling/generation,Question answering",DeepSeek,"Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, Huazuo Gao, Kaige Gao, Wenjun Gao, Ruiqi Ge, Kang Guan, Daya Guo, Jianzhong Guo, Guangbo Hao, Zhewen Hao, Ying He, Wenjie Hu, Panpan Huang, Erhang Li, Guowei Li, Jiashi Li, Yao Li, Y.K. Li, Wenfeng Liang, Fangyun Lin, A.X. Liu, Bo Liu, Wen Liu, Xiaodong Liu, Xin Liu, Yiyuan Liu, Haoyu Lu, Shanghao Lu, Fuli Luo, Shirong Ma, Xiaotao Nie, Tian Pei, Yishi Piao, Junjie Qiu, Hui Qu, Tongzheng Ren, Zehui Ren, Chong Ruan, Zhangli Sha, Zhihong Shao, Junxiao Song, Xuecheng Su, Jingxiang Sun, Yaofeng Sun, Minghui Tang, Bingxuan Wang, Peiyi Wang, Shiyu Wang, Yaohui Wang, Yongji Wang, Tong Wu, Y. Wu, Xin Xie, Zhenda Xie, Ziwei Xie, Yiliang Xiong, Hanwei Xu, R.X. Xu, Yanhong Xu, Dejian Yang, Yuxiang You, Shuiping Yu, Xingkai Yu, B. Zhang, Haowei Zhang, Lecong Zhang, Liyue Zhang, Mingchuan Zhang, Minghua Zhang, Wentao Zhang, Yichao Zhang, Chenggang Zhao, Yao Zhao, Shangyan Zhou, Shunfeng Zhou, Qihao Zhu, Yuheng Zou",2024-01-05,DeepSeek LLM: Scaling Open-Source Language Models with Longtermism,"https://arxiv.org/abs/2401.02954, https://github.com/deepseek-ai/DeepSeek-LLM",,,"One of the best open/Chinese models: ""Our evaluation results demonstrate that DeepSeek LLM 67B surpasses LLaMA-2 70B on various benchmarks, particularly in the domains of code, mathematics, and reasoning. Furthermore, open-ended evaluations reveal that DeepSeek LLM 67B Chat exhibits superior performance compared to GPT-3.5.""",67000000000.0,"67B
",8.04e+23,67B * 2T * 6 = 8.04e23,Unspecified unreleased,"""We collect 2 trillion tokens for pre-training, primarily in Chinese and English.""

""We have gained valuable insights from reputable sources such as (Computer, 2023; Gao et al., 2020; Penedo et al., 2023; Touvron et al., 2023a)... We adopted an aggressive deduplication strategy, expanding the deduplication scope. Our analysis revealed that deduplicating the entire Common Crawl corpus results in higher removal of duplicate instances compared to deduplicating within a single dump""",2000000000000.0,"""We collect 2 trillion tokens for pre-training, primarily in Chinese and English""",,,,,Confident,"The rapid development of open-source large language models (LLMs) has been truly remarkable. However, the scaling law described in previous literature presents varying conclusions, which casts a dark cloud over scaling LLMs. We delve into the study of scaling laws and present our distinctive findings that facilitate scaling of large scale models in two commonly used open-source configurations, 7B and 67B. Guided by the scaling laws, we introduce DeepSeek LLM, a project dedicated to advancing open-source language models with a long-term perspective. To support the pre-training phase, we have developed a dataset that currently consists of 2 trillion tokens and is continuously expanding. We further conduct supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) on DeepSeek LLM Base models, resulting in the creation of DeepSeek Chat models. Our evaluation results demonstrate that DeepSeek LLM 67B surpasses LLaMA-2 70B on various benchmarks, particularly in the domains of code, mathematics, and reasoning. Furthermore, open-ended evaluations reveal that DeepSeek LLM 67B Chat exhibits superior performance compared to GPT-3.5.",1.0,,Open weights (restricted use),China,,,,,,2025-05-16 10:30,,,,,,Industry,,,,,Unreleased,"repo with inference code and details, but no training code:

https://github.com/deepseek-ai/deepseek-LLM/blob/main/LICENSE-MODEL",Industry,,,,,Operation counting,deepseek-ai,,,
RT-1 + AutoRT,Robotics,Robotic manipulation,Google DeepMind,"Michael Ahn, Debidatta Dwibedi, Chelsea Finn, Montse Gonzalez Arenas, Keerthana Gopalakrishnan, Karol Hausman, Brian Ichter, Alex Irpan, Nikhil Joshi, Ryan Julian, Sean Kirmani, Isabel Leal, Edward Lee, Sergey Levine, Yao Lu, Isabel Leal, Sharath Maddineni, Kanishka Rao, Dorsa Sadigh, Pannag Sanketi, Pierre Sermanet, Quan Vuong, Stefan Welker, Fei Xia, Ted Xiao, Peng Xu, Steve Xu, Zhuo Xu",2024-01-04,AutoRT: Embodied Foundation Models for Large Scale Orchestration of Robotic Agents,https://auto-rt.github.io/,,,"Some improvements on RT-1: ""The co-fine-tuned model is evaluated on two tasks we find RT-1 generalizes poorly to: picking from
different heights, and wiping. Exact evaluation instructions and details are in Appendix F. When co-fine-tuned, RT-1â€™s performance increases from 0% to 12.5% on picking from different height, and 10% to 30% on wiping.""",35000000.0,from RT-1,,,RT-1,"fine-tuned on a mix of original RT-1 dataset, and dataset collected by AutoRT:

""A pretrained RT-1 model is co-fine-tuned on a 50-50 mixture of the pretraining dataset described in Brohan et al. (2022) and AutoRTâ€™s dataset. RT-1 is used instead of RT-2 due to training more quickly and cheaply.""",,"""Data statistics: In total, 53 robots were used to collect 77,000 new episodes over the course of 7 months, with a peak load of over 20 simultaneous robots. Over 6,650 unique instructions appear
in the dataset""",,,,,Likely,"Foundation models that incorporate language, vision, and more recently actions have revolutionized the ability to harness internet scale data to reason about useful tasks. However, one of the key challenges of training embodied foundation models is the lack of data grounded in the physical world. In this paper, we propose AutoRT, a system that leverages existing foundation models to scale up the deployment of operational robots in completely unseen scenarios with minimal human supervision. AutoRT leverages vision-language models (VLMs) for scene understanding and grounding, and further uses large language models (LLMs) for proposing diverse and novel instructions to be performed by a fleet of robots. Guiding data collection by tapping into the knowledge of foundation models enables AutoRT to effectively reason about autonomy tradeoffs and safety while significantly scaling up data collection for robot learning. We demonstrate AutoRT
proposing instructions to over 20 robots across multiple buildings and collecting 77k real robot episodes via both teleoperation and autonomous robot policies. We experimentally show that such â€œin-the-wildâ€ data collected by AutoRT is significantly more diverse, and that AutoRTâ€™s use of LLMs allows for instruction following data collection robots that can align to human preferences.",,,Unreleased,"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France",RT-1,,"fine-tuned on a mix of AutoRT data, which is 77k episodes (unclear how many network passes per episode) and original RT-1 data.

""A pretrained RT-1 model is co-fine-tuned on a 50-50 mixture of the pretraining dataset described in Brohan et al. (2022) and AutoRTâ€™s dataset. RT-1 is used instead of RT-2 due to training more quickly and cheaply.""",,,2024-09-24 11:49,,,,,,Industry,,,,,,,Industry,,,,,,,,,
babbage-002,Language,Language modeling,OpenAI,,2024-01-04,,,,,,,,,,,"According to https://techcommunity.microsoft.com/blog/azure-ai-services-blog/fine-tuning-now-available-with-azure-openai-service/3954693, â€œBabbage-002 and Davinci-002 are GPT3 base models, and the GPT-3 family has been assumed to have a knowledge cutoff date of December 31, 2019. Found by googling â€œbabbage-002 modelâ€. ",,,,,,,Unknown,,,,,United States of America,,,,,,2025-05-12 19:02,,,,,,Industry,,,,,,,Industry,,,,,,,,,
PLLaMa,Biology,Language modeling/generation,"University of California Santa Barbara (UCSB),University of Lincoln,Chinese Academy of Agricultural Sciences,Swedish University of Agricultural Sciences","Xianjun Yang, Junfeng Gao, Wenxin Xue, Erik Alexandersson",2024-01-03,PLLaMa: An Open-source Large Language Model for Plant Science,https://arxiv.org/abs/2401.01600,20.0,,,13000000000.0,,1.60209723904e+23,"57 hours pretraining on 8 A100, 2.7 hours finteuning on 4 A100
57 hours * 3.12E14 * 0.4 * 8 + 2.7 hours * 3.12E14 * 0.4 * 4 = 209723904000000000000

Base model: 1.6e+23
Total: 160209723904000000000000",,,2333115392.0,"""So the total pretraining text pieces is
2, 278, 433. Notice that the chunk window is larger than our following model training max
length of 1024 tokens""

2278433*1024=2333115392",59.7,,NVIDIA A100,,Confident,"Large Language Models (LLMs) have exhibited remarkable capabilities in
understanding and interacting with natural language across various sectors.
However, their effectiveness is limited in specialized areas requiring high accuracy, such as plant science, due to a lack of specific expertise in these fields.
This paper introduces PLLaMa, an open-source language model that evolved
from LLaMa-2. Itâ€™s enhanced with a comprehensive database, comprising
more than 1.5 million scholarly articles in plant science. This development
significantly enriches PLLaMa with extensive knowledge and proficiency in
plant and agricultural sciences. Our initial tests, involving specific datasets
related to plants and agriculture, show that PLLaMa substantially improves
its understanding of plant science-related topics. Moreover, we have formed
an international panel of professionals, including plant scientists, agricultural
engineers, and plant breeders. This team plays a crucial role in verifying
the accuracy of PLLaMaâ€™s responses to various academic inquiries, ensuring
its effective and reliable application in the field. To support further research
and development, we have made the modelâ€™s checkpoints and source codes
accessible to the scientific community. These resources are available for
download at https://github.com/Xianjun-Yang/PLLaMa.",16.0,,Open weights (unrestricted),"United States of America,United Kingdom of Great Britain and Northern Ireland,China,Sweden",Llama 2-13B,,,8.0,,2025-05-23 12:39,,,,,,"Academia,Academia,Academia",,,,,Unreleased,"Apache 2.0
https://huggingface.co/Xianjun/PLLaMa-13b-base","Academia,Academia,Academia",,,,6340.548796655679,Hardware,Xianjun,,,
Kimi Explorer,Language,Language modeling/generation,Moonshot,,2024-01-01,,https://www.53ai.com/news/LargeLanguageModel/2024101137012.html,,SOTA improvement,"Highest score at SuperCLUE benchmark for ""AISearch"" in term of ""Basic Capabilities https://www.superclueai.com/",,,,,,,,,,,,,Unknown,,,,Hosted access (no API),China,,,,,,2025-06-01 16:31,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
Qarasu-14B,Language,Chat,Lightblue,Peter Devine,2023-12-29,Karasu and Qarasu: The new top Japanese Chatbots,"https://note.com/peter_lightblue/n/ne08a7c8cc47a, https://huggingface.co/lightblue/qarasu-14B-chat-plus-unleashed",,,SOTA in class for Japanese,14000000000.0,14B,,,,"""We trained a Shisa model on roughly 7 billion tokens of mostly unstructured Japanese text. We chose the following data sources to use in our pre-training phase:

Aozora Bunko

Japanese Law Precedent Dataset

Japanese Wikipedia

.lg.jp, .go.jp, .ac.jp domain webscrapes from the Japanese subset of  CulturaX

English Ultrachat200K-gen""",7000000000.0,7B tokens,,,NVIDIA A100 SXM4 40 GB,,Confident,"In this blog post, we introduce Karasu and Qarasu - state of the art open-source Japanese chatbots.

Try chatting to our best model, Qarasu, here ( https://lightblue-qarasu.serveo.net/ )!

These models are able to chat in Japanese to a higher level than all other available LLMs of a similar size, as rated by a popular conversation benchmark. In this article, we describe the design and engineering process behind building them.",,,Open weights (unrestricted),Japan,Qwen-14B,590000000000000000000,"7B finetune tokens:

https://huggingface.co/lightblue/qarasu-14B-chat-plus-unleashed

7B * 14B * 6 = 5.9e20",,,2024-09-05 14:08,,,,,,Industry,,,,,,Apache 2.0,Industry,,,,,,,,,
CoRe,"Mathematics,Language","Quantitative reasoning,Language modeling/generation",Tsinghua University,"Xinyu Zhu, Junjie Wang, Lin Zhang, Yuxiang Zhang, Ruyi Gan, Jiaxing Zhang, Yujiu Yang",2023-12-29,Solving Math Word Problems via Cooperative Reasoning induced Language Models,https://arxiv.org/abs/2210.16257,51.0,SOTA improvement,"We evaluate our CoRe framework on several mathematical reasoning datasets and achieve decent improvement over state-of-the-art methods, up to 9.6% increase over best baselines.",12400000000.0,"""Since the default setting consists of two GPT-J (6B) and a DeBERTa-large (0.4B), we note our backbone as â€œGPT-J 12Bâ€, which implies around 12.4 billion parameters in total. """,,,"GSM8K,ASDiv","We consider several widely-used math word problem datasets: GSM8K (Cobbe et al., 2021), ASDivA (Miao et al., 2020), SingleOp (Roy et al., 2015), SinlgeEq (Koncel-Kedziorski et al., 2015) and MultiArith (Roy and Roth, 2015). (Details in Appendix A). Following the general setting as in (Kojima et al., 2022; Wei et al., 2022c), we employ accuracy as the evaluation metric for all datasets.",,,,,NVIDIA A100 SXM4 40 GB,,Speculative,"Large-scale pre-trained language models (PLMs) bring new opportunities to challenging problems, especially those that need high-level intelligence, such as the math word problem (MWPs). However, directly applying existing PLMs to MWPs can fail as the generation process lacks sufficient supervision and thus lacks fast adaptivity as humans. We notice that human reasoning has a dual reasoning framework that consists of an immediate reaction system (system 1) and a delicate reasoning system (system 2), where the entire reasoning is determined by their interaction. This inspires us to develop a cooperative reasoning-induced PLM for solving MWPs, called Cooperative Reasoning (CoRe), resulting in a human-like reasoning architecture with system 1 as the generator and system 2 as the verifier. In our approach, the generator is responsible for generating reasoning paths, and the verifiers are used to supervise the evaluation in order to obtain reliable feedback for the generator. We evaluate our CoRe framework on several mathematical reasoning datasets and achieve decent improvement over state-of-the-art methods, up to 9.6% increase over best baselines.",,,Unreleased,China,GPT-J-6B,,,,,2025-06-04 17:07,,,,,,Academia,,,,,Open (non-commercial),"no clear license
https://github.com/TianHongZXY/CoRe",Academia,,,,,,,,,
Elyza,Language,"Language modeling/generation,Chat",Elyza,"Akira Sasaki, Masato Hirakawa, Shintaro Horie, Tomoaki Nakamura, Passaglia himself, Daisuke Oba (intern)",2023-12-27,,https://huggingface.co/elyza/ELYZA-japanese-Llama-2-13b,,,,13000000000.0,13B,,,"OSCAR,Wikipedia (ja)","'ELYZA-japanese-Llama-2-13b' is a model with additional pre-training using approximately 18 billion (18B) Japanese text tokens on Meta's 'Llama-2-13b-chat.' The training data includes Japanese text data composed of OSCAR, Wikipedia, etc.",18000000000.0,,,,,,Likely,,,,Open weights (restricted use),Japan,Llama 2-13B,1,6ND = 6*13B*18B=1404*10^18=1.4*10^21,,,2024-12-02 10:24,,,,,,Industry,,,,,,Llama 2 license,Industry,,,,,Operation counting,,,,
Solar-10.7B,Language,Chat,Upstage,"Dahyun Kim, Chanjun Park, Sanghoon Kim, Wonsung Lee, Wonho Song, Yunsu Kim, Hyeonwoo Kim, Yungi Kim, Hyeonju Lee, Jihoo Kim, Changbae Ahn, Seonghoon Yang, Sukyung Lee, Hyunbyung Park, Gyoungjin Gim, Mikyoung Cha, Hwalsuk Lee, Sunghun Kim",2023-12-23,SOLAR 10.7B: Scaling Large Language Models with Simple yet Effective Depth Up-Scaling,https://arxiv.org/abs/2312.15166,89.0,,"best in compute class: 

""We introduce SOLAR-10.7B, an advanced large language model (LLM) with 10.7 billion parameters, demonstrating superior performance in various natural language processing (NLP) tasks. It's compact, yet remarkably powerful, and demonstrates unparalleled state-of-the-art performance in models with parameters under 30B.""

https://huggingface.co/upstage/SOLAR-10.7B-v1.0",10700000000.0,10.7B,,,,"Instruction tuning: Alpaca-GPT4 (52k), OpenOrca (2.91M), and Synth.
Math-Instruct(126k)
Alignment tuning: Orca DPO Pairs (12.9k), Ultrafeedback
Cleaned (60.8k), and Synth. Math-Alignment (126k)

Note however that in actual training, only 100k OpenOrca samples, 52k Synth. Math-Instruct and 20.1k Synth. Math-Alignment examples were used.

Tokens per example unclear so dataset size not entered.",,,,,,,Likely,"We introduce SOLAR 10.7B, a large language model (LLM) with 10.7 billion parameters, demonstrating superior performance in various natural language processing (NLP) tasks. Inspired by recent efforts to efficiently up-scale LLMs, we present a method for scaling LLMs called depth up-scaling (DUS), which encompasses depthwise scaling and continued pretraining. In contrast to other LLM up-scaling methods that use mixture-of-experts, DUS does not require complex changes to train and inference efficiently. We show experimentally that DUS is simple yet effective in scaling up high-performance LLMs from small ones. Building on the DUS model, we additionally present SOLAR 10.7B-Instruct, a variant fine-tuned for instruction-following capabilities, surpassing Mixtral-8x7B-Instruct. SOLAR 10.7B is publicly available under the Apache 2.0 license, promoting broad access and application in the LLM field.",,,Open weights (unrestricted),Korea (Republic of),Mistral 7B,,They integrate Mistral 7B weights into a larger model. They don't state token count for continued pretraining/fine-tuning,,,2024-11-01 10:05,,,,,,Industry,,,,,Unreleased,"Apache 2.0, but instruct version is non-commercial",Industry,,,,,,,,,
YaYi 2.0,Language,Language modeling/generation,Yayi (Wenge),"Yin Luo, Qingchao Kong, Nan Xu, Jia Cao, Bao Hao, Baoyu Qu, Bo Chen, Chao Zhu Chenyang Zhao, Donglei Zhang, Fan Feng, Feifei Zhao, Hailong Sun, Hanxuan Yang, Haojun Pan, Hongyu Liu, Jianbin Guo, Jiangtao Du, Jingyi Wang, Junfeng Li, Lei Sun, Liduo Liu, Lifeng Dong, Lili Liu, Lin Wang, Liwen Zhang, Minzheng Wang, Pin Wang, Ping Yu, Qingxiao Li, Rui Yan, Rui Zou, Ruiqun Li, Taiwen Huang, Xiaodong Wang, Xiaofei Wu, Xin Peng, Xina Zhang, Xing Fang, Xinglin Xiao, Yanni Hao, Yao Dong, Yigang Wang, Ying Liu, Yongyu Jiang, Yungan Wang, Yuqi Wang, Zhangsheng Wang, Zhaoxin Yu, Zhen Luo, Wenji Mao, Lei Wang, Dajun Zeng",2023-12-22,"YAYI 2: Multilingual Open-Source Large Language Models
",https://arxiv.org/abs/2312.14862v1,3.0,,,30000000000.0,,4.77e+23,1000 A800 GPUs,,2650000000000,2650000000000.0,,,6*30000000000*2650000000000=4.77e+23,NVIDIA A800 PCIe 40 GB,,Likely,"In this technical report, we propose YAYI 2, including both base and chat models, with 30 billion parameters. YAYI 2 is pre-trained from scratch on a multilingual corpus which contains 2.65 trillion tokens filtered by our pre-training data processing pipeline.",1.0,,Open weights (restricted use),China,,,,1000.0,,2025-05-23 12:22,,,,,,Industry,,,,,Open source,"unclear license
https://huggingface.co/wenge-research/yayi2-30b

this license file https://github.com/wenge-research/YAYI2/blob/main/COMMERCIAL_LICENSE
prohibits particular usage (i.e. military, direct competitors) 
To use YAYI2 models commercially, you must apply for a commercial license.

apache 2.0 for code",Industry,,,,495487.7674607929,,wenge-research,,,
VideoPoet,"Video,Language,Audio","Video generation,Audio generation,Text-to-video,Image-to-video,Video-to-video","Google Research,Carnegie Mellon University (CMU),Google DeepMind","Dan Kondratyuk, Lijun Yu, Xiuye Gu, JosÃ© Lezama, Jonathan Huang, Grant Schindler, Rachel Hornung, Vighnesh Birodkar, Jimmy Yan, Ming-Chang Chiu, Krishna Somandepalli, Hassan Akbari, Yair Alon, Yong Cheng, Josh Dillon, Agrim Gupta, Meera Hahn, Anja Hauth, David Hendon, Alonso Martinez, David Minnen, Mikhail Sirotenko, Kihyuk Sohn, Xuan Yang, Hartwig Adam, Ming-Hsuan Yang, Irfan Essa, Huisheng Wang, David A. Ross, Bryan Seybold, Lu Jiang",2023-12-21,VideoPoet: A Large Language Model for Zero-Shot Video Generation,"https://arxiv.org/abs/2312.14125
https://sites.research.google/videopoet/",112.0,,"Beats out all competing video generation systems in subjective evaluations across several categories. 

Notably, Lumiere wins for ""Video quality"" (presumably the main metric), however this comparison is only made in revised versions of the paper â€“ VideoPoet was released a couple of months before Lumiere.",8000000000.0,Biggest model has 8B parameters; there are also experiments with 300M and 1B models.,,"Not directly calculable, since we don't know how many epochs the main model was trained for. 

From section 5.2: ""We investigate the learning capabilities of different combinations of pretraining tasks using a model with 300 million parameters. All task combinations are trained using a learning rate of 10^âˆ’3 for the same number of steps (300k) with a batch size of 1024... The last row, â€œALL (8B)â€, is the model with 8 billion parameters, trained on the pretraining tasks as discussed in Section 3 and utilized significantly more compute.""

Compute per epoch (2T tokens) can be estimated as 9.6e22:
2 * 8B connections * 3 * 2T tokens = 9.6e22",,"Section 5.1: ""We train on a total of 1B image-text pairs and âˆ¼270M videos (âˆ¼100M with paired text, of which âˆ¼50M are used for high-quality finetuning, and âˆ¼170M with paired audio) from the public internet and other sources, i.e. around 2 trillion tokens across all modalities.""",2000000000000.0,"Section 5.1: ""We train on a total of 1B image-text pairs and âˆ¼270M videos (âˆ¼100M with paired text, of which âˆ¼50M are used for high-quality finetuning, and âˆ¼170M with paired audio) from the public internet and other sources, i.e. around 2 trillion tokens across all modalities.""",,,,Self-supervised learning,Confident,"We present VideoPoet, a language model capable of synthesizing high-quality video, with matching audio, from a large variety of conditioning signals. VideoPoet employs a decoder-only transformer architecture that processes multimodal inputs -- including images, videos, text, and audio. The training protocol follows that of Large Language Models (LLMs), consisting of two stages: pretraining and task-specific adaptation. During pretraining, VideoPoet incorporates a mixture of multimodal generative objectives within an autoregressive Transformer framework. The pretrained LLM serves as a foundation that can be adapted for a range of video generation tasks. We present empirical results demonstrating the model's state-of-the-art capabilities in zero-shot video generation, specifically highlighting VideoPoet's ability to generate high-fidelity motions.",,,Unreleased,"Multinational,United States of America,Canada,Switzerland,United States of America,United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France",,,,,,2025-05-19 12:51,,,,,,"Industry,Academia,Industry",,,,,,,"Industry,Academia,Industry",,,,,,,,,
nekomata-14b,Language,Language generation,rinna,"Tianyu Zhao, Akio Kaga, Kei Sawada",2023-12-21,rinna/nekomata-14b,https://arxiv.org/abs/2404.01657,17.0,SOTA improvement,"Nekomata-14b was released on December 21, 2023. According to rinna, on that date, the model was the best-performing one on the JSQuAD (2-shot) dataset, with an F1 score of 94.21, when evaluated using template v0.2 of Stability-AI/lm-evaluation-harness. When evaluated using template v0.3 on that same date, nekomata-14b was the best-performing model on the JCommonsenseQA (3-shot) dataset (with an accuracy of 92.23%), the MARC-ja (0-shot) dataset (with a balanced accuracy of 92.31%), and the jaqket-2 (1-shot) dataset (with an F1 score of 89.34). 

While the JSQuAD dataset alone is not a benchmark, it is part of the JGLUE benchmark. This benchmark should be recognized since it was introduced in a paper published by the LREC, which has 105 citations as of April 9, 2025 (source: https://aclanthology.org/2022.lrec-1.317/). I think nekomata-14bâ€™s performance on the JSQuAD dataset suffices to show that it is state of the art. I decided to use the evaluation with template v0.2 to determine notability because nekomata-14b was compared against dozens of other models here, unlike in the evaluation with template v0.3, where it was compared to only several models by Qwen and rinna themselves.",14200000000.0,Source: https://huggingface.co/rinna/nekomata-14b,2.5562e+23,"Begins from Qwen-14B (2.5e23 FLOP). They continue pretraining on a mix of Japanese and English text for 66B tokens.
(assuming 1 epoch, and using the C=6ND approximation)
= # of active parameters / forward pass * # of tokens * 6 FLOPs / token
~= 14.2e9 active parameters * 66e9 tokens * 6 FLOPs / token
~= 5623.2e18 FLOPs
~= 5.62e21 FLOPs

In total, 2.5562e23","Japanese CC-100,Japanese C4,The Pile,Wikipedia",The training dataset comprised Japanese OSCAR and a rinna-curated Japanese dataset in addition to the datasets above (source: https://huggingface.co/rinna/nekomata-14b).,66000000000.0,"The 66B tokens on which the model was pre-trained came from a mixture of Japanese and English datasets. The authors didnâ€™t state the Japanese-to-English ratio of these tokens, so I decided to estimate the maximum size of the dataset. Since the model is a predictive language model, the dataset size is measured in number of words (https://docs.google.com/document/d/1XWLyMzcVfDv4eFQX3yPgM8MZ3_Q1phtIFz9GKv4_KaM/edit?tab=t.0#heading=h.or67a8q9faep).

According to the source cited above, for text generation models, English has 0.75 words/token and Japanese has 1 word/token. Thus, the dataset attains maximum size when it contains only Japanese text. I donâ€™t think this assumption is unreasonable because the model is a Japanese language model. To that end,
Maximum dataset size
~= 66e9 tokens * 1 Japanese word / token
= 6.6e10 words",168.0,The pre-training job was completed within a timeframe of approximately 7 days (source: https://huggingface.co/rinna/nekomata-14b).,Amazon Trainium1,,Confident,"We conduct continual pre-training of qwen-14b on 66B tokens from a mixture of Japanese and English datasets. The continual pre-training significantly improves the model's performance on Japanese tasks. It also enjoys the following great features provided by the original Qwen model.

* The inclusive Qwen vocabulary (vocab size > 150k) enables the model to processs Japanese texts much more efficiently than the previously released youri series.
* The model supports a maximum sequence length of 8192.

The name nekomata comes from the Japanese word çŒ«åˆ/ã­ã“ã¾ãŸ/Nekomata, which is a kind of Japanese mythical creature (å¦–æ€ª/ã‚ˆã†ã‹ã„/Youkai).",,,Open weights (restricted use),Japan,Qwen-14B,,,256.0,0.171,2025-06-04 15:55,,,,,,Industry,,,,,Unreleased,"Tongyi Qianwen license: requires separate license if >100M MAUs
https://huggingface.co/rinna/nekomata-14b",Industry,,"According to the developer, the model â€œwas trained on 16 nodes of Amazon EC2 trn1.32xlarge instanceâ€ (https://huggingface.co/rinna/nekomata-14b). An Amazon EC2 trn1.32xlarge instance can provide up to 3.4 petaflops of FP16/BF16 compute power and is powered by up to 16 AWS Trainium chips (https://aws.amazon.com/blogs/aws/amazon-ec2-trn1-instances-for-high-performance-model-training-are-now-available/). Therefore,

Training compute
= utilization rate * peak FLOPS / hardware * # of hardware * training time
~= utilization rate * 3.4e15 FLOPS / Amazon EC2 trn1.32xlarge instance * 16 Amazon EC2 trn1.32xlarge instances * 7 days * 86,400 s / day
~= 5.6232e21 FLOPs

Utilization rate
~= 5.6232e21 FLOPs / ( 3.4e15 FLOPS * 16 * 7 days * 86,400 s / day )
~= 0.171",BF16,,Operation counting,rinna,,,
Suno Music Generation,Audio,Audio generation,Suno,,2023-12-20,,https://suno.com/about,,,,,,,,,,,,,,,,Unknown,,,,Hosted access (no API),United States of America,,,,,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
Gemini Nano-2,"Multimodal,Language,Vision,Audio","Chat,Image captioning,Speech recognition",Google DeepMind,Gemini Team,2023-12-19,Gemini: A Family of Highly Capable Multimodal Models,https://arxiv.org/abs/2312.11805,633.0,Significant use,"Significant use; deployed on Android phones such as the Pixel: https://store.google.com/intl/en/ideas/articles/pixel-feature-drop-december-2023/

""Despite their size, they show exceptionally strong performance on factuality,
i.e. retrieval-related tasks, and significant performance on reasoning, STEM, coding, multimodal and multilingual tasks""",3250000000.0,3.25B,,"More tokens than Chinchilla-optimal:

""The number of tokens used to train the largest models were determined following the approach in Hoffmann et al. (2022). The smaller models are trained for significantly more tokens to improve performance for a given inference budget, similar to the approach advocated in Touvron et al. (2023a)""

Chinchilla was 1.4T tokens for 70B params, so Chinchilla-optimal for 3.25B params would be ~1.4T/20 = 70B tokens.

So compute was significantly greater than 3.25B * 70B * 6, which is 1.4e21. 

Touvron et al. is the Llama 1 paper, in which a 6.7B model is trained for 1T tokens. Using the same ratio, a 3.25B model would be trained on ~500B tokens. 3.25 * 500B * 6 = 9.75e21. No guarantee that the exact ratio for Nano is close to Llama's, of course.",Unspecified unreleased,"""Gemini models are trained on a dataset that is both multimodal and multilingual. Our pre-training dataset uses data from web documents, books, and code, and includes image, audio, and video data.""",,,,,Google TPU v5e,,Confident,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.",,,Unreleased,"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France",,,,,,2025-06-04 17:29,,,,,,Industry,,,,,Unreleased,"May be API access in the future. There is an Android API but it ""is under a closed early access preview program at this time"": https://ai.google.dev/gemini-api/docs/get-started/android_aicore",Industry,,,,,,,,,
Gemini Nano-1,"Multimodal,Language,Vision,Audio","Chat,Image captioning,Speech recognition",Google DeepMind,Gemini Team,2023-12-19,Gemini: A Family of Highly Capable Multimodal Models,https://arxiv.org/abs/2312.11805,633.0,Significant use,"Significant use; deployed on Android phones such as the Pixel: https://store.google.com/intl/en/ideas/articles/pixel-feature-drop-december-2023/

""Despite their size, they show exceptionally strong performance on factuality,
i.e. retrieval-related tasks, and significant performance on reasoning, STEM, coding, multimodal and multilingual tasks""",1800000000.0,1.8B,,"More tokens than Chinchilla-optimal:

""The number of tokens used to train the largest models were determined following the approach in Hoffmann et al. (2022). The smaller models are trained for significantly more tokens to improve performance for a given inference budget, similar to the approach advocated in Touvron et al. (2023a)""",Unspecified unreleased,"""Gemini models are trained on a dataset that is both multimodal and multilingual. Our pre-training
dataset uses data from web documents, books, and code, and includes image, audio, and video data.""",,,,,Google TPU v5e,,Confident,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.",,,API access,"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France",,,,,,2025-06-04 17:30,,,,,,Industry,,,,,Unreleased,https://developer.android.com/ai/gemini-nano,Industry,,,,,,,,,
HiFi - NN,Biology,Enzyme function prediction,"Basecamp Research,Technical University of Munich,Molecular Institute of Biology,Microsoft Research",Gavin Ayres,2023-12-19,Breakthrough in Functional Annotation with HiFi-NN,https://developer.nvidia.com/blog/breakthrough-in-functional-annotation-with-hifi-nn/,,,,3000000.0,"""The model boasts over 3M parameters.""",,,,"""Small representativity, covering only 0.001% of life on earth
No consistent metadata
Lack of stakeholder consent and engagement before data collection
Basecamp opted to develop its proprietary biological data resource through biodiversity partnerships with nature parks across five continents and 23 countries. They sent their scientists on worldwide expeditions to discover new genomes, enzymes, and biological relationships from the most extreme and extraordinary biomes. 

In under two years, they created BaseGraph, the largest knowledge graph of natural biodiversity, containing over 5.5B relationships with a genomic context exceeding 70 kilobases per protein. Their extensive long-read sequencing is complemented by comprehensive metadata collection, enabling them to link proteins of interest to specific reactions and desired process conditions. """,,"""the model retrained with 3M selected, environmentally diverse sequences from Basecamp Researchâ€™s BaseGraph.""",,,NVIDIA A100,,Confident,"The accurate computational annotation of protein sequences with enzymatic function, especially those that are part of the functional and taxonomic dark matter, remains a fundamental challenge in bioinformatics. Here, we present HiFi-NN, (Hierarchically-Finetuned Nearest Neighbor search) which annotates protein sequences to the 4th level of EC (enzyme commission) number with greater precision and recall than all existing deep learning methods. HiFi-NN is a hierarchically-finetuned deep learning method based on a combination of semi-supervised representation learning and a nearest neighbours classifier. Furthermore, we show that this method can correctly identify the EC number of a given sequence to identities below 40%, where the current state of the art annotation tool, BLASTp, cannot. We proceed to improve the representations learned by increasing the diversity of the training set, not just in sequence space but also in terms of the environment the sequences have been sampled from. Finally, we use HiFi-NN to annotate a portion of microbial dark matter sequences in the MGnify database.",,,Open weights (unrestricted),"United Kingdom of Great Britain and Northern Ireland,Germany,Spain,United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational",,,,8.0,,2025-06-11 18:27,,,,,,"Industry,Academia,Academia,Industry",,,,,Open source,"MIT license
https://github.com/Basecamp-Research/HiFi-NN?tab=readme-ov-file","Industry,Academia,Academia,Industry",,,,6342.667150972759,,,,,
Bird Vocalization Classifier (Perch),Audio,Audio classification,"Google Research,Cornell University,Naturalis Biodiversity Center,Chemnitz University of Technology","Burooj Ghani, Tom Denton, Stefan Kahl, Holger Klinck",2023-12-18,Global birdsong embeddings enable superior transfer learning for bioacoustic classification,https://www.nature.com/articles/s41598-023-49989-z,,,,7800000.0,"EfficientNet-B1 architecture (7.8M parameters) - I assume this model has the same amount
embedding size: 1280",,,XenoCanto,"Datasets
Caples is an unreleased dataset collected by the California Academy of Science at the Caples Creek area in the central Californian Sierra Nevadas. Work is underway to open-source this dataset.

CoffeeFarms is a collection of annotated soundscapes from neotropical coffee farms in Colombia and Costa Rica. Part of the data was previously used in the test set for the BirdCLEF 2019 competition.

Hawaiâ€™i (Navine et al., 2022) contains soundscape recordings from Hawaiâ€™i, USA. Many species, particularly endangered honeycreepers, are endemic to Hawaiâ€™i and many are under-represented in the Xeno-Canto training set.

High Sierras is a soundscape dataset of birds from high altitudes in the Sierra Nevadas in California. Previously used as part of the test set for the Kaggle Cornell Birdcall Identification challenge. Recordings are typically sparse, but with very low SNR due to wind noise. Work is underway to open-source this dataset.

Peru is a dataset of recordings from the Amazon rainforest.

Powdermill (Chronister et al, 2021) contains high-activity dawn chorus recordings captured over four days in Pennsylvania, USA.

Sapsucker Woods (SSW) contains soundscape recordings from the Sapsucker Woods bird sanctuary in Ithaca, NY, USA.

Sierra Nevada (Kahl et al., 2022b) contains soundscape recordings from the Sierra Nevadas in California, USA.",,470 hours of audio (sum of all dataset lengths from here https://www.kaggle.com/models/google/bird-vocalization-classifier),,,,,Likely,"Automated bioacoustic analysis aids understanding and protection of both marine and terrestrial animals and their habitats across extensive spatiotemporal scales, and typically involves analyzing vast collections of acoustic data. With the advent of deep learning models, classification of important signals from these datasets has markedly improved. These models power critical data analyses for research and decision-making in biodiversity monitoring, animal behaviour studies, and natural resource management. However, deep learning models are often data-hungry and require a significant amount of labeled training data to perform well. While sufficient training data is available for certain taxonomic groups (e.g., common bird species), many classes (such as rare and endangered species, many non-bird taxa, and call-type) lack enough data to train a robust model from scratch. This study investigates the utility of feature embeddings extracted from audio classification models to identify bioacoustic classes other than the ones these models were originally trained on. We evaluate models on diverse datasets, including different bird calls and dialect types, bat calls, marine mammals calls, and amphibians calls. The embeddings extracted from the models trained on bird vocalization data consistently allowed higher quality classification than the embeddings trained on general audio datasets. The results of this study indicate that high-quality feature embeddings from large-scale acoustic bird classifiers can be harnessed for few-shot transfer learning, enabling the learning of new classes from a limited quantity of training data. Our findings reveal the potential for efficient analyses of novel bioacoustic tasks, even in scenarios where available training data is limited to a few samples.",,,Open weights (unrestricted),"Multinational,United States of America,Canada,Switzerland,United States of America,Germany,Germany",EfficientNet-B1,,,,,2025-05-01 10:42,,,,,,"Industry,Academia,Government,Academia",,,,,Open source,"Apache 2 https://www.kaggle.com/models/google/bird-vocalization-classifier

https://github.com/google-research/perch","Industry,Academia,Government,Academia",,,,,,,,,
Lyra-Fr 10B,Language,"Language modeling/generation,Semantic search,Text classification,Question answering,Text summarization,Sentiment classification",LightOn,,2023-12-15,LightOn Lyra-fr model is now available on Amazon SageMaker,https://aws.amazon.com/fr/blogs/machine-learning/lighton-lyra-fr-model-is-now-available-on-amazon-sagemaker/,,,,10000000000.0,10B,,,,"""Lyra-fr was trained on a large corpus of French curated data""",,,,,,,Likely,"We are thrilled to announce the availability of the LightOn Lyra-fr foundation model for customers using Amazon SageMaker. LightOn is a leader in building foundation models specializing in European languages. Lyra-fr is a state-of-the-art French language model that can be used to build conversational AI, copywriting tools, text classifiers, semantic search, and more. You can easily try out this model and use it with Amazon SageMaker JumpStart. JumpStart is the machine learning (ML) hub of SageMaker that provides access to foundation models in addition to built-in algorithms and end-to-end solution templates to help you quickly get started with ML.",,,API access,France,,,,,,2024-09-16 13:39,,,,,,Industry,,,,,,,Industry,,,,,,,,,
Konan LLM 41B,"Language,Vision",Language modeling/generation,Konan Technology,"Yang Seung-hyun, Wiretin, Changmin, Kim Jong-tae",2023-12-15,Konan LLM: A Korean Large Language Model,"https://en.konantech.com/en/llm/konanllm
https://techfinch.kr/ai/konan-technology-unveils-konan-llm--its-own-ai-language-model
https://www.dbpia.co.kr/journal/articleDetail?nodeId=NODE11610127 ",,,,41000000000.0,,1.722e+23,=41000000000 parameters * 700000000000 tokens [see dataset size notes]  * 6 FLOP / token / parameter =1.722 Ã— 10^23 FLOP,Unspecified unreleased,,700000000000.0,"https://www.konantech.com/pr/press?number=2628&pn=1&stype2=&sfi=subj&sword=

Since 2007, via the real-time AI analysis service pulseK, over 20.5 billion pieces of data have been independently secured.
Among them, only 2 billion high-quality, large-scale data pieces have been used for training.",,,,,Likely,"Konan LLM is a Large Language Model developed in-house by Konan Technology. Konan Technology optimized for super-large AI training, it leverages high-quality, large-scale data and over 20 years of expertise in natural language processing.
Konan LLM supports all corporate documentation and creative tasks,
leading the way in workplace innovation.",,,Hosted access (no API),Korea (Republic of),,,,,,2025-05-30 14:30,,,,,,Industry,,,,,Unreleased,,Industry,,,,,Operation counting,,,,
Poro 34B,Language,"Code generation,Language modeling/generation","High-Performance Language Technologies (HPLT),University of Turku","Risto Luukkonen, Jonathan Burdge, Elaine Zosa, Aarne Talman, Ville Komulainen, VÃ¤inÃ¶ HatanpÃ¤Ã¤, Peter Sarlin, Sampo Pyysalo",2023-12-14,Poro 34B and the Blessing of Multilinguality,https://arxiv.org/abs/2404.01856,,,,34200000000.0,https://huggingface.co/LumiOpen/Poro-34B,2.052e+23,"6ND = 6*1T*34.2B= 2.04e+23

""This allowed total training cycle throughput of 49618 TFLOPs and 174378 tokens/second.""

the training took around 18 months (https://hplt-project.org/deliverables)
49618*18*30*24*3600*10^12=2.3149774e+24

","mC4,SlimPajama,StarCoder,Dolma","https://huggingface.co/LumiOpen/Poro-34B

""The Finnish dataset is a combination of many Finnish resources:

    Finnish Internet Parsebank
    mC4 multilingual colossal, cleaned Common Crawl
    Common Crawl Finnish
    Finnish Wikipedia
    LÃ¶nnrot Projekti LÃ¶nnrot
    Suomi24 The Suomi 24 Corpus 2001-2020
    Reddit r/Suomi submissions and comments
    STT Finnish News Agency Archive 1992-2018
    Yle Finnish News Archive 2011-2018
    Yle Finnish News Archive 2019-2020
    Yle News Archive Easy-to-read Finnish 2011-2018
    Yle News Archive Easy-to-read Finnish 2019-2020""
""

""Poro is a 34B parameter decoder-only transformer pretrained on Finnish, English and code. It is being trained on 1 trillion tokens (700 billion as of this release). Poro is a fully open source model and is made available under the Apache 2.0 License.""",1000000000000.0,"1T tokens, assuming 0.75 word per token
""Poro is a 34B parameter decoder-only transformer pretrained on Finnish, English and code. It is being trained on 1 trillion tokens. Poro is a fully open source model and is made available under the Apache 2.0 License.""",,,AMD Radeon Instinct MI250X,,Confident,"The pretraining of state-of-the-art large language models now requires trillions of words of text, which is orders of magnitude more than available for the vast majority of languages. While including text in more than one language is an obvious way to acquire more pretraining data, multilinguality is often seen as a curse, and most model training efforts continue to focus near-exclusively on individual large languages. We believe that multilinguality can be a blessing and that it should be possible to substantially improve over the capabilities of monolingual models for small languages through multilingual training. In this study, we introduce Poro 34B, a 34 billion parameter model trained for 1 trillion tokens of Finnish, English, and programming languages, and demonstrate that a multilingual training approach can produce a model that not only substantially advances over the capabilities of existing models for Finnish, but also excels in translation and is competitive in its class in generating English and programming languages. We release the model parameters, scripts, and data under open licenses at this https URL.",,,Open weights (unrestricted),"Multinational,Finland",,,,512.0,,2025-05-26 19:48,,,,,,"Research collective,Academia",,,,,Open source,"Apache 2.0
https://huggingface.co/LumiOpen/Poro-34B

https://github.com/TurkuNLP/Megatron-DeepSpeed","Research collective,Academia",,,,507469.8741072884,Operation counting,LumiOpen,,,
CogAgent,"Vision,Language","Instruction interpretation,Visual question answering","Tsinghua University,Zhipu AI","Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxuan Zhang, Juanzi Li, Bin Xu, Yuxiao Dong, Ming Ding, Jie Tang",2023-12-14,CogAgent: A Visual Language Model for GUI Agents,https://arxiv.org/abs/2312.08914,150.0,SOTA improvement,See Table 1,18000000000.0,,6.707e+22,"States 12.6 TFLOP per 1120x1120 image forward pass. Trained 60k steps with 4608 batch size, and then 10k with 1024 batch size.
12.6 TFLOP * (60000*4608 + 10000*1024) = 3.76e21

Uses pretrained CogVLM as base (6.331e22 FLOP), along with EVA2-CLIP-L. EVA2-CLIP-L's FLOPs are potentially estimable, but based on details about EVA2-CLIP-g/14 (a larger model), they likely contribute negligibly to CogAgent.

Sum: 6.707e22","COYO-700M,LAION-2B,Common Crawl,Unspecified unreleased","From section 2.3, pretraining data includes:
- Synthetic renderings with text from language pre-training dataset (80M image-text pairs)
- OCR from natural images from COYO [6] and LAION-2B (18M image-text pairs)
- Academic documents (9M image-text pairs)
- Visual grounding dataset with bounding boxes (see CogVLM) (40M image-text pairs)
- Common Crawl Screenshot 400K (400k images with  140M QA pairs)",287000000.0,,,,,Supervised,Likely,"People are spending an enormous amount of time on digital devices through graphical user interfaces (GUIs), e.g., computer or smartphone screens. Large language models (LLMs) such as ChatGPT can assist people in tasks like writing emails, but struggle to understand and interact with GUIs, thus limiting their potential to increase automation levels. In this paper, we introduce CogAgent, an 18-billion-parameter visual language model (VLM) specializing in GUI understanding and navigation. By utilizing both low-resolution and high-resolution image encoders, CogAgent supports input at a resolution of 1120*1120, enabling it to recognize tiny page elements and text. As a generalist visual language model, CogAgent achieves the state of the art on five text-rich and four general VQA benchmarks, including VQAv2, OK-VQA, Text-VQA, ST-VQA, ChartQA, infoVQA, DocVQA, MM-Vet, and POPE. CogAgent, using only screenshots as input, outperforms LLM-based methods that consume extracted HTML text on both PC and Android GUI navigation tasks -- Mind2Web and AITW, advancing the state of the art. The model and codes are available at this https URL .",,,Open weights (restricted use),"China,China",CogVLM-17B,,,,,2025-05-28 16:03,,,,,,"Academia,Industry",,,,,Open source,"Code is Apache License 2.0; model is under a more restrictive custom licence which still allows commercial usage but which limits uses undermining Chinese national security and unity.

finetune code (this model is a finetune): https://github.com/THUDM/CogVLM/blob/main/finetune_demo/finetune_cogagent_demo.py ","Academia,Industry",,,BF16,,Operation counting,,,,
FunSearch,"Language,Search",Code generation,Google DeepMind,"Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Matej Balog, M. Pawan Kumar, Emilien Dupont, Francisco J. R. Ruiz, Jordan S. Ellenberg, Pengming Wang, Omar Fawzi, Pushmeet Kohli, Alhussein Fawzi ",2023-12-14,Mathematical discoveries from program search with large language models,"https://www.nature.com/articles/s41586-023-06924-6
https://deepmind.google/discover/blog/funsearch-making-new-discoveries-in-mathematical-sciences-using-large-language-models/",170.0,"SOTA improvement,Historical significance",Improved SOTA for the cap set problem. Can plausibly claim the first instance of a LLM system making a genuine and novel scientific contribution.,15000000000.0,"From the section called ""Pretrained LLM"": ""We use Codey, an LLM built on top of the PaLM2 model family... Because FunSearch relies on sampling from an LLM extensively, an important performance-defining tradeoff is between the quality of the samples and the inference speed of the LLM. In practice, we have chosen to work with a fast-inference model (rather than slower-inference, higher-quality)""

Unclear which PaLM2 model was used (of Gecko, Otter, Bison, and Unicorn); above quote indicates it was perhaps Otter or Bison, but not Unicorn. Exact parameter counts are not publicly disclosed for any of these models. In comparisons where FunSearch uses StarCoder-15B, Codey is an improvement but not obviously of an entirely different model class.

I report the 15B parameters from StarCoder-15B, used as an open-source comparison",3.87e+23,"Appendix A.5: ""Finding the full-sized symmetric admissible set I(15, 10) required the generation and analysis of approximately two million programs... To reproduce admissible set experiments done above (generating 2 million samples) one would have to use 15 instances of StarCoder-15B running on A100 40 GB GPU each and 5 CPU servers (each running 32 evaluators in parallel) for two days. We estimate that when running on Google Cloud, the price of an experiment is around $800 â€“ $1400, and the energy usage around 250 â€“ 500 kWh; i.e., 0.5% of the energy used for training StarCoder""

15 GPUs * 7.80E+13 FLOP/GPU-sec * 2 days * 24 hours/day * 3600 sec/hour = 2.02e20 FLOP for the GPU servers

We should also add the compute used to train the PaLM2 variant used as the base LLM. Since we don't have any details about this model, I use the compute from StarCoder-15B (used as the open source comparison point): 3.87e+23 FLOP

Unclear how to evaluate the compute from the CPU servers implementing the evolutionary algorithm, but this is very likely dwarfed by the pre-training compute for the LLM.",,"""The experiments carried out in this paper do not require any data corpus other than the publicly available OR-Library bin packing benchmarks""",0.0,"""The experiments carried out in this paper do not require any data corpus other than the publicly available OR-Library bin packing benchmarks""",48.0,"Appendix A.5: ""To reproduce admissible set experiments done above (generating 2 million samples) one would have to use 15 instances of StarCoder-15B running on A100 40 GB GPU each and 5 CPU servers (each running 32 evaluators in parallel) for two days""",,,Speculative,"Large language models (LLMs) have demonstrated tremendous capabilities in solving complex tasks, from quantitative reasoning to understanding natural language. However, LLMs sometimes suffer from confabulations (or hallucinations), which can result in them making plausible but incorrect statements1,2. This hinders the use of current large models in scientific discovery. Here we introduce FunSearch (short for searching in the function space), an evolutionary procedure based on pairing a pretrained LLM with a systematic evaluator. We demonstrate the effectiveness of this approach to surpass the best-known results in important problems, pushing the boundary of existing LLM-based approaches3. Applying FunSearch to a central problem in extremal combinatoricsâ€”the cap set problemâ€”we discover new constructions of large cap sets going beyond the best-known ones, both in finite dimensional and asymptotic cases. This shows that it is possible to make discoveries for established open problems using LLMs. We showcase the generality of FunSearch by applying it to an algorithmic problem, online bin packing, finding new heuristics that improve on widely used baselines. In contrast to most computer search approaches, FunSearch searches for programs that describe how to solve a problem, rather than what the solution is. Beyond being an effective and scalable strategy, discovered programs tend to be more interpretable than raw solutions, enabling feedback loops between domain experts and FunSearch, and the deployment of such programs in real-world applications.",,,Open weights (unrestricted),"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France",PaLM 2,0,No finetuning,,,2025-02-14 15:50,,,,,,Industry,,,9.899999999999999e+24,,Unreleased,"Code to run FunSearch with an LLM of your choice is open source under Apache 2.0 (software) and CC-BY (all else). However, the actual LLM used in the main experiments is unknown and may or may not be one of the Codey models available via API access.

(in other words code is available for the search tool but not for the model): https://github.com/google-deepmind/funsearch ",Industry,,,,,Hardware,,,,
Imagen 2,Image generation,"Text-to-image,Image generation,Video generation",Google DeepMind,"AÃ¤ron van den Oord, Ali Razavi, Benigno Uria, Ã‡aÄŸlar ÃœnlÃ¼, Charlie Nash, Chris Wolff, Conor Durkan, David Ding, Dawid GÃ³rny, Evgeny Gladchenko, Felix Riedel, Hang Qi, Jacob Kelly, Jakob Bauer, Jeff Donahue, Junlin Zhang, Mateusz Malinowski, MikoÅ‚aj BiÅ„kowski, Pauline Luc, Robert Riachi, Robin Strudel, Sander Dieleman, Tobenna Peter Igwe, Yaroslav Ganin, Zach Eaton-Rosen",2023-12-13,Imagen 2,https://deepmind.google/technologies/imagen-2/,,,Does not appear to meet any notability criteria; could plausibly reach significant usage given integrations into other Google products.,,,,,,,,,,,,,Unknown,"Imagen 2 is our most advanced text-to-image diffusion technology, delivering high-quality, photorealistic outputs that are closely aligned and consistent with the userâ€™s prompt. It can generate more lifelike images by using the natural distribution of its training data, instead of adopting a pre-programmed style.",,,API access,"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France",,,,,,2024-09-05 14:08,,,,,,Industry,,,,,Unreleased,Accessible through Google's Vertex AI platform. Some features currently limited to whitelisted testers.,Industry,,,,,,,,,
Phi-2,Language,"Language generation,Code generation",Microsoft,"Marah Abdin, Jyoti Aneja, Sebastien Bubeck, Caio CÃ©sar Teodoro Mendes, Weizhu Chen, Allie Del Giorno, Ronen Eldan, Sivakanth Gopi, Suriya Gunasekar, Mojan Javaheripi, Piero Kauffmann, Yin Tat Lee, Yuanzhi Li, Anh Nguyen, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Michael Santacroce, Harkirat Singh Behl, Adam Taumann Kalai, Xin Wang, Rachel Ward, Philipp Witte, Cyril Zhang, Yi Zhang",2023-12-12,Phi-2: The surprising power of small language models,"https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/
https://huggingface.co/microsoft/phi-2 ",,,"""With only 2.7 billion parameters, Phi-2 surpasses the performance of Mistral and Llama-2 models at 7B and 13B parameters on various aggregated benchmarks. Notably, it achieves better performance compared to 25x larger Llama-2-70B model on muti-step reasoning tasks, i.e., coding and math. Furthermore, Phi-2 matches or outperforms the recently-announced Google Gemini Nano 2, despite being smaller in size.""",2700000000.0,2.7B,2.27e+22,"2.7B params, trained on 1.4T tokens

2.7 billion * 1.4 trillion * 6 = 2.27e22

96*14 A100-days

14 * 96 * 312 trillion * 24 * 3600 * 0.3 = 1.09e22",Phi-2 Dataset,"""Our training data mixture contains synthetic datasets specifically created to teach the model common sense reasoning and general knowledge, including science, daily activities, and theory of mind, among others. We further augment our training corpus with carefully selected web data that is filtered based on educational value and content quality. Secondly, we use innovative techniques to scale up, starting from our 1.3 billion parameter model, Phi-1.5, and embedding its knowledge within the 2.7 billion parameter Phi-2. This scaled knowledge transfer not only accelerates training convergence but shows clear boost in Phi-2 benchmark scores.""",250000000000.0,"""trained on 1.4T tokens from multiple passes on a mixture of Synthetic and Web datasets for NLP and coding""
From Huggingface Hub: ""Dataset size: 250B tokens, combination of NLP synthetic data created by AOAI GPT-3.5 and filtered web data from Falcon RefinedWeb and SlimPajama, which was assessed by AOAI GPT-4.""",336.0,14 days,NVIDIA A100,,Confident,"Over the past few months, our Machine Learning Foundations team at Microsoft Research has released a suite of small language models (SLMs) called â€œPhiâ€ that achieve remarkable performance on a variety of benchmarks. Our first model, the 1.3 billion parameter Phi-1(opens in new tab), achieved state-of-the-art performance on Python coding among existing SLMs (specifically on the HumanEval and MBPP benchmarks). We then extended our focus to common sense reasoning and language understanding and created a new 1.3 billion parameter model named Phi-1.5(opens in new tab), with performance comparable to models 5x larger.

We are now releasing Phi-2(opens in new tab), a 2.7 billion-parameter language model that demonstrates outstanding reasoning and language understanding capabilities, showcasing state-of-the-art performance among base language models with less than 13 billion parameters. On complex benchmarks Phi-2 matches or outperforms models up to 25x larger, thanks to new innovations in model scaling and training data curation.",,,Open weights (unrestricted),"United States of America,Multinational,India,Belgium",,,,,,2025-01-27 17:56,,,,,,Industry,,,,,Unreleased,MIT,Industry,,,,,"Operation counting,Hardware",,,,
Mixtral 8x7B,Language,"Language modeling/generation,Question answering,Quantitative reasoning,Translation",Mistral AI,"Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, LÃ©lio Renard Lavaud, Louis Ternon, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, ThÃ©ophile Gervet, Thibaut Lavril, Thomas Wang, TimothÃ©e Lacroix, William El Sayed.",2023-12-11,Mixtral of experts: A high quality Sparse Mixture-of-Experts.,"https://mistral.ai/news/mixtral-of-experts/, https://arxiv.org/abs/2401.04088",,Significant use,"Frequently downloaded: https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1

Probably the best OS model by a big margin at time of release, e.g. #7 on Chatbot Arena, above Gemini Pro and Claude 2.1: https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard
",46700000000.0,"46.7B *sparse* params. 12.9B params used on average:

""Concretely, Mixtral has 46.7B total parameters but only uses 12.9B parameters per token. It, therefore, processes input and generates output at the same speed and for the same cost as a 12.9B model.""",7.74e+23,"Assuming the model was trained on ~1-10 trillions of tokens (same OOM as the models from the comparison in Figure 1. Llama 2 was trained on 2T tokens) + Mistral Small 3 was trained on 8T of tokens, we can estimate training compute with ""speculative"" confidence:

6 FLOP / token / parameter * 12.9 * 10^9 active parameters * 10*10^12 tokens [speculatively] = 7.74e+23 FLOP",Unspecified unreleased,"""Mixtral is pretrained with multilingual data using a context size of 32k tokens""",,,,,,,Speculative,"Today, the team is proud to release Mixtral 8x7B, a high-quality sparse mixture of experts model (SMoE) with open weights. Licensed under Apache 2.0. Mixtral outperforms Llama 2 70B on most benchmarks with 6x faster inference. It is the strongest open-weight model with a permissive license and the best model overall regarding cost/performance trade-offs. In particular, it matches or outperforms GPT3.5 on most standard benchmarks.",,,Open weights (unrestricted),France,,,,,,2025-05-30 16:23,,,,,,Industry,,,,,Unreleased,Apache 2.0,Industry,,,,,Operation counting,mistralai,,,18
Mistral Medium,Language,Chat,Mistral AI,,2023-12-11,La Plateforme,https://mistral.ai/news/la-plateforme/,,,"One of the strongest models on Chatbot Arena, behind only GPT-4 and Bard/Gemini Pro: https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard.

""Our highest-quality endpoint currently serves a prototype model, that is currently among the top serviced models available based on standard benchmarks""
https://mistral.ai/news/la-plateforme/",,"May be 70B, based on this weird leak episode. 

https://venturebeat.com/ai/mistral-ceo-confirms-leak-of-new-open-source-ai-model-nearing-gpt-4-performance/",,,,,,,,,,,Unknown,"Mistral-medium. Our highest-quality endpoint currently serves a prototype model, that is currently among the top serviced models available based on standard benchmarks. It masters English/French/Italian/German/Spanish and code and obtains a score of 8.6 on MT-Bench. The following table compare the performance of the base models of Mistral-medium, Mistral-small and the endpoint of a competitor.",,,API access,France,,,,,,2025-05-29 00:28,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
ruDalle: Kandinsky 3.1,Image generation,"Text-to-image,Image generation",Sber,,2023-12-11,,https://ai-forever.github.io/Kandinsky-3/K31/,,,,,,,lower bound is taken from Kandinsky 3.0 estimate,"Unspecified unreleased,LAION,COYO-700M",,,,,,NVIDIA A100,,Unknown,"We present Kandinsky 3.1, the follow-up to the Kandinsky 3.0 model, a large-scale text-to-image generation model based on latent diffusion, continuing the series of text-to-image Kandinsky models and reflecting our progress to achieve higher quality and realism of image generation, which we have enhanced and enriched with a variety of useful features and modes to give users more opportunities to fully utilise the power of our new model.",,,Open weights (unrestricted),Russia,,,,,,2025-04-30 11:35,,,,,,"Industry,Government",,2.014908000001e+20,,,Open source,"Apache 2 license

https://github.com/ai-forever/Kandinsky-3","Industry,Government",,,,,,ai-forever,,,
CRYSTALCODER,Language,"Language modeling/generation,Question answering,Code generation","Mohamed bin Zayed University of Artificial Intelligence (MBZUAI),Petuum,University of Southern California,Carnegie Mellon University (CMU),University of Illinois Urbana-Champaign (UIUC),University of California San Diego,LLM360","Zhengzhong Liu, Aurick Qiao, Willie Neiswanger, Hongyi Wang, Bowen Tan, Tianhua Tao, Junbo Li, Yuqi Wang, Suqi Sun, Omkar Pangarkar, Richard Fan, Yi Gu, Victor Miller, Yonghao Zhuang, Guowei He, Haonan Li, Fajri Koto, Liping Tang, Nikhil Ranjan, Zhiqiang Shen, Xuguang Ren, Roberto Iriondo, Cun Mu, Zhiting Hu, Mark Schulze, Preslav Nakov, Tim Baldwin, Eric P. Xing",2023-12-11,LLM360: Towards Fully Transparent Open-Source LLMs,https://arxiv.org/abs/2312.06550,,,,6700000000.0,"6.7B
We used the exact same model architecture as LLaMA 7B",5.55564e+22,6*6700000000.00*1382000000000 = 5.55564e+22,"StarCoder,SlimPajama","our pretraining data is a
mixture of RefinedWeb, StarCoder, and RedPajama-v1",1382000000000.0, 1382B tokens in total,,,,,Confident,"The recent surge in open-source Large Language Models (LLMs), such as LLaMA, Falcon, and Mistral, provides diverse options for AI practitioners and researchers. However, most LLMs have only released partial artifacts, such as the final model weights or inference code, and technical reports increasingly limit their scope to high-level design choices and surface statistics. These choices hinder progress in the field by degrading transparency into the training of LLMs and forcing teams to rediscover many details in the training process. We present LLM360, an initiative to fully open-source LLMs, which advocates for all training code and data, model checkpoints, and intermediate results to be made available to the community. The goal of LLM360 is to support open and collaborative AI research by making the end-to-end LLM training process transparent and reproducible by everyone. As a first step of LLM360, we release two 7B parameter LLMs pre-trained from scratch, Amber and CrystalCoder, including their training code, data, intermediate checkpoints, and analyses (at this https URL). We are committed to continually pushing the boundaries of LLMs through this open-source effort. More large-scale and stronger models are underway and will be released in the future.",,,Open weights (unrestricted),"United Arab Emirates,United States of America,United States of America,United States of America,United States of America,United States of America",,,,,,2025-05-01 10:42,Cerebras,"CRYSTALCODER is trained on the Cerebras Condor Galaxy 1 (CG-1), a 4 exaFLOPS, 54 million core, 64-node cloud AI supercomputer",,,,"Academia,Industry,Academia,Academia,Academia,Academia,Research collective",,,,,Open source,"https://huggingface.co/LLM360/Crystal
https://github.com/LLM360/crystalcoder-train
apache 2","Academia,Industry,Academia,Academia,Academia,Academia,Research collective",,,,,Operation counting,,,,
Amber,Language,"Language modeling/generation,Question answering","Mohamed bin Zayed University of Artificial Intelligence (MBZUAI),Petuum,University of Southern California,Carnegie Mellon University (CMU),University of Illinois Urbana-Champaign (UIUC),University of California San Diego,LLM360","Zhengzhong Liu, Aurick Qiao, Willie Neiswanger, Hongyi Wang, Bowen Tan, Tianhua Tao, Junbo Li, Yuqi Wang, Suqi Sun, Omkar Pangarkar, Richard Fan, Yi Gu, Victor Miller, Yonghao Zhuang, Guowei He, Haonan Li, Fajri Koto, Liping Tang, Nikhil Ranjan, Zhiqiang Shen, Xuguang Ren, Roberto Iriondo, Cun Mu, Zhiting Hu, Mark Schulze, Preslav Nakov, Tim Baldwin, Eric P. Xing",2023-12-11,LLM360: Towards Fully Transparent Open-Source LLMs,https://arxiv.org/abs/2312.06550,,,,6700000000.0,"6.7B
We used the exact same model architecture as LLaMA 7B",4.7898069e+22,"6*6,7*10^9*1259130000000=5.0617026e+22

312000000000000*600.5*3600*224*0.3 = 4.5325164e+22

Sqrt(4.5325164e+22*5.0617026e+22) = 4.7898069e+22","RefinedWeb,StarCoder,RedPajama","our pretraining data is a
mixture of RefinedWeb, StarCoder, and RedPajama-v1",1259130000000.0,1259.13 billion tokens (table 2),600.5,"The GPU cluster consists
of 56 DGX A100 nodes, each equipped with 4Ã— 80GB A100 GPUs

""The throughput we manage to achieve with our distributed training framework is around 582.4k tokens per second.""

1259130000000 / 582400 / 3600 = 600.5 hours

",NVIDIA A100 SXM4 80 GB,,Confident,"The recent surge in open-source Large Language Models (LLMs), such as LLaMA, Falcon, and Mistral, provides diverse options for AI practitioners and researchers. However, most LLMs have only released partial artifacts, such as the final model weights or inference code, and technical reports increasingly limit their scope to high-level design choices and surface statistics. These choices hinder progress in the field by degrading transparency into the training of LLMs and forcing teams to rediscover many details in the training process. We present LLM360, an initiative to fully open-source LLMs, which advocates for all training code and data, model checkpoints, and intermediate results to be made available to the community. The goal of LLM360 is to support open and collaborative AI research by making the end-to-end LLM training process transparent and reproducible by everyone. As a first step of LLM360, we release two 7B parameter LLMs pre-trained from scratch, Amber and CrystalCoder, including their training code, data, intermediate checkpoints, and analyses (at this https URL). We are committed to continually pushing the boundaries of LLMs through this open-source effort. More large-scale and stronger models are underway and will be released in the future.",,,Open weights (unrestricted),"United Arab Emirates,United States of America,United States of America,United States of America,United States of America,United States of America",,,,224.0,,2025-05-01 10:42,,,,,,"Academia,Industry,Academia,Academia,Academia,Academia,Research collective",,,,,Open source,"https://huggingface.co/LLM360/Amber
https://github.com/LLM360/amber-train
apache 2","Academia,Industry,Academia,Academia,Academia,Academia,Research collective",,,BF16,177626.32242072464,"Operation counting,Hardware",,,,
W.A.L.T,Video,"Video generation,Text-to-video","Stanford University,Google Research,Georgia Institute of Technology","Agrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera Hahn, Li Fei-Fei, Irfan Essa, Lu Jiang, JosÃ© Lezama",2023-12-11,Photorealistic Video Generation with Diffusion Models,https://arxiv.org/abs/2312.06662,,,,4719000000.0,"""We train our base model at resolution 17 Ã— 128 Ã— 128 (3B parameters), and two 2Ã— cascaded super-resolution models for 17 Ã— 128 Ã— 224 â†’ 17 Ã— 256 Ã— 448 (L, 1.3B, p = 2) and 17 Ã— 256 Ã— 448 â†’
17 Ã— 512 Ã— 896 (L, 419M, p = 2) respectively""

3B + 1.3B + 419M = 4.719B",,,Unspecified unreleased,,,"""We train W.A.L.T for text-to-video jointly on text-image and text-video pairs (Sec. 4.2). We used a dataset of âˆ¼970M text-image pairs and âˆ¼89M text-video pairs from the public internet and internal sources. """,,,,,Confident,"We present W.A.L.T, a transformer-based approach for photorealistic video generation via diffusion modeling. Our approach has two key design decisions. First, we use a causal encoder to jointly compress images and videos within a unified latent space, enabling training and generation across modalities. Second, for memory and training efficiency, we use a window attention architecture tailored for joint spatial and spatiotemporal generative modeling. Taken together these design decisions enable us to achieve state-of-the-art performance on established video (UCF-101 and Kinetics-600) and image (ImageNet) generation benchmarks without using classifier free guidance. Finally, we also train a cascade of three models for the task of text-to-video generation consisting of a base latent video diffusion model, and two video super-resolution diffusion models to generate videos of  resolution at  frames per second.",,,Unreleased,"United States of America,Multinational,United States of America,Canada,Switzerland,United States of America",,,,,,2025-05-19 15:22,,,,,,"Academia,Industry,Academia",,,,,Unreleased,,"Academia,Industry,Academia",,,,,,,,,
XVERSE-65B-2,Language,"Chat,Language modeling/generation","XVERSE Technology,Shenzhen Yuanxiang Technology",,2023-12-08,,https://github.com/xverse-ai/XVERSE-65B/blob/main/README_EN.md,,,,65000000000.0,Based on the name. Exact count unknown but may be listed on Hugging Face.,1.24800000000001e+24,C = 6ND = 6 * 3.2T tokens * 65B params = 1.248e24 FLOP,,"[2023/12/08] Released the XVERSE-65B-2 base model. This model builds upon its predecessor through Continual Pre-Training, reaching a total training volume of 3.2 trillion tokens.",3200000000000.0,"Training Data: The model has been thoroughly trained on a diversified and high-quality dataset consisting of 2.6 trillion of tokens, including more than 40 languages such as Chinese, English, Russian, and Spanish. The sampling ratio of different types of data is finely set, which makes the performance of Chinese and English excellent, and also takes into account the effect of other languages.

Assume 0.85 words per token on average for the mix of languages.",4096.0,November 6 to December 8 is 32 days. They did 600B tokens of continual pretraining during this period. The model's total tokens are 3200B. Therefore the total pretraining time was around (32 days * 24 hours/day)*(3200/600) = 4096 hours.,,,Confident,,,,Open weights (restricted use),"China,China",,,,,,2025-02-14 15:51,,"There is no paper to reference, no information about hardware used for training found in media.",,,,"Industry,Industry",,,,,Open source,"https://github.com/xverse-ai/XVERSE-65B/blob/main/README_EN.md 

license info:
""The use of the source code in this repository must follow the Apache-2.0 open-source license, while the use of the model weights of XVERSE-65B needs to adhere to the Model License Agreement.
The XVERSE-65B model weights are fully open to academic research and support free commercial use. To apply for a commercial license, please fill in the application form. For other questions or collaborations, please contact opensource@xverse.cn.""","Industry,Industry",,,,,Operation counting,,,,
SeamlessM4T,"Speech,Language","Translation,Speech synthesis,Speech recognition","Facebook,INRIA,University of California (UC) Berkeley","LoÃ¯c Barrault, Yu-An Chung, Mariano Coria Meglioli, David Dale, Ning Dong, Mark Duppenthaler, Paul-Ambroise Duquenne, Brian Ellis, Hady Elsahar, Justin Haaheim, John Hoffman, Min-Jae Hwang, Hirofumi Inaguma, Christopher Klaiber, Ilia Kulikov, Pengwei Li, Daniel Licht, Jean Maillard, Ruslan Mavlyutov, Alice Rakotoarison, Kaushik Ram Sadagopan, Abinesh Ramakrishnan, Tuan Tran, Guillaume Wenzek, Yilin Yang, Ethan Ye, Ivan Evtimov, Pierre Fernandez, Cynthia Gao, Prangthip Hansanti, Elahe Kalbassi, Amanda Kallet, Artyom Kozhevnikov, Gabriel Mejia Gonzalez, Robin San Roman, Christophe Touret, Corinne Wong, Carleigh Wood, Bokai Yu, Pierre Andrews, Can Balioglu, Peng-Jen Chen, Marta R. Costa-jussÃ , Maha Elbayad, Hongyu Gong, Francisco GuzmÃ¡n, Kevin Heffernan, Somya Jain, Justine Kao, Ann Lee, Xutai Ma, Alex Mourachko, Benjamin Peloquin, Juan Pino, Sravya Popuri, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, Anna Sun, Paden Tomasello, Changhan Wang, Jeff Wang, Skyler Wang, Mary Williamson",2023-12-08,Seamless: Multilingual Expressive and Streaming Speech Translation,"https://arxiv.org/abs/2312.05187, https://huggingface.co/facebook/seamless-m4t-v2-large",93.0,SOTA improvement,"""As an improved version of SeamlessM4T,
SeamlessM4T v2 delivers state-of-the-art semantic accuracy across different speech and text translation tasks
while supporting nearly 100 languages as input speech or text""",2300000000.0,2.3B,,,,"Several datasets including unlabeled speech, ASR data, TTS data",,~5M hours of audio data (figure 2),,,NVIDIA V100,Self-supervised learning,Confident,"Large-scale automatic speech translation systems today lack key features that help machine-mediated communication feel seamless when compared to human-to-human dialogue. In this work, we introduce a family of models that enable end-to-end expressive and multilingual translations in a streaming fashion. First, we contribute an improved version of the massively multilingual and multimodal SeamlessM4T model-SeamlessM4T v2. This newer model, incorporating an updated UnitY2 framework, was trained on more low-resource language data. SeamlessM4T v2 provides the foundation on which our next two models are initiated. SeamlessExpressive enables translation that preserves vocal styles and prosody. Compared to previous efforts in expressive speech research, our work addresses certain underexplored aspects of prosody, such as speech rate and pauses, while also preserving the style of one's voice. As for SeamlessStreaming, our model leverages the Efficient Monotonic Multihead Attention mechanism to generate low-latency target translations without waiting for complete source utterances. As the first of its kind, SeamlessStreaming enables simultaneous speech-to-speech/text translation for multiple source and target languages. To ensure that our models can be used safely and responsibly, we implemented the first known red-teaming effort for multimodal machine translation, a system for the detection and mitigation of added toxicity, a systematic evaluation of gender bias, and an inaudible localized watermarking mechanism designed to dampen the impact of deepfakes. Consequently, we bring major components from SeamlessExpressive and SeamlessStreaming together to form Seamless, the first publicly available system that unlocks expressive cross-lingual communication in real-time. The contributions to this work are publicly released and accessible at this https URL",,,Open weights (unrestricted),"United States of America,France,United States of America",W2v-BERT,,expanded from 1M hours data to 4.5M hours,,,2025-05-28 16:03,,,,,,"Industry,Academia,Academia",,,,,Open source,"looks like code is MIT licensed, model is CC 4.0?
https://github.com/facebookresearch/seamless_communication?tab=readme-ov-file
train code: https://github.com/facebookresearch/seamless_communication/blob/main/src/seamless_communication/cli/m4t/finetune/trainer.py","Industry,Academia,Academia",,,FP16,,,,,,
Llama Guard,Language,Chat,Meta AI,"Hakan Inan, Kartikeya Upasani, Jianfeng Chi, Rashi Rungta, Krithika Iyer, Yuning Mao, Davide Testuggine, Madian Khabsa",2023-12-07,Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations,https://arxiv.org/abs/2312.06674,201.0,SOTA improvement,"""Llama Guard, a Llama2-7b model that is instruction-tuned on our collected dataset, albeit low in volume, demonstrates strong performance on existing benchmarks such as the OpenAI Moderation Evaluation dataset and oxicChat, where its performance matches or exceeds that of currently available content moderation tools. """,7000000000.0,7B,1.6e+23,"1.7e17 finetune compute, plus Llama 2-13B pretrain compute (1.6e+23)",,"Dataset of prompt-response pairs of human-AI conversations

""We leverage the human preference data about harmlessness from Anthropic (Ganguli et al., 2022). From
this dataset, we pick the first human prompt and discard the corresponding response from the assistant, as
well as all the other turns to create an initial single-turn prompt dataset. Next, we use one of our internal
Llama checkpoints to generate a mix of cooperating and refusing responses for these prompts. We employ
our expert, in-house red team to label the prompt and response pairs for the corresponding category based
on the taxonomy defined in Section 2. The red-teamers annotate the dataset for 4 labels: prompt-category,
response-category, prompt-label (safe or unsafe), and response-label (safe or unsafe). During the annotation
process, we also do data cleaning, and discard examples with badly formatted inputs or outputs. The final
dataset comprises of 13,997 prompts and responses, with their respective annotations.""",4096000.0,"14k prompt-response pairs. Based on training details it's trained on ~4M tokens, which is stated to be ~1 epoch:
2 * 4096 * 500 = 4,096,000
(batch size) * (sequence length) * (steps)",,,NVIDIA A100 SXM4 80 GB,,Confident,"We introduce Llama Guard, an LLM-based input-output safeguard model geared towards Human-AI conversation use cases. Our model incorporates a safety risk taxonomy, a valuable tool for categorizing a specific set of safety risks found in LLM prompts (i.e., prompt classification). This taxonomy is also instrumental in classifying the responses generated by LLMs to these prompts, a process we refer to as response classification. For the purpose of both prompt and response classification, we have meticulously gathered a dataset of high quality. Llama Guard, a Llama2-7b model that is instruction-tuned on our collected dataset, albeit low in volume, demonstrates strong performance on existing benchmarks such as the OpenAI Moderation Evaluation dataset and ToxicChat, where its performance matches or exceeds that of currently available content moderation tools. Llama Guard functions as a language model, carrying out multi-class classification and generating binary decision scores. Furthermore, the instruction fine-tuning of Llama Guard allows for the customization of tasks and the adaptation of output formats. This feature enhances the model's capabilities, such as enabling the adjustment of taxonomy categories to align with specific use cases, and facilitating zero-shot or few-shot prompting with diverse taxonomies at the input. We are making Llama Guard model weights available and we encourage researchers to further develop and adapt them to meet the evolving needs of the community for AI safety.",1.0,,Open weights (restricted use),United States of America,Llama 2-7B,170000000000000000,"""We train on a single machine with 8xA100 80GB GPUs using a batch size of 2, with sequence length of 4096, using model parallelism of 1 and a learning rate of 2 Ã— 10âˆ’6. We train for 500 steps, which corresponds to âˆ¼1 epoch over our training set.""

6 * 2*4096*500 * 7 billion = 1.7e17",,,2025-05-16 10:30,,,,,,Industry,,,,,Unreleased,"Llama 2 license
https://github.com/facebookresearch/PurpleLlama/tree/main/Llama-Guard",Industry,,,,,Operation counting,,,,
StableLM - Zephyr 3B,Language,"Language modeling/generation,Question answering",Stability AI,,2023-12-07,"Introducing Stable LM Zephyr 3B: A New Addition to Stable LM, Bringing Powerful LLM Assistants to Edge Devices",https://huggingface.co/stabilityai/stablelm-zephyr-3b,,,,3000000000.0,3B,,,"MetaMathQA,WizardLM,SlimOrca,UltraFeedback,Capybara-DPO 7K binarized","SFT Datasets
HuggingFaceH4/ultrachat_200k
meta-math/MetaMathQA
WizardLM/WizardLM_evol_instruct_V2_196k
Open-Orca/SlimOrca

Preference Datasets:
HuggingFaceH4/ultrafeedback_binarized
Intel/orca_dpo_pairs",,,,"""Hardware: StableLM Zephyr 3B was trained on the Stability AI cluster across 8 nodes with 8 A100 80GBs GPUs for each nodes.""",NVIDIA A100 SXM4 80 GB,,Confident,"StableLM Zephyr 3B is a 3 billion parameter instruction tuned inspired by HugginFaceH4's Zephyr 7B training pipeline this model was trained on a mix of publicly available datasets, synthetic datasets using Direct Preference Optimization (DPO), evaluation for this model based on MT Bench and Alpaca Benchmark",,,Open weights (restricted use),United Kingdom of Great Britain and Northern Ireland,StableLM-3B-4E1T,,,8.0,,2025-05-27 10:50,,,,,,Industry,,,,,,"STABILITY AI COMMUNITY LICENSE AGREEMENT

If at any time You or Your Affiliate(s), either individually or in aggregate, generate more than USD $1,000,000 in annual revenue <..> You must request a license from Stability AI ",Industry,,,,6344.362343983899,,stabilityai,,,
Gemini 1.0 Ultra,"Multimodal,Language,Vision","Language modeling,Visual question answering,Chat,Translation",Google DeepMind,Gemini Team,2023-12-06,Gemini: A Family of Highly Capable Multimodal Models,https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,633.0,"SOTA improvement,Training cost","""Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks â€” notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined.""",,,5.0000000001e+25,"This number is an estimate based on limited evidence. In particular, we combine information about the performance of Gemini Ultra on various benchmarks compared to other models, and guesstimates about the hardware setup used for training to arrive at our estimate. Our reasoning and calculations are detailed in this Colab notebook.
https://colab.research.google.com/drive/1sfG91UfiYpEYnj_xB5YRy07T5dv-9O_c",Unspecified unreleased,"""Gemini models are trained on a dataset that is both multimodal and multilingual. Our pretraining dataset uses data from web documents, books, and code, and includes image, audio, and video data... We find that data quality is critical to a highlyperforming model, and believe that many interesting questions remain around finding the optimal dataset distribution for pretraining.""",,,2400.0,"Dylan Patel, author of SemiAnalysis, speculates that the training duration of Gemini may have been 100 days.",Google TPU v4,,Speculative,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks â€” notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",,,API access,"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France",,,,57000.0,,2025-02-25 15:51,,,,,,Industry,,,,132000000.0,Unreleased,API access: https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models,Industry,$29827341.92,,,19211950.056161165,"Benchmarks,Hardware",,,,1
Gemini 1.0 Pro,"Multimodal,Language,Vision","Language modeling,Visual question answering,Chat,Translation",Google DeepMind,Gemini Team,2023-12-06,Gemini: A Family of Highly Capable Multimodal Models,https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,633.0,Significant use,"Default/free model on gemini.google.com

From paper:
""Broadly, we find that the performance of Gemini Pro outperforms inference-optimized models such as GPT-3.5 and performs comparably with several of the most capable models available, and Gemini Ultra outperforms all current models. In this section, we examine some of these findings.""",,,1.830001e+24,"Training compute estimated from benchmark scores.

Our reasoning and calculations for Gemini 1 Ultra are detailed in this Colab notebook.
https://colab.research.google.com/drive/1sfG91UfiYpEYnj_xB5YRy07T5dv-9O_c

",Unspecified unreleased,"""Gemini models are trained on a dataset that is both multimodal and multilingual. Our pretraining dataset uses data from web documents, books, and code, and includes image, audio, and video data... We find that data quality is critical to a highlyperforming model, and believe that many interesting questions remain around finding the optimal
dataset distribution for pretraining.""",,,,,Google TPU v4,,Speculative,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks â€” notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",,,API access,"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France",,,,,,2025-01-29 21:34,,,,,,Industry,,,5.786971280399999e+25,,Unreleased,API access: https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models,Industry,,,,,Benchmarks,,,,12
OneLLM,"Multimodal,Language,Vision","Chat,Visual question answering,Question answering,Image captioning,Video description,Speech recognition","Chinese University of Hong Kong (CUHK),Shanghai AI Lab","Jiaming Han, Kaixiong Gong, Yiyuan Zhang, Jiaqi Wang, Kaipeng Zhang, Dahua Lin, Yu Qiao, Peng Gao, Xiangyu Yue",2023-12-06,OneLLM: One Framework to Align All Modalities with Language,https://arxiv.org/abs/2312.03700,48.0,,It is using many modalities - possible SOTA improvement on chating about fMRI scans.,7000000000.0,7B from Table 1,,,"Conceptual Captions (CC3M),LAION-400M,LAION-COCO,WebVid-2.5M,Wavcaps,Ego4D","""The source dataset is a subset of CC3M [73], around 0.5M image-text pairs. For IMU-text pairs, we use the IMU sensor data of Ego4D [27] and the
corresponding video narrations (i.e., text annotations). For fMRI-text pairs, we use the subj01 imaging session of NSD [5] and follow the same data split with [72]. Note that the visual stimulus, i.e., images shown to participants, are
from MS COCO [14]. Therefore, we use the image captions in COCO Captions as text annotations of fMRI-text pairs""

""Finally, our mutlimodal IT datasets have about 2M items, covering
multiple tasks such as detailed description/reasoning, conversation, short question answering and captioning.""",1007006000.0,Table 9: 1005000000 + 2006000 = 1007006000 datapoints,,,NVIDIA A100,,Likely,"Multimodal large language models (MLLMs) have gained significant attention due to their strong multimodal understanding capability. However, existing works rely heavily on modality-specific encoders, which usually differ in architecture and are limited to common modalities. In this paper, we present OneLLM, an MLLM that aligns eight modalities to language using a unified framework. We achieve this through a unified multimodal encoder and a progressive multimodal alignment pipeline. In detail, we first train an image projection module to connect a vision encoder with LLM. Then, we build a universal projection module (UPM) by mixing multiple image projection modules and dynamic routing. Finally, we progressively align more modalities to LLM with the UPM. To fully leverage the potential of OneLLM in following instructions, we also curated a comprehensive multimodal instruction dataset, including 2M items from image, audio, video, point cloud, depth/normal map, IMU and fMRI brain activity. OneLLM is evaluated on 25 diverse benchmarks, encompassing tasks such as multimodal captioning, question answering and reasoning, where it delivers excellent performance. Code, data, model and online demo are available at this https URL",,,Open weights (non-commercial),"Hong Kong,China,China",Llama 2-7B,42000000000000000000,"assuming that sizes in Table 9 are tokens, we can use 6ND approximation:
6*1B*7B = 4.2e19",16.0,,2024-12-08 14:04,,,,,,"Academia,Academia",,,,,Open (non-commercial),"https://github.com/csuhan/OneLLM
LLama license","Academia,Academia",,,,12689.007261035234,,,,,
NexusRaven-V2,Language,"Language modeling/generation,Code generation",Nexusflow,,2023-12-05,NexusRaven-V2: Surpassing GPT-4 for Zero-shot Function Calling,https://nexusflow.ai/blogs/ravenv2,,,,13000000000.0,"13B
",,,,,,,,,,,Confident,"We are thrilled to open source NexusRaven-V2, a 13B LLM outperforming GPT-4 in zero-shot function calling, the capability to turn natural language instructions into executable code to use tools. The function calling capability lies at the core of the OpenAI Assistants API, and serves as the key to enabling copilots and agents to use software tools. With the goal of advancing open source models for copilots and agents, NexusRaven-V2 marks an exciting step in collaboration with the community to expand the open model ecosystem for technological and societal impacts. The highlights of this release include:

State-of-the-art & Generalizable Capability. NexusRaven-V2 surpasses GPT-4 by up to 7% in function calling success rates in human-generated use cases involving nested and composite functions. NexusRaven-V2 has never been trained on the functions used in evaluation.
Open-source and Commercially Permissive. NexusRaven-V2 is further instruction-tuned on Meta's CodeLlama-13B-instruct, leveraging curated data generated through Nexusflow's pipeline, exclusively sourced from open-code corpora without using proprietary LLMs. It is commercially permissive for both community developers and enterprises.
Ease of Integration. We release open-source utility artifacts that enable users to seamlessly replace mainstream proprietary function calling APIs with NexusRaven-V2 in their software workflow. We also provide online demos and Colab notebooks for onboarding and integration demonstration.
Function Calling Benchmark and Leaderboard. We open source our evaluation benchmark Nexus-Function-Calling and establish a Huggingface leaderboard which includes an extensive collection of real-life human-curated function-calling examples, covering a diverse range of function-calling use cases and difficulties. These hundreds of examples, across 9 tasks, have been curated with input from domain experts, and their ground truth has been meticulously checked. We open source 8 out of the 9 benchmarks, leaving one as an internal benchmark for testing new models.",,,Open weights (restricted use),United States of America,Code Llama-13B,,,,,2025-05-01 10:42,,,,,,Industry,,,,,,"This model was trained on commercially viable data and is licensed under the Nexusflow community license.

Needs special permission ifnumber of users exceeds certain threshold

https://huggingface.co/Nexusflow/NexusRaven-V2-13B",Industry,,,,,,,,,
SARA-RT-2,Robotics,Robotic manipulation,Google DeepMind,"Isabel Leal, Krzysztof Choromanski, Deepali Jain, Avinava Dubey, Jake Varley, Michael Ryoo, Yao Lu, Frederick Liu, Vikas Sindhwani, Quan Vuong, Tamas Sarlos, Ken Oslund, Karol Hausman, Kanishka Rao",2023-12-04,SARA-RT: Scaling up Robotics Transformers with Self-Adaptive Robust Attention,https://arxiv.org/abs/2312.01990,,,"from blog: https://deepmind.google/discover/blog/shaping-the-future-of-advanced-robotics/

""The best SARA-RT-2 models were 10.6% more accurate and 14% faster than RT-2 models after being provided with a short history of images""

Not sure if this is SOTA since they used a smaller (5B) RT-2 variant",5000000000.0,"5B

""We focus on the 5B PaLI-X variant, as more practical for the on-robot deployment than the 55B variant""",,,,,,,,,,,Confident,"We present Self-Adaptive Robust Attention for Robotics Transformers (SARA-RT): a new paradigm for addressing the emerging challenge of scaling up Robotics Transformers (RT) for on-robot deployment. SARA-RT relies on the new method of fine-tuning proposed by us, called up-training. It converts pre-trained or already fine-tuned Transformer-based robotic policies of quadratic time complexity (including massive billion-parameter vision-language-action models or VLAs), into their efficient linear-attention counterparts maintaining high quality. We demonstrate the effectiveness of SARA-RT by speeding up: (a) the class of recently introduced RT-2 models, the first VLA robotic policies pre-trained on internet-scale data, as well as (b) Point Cloud Transformer (PCT) robotic policies operating on large point clouds. We complement our results with the rigorous mathematical analysis providing deeper insight into the phenomenon of SARA.",,,Unreleased,"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France",RT-2,,,,,2025-06-16 14:52,,,,,,Industry,,,,,Unreleased,demos here https://sites.google.com/view/rtsara/,Industry,,,,,,,,,
Mamba-24M (SC09),Speech,Audio generation,"Carnegie Mellon University (CMU),Princeton University","Albert Gu, Tri Dao",2023-12-01,Mamba: Linear-Time Sequence Modeling with Selective State Spaces,https://arxiv.org/abs/2312.00752,1195.0,SOTA improvement,"""SC09 is a benchmark speech generation dataset (Donahue, McAuley, and Puckette 2019; Warden 2018), consisting
of 1-second clips sampled at 16000 Hz of the digits â€œzeroâ€ through â€œnineâ€ with highly variable characteristics. We
largely follow the autoregressive training setup and generation protocol of Goel et al. (2022).
Table 4 shows automated metrics of the Mamba-UNet model compared to a variety of baselines from Goel et al.
(2022): WaveNet (Oord et al. 2016), SampleRNN (Mehri et al. 2017), WaveGAN (Donahue, McAuley, and Puckette
2019), DiffWave (Z. Kong et al. 2021), and SaShiMi. A small Mamba model outperforms the state-of-the-art
(and much larger) GAN- and diffusion- based models. A larger model parameter-matched to the baselines further
improves on fidelity metrics dramatically.""",23400000.0,Table 4,,,SC09,"""SC09 is a benchmark speech generation dataset (Donahue, McAuley, and Puckette 2019; Warden 2018), consisting of 1-second clips sampled at 16000 Hz of the digits â€œzeroâ€ through â€œnineâ€ with highly variable characteristics""",305280000.0,"Section 4.4.2: ""We largely follow the autoregressive training setup and generation protocol of Goel et al. (2022)""
In which they model raw audio waveforms, such that each sample is a datapoint.

SC09 is 5.3 hours long. 5.3h * 3600 sec/h * 16k samples/sec = 305,280,000 samples

Appendix E.4.2: ""We used a learning rate of 0.002 and 200000 training steps at a batch size of 16... training went through 100 epochs""",,,,,Confident,"Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5Ã— higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.",100.0,,Unreleased,"United States of America,United States of America",,,,,,2025-06-02 15:48,,,,,,"Academia,Academia",,,,,Unreleased,"no code or weights for 24M Mamaba model, but there are code and weights for other Mamba models:
https://github.com/state-spaces/mamba","Academia,Academia",,,,,,,,,
Mamba-2.8B,Language,"Language generation,Question answering","Carnegie Mellon University (CMU),Princeton University","Albert Gu, Tri Dao",2023-12-01,Mamba: Linear-Time Sequence Modeling with Selective State Spaces,https://arxiv.org/abs/2312.00752,1195.0,,,2800000000.0,"2.8B

https://github.com/state-spaces/mamba",5.400000000000001e+21,"""Table 3 shows the performance of Mamba on a range of popular downstream zero-shot evaluation tasks. We compare against the most well-known open source models at these sizes, most importantly Pythia (Biderman et al. 2023) and RWKV (B. Peng et al. 2023) which were trained with the same tokenizer, dataset, and training length (300B tokens) as our models.""

3b * 300b * 6 = 5.4e21

Note: this is a new architecture so not sure how well 6*params*data works as a heuristic

Figure 4 shows perplexity curves where Mamba is trained up to 2e20 FLOP, but those are for the 125M and 1.3B variants.",The Pile,,300000000000.0,"300B tokens of text 

""We compare against the most well-known open source models at these sizes, most importantly Pythia (Biderman et al. 2023) and RWKV (B. Peng et al. 2023) which were trained with the same tokenizer, dataset, and training length (300B tokens) as our models.""",,,,,Likely,"Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5Ã— higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.",,,Open weights (unrestricted),"United States of America,United States of America",,,,,,2024-11-10 21:04,,,,,,"Academia,Academia",,,,,Unreleased,"Apache 2.0 for model.
inference/model code:
https://github.com/state-spaces/mamba 

https://huggingface.co/state-spaces/mamba-2.8b","Academia,Academia",,,,,Operation counting,,,,
tsuzumi 7B,Language,"Chat,Language modeling/generation",NTT Communication Science Laboratories,,2023-12-01,"NTT's Large Language Model ""tsuzumi"" is Here!",https://group.ntt/en/magazine/blog/tsuzumi/,,,,7000000000.0,7B,,,,,,,,,,,Confident,,,,,Japan,,,,,,2024-09-05 14:08,,,,,,Industry,,,,,,,Industry,,,,,,,,,
NASA SMD,Language,"Language modeling/generation,Question answering","NASA,IBM","Masayasu Maraoka, Bishwaranjan Bhattacharjee, Muthukumaran Ramasubramanian, Ikhsa Gurung, Rahul Ramachandran, Manil Maskey, Kaylin Bugbee, Rong Zhang, Yousef El Kurdi, Bharath Dandala, Mike Little, Elizabeth Fancher, Lauren Sanders, Sylvain Costes, Sergi Blanco-Cuaresma, Kelly Lockhart, Thomas Allen, Felix Grazes, Megan Ansdell, Alberto Accomazzi, Sanaz Vahidinia, Ryan McGranaghan, Armin Mehrabian, Tsendgar Lee",2023-12-01,nasa-smd-ibm-v0.1 (Revision f01d42f),https://huggingface.co/nasa-impact/nasa-smd-ibm-v0.1,,,,125000000.0,,,,"Wikipedia,PubMed Abstracts,NASA ADS,PubMed Central","Total: 66.24B tokens
",66240000000.0,"Training Data
Wikipedia English (Feb 1, 2020)
AGU Publications
AMS Publications
Scientific papers from Astrophysics Data Systems (ADS)
PubMed abstracts
PubMedCentral (PMC) (commercial license subset)",,,,,Speculative,"nasa-smd-ibm-v0.1 is a RoBERTa-based, Encoder-only transformer model, domain-adapted for NASA Science Mission Directorate (SMD) applications. It's fine-tuned on scientific journals and articles relevant to NASA SMD, aiming to enhance natural language technologies like information retrieval and intelligent search.

IBM and NASA will build two foundation models. The first will be trained on reams of earth science journals to thematically organize the literature and make it easier to search and discover new knowledge. ",,,Open weights (unrestricted),"United States of America,United States of America",,49680000000000000000,66240000000*125000000.00*6,,,2024-09-05 14:08,,,,,,"Government,Industry",,,,,,"License:
apache-2.0
https://huggingface.co/nasa-impact/nasa-smd-ibm-v0.1","Government,Industry",,,,,,,,,
Qwen-72B,Language,"Chat,Code generation",Alibaba,"Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, Tianhang Zhu",2023-11-30,,https://huggingface.co/Qwen/Qwen-72B,,SOTA improvement,"SOTA on several Chinese benchmarks, with highest average rating overall for Chinese benchmarks:

https://opencompass.org.cn/leaderboard-llm",72000000000.0,72B,1.3e+24,"72 billion params, 3 trillion tokens
72b * 3T * 6 = 1.3e24",,"""It is pretrained on over 3 trillion tokens, including Chinese, English, multilingual texts, code, and mathematics, covering general and professional fields""",3000000000000.0,Assuming not trained for multiple epochs.,,,,,Confident,"Qwen-72B is the 72B-parameter version of the large language model series, Qwen (abbr. Tongyi Qianwen), proposed by Alibaba Cloud. Qwen-72B is a Transformer-based large language model, which is pretrained on a large volume of data, including web texts, books, codes, etc. Additionally, based on the pretrained Qwen-72B, we release Qwen-72B-Chat, a large-model-based AI assistant, which is trained with alignment techniques.",,,Open weights (restricted use),China,,,,,,2025-05-28 16:03,,"The paper does not mention any hardware, GPUs or any information regarding the hardware used.",,4000000.0,"Table 1 https://arxiv.org/abs/2309.16609
(this is uncertain because this table only lists sizes up to 14B. 72B was released after the paper)",Industry,,,4.11096e+24,,Unreleased,"up to 100m active users:
https://github.com/QwenLM/Qwen/blob/main/Tongyi%20Qianwen%20LICENSE%20AGREEMENT",Industry,,,BF16,,Operation counting,,,,11
Granite 13B,Language,"Chat,Language modeling/generation,Question answering,Text summarization",IBM,,2023-11-30,Granite Foundation Models,https://www.ibm.com/downloads/cas/X9W4O6BM,,,"Possible significant use - it is (one of) the foundation models behind IBM's ""watsonx"" product, alongside open models like Llama 2

https://www.ibm.com/products/watsonx-ai",13000000000.0,13 billion,2.44e+23,"Estimate using hardware:

""Granite.13b.v1 used 256 A100 GPUs for 1056 hours and 120 TFLOPs.
Granite.13b.v2 was trained on the same infrastructure for an
additional 1152 hours with 120 TFLOPS, bringing the total to
2208 hours""

Seems like 120 TFLOPS is the output per GPU after utilization, though they don't explicitly explain that part. That's 38% utilization.

256 * 2208 * 3600 * 120 TFLOPS = 2.44e23

Using 6ND:

""The second version of the granite.13b models leverages an updated base model trained on 2.5T trillion tokens.""

""The granite.13b.v1 base model is trained for 300K iterations,
with a batch size of 4M tokens, for a total of 1.25 trillion
5 tokens. The granite.13b.v2 base model continued pre-training
on top of the granite.13b.v1 checkpoint for an additional 300K
iterations and a total of 2.5 trillion tokens.""

2.5T * 13B * 6 = 1.95e23","Unspecified unreleased,Common Crawl,arXiv,OPENWEBTEXT","""To support the training of large enterprise-grade foundation
models, including granite.13b, IBM curated a massive dataset
of relevant unstructured language data from sources across
academia, the internet, enterprise (e.g., financial, legal), and
code.""

More breakdowns in paper, 20 sources in total

https://www.ibm.com/docs/en/cloud-paks/cp-data/4.8.x?topic=models-granite-13b-v1-model-card ",2500000000000.0,"2.5T tokens, 1.875T words at 0.75 words/token

https://www.ibm.com/docs/en/cloud-paks/cp-data/5.0.x?topic=models-granite-13b-chat-v2-model-card",2208.0,"""Granite.13b.v1 used 256 A100 GPUs for 1056 hours and 120 TFLOPs. Granite.13b.v2 was trained on the same infrastructure for an
additional 1152 hours with 120 TFLOPS, bringing the total to
2208 hours""",NVIDIA A100,,Likely,"We introduce the Granite series of decoder-only foundation models for generative artificial intelligence (AI) tasks that are ready for enterprise use. We report on the architecture, capabilities, underlying data and data governance, training algorithms, compute infrastructure, energy and carbon footprint,
testing and evaluation, socio-technical harms and mitigations, and usage policies.",1.0,,API access,United States of America,,,,,,2025-05-16 10:30,,,,,,Industry,,,,,Unreleased,,Industry,,,,,"Hardware,Operation counting",,,,
Cohere Command Light,Language,"Language generation,Chat,Text summarization,Language modeling/generation",Cohere,,2023-11-30,Cohereâ€™s Embed and Command Light Models with Fine-tuning Now Available on Amazon Bedrock,https://txt.cohere.com/embed-command-light-fine-tuning-on-amazon-bedrock/,,,,6000000000.0,"6B

https://aws.amazon.com/bedrock/cohere-command-embed/",1.001e+22,"https://docs.cohere.com/docs/environmental-impact

2700kg CO2 equivalent. Cohere used this calculator: https://mlco2.github.io/impact/ 

This calculator claims that ~40000 TPUv3 hours causes ~3000 kg CO2 emissions in the ""us-west1"", ""us-west2"", and ""us-west3"" regions. Not clear what region the data center Cohere used was in. Google has data centers around the world; *most* regions are similarly carbon intensive as us-west but north-america-northeast is 10x less carbon intensive and south Asia is 5x more carbon intensive. So the calculation below could be quite off.

Cohere most likely used TPUv4s, which the calculator does not support, which seem to be much more efficient (2.7x more, according to this https://cloud.google.com/blog/topics/systems/tpu-v4-enables-performance-energy-and-co2e-efficiency-gains)

40000 hours * 123 teraflops * 3600 * 0.3 utilization * 2.7 = 1.4e22

",,,,,,,Google TPU v4,,Speculative,,,,API access,Canada,,,,,,2025-02-14 15:51,,,,,,Industry,,,,,Unreleased,,Industry,,,,,Hardware,,,,
GNoME for crystal discovery,Materials science,Crystal discovery,Google DeepMind,"Amil Merchant, Simon Batzner, Samuel S. Schoenholz, Muratahan Aykol, Gowoon Cheon, Ekin Dogus Cubuk",2023-11-29,Scaling deep learning for materials discovery,https://www.nature.com/articles/s41586-023-06735-9,,Historical significance,Economic impacts from development of commercially and socially valuable protein designs and materials,16240000.0,"""The pretrained potential has 16.24 million parameters.""
This refers to the GNoME network, which is a ""Gaussian Network Model of Energy"" for predicting crystal potential energy of new crystals.",,"Pretraining involved 36.43M steps:
""The learning rate was decreased to a new value of 2â€‰Ã—â€‰10âˆ’4 after approximately 23 million steps, to 5â€‰Ã—â€‰10âˆ’5 after a further approximately 11 million steps and then trained for a final 2.43 million steps. Training was performed on four TPU v3 chips.""

""Inference on an A100 GPU on a 50-atom system takes approximately 14â€‰ms""
3.12e14 FLOP/s * 0.014 s = 4.368e12 FLOP per 50 atom system

Batch size was 32. Multiply inference FLOPs by 3 to account for forward and backward passes during training.
32 * 4.368e12 * 36.43 million * 3 = 1.53e22

This seems implausible â€“ on 4 TPUv3 chips this would take
(1.53e22 / (4 * 1.23e14)) / (3600 * 24) = 360 days",,"""Initial models are trained on a snapshot of the Materials Project from 2018 of approximately 69,000 materials""",69000.0,"""Initial models are trained on a snapshot of the Materials Project from 2018 of approximately 69,000 materials""",,,Google TPU v4,,Likely,"Novel functional materials enable fundamental breakthroughs across technological applications from clean energy to information processing. From microchips to batteries and photovoltaics, discovery of inorganic crystals has been bottlenecked by expensive trial-and-error approaches. Concurrently, deep-learning models for language, vision and biology have showcased emergent predictive capabilities with increasing data and computation. Here we show that graph networks trained at scale can reach unprecedented levels of generalization, improving the efficiency of materials discovery by an order of magnitude. Building on 48,000 stable crystals identified in continuing studies15,16,17, improved efficiency enables the discovery of 2.2 million structures below the current convex hull, many of which escaped previous human chemical intuition. Our work represents an order-of-magnitude expansion in stable materials known to humanity. Stable discoveries that are on the final convex hull will be made available to screen for technological applications, as we demonstrate for layered materials and solid-electrolyte candidates. Of the stable structures, 736 have already been independently experimentally realized. The scale and diversity of hundreds of millions of first-principles calculations also unlock modelling capabilities for downstream applications, leading in particular to highly accurate and robust learned interatomic potentials that can be used in condensed-phase molecular-dynamics simulations and high-fidelity zero-shot prediction of ionic conductivity.",1000.0,,Unreleased,"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France",,,,4.0,,2024-11-27 09:20,,,,,,Industry,,,,,Unreleased,,Industry,,,,1348.4172039257987,,,,,
PPLX-70B-Online,Language,"Question answering,Chat,Language modeling/generation",Perplexity,"Lauren Yang, Kevin Hu, Aarash Heydari, Gradey Wang, Dmitry Pervukhin, Nikhil Thota, Alexandr Yarats, Max Morozov, Denis Yarats",2023-11-29,Introducing PPLX Online LLMs ,https://blog.perplexity.ai/blog/introducing-pplx-online-llms,,Significant use,"Probably significant use: ""Perplexity, which has just 41 employees and is based out of a shared working space in San Francisco, has 10 million monthly active users, an impressive number for a young start-up."" However, this includes everyone who uses Perplexity's app which also uses third party models like GPT-4.

https://www.nytimes.com/2024/02/01/technology/perplexity-search-ai-google.html

",70000000000.0,70B,,,,"Fine-tuned on website excerpts:

""Website excerpts, which we call â€œsnippetsâ€, are provided to our pplx-online models to enable responses with the most up-to-date information.

Fine-tuning: our PPLX models have been fine-tuned to effectively use snippets to inform their responses. Using our in-house data contractors, we carefully curate high quality, diverse, and large training sets in order to achieve high performance on various axes like helpfulness, factuality, and freshness.""",,,,,,,Likely,"Weâ€™re excited to share two new PPLX models: pplx-7b-online and pplx-70b-online! Our online models are focused on delivering helpful, up-to-date, and factual responses, and are publicly available via pplx-api, making it a first-of-its-kind API. pplx-7b-online and pplx-70b-online are also accessible via Perplexity Labs, our LLM playground.",,,API access,United States of America,Llama 2-70B,,"""Fine-tuning: our PPLX models have been fine-tuned to effectively use snippets to inform their responses. Using our in-house data contractors, we carefully curate high quality, diverse, and large training sets in order to achieve high performance on various axes like helpfulness, factuality, and freshness. Our models are regularly fine-tuned to continually improve performance.""",,,2025-06-06 12:51,,,,,,Industry,,,,,Unreleased,https://docs.perplexity.ai/home,Industry,,,,,,,,,
Amazon Titan Text Express,Language,"Language modeling/generation,Text summarization,Chat,Code generation",Amazon,,2023-11-29,,https://aws.amazon.com/ru/about-aws/whats-new/2023/11/amazon-titan-models-express-lite-bedrock/,,,,,,,,,,,,,,,,Unknown,"Titan Text Express â€“ Titan Text Express has a maximum context length of 8,192 tokens and is ideal for a wide range of tasks, such as open-ended text generation and conversational chat, and support within Retrieval Augmented Generation (RAG) workï¬‚ows.",,,API access,United States of America,,,,,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
Amazon Titan Text Lite,Language,"Language modeling/generation,Text summarization,Chat,Code generation",Amazon,,2023-11-29,,https://aws.amazon.com/ru/about-aws/whats-new/2023/11/amazon-titan-models-express-lite-bedrock/,,,,,,,,,,,,,,,,Unknown,"Titan Text Lite â€“ Titan Text Lite has a maximum context length of 4,096 tokens and is a price-performant version that is ideal for English-language tasks. The model is highly customizable and can be fine-tuned for tasks such as article summarization and copywriting.",,,API access,United States of America,,,,,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
Yuan 2.0,Language,"Language modeling/generation,Translation,Code generation",Inspur,"Shaohua Wu, Xudong Zhao, Shenling Wang, Jiangang Luo, Lingjun Li, Xi Chen, Bing Zhao, Wei Wang, Tong Yu, Rongguo Zhang, Jiahua Zhang, Chao Wang",2023-11-27,YUAN 2.0: A Large Language Model with Localized Filtering-based Attention,https://arxiv.org/abs/2311.15786v1,,,,102600000000.0,102.6 billion,1.78e+23,"Trained on 288B tokens

6*102.6b*288b = 1.78e23",,"""The pretraining corpus includes a mix of books, codes, and encyclopedia in both Chinese and English (Table 2)""

with synthetic code data:
""Code (CN). Considering the diversity of programming tasks, we also build a synthesized instruction dataset
with 4 million code samples in Chinese. To cover the concepts involved in programming tasks as many as
possible, we collect 15,000 words of programming, computer science, mathematics, and other relevant
topics from the Sogou input dictionary. Two topic words are randomly selected as the seeds for a wellcrafted prompt in each time. Then the prompt will be fed to GPT-3.5 to generate a programming task and
corresponding Python solution. ""

and translated open-source fine-tuning instruction data:
""We construct a fine-tuning dataset focused on code, math and chat tasks.
Code Instruction dataset. We collect some open-source code instruction datasets, including CodeAlpaca20k [28], Evol-Instruct-Code-80k[38], CodeFuse-CodeExercise-Python-27k and CodeFuse-Evolinstruction-66k [39]. We translate the English code instruction into Chinese with GPT-3.5""",288000000000.0,"Most likely the 288B tokens do not represent multiple epochs. As a sense check, Table 2 appears to indicate that 5.73% of pre-training tokens come from synthetically generated text output by GPT-3.5. If the full training corpus is 288B tokens, this would imply ~$24k in API costs at $1.50/1M tokens to generate the data, which seems plausible.",,,,,Confident,"In this work, the Localized Filtering-based Attention (LFA) is introduced to incorporate prior knowledge of local dependencies of natural language into Attention. Based on LFA, we develop and release Yuan 2.0, a large language model with parameters ranging from 2.1 billion to 102.6 billion. A data filtering and generation method is presented to build pretraining and fine-tuning dataset in high quality. A distributed training method with non-uniform pipeline parallel, data parallel, and optimizer parallel is proposed, which greatly reduces the bandwidth requirements of intra-node communication, and achieves good performance in large-scale distributed training. Yuan 2.0 models display impressive ability in code generation, math problem-solving, and chat compared with existing models. The latest version of YUAN 2.0, including model weights and source code, is accessible at Github.",1.0,,Open weights (restricted use),China,,,,,,2025-05-26 19:55,,,,,,Industry,,,,,Open source,"commercial ok, but nothing that ""may cause harm to the country and society, or for any services that have not undergone security assessment and filing""
https://huggingface.co/IEITYuan/Yuan2-102B-hf

https://github.com/IEIT-Yuan/Yuan-2.0?tab=License-1-ov-file",Industry,,,,,Operation counting,IEITYuan,,,
StripedHyena-Hessian-7B,Language,"Chat,Language modeling/generation","Together,Nous Research","Michael Poli, Jue Wang, Stefano Massaroli, Jeffrey Quesnelle, Ryan Carlow, Eric Nguyen, Armin Thomas",2023-11-27,StripedHyena-Hessian-7B (SH 7B) ,"https://huggingface.co/togethercomputer/StripedHyena-Hessian-7B
https://github.com/togethercomputer/stripedhyena
https://www.together.ai/blog/stripedhyena-7b",,,,7000000000.0,7B,8e+19,see Figure at https://www.together.ai/blog/stripedhyena-7b,RedPajama,"StripedHyena was obtained by grafting architectural components of Transformers and Hyena, and trained on a mix of the RedPajama dataset, augmented with longer-context data.",,,,,,,Confident,"StripedHyena is the first alternative model architecture competitive with the best open-source Transformers of similar sizes in short and long-context evaluations.

StripedHyena is a deep signal processing, hybrid architecture composed of rotary (grouped) attention and gated convolutions arranged in Hyena blocks, with improved scaling over decoder-only Transformers. StripedHyena is designed to leverage the specialization of each of its layer classes, with Hyena layers implementing the bulk of the computation required for sequence processing and attention layers supplementing the ability to perform targeted pattern recall.

Efficient autoregressive generation via a recurrent mode (>500k generation with a single 80GB GPU)
Low latency, faster decoding and higher throughput than Transformers.
Significantly faster training and finetuning at long context (>3x at 131k)
Improved scaling laws over state-of-the-art architectures (e.g., Transformer++) on both natural language and biological sequences.
Robust to training beyond the compute-optimal frontier e.g., training way beyond Chinchilla-optimal token amounts",,,Open weights (unrestricted),"United States of America,United States of America",,,,,,2024-11-24 15:49,,,,,,"Industry,Industry",,,,,Unreleased,Apache 2.0,"Industry,Industry",,,,,Reported,,,,
Starling-LM-7B-alpha,Language,"Chat,Language modeling/generation",University of California (UC) Berkeley,"Zhu, Banghua and Frick, Evan and Wu, Tianhao and Zhu, Hanlin and Jiao, Jiantao",2023-11-25, Starling-LM-7B-alpha,"https://huggingface.co/berkeley-nest/Starling-LM-7B-alpha
https://starling.cs.berkeley.edu/",,,,7000000000.0,7B,,,,"'We present Nectar, the first high-quality 7-wise comparison dataset, generated through GPT-4-based ranking.' 'Our datasetâ€™s prompts are an amalgamation of diverse sources, including lmsys-chat-1M, ShareGPT, Antropic/hh-rlhf, UltraFeedback, Evol-Instruct, and Flan. Responses are primarily derived from a variety of models, namely GPT-4, GPT-3.5-turbo, GPT-3.5-turbo-instruct, LLama-2-7B-chat, and Mistral-7B-Instruct, alongside other existing datasets and models.'",,"'We release Nectar, a GPT-4 labeled ranking dataset composed of 183K chat prompts' size on https://huggingface.co/datasets/berkeley-nest/Nectar is 518MB

",,,NVIDIA A100,,Confident,"We introduce Starling-7B, an open large language model (LLM) trained by Reinforcement Learning from AI Feedback (RLAIF). The model harnesses the power of our new GPT-4 labeled ranking dataset, Nectar, and our new reward training and policy tuning pipeline. Starling-7B-alpha scores 8.09 in MT Bench with GPT-4 as a judge, outperforming every model to date on MT-Bench except for OpenAIâ€™s GPT-4 and GPT-4 Turbo. We release the ranking dataset Nectar, the reward model Starling-RM-7B-alpha and the language model Starling-LM-7B-alpha on HuggingFace, and an online demo in LMSYS Chatbot Arena. Stay tuned for our forthcoming code and paper, which will provide more details on the whole process.",4.0,,Open weights (non-commercial),United States of America,OpenChat 3.5-7B,,The model is trained on 8 A100 GPUs with batch size 28 and 10k steps in total.,8.0,,2024-12-02 12:34,,,,,,Academia,,,,,,"""The dataset, model and online demo is a research preview intended for non-commercial use only, subject to the data distillation License of LLaMA, Terms of Use of the data generated by OpenAI, and Privacy Practices of ShareGPT. Please contact us if you find any potential violation.""",Academia,,,,6346.057990066165,,,,,
Stable Video Diffusion,"Image generation,Video","Image generation,Video generation,Text-to-video,Image-to-video",Stability AI,"Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, Varun Jampani, Robin Rombach",2023-11-25,Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets,https://arxiv.org/abs/2311.15127,,,,,,6.7392e+22,312000000000000 FLOP / GPU / sec [bf16 assumed] * 200000 GPU-hours * 3600 sec / hour *  0.3 [assumed utilization] = 6.7392e+22 FLOP,,,,,,"""Training the SVD checkpoints required a total of approximately 200,000 A100 80GB hours. The majority of the training occurred on 48 * 8 A100s, while some stages took more/less than that. The resulting CO2 emission is ~19,000kg CO2 eq., and energy consumed is ~64000 kWh. """,NVIDIA A100 SXM4 80 GB,,Confident,"We present Stable Video Diffusion - a latent video diffusion model for high-resolution, state-of-the-art text-to-video and image-to-video generation. Recently, latent diffusion models trained for 2D image synthesis have been turned into generative video models by inserting temporal layers and finetuning them on small, high-quality video datasets. However, training methods in the literature vary widely, and the field has yet to agree on a unified strategy for curating video data. In this paper, we identify and evaluate three different stages for successful training of video LDMs: text-to-image pretraining, video pretraining, and high-quality video finetuning. Furthermore, we demonstrate the necessity of a well-curated pretraining dataset for generating high-quality videos and present a systematic curation process to train a strong base model, including captioning and filtering strategies. We then explore the impact of finetuning our base model on high-quality data and train a text-to-video model that is competitive with closed-source video generation. We also show that our base model provides a powerful motion representation for downstream tasks such as image-to-video generation and adaptability to camera motion-specific LoRA modules. Finally, we demonstrate that our model provides a strong multi-view 3D-prior and can serve as a base to finetune a multi-view diffusion model that jointly generates multiple views of objects in a feedforward fashion, outperforming image-based methods at a fraction of their compute budget. We release code and model weights at this https URL .",,,Open weights (non-commercial),United Kingdom of Great Britain and Northern Ireland,,,,384.0,,2025-05-30 14:36,,,,,,Industry,,,,200000.0,Open (non-commercial),"https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt

https://github.com/Stability-AI/generative-models",Industry,,,,304610.7835231759,Hardware,,,,
Belle VL,"Language,Vision,Multimodal","Language modeling/generation,Visual question answering",KE Holdings Inc. (â€œBeikeâ€),,2023-11-24,,https://huggingface.co/BELLE-2/BELLE-VL,,,,14000000000.0,I assume the same as Qwen 14B,,,"Flickr30K Entities,LLaVAR,LLaVA-Pretrain-595k,LRV-Instruction,LVIS-Instruct4V","""Pre-training data: Pre-training data is mainly based on LLava558kEnglish instruction data and its corresponding Chinese translation data, in addition we also collectedFlickr30k-CNA And fromAI ChallengerRandomly selected 100k data

Multi-mode instruction data: In the fine-tuning stage of the instruction, the data mainly comes fromLLava, LRV-Instruction, LLaVAR,LVIS-INSTRUCT4VWaiting for the open source project, we have also translated some of the data, and sincerely thank them for their contributions to the open source!""",,,,,,,Confident,"Open source BELLE-VL A multi-modular large-language model, based on a more capable language model base to expand the visual capabilities of the model, providing a more flexible choice for the community (currently the latest BELLE-VL model is in MME Perception assessment dimensions are obtained together 1620.10 (Over Qwen-VL, Llava, mplug-owl)",,,Open weights (unrestricted),China,Qwen-14B,,,,,2025-05-01 10:42,,,,,,Industry,,,,,,Apache 2.0,Industry,,,,,,,,,
TAIWAN-LLM 13B,Language,Chat,National Taiwan University,"Yen-Ting Lin, Yun-Nung Chen",2023-11-23,"""TAIWAN-LLM: Bridging the Linguistic Divide with
a Culturally Aligned Language Model""",https://arxiv.org/pdf/2311.17487,23.0,,,13000000000.0,,,,,,35100000000.0,,,,NVIDIA H100 SXM5 80GB,,Confident,,,,,Taiwan,Llama 2-13B,,,48.0,,2025-05-01 10:42,,,,,,Academia,,,,,,,Academia,,,,66636.57673893808,,,,,
TAIWAN-LLM 7B,Language,Chat,National Taiwan University,"Yen-Ting Lin, Yun-Nung Chen",2023-11-23,"""TAIWAN-LLM: Bridging the Linguistic Divide with
a Culturally Aligned Language Model""",https://arxiv.org/pdf/2311.17487,23.0,,,7000000000.0,,,,,,35100000000.0,,,,NVIDIA H100 SXM5 80GB,,Confident,,,,,Taiwan,Llama 2-7B,,,48.0,,2025-05-01 10:42,,,,,,Academia,,,,,,,Academia,,,,66636.57673893808,,,,,
Inflection-2,Language,"Language modeling,Language modeling/generation,Chat,Question answering",Inflection AI,,2023-11-22,Inflection-2: The Next Step Up,https://inflection.ai/inflection-2,,"Significant use,Training cost","Inflection-2 either already powers Pi or soon will: https://inflection.ai/inflection-2

Inflection has claimed that Pi has >1m users: https://x.com/inflectionAI/status/1699100179390210091?s=20",,,1.001e+25,"""Inflection-2 was trained on 5,000 NVIDIA H100 GPUs in fp8 mixed precision for ~10Â²âµ FLOPs""

(the second 1 is there because of airtable being wonky, it's not a real sig fig)",Unspecified unreleased,,,,,,NVIDIA H100 SXM5 80GB,,Confident,"Today we are proud to announce that we have completed training of Inflection-2, the best model in the world for its compute class and the second most capable LLM in the world today. Our mission at Inflection is to create a personal AI for everyone. Just a few months ago, we announced Inflection-1 â€” a best-in-class language model that currently powers Pi. Our new model, Inflection-2, is substantially more capable than Inflection-1, demonstrating much improved factual knowledge, better stylistic control, and dramatically improved reasoning.",,,Hosted access (no API),United States of America,,,,5000.0,,2025-02-11 17:15,,,,,,Industry,checked,,,,Unreleased,"via Pi, no API",Industry,$12961959.00,,FP8,6941464.657305156,"Hardware,Benchmarks",,,,3
OmniFusion-7B (InternViT-6B-448px V1-2),"Multimodal,Vision,Language","Visual question answering,Language modeling/generation,Question answering","AIRI Artificial Intelligence Research Institute,Sber,Skolkovo Institute of Science and Technology","Elizaveta Goncharova, Anton Razzhigaev, Matvey Mikhalchuk, Maxim Kurkin, Irina Abdullaeva, Matvey Skripkin, Ivan Oseledets, Denis Dimitrov, Andrey Kuznetsov",2023-11-22,OmniFusion Technical Report,https://arxiv.org/abs/2404.06212,6.0,,,12540000000.0,"The model uses InternViT-6B-448px V1-2 as the visual encoder, which undergoes pre-training, and GigaChat-7B as the LLM, which undergoes fine-tuning [1]. InternViT-6B-448px V1-2 has 5.54B parameters [2], and GigaChat-7B, being closed-source is assumed to have 7B parameters.
1. https://arxiv.org/pdf/2404.06212
2. https://huggingface.co/OpenGVLab/InternViT-6B-448px-V1-2",,"This is the pre-training compute for InternViT-6B-448px V1-2. I assumed the adapter processing the visual encoder's outputs required comparatively negligible pre-training compute because according to the developers, â€œthe adapter approach is undemanding of computational resources in comparison with end-to-end training pipelines"" [1]. The visual encoder and the LLM have the same order of magnitude of parameters, and the pre-training and fine-tuning datasets have the same order of magnitude of image-caption pairs. Therefore, pre-training compute and fine-tuning compute will be roughly the same order of magnitude.

Since the developers donâ€™t provide the number of training steps or epochs, I will assume they trained - meaning both pre-trained and fine-tuned - each model for one epoch.

The pre-training dataset was said to comprise 1.2M image-caption pairs, corresponding to 1.2M training examples. Then assuming InternViT-6B-448px V1-2 has dense architecture,
Pre-training compute
= 2 * # of connections * 3 * # of training examples * # of epochs
~= 2 * 5.54e9 parameters * 3 * 1.2e6 training examples * 1 epoch
= 39.9e15 FLOPS
= 3.99e16 FLOPS

More details here: https://docs.google.com/document/d/1BTmyZ9KVTIwkp9z9tRRFsWM5A0QcXiufXrIhujffyso/edit?tab=t.udrxq75icrez#heading=h.o7uo1xxfzje3

1. https://arxiv.org/pdf/2404.06212",,"The â€œadapters and special tokens undergo pretraining on a vast dataset of image-text pairsâ€ derived from â€œShareGPT4V-PT (695K pairs), LAION-CC-SBU with BLIP captions (558K pairs)"" [1].
1. https://arxiv.org/pdf/2404.06212",1200000.0,"The â€œadapters and special tokens undergo pretraining on a vast dataset of image-text pairsâ€ derived from â€œShareGPT4V-PT (695K pairs), LAION-CC-SBU with BLIP captions (558K pairs). Overall, we utilize 1.2M image captions"" [1]. Therefore, the dataset had 1.2M image-text pairs, or training examples [2].
1. https://arxiv.org/pdf/2404.06212
2. https://docs.google.com/document/d/1XWLyMzcVfDv4eFQX3yPgM8MZ3_Q1phtIFz9GKv4_KaM/edit?tab=t.0#heading=h.or67a8q9faep",,"No training time, number of training steps, or number of epochs provided.",NVIDIA A100,,Confident,"Last year, multimodal architectures served up a revolution in AI-based approaches and solutions, extending the capabilities of large language models (LLM). We propose an \textit{OmniFusion} model based on a pretrained LLM and adapters for visual modality. We evaluated and compared several architecture design principles for better text and visual data coupling: MLP and transformer adapters, various CLIP ViT-based encoders (SigLIP, InternVIT, etc.), and their fusing approach, image encoding method (whole image or tiles encoding) and two 7B LLMs (the proprietary one and open-source Mistral). Experiments on 8 visual-language benchmarks show the top score for the best OmniFusion setup in terms of different VQA tasks in comparison with open-source LLaVA-like solutions: VizWiz, Pope, MM-Vet, ScienceQA, MMBench, TextVQA, VQAv2, MMMU. We also propose a variety of situations, where OmniFusion provides highly-detailed answers in different domains: housekeeping, sightseeing, culture, medicine, handwritten and scanned equations recognition, etc. Mistral-based OmniFusion model is an open-source solution with weights, training and inference scripts available at https://github.com/AIRI-Institute/OmniFusion.",,,Open weights (unrestricted),"Russia,Russia,Russia","InternViT-6B,GigaChat-7B",39700000000000000,"GigaChat is closed-source, so I can only assume that it has dense architecture. The fine-tuning dataset was quite heterogeneous as well, so I will assume that the 945.4K image-caption pairs in the dataset correspond to 945.4K training examples. Then,
Fine-tuning compute
= 2 * # of connections * 3 * # of training examples * # of epochs
~= 2 * 7e9 parameters * 3 * 9.455e5 training examples * 1 epoch
= 397e14 FLOPS
= 3.97e16 FLOPS

More details here: https://docs.google.com/document/d/1BTmyZ9KVTIwkp9z9tRRFsWM5A0QcXiufXrIhujffyso/edit?tab=t.udrxq75icrez#heading=h.o7uo1xxfzje3",8.0,,2025-06-09 15:09,,,,,,"Research collective,Industry,Government,Academia",,,,,Open source,"Apache 2.0 for code
https://github.com/AIRI-Institute/OmniFusion

Apache 2.0 for weights
https://huggingface.co/AIRI-Institute/OmniFusion","Research collective,Industry,Government,Academia",,,,6346.481972393285,,,,,
Claude 2.1,Language,"Language modeling,Chat,Language modeling/generation,Question answering",Anthropic,,2023-11-21,Introducing Claude 2.1,https://www.anthropic.com/index/claude-2-1,,Significant use,,,,,,Unspecified unreleased,"January 1, 2023, according to https://www.prompthub.us/models/claude-2-1#:~:text=The%20knowledge%20cut%2Doff%20date,2.1%20is%20January%201%2C%202023.",,,,,,Reinforcement learning,Unknown,"Our latest model, Claude 2.1, is now available over API in our Console and is powering our claude.ai chat experience. Claude 2.1 delivers advancements in key capabilities for enterprisesâ€”including an industry-leading 200K token context window, significant reductions in rates of model hallucination, system prompts and our new beta feature: tool use.",,,API access,United States of America,Claude 2,,,,,2025-05-12 19:51,,,,,,Industry,checked,,,,Unreleased,,Industry,,,,,,,,,
Orca 2-13B,Language,"Language modeling/generation,Question answering",Microsoft Research,"Arindam Mitra, Luciano Del Corro, Shweti Mahajan, Andres Codas, Clarisse Simoes, Sahaj Agarwal, Xuxi Chen, Anastasia Razdaibiedina, Erik Jones, Kriti Aggarwal, Hamid Palangi, Guoqing Zheng, Corby Rosset, Hamed Khanpour, Ahmed Awadallah",2023-11-21,Orca 2: Teaching Small Language Models How to Reason,"https://arxiv.org/abs/2311.11045, https://huggingface.co/microsoft/Orca-2-13b",99.0,,"""Orca 2 significantly surpasses models of similar size and attains performance levels similar or better
to those of models 5-10x larger, as assessed on complex tasks that test advanced reasoning
abilities in zero-shot settings""",13000000000.0,based on Llama 13B,4.6e+22,4.55e22 base compute from Llama-13 + 8.6e20 finetune compute ~= 4.6e22 ,"Flan,Orca 2 Dataset","""For Orca 2, we created a new dataset with ~817K training instances, which we will refer as Orca 2 dataset. Following Orca 1, Orca 2 has been trained with progressive learning, with subsets of data obtained from combining the original FLAN [33] annotations, Orca 1 dataset and the Orca 2 dataset. We also describe the details about the progressive learning.""",800000000.0,"817k training instances, ~1000 tokens per instance. See finetune notes",80.0,"17+40+23 hours

""We trained Orca 2 on 32 NVIDIA A100 GPUs with 80GB memory with bfloat16.
For the 13B checkpoint, it took ~17 hours to train Orca 2 on FLAN dataset for one epoch,
~40 hours to train on 5 million ChatGPT data for 3 epochs and ~23 hours to continue
training on ~1.8 million GPT-4 data for 4 epochs""",NVIDIA A100 SXM4 80 GB,,Confident,"Orca 1 learns from rich signals, such as explanation traces, allowing it to outperform conventional instruction-tuned models on benchmarks like BigBench Hard and AGIEval. In Orca 2, we continue exploring how improved training signals can enhance smaller LMs' reasoning abilities. Research on training small LMs has often relied on imitation learning to replicate the output of more capable models. We contend that excessive emphasis on imitation may restrict the potential of smaller models. We seek to teach small LMs to employ different solution strategies for different tasks, potentially different from the one used by the larger model. For example, while larger models might provide a direct answer to a complex task, smaller models may not have the same capacity. In Orca 2, we teach the model various reasoning techniques (step-by-step, recall then generate, recall-reason-generate, direct answer, etc.). More crucially, we aim to help the model learn to determine the most effective solution strategy for each task. We evaluate Orca 2 using a comprehensive set of 15 diverse benchmarks (corresponding to approximately 100 tasks and over 36,000 unique prompts). Orca 2 significantly surpasses models of similar size and attains performance levels similar or better to those of models 5-10x larger, as assessed on complex tasks that test advanced reasoning abilities in zero-shot settings. make Orca 2 weights publicly available at this http URL to support research on the development, evaluation, and alignment of smaller LMs",,,Open weights (non-commercial),"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational",LLaMA-13B,860000000000000000000,"""We trained Orca 2 on 32 NVIDIA A100 GPUs with 80GB memory with bfloat16.
For the 13B checkpoint, it took ~17 hours to train Orca 2 on FLAN dataset for one epoch, ~40 hours to train on 5 million ChatGPT data for 3 epochs and ~23 hours to continue training on ~1.8 million GPT-4 data for 4 epochs""

32 * (17+40+23) * 3600 * 312 trillion flop/s * 0.3 utilization = 8.6e20

(each 10 hours is ~1e20 flop, so the 5 million ChatGPT data took 4e20 FLOP. 4e20 / 5 million / 13 billion / 6 = 1025, so ~1000 tokens in each example)",,,2025-02-14 15:52,,,,,,Industry,,,,,Unreleased,"non commercial

https://huggingface.co/microsoft/Orca-2-13b/blob/main/LICENSE",Industry,,,,,Hardware,,,,
Tulu V2 DPO 70B,Language,"Language modeling/generation,Question answering","Allen Institute for AI,University of Washington","Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah A. Smith, Iz Beltagy, Hannaneh Hajishirzi",2023-11-20,Camels in a Changing Climate: Enhancing LM Adaptation with Tulu 2,https://arxiv.org/abs/2311.10702,,,,70000000000.0,70B,,,TÃœLU-V2-mix,,150000000.0,"""After filtering, the V2 mixture consists of 326,154 samples""
""The mean length of a sample is 1097 tokens""

-> total ~ 357 790 938 tokens

from figure 1 (more accurate):

(1500*10^4.2)+(2500*10^4)+(3500*10^4.2)+(4500*10^4)+(5500*100)  = 149 795 000 tokens
",168.0,"""The 70B variant of TÃœLU V2-DPO was trained on a 512-core TPUv3, completing three epochs in approximately 7 days.""

""All models except QLoRA models were trained on a 256-chip (512-chip for 70B DPO training) TPU v3 pod.""",Google TPU v3,,Likely,"Since the release of TÃœLU [Wang et al., 2023b], open resources for instruction tuning have developed quickly, from better base models to new finetuning techniques. We test and incorporate a number of these advances into TÃœLU, resulting in TÃœLU 2, a suite of improved TÃœLU models for advancing the understanding and best practices of adapting pretrained language models to downstream tasks and user preferences. Concretely, we release: (1) TÃœLU-V2-mix, an improved collection of high-quality instruction datasets; (2) TÃœLU 2, LLAMA-2 models finetuned on the V2 mixture; (3) TÃœLU 2+DPO, TÃœLU 2 models trained with direct preference optimization (DPO), including the largest DPO-trained model to date (TÃœLU 2+DPO 70B); (4) CODE TÃœLU 2, CODE LLAMA models finetuned on our V2 mix that outperform CODE LLAMA and its instruction-tuned variant, CODE LLAMA-Instruct. Our evaluation from multiple perspectives shows that the TÃœLU 2 suite achieves state-of-the-art performance among open models and matches or exceeds the performance of GPT-3.5-turbo-0301 on several benchmarks. We release all the checkpoints, data, training and evaluation code to facilitate future open efforts on adapting large language models.",3.0,,Open weights (restricted use),"United States of America,United States of America",Llama 2-70B,5,"123000000000000*512*168*3600*0.3*(1 chip / 2 cores)= 5.7131825e+21

6*70*10^9*150000000*3 = 1.89e+20

not sure why here is such a big difference",512.0,,2025-05-01 10:42,Google Cloud,,,,,"Research collective,Academia",,,,,Open source,"llama license for weights
https://huggingface.co/allenai/tulu-2-dpo-70b

apache 2.0 for insturction code
https://github.com/allenai/open-instruct","Research collective,Academia",,,BF16,223406.1154341212,"Hardware,Operation counting",,,,
Lyria,Audio,Audio generation,Google DeepMind,"Kazuya Kawakami, David Ding, BjÃ¶rn Winckler, CÄƒtÄƒlina Cangea, Tobenna Peter Igwe, Will Grathwohl, Yan Wu, Yury Sulsky, Jacob Kelly, Charlie Nash, Conor Durkan, Yaroslav Ganin, Tom Eccles, Zach Eaton-Rosen, Jakob Bauer, Mikita Sazanovich, Morgane RiviÃ¨re, Evgeny Gladchenko, MikoÅ‚aj BiÅ„kowski, Ali Razavi, Jeff Donahue, Benigno Uria, Sander Dieleman, Sherjil Ozair, John Schultz, Ankush Gupta, Junlin Zhang, Drew Jaegle, AÃ¤ron van den Oord.",2023-11-16,Transforming the future of music creation,https://deepmind.google/discover/blog/transforming-the-future-of-music-creation/,,,,,,,,Unspecified unreleased,,,,,,,,Unknown,"Music contains huge amounts of information â€” consider every beat, note, and vocal harmony in every second. When generating long sequences of sound, itâ€™s difficult for AI models to maintain musical continuity across phrases, verses, or extended passages. Since music often includes multiple voices and instruments at the same time, it's much harder to create than speech.

Built by Google DeepMind, the Lyria model excels at generating high-quality music with instrumentals and vocals, performing transformation and continuation tasks, and giving users more nuanced control of the outputâ€™s style and performance.",,,Unreleased,"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France",,,,,,2025-06-16 15:20,,,,,,Industry,checked,,,,Unreleased,,Industry,,,,,,,,,
Mistral 7B + OVM,Language,Quantitative reasoning,Chinese University of Hong Kong (CUHK),"Fei Yu, Anningzhe Gao, Benyou Wang",2023-11-16,Outcome-supervised Verifiers for Planning in Mathematical Reasoning,https://arxiv.org/abs/2311.09724,21.0,,"""Notably, in GSM8K, our OVM-7B model achieves state-of-the-art
results among LLMs up to 13B parameters;
especially it does not utilize GPT-4 or code
execution""",7000000000.0,"7B, based on Mistral 7B",,"Fine tuning likely small fraction of pretraining. E.g. Game of 24 has 90k solution paths, if average path is 1024 tokens and they do 10 epochs that is 
6 * 90k * 1024 * 10 * 7B = 3.9e19 FLOP. 
We don't have exact flops for Mistral 7B but compare at 8.4e22 FLOP for Llama 2-7B",,"They generated a synthetic dataset using fine-tuned Mistral 7B, which they then use to further train the model:

""OVMs were initialized from the corresponding generator checkpoints. We sampled 100 solution paths for each question from the respective generator (resulting in 747,300 for GSM8K and 90,000 for Game of
24), with a temperature setting of 0.7. In GSM8K, OVM was trained for 1 epoch with a batch size of 512. In Game of 24, OVM is trained for 10 epochs
with a batch size of 128, due to its smaller training set. ",837300.0,"""This allows us to produce numerous OVM training samples using just question-answer pairs, without path annotations, resulting in 747,300 training samples for GSM8K and 90,000 for Game of 24.""

747300+90000=837300 training samples",,,,,Confident,"Large language models (LLMs) often struggle with maintaining accuracy across a sequence of intermediate reasoning steps in mathematical reasoning, leading to error propagation that undermines the final result. The current methodology to mitigate this issue primarily involves using a verifier model to assess the correctness of generated solution candidates, focusing either on the overall reasoning path or on an incomplete reasoning path. By rethinking this approach, we argue that assessing potentials of incomplete reasoning paths could be more advantageous as it guides towards correct final answers, transforming the task into a \textit{planning} problem. Our proposed verifier, the Outcome-supervision Value Model (OVM), employs outcome supervision for training, offering an efficient and intuitive method for \textit{planning} by prioritizing steps that lead to accurate conclusions over mere per-step correctness. Furthermore, the OVM eschews the need for labor-intensive annotations on step-level correctness, enhancing its scalability. Our experiments on two multi-step mathematical reasoning datasets, GSM8K and Game of 24, demonstrate the superior performance of the OVM model. Notably, in GSM8K, our \textbf{OVM-7B model achieves state-of-the-art results among LLMs up to 13B parameters}; especially it does not utilize GPT-4 or code execution. These findings offer a novel perspective on the role of outcome supervision in training verifiers for multi-step reasoning tasks and provide theoretical justification for its advantage in value estimation for planning.",,,Open weights (non-commercial),"Hong Kong,China",Mistral 7B,,,,,2024-11-01 10:05,,,,,,Academia,,,,,Open (non-commercial),no license,Academia,,,,,,,,,
AndesGPT,Language,"Language modeling/generation,Question answering",Oppo Mobile Telecommunications,,2023-11-16,,https://www.oppo.com/en/newsroom/press/2023-oppo-developers-conference-odc23/,,SOTA improvement,Highest score at SuperCLUE Safety benchmark. https://www.superclueai.com/,,,,,,,,,,,,,Unknown,,,,Hosted access (no API),China,,,,,,2025-06-02 10:24,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
Nemotron-3-8B,Language,"Chat,Language generation,Language modeling/generation,Translation,Code generation,Question answering",NVIDIA,,2023-11-15,NVIDIA AI Foundation Models: Build Custom Enterprise Chatbots and Co-Pilots with Production-Ready LLMs,"https://developer.nvidia.com/blog/nvidia-ai-foundation-models-build-custom-enterprise-chatbots-and-co-pilots-with-production-ready-llms/

https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/nemotron-3-8b-base-4k",,SOTA improvement,"""The Nemotron-3-8B-QA model offers state-of-the-art performance, achieving a zero-shot F1 score of 41.99% on the Natural Questions dataset. This metric measures how closely the generated answer resembles the truth in â€ŒQA. """,8000000000.0,,1.8e+23,"https://huggingface.co/nvidia/nemotron-3-8b-base-4k

""This model was trained on a dataset containing 3.8 Trillion tokens of text""

8 billion * 3.8 trillion * 6 = 1.8e23

Also, using the hardware method: ""1,024 A100s were used for 19 days to train the model.""

19*1024 * 312 trillion * 24 * 3600 * 0.3 = 1.57e23","Unspecified unreleased,Flan,P3 (Public Pool of Prompts)","""NVIDIA models are trained on a diverse set of public and proprietary datasets. This model was trained on a dataset containing 3.8 Trillion tokens of text. The dataset contains 53 different human languages (including English, German, Russian, Spanish, French, Japanese, Chinese, Italian, and Dutch) and 37 programming languages. The model also uses the training subsets of downstream academic benchmarks from sources like FLANv2, P3, and NaturalInstructions v2""",3800000000000.0,,456.0,19 days,NVIDIA A100,,Confident,"Large language models (LLMs) are revolutionizing data science, enabling advanced capabilities in natural language understanding, AI, and machine learning. Custom LLMs, tailored for domain-specific insights, are finding increased traction in enterprise applications.

The NVIDIA Nemotron-3 8B family of foundation models is a powerful new tool for building production-ready generative AI applications for the enterpriseâ€“fostering innovations ranging from customer service AI chatbots to cutting-edge AI products.",,,Open weights (restricted use),United States of America,,,,1024.0,0.3473,2025-05-26 19:54,,,,,,Industry,,,,,Unreleased,"can't use to train other models:

https://developer.download.nvidia.com/ai-foundation-models/nvidia-ai-foundation-models-license-10Nov2023.pdf",Industry,$214467.02,"Ops counting: 8 billion * 3.8 trillion * 6 = 1.82e23
GPU-time: 19 * 1024 * 3.12e14 * 24 * 3600 = 5.24e23
Implied utilization: 1.82e23 / 5.24e23 = 0.3473",,812476.3359553809,"Operation counting,Hardware",,,,
GraphCast,Earth science,Weather forecasting,Google DeepMind,"Remi Lam, Alvaro Sanchez-Gonzalez, Matthew Willson, Peter Wirnsberger, Meire Fortunato, Ferran Alet, Suman Ravuri, Timo Ewalds, Zach Eaton-Rosen, Weihua Hu, Alexander Merose, Stephan Hoyer, George Holland, Oriol Vinyals, Jacklynn Stott, Alexander Pritzel, Shakir Mohamed, Peter Battaglia",2023-11-14,Learning skillful medium-range globalweather forecasting,https://www.science.org/doi/epdf/10.1126/science.adi2336,,SOTA improvement,"""Our state-of-the-art model delivers 10-day weather predictions at unprecedented accuracy in under one minute""",,Not mentioned in paper.,2.1e+22,"""Training GraphCast took roughly four weeks on 32 Cloud TPU v4 devices using batch parallelism.""

4.6: ""we use bfloat16 floating point precision""

2.1e22 = 2.75E+14 FLOP/s * 32 * 60* 60 * 24 * 7 * 4",,"According to the blog post, ""we trained GraphCast on four decades of weather reanalysis data, from the ECMWFâ€™s ERA5 dataset. This trove is based on historical weather observations such as satellite images, radar, and weather stations using a traditional NWP to â€˜fill in the blanksâ€™ where the observations are incomplete, to reconstruct a rich record of global historical weather.""
https://deepmind.google/discover/blog/graphcast-ai-model-for-faster-and-more-accurate-global-weather-forecasting/",,,,,,,Speculative,"Global medium-range weather forecasting is critical to decision-making across many social and economic domains. Traditional numerical weather prediction uses increased compute resources to improve forecast accuracy but does not directly use historical weather data to improve the underlying model. Here, we introduce GraphCast, a machine learningâ€“based method trained directly from reanalysis data. It predicts hundreds of weather variables for the next 10 days at 0.25Â° resolution globally in under 1 minute. GraphCast significantly outperforms the most accurate operational deterministic systems on 90% of 1380 verification targets, and its forecasts support better severe event prediction, including tropical cyclone tracking, atmospheric rivers, and extreme temperatures. GraphCast is a key advance in accurate and efficient weather forecasting and helps realize the promise of machine learning for modeling complex dynamical systems.",,,Open weights (unrestricted),"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France",,,,,,2025-06-06 10:54,,,,,,Industry,,,,,Open source,"Apache 2 
https://github.com/google-deepmind/graphcast",Industry,,,BF16,,Hardware,,,,
Qwen-Audio-Chat,"Language,Speech,Audio","Audio question answering,Chat,Speech recognition,Translation,Transcription,Text classification,Question answering,Audio classification,Voice identification,Part-of-speech tagging",Alibaba,"Yunfei Chu, Jin Xu, Xiaohuan Zhou, Qian Yang, Shiliang Zhang, Zhijie Yan, Chang Zhou, Jingren Zhou",2023-11-14,Qwen-Audio: Advancing Universal Audio Understanding via Unified Large-Scale Audio-Language Models,https://arxiv.org/abs/2311.07919,144.0,SOTA improvement,"""A notable achievement of Qwen-Audio is its state-of-the-art performance on the test set of Aishell1, cochlscene, ClothoAQA, and VocalSound""",8460000000.0,"the model has two components - audio and language.
670M + 7.7B = 8.46B
""The audio encoder is composed of 640M parameters""

""Qwen-Audio incorporates a large language model as its foundational component.
The model is initialized using pre-trained weights derived from Qwen-7B (Bai et al., 2023a). Qwen-7B is a 32-layer Transformer decoder model with a hidden size of 4096, encompassing a total of 7.7B parameters.""",,,,multiple audio and language sources,,not clear,,,,,Likely," Recently, instruction-following audio-language models have received broad attention for audio interaction with humans. However, the absence of pre-trained audio models capable of handling diverse audio types and tasks has hindered progress in this field. Consequently, most existing works have only been able to support a limited range of interaction capabilities. In this paper, we develop the Qwen-Audio model and address this limitation by scaling up audio-language pre-training to cover over 30 tasks and various audio types, such as human speech, natural sounds, music, and songs, to facilitate universal audio understanding abilities. However, directly co-training all tasks and datasets can lead to interference issues, as the textual labels associated with different datasets exhibit considerable variations due to differences in task focus, language, granularity of annotation, and text structure. To overcome the one-to-many interference, we carefully design a multi-task training framework by conditioning on a sequence of hierarchical tags to the decoder for encouraging knowledge sharing and avoiding interference through shared and specified tags respectively. Remarkably, Qwen-Audio achieves impressive performance across diverse benchmark tasks without requiring any task-specific fine-tuning, surpassing its counterparts. Building upon the capabilities of Qwen-Audio, we further develop Qwen-Audio-Chat, which allows for input from various audios and text inputs, enabling multi-turn dialogues and supporting various audio-central scenarios. ",,,Open weights (restricted use),China,,,,,,2025-06-06 11:40,,,,,,Industry,,,,,Open (restricted use),"Qwen license:

https://github.com/QwenLM/Qwen-Audio/blob/main/LICENSE

https://huggingface.co/Qwen/Qwen-Audio

separate license required for companies with 100M+ MAU",Industry,,,BF16,,,,,,
SPHINX (Llama 2 13B),"Vision,Language,Multimodal","Visual question answering,Image captioning","Shanghai AI Lab,Chinese University of Hong Kong (CUHK),ShanghaiTech University","Ziyi Lin, Chris Liu, Renrui Zhang, Peng Gao, Longtian Qiu, Han Xiao, Han Qiu, Chen Lin, Wenqi Shao, Keqin Chen, Jiaming Han, Siyuan Huang, Yichi Zhang, Xuming He, Hongsheng Li, Yu Qiao",2023-11-13,"SPHINX: The Joint Mixing of Weights, Tasks, and Visual Embeddings for Multi-modal Large Language Models",https://arxiv.org/abs/2311.07575,,SOTA improvement,"""as shown in Figure 2, SPHINX can achieve impressive fine-grained visual perception for high-resolution images, which exhibits state-of-the-art performance on extensive evaluation benchmarks, e.g., MMBench (Liu et al., 2023f), MME (Fu et al., 2023a), and POPE (Li et al., 2023e).""",19900000000.0,"SPHINX + Llama 2 13B
SPHINX component involves four vision encoders:
- CLIP - ViT
- CLIP - ConvNeXt V2 (89M to 659M params, depending on size)
- DinoV2 - ViT (22M to 1.14B params, depending on size)
- Q-former (188M params)
Also involves to projection networks

Huggingface Hub model files appear to be 39.8GB. Assuming models are stored in fp16 there are 2 bytes per parameter, so 39.8 / 2 = 19.9B parameters.",3.04e+22,"""The pre-training time is around 125 hours on 32 A100 GPUs with a 7B
language model and about twice the time with a 13B language model... The fine-tuning takes about 38 hours with 16 A100 GPUs with a 13B
language model.""

((125*2 * 32) + (38 * 16)) * 3.12e14 * 3600 * 0.3 = 2.9e21

Component vision encoders were initialized from pre-trained:
- CLIP ViT: 1.5e22 FLOPs for L/14@336
- ConvNeXt V2: 6.8e21 FLOPs for largest
- DinoV2: 7.42e+21 FLOPs for largest
- Q-former: 1.2e21 FLOPs for largest

(Based on full parameter count, SPHINX probably uses largest models)

Sum: 3.04e22 FLOPs","LAION-400M,LAION-COCO,RefinedWeb","""We use two image captioning datasets LAION-400M (Schuhmann et al.,
2021) and LAION-COCO (Schuhmann et al., 2022) for multi-modal alignment. As we full-finetune the language model backbone for long steps, we also jointly train with a text-only dataset RefinedWeb (Penedo et al., 2023) to avoid harming its text reasoning capability due to catastrophic forgetting.""",,""" For the joint training on both images and texts, we form each
batch with 640 image-text pairs from LAION-400M or LAION-COCO and 65, 536 text tokens from RefinedWeb""

As per Figure 7 there was 20000 training steps",290.0,"""The pre-training time is around 125 hours on 32 A100 GPUs with a 7B
language model and about twice the time with a 13B language model.""
"" The fine-tuning takes about 38 hours with 16 A100 GPUs with a 13B
language model.""",NVIDIA A100 SXM4 40 GB,,Likely,"We present SPHINX, a versatile multi-modal large language model (MLLM) with a joint mixing of model weights, tuning tasks, and visual embeddings. First, for stronger vision-language alignment, we unfreeze the large language model (LLM) during pre-training, and introduce a weight mix strategy between LLMs trained by real-world and synthetic data. By directly integrating the weights from two domains, the mixed LLM can efficiently incorporate diverse semantics with favorable robustness. Then, to enable multi-purpose capabilities, we mix a variety of tasks for joint visual instruction tuning, and design task-specific instructions to avoid inter-task conflict. In addition to the basic visual question answering, we include more challenging tasks such as region-level understanding, caption grounding, document layout detection, and human pose estimation, contributing to mutual enhancement over different scenarios. Additionally, we propose to extract comprehensive visual embeddings from various network architectures, pre-training paradigms, and information granularity, providing language models with more robust image representations. Based on our proposed joint mixing, SPHINX exhibits superior multi-modal understanding capabilities on a wide range of applications. On top of this, we further propose an efficient strategy aiming to better capture fine-grained appearances of high-resolution images. With a mixing of different scales and high-resolution sub-images, SPHINX attains exceptional visual parsing and reasoning performance on existing evaluation benchmarks. We hope our work may cast a light on the exploration of joint mixing in future MLLM research.",,,Open weights (restricted use),"China,Hong Kong,China,China",Llama 2-13B,4,"32 A100 * 312 TFLOPS/A100 * 290 hours * 40% utilization ~= 4e21 FLOP
https://www.wolframalpha.com/input?i=250+hours+*+312+TFLOPS+*+32+*+0.4",32.0,,2025-05-28 16:04,,,,,,"Academia,Academia,Academia",,,,9280.0,Open (restricted use),"https://github.com/Alpha-VLLM/LLaMA2-Accessory

looks like same as LLama license

finetune code: https://github.com/Alpha-VLLM/LLaMA2-Accessory/tree/main/SPHINX ","Academia,Academia,Academia",$239188.69,,BF16,25391.016357362598,Hardware,Alpha-VLLM,,,
Volcano 13B,"Language,Multimodal,Vision","Language modeling/generation,Visual question answering","Korea University,Korea Advanced Institute of Science and Technology (KAIST),LG","Seongyun Lee, Sue Hyun Park, Yongrae Jo, Minjoon Seo",2023-11-13,Volcano: Mitigating Multimodal Hallucination through Self-Feedback Guided Revision,https://arxiv.org/abs/2311.07362,28.0,SOTA improvement,"""Volcano effectively reduces multimodal hallucination and achieves state-of-the-art on MMHal-Bench, POPE, and GAVIE"" (hallucination benchmarks)",13000000000.0,13B,4.56e+22,"Base model is LLaVa-1.5 13B, which used 4.55e22 FLOP (mostly coming from Llama base)

""For this research, we used an NVIDIA A100-SXM4-80GB GPU and an AMD EPYC 7513 32-Core Processor running at 2.0778 GHz. Training
VOLCANO 7B required 8 GPUs and took a total of 15 hours, while training VOLCANO 13B took 30 hours.""
3.12e14 * 8 * 30 * 3600 * 0.3 = 8.1e19 finetune compute","LAION,SBU,ShareGPT4V,Unspecified unreleased","trained on synthetic data: ""To train VOLCANO, we collect initial responses for visual questions from an open-source LMM and
generate feedback and revisions using a proprietary
LLM as shown in Figure 3 (AkyÃ¼rek et al., 2023;
Madaan et al., 2023; Ye et al., 2023b; Wang et al.,
2023d; Kim et al., 2023).""

https://huggingface.co/datasets/kaist-ai/volcano-train",,"https://huggingface.co/datasets/kaist-ai/volcano-train

558k image-text pairs, rest of dataset is ~1M examples of text data; length per sequence is not clear",30.0,,NVIDIA A100 SXM4 80 GB,,Likely,"Large multimodal models (LMMs) suffer from multimodal hallucination, where they provide incorrect responses misaligned with the given visual information. Recent works have conjectured that one of the reasons behind multimodal hallucination might be due to the vision encoder failing to ground on the image properly. To mitigate this issue, we propose a novel approach that leverages self-feedback as visual cues. Building on this approach, we introduce Volcano, a multimodal self-feedback guided revision model. Volcano generates natural language feedback to its initial response based on the provided visual information and utilizes this feedback to self-revise its initial response. Volcano effectively reduces multimodal hallucination and achieves state-of-the-art on MMHal-Bench, POPE, and GAVIE. It also improves on general multimodal abilities and outperforms previous models on MM-Vet and MMBench. Through a qualitative analysis, we show that Volcano's feedback is properly grounded on the image than the initial response. This indicates that Volcano can provide itself with richer visual information, helping alleviate multimodal hallucination. We publicly release Volcano models of 7B and 13B sizes along with the data and code at this https URL.",1.0,,Open weights (non-commercial),"Korea (Republic of),Korea (Republic of),Korea (Republic of)",LLaVA 1.5,81000000000000000000,"""For this research, we used an NVIDIA A100- SXM4-80GB GPU and an AMD EPYC 7513 32-Core Processor running at 2.0778 GHz. Training VOLCANO 7B required 8 GPUs and took a total of 15 hours, while training VOLCANO 13B took 30 hours""

= 8 * 312 teraflops * 30 * 3600 * 0.4 utilization (assumed)
= 8.1e19
",,,2025-06-06 12:01,,,,128.0,,"Academia,Academia,Industry",,,,,Open (non-commercial),"dataset and weights are open, no license

https://github.com/kaistAI/Volcano

https://huggingface.co/kaist-ai/volcano-13b","Academia,Academia,Industry",,,FP16,,Hardware,kaist-ai,,,
LLaVA + LVIS-INSTRUCT4V,"Multimodal,Language,Vision","Language modeling/generation,Visual question answering","Fudan University,University of Maryland","Junke Wang, Lingchen Meng, Zejia Weng, Bo He, Zuxuan Wu, Yu-Gang Jiang",2023-11-13,To See is to Believe: Prompting GPT-4V for Better Visual Instruction Tuning,https://arxiv.org/abs/2311.07574,66.0,,"""Notably, by simply replacing the LLaVA-Instruct with our LVISINSTRUCT4V, we achieve better results than LLaVA on most challenging LMM
benchmarks, e.g., LLaVAw (76.7 vs. 70.7) and MM-Vet (40.2 vs. 35.4).""",13000000000.0,,,,LVIS-Instruct4V,"""To address this challenge, we introduce a fine-grained visual instruction dataset, LVIS-INSTRUCT4V, which contains 220K visually aligned and context-aware instructions produced by prompting the powerful GPT-4V with images from LVIS.""",,"""To the end, we use 110K images from LVIS and generate 220K high-quality visual instructions"" provided examples are roughly 30-60 words each",,,,,Likely,"Existing visual instruction tuning methods typically prompt large language models with textual descriptions to generate instruction-following data. Despite the promising performance achieved, these descriptions are derived from image annotations, which are oftentimes coarse-grained. Furthermore, the instructions might even contradict the visual content without observing the entire visual context. To address this challenge, we introduce a fine-grained visual instruction dataset, LVIS-Instruct4V, which contains 220K visually aligned and context-aware instructions produced by prompting the powerful GPT-4V with images from LVIS. Through experimental validation and case studies, we demonstrate that high-quality visual instructional data could improve the performance of LLaVA-1.5, a state-of-the-art large multimodal model, across a wide spectrum of benchmarks by clear margins. Notably, by simply replacing the LLaVA-Instruct with our LVIS-Instruct4V, we achieve better results than LLaVA on most challenging LMM benchmarks, e.g., LLaVAw (76.7 vs. 70.7) and MM-Vet (40.2 vs. 35.4). We release our data and model at this https URL.",1.0,,Open weights (unrestricted),"China,United States of America",LLaVA 1.5,,,,,2024-11-01 10:05,,,,,,"Academia,Academia",,,,,,MIT for weights and data,"Academia,Academia",,,,,,,,,
Intel Aurora 1T,Language,"Language modeling,Language modeling/generation","Intel,Argonne National Laboratory",Intel,2023-11-13,"Intel SC23 Update: 1-Trillion Parameter AI Model Running on Aurora Supercomputer, Granite Rapids Benchmarks","https://www.tomshardware.com/news/intel-supercomputing-2023-aurora-xeon-max-gpu-gaudi-granite-rapids

https://newsroom.intel.com/artificial-intelligence/intel-supercomputing-2023-news",,,,1000000000000.0,1T,,,Unspecified unreleased,,,,,,,Supervised,Confident,"At Supercomputing 2023, Intel provided a slew of updates on its latest HPC and AI initiatives, including new information about its fifth-gen Emerald Rapids and future Granite Rapids Xeon CPUs, Guadi accelerators, new Max Series GPU benchmarks against Nvidia's H100 GPUs, and the company's work on the 'genAI' 1-trillion-parameter AI model that runs on the Aurora supercomputer.

The Aurora genAI project is a collaboration with Argonne, Intel and partners to create state-of-the-art foundational AI models for science. The models will be trained on scientific texts, code and science datasets at scales of more than 1 trillion parameters from diverse scientific domains. Using the foundational technologies of Megatron with DeepSpeed, the genAI project will service multiple scientific disciplines, including biology, cancer research, climate science, cosmology and materials science.",,,Unreleased,"United States of America,United States of America",,,,,,2025-05-09 11:32,,Aurora supercomputer ,,,,"Industry,Government",,,,,Unreleased,,"Industry,Government",,,,,,,,,
tts-1,Speech,Text-to-speech,OpenAI,,2023-11-09,Text-to-speech model optimized for speed,https://platform.openai.com/docs/models/tts-1,,,,,,,,Unspecified unreleased,,,,,,,,Unknown,"Text to Speech model from OpenAI

TTS is a model that converts text to natural sounding spoken text. The tts-1 model is optimized for realtime text-to-speech use cases. Use it with the Speech endpoint in the Audio API.",,,API access,United States of America,,,,,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
tts-1-hd,Speech,Text-to-speech,OpenAI,,2023-11-09,Text-to-speech model optimized for quality,https://platform.openai.com/docs/models/tts-1-hd,,,,,,,,Unspecified unreleased,,,,,,,,Unknown,"Text to speech model from OpenAI

TTS is a model that converts text to natural sounding spoken text. The tts-1-hd model is optimized for high quality text-to-speech use cases. Use it with the Speech endpoint in the Audio API.",,,API access,United States of America,,,,,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
MultiBand Diffusion,"Audio,Speech",Audio generation,"Meta AI,Hebrew University of Jerusalem,LORIA","Robin San Roman, Yossi Adi, Antoine Deleforge, Romain Serizel, Gabriel Synnaeve, Alexandre DÃ©fossez",2023-11-08,From Discrete Tokens to High-Fidelity Audio Using Multi-Band Diffusion,https://arxiv.org/abs/2308.02560,13.0,SOTA improvement,"""At equal bit rate, the proposed approach outperforms state-of-the-art generative techniques in terms of perceptual quality""",,,2.6e+19,"""It takes around 2 days on 4 Nvidia V100 with 16 GB to train one of the 4 models.""

125 tflop/s for V100 SXM (not clear which they used, could be PCI given small number - still same OOM thus confident)
4 * 125 trillion * 2 * 24 * 3600 * 0.3 = 2.6e19","Common Voice,DNS,MTG-Jamendo,FSD50K,AudioSet","""We train on a diverse set of domains and data. We use speech from the train set of Common Voice 7.0 (9096 hours) [Ardila et al., 2019] together with the DNS challenge 4 (2425 hours) [Dubey et al., 2022]. For music, we use the MTG-Jamendo dataset (919h) [Bogdanov et al., 2019]. For the environmental
sound we use FSD50K (108 hours) [Fonseca et al., 2021] and AudioSet (4989 hours) [Gemmeke et al., 2017]. We used AudioSet only for the research that is described in the publication and for the benefit of replicability. For evaluation, we also use samples from an internal music dataset.""",,9096+2425+919+108+4989=17537 hours,48.0,around 2 days,NVIDIA V100,,Confident,"Deep generative models can generate high-fidelity audio conditioned on various types of representations (e.g., mel-spectrograms, Mel-frequency Cepstral Coefficients (MFCC)). Recently, such models have been used to synthesize audio waveforms conditioned on highly compressed representations. Although such methods produce impressive results, they are prone to generate audible artifacts when the conditioning is flawed or imperfect. An alternative modeling approach is to use diffusion models. However, these have mainly been used as speech vocoders (i.e., conditioned on mel-spectrograms) or generating relatively low sampling rate signals. In this work, we propose a high-fidelity multi-band diffusion-based framework that generates any type of audio modality (e.g., speech, music, environmental sounds) from low-bitrate discrete representations. At equal bit rate, the proposed approach outperforms state-of-the-art generative techniques in terms of perceptual quality. Training and, evaluation code, along with audio samples, are available on the facebookresearch/audiocraft Github page.",,,Open weights (unrestricted),"United States of America,Israel,France",,,,,,2025-02-14 15:53,,,,,,"Industry,Academia,Academia",,,,,Open source,"training, inference, and models (MIT)
https://github.com/facebookresearch/audiocraft/blob/main/docs/MBD.md ","Industry,Academia,Academia",$22.81,,,,Hardware,,,,
Samsung Gauss Language,Language,"Language modeling/generation,Question answering,Text summarization,Translation",Samsung,,2023-11-08,,https://techcrunch.com/2023/11/08/samsung-unveils-chatgpt-alternative-samsung-gauss-that-can-generate-text-code-and-images/,,,"To be deployed in Samsung devices: ""Samsung plans to incorporate the large language model into its devices like phones, laptops and tablets""",,,,,,,,,,,,,Unknown,"Just a few days after OpenAIâ€™s developer event, Samsung unveiled its own generative AI model, Samsung Gauss, at the Samsung AI Forum 2023.

Samsung Gauss, developed by the tech giantâ€™s research unit Samsung Research, consists of three tools: Samsung Gauss Language, Samsung Gauss Code and Samsung Gauss Image.

Samsung Gauss Language is a large language model that can understand human language and answer questions like ChatGPT. It can be used to increase productivity in several ways. For instance, it can help you write and edit emails, summarize documents and translate languages. Samsung plans to incorporate the large language model into its devices like phones, laptops and tablets to make the companyâ€™s smart devices a bit smarter. When asked if it supports both English and Korean as interaction languages, a spokesperson of Samsung declined to comment on it.",,,,Korea (Republic of),,,,,,2024-09-05 14:08,,,,,,Industry,,,,,,,Industry,,,,,,,,,
Prithvi-100M,Earth science,"Cloud monitoring / analysis,Flood Mapping,Wildfire Mapping,Crop Mapping / Segmentation","IBM,NASA","Johannes Jakubik, Sujit Roy, C. E. Phillips, Paolo Fraccaro, Denys Godwin, Bianca Zadrozny, Daniela Szwarcman, Carlos Gomes, Gabby Nyirjesy, Blair Edwards, Daiki Kimura, Naomi Simumba, Linsong Chu, S. Karthik Mukkavilli, Devyani Lambhate, Kamal Das, Ranjini Bangalore, Dario Oliveira, Michal Muszynski, Kumar Ankur, Muthukumaran Ramasubramanian, Iksha Gurung, Sam Khallaghi, Hanxi (Steve)Li, Michael Cecil, Maryam Ahmadi, Fatemeh Kordi, Hamed Alemohammad, Manil Maskey, Raghu Ganti, Kommy Weldemariam, Rahul Ramachandran",2023-11-08,Foundation Models for Generalist Geospatial Artificial Intelligence,https://arxiv.org/abs/2310.18660,,,,100000000.0,,2.299133952e+21,"compute hardware: 
0.3*311.84 (peak TFLOPS)*384 (s per epoch)*64 (GPUs)*1000 (epochs) *(10^12) = 2299133952000000000000 = 2.299133952Ã—10^21",HLS / Harmonized Landsat and Sentinel-2,model pre-trained on more than 1TB of multispectral satellite imagery from the Harmonized Landsat-Sentinel 2 (HLS) dataset,,"

",,,NVIDIA A100,,Speculative,"Significant progress in the development of highly adaptable and reusable Artificial Intelligence (AI) models is expected to have a significant impact on Earth science and remote sensing. Foundation models are pre-trained on large unlabeled datasets through self-supervision, and then fine-tuned for various downstream tasks with small labeled datasets. This paper introduces a first-of-a-kind framework for the efficient pre-training and fine-tuning of foundational models on extensive geospatial data. We have utilized this framework to create Prithvi, a transformer-based geospatial foundational model pre-trained on more than 1TB of multispectral satellite imagery from the Harmonized Landsat-Sentinel 2 (HLS) dataset. Our study demonstrates the efficacy of our framework in successfully fine-tuning Prithvi to a range of Earth observation tasks that have not been tackled by previous work on foundation models involving multi-temporal cloud gap imputation, flood mapping, wildfire scar segmentation, and multi-temporal crop segmentation. Our experiments show that the pre-trained model accelerates the fine-tuning process compared to leveraging randomly initialized weights. In addition, pre-trained Prithvi compares well against the state-of-the-art, e.g., outperforming a conditional GAN model in multi-temporal cloud imputation by up to 5pp (or 5.7%) in the structural similarity index. Finally, due to the limited availability of labeled data in the field of Earth observation, we gradually reduce the quantity of available labeled data for refining the model to evaluate data efficiency and demonstrate that data can be decreased significantly without affecting the model's accuracy. The pre-trained 100 million parameter model and corresponding fine-tuning workflows have been released publicly as open source contributions to the global Earth sciences community through Hugging Face.",1000.0,,Open weights (unrestricted),"United States of America,United States of America",,,,64.0,,2025-05-22 12:21,IBM,,,1024.0,"We use AdamW optimizer with Î²1 = 0.9, Î²2 = 0.999, batch size of 1024, one-cycle cosine learning rate scheduler, with a maximum learning rate of 5e-4. We experimented with ViTbase and ViT-large backbones and trained the models for 1000 epochs. ","Industry,Government",,,,,Unreleased,https://huggingface.co/ibm-nasa-geospatial/Prithvi-100M,"Industry,Government",,,,50787.68744924102,Hardware,ibm-nasa-geospatial,,,
HGRN 1B (WT 103),Language,"Language modeling/generation,Question answering","Shanghai AI Lab,Massachusetts Institute of Technology (MIT)","Zhen Qin, Songlin Yang, Yiran Zhong",2023-11-08,Hierarchically Gated Recurrent Neural Network for Sequence Modeling,https://arxiv.org/abs/2311.04823,,,,1000000000.0,1B (table 4 and hugging  face repo),1.96608e+19,6ND = 6* 3 276 800 000 *10^9 = 1.96608e+19,"WikiText-103,The Pile",,3276800000.0,"Sequence length 512 
Total batch size 128
50k updates

512*128*50000 = 3 276 800 000",,,NVIDIA A100,,Confident,"Transformers have surpassed RNNs in popularity due to their superior abilities in parallel training and long-term dependency modeling. Recently, there has been a renewed interest in using linear RNNs for efficient sequence modeling. These linear RNNs often employ gating mechanisms in the output of the linear recurrence layer while ignoring the significance of using forget gates within the recurrence. In this paper, we propose a gated linear RNN model dubbed Hierarchically Gated Recurrent Neural Network (HGRN), which includes forget gates that are lower bounded by a learnable value. The lower bound increases monotonically when moving up layers. This allows the upper layers to model long-term dependencies and the lower layers to model more local, short-term dependencies. Experiments on language modeling, image classification, and long-range arena benchmarks showcase the efficiency and effectiveness of our proposed model. The source code is available at this https URL.",,,Open weights (unrestricted),"China,United States of America",,,,8.0,,2025-06-05 07:04,,,,,,"Academia,Academia",,,,,,"apache 2.0
https://huggingface.co/OpenNLPLab/HGRN-1B","Academia,Academia",,,,6348.460931155128,Operation counting,OpenNLPLab,,,
OmniVec,"Multimodal,Vision,Speech,Language,Video,3D modeling","Image classification,Speech recognition",TensorTour,"Siddharth Srivastava, Gaurav Sharma",2023-11-07,OmniVec: Learning robust representations with cross modal sharing,https://arxiv.org/abs/2311.05709v1,38.0,SOTA improvement,"Table 13.

E.g. SOTA on ImageNet at 92.4 top-1 accuracy",,,,,"AudioSet,Something-Something v2 (SSv2),English Wikipedia,ImageNet-1k,SUN RGB-D,ModelNet40","Many datasets across several modalities

""We use AudioSet (audio) [23], Something-Something v2 (SSv2)(video) [31], English Wikipedia (text), ImageNet1K (image) [17], SUN RGB-D (depth maps) [71], ModelNet40 (3D point cloud) [93] for pretraining the network.""",,,,,,,Unknown,"Majority of research in learning based methods has been towards designing and training networks for specific tasks. However, many of the learning based tasks, across modalities, share commonalities and could be potentially tackled in a joint framework. We present an approach in such direction, to learn multiple tasks, in multiple modalities, with a unified architecture. The proposed network is composed of task specific encoders, a common trunk in the middle, followed by task specific prediction heads. We first pre-train it by self-supervised masked training, followed by sequential training for the different tasks. We train the network on all major modalities, e.g.\ visual, audio, text and 3D, and report results on 22 diverse and challenging public benchmarks. We demonstrate empirically that, using a joint network to train across modalities leads to meaningful information sharing and this allows us to achieve state-of-the-art results on most of the benchmarks. We also show generalization of the trained network on cross-modal tasks as well as unseen datasets and tasks.",2000.0,,Unreleased,United States of America,BERT-Large,,"Appears to build on several models, like BERT and ViT (Table 1)",,,2025-06-02 13:45,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
mPLUG-Owl2,"Vision,Language,Multimodal","Visual question answering,Image captioning,Language modeling/generation",Alibaba,"Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu, Haowei Liu, Qi Qian, Ji Zhang, Fei Huang, Jingren Zhou",2023-11-07,mPLUG-Owl2: Revolutionizing Multi-modal Large Language Model with Modality Collaboration,https://arxiv.org/abs/2311.04257,,SOTA improvement,"""Extensive experiments illustrate the effectiveness and generalization abilities of mPLUG-Owl2, which achieves state-of-the-art performance on 8 classic vision-language benchmarks using a single generic model.""",7120000000.0,"""As depicted in Figure 2, our model, referred to as mPLUGOwl2, is composed of three main components: a fundamental vision encoder, a visual abstractor, and a language decoder. Specifically, we utilize ViT-L/14 as the
vision encoder and LLaMA-2-7B [58] as the language decoder""
ViT-L/14 has 123M parameters and Llama 2 7B has 7B parameters.",,"ViT-L/14 and Llama 2-7b compute, plus 1.7e19 joint pretrain FLOP (6 * 400M * 7.1B) and 4e16 joint finetune FLOP. Everything is a negligible fraction except the Llama 2 compute.","Conceptual Captions (CC3M),Conceptual Captions 12M (CC12M),COCO,LAION,COYO-700M","""mPLUG-Owl2 is first pre-trained on image-text pairs and fine-tunes on mono-modal and multi-modal instruction data. For pre-training data, we randomly pick about 400 million image-text pairs from five public datasets: Conceptual Captions (CC3M/CC12M) [9], COCO [35], Laion-en [49], COYO [7], DataComp [18]. For instruction data, we collect 5 types of datasets including 1) image captioning (i.e., TextCaps [53], COCO [35]); 2) image question answering (i.e., VQAv2 [21], OKVQA [43], OCR-VQA [44], GQA [24], and A-OKVQA [50]); 3) region-aware QA (i.e., Ref-COCO [69], VisualGenome [26]); 4) multi-modal instruct data (i.e., LLaVA-instruct-150K [38]); 5) text-only instruct data (i.e., ShareGPT-80K [1], SlimOrca [34]). Details can be found in the Appendix.""
According to the appendix, the instruction-tuning dataset was 1.23MB total across text, dialog, captions, and visual question-answering. This can't be much more than 1.5M updates per epoch, and the paper says ""For the instruction tuning stage, we train the whole model for 1 epoch with a learning rate of 2e-5 and batch size 256"".",400672000.0,"400 million image-text pairs at pre-training + 672k image-text pairs at instruction tuning (table 14) = 400672000 images

+ 558k text instructions (table 14) ",,,,,Speculative,"Multi-modal Large Language Models (MLLMs) have demonstrated impressive instruction abilities across various open-ended tasks. However, previous methods primarily focus on enhancing multi-modal capabilities. In this work, we introduce a versatile multi-modal large language model, mPLUG-Owl2, which effectively leverages modality collaboration to improve performance in both text and multi-modal tasks. mPLUG-Owl2 utilizes a modularized network design, with the language decoder acting as a universal interface for managing different modalities. Specifically, mPLUG-Owl2 incorporates shared functional modules to facilitate modality collaboration and introduces a modality-adaptive module that preserves modality-specific features. Extensive experiments reveal that mPLUG-Owl2 is capable of generalizing both text tasks and multi-modal tasks and achieving state-of-the-art performances with a single generic model. Notably, mPLUG-Owl2 is the first MLLM model that demonstrates the modality collaboration phenomenon in both pure-text and multi-modal scenarios, setting a pioneering path in the development of future multi-modal foundation models.
",1.0,,Open weights (unrestricted),China,Llama 2-7B,17000000001000000000,https://www.wolframalpha.com/input?i=6+*+400+million+*+7.12+billion,,,2025-06-04 18:02,,,,,,Industry,,,,,Open source,"Apache 2
https://github.com/X-PLUG/mPLUG-Owl/tree/main/mPLUG-Owl2

https://huggingface.co/Mizukiluke/mplug_owl_2_1",Industry,,,BF16,,,Mizukiluke,,,
OtterHD-8B,"Multimodal,Vision,Language","Chat,Visual question answering",Nanyang Technological University,"Bo Li, Peiyuan Zhang, Jingkang Yang, Yuanhan Zhang, Fanyi Pu, Ziwei Liu",2023-11-07,OtterHD: A High-Resolution Multi-modality Model,https://arxiv.org/abs/2311.04219,49.0,,"from page 2: ""We present OtterHD-8B, a novel model based on the Fuyu-8B architecture, optimized for varying input resolutions. Our empirical evaluations suggest that the model exhibits state-of-the-art performance across multiple tasks when instruction-tuned with higher resolutions.""",8000000000.0,8B,,,"LLaVA-Instruct-150k,VQAv2,OKVQA,TextVQA,OCR-VQA,ImageNet","""We compiled a total of 370K instruction/response pairs sourced from the following public datasets: LLaVA-Instruct [ 30], VQAv2 [2], GQA [ 23 ], OKVQA [ 36 ], OCRVQA [38 ], A-OKVQA [ 45], COCO-GOI [33 ], COCO-Caption [ 10], TextQA [ 48], RefCOCO [58], COCO-ITM [ 28 ], ImageNet [17 ], and LLaVA-RLHF [ 51 ].""",,"""We compiled a total of 370K instruction/response pairs sourced from the follow-
ing public datasets: LLaVA-Instruct [ 30], VQAv2 [2], GQA [ 23 ], OKVQA [ 36 ], OCRVQA [38 ],
A-OKVQA [ 45], COCO-GOI [33 ], COCO-Caption [ 10], TextQA [ 48], RefCOCO [58], COCO-ITM [ 28 ], ImageNet [17 ], and LLaVA-RLHF [ 51 ].""",9.0,3 hours per epoch over 3 epochs: 9 hours,NVIDIA A100,,Confident,"In this paper, we present OtterHD-8B, an innovative multimodal model evolved from Fuyu-8B, specifically engineered to interpret high-resolution visual inputs with granular precision. Unlike conventional models that are constrained by fixed-size vision encoders, OtterHD-8B boasts the ability to handle flexible input dimensions, ensuring its versatility across various inference requirements. Alongside this model, we introduce MagnifierBench, an evaluation framework designed to scrutinize models' ability to discern minute details and spatial relationships of small objects. Our comparative analysis reveals that while current leading models falter on this benchmark, OtterHD-8B, particularly when directly processing high-resolution inputs, outperforms its counterparts by a substantial margin. The findings illuminate the structural variances in visual information processing among different models and the influence that the vision encoders' pre-training resolution disparities have on model effectiveness within such benchmarks. Our study highlights the critical role of flexibility and high-resolution input capabilities in large multimodal models and also exemplifies the potential inherent in the Fuyu architecture's simplicity for handling complex visual data. ",3.0,,,Singapore,Fuyu-8B,24000000000000000000,"flops = (8) * (312 * 10**12) * (3 * 3 * 3600) * (0.3) = 2.4e19
(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)

'Our implementation permits the completion of full-parameter training within 3 hours per epoch on 8Ã—A100 GPUs. '
Table 4 indicates 3 epochs for the full finetune.",8.0,,2024-11-01 10:05,,,,,,Academia,,,,24.0,,,Academia,,,,6348.602308962822,,,,,
XVERSE-13B-2,Language,"Language generation,Language modeling/generation,Question answering,Text summarization,Translation","XVERSE Technology,Shenzhen Yuanxiang Technology",,2023-11-06,,https://huggingface.co/xverse/XVERSE-13B,,,,13000000000.0,13B,,"Not enough info, eg number of epochs",,"""The model has been thoroughly trained on a diversified and high-quality dataset consisting of 3.2 trillion of tokens, including more than 40 languages such as Chinese, English, Russian, and Spanish. The sampling ratio of different types of data is finely set, which makes the performance of Chinese and English excellent, and also takes into account the effect of other languages.""",3200000000000.0,"Multilingual, 3.2 trillion tokens. Note that model was originally stated to have been trained on 1.4T tokens, so while the wording suggests a dataset of 3.2T unique tokens, it may actually be referencing the number of tokens seen by the model (i.e. possibly over multiple epochs).",,,,,Likely,"XVERSE-13B is a multilingual large language model, independently developed by Shenzhen Yuanxiang Technology. Its key features are as follows:

Model Structure: XVERSE-13B uses the mainstream Decoder-only Transformer network structure, supports 8k context length, the longest one among models of the same size, which can meet the need of longer multi-round dialogues, knowledge question-answering, and summarization. This makes the model more versatile in application scenarios.
Training Data: The model has been thoroughly trained on a diversified and high-quality dataset consisting of 3.2 trillion of tokens, including more than 40 languages such as Chinese, English, Russian, and Spanish. The sampling ratio of different types of data is finely set, which makes the performance of Chinese and English excellent, and also takes into account the effect of other languages.
Tokenization: Based on the BPE (Byte-Pair Encoding) algorithm, a tokenizer with a vocabulary size of 100,534 has been trained using hundreds of gigabytes of language data. This tokenizer is capable of supporting multilingual without the need for additional vocabulary expansion.
Training Framework: Several key technologies have also been independently developed, including efficient operators, memory optimization, parallel scheduling strategies, overlap of data-computation-communication, and synergy between platforms and frameworks. These advancements enhance training efficiency and model stability. With these technologies, the peak computational power utilization rate on a thousand-card cluster can reach 58.5%, ranking at the forefront of the industry.",,,Open weights (restricted use),"China,China",,,,,,2024-09-05 14:08,,,,,,"Industry,Industry",,,,,Open source,must apply for commercial license,"Industry,Industry",,,,,,,,,
Whisper v3,Speech,Speech recognition,OpenAI,,2023-11-06,,https://huggingface.co/openai/whisper-large-v3,,,"seems not SOTA: ""Our studies show that... accuracy on speech recognition and translation is near the state-of-the-art level""",1550000000.0,,2.7e+23,"Could derive this in terms of Whisper v1, which according to the paper was trained for 680k hours for between 2-3 epochs. Whisper v3 was trained on 5 million hours for 2 epochs, or ~5-7x as much data, and has the same architecture. We have an estimate of 4.65e22 for Whisper 1.

Assume Whisper v1 was trained on 2.5 epochs, or 2.5*680k = 1.7M hours. Whisper v3 was trained on 10M hours. 10/1.7 * 4.65e22 ~= 2.7e23",Unspecified unreleased,"""The Whisper large-v3 model is trained on 1 million hours of weakly labeled audio and 4 million hours of pseudolabeled audio collected using Whisper large-v2""",60000000000.0,"English audio is roughly 228 wpm: https://docs.google.com/document/d/1G3vvQkn4x_W71MKg0GmHVtzfd9m0y3_Ofcoew0v902Q/edit#heading=h.sxcem9l5k3ce

The dataset is multilingual and other languages seem to have lower wpms. So using 200 wpm, we have

200*60*5 million hours = 60,000,000,000 (60B) words",,,,Supervised,Likely,"Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labelled data, Whisper models demonstrate a strong ability to generalise to many datasets and domains without the need for fine-tuning.

Whisper was proposed in the paper Robust Speech Recognition via Large-Scale Weak Supervision by Alec Radford et al. from OpenAI. The original code repository can be found here.",2.0,,Open weights (unrestricted),United States of America,,,,,,2025-05-16 10:30,,,,,,Industry,,,,,Unreleased,"Apache 2.0: https://huggingface.co/openai/whisper-large-v3 

this seems to be inference code not training: https://github.com/openai/whisper ",Industry,,,,,Comparison with other models,,,,
Consistency Decoder,Image generation,Image generation,OpenAI,,2023-11-06,,https://github.com/openai/consistencydecoder,,,,,"The architecture is described in the DALL-E 3 paper, which is linked to from the GitHub page.

""For DALL-E 3, we trained our own diffusion decoder on top of the latent space learned by the VAE trained
by Rombach et al. (2022). We found that using a diffusion decoder here provided marked improvements to
fine image details, for example text or human faces.
This diffusion decoder is a convolutional U-Net identical to the one described in Ho et al. (2020). Once
trained, we used the consistency distillation process described in Song et al. (2023) to bring it down to two
denoising steps.""

The models in Ho et al are 35.7 million to 256 million. Not sure what the parameter count for this one is.  ",,,,,,,,,,,Unknown,"We are also open sourcing the Consistency Decoder, a drop in replacement for the Stable Diffusion VAE decoder. This decoder improves all images compatible with the by Stable Diffusion 1.0+ VAE, with significant improvements in text, faces and straight lines.

https://openai.com/blog/new-models-and-developer-products-announced-at-devday",,,Open weights (unrestricted),United States of America,Denoising Diffusion Probabilistic Models (LSUN Bedroom),,"This model is derived from the denoising model as described below:

""For DALL-E 3, we trained our own diffusion decoder on top of the latent space learned by the VAE trained
by Rombach et al. (2022). We found that using a diffusion decoder here provided marked improvements to
fine image details, for example text or human faces.

This diffusion decoder is a convolutional U-Net identical to the one described in Ho et al. (2020). Once
trained, we used the consistency distillation process described in Song et al. (2023) to bring it down to two
denoising steps.""

Song et al is here: https://arxiv.org/pdf/2303.01469.pdf That paper just describes the general process, not specifically for this model I believe.",,,2024-09-05 14:08,,,,,,Industry,,,,,Unreleased,MIT,Industry,,,,,,,,,
GPT-4 Turbo,"Multimodal,Vision,Language,Image generation","Chat,Language modeling/generation,Image generation,Speech synthesis,Table tasks,Visual question answering,Image captioning",OpenAI,,2023-11-06,New models and developer products announced at DevDay,https://openai.com/blog/new-models-and-developer-products-announced-at-devday,,SOTA improvement,"""More capable"" than GPT-4 according to OpenAI, with larger context window",,Not known. Maybe smaller/sparser than GPT-4.,2.2e+25,Estimated using benchmark imputation,Unspecified unreleased,"This model was announced in November 2023, so I assume that this is the preview model, which has a knowledge cutoff date of April 2023 - which I assume means April 1, 2023 - according to https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models?tabs=global-standard%2Cstandard-chat-completions#gpt-35. ",,,,,,,Unknown,"Today, we shared dozens of new additions and improvements, and reduced pricing across many parts of our platform. These include:

New GPT-4 Turbo model that is more capable, cheaper and supports a 128K context window",,,API access,United States of America,,,,,,2025-05-12 18:58,,,,,,Industry,checked,,,,Unreleased,,Industry,,,,,Benchmarks,,,,1
CogVLM-17B,"Multimodal,Vision,Language","Image captioning,Visual question answering,Chat","Tsinghua University,Zhipu AI,Beihang University","Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, Jiazheng Xu, Bin Xu, Juanzi Li, Yuxiao Dong, Ming Ding, Jie Tang",2023-11-06,CogVLM: Visual Expert for Pretrained Language Models,"https://arxiv.org/abs/2311.03079
https://huggingface.co/THUDM/cogvlm-chat-hf
https://github.com/THUDM/CogVLM
",297.0,SOTA improvement,"""CogVLM-17B
achieves state-of-the-art performance on 17 clas-
sic cross-modal benchmarks, including 1) im-
age captioning datasets: NoCaps, Flicker30k, 2)
VQA datasets: OKVQA, TextVQA, OCRVQA,
ScienceQA, 3) LVLM benchmarks: MM-
Vet, MMBench, SEED-Bench, LLaVABench,
POPE, MMMU, MathVista, 4) visual grounding
datasets: RefCOCO, RefCOCO+, RefCOCOg,
Visual7W. Codes and checkpoints are available at
https://github.com/THUDM/CogVLM""",17000000000.0,"CogVLM-17B has 10 billion vision parameters and 7 billion language parameters. However, ""the total number of trainable parameters is 6.5B"".

""CogVLM model comprises four fundamental components: a vision transformer (ViT) encoder, an MLP adapter, a pretrained large language model (GPT), and a visual expert module.""

ViT: EVA2-CLIP-E, last layer removed (5B params with last layer, non-trainable)
MLP adapter: 2 layers, parameter count unavailable
GPT: Vicuna1.5-7B (7B params)
Visual expert module: parameter count unclear",6.331000000000001e+22,"from table 8 on page 17

230.1 FLOPS*days 
so 
10**15*24*3600*230.1= 1.988e22

Since this training uses pretrained weights from EVA02-CLIP-E and Vicuna1.5-7B, we report the full number of FLOPs baked into the model.

EVA02-CLIP-g/14 is stated to have taken 25 days to train 12B samples using 64 A100-40GB GPUs, implying: 
25 days * 24 hr/day * 3600 sec/hr * 64 GPU * 7.80E+13 FLOP/GPU-sec * 30% efficiency = 3.23e21

EVA02-CLIP-E doesn't give a training time; it saw 1/4 as many samples as the g/14 model but has 4.27x more parameters; as a rough estimate, assume it took the same number of FLOPs to train.

Vicuna1.5-7B's training compute is ~entirely embedded in the base Llama-7b weights, which took 4.02e+22 FLOPs

Total: 1.988e22 + 3.23e21 + 4.02e22 = 6.331E22","VQAv2,LAION-2B,COYO-700M,OKVQA,TextVQA,OCR-VQA,ScienceQA,LLaVA-Instruct-150k,LRV-Instruction,LLaVAR,Flickr30K Entities,RefCOCO,Visual7W,VisualGenome,COCO,TextCaps","Pretraining uses LAION-2B, COYO-700M, plus a newly created visual grounding dataset of 40M images.
Generalist models CogVLM-Chat and CogVLM-Grounding are additionally finetuned on VQAv2, OKVQA, TextVQA, OCRVQA, ScienceQA, LLaVA-Instruct, LRV-Instruction, LLaVAR, Flickr30K Entities, RefCOCO, Visual7W, and VisualGenome.
Additional tests finetune on the training sets from COCO and TextCaps.",1518534581.0,"After filtering, about 1.5B image-text pairs are left for pretraining in stage one. Stage two of pretraining adds a visual grounding dataset of 40M images with generated noun bounding boxes. These are filtered from LAION-115M so that 75% of images contain at least two bounding boxes.

Two different kinds of finetuning are done, each using a number of datasets:
- CogVLM-Chat: VQAv2 (11059040), OKVQA (70275), TextVQA (453360), OCRVQA (1002146), ScienceQA (21208), LLaVAInstruct (150000), LRV-Instruction (300000), LLaVAR (1633000)
- CogVLM-Grounding: Flickr30K Entities (520000), RefCOCO (142209), Visual7W (889388), VisualGenome (1700000)

Additional experiments finetune using the training sets from COCO (413915 in train) and TextCaps (109765 in train)

In sum, pretraining and finetuning appear to contain 1,500,000,000 and 18,534,581 datapoints, respectively.",,,,Supervised,Confident,"We introduce CogVLM, a powerful open-source visual language foundation model. Different from the popular shallow alignment method which maps image features into the input space of language model, CogVLM bridges the gap between the frozen pretrained language model and image encoder by a trainable visual expert module in the attention and FFN layers. As a result, CogVLM enables deep fusion of vision language features without sacrificing any performance on NLP tasks. CogVLM-17B achieves state-of-the-art performance on 10 classic cross-modal benchmarks, including NoCaps, Flicker30k captioning, RefCOCO, RefCOCO+, RefCOCOg, Visual7W, GQA, ScienceQA, VizWiz VQA and TDIUC, and ranks the 2nd on VQAv2, OKVQA, TextVQA, COCO captioning, etc., surpassing or matching PaLI-X 55B. Codes and checkpoints are available at this https URL. ",,,Open weights (restricted use),"China,China,China",Vicuna-7B v0,2,Trained from Vicuna1.5-7B weights,,,2025-05-28 16:04,,,,,"8192 in pretraining stage 1, 1024 in stage 2","Academia,Industry,Academia",checked,,,,Unreleased,"code is Apache, model more restrictive, commercial allowed, subject to PRC laws and interests

code isn't training code","Academia,Industry,Academia",,,BF16,,Reported,,,,
LLaVA 1.5,"Multimodal,Language,Vision","Chat,Question answering,Visual question answering","University of Wisconsin Madison,Microsoft Research","Haotian Liu, Chunyuan Li, Yuheng Li, Yong Jae Lee",2023-11-05,Improved Baselines with Visual Instruction Tuning,"https://arxiv.org/abs/2310.03744,
",1430.0,SOTA improvement,"from abstract: ""we establish stronger baselines that achieve state-of-the-art across 11 benchmark""",13000000000.0,"from abstract ""Our final 13B checkpoint uses merely 1.2M publicly available data, and finishes full training in ~1 day on a single 8-A100 node. """,7.807e+22,"""Due to the increased image input resolution to 336^2, the training of LLaVA-1.5 is âˆ¼2Ã— as long as LLaVA: âˆ¼6 hours of pretraining and âˆ¼20 hours of visual instruction tuning using 8Ã— A100s.""
26 * 3600 * 8 * 3.12e14 * 0.3 = 7.0e19

Fine-tuned from Vicuna-13B (itself finetuned from LLaMa-13B), which used 7.8e22 FLOPs

7.0e19 + 7.8e22",Unspecified unreleased,from https://huggingface.co/liuhaotian/llava-v1.5-13b#training-dataset,1200000.0,1.2M text-image pairs from https://huggingface.co/liuhaotian/llava-v1.5-13b#training-dataset,24.0,"from abstract ""Our final 13B checkpoint uses merely 1.2M publicly available data, and finishes full training in ~1 day on a single 8-A100 node. """,NVIDIA A100,,Confident,"Large multimodal models (LMM) have recently shown encouraging progress with visual instruction tuning. In this note, we show that the fully-connected vision-language cross-modal connector in LLaVA is surprisingly powerful and data-efficient. With simple modifications to LLaVA, namely, using CLIP-ViT-L-336px with an MLP projection and adding academic-task-oriented VQA data with simple response formatting prompts, we establish stronger baselines that achieve state-of-the-art across 11 benchmarks. Our final 13B checkpoint uses merely 1.2M publicly available data, and finishes full training in ~1 day on a single 8-A100 node. We hope this can make state-of-the-art LMM research more accessible. Code and model will be publicly available. ",1.0,,Open weights (restricted use),"United States of America,United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational",Vicuna-13B v0,70087680000000000000,"""Due to the increased image input resolution to 336^2, the training of LLaVA-1.5 is âˆ¼2Ã— as long as LLaVA: âˆ¼6 hours of pretraining and âˆ¼20 hours of visual instruction tuning using 8Ã— A100s.""

26 * 3600 * 8 * 3.12e14 * 0.3 = 7.0e19",8.0,,2025-06-06 11:59,,,,,,"Academia,Industry",,,,192.0,Open source,"Llama 2 license for weights

https://huggingface.co/liuhaotian/llava-v1.5-13b

Apache 2.0 license for code:
https://github.com/haotian-liu/LLaVA","Academia,Industry",,,BF16,6348.88507402357,Hardware,liuhaotian,,,
Grok-1,Language,"Language modeling,Chat",xAI,,2023-11-04,Announcing Grok,"https://x.ai/model-card/, https://x.ai/blog/grok-os",,Training cost,"""On these benchmarks, Grok-1 displayed strong results, surpassing all other models in its compute class, including ChatGPT-3.5 and Inflection-1""",314000000000.0,"""314B parameter Mixture-of-Experts model with 25% of the weights active on a given token"". So effectively 78B parameters

Mixture of 8 experts: https://github.com/xai-org/grok-1",2.90000000001e+24,"""On these benchmarks, Grok-1 displayed strong results, surpassing all other models in its compute class, including ChatGPT-3.5 and Inflection-1. It is only surpassed by models that were trained with a significantly larger amount of training data and compute resources like GPT-4""

Per table, Grok-1 is surpassed by Palm 2, Claude 2, GPT-4, so it required less compute than these three models. Palm 2 was trained on 7e24 FLOP.

GPT-3.5 is ~2.6e24. Inflection-1's compute is not public/known by us but Inflection says Inflection-1 compute was <= Palm-540B's (which was ~2.5e24). 

For optimal training, our current working hypothesis is that you still need something like Chinchilla scaling on the total number of parameters in the model, even for MoE models, so optimal dataset size would be 20*310B tokens. With 25%*314B params active per forward pass, this would be around 3e24 FLOP.
https://www.wolframalpha.com/input?i=20*310+billion+*+6+*+25%25+*+314+billion",Unspecified unreleased,"""Base model trained on a large amount of text data, not fine-tuned for any particular task.""

""The training data used for the release version of Grok-1 comes from both the Internet up to Q3 2023 and the data provided by our AI Tutors.""

Knowledge cutoff date is October 2023, according to https://www.promptingguide.ai/models/grok-1.",6200000000000.0,"(Speculative confidence, see compute notes)",,,,,Likely,"Grok is an AI modeled after the Hitchhikerâ€™s Guide to the Galaxy, so intended to answer almost anything and, far harder, even suggest what questions to ask!

Grok is designed to answer questions with a bit of wit and has a rebellious streak, so please donâ€™t use it if you hate humor!

A unique and fundamental advantage of Grok is that it has real-time knowledge of the world via the ð• platform. It will also answer spicy questions that are rejected by most other AI systems.

Grok is still a very early beta product â€“ the best we could do with 2 months of training â€“ so expect it to improve rapidly with each passing week with your help.",,,Open weights (unrestricted),United States of America,,,,,,2025-05-13 01:41,,,,,,Industry,checked,2.0000000000100005e+24,7.00000000001e+24,,Unreleased,apache 2.0,Industry,,,,,Benchmarks,,,,6
Grok-0,Language,"Chat,Language modeling/generation",xAI,,2023-11-04,Announcing Grok,https://x.ai/,,,,33000000000.0,33 billion,,"Half of Llama 2-70B? (which we estimated at 8e23) ""This early model approaches LLaMA 2 (70B) capabilities on standard LM benchmarks but uses only half of its training resources""",Unspecified unreleased,,,no information about the dataset found except for this speculation https://docs.google.com/spreadsheets/d/1O5KVQW1Hx5ZAkcg8AIRjbQLQzx2wVaLl0SqUu-ir9Fs/edit?gid=1158069878#gid=1158069878,,,,,Likely,"""The engine powering Grok is Grok-1, our frontier LLM, which we developed over the last four months. Grok-1 has gone through many iterations over this span of time.

After announcing xAI, we trained a prototype LLM (Grok-0) with 33 billion parameters. This early model approaches LLaMA 2 (70B) capabilities on standard LM benchmarks but uses only half of its training resources.""",,,Hosted access (no API),United States of America,,,,,,2025-03-24 14:55,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
BLUUMI,Language,Language modeling,"University of Turku,Hugging Face","Risto Luukkonen, Ville Komulainen, Jouni Luoma, Anni Eskelinen, Jenna Kanerva, Hanna-Mari Kupari, Filip Ginter, Veronika Laippala, Niklas Muennighoff, Aleksandra Piktus, Thomas Wang, Nouamane Tazi, Teven Le Scao, Thomas Wolf, Osma Suominen, Samuli Sairanen, Mikko Merioksa, Jyrki Heinonen, Aija Vahtola, Samuel Antao, Sampo Pyysalo",2023-11-03,FinGPT: Large Generative Models for a Small Language,https://arxiv.org/abs/2311.05640,28.0,SOTA improvement,"SOTA for Finnish: ""Our best monolingual model outperforms this result by over 10% points and the BLUUMI model by over 20% points, representing a substantial advance in the state of the art in the capability of generative models trained for Finnish.""",176000000000.0,176 billion,,,"Parsebank,mC4,Common Crawl,Wikipedia",Finnish data from several sources,38000000000.0,"38B tokens
""In total, the final pretraining dataset (including oversampling) consists of 38 billion tokens when processed with our Finnish tokenizer.""",,,AMD Radeon Instinct MI250X,,Likely,"Large language models (LLMs) excel in many tasks in NLP and beyond, but most open models have very limited coverage of smaller languages and LLM work tends to focus on languages where nearly unlimited data is available for pretraining. In this work, we study the challenges of creating LLMs for Finnish, a language spoken by less than 0.1% of the world population. We compile an extensive dataset of Finnish combining web crawls, news, social media and eBooks. We pursue two approaches to pretrain models: 1) we train seven monolingual models from scratch (186M to 13B parameters) dubbed FinGPT, 2) we continue the pretraining of the multilingual BLOOM model on a mix of its original training data and Finnish, resulting in a 176 billion parameter model we call BLUUMI. For model evaluation, we introduce FIN-bench, a version of BIG-bench with Finnish tasks. We also assess other model qualities such as toxicity and bias. Our models and tools are openly available at this https URL.",8.0,,Open weights (restricted use),"Finland,Multinational,United States of America",BLOOM-176B,,"They ""continued pretraining"" of BLOOM on Finnish data. Don't think they specify the number of tokens they trained BLOOM/BLUUMI on; for their smaller models it was 300b.",,,2025-06-06 11:29,,LUMI Supercomputer,,4194304.0,Table 5.,"Academia,Industry",checked,,,,Unreleased,"The BigScience RAIL License
https://turkunlp.org/gpt3-finnish
https://huggingface.co/TurkuNLP/bloom-finnish-176b

""The Responsible AI License allows users to take advantage of the model in a wide range of settings (including free use and redistribution) as long as they respect the specific use case restrictions outlined, which correspond to model applications the licensor deems ill-suited for the model or are likely to cause harm.""","Academia,Industry",,,,,,TurkuNLP,,,
RT-Trajectory,Robotics,Robotic manipulation,"Google DeepMind,University of California San Diego,Stanford University","Jiayuan Gu, Sean Kirmani, Paul Wohlhart, Yao Lu, Montserrat Gonzalez Arenas, Kanishka Rao, Wenhao Yu, Chuyuan Fu, Keerthana Gopalakrishnan, Zhuo Xu, Priya Sundaresan, Peng Xu, Hao Su, Karol Hausman, Chelsea Finn, Quan Vuong, Ted Xiao",2023-11-03,RT-Trajectory: Robotic Task Generalization via Hindsight Trajectory Sketches,https://arxiv.org/abs/2311.01977,28.0,SOTA improvement,"from blog https://deepmind.google/discover/blog/shaping-the-future-of-advanced-robotics/

""When tested on 41 tasks unseen in the training data, an arm controlled by RT-Trajectory more than doubled the performance of existing state-of-the-art RT models: it achieved a task success rate of 63%, compared with 29% for RT-2.""",,seems to be based on the RT-1 architecture (35M parameters) with some modifications (section 3.3),,"Given the architecture seems to use 35M parameters, it seems unlikely this is above 1e23 FLOP.",RT-1,"""We use the RT-1 (Brohan et al., 2023b) demonstration dataset for training""

also trained with retroactively-generated trajectories created by humans, by code written by GPT-4, and image generation models",,,,,,Reinforcement learning,Unknown,"Generalization remains one of the most important desiderata for robust robot learning systems. While recently proposed approaches show promise in generalization to novel objects, semantic concepts, or visual distribution shifts, generalization to new tasks remains challenging. For example, a language-conditioned policy trained on pick-and-place tasks will not be able to generalize to a folding task, even if the arm trajectory of folding is similar to pick-and-place. Our key insight is that this kind of generalization becomes feasible if we represent the task through rough trajectory sketches. We propose a policy conditioning method using such rough trajectory sketches, which we call RT-Trajectory, that is practical, easy to specify, and allows the policy to effectively perform new tasks that would otherwise be challenging to perform. We find that trajectory sketches strike a balance between being detailed enough to express low-level motion-centric guidance while being coarse enough to allow the learned policy to interpret the trajectory sketch in the context of situational visual observations. In addition, we show how trajectory sketches can provide a useful interface to communicate with robotic policies: they can be specified through simple human inputs like drawings or videos, or through automated methods such as modern image-generating or waypoint-generating methods. We evaluate RT-Trajectory at scale on a variety of real-world robotic tasks, and find that RT-Trajectory is able to perform a wider range of tasks compared to language-conditioned and goal-conditioned policies, when provided the same training data.",,,Unreleased,"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France,United States of America,United States of America",,,,,,2025-05-30 12:39,,,,,,"Industry,Academia,Academia",,,,,Unreleased,,"Industry,Academia,Academia",,,,,,,,,
Yi-34B,Language,"Chat,Language modeling/generation,Translation,Code generation",01.AI,"Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming Yang, Tao Yu, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, Zonghong Dai",2023-11-02,Yi: Open Foundation Models by 01.AI,https://arxiv.org/abs/2403.04652,,Significant use,"2nd most popular model on HuggingFace: https://decrypt.co/206195/new-open-source-ai-model-from-china-boasts-twice-the-capacity-of-chatgpt

also maybe the best open-source model, does better than Llama 2-70B on several benchmarks",34000000000.0,34b,6.1e+23,"""The dataset we use contains Chinese & English only. We used approximately 3T tokens"" sounds like this means it was trained on 3T tokens, not necessarily that the dataset contains 3T tokens?

If so, 34b * 3T * 6 = 6.1e23",Unspecified unreleased,"Chinese and English dataset

""For pretraining, we construct 3.1 trillion tokens of English and Chinese corpora using a cascaded data deduplication and quality filtering pipeline. For finetuning, we polish a small scale (less than 10K) instruction dataset over multiple iterations such that every single instance has been verified directly by our machine learning engineers. For vision-language, we combine the chat language model with a vision transformer encoder and train the model to align visual representations to the semantic space of the language model.""",3100000000000.0,"""language models pretrained from scratch on 3.1T highly-engineered large amount of data, and finetuned on a small but meticulously polished alignment data.""",,,NVIDIA A100,,Confident,The Yi series models are large language models trained from scratch by developers at 01.AI.,,,Open weights (restricted use),China,,,,128.0,,2025-05-28 16:04,,,Yi-34B,,,Industry,,,,,Unreleased,"apply for commercial license:
no training code
https://github.com/01-ai/Yi/blob/main/MODEL_LICENSE_AGREEMENT.txt

the model https://huggingface.co/01-ai/Yi-34B-Chat Apache 2.0
""If you create derivative works based on this model, please include the following attribution in your derivative works: ....""",Industry,,,FP16,101588.94792366636,Operation counting,01-ai,,,14
Cohere Embed,Language,Semantic embedding,Cohere,"Nils Reimers, Elliott Choi, Amr Kayid, Alekhya Nandula, Manoj Govindassamy, Abdullah Elkady",2023-11-02,Cohere Command & Embed on Amazon Bedrock,https://txt.cohere.com/introducing-embed-v3/,,SOTA improvement,"""We are releasing new English and multilingual Embed versions with either 1024 or 384 dimensions. All models can be accessed via our APIs. As of October 2023, these models achieve state-of-the-art performance among 90+ models on the Massive Text Embedding Benchmark (MTEB) and state-of-the-art performance for zero-shot dense retrieval on BEIR.""",,,,"https://docs.cohere.com/docs/environmental-impact

Embed v2 (older version) produced 6689.76 kg CO2 to train. Using the calculator Cohere links (https://mlco2.github.io/impact/) that's the equivalent of 80,000 TPUv3-hours in the ""us-west1"" region. That's 3.5e22 FLOP without considering utilization. However, I have no idea which region Cohere's GPUs are in (looks like CO2/energy can vary a lot by region), and they probably used a more recent GPU.",Unspecified unreleased,,,"""First, they have been trained on questions and answers from a large web crawl. When we presented our multilingual-v2.0 model last year, we had a collection of over 1.4 billion question-and-answer pairs from 100+ languages on basically every topic on the internet.""

""Hence, the second stage involved measuring content quality. We used over 3 million search queries from search engines and retrieved the top-10 most similar documents for each query.""",,,,,Unknown,"We're excited to introduce Embed v3, our latest and most advanced embeddings model. Embed v3 offers state-of-the-art performance per trusted MTEB and BEIR benchmarks.",,,API access,Canada,,,,,,2024-09-05 14:08,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
BlueLM 70B,Language,"Chat,Language modeling/generation,Question answering",vivo AI lab,,2023-11-02,,https://baijiahao.baidu.com/s?id=1781445143383237948&wfr=spider&for=pc,,,,70000000000.0,,4.2e+23,6ND = 6*70B*1000B=4.2e+23,Unspecified unreleased,,,"1000B Text data
10B Image data
100M video data
100M Knowledge graph

(from the conference handout)",,,,,Confident,,,,Unreleased,China,,,,,,2025-05-01 10:42,,,,,,Industry,,,1.33e+25,,Unreleased,information about the model is from their paper catalogue and not found on the internet,Industry,,,,,Operation counting,,,,
BlueLM 130B,Language,"Chat,Language modeling/generation,Question answering",vivo AI lab,,2023-11-02,,https://baijiahao.baidu.com/s?id=1781445143383237948&wfr=spider&for=pc,,,,130000000000.0,,7.8e+23,6ND = 6*130B*1000B=7.8e+23,Unspecified unreleased,,,"1000B Text data
10B Image data
100M video data
100M Knowledge graph

(from the conference handout)",,,,,Confident,,,,Unreleased,China,,,,,,2025-05-01 10:42,,,,,,Industry,,,2.4699999999999996e+25,,Unreleased,information about the model is from their paper catalogue and not found on the internet,Industry,,,,,Operation counting,,,,
BlueLM 175B,Language,"Chat,Language modeling/generation,Question answering",vivo AI lab,,2023-11-02,,https://baijiahao.baidu.com/s?id=1781445143383237948&wfr=spider&for=pc,,,,175000000000.0,,1.05e+24,6ND = 6*175B*1000B=1.05e+24,Unspecified unreleased,,,"1000B Text data
10B Image data
100M video data
100M Knowledge graph

(from the conference handout)",,,,,Confident,,,,Unreleased,China,,,,,,2025-05-01 10:42,,"There is no paper to reference, no information about hardware used for training found in media.",,,,Industry,,,3.3199999999999997e+25,,Unreleased,information about the model is from their paper catalogue and not found on the internet,Industry,,,,,Operation counting,,,,
Yi 6B,Language,"Chat,Language modeling/generation,Translation,Code generation",01.AI,"Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming Yang, Tao Yu, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, Zonghong Dai",2023-11-02,Yi: Open Foundation Models by 01.AI,https://arxiv.org/abs/2403.04652,,,,6000000000.0,6B,1.26e+23,6*7*10^9*3*10^12 = 1.26e+23,Unspecified unreleased,,3100000000000.0,"""language models pretrained from scratch on 3.1T highly-engineered large amount of
data, and finetuned on a small but meticulously polished alignment data.""",,,,,Confident,The Yi series models are large language models trained from scratch by developers at 01.AI.,,,Open weights (restricted use),China,,,,,,2025-05-29 00:11,,,,,,Industry,,,,,Unreleased,"llama license
https://huggingface.co/01-ai/Yi-6B

no training code",Industry,,,,,Operation counting,,,,
Nanbeige-16B,Language,"Chat,Language modeling/generation,Code generation,Question answering",Nanbeige LLM Lab,,2023-11-01,,https://github.com/Nanbeige/Nanbeige/blob/main/README_EN.md,,,"a little worse than Qwen on most metrics, and well short of GPT-4, ofc",16000000000.0,16 billion,2.4e+23,"""It uses 2.5T Tokens for pre-training"". I think that's the number of tokens the model was trained on, not the dataset size, but I'm not sure.

16 billion * 2.5 trillion * 6 = 2.4e23",Unspecified unreleased,"""The training data includes a large amount of high-quality internet corpus, various books, code, etc""",2500000000000.0,"""It uses 2.5T Tokens for pre-training""",,,,,Likely,"Nanbeige-16B is a 16 billion parameter language model developed by Nanbeige LLM Lab. It uses 2.5T Tokens for pre-training. The training data includes a large amount of high-quality internet corpus, various books, code, etc. It has achieved good results on various authoritative evaluation data sets. This release includes the Base, Chat, Base-32k and Chat-32k.",,,Open weights (unrestricted),China,,,,,,2025-05-16 10:30,,,,,,Industry,,,,,Open source,"Apache 2.0

training code: https://github.com/Nanbeige/Nanbeige/blob/main/scripts/train.sh ",Industry,,,,,Operation counting,,,,
LingoWhale-8B,Language,"Language modeling/generation,Code generation,Translation",DeepLang AI,,2023-11-01,,https://github.com/DeepLangAI/LingoWhale-8B/blob/main/README_EN.md,,,competitive with Qwen-7B on C-Eval,8000000000.0,,,,,"""pre-trained on a large volume of high-quality bilingual data"" Chinese + English",,,,,,,Confident,"LingoWhale-8B is the first open-source model in the LingoWhale series introduced by DeepLangAI. It's a bilingual (Chinese-English) large language model.

LingoWhale-8B has been pre-trained on a large volume of high-quality bilingual data and exhibits powerful capabilities as a foundation model. It has achieved leading results on multiple public benchmarks. During its pre-training phase, the model was trained with a context window of 8K, allowing it to comprehend and generate longer sequences.

LingoWhale-8B is fully open for academic research. Users can apply for commercial use by email, and once granted official commercial permission, they can use it for free.",,,Open weights (non-commercial),China,,,,,,2024-09-05 14:08,,,,,,Industry,,,,,Open (non-commercial),"requires form for commercial:
https://github.com/DeepLangAI/LingoWhale-8B/blob/main/MODEL_LICENSE.md",Industry,,,,,,,,,
Calm2-7B,Language,Language modeling/generation,CyberAgent,Ryosuke Ishigami,2023-11-01, CyberAgentLM2-7B (CALM2-7B)   https://huggingface.co/cyberagent/calm2-7b,https://huggingface.co/cyberagent/calm2-7b,,,,7000000000.0,7B,5.459999999999999e+22,"6*7B*1.3T = 54600000000000000000000
6ND aproximation",,"""1.3T tokens of publicly available Japanese and English datasets. """,1300000000000.0,"""1.3T tokens of publicly available Japanese and English datasets.""",,,,Self-supervised learning,Confident,CyberAgentLM2 is a decoder-only language model pre-trained on the 1.3T tokens of publicly available Japanese and English datasets.,,,Open weights (unrestricted),Japan,,,,,,2025-05-09 11:32,,,,,,Industry,,,,,Unreleased,"apache 2.0.

they say datasets are public, but they don't specify which were used",Industry,,,,,Operation counting,,,,
OpenChat 3.5-7B,Language,"Chat,Language modeling/generation",Tsinghua University,,2023-11-01,,https://huggingface.co/openchat/openchat_3.5,,,Creators claim it's the best 7B model and better than GPT-3.5,7000000000.0,7B,,,,"Fine-tuned on instruction data

""OpenChat 3.5 was trained with C-RLFT on a collection of publicly available high-quality instruction data, with a custom processing pipeline. We detail some notable subsets included here:

OpenChat ShareGPT
Open-Orca with FLAN answers
Capybara 1 2 3
GOAT
Glaive
MetaMathQA
MathInstruct
OpenAssistant""",,,,,,,Confident,"OpenChat is an innovative library of open-source language models, fine-tuned with C-RLFT - a strategy inspired by offline reinforcement learning. Our models learn from mixed-quality data without preference labels, delivering exceptional performance on par with ChatGPT, even with a 7B model. Despite our simple approach, we are committed to developing a high-performance, commercially viable, open-source large language model, and we continue to make significant strides toward this vision.",,,Open weights (unrestricted),China,Llama 2-7B,,"Like a Llama 2 finetune though the HF page doesn't specify. Their paper describes a Llama 2-13B finetune: https://arxiv.org/abs/2309.11235

""OpenChat 3.5 was trained with C-RLFT on a collection of publicly available high-quality instruction data, with a custom processing pipeline. We detail some notable subsets included here:

OpenChat ShareGPT
Open-Orca with FLAN answers
...
""",,,2024-09-05 14:08,,,,,,Academia,,,,,Open source,"apache 2.0: https://github.com/imoneoi/openchat

data here (MIT): https://huggingface.co/datasets/openchat/openchat_sharegpt_v3 ",Academia,,,,,,,,,
MuggleMath,"Language,Mathematics","Language modeling/generation,Chat","University of Science and Technology of China (USTC),Alibaba","Chengpeng Li, Zheng Yuan, Hongyi Yuan, Guanting Dong, Keming Lu, Jiancan Wu, Chuanqi Tan, Xiang Wang, Chang Zhou",2023-11-01,Query and Response Augmentation Cannot Help Out-of-domain Math Reasoning Generalization,https://arxiv.org/abs/2310.05506,,,,70000000000.0,"model: Apache 2  https://huggingface.co/OFA-Sys/MuggleMath_7B
code: undefined license https://github.com/OFA-Sys/gsm8k-ScRel?tab=readme-ov-file#readme",,,GSM8K,"By augmenting GSM8K with various queries and multiple reasoning paths, we curated GSM8K to a new dataset named AugGSM8K.",,"The original GSM8K training set has 7,473 samples. We augment 5 more
queries for each query in the training set and yield 7, 473 Ã— 5 = 37, 365 augmented queries.",,,NVIDIA A100,,Confident,"In math reasoning with large language models (LLMs), fine-tuning data augmentation by query evolution and diverse reasoning paths is empirically verified effective, profoundly narrowing the gap between open-sourced LLMs and cutting-edge proprietary LLMs. In this paper, we conduct an investigation for such data augmentation in math reasoning and are intended to answer: (1) What strategies of data augmentation are more effective; (2) What is the scaling relationship between the amount of augmented data and model performance; and (3) Can data augmentation incentivize generalization to out-of-domain mathematical reasoning tasks? To this end, we create a new dataset, AugGSM8K, by complicating and diversifying the queries from GSM8K and sampling multiple reasoning paths. We obtained a series of LLMs called MuggleMath by fine-tuning on subsets of AugGSM8K. MuggleMath substantially achieves new state-of-the-art on GSM8K (from 54% to 68.4% at the scale of 7B, and from 63.9% to 74.0% at the scale of 13B). A log-linear relationship is presented between MuggleMath's performance and the amount of augmented data. We also find that MuggleMath is weak in out-of-domain math reasoning generalization to MATH. This is attributed to the differences in query distribution between AugGSM8K and MATH which suggest that augmentation on a single benchmark could not help with overall math reasoning performance. Codes and AugGSM8K will be uploaded to this https URL.",3.0,,Open weights (unrestricted),"China,China",Llama 2-70B,,,32.0,,2024-09-05 14:08,,,,,,"Academia,Industry",,,,,Open source,https://huggingface.co/OFA-Sys/MuggleMath_7B,"Academia,Industry",,,,25397.802567713887,,,,,
BlueLM 7B,Language,"Chat,Translation,Language modeling/generation,Question answering,Code generation",vivo AI lab,,2023-10-31,BlueLM: An Open Multilingual 7B Language Model,https://github.com/vivo-ai-lab/BlueLM/blob/main/BlueLM_technical_report.pdf,,,,7000000000.0,"""BlueLM is a large-scale pre-trained language model independently developed by vivo AI Global Research Institute. This release includes 7B base (base) model and 7B conversation (chat) model. At the same time, we have open sourced the long text base (base) model that supports 32K and conversation (chat) model."" from GitHub https://github.com/vivo-ai-lab/BlueLM

",1.0920000000001e+23,"C = 6DN = 6 * 2.6T * 7B = 1.092*10^23 FLOP
https://www.wolframalpha.com/input?i=6*7+billion+*+2.6+trillion
(assuming 1 epoch)

Figure 1 gives compute of 10^12 FLOPs  but this seems improbable

Training over 2.59T tokens took approximately 26 days using the vivolm system, with a throughput of 3150 tokens/sec/GPU.",Unspecified unreleased,,2592000000000.0,"""Larger amounts of high-quality data : high-quality corpus for training, reaching a scale of 2.6 trillion tokens. The corpus includes Chinese, English and a small amount of Japanese and Korean data"" from GitHub

see 2.1 https://github.com/vivo-ai-lab/BlueLM/blob/main/BlueLM_technical_report.pdf",,,,,Confident,"BlueLM is a large-scale open-source language model independently developed by the vivo AI Lab. This release includes 2K and 32K context length versions for both Base and Chat models.

High-quality Data: BlueLM is trained on a high-quality data with 2.6 trillion tokens. Our train corpus mainly consists of Chinese and English data, with a small amount of Japanese and Korean data.
Stronger Performance: BlueLM-7B-Chat achieves a strong competitive performance in C-Eval and CMMLU benchmarks of the same size.
Longer Context: We have extended the context length of both BlueLM-7B-Base-32K and BlueLM-7B-Chat-32K models from 2K to 32K. The models can support longer context understanding while maintaining the same basic capabilities.
Model License: BlueLM weights are open for academic research and commercial use.",,,Open weights (restricted use),China,,,,,,2025-05-16 10:30,,,,,,Industry,,,,,Unreleased,"https://github.com/vivo-ai-lab/BlueLM/blob/main/MODEL_LICENSE_EN.pdf

Our code is licensed under the Apache-2.0 and Community License for BlueLM Model. The BlueLM weights are completely open for academic research, and free commercial use is allowed after completing the questionnaire.

""BlueLM weights are open for academic research and commercial use.""",Industry,,,,,Operation counting,,,,
Tongyi Qianwen 2.0,Language,"Chat,Language modeling/generation",Alibaba,,2023-10-31,Alibaba Cloud Launches Tongyi Qianwen 2.0 and Industry-specific Models to Support Customers Reap Benefits of Generative AI,https://www.alibabacloud.com/blog/alibaba-cloud-launches-tongyi-qianwen-2-0-and-industry-specific-models-to-support-customers-reap-benefits-of-generative-ai_600526,,,,,"""Tongyi Qianwen 2.0, a generic LLM with a few hundreds of billions of parameters""",,,Unspecified unreleased,,,,,,,,Unknown,"Alibaba Cloud, the digital technology and intelligence backbone of Alibaba Group, today announced the launch of Tongyi Qianwen 2.0, its latest large language model (LLM), along with new industry-specific models at its annual flagship tech event Apsara Conference. This release signifies another significant progress in Alibaba Cloud's pursuit of cutting-edge AI innovation and its ongoing commitment to fuel digital transformation in businesses.",,,API access,China,,,,,,2024-09-05 14:08,,,,,,Industry,,,,,,,Industry,,,,,,,,,
Mi:dm 200B,Language,Language modeling/generation,KT,,2023-10-31,,https://genielabs.ai/midm/about,,,,200000000000.0,200B,1.1999999999999999e+24,6ND=1000000000000*200000000000.00*6=1.2 Ã— 10^24,,,1000000000000.0,Mi:dm is the first Korean LLM trained on over 1 trillion tokens.,,,,,Confident,"TL;DR:
KT Corp introduces Mi:dm, a massive AI model aimed at diverse sectors.
Mi:dm is the first Korean LLM trained on over 1 trillion tokens.
It offers four models, from basic to large, with up to 200 billion parameters.
KT plans to share Mi:dmâ€™s foundational model with other companies.
Three advanced technologies reduce AI hallucinations by up to 70%.
Collaborations with AI startups, including Upstage, aim to conquer the global generative AI market.",,,API access,Korea (Republic of),,,,,,2025-02-14 16:01,,,,,,Industry,,,,,Unreleased,"KT said it will open up the foundation model of Mi:dm to other companies, providing a full AI development package, including KT Cloud's hyperscale AI computing service and AI chip startup Rebellions Inc.'s neural processing unit infrastructure, fostering the development of various AI services.",Industry,,,,,Operation counting,,,,
Skywork-13B,Language,"Language modeling,Language modeling/generation,Translation",Kunlun Inc.,"Tianwen Wei, Liang Zhao, Lichang Zhang, Bo Zhu, Lijie Wang, Haihua Yang, Biye Li, Cheng Cheng, Weiwei LÃ¼, Rui Hu, Chenxia Li, Liu Yang, Xilin Luo, Xuejie Wu, Lunan Liu, Wenjun Cheng, Peng Cheng, Jianhao Zhang, Xiaoyu Zhang, Lei Lin, Xiaokun Wang, Yutuan Ma, Chuanhai Dong, Yanqi Sun, Yifu Chen, Yongyi Peng, Xiaojuan Liang, Shuicheng Yan, Han Fang, Yahui Zhou",2023-10-30,Skywork: A More Open Bilingual Foundation Model,https://arxiv.org/abs/2310.19341,75.0,SOTA improvement,"""We show that our model not only excels on popular benchmarks, but also achieves state of the art performance in Chinese language modeling on diverse domains""",13000000000.0,13B,2.5e+23,"""Our Skywork-13B is trained on a cluster of 64 NVIDIA-HGX-A800 nodes, a total of 512 A800-80G SXM GPUs... The training process of Skywork-13B spanned a total of 39 days.""

They note that ""we achieved a token throughput of 1873 per GPU per second and a model flops utilization (MFU) of 56.5%... "". 

""MFU"" was coined in the Palm paper (https://arxiv.org/pdf/2204.02311.pdf) and only counts operations used to train the model, not all operations observed on the hardware. MFU is lower than traditionally measured utilization.

Using the 56.5% number, and a peak tensor performance of 623.8 TFLOPS for the A800, this suggests 512 * 623.8 TFLOPS * 39 days * 86400 seconds/day * 0.565 = 6.08e23 FLOP.

Based on C=6ND, with 13B parameters and 3.2T tokens, we have C=6*(13B)*(3.2T)=2.5e23 FLOP.

Since the reported MFU is quite high, and would imply a higher compute usage than 6ND, it seems they may have trained on mixed precision and with the GPUs not always operating in the 623.8 TFLOPS mode.",SkyPile,"""In order to train Skywork-13B, we build SkyPile, a vast, high quality corpus comprising more than 6 trillion tokens. A segment of the corpus, comprising over 150 billion tokens of web text, has been open sourced to facilitate research and training on Chinese LLMs""",3180000000000.0,"The full SkyPile dataset is 6 trillion tokens, roughly half English and half Chinese: (https://huggingface.co/Skywork/Skywork-13B-base).

The model is trained for the equivalent of 0.53 epochs on the full dataset, or 3.18 trillion unique tokens. This is around 2.78 trillion words, based on an average of 1 word/token for the Chinese portion and 0.75 word/token on the English portion.",940.0,39 days,NVIDIA A800 PCIe 40 GB,,Confident,"In this technical report, we present Skywork-13B, a family of large language models (LLMs) trained on a corpus of over 3.2 trillion tokens drawn from both English and Chinese texts. This bilingual foundation model is the most extensively trained and openly published LLMs of comparable size to date. We introduce a two-stage training methodology using a segmented corpus, targeting general purpose training and then domain-specific enhancement training, respectively. We show that our model not only excels on popular benchmarks, but also achieves state of the art performance in Chinese language modeling on diverse domains. Furthermore, we propose a novel leakage detection method, demonstrating that test data contamination is a pressing issue warranting further investigation by the LLM community. To spur future research, we release Skywork-13B along with checkpoints obtained during intermediate stages of the training process. We are also releasing part of our SkyPile corpus, a collection of over 150 billion tokens of web text, which is the largest high quality open Chinese pre-training corpus to date. We hope Skywork-13B and our open corpus will serve as a valuable open-source resource to democratize access to high-quality LLMs.",1.0,,Open weights (restricted use),China,,,,512.0,0.565,2025-05-28 16:04,,,,16000000.0,Table 3,Industry,,,,,Open (restricted use),"commercial but restrictive license: https://github.com/SkyworkAI/Skywork/blob/main/LICENSE

part of the training data is open, but only 2.5%: ""In order to train Skywork-13B, we build SkyPile, a vast, high quality corpus comprising more than 6 trillion tokens. A segment of the corpus, comprising over 150 billion tokens of web text, has been open sourced to facilitate research and training on Chinese LLMs""

training code: https://github.com/SkyworkAI/Skywork/blob/main/train/train.py ",Industry,,"""With this configuration, we achieved a token throughput of 1873 per GPU per second and a model flops utilization (MFU) of 56.5%."" See also Table 10.

Using reported value, though this somewhat disagrees with a manual calculation:
Ops-counting method: 6 * 13B * 3.2T = 2.496e23 FLOP
FLOPs from 39 days on 512 A800s: 39 * 24 * 3600 * 512 * 3.12e14 = 5.383e23 FLOP

2.496e23 / 5.383e23 = 0.4637",BF16,253989.3377909499,Operation counting,,,,
ChatGLM3-6B,"Multimodal,Language,Vision","Chat,Visual question answering,Code generation",Zhipu AI,"Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Diego Rojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, Hao Yu, Hongning Wang, Jiadai Sun, Jiajie Zhang, Jiale Cheng, Jiayi Gui, Jie Tang, Jing Zhang, Juanzi Li, Lei Zhao, Lindong Wu, Lucen Zhong, Mingdao Liu, Minlie Huang, Peng Zhang, Qinkai Zheng, Rui Lu, Shuaiqi Duan, Shudan Zhang, Shulin Cao, Shuxun Yang, Weng Lam Tam, Wenyi Zhao, Xiao Liu, Xiao Xia, Xiaohan Zhang, Xiaotao Gu, Xin Lv, Xinghan Liu, Xinyi Liu, Xinyue Yang, Xixuan Song, Xunkai Zhang, Yifan An, Yifan Xu, Yilin Niu, Yuantao Yang, Yueyan Li, Yushi Bai, Yuxiao Dong, Zehan Qi, Zhaoyu Wang, Zhen Yang, Zhengxiao Du, Zhenyu Hou, Zihan Wang",2023-10-27,Zhipu AI launches third-generation base model,https://www.zhipuai.cn/en/news/76,,SOTA improvement,"Aiming at GPT-4V, ChatGLM3 has implemented iterative upgrades of several new functions this time, including:

CogVLM with multi-modal understanding capabilities, looks at image semantics, and achieved SOTA on more than 10 international standard image and text evaluation data sets;",6000000000.0,6B from https://arxiv.org/abs/2406.12793,5.04e+22,"Highly speculative.
Assume 1 epoch on 1.4T tokens.
6 FLOP/token/param * 1.4T tokens * 6B params=50.4 * 10 ^(12+9) = 5.04*10^(22)",Unspecified unreleased,ChatGLM2 corpus pretraining plus human preference alignment training,1400000000000.0,"""ChatGLM-6B was pre-trained on approximately one trillion tokens of Chinese and English corpus""
""By further realizing more diverse training datasets, more sufficient training steps, and more optimized training strategies, ChatGLM3-6B topped 42 benchmarks across semantics, mathematics, reasoning, code, and knowledge.""
The ChatGLM website states that the latest ChatGLM service is based on (and upgraded from) ChatGLM2, which was trained on 1.4T tokens. Assume that ChatGLM3 is trained on at least the same number of tokens.
Sources:
https://chatglm.cn/
https://github.com/THUDM/ChatGLM2-6B/blob/main/README_EN.md
https://www.zhipuai.cn/en/news/76

here (https://github.com/Kwai-Kolors/Kolors/blob/master/imgs/Kolors_paper.pdf) they confirm the dataset size ""Consequently, in Kolors, we utilize the open-source ChatGLM3-6B-Base as text encoder, which has been pre-trained with over 1.4 trillion bilingual tokens, resulting in a robust capability for Chinese language understanding.""",,,,,Speculative,"On October 27, 2023, at the 2023 China Computer Conference (CNCC), Zhipu AI launched the fully self-developed third-generation large base model ChatGLM3 and related series of products.",,,Open weights (restricted use),China,,,,,,2025-05-09 15:57,,,,,,Industry,checked,,,,Unreleased,weights available with restricted license: https://huggingface.co/THUDM/chatglm-6b/blob/main/MODEL_LICENSE ,Industry,,,,,Operation counting,,,,
CODEFUSION (Python),Language,Code generation,"Microsoft,Microsoft Research","Mukul Singh, JosÃ© Cambronero, Sumit Gulwani, Vu Le, Carina Negreanu, Gust Verbruggen",2023-10-26,CODEFUSION: A Pre-trained Diffusion Model for Code Generation,"https://arxiv.org/abs/2310.17680 (was withdrawn)

https://www.microsoft.com/en-us/research/wp-content/uploads/2023/11/CodeFusion-Revised-CameraReady.pdf",17.0,SOTA improvement,"See Table 1, SOTA in Python code generation",75000000.0,Table 1,7.92e+18,"V100 performance: 125 teraFLOPS according to https://www.nvidia.com/en-us/data-center/v100/

11 hours * 4 GPUs * 125 teraFLOPS/GPU * 0.40 utilization = 7.92e18 FLOP",,,4390400.0,"Section A3, Table 5: for python, 56k samples with an average length of 78.4 tokens",11.0,"""The system used to run the experiments uses an Intel Core i7 processor (base at 1.8 GHz) along with 4 V100 GPU units, a 64-bit operating system, and 56 GB RAM. CODEFUSION took 8 hours to pre-train and 3 hours to fine-tune on average for each dataset.""",NVIDIA V100,Self-supervised learning,Confident,"Imagine a developer who can only change their last line of code, how often would they have to start writing a function from scratch before it is correct? Auto-regressive models for code generation from natural language have a similar limitation: they do not easily allow reconsidering earlier tokens generated. We introduce CodeFusion, a pre-trained diffusion code generation model that addresses this limitation by iteratively denoising a complete program conditioned on the encoded natural language. We evaluate CodeFusion on the task of natural language to code generation for Bash, Python, and Microsoft Excel conditional formatting (CF) rules. Experiments show that CodeFusion (75M parameters) performs on par with state-of-the-art auto-regressive systems (350M-175B parameters) in top-1 accuracy and outperforms them in top-3 and top-5 accuracy due to its better balance in diversity versus quality.",,,Unreleased,"United States of America,Multinational,India,Belgium,United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational",,,,4.0,,2025-06-01 16:21,,,,,,"Industry,Industry",,,,,Unreleased,,"Industry,Industry",$8.54,,,2381.36215809478,Hardware,,,,
DiT-XL/2 + CADS,Image generation,Image generation,"ETH Zurich,Disney Research","Seyedmorteza Sadat, Jakob Buhmann, Derek Bradley, Otmar Hilliges, Romann M. Weber",2023-10-26,CADS: Unleashing the Diversity of Diffusion Models through Condition-Annealed Sampling,https://arxiv.org/abs/2310.17347v2,,SOTA improvement,"""Further, using an existing pretrained diffusion model, CADS achieves a new state-of-the-art FID of 1.70 and 2.31 for class-conditional ImageNet generation at 256Ã—256 and 512Ã—512 respectively""",675000000.0,original parameter count for DiT-XL/2,,,ImageNet,,,,,,,,Likely,"While conditional diffusion models are known to have good coverage of the data distribution, they still face limitations in output diversity, particularly when sampled with a high classifier-free guidance scale for optimal image quality or when trained on small datasets. We attribute this problem to the role of the conditioning signal in inference and offer an improved sampling strategy for diffusion models that can increase generation diversity, especially at high guidance scales, with minimal loss of sample quality. Our sampling strategy anneals the conditioning signal by adding scheduled, monotonically decreasing Gaussian noise to the conditioning vector during inference to balance diversity and condition alignment. Our Condition-Annealed Diffusion Sampler (CADS) can be used with any pretrained model and sampling algorithm, and we show that it boosts the diversity of diffusion models in various conditional generation tasks. Further, using an existing pretrained diffusion model, CADS achieves a new state-of-the-art FID of 1.70 and 2.31 for class-conditional ImageNet generation at 256Ã—256 and 512Ã—512 respectively.",,,Unreleased,"Switzerland,United States of America,Switzerland,United Kingdom of Great Britain and Northern Ireland",DiT-XL/2,,,,,2025-05-30 11:09,,,,,,"Academia,Industry",,,,,Unreleased,,"Academia,Industry",,,,,,,,,
QMoE: compressed SwitchTransformer,Language,Language modeling,"Institute of Science and Technology Austria (ISTA),Neural Magic","Elias Frantar, Dan Alistarh",2023-10-25,QMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models,https://arxiv.org/abs/2310.16795,15.0,,,1571000000000.0,"""Concretely, QMoE can compress the 1.6 trillion parameter SwitchTransformer-c2048 model to less than 160GB""
Same parameter count as base model. This paper compresses the model; there is no learning from data.",,,C4,"In this paper, the authors compress the SwitchTransformer-c2048 model. There is no training dataset used.",576000000000.0,"In this paper, the authors compress the SwitchTransformer-c2048 model. There is no training dataset used.",,,NVIDIA RTX A6000,,Confident,"Mixture-of-Experts (MoE) architectures offer a general solution to the high inference costs of large language models (LLMs) via sparse routing, bringing faster and more accurate models, at the cost of massive parameter counts. For example, the SwitchTransformer-c2048 model has 1.6 trillion parameters, requiring 3.2TB of accelerator memory to run efficiently, which makes practical deployment challenging and expensive. In this paper, we present a solution to this memory problem, in form of a new compression and execution framework called QMoE. Specifically, QMoE consists of a scalable algorithm which accurately compresses trillion-parameter MoEs to less than 1 bit per parameter, in a custom format co-designed with bespoke GPU decoding kernels to facilitate efficient end-to-end compressed inference, with minor runtime overheads relative to uncompressed execution. Concretely, QMoE can compress the 1.6 trillion parameter SwitchTransformer-c2048 model to less than 160GB (20x compression, 0.8 bits per parameter) at only minor accuracy loss, in less than a day on a single GPU. This enables, for the first time, the execution of a trillion-parameter model on affordable commodity hardware, like a single server with 4x NVIDIA A6000 or 8x NVIDIA 3090 GPUs, at less than 5% runtime overhead relative to ideal uncompressed inference. The source code and compressed models are available at github.com/IST-DASLab/qmoe.",,,Open weights (unrestricted),"Austria,United States of America",Switch,1003363200000000000,"(1) * (38.71 * 10 ** 12) * (0.3) * (24 * 3600) = 1003363200000000000
(num gpu) * (peak flop) * (assumed utilization rate) * (time in seconds)
from the paper: ""This allows us to apply data-dependent compression to massive MoEs, while preserving the key feature of post-training
compression techniques: the ability to perform effective
compression using only modest computational resources,
e.g., a single NVIDIA A6000 GPU and less than one day of
compute.""
A6000 have 38.71 TFLOPs from https://www.techpowerup.com/gpu-specs/rtx-a6000.c3686",,,2025-05-01 10:42,,,,,,"Academia,Industry",,,,,Open source,"apache 2.0
github.com/IST-DASLab/qmoe","Academia,Industry",,,,,,,,,
Xinghan Foundation Model,"Multimodal,Video,Language,Vision","Video description,Visual question answering,Language modeling/generation",Dahua Technology,,2023-10-25,Dahua Announces Think# 2.0 Strategy to Accelerate Innovation for a Digital Intelligent Future,https://www.prnewswire.com/news-releases/dahua-announces-think-2-0-strategy-to-accelerate-innovation-for-a-digital-intelligent-future-301967122.html,,,,,,,,,,,,,,,,Unknown,"At the summit, Dahua's ""Xinghan"" Foundation Model was launched. With video as the core, this multimodal fusion industry foundation model catapults the accuracy and generalization of AI algorithms, with breakthroughs in visual cognition capabilities, independent analysis of various scenarios, and efficient fulfillment of massive fragmented needs, especially urban governance and power industry applications.",,,,China,,,,,,2024-09-05 14:08,,,,,,Industry,,,,,,,Industry,,,,,,,,,
Zephyr 7B,Language,"Language modeling/generation,Question answering",Hugging Face,"Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, ClÃ©mentine Fourrier, Nathan Habib, Nathan Sarrazin, Omar Sanseviero, Alexander M. Rush, Thomas Wolf",2023-10-25,Zephyr: Direct Distillation of LM Alignment,https://arxiv.org/abs/2310.16944,,,,7000000000.0,7B,,,"UltraChat,UltraFeedback",,,,4.0,"""All experiments were run on 16 A100s using bfloat16 precision
and typically took 2-4 hours to complete.""",NVIDIA A100,,Confident,"We aim to produce a smaller language model that is aligned to user intent. Previous research has shown that applying distilled supervised fine-tuning (dSFT) on larger models significantly improves task accuracy; however, these models are unaligned, i.e. they do not respond well to natural prompts. To distill this property, we experiment with the use of preference data from AI Feedback (AIF). Starting from a dataset of outputs ranked by a teacher model, we apply distilled direct preference optimization (dDPO) to learn a chat model with significantly improved intent alignment. The approach requires only a few hours of training without any additional sampling during fine-tuning. The final result, Zephyr-7B, sets the state-of-the-art on chat benchmarks for 7B parameter models, and requires no human annotation. In particular, results on MT-Bench show that Zephyr-7B surpasses Llama2-Chat-70B, the best open-access RLHF-based model. Code, models, data, and tutorials for the system are available at this https URL.",3.0,,Open weights (unrestricted),"Multinational,United States of America",Mistral 7B,21565440000000000000,312000000000000*16*4*3600*0.3 = 2.156544e+19,16.0,,2025-05-01 10:42,,,,,,Industry,,,,,Open source,"Apache 2.0

https://github.com/huggingface/alignment-handbook

https://huggingface.co/HuggingFaceH4/zephyr-7b-beta",Industry,,,BF16,12700.881013991002,Hardware,,,,
Spark 3.0,Language,"Code generation,Language generation,Language modeling/generation,Question answering",iFlytek,,2023-10-24,,https://www.gizmochina.com/2023/10/26/iflytek-spark-3-0-vs-openai-gpt/,,,"""One such claim comes from Chinese company iFlytek, which asserts that its latest large language model, Spark 3.0, surpasses OpenAIâ€™s GPT-3.5 in Chinese language tasks while demonstrating comparable performance in English contexts""",,,,"""The company said that Spark 3.0 has been trained on a dataset of 1.2 trillion words and code, and that it can perform a variety of tasks, including text generation, language understanding, knowledge answering, logical reasoning, mathematical computation, code generation, and multimodal interaction."" https://www.gizmochina.com/2023/10/26/iflytek-spark-3-0-vs-openai-gpt/",,,1200000000000.0,"""Spark 3.0 has been trained on a dataset of 1.2 trillion words and code""",,,,,Likely,,,,,China,,,,,,2024-09-05 14:08,,,,,,Industry,,,,,,,Industry,,,,,,,,,
Stockmark-13B,Language,Language modeling/generation,Stockmark,,2023-10-23, stockmark/stockmark-13b,https://huggingface.co/stockmark/stockmark-13b,,,,13200000000.0,13.2B from https://huggingface.co/stockmark/stockmark-13b,1.7160000000000002e+22,"6ND = 6*13B*220B = 17160000000000000000000
""Stockmark-13b is a 13 billion parameter LLM pretrained from scratch based on Japanese corpus of about 220B tokens. This model is developed by Stockmark Inc.""
","CC100,mC4,Wikipedia (ja),Common Crawl (ja)", Training dataset section from https://huggingface.co/stockmark/stockmark-13b,220000000000.0,220B tokens,,,,Self-supervised learning,Likely,,,,Open weights (unrestricted),Japan,,,,,,2025-05-09 11:32,,,,,,Industry,,,,,Unreleased,MIT license,Industry,,,,,Operation counting,,,,
SILC-S* (86M),Vision,"Image classification,Image segmentation","ETH Zurich,DeepMind,Google,Technical University of Munich","Muhammad Ferjad Naeem, Yongqin Xian, Xiaohua Zhai, Lukas Hoyer, Luc Van Gool, Federico Tombari",2023-10-20,SILC: Improving Vision Language Pretraining with Self-Distillation,https://arxiv.org/abs/2310.13355,,,,86000000.0,"""SILC models set a new state-of-the-art for these tasks at ViT/B16 model size"" (https://arxiv.org/pdf/2310.13355, page 5). VIT/B16-224 has 86.6M parameters, and VIT/B16-384 has 86.9M parameters (https://huggingface.co/google/vit-base-patch16-224, https://huggingface.co/google/vit-base-patch16-384), so I will assume SILC-S has 86M parameters.",1.004e+22,"""SILC* at ViT B/16 can be trained on 256 TPUv4 chips meanwhile the B/8 and L/16 models require 512 chips. The training takes around 5 days"" (https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03093-supp.pdf, page 7). Assuming a 33% utilization rate,
Training compute
= utilization rate * # of chips used * peak FLOPS / chip * training time
= 0.33 * 256 TPUv4 chip * 2.75e14 FLOPS / TPUv4 chip * 5 days * 24 h / day * 3600 s / h
~= 1.004e22 FLOPS",WebLI,"""Our models trained on WebLI are marked as SILC*"" (https://arxiv.org/pdf/2310.13355, page 5).",20000000000.0,"SILC-S* saw 20B examples of image-text pairs during training (https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03093-supp.pdf, page 4).",120.0,"""SILC* at ViT B/16 can be trained on 256 TPUv4 chips meanwhile the B/8 and L/16 models require 512 chips. The training takes around 5 days"" (https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03093-supp.pdf, page 7).",Google TPU v4,,Confident,"Image-Text pretraining on web-scale image caption datasets has become the default recipe for open vocabulary classification and retrieval models thanks to the success of CLIP and its variants. Several works have also used CLIP features for dense prediction tasks and have shown the emergence of open-set abilities. However, the contrastive objective used by these models only focuses on image-text alignment and does not incentivise image feature learning for dense prediction tasks. In this work, we introduce SILC, a novel framework for vision language pretraining. SILC improves image-text contrastive learning with the simple addition of local-to-global correspondence learning by self-distillation. We show that distilling local image features from an exponential moving average (EMA) teacher model significantly improves model performance on dense predictions tasks like detection and segmentation, while also providing improvements on image-level tasks such as classification and retrieval. SILC models sets a new state of the art for zero-shot classification, few shot classification, image and text retrieval, zero-shot segmentation, and open vocabulary segmentation. We further show that SILC features greatly benefit open vocabulary detection, captioning and visual question answering.",,,Unreleased,"Switzerland,United Kingdom of Great Britain and Northern Ireland,United States of America,Germany",,,,256.0,,2025-06-16 12:19,,,,16000.0,"""We trained with a batch size of 16k on Google TPUs"" (https://arxiv.org/pdf/2310.13355, page 5).","Academia,Industry,Industry,Academia",,,,,Unreleased,,"Academia,Industry,Industry,Academia",,,,86375.60801213082,,,,,
DALLÂ·E 3,Image generation,"Image generation,Text-to-image",OpenAI,"James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, Wesam Manassra, Prafulla Dhariwal, Casey Chu, Yunxin Jiao, Aditya Ramesh",2023-10-19,Improving Image Generation with Better Captions,https://cdn.openai.com/papers/dall-e-3.pdf,622.0,SOTA improvement,,,,,,Unspecified unreleased,,,,,,,,Unknown,"We show that prompt following abilities of text-to-image models can be substantially improved by training on highly descriptive generated image captions.
Existing text-to-image models struggle to follow detailed image descriptions and often ignore words or confuse the meaning of prompts. We hypothesize that this issue stems from noisy and inaccurate image captions in the training dataset. We address this by training a bespoke image captioner and use it to recaption the training dataset. We then train several text-to-image models and find that training on these synthetic captions reliably improves prompt following ability. Finally, we use these findings to build DALL-E 3: a new text-to-image generation system, and benchmark its performance on an evaluation designed to measure prompt following, coherence, and aesthetics, finding that it compares favorably to competitors. We publish samples and code for these evaluations so that future research can continue optimizing this important aspect of text-to-image systems.",,,API access,United States of America,,,,,,2025-02-03 11:46,,,,,,Industry,checked,,,,Unreleased,https://platform.openai.com/docs/models/dall-e,Industry,,,,,,,,,
KwaiYiiMath,Language,"Quantitative reasoning,Chat",Kuaishou Technology,"Jiayi Fu, Lei Lin, Xiaoyang Gao, Pengli Liu, Zhengzong Chen, Zhirui Yang, Shengnan Zhang, Xue Zheng, Yan Li, Yuliang Liu, Xucheng Ye, Yiqiao Liao, Chao Liao, Bin Chen, Chengru Song, Junchen Wan, Zijia Lin, Fuzheng Zhang, Zhongyuan Wang, Di Zhang, Kun Gai",2023-10-19,KwaiYiiMath: Technical Report,https://arxiv.org/abs/2310.07488,,,,13000000000.0,13B - from Table 2,,,"GSM8K,Unspecified unreleased","""We chose a sample from the train set of GSM8k.""

""we first collect math data from a wide range of sources, including different difficulties (e.g., primary school, middle school, and university, etc.),
and different fields of math (e.g., algebra, geometry, and probability, etc.). Then, we generate intermediate rationales for math questions only with the final answer or without the answer using open-source LLMs and ensure the correctness of intermediate rationales and answers through manual annotation""",,,,,,,Confident,"Recent advancements in large language models (LLMs) have demonstrated remarkable abilities in handling a variety of natural language processing (NLP) downstream tasks, even on mathematical tasks requiring multi-step reasoning. In this report, we introduce the KwaiYiiMath which enhances the mathematical reasoning abilities of KwaiYiiBase1, by applying Supervised Fine-Tuning (SFT) and Reinforced Learning from Human Feedback (RLHF), including on both English and Chinese mathematical tasks. Meanwhile, we also constructed a small-scale Chinese primary school mathematics test set (named KMath), consisting of 188 examples to evaluate the correctness of the problem-solving process generated by the models. Empirical studies demonstrate that KwaiYiiMath can achieve state-of-the-art (SOTA) performance on GSM8k, CMath, and KMath compared with the similar size models, respectively. ",,,Unreleased,China,KwaiYii 13B,,,,,2024-09-05 14:08,,,,,,Industry,,,,,Open source,https://github.com/kwai/KwaiYii/,Industry,,,,,,,,,
ERNIE 4.0,"Multimodal,Language,Video,Image generation","Chat,Language modeling/generation,Video generation,Image generation",Baidu,,2023-10-17,"Baidu Launches ERNIE 4.0 Foundation Model, Leading a New Wave of AI-Native Applications",https://www.prnewswire.com/news-releases/baidu-launches-ernie-4-0-foundation-model-leading-a-new-wave-of-ai-native-applications-301958681.html,,Significant use,"Likely SOTA for Mandarin? But very little info available.

Lots of users (https://www.cnn.com/2023/12/15/tech/gpt4-china-baidu-ernie-ai-comparison-intl-hnk/index.html):

""Baidu says ERNIE has racked up 70 million users. Thatâ€™s compared with 150 million users for ChatGPT, according to an estimate from Similarweb, a digital data and analytics company.""",,"""similar architecture with 3.5 version""  -interpreter dub at 01:25:08 https://www.youtube.com/watch?v=wYozcsavRuM",,may be mentioned here https://www.youtube.com/watch?v=wYozcsavRuM,,may be mentioned here https://www.youtube.com/watch?v=wYozcsavRuM,,,,,,,Unknown,"Baidu, Inc. (NASDAQ: BIDU and HKEX: 9888), a leading AI company with strong Internet foundation, today hosted its annual flagship technology conference Baidu World 2023 in Beijing, marking the conference's return to an offline format after four years. With the theme ""Prompt the World,"" this year's Baidu World conference saw Baidu launch ERNIE 4.0, Baidu's next-generation and most powerful foundation model offering drastically enhanced core AI capabilities. Baidu also showcased some of its most popular applications, solutions, and products re-built around the company's state-of-the-art generative AI.                                                                               

Robin Li, Co-founder, Chairman and CEO of Baidu, announced ERNIE 4.0 at Baidu World 2023
""ERNIE 4.0 has achieved a full upgrade with drastically improved performance in understanding, generation, reasoning, and memory,"" Robin Li, Co-founder, Chairman and CEO of Baidu, said at the event. ""These four core capabilities form the foundation of AI-native applications and have now unleashed unlimited opportunities for new innovations.""

",,,API access,China,,,,,,2025-05-28 17:31,,,,,,Industry,checked,,,,Unreleased,"""ERNIE 4.0 is now accessible to invited users on ERNIE Bot, and the API will be available upon application to enterprise clients via Qianfan foundation model platform.""",Industry,,,,,,,,,
Fuyu-8B,"Multimodal,Language,Vision","Chat,Image classification,Visual question answering,Image captioning",Adept,"Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell Nye, Augustus Odena, Arushi Somani, SaÄŸnak TaÅŸÄ±rlar",2023-10-17,Fuyu-8B: A Multimodal Architecture for AI Agents,"https://www.adept.ai/blog/fuyu-8b, www.huggingface.co/adept/fuyu-8b",,,"""The Fuyu models perform well according to these metrics, even though they are heavily focused on natural images. Fuyu-8B improves over QWEN-VL and PALM-e-12B on 2 out of 3 metrics despite having 2B and 4B fewer parameters, respectively.""",8000000000.0,"8B

Also a ""Fuyu-medium"" with unstated param count (<56B: ""Fuyu-Medium performs comparably to PALM-E-562B despite having fewer than a tenth as many parameters"")",,,,,,,,,,,Confident,"Weâ€™re releasing Fuyu-8B, a small version of the multimodal1 model that powers our product. The model is available on HuggingFace. We think Fuyu-8B is exciting because:

It has a much simpler architecture and training procedure than other multi-modal models, which makes it easier to understand, scale, and deploy.
Itâ€™s designed from the ground up for digital agents, so it can support arbitrary image resolutions, answer questions about graphs and diagrams, answer UI-based questions, and do fine-grained localization on screen images.
Itâ€™s fast - we can get responses for large images in less than 100 milliseconds.
Despite being optimized for our use-case, it performs well at standard image understanding benchmarks such as visual question-answering and natural-image-captioning.",,,Open weights (non-commercial),United States of America,,,,,,2024-09-05 14:08,,,,,,Industry,,,,,Unreleased,non-commercial: https://huggingface.co/adept/fuyu-8b,Industry,,,,,,,,,
Aquila2 34B,Language,"Chat,Language modeling/generation",Beijing Academy of Artificial Intelligence / BAAI,,2023-10-13,,"https://github.com/FlagAI-Open/Aquila2
https://huggingface.co/BAAI/Aquila2-34B",,,,34000000000.0,"34B
safetensors say it is 18.2B params

There's also a 70B ""experimental"" version: https://github.com/FlagAI-Open/Aquila2",,"6ND = 6*34000000000*2000000000000=4.08e+23

'Likely' confidence because unknown number of epochs, dataset size and number of parameters are not certain.",Unspecified unreleased,,2000000000000.0,"""we have investigated all 2 trillion tokens of data"" from https://github.com/FlagAI-Open/Aquila2",,,,,Likely,"We announce that our Aquila2 series is now open source, comprising Aquila2 (the base language models: Aquila2-7B, Aquila2-34B and Aquila2-70B-Expr) and AquilaChat2 (the chat models, namely AquilaChat2-7B, AquilaChat2-34B and AquilaChat2-70B-Expr, as well as the long-text chat models, namely AquilaChat2-7B-16k and AquilaChat2-34B-16k). You can find the links in the following table. Kindly click on them to access the model cards.",,,Open weights (unrestricted),China,,,,,,2024-09-05 14:08,,,,,,Academia,,,,,Open source,apache 2.0,Academia,,,,,,,,,
Table-GPT,Language,"Language modeling/generation,Table tasks",Microsoft Research,"Peng Li, Yeye He, Dror Yashar, Weiwei Cui, Song Ge, Haidong Zhang, Danielle Rifinski Fainman, Dongmei Zhang, Surajit Chaudhuri",2023-10-13,Table-GPT: Table-tuned GPT for Diverse Table Tasks,https://arxiv.org/abs/2310.09263,44.0,,Likely the best model for the table-tasks the authors tested on. No standard benchmark though.,175000000000.0,,,,,"Fine-tuning data was synthesized examples of tables:

""In our default settings, we use a total of 14 types of table-tasks, listed as T-5 to T-18 in Table 2, as training data for table-tuning.
In all but two task-types (T-6: Entity Matching and T-12: NL-toSQL), we use synthesized instances of table-tasks. For each task type, we generate 1000 instances of table-tasks using a 50:50 mix of zero-shot and few-shot templates, following a synthesis-thenaugment approach described in Section 4""

14000 instances in total",,,,,,,Confident,"Language models, such as GPT-3.5 and ChatGPT, demonstrate remarkable abilities to follow diverse human instructions and perform a wide range of tasks. However, when probing language models using a range of basic table-understanding tasks, we observe that today's language models are still sub-optimal in many table-related tasks, likely because they are pre-trained predominantly on \emph{one-dimensional} natural-language texts, whereas relational tables are \emph{two-dimensional} objects.
In this work, we propose a new ""\emph{table-tuning}"" paradigm, where we continue to train/fine-tune language models like GPT-3.5 and ChatGPT, using diverse table-tasks synthesized from real tables as training data, with the goal of enhancing language models' ability to understand tables and perform table tasks. We show that our resulting Table-GPT models demonstrate (1) better \emph{table-understanding} capabilities, by consistently outperforming the vanilla GPT-3.5 and ChatGPT, on a wide-range of table tasks, including holdout unseen tasks, and (2) strong \emph{generalizability}, in its ability to respond to diverse human instructions to perform new table-tasks, in a manner similar to GPT-3.5 and ChatGPT.",,,Unreleased,"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational",GPT-3.5,,"They fine-tuned GPT-3.5 and ChatGPT-3.5 on roughly 14k examples of tables. Not sure how many tokens per instance.

""Training tasks and data. In our default settings, we use a total of
14 types of table-tasks, listed as T-5 to T-18 in Table 2, as training
data for table-tuning.
In all but two task-types (T-6: Entity Matching and T-12: NL-toSQL), we use synthesized instances of table-tasks. For each task type, we generate 1000 instances of table-tasks using a 50:50 mix of zero-shot and few-shot templates, following a synthesis-thenaugment approach described in Section 4""",,,2024-11-01 10:05,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
RT-2-X,Robotics,Robotic manipulation,Google DeepMind,Open X-Embodiment Collaboration,2023-10-13,Open X-Embodiment: Robotic Learning Datasets and RT-X Models,https://arxiv.org/abs/2310.08864,255.0,SOTA improvement,"""Emergent skills evaluation. To investigate the transfer
of knowledge across robots, we conduct experiments with
the Google Robot, assessing the performance on tasks like
the ones shown in Fig. 5. These tasks involve objects and
skills that are not present in the RT-2 dataset but occur in the
Bridge dataset [95] for a different robot (the WidowX robot).
Results are shown in Table II, Emergent Skills Evaluation
column. Comparing rows (1) and (2), we find that RT-2-X
outperforms RT-2 by âˆ¼ 3Ã—, suggesting that incorporating
data from other robots into the training improves the range
of tasks that can be performed even by a robot that already
has large amounts of data available. Our results suggest that
co-training with data from other platforms imbues the RT-2-
X controller with additional skills for the platform that are
not present in that platformâ€™s original dataset.""

Top10 recent paper from Sebastian Sartor 2025-05-14",55000000000.0,55B,,,Open X-Embodiment,"""The Open X-Embodiment Dataset contains 1M+ real robot
trajectories spanning 22 robot embodiments, from single
robot arms to bi-manual robots and quadrupeds. The dataset
was constructed by pooling 60 existing robot datasets from
34 robotic research labs around the world and converting
them into a consistent data format for easy download and
usage. We use the RLDS data format [119], which saves data
in serialized tfrecord files and accommodates the various
action spaces and input modalities of different robot setups,
such as differing numbers of RGB cameras, depth cameras
and point clouds. It also supports efficient, parallelized data
loading in all major deep learning frameworks. For more
details about the data storage format and a breakdown of all
60 datasets, see robotics-transformer-x.github.io.""

""Note that the robotics data mixture used in our experiments includes 9 embodiments which is fewer than the entire Open X-Embodiment dataset
(22) â€“ the practical reason for this difference is that we have
continued to extend the dataset over time, and at the time
of the experiments, the dataset above represented all of the
data""",,,,,,,Confident,"Large, high-capacity models trained on diverse datasets have shown remarkable successes on efficiently tackling downstream applications. In domains from NLP to Computer Vision, this has led to a consolidation of pretrained models, with general pretrained backbones serving as a starting point for many applications. Can such a consolidation happen in robotics? Conventionally, robotic learning methods train a separate model for every application, every robot, and even every environment. Can we instead train generalist X-robot policy that can be adapted efficiently to new robots, tasks, and environments? In this paper, we provide datasets in standardized data formats and models to make it possible to explore this possibility in the context of robotic manipulation, alongside experimental results that provide an example of effective X-robot policies. We assemble a dataset from 22 different robots collected through a collaboration between 21 institutions, demonstrating 527 skills (160266 tasks). We show that a high-capacity model trained on this data, which we call RT-X, exhibits positive transfer and improves the capabilities of multiple robots by leveraging experience from other platforms. More details can be found on the project website this https URL.",,,Unreleased,"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France",RT-2,,"""RT-2-X is trained via
co-fine-tuning (similarly to the original RT-2 [9]), with an approximately one to one split of the original VLM data
and the robotics data mixture. N""

RT-2 is in turn a fine-tune of Pali-X 55B",,,2025-06-11 21:39,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
Jiutian,Language,Language modeling/generation,China Mobile,,2023-10-12,,https://www.globaltimes.cn/page/202310/1299716.shtml,,,,13900000000.0,"A 13.9B parameter model is mentioned prominently at
https://jiutian.10086.cn/portal/#/home
2025-01-13.",1.668e+23,6*13.9e9*2e12=1.668e23,,,2000000000000.0,"""Designed to enhance efficiency, the model has trained over 2 trillion tokens""",,,,,Likely,"China Mobile, the largest telecom operator in the world by subscribers, unveiled its ""Jiutian"" artificial intelligence (AI) large-scale model on Thursday, which has reportedly won support from large enterprises including China Ocean Shipping (Group) Co and China Railway Construction Co.",,,Open weights (unrestricted),China,,,,,,2025-05-23 13:47,,,,,,Industry,,,,,Unreleased,"seems like it is the same model as this one under Apache 2.0 license:
https://modelscope.cn/models/JiuTian-AI/JIUTIAN-139MoE-chat",Industry,,,,,Operation counting,,,,
Ferret (13B),"Multimodal,Language,Vision","Object recognition,Language modeling","Columbia University,Apple","Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang, Yinfei Yang",2023-10-11,Ferret: Refer and Ground Anything Anywhere at Any Granularity,https://arxiv.org/abs/2310.07704,193.0,SOTA improvement,"claimed SOTA on a new benchmark ""To evaluate this new capability, we introduce Ferret-Bench, covering three new types of tasks: Referring Description, Referring Reasoning, and Grounding in Conversation. We benchmark existing MLLMs and observe that Ferret can outperform the best of them by 20.4% on average.""",13000000000.0,13B,,"Fine-tuned from Vicuna-13B, which we don't have an estimate for. Finetuning cost is ~4e19.

""Training Details. We initialize the image encoder with CLIP-ViT-L/14@336p, the LLM with Vicuna, and the projection layer with LLaVAâ€™s first-stage weights, leaving the visual sampler randomly initialized. After the initialization, Ferret is trained on the aforementioned GRIT data for three epochs, optimized by Loshchilov & Hutter (2017) with a learning rate of 2e âˆ’ 5 and a batch size of 128. The training takes âˆ¼5/2.5 days on 8 A100 GPU for a Ferret-13B/7B.""

5 * 24 * 3600 * 0.3 utilization (assumption) * 312 TFLOP/s = 4.04e19",GRIT,"""In order to make the refer-and-ground capability in Ferret open-vocabulary, instruction-following, and robust, we collect GRIT, a Ground-and-Refer Instruction-Tuning dataset with 1.1M samples. GRIT contains multiple levels of spatial knowledge, covering objects, relationships, region descriptions, and complex reasoning. It includes both text-in location-out (grounding) and location-in textout (referring) data, as well as data that mixes location and text in both input and output""",,,120.0,"""The training takes âˆ¼5/2.5 days on 8 A100 GPU for a Ferret-13B/7B.""",NVIDIA A100,,Confident,"We introduce Ferret, a new Multimodal Large Language Model (MLLM) capable of understanding spatial referring of any shape or granularity within an image and accurately grounding open-vocabulary descriptions. To unify referring and grounding in the LLM paradigm, Ferret employs a novel and powerful hybrid region representation that integrates discrete coordinates and continuous features jointly to represent a region in the image. To extract the continuous features of versatile regions, we propose a spatial-aware visual sampler, adept at handling varying sparsity across different shapes. Consequently, Ferret can accept diverse region inputs, such as points, bounding boxes, and free-form shapes. To bolster the desired capability of Ferret, we curate GRIT, a comprehensive refer-and-ground instruction tuning dataset including 1.1M samples that contain rich hierarchical spatial knowledge, with 95K hard negative data to promote model robustness. The resulting model not only achieves superior performance in classical referring and grounding tasks, but also greatly outperforms existing MLLMs in region-based and localization-demanded multimodal chatting. Our evaluations also reveal a significantly improved capability of describing image details and a remarkable alleviation in object hallucination.",3.0,,Open weights (non-commercial),"United States of America,United States of America",Vicuna-13B v0,40400000000000000000,"""The training takes ~5 days on 8 A100 GPU for a Ferret-13B""

5 * 24 * 3600 * 0.3 utilization (assumption) * 312 TFLOP/s = 4.04e19",8.0,,2025-05-28 16:04,,,,,,"Academia,Industry",,,,960.0,Open (non-commercial),"https://github.com/apple/ml-ferret?tab=License-1-ov-file#readme
confusingly, the license page in the repo is permissive and MIT-like, but the README says ""The data, and code is intended and licensed for research use only. They are also restricted to uses that follow the license agreement of LLaMA, Vicuna and GPT-4. The dataset is CC BY NC 4.0 (allowing only non-commercial use) and models trained using the dataset should not be used outside of research purposes."" 
train script: https://github.com/apple/ml-ferret/blob/main/experiments/ferret_13b_train.sh ","Academia,Industry",,,BF16,6352.420700106825,,,,,
Mistral 7B,Language,"Code generation,Language generation,Language modeling/generation,Question answering",Mistral AI,"Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, LÃ©lio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, TimothÃ©e Lacroix, William El Sayed",2023-10-10,Mistral 7B,https://arxiv.org/abs/2310.06825,1244.0,,frequently downloaded: https://huggingface.co/mistralai/Mistral-7B-v0.1 ,7000000000.0,,,,Unspecified unreleased,"""Unfortunately we're unable to share details about the training and the datasets (extracted from the open Web) due to the highly competitive nature of the field.""",,,,,,,Confident,"We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses the Llama 2 13B -- Chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license.",,,Open weights (unrestricted),France,,,,,,2025-04-30 14:44,,,,,,Industry,,,,,Unreleased,apache 2.0,Industry,,,,,,mistralai,,,
CodeFuse-13B,Language,Code generation,Ant Group,"Peng Di, Jianguo Li, Hang Yu, Wei Jiang, Wenting Cai, Yang Cao, Chaoyu Chen, Dajun Chen, Hongwei Chen, Liang Chen, Gang Fan, Jie Gong, Zi Gong, Wen Hu, Tingting Guo, Zhichao Lei, Ting Li, Zheng Li, Ming Liang, Cong Liao, Bingchang Liu, Jiachen Liu, Zhiwei Liu, Shaojun Lu, Min Shen, Guangpei Wang, Huan Wang, Zhi Wang, Zhaogui Xu, Jiawei Yang, Qing Ye, Gehao Zhang, Yu Zhang, Zelin Zhao, Xunjin Zheng, Hailian Zhou, Lifu Zhu, Xianying Zhu",2023-10-10,CodeFuse-13B: A Pretrained Multi-lingual Code Large Language Model,https://arxiv.org/abs/2310.06266,3.0,,"""The results demonstrate that CodeFuse-13B achieves a HumanEval pass@1 score of 37.10%, positioning it as one of the top multi-lingual code LLMs with similar parameter sizes""",13000000000.0,,3.09e+23,"""CodeFuse-13B was trained using 512 Nvidia A100 GPU cards, with
a Hardware FLOPs Utilization (HFU) of approximately 60%. The
training process took approximately 40 days to complete."" Later they state utilization of 56%

512 * 312 trillion * 40 * 24 * 3600 * 0.56 = 3.09e23

Using params*tokens, we have 13 billion * 1 trillion * 6 = 7.8e22. might be a sign of multiple epochs? 1T is the size of the dataset; they don't clearly state the number of training tokens","The Stack,GitHub","80% code, 10% English, 10% Chinese: ""The pre-training data for CodeFuse consists of 196TB of code, 1.75TB of Chinese raw data, and 1.7TB of English raw data, totaling 200TB, that are tokenized into 800 billion
tokens of code, 100 billion tokens of Chinese corpus, and 100 billion
tokens of English corpus (see Section 3.1).""

""We collected about 200+ TB of code-related data, and finally refined it to around 1.6TB (1T Token) of clean data suitable for pre-training.""",1000000000000.0,"1T tokens, mostly code but some Chinese/English",960.0,~40 days,NVIDIA A100 SXM4 80 GB,,Confident,"Code Large Language Models (Code LLMs) have gained significant attention in the industry due to their wide applications in the full lifecycle of software engineering. However, the effectiveness of existing models in understanding non-English inputs for multi-lingual code-related tasks is still far from well studied. This paper introduces CodeFuse-13B, an open-sourced pre-trained code LLM. It is specifically designed for code-related tasks with both English and Chinese prompts and supports over 40 programming languages. CodeFuse achieves its effectiveness by utilizing a high quality pre-training dataset that is carefully filtered by program analyzers and optimized during the training process. Extensive experiments are conducted using real-world usage scenarios, the industry-standard benchmark HumanEval-x, and the specially designed CodeFuseEval for Chinese prompts. To assess the effectiveness of CodeFuse, we actively collected valuable human feedback from the AntGroup's software development process where CodeFuse has been successfully deployed. The results demonstrate that CodeFuse-13B achieves a HumanEval pass@1 score of 37.10%, positioning it as one of the top multi-lingual code LLMs with similar parameter sizes. In practical scenarios, such as code generation, code translation, code comments, and testcase generation, CodeFuse performs better than other models when confronted with Chinese prompts.",,,Open weights (unrestricted),China,,,,512.0,0.56,2025-05-16 10:30,,,,16777216.0,"4096 batch size, 4096 sequence length",Industry,,,,,Unreleased,apache: https://github.com/codefuse-ai/codefuse-chatbot?tab=License-1-ov-file#readme,Industry,,"""we achieved 180 TFLOPS and 56% average utilization rate of tensor cores on 512 GPUs.""",,406563.97863021225,Hardware,,,,
FinGPT-13B,Language,"Named entity recognition,Sentiment classification,Language modeling/generation","University of California Los Angeles (UCLA),Columbia University,New York University (NYU)","Neng Wang, Hongyang Yang, Christina Dan Wang",2023-10-07,FinGPT: Instruction Tuning Benchmark for Open-Source Large Language Models in Financial Datasets,https://arxiv.org/abs/2310.04793; https://github.com/AI4Finance-Foundation/FinGPT,33.0,SOTA improvement,SOTA for financial sentiment analysis,13000000000.0,"Finetunes using LoRA, so only trains 3.67 million parameters",1.6e+23,From Llama 2-13B,,Financial sentiment data (for fine-tuning): https://huggingface.co/datasets/FinGPT/fingpt-sentiment-train,,,17.3,https://github.com/AI4Finance-Foundation/FinGPT?tab=readme-ov-file,NVIDIA GeForce RTX 3090,Supervised,Likely,"In the swiftly expanding domain of Natural Language Processing (NLP), the potential of GPT-based models for the financial sector is increasingly evident. However, the integration of these models with financial datasets presents challenges, notably in determining their adeptness and relevance. This paper introduces a distinctive approach anchored in the Instruction Tuning paradigm for open-source large language models, specifically adapted for financial contexts. Through this methodology, we capitalize on the interoperability of open-source models, ensuring a seamless and transparent integration. We begin by explaining the Instruction Tuning paradigm, highlighting its effectiveness for immediate integration. The paper presents a benchmarking scheme designed for end-to-end training and testing, employing a cost-effective progression. Firstly, we assess basic competencies and fundamental tasks, such as Named Entity Recognition (NER) and sentiment analysis to enhance specialization. Next, we delve into a comprehensive model, executing multi-task operations by amalgamating all instructional tunings to examine versatility. Finally, we explore the zero-shot capabilities by earmarking unseen tasks and incorporating novel datasets to understand adaptability in uncharted terrains. Such a paradigm fortifies the principles of openness and reproducibility, laying a robust foundation for future investigations in open-source financial large language models (FinLLMs).",,,Open weights (unrestricted),"United States of America,United States of America,United States of America",Llama 2-13B,653248800000000000,"fine-tuned Llama 2 13B

RTX 3090 for 17 hours, at a cost of $17

35.5 trillion flops * 17 * 3600 * 0.3 = 6.532488e+17",1.0,,2025-05-16 10:30,,,,,,"Academia,Academia,Academia",,,,,Open source,"MIT license (though probably subject to Llama 2 license too)
https://github.com/AI4Finance-Foundation/FinGPT/blob/master/LICENSE

train code: https://github.com/AI4Finance-Foundation/FinGPT/blob/master/fingpt/FinGPT_Benchmark/train.sh ","Academia,Academia,Academia",,,FP16,381.7900590727425,Hardware,,,,
NAEPro,Biology,Protein design,"University of California Santa Barbara (UCSB),Massachusetts Institute of Technology (MIT),Carnegie Mellon University (CMU)","Zhenqiao Song, Yunlong Zhao, Wenxian Shi, Yang Yang, Lei Li",2023-10-06,Functional Geometry Guided Protein Sequence and Backbone Structure Co-Design,https://arxiv.org/abs/2310.04343,,,,,,,"
",,,1552122.0,"Î²-lactamase calculation: 5,427 proteins Ã— 286 residues = 1,552,122 residues
Myoglobin calculation: 3,381 proteins Ã— 153 residues = 517,143 residues
Total datapoints: 1,552,122 + 517,143 = 2,069,265 residues (â‰ˆ 2.1e6)
- These are two separate models, using Î²-lactamase",,,NVIDIA RTX A6000,,Confident,"Proteins are macromolecules responsible for essential functions in almost all living organisms. Designing reasonable proteins with desired functions is crucial. A proteinâ€™s sequence and structure are strongly correlated and they together determine its function. In this paper, we propose NAEPro, a model to jointly design Protein sequence and structure based on automatically detected functional and conserved sites. NAEPro is powered by an interleaving network of attention and equivariant layers, which can capture global correlation in a whole sequence and local influence from nearest amino acids in three dimensional (3D) space. Such an architecture facilitates effective yet economic message passing at two levels. We evaluate our model and several strong baselines on two protein datasets, Î²-lactamase and myoglobin. Experimental results show that our model achieves the highest binding affinity scores among the top-5, top-10 and top-30 candidates. These findings prove the capability of our model to design functional proteins. Furthermore, in-depth analysis further confirms our modelâ€™s ability to generate highly effective proteins capable of binding to their target metallocofactors1.",100.0,,,"United States of America,United States of America,United States of America",,,,1.0,,2025-05-01 10:42,,,,,,"Academia,Academia,Academia",,,,,,,"Academia,Academia,Academia",,,,327.2559097645607,Hardware,,,,
FoldFlow,Biology,Protein generation,"McGill University,Mila - Quebec AI (originally Montreal Institute for Learning Algorithms),Dreamfold,University of Montreal / UniversitÃ© de MontrÃ©al,University of Oxford","Avishek Joey Bose, Tara Akhound-Sadegh, Guillaume Huguet, Kilian Fatras, Jarrid Rector-Brooks, Cheng-Hao Liu, Andrei Cristian Nica, Maksym Korablyov, Michael Bronstein, Alexander Tong",2023-10-03,SE(3) Stochastic Flow Matching for Protein Backbone Generation,https://arxiv.org/abs/2310.02391,41.0,,,,,1.1000000000000008e+20,"1. Hardware: 4x NVIDIA A100-80GB GPUs (3.12e14 FLOP/s per GPU)
2. Training duration: 2.5 days = 216,000 seconds (directly provided)
3. Utilization rate: 40%
4. Calculation: 3.12e14 FLOP/s Ã— 4 GPUs Ã— 216,000s Ã— 0.40 = 1.1e20 FLOP",,,16000001.0,"PDB Dataset: 22,248 proteins Ã— 200 residues = 4,449,600 tokens
MD Dataset: 200,000 frames Ã— 58 residues = 11,600,000 tokens
Total: 4,449,600 + 11,600,000 = 16,049,600 tokens (1.6 Ã— 10â·)",60.0,,NVIDIA A100,,Confident,"The computational design of novel protein structures has the potential to impact numerous scientific disciplines greatly. Toward this goal, we introduce FOLDFLOW a series of novel generative models of increasing modeling power based on the flow-matching paradigm over 3D rigid motionsâ€”i.e. the group SE(3)â€”enabling accurate modeling of protein backbones. We first introduce FOLDFLOW-BASE a simulation-free approach to learning deterministic continuous-time dynamics and matching invariant target distributions on SE(3). We next accelerate training by incorporating Riemannian optimal transport to create FOLDFLOW-OT leading to the construction of both more simple and stable flows. Finally, we design FOLDFLOW-SFM coupling both Riemannian OT and simulation-free training to learn stochastic continuous-time dynamics over SE(3). Our family of FOLDFLOW generative models offers several key advantages over previous approaches to the generative modeling of proteins: they are more stable and faster to train than diffusion-based approaches, and our models enjoy the ability to map any invariant source distribution to any invariant target distribution over SE(3). Empirically, we validate FOLDFLOW on protein backbone generation of up to 300 amino acids leading to high-quality designable, diverse, and novel samples.",,,,"Canada,Canada,Canada,Canada,United Kingdom of Great Britain and Northern Ireland",,,,4.0,,2025-05-01 10:42,,,,,,"Academia,Academia,Industry,Academia,Academia",,,,,,,"Academia,Academia,Industry,Academia,Academia",,,,3176.776258121856,Hardware,,,,
LLaMA-7B (protein-oriented instruction-tuned),"Language,Biology","Protein folding prediction,Protein generation,Other Biological Modeling,Protein or nucleotide language model (pLM/nLM),Protein question answering",Zhejiang University (ZJU),"Yin Fang, Xiaozhuan Liang, Ningyu Zhang, Kangwei Liu, Rui Huang, Zhuo Chen, Xiaohui Fan, Huajun Chen",2023-10-02,Mol-Instructions: A Large-Scale Biomolecular Instruction Dataset for Large Language Models,https://arxiv.org/abs/2306.08018,54.0,,,7000000000.0,In the name,2.78e+22,"Estimate 1: 1T tokens * 6.7B parameters * 6 FLOP/token/parameter = 4e22 FLOP

from paper, Llama-7B took 82,432 GPU hours using A100s
Estimate 2: 312 trillion FLOP/s * (82,432 * 3600) s * 0.3 = 2.78e22 FLOP",Mol instructions,"""we introduce Mol-Instructions, a comprehensive instruction dataset designed for the biomolecular domain. Mol-Instructions encompasses three key components: molecule-oriented instructions, protein-oriented instructions, and biomolecular text instructions""",204000001.0,"Data estimate summary:
* Instructions: 2,043,587
* Tokens per instruction: 100 
* Total = 2,043,587 Ã— 100 = 2.04e8 tokens",,,,,Confident,"Large Language Models (LLMs), with their remarkable task-handling capabilities and innovative outputs, have catalyzed significant advancements across a spectrum of fields. However, their proficiency within specialized domains such as biomolecular studies remains limited. To address this challenge, we introduce MolInstructions, a comprehensive instruction dataset designed for the biomolecular domain. Mol-Instructions encompasses three key components: molecule-oriented instructions, protein-oriented instructions, and biomolecular text instructions. Each component aims to improve the understanding and prediction capabilities of LLMs concerning biomolecular features and behaviors. Through extensive instruction tuning experiments on LLMs, we demonstrate the effectiveness of Mol-Instructions in enhancing large modelsâ€™ performance in the intricate realm of biomolecular studies, thus fostering progress in the biomolecular research community. Mol-Instructions is publicly available for ongoing research and will undergo regular updates to enhance its applicability.",,,Open weights (unrestricted),China,LLaMA-7B,,,,,2025-05-01 10:42,,,,,,Academia,,,,,Unreleased,"MIT. includes data
https://github.com/zjunlp/Mol-Instructions",Academia,,,,,"Hardware,Operation counting",,,,
MiniGPT4 (Vicuna finetune),"Language,Vision,Multimodal","Language modeling/generation,Chat,Visual question answering,Image captioning",King Abdullah University of Science and Technology (KAUST),"Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, Mohamed Elhoseiny",2023-10-02,MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models,https://arxiv.org/abs/2304.10592,,,,13000000000.0,13B as Vicuna,,,"LAION,Conceptual Captions 12M (CC12M),SBU",,,"""We train MiniGPT-4 with two stages. The first traditional pretraining stage is trained using roughly 5 million aligned image-text pairs""

""we propose a novel way to create high-quality image-text pairs by the model itself and ChatGPT together. Based on this, we then create a small (3500 pairs in total) yet high-quality dataset.
The second finetuning stage is trained on this dataset in a conversation template to significantly improve its generation reliability and overall usability.""",10.0,,NVIDIA A100,,Confident,"The recent GPT-4 has demonstrated extraordinary multi-modal abilities, such as directly generating websites from handwritten text and identifying humorous elements within images. These features are rarely observed in previous vision-language models. However, the technical details behind GPT-4 continue to remain undisclosed. We believe that the enhanced multi-modal generation capabilities of GPT-4 stem from the utilization of sophisticated large language models (LLM). To examine this phenomenon, we present MiniGPT-4, which aligns a frozen visual encoder with a frozen advanced LLM, Vicuna, using one projection layer. Our work, for the first time, uncovers that properly aligning the visual features with an advanced large language model can possess numerous advanced multi-modal abilities demonstrated by GPT-4, such as detailed image description generation and website creation from hand-drawn drafts. Furthermore, we also observe other emerging capabilities in MiniGPT-4, including writing stories and poems inspired by given images, teaching users how to cook based on food photos, and so on. In our experiment, we found that the model trained on short image caption pairs could produce unnatural language outputs (e.g., repetition and fragmentation). To address this problem, we curate a detailed image description dataset in the second stage to finetune the model, which consequently improves the model's generation reliability and overall usability. Our code, pre-trained model, and collected dataset are available at this https URL.",,,Open weights (unrestricted),Saudi Arabia,Vicuna-13B v0,13478400000000000000,"Our MiniGPT-4 only requires training approximately 10 hours on 4 A100 GPUs

4*312000000000000 peak FLOPs * 10*3600*0.3=1.34784e+19",4.0,,2024-12-02 10:20,,,,,,Academia,,,,,Open source,"BSD-3 Clause
https://github.com/Vision-CAIR/MiniGPT-4
https://minigpt-4.github.io/",Academia,,,,3176.847003719534,Hardware,,,,
Phi-1,Language,"Language modeling/generation,Code generation",Microsoft Research,"Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio CÃ©sar Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, SÃ©bastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, Yuanzhi Li",2023-10-02,Textbooks Are All You Need,https://arxiv.org/abs/2306.11644,,,,1300000000.0,"1.3B
The architecture for our 1.3B parameter phi-1 model consists of 24 layers, hidden dimension of 2048, MLP-inner dimension of 8192, and 32 attention heads of dimension 64 each.",3.3234195e+20,"6ND = 6 *1.3*10^9 parameters * 51 * 10^9 tokens = 3.978e+20

312000000000000 FLOP/s * 8 GPUs *103 hours *3600 sec / hour *0.3 [assumed utilization] = 2.7765504e+20

geometric mean sqrt(2.7765504e+20*3.978e+20) = 3.3234195e+20
","The Stack,StackOverflow","selection of ``textbook quality"" data from the web (6B tokens) and synthetically generated textbooks and exercises with GPT-3.5 (1B tokens). ",7180000000.0,"A filtered code-language dataset, which is a subset of The Stack and StackOverflow, obtained by using a language model-based classifier (consisting of about 6B tokens).
â€¢ A synthetic textbook dataset consisting of <1B tokens of GPT-3.5 generated Python textbooks.
â€¢ A small synthetic exercises dataset consisting of âˆ¼180M tokens of Python exercises and solutions

For the 1.3B models, phi-1 and phi-1-base are checkpoints
after training on 51B tokens (770 GPU hours)

Training tokens: 54B tokens (7B unique tokens)",103.0,"4 days on 8 A100s

""Finetuning to obtain phi-1 used an additional 7 hours on the same hardware""

4*24 + 7 =103 hours",NVIDIA A100,,Confident,"We introduce phi-1, a new large language model for code, with significantly smaller size than competing models: phi-1 is a Transformer-based model with 1.3B parameters, trained for 4 days on 8 A100s, using a selection of ``textbook quality"" data from the web (6B tokens) and synthetically generated textbooks and exercises with GPT-3.5 (1B tokens). Despite this small scale, phi-1 attains pass@1 accuracy 50.6% on HumanEval and 55.5% on MBPP. It also displays surprising emergent properties compared to phi-1-base, our model before our finetuning stage on a dataset of coding exercises, and phi-1-small, a smaller model with 350M parameters trained with the same pipeline as phi-1 that still achieves 45% on HumanEval.",7.3,,Open weights (unrestricted),"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational",,,,8.0,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,"https://huggingface.co/microsoft/phi-1

MIT license
",Industry,,,,6353.694007439068,"Operation counting,Hardware",microsoft,,,
CTM (CIFAR-10),Image generation,"Image generation,Text-to-image","Stanford University,Sony","Dongjun Kim, Chieh-Hsin Lai, Wei-Hsiang Liao, Naoki Murata, Yuhta Takida, Toshimitsu Uesaka, Yutong He, Yuki Mitsufuji, Stefano Ermon",2023-10-01,Consistency Trajectory Models: Learning Probability Flow ODE Trajectory of Diffusion,https://arxiv.org/abs/2310.02279v1,108.0,SOTA improvement,"""CTM... achieves new state-of-the-art FIDs for single-step diffusion model sampling on CIFAR-10 (FID 1.73)""",,,,"Almost certainly <1e23 FLOP due to the small scale experiments.

""We use 4Ã—V100 (16G) GPUs for CIFAR-10 experiment""
100K training iterations
",CIFAR-10,,60000.0,100K training iterations / 60K images in the training dataset = 1.7 epochs,,,NVIDIA V100,,Unknown,"Consistency Models (CM) (Song et al., 2023) accelerate score-based diffusion model sampling at the cost of sample quality but lack a natural way to trade-off quality for speed. To address this limitation, we propose Consistency Trajectory Model (CTM), a generalization encompassing CM and score-based models as special cases. CTM trains a single neural network that can -- in a single forward pass -- output scores (i.e., gradients of log-density) and enables unrestricted traversal between any initial and final time along the Probability Flow Ordinary Differential Equation (ODE) in a diffusion process. CTM enables the efficient combination of adversarial training and denoising score matching loss to enhance performance and achieves new state-of-the-art FIDs for single-step diffusion model sampling on CIFAR-10 (FID 1.73) and ImageNet at 64X64 resolution (FID 2.06). CTM also enables a new family of sampling schemes, both deterministic and stochastic, involving long jumps along the ODE solution trajectories. It consistently improves sample quality as computational budgets increase, avoiding the degradation seen in CM. Furthermore, CTM's access to the score accommodates all diffusion model inference techniques, including exact likelihood computation.",1.7,,Open weights (unrestricted),"United States of America,Japan",,,,4.0,,2025-06-02 12:24,,,,128.0,,"Academia,Industry",,,,,Open source,"MIT license
https://github.com/Kim-Dongjun/ctm-cifar10
https://drive.google.com/drive/folders/1ei4PLmTrAlj-j_yUfLqXSpI5OOIqDlgv","Academia,Industry",,,FP16,2382.6883131695167,,,,,
TinyLlama-1.1B (3T token checkpoint),Language,"Chat,Language modeling/generation,Translation,Question answering",Singapore University of Technology & Design,"Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, Wei Lu",2023-10-01,TinyLlama: An Open-Source Small Language Model,https://arxiv.org/abs/2401.02385,219.0,,,1100000000.0,1.1B,2.173796352e+22,"6ND approximation: 6*1.1B * 3T = 19800000000000000000000
Extrapolation from the 1T checkpoint:
flops = (16) * (312 * 10**12) * (3 * 30 * 24 * 3600) * (0.56) = 7245987840000001048576
(num gpu) * (peak flops) * (time in seconds) * (reported utilization rate)

source: https://github.com/jzhang38/TinyLlama
""Thanks to those optimizations, we achieve a throughput of 24k tokens per second per A100-40G GPU, which translates to 56% model flops utilization""
and Releases Schedule from the same link","SlimPajama,StarCoderData",Slimpajama & Starcoderdata from Training Details from https://github.com/jzhang38/TinyLlama,1000000000000.0,1T tokens checkpoint so around 0.75T words,2160.0,"1T checkpoint was released after 1 month. Assume the 3T checkpoint took 3 months.

source: https://github.com/jzhang38/TinyLlama",NVIDIA A100 SXM4 40 GB,Self-supervised learning,Confident,"We present TinyLlama, a compact 1.1B language model pretrained on around 1 trillion tokens for approximately 3 epochs. Building on the architecture and tokenizer of Llama 2, TinyLlama leverages various advances contributed by the open-source community (e.g., FlashAttention), achieving better computational efficiency. Despite its relatively small size, TinyLlama demonstrates remarkable performance in a series of downstream tasks. It significantly outperforms existing open-source language models with comparable sizes. Our model checkpoints and code are publicly available on GitHub at https://github.com/jzhang38/TinyLlama",3.0,,Open weights (unrestricted),Singapore,,,,16.0,0.56,2025-05-09 11:32,,,,2000000.0,,Academia,,,,34560.0,Open source,apache 2.0: https://github.com/jzhang38/TinyLlama,Academia,,"Per https://github.com/jzhang38/TinyLlama: ""we achieve a throughput of 24k tokens per second per A100-40G GPU, which translates to 56% model flops utilization without activation checkpointing (We expect the MFU to be even higher on A100-80G)""",,12707.671003570757,"Hardware,Operation counting",,,,
BITTERS,Vision,Image captioning,"LG,Shutterstock","Taehoon Kim, Mark Marsden, Pyunghwan Ahn, Sangyun Kim, Sihaeng Lee, Alessandra Sala, Seung Hwan Kim",2023-10-01,Large-Scale Bidirectional Training for Zero-Shot Image Captioning,https://arxiv.org/abs/2211.06774,,,,650000000.0, BITTERS has 650 million parameters in total.,7.8015e+17,"WaveVAE: 
""We train the model for 10 epochs with batch size 3840.""
""We reduced the number of parameters to 25 million in total""

25000*100000000*6*10=1.5e+14

BiART:
""We train our model for 2 epochs in total with batch size 1280""
""BITTERS has 650 million parameters in total""

650000000*100000000*6*2=7.8e+17

7.8e+17+1.5e+14=7.8015e+17",ShutterStock,"All images are random sampled from Shutterstockâ€™s1
image catalog. Images are 500px on the longest side with a
single ground truth caption provided. A list of keywords is
also included for each image to enable keyword extraction
model training. All captions and keywords are in English
and were moderated for hate speech, slurs, and expletives",100000000.0,"we train BITTERS using a new, quality-controlled 100 million image dataset which we will refer to as Text Image Pairs 100 Million (TIP100M)

WaveVAE module
We also resize each image to 256 Ã— 256 Ã— 3 
We train the model for 3 epochs with a batch size of 480

Batch size: 1280
Number of updates: 2 epochs
Image resolution: 256x256
Sequence length: 64 text tokens, 1024 image tokens",,,,,Likely,"When trained on large-scale datasets, image captioning models can understand the content of images from a general domain but often fail to generate accurate, detailed captions. To improve performance, pretraining-and-finetuning has been a key strategy for image captioning. However, we find that large-scale bidirectional training between image and text enables zero-shot image captioning. In this paper, we introduce Bidirectional Image Text Training in largER Scale, BITTERS, an efficient training and inference framework for zero-shot image captioning. We also propose a new evaluation benchmark which comprises of high quality datasets and an extensive set of metrics to properly evaluate zero-shot captioning accuracy and societal bias. We additionally provide an efficient finetuning approach for keyword extraction. We show that careful selection of large-scale training set and model architecture is the key to achieving zero-shot image captioning.",,,,"Korea (Republic of),United States of America",,,,,,2025-02-14 16:05,,,,,,"Industry,Industry",,,,,,,"Industry,Industry",,,,,Operation counting,,,,
PIXART-Î±,Image generation,"Image generation,Text-to-image","Huawei Noah's Ark Lab,The University of Hong Kong,Hong Kong University of Science and Technology (HKUST)","Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, Zhenguo Li",2023-09-30,PIXART-Î±: Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis,"https://arxiv.org/abs/2310.00426
https://openreview.net/pdf?id=eAKmQPe3m1
https://github.com/PixArt-alpha/PixArt-alpha",,,,600000000.0,0.6B,1.541475e+21,"PixArt-Î± only takes 12% of Stable Diffusion v1.5's training time (753 vs. 6,250 A100 GPU days), saving nearly $300,000 ($28,000 vs. $320,000) and reducing 90% CO2 emissions. Moreover, compared with a larger SOTA model, RAPHAEL, our training cost is merely 1%.
To ensure fairness, we convert the V100 GPU days (1656) of our training to A100 GPU days (753)

they compare their compute with Imagen as 7132:753 (see Table 2)
Imagen compute was 1.4600000000000002e+22 FLOPS (from Epoch table) then PIXART-Î± is
(1.4600000000000002e+22/7132)*753=1.541475e+21 FLOPS (the most likely)

Another calculation: 

753 A100 GPU days (most likely A100 SXM4 80 GB)
753 days * 24 hours * 3600 s * 77970000000000 FLOPS/s (assume FP16) * 40% utilization rate = 2.0290663296e+21 FLOPS
753 days * 24 hours * 3600 s * 312000000000000 FLOPS/s (assume FP16 Tensor core) * 40% utilization rate = 8.11938816e+21 FLOPS (unlikely)

1656 V100 GPU days 
1656*24*3600*125000000000000 (assume Tensor float 32)*40%=7.15392e+21 FLOPS (unlikely)
1656*24*3600*31330000000000 (assume the most popular NVIDIA Tesla V100 DGXS 32 GB FP16)*40% = 1.7930585088e+21 FLOPS

The count of GPU days excludes the time for data labeling.","Segment Anything 1B,JourneyDB,ImageNet","we propose an autolabeling pipeline utilizing the state-of-the-art vision-language model (LLaVA (Liu et al., 2023)) to
generate captions on the SAM (Kirillov et al., 2023). Referencing in Section 2.4, the SAM dataset is
advantageous due to its rich and diverse collection of objects, making it an ideal resource for creating
high-information-density text-image pairs, more suitable for text-image alignment learning

In the third stage, we construct our training dataset by incorporating JourneyDB (Pan et al., 2023)
and a 10M internal dataset to enhance the aesthetic quality of generated images beyond realistic
photographs.",25000000.0,Table 2,,,NVIDIA V100,,Confident,"PixArt-Î± is a Transformer-based T2I diffusion model whose image generation quality is competitive with state-of-the-art image generators (e.g., Imagen, SDXL, and even Midjourney), and the training speed markedly surpasses existing large-scale T2I models, e.g., PixArt-Î± only takes 10.8% of Stable Diffusion v1.5's training time (675 vs. 6,250 A100 GPU days).",,,Open weights (unrestricted),"China,Hong Kong,China,Hong Kong,China",,,,,,2025-05-01 05:32,,,,,,"Industry,Academia,Academia",,,,,,copyleft license: https://github.com/PixArt-alpha/PixArt-alpha,"Industry,Academia,Academia",,,,,"Hardware,Comparison with other models",,,,
HiDream Foundation Model 1.0,"Video,Image generation,Vision,3D modeling","Video generation,Image-to-video,Text-to-video,Image generation,Text-to-image",HiDream,,2023-09-30,,"https://blog.csdn.net/Datawhale/article/details/133366053

https://www.sohu.com/a/722858298_490443",,,,6000000000.0,6B,,,Unspecified unreleased,,,,,,,,Likely,"On September 20, 2023, the General Artificial Intelligence Innovation and Development Conference was held in Hefei. Academician Mei Tao, founder and CEO of HiDream.ai Zhixiang Future, officially released the ""Intelligent Visual Multimodal Generative Large Model"" at the conference and demonstrated HiDream.ai's current basic model capabilities and application progress.

""Intelligent Visual Multimodal Generative Large Model"" has more than 6 billion parameters, which can realize multimodal content generation such as text, pictures, and videos, and realize accurate and controllable content generation through interactive generation technology; at the same time, relying on the powerful base capabilities of the multimodal basic model, it can quickly adapt to customer-specific field applications. Since the internal test was opened in August, the model has undergone multiple iterations and updates, aiming to provide users with a more optimized creation experience.",,,API access,China,,,,,,2025-05-22 16:00,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
StableLM-3B-4E1T,Language,Language generation,Stability AI,"Jonathan Tow, Marco Bellagente, Dakota Mahan, Carlos Riquelme Ruiz",2023-09-29,,https://stability.wandb.io/stability-llm/stable-lm/reports/StableLM-3B-4E1T--VmlldzoyMjU4?accessToken=u3zujipenkx5g7rtcj9qojjgxpconyjktjkli2po09nffrffdhhchq045vp0wyfo,,,,2795443200.0,,6.21e+22,"""StableLM-3B-4E1T was trained on the Stability AI cluster across 256 NVIDIA A100 40GB GPUs (AWS P4d instances). Training began on August 23, 2023, and took approximately 30 days to complete.""

256 * 30 * 24* 3600 * 312 trillion * 0.3 utilization (assumption) = 6.21e22

6ND = 6*2795443200*1000000000000*4 epochs = 6.7090637e+22","RefinedWeb,RedPajama-Data,The Pile","""The dataset is comprised of a filtered mixture of open-source large-scale datasets available on the HuggingFace Hub: Falcon RefinedWeb extract (Penedo et al., 2023), RedPajama-Data (Together Computer, 2023) and The Pile (Gao et al., 2020), both without the Books3 subset, and StarCoder (Li et al., 2023). The complete list is provided in Table 1.""",1000000000000.0,Trained on 1T tokens (~750B words),720.0,approximately 30 days,NVIDIA A100,,Confident,"StableLM-3B-4E1T is a 3 billion (3B) parameter language model pre-trained under the multi-epoch regime to study the impact of repeated tokens on downstream performance. Given prior success in this area (Taylor et al., 2022 and Tay et al., 2023), we train on 1 trillion (1T) tokens for 4 epochs following the observations of Muennighoff et al. (2023) in ""Scaling Data-Constrained Language Models"" in which they find ""training with up to 4 epochs of repeated data yields negligible changes to loss compared to having unique data."" Further inspiration for the token count is taken from ""Go smol or go home"" (De Vries, 2023), which suggests a 2.96B model trained for 2.85 trillion tokens achieves a similar loss to a Chinchilla compute-optimal 9.87B language model.",4.0,,Open weights (unrestricted),United Kingdom of Great Britain and Northern Ireland,,,,,,2024-10-26 12:37,,,,4194304.0,"""The batch size is set to 1024 (4,194,304 tokens).""",Industry,,,,,,"cc 4.0
https://creativecommons.org/licenses/by-sa/4.0/",Industry,,,,,Hardware,,,,
Wuerstchen,Image generation,"Text-to-image,Image generation","Technische Hochschule Ingolstadt,University of Montreal / UniversitÃ© de MontrÃ©al,Mila - Quebec AI (originally Montreal Institute for Learning Algorithms),Polytechnique Montreal,Wand Technologies","Pablo Pernias, Dominic Rampas, Mats L. Richter, Christopher J. Pal, Marc Aubreville",2023-09-29,Wuerstchen: An Efficient Architecture for Large-Scale Text-to-Image Diffusion Models,https://arxiv.org/abs/2306.00637,,,,1000000000.0,"1B
",8.2898899e+21,"Table 2: 24,602 GPU Hours A100

312*10^12 FLOP / sec / GPU * 24602 GPU - hours * 3600 sec / hour * 0.3 [assumed utilization] = 8.2898899e+21 FLOP",LAION,"""All stages were trained on subsets of the improved-aesthetic LAION-5B (Schuhmann et al., 2022) dataset.""",,,,,,,Confident,"We introduce WÃ¼rstchen, a novel architecture for text-to-image synthesis that combines competitive performance with unprecedented cost-effectiveness for large-scale text-to-image diffusion models. A key contribution of our work is to develop a latent diffusion technique in which we learn a detailed but extremely compact semantic image representation used to guide the diffusion process. This highly compressed representation of an image provides much more detailed guidance compared to latent representations of language and this significantly reduces the computational requirements to achieve state-of-the-art results. Our approach also improves the quality of text-conditioned image generation based on our user preference study. The training requirements of our approach consists of 24,602 A100-GPU hours - compared to Stable Diffusion 2.1's 200,000 GPU hours. Our approach also requires less training data to achieve these results. Furthermore, our compact latent representations allows us to perform inference over twice as fast, slashing the usual costs and carbon footprint of a state-of-the-art (SOTA) diffusion model significantly, without compromising the end performance. In a broader comparison against SOTA models our approach is substantially more efficient and compares favorably in terms of image quality. We believe that this work motivates more emphasis on the prioritization of both performance and computational accessibility.",,,Open weights (unrestricted),"Germany,Canada,Canada,Canada,United States of America",,,,,,2025-05-14 15:32,,,,,,"Academia,Academia,Academia,Academia,Industry",,,,,,"MIT license

https://huggingface.co/dome272/wuerstchen","Academia,Academia,Academia,Academia,Industry",,,,,Hardware,dome272,,,
GAIA-1,"Video,Vision,Multimodal,Language","Self-driving car,Video generation,Instruction interpretation",Wayve,"Anthony Hu, Lloyd Russell, Hudson Yeo, Zak Murez, George Fedoseev, Alex Kendall, Jamie Shotton, Gianluca Corrado",2023-09-29,GAIA-1: A Generative World Model for Autonomous Driving,https://arxiv.org/abs/2309.17080,,,,9000000000.0,"9B

""GAIA-1â€™s world model has 6.5 billion parameters""

""GAIA-1â€™s video decoder has 2.6 billion parameters""",1.1645338e+22,"312000000000000 FLOP / GPU / sec [A100 reported, bf16 assumed] * 34560 GPU-hours [see training time notes] * 3600 sec/ hour * 0.3 [assumed utilization] = 1.1645338e+22 FLOP

they also report compute of ~1*10^22 via a graph here: https://wayve.ai/thinking/scaling-gaia-1/",Unspecified unreleased,,,"""Our training dataset consists of 4,700 hours at 25Hz of proprietary driving data collected in London,
UK between 2019 and 2023. This corresponds to approximately 420M unique images""",,"""GAIA-1â€™s world model <..> was trained for 15 days on 64 NVIDIA A100s.""
""GAIA-1â€™s video decoder <..> was trained for 15 days on 32 NVIDIA A100s.""

15 days * 24 hours / day * (64+32) GPUs = 34560 GPU-hours",NVIDIA A100,,Confident,"Autonomous driving promises transformative improvements to transportation, but building systems capable of safely navigating the unstructured complexity of real-world scenarios remains challenging. A critical problem lies in effectively predicting the various potential outcomes that may emerge in response to the vehicle's actions as the world evolves.
To address this challenge, we introduce GAIA-1 ('Generative AI for Autonomy'), a generative world model that leverages video, text, and action inputs to generate realistic driving scenarios while offering fine-grained control over ego-vehicle behavior and scene features. Our approach casts world modeling as an unsupervised sequence modeling problem by mapping the inputs to discrete tokens, and predicting the next token in the sequence. Emerging properties from our model include learning high-level structures and scene dynamics, contextual awareness, generalization, and understanding of geometry. The power of GAIA-1's learned representation that captures expectations of future events, combined with its ability to generate realistic samples, provides new possibilities for innovation in the field of autonomy, enabling enhanced and accelerated training of autonomous driving technology.",,,Unreleased,United Kingdom of Great Britain and Northern Ireland,,,,,,2025-06-12 11:49,,,,,,Industry,,,,34560.0,Unreleased,,Industry,,,,,"Hardware,Reported",,,,
Amazon Titan,"Language,Image generation","Semantic search,Image generation,Language modeling/generation,Code generation,Chat,Text-to-image,Translation",Amazon,,2023-09-28,,https://aws.amazon.com/bedrock/titan/,,Training cost,,200000000000.0,"200B dense model
https://importai.substack.com/p/import-ai-365-wmd-benchmark-amazon",4.7999999999999996e+24,"trained using NVIDIA NeMo: https://blogs.nvidia.com/blog/nemo-amazon-titan/

13,760 NVIDIA A100 chips (using 1,720 P4d nodes). It took 48 days to train.
from https://importai.substack.com/p/import-ai-365-wmd-benchmark-amazon

counting operations: 6*200000000000*4000000000000=4.8e+24

gpu usage: 312000000000000(FLOP/s)*0.3*13760*1152*3600=5.3413281792e+24",,,4000000000000.0,"4T tokens of data, based on comments from amazon engineer James Hamilton at a 2024 talk: https://perspectives.mvdirona.com/2024/01/cidr-2024/
Also cited here:
https://lifearchitect.ai/titan/",1152.0,,NVIDIA A100,,Likely,,,,API access,United States of America,,,,13760.0,0.2696,2025-01-29 21:08,,,,,,Industry,,,9.899999999999999e+24,,Unreleased,,Industry,,"6ND gives 4.8e24
Training took 48 days on 13,760 NVIDIA A100 chips â€“> 3.12e14 * 13760 * 48 * 24 * 3600 = 1.78e25 FLOPs at full utilization
Implies 0.2696 hardware utilization.",,10929327.206416875,"Hardware,Operation counting",,,,3
PLaMo-13B,Language,"Language modeling/generation,Chat,Question answering",Preferred Networks Inc,"Preferred Networks, Inc",2023-09-28, PLaMo-13B,https://huggingface.co/pfnet/plamo-13b,,,,13000000000.0,,1.17e+23,"6ND = 6*13e9*1.5e12=1.17e+23
from https://huggingface.co/pfnet/plamo-13b#model-details

480 GPUs * 30 days [assumed, likely less] * 24 hours * 3600 s * 77970000000000 FLOP/s * 41.0 [reported utilization] = 3.9772934e+24


","C4,Project Gutenberg,RedPajama,mC4,Wikipedia (ja)",from https://huggingface.co/pfnet/plamo-13b#training-dataset,1500000000000.0,"Trained tokens: 1.5T tokens (English: 1.32T tokens, Japanese: 0.18T tokens)
from https://huggingface.co/pfnet/plamo-13b#model-details

0.75*1.32T + 0.18T = 1170000000000
0.75 words per token for English
1 for Japanese ",720.0,"""We used 60 ABCI A nodes (480 GPUs) for just under a month, and trained the training data with a total of 1.4T tokens with a context length of 4096.""

https://tech.preferred.jp/ja/blog/llm-plamo/",NVIDIA A100 SXM4 40 GB,Self-supervised learning,Confident,,,,Open weights (unrestricted),Japan,,,,480.0,0.41,2025-05-16 10:30,,,,,,Industry,,,,345600.0,Unreleased,Apache 2.0 for weights. Open data,Industry,,"Given in table 3, though note this doesn't quite agree with a manual calculation:
1.17e23 FLOP (per ops-counting method) / (720 (h) * 3600 (s/h) * 3.12e14 (FLOP/GPU-s) * 480 (GPUs)) = 0.3014
Plausibly this is due to our time assumption â€“ blog post just says ""less than a month"". 41% utilization would imply ~22 days of training.",,381255.6002238446,Operation counting,,,,
Show-1,Video,"Video generation,Text-to-video",National University of Singapore,"David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, Mike Zheng Shou",2023-09-27,Show-1: Marrying Pixel and Latent Diffusion Models for Text-to-Video Generation,https://arxiv.org/abs/2309.15818,,SOTA improvement,"""Our approach achieves state-of-the-art performance on standard benchmarks including UCF-101 and MSR-VTT.""",,,,,WebVid-10M,"""WebVid-10M is a large-scale dataset of short videos with textual descriptions sourced from stock footage sites. The videos are diverse and rich in their content. 10.7M video-caption pairs. 52K total video hours.""",,"WebVid-10M
10.7M video-caption pairs. 52K total video hours.",,,NVIDIA A100,,Unknown,"Significant advancements have been achieved in the realm of large-scale pre-trained text-to-video Diffusion Models (VDMs). However, previous methods either rely solely on pixel-based VDMs, which come with high computational costs, or on latent-based VDMs, which often struggle with precise text-video alignment. In this paper, we are the first to propose a hybrid model, dubbed as Show-1, which marries pixel-based and latent-based VDMs for text-to-video generation. Our model first uses pixel-based VDMs to produce a low-resolution video of strong text-video correlation. After that, we propose a novel expert translation method that employs the latent-based VDMs to further upsample the low-resolution video to high resolution. Compared to latent VDMs, Show-1 can produce high-quality videos of precise text-video alignment; Compared to pixel VDMs, Show-1 is much more efficient (GPU memory usage during inference is 15G vs 72G). We also validate our model on standard video generation benchmarks. Our code and model weights are publicly available at this https URL.",,,Open weights (non-commercial),Singapore,,,,,,2025-05-19 12:53,,,,,,Academia,,,,,Unreleased,"https://github.com/showlab/Show-1 don't see training code
Attribution-NonCommercial 4.0 International
",Academia,,,,,,,,,
Emu (Meta),Image generation,"Text-to-image,Image generation",Meta AI,"Xiaoliang Dai, Ji Hou, Chih-Yao Ma, Sam Tsai, Jialiang Wang, Rui Wang, Peizhao Zhang, Simon Vandenhende, Xiaofang Wang, Abhimanyu Dubey, Matthew Yu, Abhishek Kadian, Filip Radenovic, Dhruv Mahajan, Kunpeng Li, Yue Zhao, Vladan Petrovic, Mitesh Kumar Singh, Simran Motwani, Yi Wen, Yiwen Song, Roshan Sumbaly, Vignesh Ramanathan, Zijian He, Peter Vajda, Devi Parikh",2023-09-27,Emu: Enhancing Image Generation Models Using Photogenic Needles in a Haystack,https://ai.meta.com/research/publications/emu-enhancing-image-generation-models-using-photogenic-needles-in-a-haystack/,,,,2800000000.0,"""We use a large U-Net with 2.8B trainable parameters.""",,,,,1100000000.0,"""We curate a large internal pre-training dataset consisting of 1.1 billion images to train our model. The model is trained with progressively increasing resolutions""",,,,,Confident,"Training text-to-image models with web scale image-text
pairs enables the generation of a wide range of visual concepts from text. However, these pre-trained models often
face challenges when it comes to generating highly aesthetic images. This creates the need for aesthetic alignment
post pre-training. In this paper, we propose quality-tuning
to effectively guide a pre-trained model to exclusively generate highly visually appealing images, while maintaining
generality across visual concepts. Our key insight is that
supervised fine-tuning with a set of surprisingly small but
extremely visually appealing images can significantly improve the generation quality. We pre-train a latent diffusion
model on 1.1 billion image-text pairs and fine-tune it with
only a few thousand carefully selected high-quality images.
The resulting model, Emu, achieves a win rate of 82.9%
compared with its pre-trained only counterpart. Compared
to the state-of-the-art SDXLv1.0, Emu is preferred 68.4%
and 71.3% of the time on visual appeal on the standard
PartiPrompts and our Open User Input benchmark based
on the real-world usage of text-to-image models. In addition, we show that quality-tuning is a generic approach that
is also effective for other architectures, including pixel diffusion and masked generative transformer models.",,,Unreleased,United States of America,,,,,,2025-06-12 13:24,,,,,,Industry,checked,,,,Unreleased,,Industry,,,,,,,,,
InternLM-XComposer,"Multimodal,Language,Vision","Chat,Visual question answering,Translation",Shanghai AI Lab,"Pan Zhang, Xiaoyi Dong, Bin Wang, Yuhang Cao, Chao Xu, Linke Ouyang, Zhiyuan Zhao, Haodong Duan, Songyang Zhang, Shuangrui Ding, Wenwei Zhang, Hang Yan, Xinyue Zhang, Wei Li, Jingwen Li, Kai Chen, Conghui He, Xingcheng Zhang, Yu Qiao, Dahua Lin, Jiaqi Wang",2023-09-26,InternLM-XComposer: A Vision-Language Large Model for Advanced Text-image Comprehension and Composition,https://arxiv.org/abs/2309.15112,159.0,,Table 9 and Table 10 suggests that model is better in image selection from GPT4-V based on human preference,7000000000.0,7B from https://huggingface.co/internlm/internlm-xcomposer-vl-7b,5.28e+21,"""The training procedure employs a batch size of approximately 15.7 million tokens and spans 8,000 iterations""
6ND = 6 * 7B params * 15.7M tokens/iteration * 8000 iterations = 5.28e21 FLOP","MMC4 / Multimodal C4,Wanjuan,Conceptual Captions (CC3M),LAION-400M,Conceptual Captions 12M (CC12M),Unspecified unreleased,TaiSu","WanJuan, CC 3M, SBU-Caption, LAION400M, CC 12M, In-house Concept data, Multimodal C4, TaiSu, WuKong, LAION-CN - form Table 1",359300000000.0,"from appendix A.1  55.6B English tokens and 22.1B Chineese tokens and 1.1B images.

""images are resized to
a consistent dimension of 224 Ã— 224 and subsequently dissected into patches with a stride of 14. These patches serve
as input tokens""
There are 224^2 / 14^2 = 256 patches (tokens) per image.

So 55.6e9 + 22.1e9 + 256*1.1e9 = 359.3e9",80.0,from appendix A.1,NVIDIA A100,Supervised fine-tuning (SFT),Confident,"We propose InternLM-XComposer, a vision-language large model that enables advanced image-text comprehension and composition. The innovative nature of our model is highlighted by three appealing properties: 1) Interleaved Text-Image Composition: InternLM-XComposer can effortlessly generate coherent and contextual articles that seamlessly integrate images, providing a more engaging and immersive reading experience. Simply provide a writing instruction, and our system will generate the corresponding manuscript. It can intelligently identify the areas in the text where images would enhance the content and automatically insert the most appropriate visual candidates. 2) Comprehension with Rich Multilingual Knowledge: The text-image comprehension is empowered by training on an extensive multi-modal multilingual database with carefully crafted strategies, resulting in a deep understanding of visual content. 3) State-of-the-art Performance: Our model consistently achieves state-of-the-art results across various mainstream benchmarks for vision-language foundational models, including MME Benchmark, MMBench, MMBench-CN, Seed-Bench, CCBench (Chinese Cultural Benchmark), QBench and Tiny LVLM. Owing to the absence of established metrics for quantitatively assessing text-image composition, we have devised a robust evaluation procedure that comprises both human and GPT4-Vision (GPT4-V) to ensure reliability. Notably, our InternLM-XComposer achieves competitive text-image composition scores compared to public solutions, including GPT4-V and GPT3.5. Collectively, InternLM-XComposer seamlessly blends advanced text-image comprehension and composition, revolutionizing vision-language interaction and offering new insights and opportunities. The InternLM-XComposer model series are publicly available at this https URL. ",,,Open weights (restricted use),China,InternLM,3,"flops = (128) * (312 * 10**12) * (80 * 3600) * (0.3) = 3.45e21
(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)

from section A.1 we have 128xA100 used for 80 hours",128.0,0.4591,2025-05-09 11:32,,,,,,Academia,,,,10240.0,Open source,"code is apache 2.0
weights allow for free commercial use, apply for full commercial license
https://github.com/InternLM/InternLM-XComposer",Academia,,5.28e21 FLOP / (128 A100s * 80 hours * 60 * 60) / (312e12 FLOP/s/A100) ~= 0.4591,,101672.68833253944,"Hardware,Operation counting",,,,
GPT-4V,"Multimodal,Vision,Language","Language modeling,Visual question answering",OpenAI,,2023-09-25,GPT-4V(ision) system card,https://cdn.openai.com/papers/GPTV_System_Card.pdf,,Significant use,Incorporated into ChatGPT,,,,,Unspecified unreleased,"""The pre-trained model was first trained to predict the next word in a document, using a large dataset of text and image data from the Internet as well as licensed sources of data. It was then fine-tuned with additional data, using an algorithm called reinforcement learning from human feedback (RLHF),[8, 9] to produce outputs that are preferred by human trainers.""

The system card was published in September 2023, so I assume that this is the preview model, which has a knowledge cutoff date of April 2023 - which I assume means April 1, 2023 - according to https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models?tabs=global-standard%2Cstandard-chat-completions#gpt-35.",,,,,,Reinforcement learning,Unknown,"GPT-4 with vision (GPT-4V) enables users to instruct GPT-4 to analyze image inputs provided by the user, and is the latest capability we are making broadly available. Incorporating additional modalities (such as image inputs) into large language models (LLMs) is viewed by some as a key frontier in artificial intelligence research and development. Multimodal LLMs offer the possibility of expanding the impact of language-only systems with novel interfaces and capabilities, enabling them to solve new tasks and provide novel experiences for their users. In this system card, we analyze the safety properties of GPT-4V. Our work on safety for GPT-4V builds on the work done for GPT-4 and here we dive deeper into the evaluations, preparation, and mitigation work done specifically for image inputs.",,,API access,United States of America,GPT-4,,,,,2025-05-12 18:51,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
AlphaMissense,Biology,"Protein pathogenicity prediction,Protein folding prediction,Proteins",Google DeepMind,"Jun Cheng, Guido Novati, Joshua Pan, Clare Bycroft, Akvile Ì‡Å½emgulyte Ì‡, Taylor Applebaum, Alexander Pritzel, Lai Hong Wong, Michal Zielinski, Tobias Sargeant, Rosalia G. Schneider,Andrew W. Senior, John Jumper, Demis Hassabis, Pushmeet Kohli,Å½iga Avsec",2023-09-22,Accurate proteome-wide missense variant effect prediction with AlphaMissense,https://www.science.org/doi/10.1126/science.adg7492,425.0,SOTA improvement,"""By combining structural context and evolutionary conservation, our model achieves state-of-the-art results across a wide range of genetic and experimental benchmarks, all without explicitly training on such data."" [Abstract]",93000000.0,"""The model architecture is similar to that of AlphaFold (21), with minor modifications""
Reference is to the AlphaFold 2 paper; that model had 93 million parameters",,"From supplementary materials: ""We independently trained three AlphaFold models and fine-tuned them independently on variants. We followed the training procedure described in (21), (only the â€œInitial trainingâ€ stage) ... AF training is carried out for about 7e6 steps on single-chain structures ... Fine-tuning is carried out @until auROC of the evaluation set converges (about 350k samples, each training sample contains maximum 50 variants)""

Table S4 gives details. Total samples seen across the three pretraining models are (7.8M + 7.5M + 5.85M) = 21.15M

Each sequence is cropped to 256 elements long, which suggests 5.4B tokens seen in training.","MGnify,UniRef90","Supplemental materials section on training data lists sources:
75% of pre-training structures are self-distillation data sampled from MGnify and UniRef90.

Fine-tuning data on benign variants come from gnomAD v2.1.1 (1.25M variants), the Great Ape project (95k variants), and FigShare (2k variants).
Fine-tuning data for pathogenic variants are sampled from the missense proteome map to create a dataset with balanced positive and negative labels.
Suggests a total of 2.7M variants, each 256 long.",9000000.0,"7800000 samples - size of training dataset (see Table S4 in supplementary materials)
+1,345,605 variants for fine-tuning (but less could be used)  see Table S1

around 9000000 samples is quite confident estimation",,,,,Likely,"The vast majority of missense variants observed in the human genome are of unknown clinical significance. We present AlphaMissense, an adaptation of AlphaFold fine-tuned on human and primate variant population frequency databases to predict missense variant pathogenicity. By combining structural context and evolutionary conservation, our model achieves state-of-the-art results across a wide range of genetic and experimental benchmarks, all without explicitly training on such data. The average pathogenicity score of genes is also predictive for their cell essentiality, capable of identifying short essential genes that existing statistical approaches are underpowered to detect. As a resource to the community, we provide a database of predictions for all possible human single amino acid substitutions and classify 89% of missense variants as either likely benign or likely pathogenic.",4.0,,Unreleased,"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France",AlphaFold 2,,,,,2025-05-28 16:05,,,,,,Industry,,,,,Open source,"Apache for code. weights not released
https://github.com/google-deepmind/alphamissense",Industry,,,BF16,,,,,,
BTLM-3B,Language,"Language generation,Code generation",Cerebras Systems,"Nolan Dey, Daria Soboleva, Faisal Al-Khateeb, Bowen Yang, Ribhu Pathria, Hemant Khachane, Shaheer Muhammad, Zhiming (Charles)Chen, Robert Myers, Jacob Robert Steeves, Natalia Vassilieva, Marvin Tom, Joel Hestness",2023-09-20,BTLM-3B-8K: 7B Parameter Performance in a 3B Parameter Model,https://arxiv.org/abs/2309.11568,,,"SOTA for its parameter class: ""BTLM-3B-8K achieves state-of-the-art performance among 3B parameter models""",2600000000.0,"2.6B, per paper",9.8e+21,2.6b params * 627b tokens * 6 = 9.8e21,SlimPajama,"""To bolster BTLMâ€™s performance, we create a high quality
627B token dataset called SlimPajama ((Soboleva et al., 2023)). Starting from the 1.21T token RedPajama dataset Computer (2023), we apply filtering and deduplication to improve data quality. First, we remove documents containing fewer than 200 characters, as we find these typically contain only metadata. Next, we perform global deduplication using MinHashLSH (Leskovec et al., 2014) to extensively remove documents with significant overlapping text.""",627000000000.0,"""To bolster BTLMâ€™s performance, we create a high quality 627B token dataset called SlimPajama""",,,Cerebras CS-2,,Confident,"We introduce the Bittensor Language Model, called ""BTLM-3B-8K"", a new state-of-the-art 3 billion parameter open-source language model. BTLM-3B-8K was trained on 627B tokens from the SlimPajama dataset with a mixture of 2,048 and 8,192 context lengths. BTLM-3B-8K outperforms all existing 3B parameter models by 2-5.5% across downstream tasks. BTLM-3B-8K is even competitive with some 7B parameter models. Additionally, BTLM-3B-8K provides excellent long context performance, outperforming MPT-7B-8K and XGen-7B-8K on tasks up to 8,192 context length. We trained the model on a cleaned and deduplicated SlimPajama dataset; aggressively tuned the \textmu P hyperparameters and schedule; used ALiBi position embeddings; and adopted the SwiGLU nonlinearity.
On Hugging Face, the most popular models have 7B parameters, indicating that users prefer the quality-size ratio of 7B models. Compacting the 7B parameter model to one with 3B parameters, with little performance impact, is an important milestone. BTLM-3B-8K needs only 3GB of memory with 4-bit precision and takes 2.5x less inference compute than 7B models, helping to open up access to a powerful language model on mobile and edge devices. BTLM-3B-8K is available under an Apache 2.0 license on Hugging Face: this https URL.",1.0,,Open weights (unrestricted),Multinational,,,,,,2025-02-14 16:06,,,,3932160.0,,Industry,,,,,Unreleased,"Apache for weights: https://huggingface.co/cerebras/btlm-3b-8k-base

dataset is SlimPajama, with various licenses: https://huggingface.co/datasets/cerebras/SlimPajama-627B",Industry,,,,,Operation counting,,,,
DreamLLM,"Multimodal,Language,Vision,Image generation","Language modeling/generation,Vision-language generation,Image generation","Xiâ€™an Jiaotong University,Megvii Inc,Tsinghua University,Huazhong University of Science and Technology","Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu Zhou, Haoran Wei, Xiangwen Kong, Xiangyu Zhang, Kaisheng Ma, Li Yi",2023-09-20,DreamLLM: Synergistic Multimodal Comprehension and Creation,https://arxiv.org/abs/2309.11499,105.0,,,7000000000.0,7B,7.547904e+20,"Table 13: 128xA800 GPU for (6+10+1.5) hours

flops = (128) * ( 312 * 10**12) * (17.5 * 3600) * (0.3) = 7.5e20
(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)
","LLaVA-Pretrain-595k,MMC4 / Multimodal C4,LLaVA-Instruct-150k,LAION-400M,LAION-COCO,BLIP-LAION","LLaVAPretrain (558K), MMC4 (2M), LLaVAInstruct (80K), BLIP-LAION (8M), BLIP-LAION (2M), InstructMMC4 (20K), LAION400M (11M), Instruct-BLIP-LAION (20K), LAION-COCO (11M) from Table 11",,"from Table 13: 30M pairs image and text, 4M interleaved image-text documents, and 120k instruction examples",17.5,from Table 11: (6+10+1.5) hours,NVIDIA A800 PCIe 40 GB,,Confident,"This paper presents DreamLLM, a learning framework that first achieves versatile Multimodal Large Language Models (MLLMs) empowered with frequently overlooked synergy between multimodal comprehension and creation. DreamLLM operates on two fundamental principles. The first focuses on the generative modeling of both language and image posteriors by direct sampling in the raw multimodal space. This approach circumvents the limitations and information loss inherent to external feature extractors like CLIP, and a more thorough multimodal understanding is obtained. Second, DreamLLM fosters the generation of raw, interleaved documents, modeling both text and image contents, along with unstructured layouts. This allows DreamLLM to learn all conditional, marginal, and joint multimodal distributions effectively. As a result, DreamLLM is the first MLLM capable of generating free-form interleaved content. Comprehensive experiments highlight DreamLLM's superior performance as a zero-shot multimodal generalist, reaping from the enhanced learning synergy. ",1.0,,,"China,China,China,China",,,,128.0,,2025-02-14 16:06,,,,,,"Academia,Industry,Academia,Academia",,,,2240.0,,,"Academia,Industry,Academia,Academia",,,,63553.92147577903,Hardware,,,,
GPT-MolBERTa,Biology,Molecular property prediction,Carnegie Mellon University (CMU),"Suryanarayanan Balaji, Rishikesh Magar, Yayati Jadhav, Amir Barati Farimani",2023-09-20,GPT-MolBERTa: GPT Molecular Features Language Model for molecular property prediction,https://arxiv.org/abs/2310.03030,11.0,,,,,,,,,32600001.0,"326,000 molecules Ã— 100 tokens/molecule = 32,600,000 (3.26e7) total tokens",,,,,Confident,"With the emergence of Transformer architectures and their powerful understanding of textual data, a new horizon has opened up to predict the molecular properties based on text description. While SMILES are the most common form of representation, they are lacking robustness, rich information and canonicity, which limit their effectiveness in becoming generalizable representations. Here, we present GPT-MolBERTa, a self-supervised large language model (LLM) which uses detailed textual descriptions of molecules to predict their properties. A text based description of 326000 molecules were collected using ChatGPT and used to train LLM to learn the representation of molecules. To predict the properties for the downstream tasks, both BERT and RoBERTa models were used in the finetuning stage. Experiments show that GPT-MolBERTa performs well on various molecule property benchmarks, and approaching state of the art performance in regression tasks. Additionally, further analysis of the attention mechanisms show that GPT-MolBERTa is able to pick up important information from the input textual data, displaying the interpretability of the model.",,,Open weights (unrestricted),United States of America,,,,,,2025-06-12 14:30,,,,,,Academia,,,,,Open source,"The Python code and datasets used in this study can be accessed on GitHub using the
following link: https://github.com/Suryanarayanan-Balaji/GPT-MolBERTa

MIT license",Academia,,,,,,,,,
Baichuan 2-7B,Language,"Language modeling/generation,Question answering,Translation",Baichuan,"Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong Yan, Fan Yang, Fei Deng, Feng Wang, Feng Liu, Guangwei Ai, Guosheng Dong, Haizhou Zhao, Hang Xu, Haoze Sun, Hongda Zhang, Hui Liu, Jiaming Ji, Jian Xie, Juntao Dai, Kun Fang, Lei Su, Liang Song, Lifeng Liu, Liyun Ru, Luyao Ma, Mang Wang, Mickel Liu, MingAn Lin, Nuolan Nie, Peidong Guo, Ruiyang Sun, Tao Zhang, Tianpeng Li, Tianyu Li, Wei Cheng, Weipeng Chen, Xiangrong Zeng, Xiaochuan Wang, Xiaoxi Chen, Xin Men, Xin Yu, Xuehai Pan, Yanjun Shen, Yiding Wang, Yiyu Li, Youxin Jiang, Yuchen Gao, Yupeng Zhang, Zenan Zhou, Zhiying Wu",2023-09-20,"Baichuan 2: Open Large-scale Language Models
",https://arxiv.org/pdf/2309.10305,405.0,,,7000000000.0,,1.0919999999999998e+23,"7b * 2.6t * 6 = 1.092e23
Also mentions 1,024 NVIDIA A800 GPUs at 180 TFLOPS per GPU",,,2600000000000.0,,,,NVIDIA A800 PCIe 40 GB,,Confident,"In this technical report, we present Baichuan 2, a series of large-scale multilingual language models containing 7 billion and 13 billion parameters, trained from scratch, on 2.6 trillion tokens. ",,,Open weights (restricted use),China,,,,1024.0,0.5772,2025-06-10 16:10,,,,,,Industry,,,,,Open source,"https://huggingface.co/baichuan-inc/Baichuan2-7B-Base

license here: https://github.com/baichuan-inc/Baichuan2?tab=readme-ov-file
Baichuan 2 æ¨¡åž‹ç¤¾åŒºè®¸å¯åè®® (Community License Agreement)
restrictions on commercial applications with many DAUs and particular types of businesses

Apache 2.0 for code",Industry,,"""By integrating these strategies, our system is
capable of training Baichuan 2-7B and Baichuan
2-13B models efficiently on 1,024 NVIDIA A800
GPUs, achieving a computational efficiency that
exceeds 180 TFLOPS.""

180 TFLOPS / 311.84 TFLOPS = 57.7%",,508431.3718062322,Operation counting,baichuan-inc,,,
OpenChat-13b,Language,Question answering,"Tsinghua University,Shanghai AI Lab,01.AI","Guan Wang, Sijie Cheng, Xianyuan Zhan, Xiangang Li, Sen Song, Yang Liu",2023-09-20,OpenChat: Advancing Open-source Language Models with Mixed-Quality Data,https://arxiv.org/pdf/2309.11235,221.0,,The model was not SOTA on any of the benchmarks it was evaluated on. More details here: docs.google.com/document/d/1BTmyZ9KVTIwkp9z9tRRFsWM5A0QcXiufXrIhujffyso/edit?tab=t.cvou75477sx3#heading=h.ntgr0ob1opib,13000000000.0,"""The openchat-13b is based on the llama-2-13b,"" according to arxiv.org/pdf/2309.11235.",7.805217030000001e+22,"The developers only fine-tuned Llama-2-13b, according to arxiv.org/pdf/2309.11235.

Finetune FLOP: 5.23e19
Base model FLOP: 7.8e+22
Total: 78052170300000010000000",,,133770000.0,"Assuming the fine-tuning dataset contained 70k processed conversations from the ShareGPT dataset and that the elements in the dataset have a mean length of 1911 tokens [2], the size of the dataset is
Dataset size 
= # of conversations * # of tokens / conversation
= 7e4 conversations * 1911 tokens / conversation
~= 133770000 tokens
~= 1.34e8 tokens",,,,,Confident,"Nowadays, open-source large language models like LLaMA have emerged. Recent developments have incorporated supervised fine-tuning (SFT) and reinforcement learning fine-tuning (RLFT) to align these models with human goals. However, SFT methods treat all training data with mixed quality equally, while RLFT methods require high-quality pairwise or ranking-based preference data. In this study, we present a novel framework, named OpenChat, to advance open-source language models with mixed-quality data. Specifically, we consider the general SFT training data, consisting of a small amount of expert data mixed with a large proportion of sub-optimal data, without any preference labels. We propose the C(onditioned)-RLFT, which regards different data sources as coarse-grained reward labels and learns a class-conditioned policy to leverage complementary data quality information. Interestingly, the optimal policy in C-RLFT can be easily solved through single-stage, RL-free supervised learning, which is lightweight and avoids costly human preference labeling. Through extensive experiments on three standard benchmarks, our openchat-13b fine-tuned with C-RLFT achieves the highest average performance among all 13b open-source language models. Moreover, we use AGIEval to validate the model generalization performance, in which only openchat-13b surpasses the base model. Finally, we conduct a series of analyses to shed light on the effectiveness and robustness of OpenChat. Our code, data, and models are publicly available at https://github.com/imoneoi/openchat and https://huggingface.co/openchat.",5.0,,Open weights (unrestricted),"China,China,China",LLaMA-13B,52300000000000000000,"â€œThe openchat-13b is based on the llama-2-13b (Touvron et al., 2023b). We fine-tune the model for 5 epochs on the ShareGPT dataset using the AdamW optimizer with a sequence length of 4,096 tokens and an effective batch size of 200k tokens"" [1]. Moreover, the ""ShareGPT dataset consists of approximately 70k user-shared conversations.â€ The fine-tuning method was Conditioned Reinforcement Learning (C-RLFT) [1].

The original ShareGPT dataset hasnâ€™t been published. However, the OpenChat GitHub repository has a Jupyter notebook that was published in advance of the model and that provides details on a processed ShareGPT dataset that appears to have been used for training the model, based on a few details. The dataset is named â€œdataset_processed_v3/sharegpt_v3.2.train.parquetâ€ and has a tokenizer named â€œimone/LLaMA2_13B_with_EOT_tokenâ€ [2]. The elements in the dataset have a maximum length of 4096 tokens, the same sequence length the developers said they used [2], [1, page 6]. The dataset has at least 77k elements, which aligns with the developersâ€™ description of the ShareGPT dataset as containing approximately 70k user conversations [2], [1, page 5]. The elements in the dataset have a mean length of 1911 tokens, so I will use that to estimate the number of tokens in the ShareGPT dataset [2].

Assuming the fine-tuning dataset contained 70k processed conversations from the ShareGPT dataset and that the elements in the dataset have a mean length of 1911 tokens [2], the size of the dataset is
Dataset size 
= # of conversations * # of tokens / conversation
= 7e4 conversations * 1911 tokens / conversation
~= 133770000 tokens
~= 1.34e8 tokens

Since Llama-2-13b has dense architecture, the 6ND approximation yields
Fine-tuning compute
= # of active parameters / forward pass * # of tokens / epoch * # of epochs * 6 FLOPs / token
~= 13e9 parameters * 1.34e8 tokens / epoch * 5 epochs * 6 FLOPs / token
= 522.6e19 FLOPS
~= 5.23e19 FLOPS

1. https://arxiv.org/pdf/2309.11235
2. https://github.com/imoneoi/openchat/blob/master/ochat/experimental/verify_dataset.ipynb ",,,2025-05-01 10:42,,,,,,"Academia,Academia,Industry",,,,,,,"Academia,Academia,Industry",,,,,,,,,
Nova-2,Speech,Speech recognition,Deepgram,,2023-09-19,Meet Deepgram Nova-2: Raising the Bar for Speech-to-Text,https://deepgram.com/learn/nova-2-speech-to-text-api,,,,,,,,Unspecified unreleased,,,,,,,,Unknown,"Our next-gen speech-to-text model, Nova-2, outperforms all alternatives in terms of accuracy, speed, and cost (starting at $0.0043/min), and we have the benchmarks to prove it.

Nova-2 is 18% more accurate than our previous Nova model and offers a 36% relative WER improvement over OpenAI Whisper (large).

Weâ€™re excited to announce Deepgram Nova-2, the most powerful speech-to-text (STT) model in the world, is now available in English (both pre-recorded and streaming audio) for early access customers. Compared to leading alternatives, Nova-2 delivers:

An average 30% reduction in word error rate (WER)[1] over competitors for both pre-recorded and real-time transcription

5-40x faster pre-recorded inference time

The same low price as Nova (starting at only $0.0043/min for pre-recorded audio) with 3-5x lower cost than the competition

Since the launch of our initial Nova model (Nova-1) earlier this year, we have been dedicated to delivering enhanced capabilities. These new features encompass improved speaker diarization, smart formatting, filler words support, and our inaugural domain-specific language model for summarization. These additions not only elevate the value we provide to our customers but also underline our commitment to advancing the forefront of language AI.

Furthermore, our model research team has maintained an exceptional level of productivity, upholding our longstanding tradition of relentless improvement in the quest for flawless speech-to-text accuracy and even superhuman transcription performance (refer to Fig. 1). With Nova-2â€™s word error rates consistently below 10% across domains, we proudly announce the realization of this monumental achievement.",,,API access,United States of America,,,,,,2025-05-12 15:02,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
bge-reranker-large,Language,Semantic embedding,"Beijing Academy of Artificial Intelligence / BAAI,Hugging Face","Shitao Xiao, Zheng Liu, Peitian Zhang, Niklas Muennighoff",2023-09-14,C-Pack: Packaged Resources To Advance General Chinese Embedding,"https://arxiv.org/abs/2309.07597v2
https://huggingface.co/BAAI/bge-reranker-large",86.0,,,560000000.0,"560M from https://huggingface.co/BAAI/bge-reranker-large
""This reranker is initialized from xlm-roberta-base"" from https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/reranker
",,"It is fine tuning of xlm-roberta-base
citation from https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/reranker
""This reranker is initialized from xlm-roberta-base,""
xlm-roberta-base is a dense 279M parameter model. From it's paper, it was trained on 286B multi-lingual tokens https://arxiv.org/pdf/1911.02116
6CN = 6 * 279M * 286B = 4.79e20",MTP,"T2ranking, MMmarco, dulreader, Cmedqa-v2, and nli-zh
msmarco, nq, hotpotqa, NLI, Mr.TyDi

citation from https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/reranker ""This reranker is initialized from xlm-roberta-base, and we train it on a mixture of multilingual datasets:

    Chinese: 788,491 text pairs from T2ranking, MMmarco, dulreader, Cmedqa-v2, and nli-zh
    English: 933,090 text pairs from msmarco, nq, hotpotqa, and NLI
    Others: 97,458 text pairs from Mr.TyDi (including arabic, bengali, english, finnish, indonesian, japanese, korean, russian, swahili, telugu, thai)

In order to enhance the cross-language retrieval ability, we construct two cross-language retrieval datasets bases on MMarco. Specifically, we sample 100,000 english queries to retrieve the chinese passages, and also sample 100,000 chinese queries to retrieve english passages."" ",,"1819039 text pairs
citation from https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/reranker""This reranker is initialized from xlm-roberta-base, and we train it on a mixture of multilingual datasets:

    Chinese: 788,491 text pairs from T2ranking, MMmarco, dulreader, Cmedqa-v2, and nli-zh
    English: 933,090 text pairs from msmarco, nq, hotpotqa, and NLI
    Others: 97,458 text pairs from Mr.TyDi (including arabic, bengali, english, finnish, indonesian, japanese, korean, russian, swahili, telugu, thai)

In order to enhance the cross-language retrieval ability, we construct two cross-language retrieval datasets bases on MMarco. Specifically, we sample 100,000 english queries to retrieve the chinese passages, and also sample 100,000 chinese queries to retrieve english passages."" ",,,,,Confident,"We introduce C-Pack, a package of resources that significantly advance the field of general Chinese embeddings. C-Pack includes three critical resources. 1) C-MTEB is a comprehensive benchmark for Chinese text embeddings covering 6 tasks and 35 datasets. 2) C-MTP is a massive text embedding dataset curated from labeled and unlabeled Chinese corpora for training embedding models. 3) C-TEM is a family of embedding models covering multiple sizes. Our models outperform all prior Chinese text embeddings on C-MTEB by up to +10% upon the time of the release. We also integrate and optimize the entire suite of training methods for C-TEM. Along with our resources on general Chinese embedding, we release our data and models for English text embeddings. The English models achieve state-of-the-art performance on MTEB benchmark; meanwhile, our released English data is 2 times larger than the Chinese data. All these resources are made publicly available at this https://github.com/FlagOpen/FlagEmbedding",,,Open weights (unrestricted),"China,Multinational,United States of America",XLM-RoBERTa,,,,,2024-09-05 14:08,,,,,,"Academia,Industry",,,,,,MIT,"Academia,Industry",,,,,,,,,
DeciLM 6B,Language,Chat,Deci AI,DeciAI Research Team,2023-09-13,DeciLM 6B,https://huggingface.co/Deci/DeciLM-6b,,,,5700000000.0,"""DeciLM 6B is a 5.7 billion parameter decoder-only text generation model. """,1.026e+22,"assume they used 50% of SlimPajama dataset (300B tokens) - then we are  within 'Likely' confidence interval

6*300B*5700000000=1.026e+22",SlimPajama,"""DeciLM 6B underwent training utilizing a subset of the SlimPajamas dataset"" from https://deci.ai/blog/decilm-15-times-faster-than-llama2-nas-generated-llm-with-variable-gqa/?utm_campaign=repos&utm_source=hugging-face&utm_medium=model-card&utm_content=decilm-6b",,"subset of the SlimPajamas dataset, but we don't know which subset
""DeciLM 6B underwent training utilizing a subset of the SlimPajamas dataset"" from https://deci.ai/blog/decilm-15-times-faster-than-llama2-nas-generated-llm-with-variable-gqa/?utm_campaign=repos&utm_source=hugging-face&utm_medium=model-card&utm_content=decilm-6b",,,,,Likely,,,,Open weights (restricted use),Israel,,,,,,2025-02-14 16:07,,,,,,Industry,,,,,Unreleased,"Llama 2 license (restrictive)
dataset: https://huggingface.co/datasets/cerebras/SlimPajama-627B",Industry,,,,,Operation counting,,,,
Robot Parkour,Robotics,Animal (human/non-human) imitation,"Shanghai Qi Zhi institute,Stanford University,Carnegie Mellon University (CMU),Tsinghua University","Ziwen Zhuang, Zipeng Fu, Jianren Wang, Christopher Atkeson, Soeren Schwertfeger, Chelsea Finn, Hang Zhao",2023-09-12,Robot Parkour Learning,https://arxiv.org/abs/2309.05665,97.0,SOTA improvement,,500000.0,"Parkour policy details on page 8, table 11.",,"The paper provides some details on the training time and hardware used:

Each specialized skill policy (climbing, leaping, etc) was pre-trained with soft dynamics constraints for 12 hours using 1 Nvidia RTX 3090 GPU.
The skills were then fine-tuned with hard dynamics constraints for 6 hours each.
The final parkour policy distillation process used 4 computers with 1 RTX 3090 GPU each, training for an unspecified amount of time.
So the total training time was at least 12 + 6 x 5 = 42 hours for the initial skills, plus an additional unknown time for the distillation.

The hardware used was high-end Nvidia RTX 3090 GPUs, which at the time of paper writing would have been top of the line GPUs. Multiple GPUs were used in parallel during the distillation stage.",,"Isaac Gym simulated proprioceptive data, images, and actions",,,,,NVIDIA GeForce RTX 3090,,Confident,"Parkour is a grand challenge for legged locomotion that requires robots to overcome various obstacles rapidly in complex environments. Existing methods can generate either diverse but blind locomotion skills or vision-based but specialized skills by using reference animal data or complex rewards. However, autonomous parkour requires robots to learn generalizable skills that are both vision-based and diverse to perceive and react to various scenarios. In this work, we propose a system for learning a single end-to-end vision-based parkour policy of diverse parkour skills using a simple reward without any reference motion data. We develop a reinforcement learning method inspired by direct collocation to generate parkour skills, including climbing over high obstacles, leaping over large gaps, crawling beneath low barriers, squeezing through thin slits, and running. We distill these skills into a single vision-based parkour policy and transfer it to a quadrupedal robot using its egocentric depth camera. We demonstrate that our system can empower two different low-cost robots to autonomously select and execute appropriate parkour skills to traverse challenging real-world environments.",,,Unreleased,"China,United States of America,United States of America,China",,,,,,2025-06-02 12:03,,,,,,"Academia,Academia,Academia",,,,,Open source,"MIT license for training and inference code:
https://github.com/ZiwenZhuang/parkour","Academia,Academia,Academia",,,,,,,,,
Phi-1.5,Language,Language generation,Microsoft,"Yuanzhi Li, SÃ©bastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, Yin Tat Lee",2023-09-11,Textbooks Are All You Need II: phi-1.5 technical report,"https://arxiv.org/abs/2309.05463, https://huggingface.co/microsoft/phi-1_5",324.0,,,1300000000.0,,1.17e+21,"150B training tokens

150B*1.3B*6 = 1.17e21

also, took 1.5k GPU-hours with A100s, per Table 1

1500 * 312 trillion * 3600 * 0.3 (utilization assumption) = 5.05e20

so utilization was likely ~60%. may be high due to the relatively small cluster.
",,"Synthetic ""textbook"" data: 

""Our training data for phi-1.5 is a combination of phi-1â€™s training data (7B tokens) and newly created
synthetic, â€œtextbook-likeâ€ data (roughly 20B tokens) for the purpose of teaching common sense reasoning
and general knowledge of the world (science, daily activities, theory of mind, etc.)""",30000000000.0,"""In a nutshell we build phi-1.5, a 1.3 billion parameter model trained on a dataset of 30 billion tokens""",192.0,,NVIDIA A100 SXM4 40 GB,,Confident,"We continue the investigation into the power of smaller Transformer-based language models as initiated by \textbf{TinyStories} -- a 10 million parameter model that can produce coherent English -- and the follow-up work on \textbf{phi-1}, a 1.3 billion parameter model with Python coding performance close to the state-of-the-art. The latter work proposed to use existing Large Language Models (LLMs) to generate ``textbook quality"" data as a way to enhance the learning process compared to traditional web data. We follow the ``Textbooks Are All You Need"" approach, focusing this time on common sense reasoning in natural language, and create a new 1.3 billion parameter model named \textbf{phi-1.5}, with performance on natural language tasks comparable to models 5x larger, and surpassing most non-frontier LLMs on more complex reasoning tasks such as grade-school mathematics and basic coding. More generally, \textbf{phi-1.5} exhibits many of the traits of much larger LLMs, both good -- such as the ability to ``think step by step"" or perform some rudimentary in-context learning -- and bad, including hallucinations and the potential for toxic and biased generations -- encouragingly though, we are seeing improvement on that front thanks to the absence of web data. We open-source \textbf{phi-1.5} to promote further research on these urgent topics.",5.0,,Open weights (unrestricted),"United States of America,Multinational,India,Belgium",,,,,,2025-01-27 17:47,,,,,,Industry,checked,,,,Unreleased,MIT license for weights,Industry,,,,,"Operation counting,Hardware",,,,
MADLAD-400 10B,Language,Translation,"Google DeepMind,Google Research","Sneha Kudugunta, Isaac Caswell, Biao Zhang, Xavier Garcia, Christopher A. Choquette-Choo, Katherine Lee, Derrick Xin, Aditya Kusupati, Romi Stella, Ankur Bapna, Orhan Firat",2023-09-09,MADLAD-400: A Multilingual And Document-Level Large Audited Dataset,https://arxiv.org/abs/2309.04662,73.0,,,10700000000.0,10.7B  from appendix A.8,1.605e+22,"6ND = 10.7B * 250B = 1.6e22
'MADLAD-400-10B-MT is a multilingual machine translation model based on the T5 architecture that was trained on 250 billion tokens covering over 450 languages using publicly available data. '
10.7B  params from appendix A.8",MADLAD-400," 'We introduce MADLAD-400, a manually audited, general domain 3T token monolingual dataset based on CommonCrawl, spanning 419 languages.'",3000000000000.0,"MADLAD-400, dataset released with paper, is 3T tokens: 'We introduce MADLAD-400, a manually audited, general domain 3T token monolingual dataset based on CommonCrawl, spanning 419 languages.'

However, model in question was trained on only 250B tokens: ""We then train and release a 10.7B-parameter multilingual machine translation model on 250 billion tokens covering over 450 languages using publicly available data""",,,,,Confident,"We introduce MADLAD-400, a manually audited, general domain 3T token monolingual dataset based on CommonCrawl, spanning 419 languages. We discuss the limitations revealed by self-auditing MADLAD-400, and the role data auditing had in the dataset creation process. We then train and release a 10.7B-parameter multilingual machine translation model on 250 billion tokens covering over 450 languages using publicly available data, and find that it is competitive with models that are significantly larger, and report the results on different domains. In addition, we train a 8B-parameter language model, and assess the results on few-shot translation. We make the baseline models available to the research community. ",1.0,,Open weights (unrestricted),"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France,Multinational,United States of America,Canada,Switzerland",,,,,,2025-02-14 16:07,,,,,,"Industry,Industry",,,,,,"Apache 2.0:
https://github.com/google-research/google-research/tree/master","Industry,Industry",,,,,Operation counting,,,,
Mobile V-MoEs,Vision,Image classification,Apple,"Erik Daxberger, Floris Weers, Bowen Zhang, Tom Gunter, Ruoming Pang, Marcin Eichner, Michael Emmersberger, Yinfei Yang, Alexander Toshev, Xianzhi Du",2023-09-08,Mobile V-MoEs: Scaling Down Vision Transformers via Sparse Mixture-of-Experts,https://arxiv.org/abs/2309.04354,,,,,"total number of layers: 12
embedding size: 384
number of multi-head self-attention heads: 6
number of experts: 10 ",,,ImageNet-1k,"We train all models from scratch on the ImageNet-1k training set of
1.28M images, and then evaluate their top-1 accuracy on
the held-out validation set of 50K images.",1280000.0,,,,,,Confident,"Sparse Mixture-of-Experts models (MoEs) have recently gained popularity due to their ability to decouple model size from inference efficiency by only activating a small subset of the model parameters for any given input token. As such, sparse MoEs have enabled unprecedented scalability, resulting in tremendous successes across domains such as natural language processing and computer vision. In this work, we instead explore the use of sparse MoEs to scale-down Vision Transformers (ViTs) to make them more attractive for resource-constrained vision applications. To this end, we propose a simplified and mobile-friendly MoE design where entire images rather than individual patches are routed to the experts. We also propose a stable MoE training procedure that uses super-class information to guide the router. We empirically show that our sparse Mobile Vision MoEs (V-MoEs) can achieve a better trade-off between performance and efficiency than the corresponding dense ViTs. For example, for the ViT-Tiny model, our Mobile V-MoE outperforms its dense counterpart by 3.39% on ImageNet-1k. For an even smaller ViT variant with only 54M FLOPs inference cost, our MoE achieves an improvement of 4.66%.",,,Unreleased,United States of America,,,,,,2025-06-11 17:19,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
Persimmon-8B,Language,Language modeling,Adept,"Erich Elsen, Augustus Odena, Maxwell Nye, SaÄŸnak TaÅŸÄ±rlar, Tri Dao, Curtis Hawthorne, Deepak Moparthi, Arushi Somani",2023-09-07,Releasing Persimmon-8B,https://www.adept.ai/blog/persimmon-8b,,,,9300000000.0,"""The checkpoint we are releasing has approximately 9.3B parameters. In order to make pipelining during training more efficient, we chose to decouple the input and output embeddings. Doing this does not increase the capacity of the modelâ€“it is purely a systems optimization to avoid all-reducing the gradients for the (very large) embeddings across potentially slow communication links. In terms of inference cost, the model is equivalent to an 8B parameter model with coupled input/output embeddings.""",4.11246e+22,6*9300000000*737000000000=4.11246e+22,,"We train the model from start to finish with a sequence length of 16K on 737B tokens uniformly sampled from a much larger dataset, which is a mix of text (~75%) and code (~25%).",552750000000.0,737B tokens = 552750M words,,,,,Confident,,,,Open weights (unrestricted),United States of America,,,,,,2025-02-14 16:07,,,,,,Industry,,,,,,apache 2.0,Industry,,,,,Operation counting,,,,
FLM-101B,Language,"Language modeling/generation,Question answering,Chat","Chinese Academy of Sciences,Harbin Institute of Technology,Nanyang Technological University,Beijing Academy of Artificial Intelligence / BAAI","Xiang Li, Yiqun Yao, Xin Jiang, Xuezhi Fang, Xuying Meng, Siqi Fan, Peng Han, Jing Li, Li Du, Bowen Qin, Zheng Zhang, Aixin Sun, Yequan Wang",2023-09-07,FLM-101B: An Open LLM and How to Train It with $100K Budget,https://arxiv.org/abs/2309.03852,18.0,,,101000000000.0,,5.720000000000001e+22,"192 GPUs * 160 TFLOP/s per GPU (reported, adjusted for utilization) * 21.54 days * 24 * 3600 = 5.72e22 (confident)

6*101000000000*311540000000=1.8879324e+23 (less confident)",,"""By design, FLM-101B is an English-Chinese bilingual model pre-trained with causal language modeling. It mixes English and Chinese corpora at a ratio of approximately 53.5% : 46.5% for language modeling.""",272125000000.0,"Trained with 311.54B tokens. The dataset is approximately 50/50 English/Chinese: ""It mixes English and Chinese corpora at a ratio of approximately 53.5% : 46.5% for language modeling"". We assume 1 Chinese word per token and 0.75 English words per token (0.875 on average). 311B * 0.875 ~= 272B.",517.0,"""Under this growth schedule, the total time cost for our 101B model is 21.54 days""",NVIDIA A800 PCIe 40 GB,,Confident,"Large language models (LLMs) have achieved remarkable success in NLP and multimodal tasks, among others. Despite these successes, two main challenges remain in developing LLMs: (i) high computational cost, and (ii) fair and objective evaluations. In this paper, we report a solution to significantly reduce LLM training cost through a growth strategy. We demonstrate that a 101B-parameter LLM with 0.31T tokens can be trained with a budget of 100K US dollars. Inspired by IQ tests, we also consolidate an additional range of evaluations on top of existing evaluations that focus on knowledge-oriented abilities. These IQ evaluations include symbolic mapping, rule understanding, pattern mining, and anti-interference. Such evaluations minimize the potential impact of memorization. Experimental results show that our model, named FLM-101B, trained with a budget of 100K US dollars, achieves performance comparable to powerful and well-known models, e.g., GPT-3 and GLM-130B, especially on the additional range of IQ evaluations.",,,Open weights (unrestricted),"China,China,Singapore,China",,,,,,2025-02-14 16:07,,,,4310000.0,Table 1,"Academia,Academia,Academia,Academia",,,,,,"apache 2.0
https://huggingface.co/CofeAI/FLM-101B","Academia,Academia,Academia,Academia",,,,,Hardware,,,,
XGen-7B,Language,Language generation,Salesforce,"Erik Nijkamp, Tian Xie, Hiroaki Hayashi, Bo Pang, Congying Xia, Chen Xing, Jesse Vig, Semih Yavuz, Philippe Laban, Ben Krause, Senthil Purushwalkam, Tong Niu, Wojciech Kryscinski, Lidiya Murakhovsâ€™ka, Prafulla Kumar Choubey, Alex Fabbri, Ye Liu, Rui Meng, Lifu Tu, Meghana Bhat, Chien-Sheng Wu, Silvio Savarese, Yingbo Zhou, Shafiq Joty, Caiming Xiong",2023-09-07,XGen-7B Technical Report,https://arxiv.org/abs/2309.03450,12.0,,"""Our evaluation on standard benchmarks shows that XGen-7B models
achieve comparable or better results when compared with state-of-the-art opensource LLMs""",6700000000.0,"I think this suggests the same number of params as Llama-7b. In any case ~7B. 
""The model architecture follows LLaMA with exact numerical compatibility to ease adoption in third-party frameworks. The hyperparameters closely follow LLaMA-7B [34] with the following alterations""",8.02e+22,"270,336 TPUv4-hours per the carbon emissions section. They must mean chip-hours, not core-hours, because they multiply by 192W, which is the max power consumption of a TPU-v4 chip.

https://cloud.google.com/tpu/docs/system-architecture-tpu-vm#tpu_v4

This is probably the total for the whole paper? They released two versions of the base model, one of which is just trained further than the other, and they have two fine-tuned versions. If fine-tuning compute is minor, then this total is close to the largest training compute for any single model in the paper. 

270336 * 275 teraflop/s * 3600 * 0.3 = 8.02e22 FLOP

also, using 6ND:

1484 billion tokens * 6.7 billion * 6 = 5.97e22",,"""Our pre-training dataset is a mixture of data from several public sources, reported in Table 2. We employ a two-stage training strategy, where each stage uses a different data mixture, as shown in
Table 3.
Natural language data for stage 1. Natural language data is a mixture of publicly available data.
We made an effort to improve safety and diversity of the data.
Code data for stage 1. We use the GitHub subset from the recently released RedPajama dataset [9].
We also added Apex code data to enhance our modelâ€™s proficiency in Apex code generation. Apex is
a widely used object-oriented programming language in Salesforce products.
BigCode Starcoder data for stage 2. We use all the 86 programming languages from the Starcoder [15] data, preserving the original weight of each. Subsequently, we further filter the data
according to a stronger permissive license guideline.""",1484000000000.0,1484B tokens per Table 2,,,Google TPU v4,,Confident,"Large Language Models (LLMs) have become ubiquitous across various domains,
transforming the way we interact with information and conduct research. However,
most high-performing LLMs remain confined behind proprietary walls, hindering
scientific progress. Most open-source LLMs, on the other hand, are limited in their
ability to support longer sequence lengths, which is a key requirement for many
tasks that require inference over an input context. To address this, we have trained
XGen-7B, a series of 7B parameter models on up to 8K sequence length for up
to 1.5T tokens. We have also finetuned the XGen-7B models on public-domain
instructional data, creating their instruction-tuned counterparts (XGen-7B-Inst).
We open-source our models for both research advancements and commercial
applications. Our evaluation on standard benchmarks shows that XGen-7B models
achieve comparable or better results when compared with state-of-the-art opensource LLMs. Our targeted evaluation on long sequence modeling tasks shows the
benefits of our 8K-sequence models over 2K-sequence open-source LLMs.",1.0,,Open weights (unrestricted),United States of America,,,,,,2025-02-14 16:07,,,,1048576.0,"""batch size of 128, and a sequence length of 8,192 tokens""",Industry,,,,270336.0,Unreleased,"Apache 2.0 
https://github.com/salesforce/XGen",Industry,,,,,"Hardware,Operation counting",,,,
Hunyuan,"Language,Image generation,Multimodal","Language modeling/generation,Image generation,Question answering",Tencent,,2023-09-07,"Tencent Unveils Hunyuan, its Proprietary Large Foundation Model on Tencent Cloud",https://www.tencent.com/en-us/articles/2201685.html,,,,100000000000.0,"""Presently, the Hunyuan model has over 100 billion parameters, with more than two trillion tokens in pre-training data.""",1.1999999999999999e+24,6ND = 6*100*10^9*2*10^12 = 1.2*10^24,Unspecified unreleased,,2000000000000.0,"""Presently, the Hunyuan model has over 100 billion parameters, with more than two trillion tokens in pre-training data.""",,,,,Confident,"Enterprises in China may now access Hunyuan via Tencentâ€™s public cloud platform and finetune it to their specific needs. The platform features strong Chinese language processing abilities, advanced logical reasoning, and comes with reliable task execution abilities.

Tencentâ€™s foundation model supports a wide array of functions spanning the creation of images, copywriting, text recognition, and customer service, to name a few. These will be instrumental in key industries like finance, public services, social media, e-commerce, transportation, games, and many more.

This empowers enterprises to build powerful tools, in addition to training their own unique large models derived from Tencentâ€™s Model-as-a-Service (MaaS) offering, which was first introduced in June this year. The MaaS provides enterprises with economically viable, industry-specific large models, featuring more than 50 solutions spanning 20 major industries. This creates a virtuous cycle in which enterprises refine their large models with Hunyuan to create uniquely intelligent services across their operations. ",,,API access,China,,,,,,2025-05-16 10:30,,"There is no paper to reference, no information about hardware used for training found in media.",,,,Industry,,,,,Unreleased,,Industry,,,,,Operation counting,,,,
ELIXR-C,"Vision,Medicine,Multimodal","Image embedding,Semantic search,Image classification","Google,Northwestern Medicine,Apollo Radiology International","Shawn Xu, Lin Yang, Christopher Kelly, Marcin Sieniek, Timo Kohlberger, Martin Ma, Wei-Hung Weng, Atilla Kiraly, Sahar Kazemzadeh, Zakkai Melamed, Jungyeon Park, Patricia Strachan, Yun Liu, Chuck Lau, Preeti Singh, Christina Chen, Mozziyar Etemadi, Sreenivasa Raju Kalidindi, Yossi Matias, Katherine Chou, Greg S. Corrado, Shravya Shetty, Daniel Tse, Shruthi Prabhakara, Daniel Golden, Rory Pilgrim, Krish Eswaran, Andrew Sellergren",2023-09-07,ELIXR: Towards a general purpose X-ray artificial intelligence system through alignment of large language models and radiology vision encoders,https://arxiv.org/abs/2308.01317,,,,,,,,,,,,,,,,Unknown,"In this work, we present an approach, which we call Embeddings for Language/Image-aligned X-Rays, or ELIXR, that leverages a language-aligned image encoder combined or grafted onto a fixed LLM, PaLM 2, to perform a broad range of chest X-ray tasks. We train this lightweight adapter architecture using images paired with corresponding free-text radiology reports from the MIMIC-CXR dataset. ELIXR achieved state-of-the-art performance on zero-shot chest X-ray (CXR) classification (mean AUC of 0.850 across 13 findings), data-efficient CXR classification (mean AUCs of 0.893 and 0.898 across five findings (atelectasis, cardiomegaly, consolidation, pleural effusion, and pulmonary edema) for 1% (~2,200 images) and 10% (~22,000 images) training data), and semantic search (0.76 normalized discounted cumulative gain (NDCG) across nineteen queries, including perfect retrieval on twelve of them). Compared to existing data-efficient methods including supervised contrastive learning (SupCon), ELIXR required two orders of magnitude less data to reach similar performance. ELIXR also showed promise on CXR vision-language tasks, demonstrating overall accuracies of 58.7% and 62.5% on visual question answering and report quality assurance tasks, respectively. These results suggest that ELIXR is a robust and versatile approach to CXR AI.

ELIXR-C (Xu et al., 2023) is an image/text encoder trained with the CLIP method (Radford et al., 2021). This model tends to perform better at zero-shot tasks, i.e. those that do not require further data for training.",,,,"United States of America,United States of America,India","EfficientNet-L2,CXR Foundation",,,,,2025-05-01 10:42,,,,,,Industry,,,,,,,Industry,,,,,,,,,
ELIXR-B,"Vision,Medicine","Image embedding,Visual question answering,Semantic search,Image classification,Language modeling/generation","Google,Northwestern Medicine,Apollo Radiology International","Shawn Xu, Lin Yang, Christopher Kelly, Marcin Sieniek, Timo Kohlberger, Martin Ma, Wei-Hung Weng, Atilla Kiraly, Sahar Kazemzadeh, Zakkai Melamed, Jungyeon Park, Patricia Strachan, Yun Liu, Chuck Lau, Preeti Singh, Christina Chen, Mozziyar Etemadi, Sreenivasa Raju Kalidindi, Yossi Matias, Katherine Chou, Greg S. Corrado, Shravya Shetty, Daniel Tse, Shruthi Prabhakara, Daniel Golden, Rory Pilgrim, Krish Eswaran, Andrew Sellergren",2023-09-07,ELIXR: Towards a general purpose X-ray artificial intelligence system through alignment of large language models and radiology vision encoders,https://arxiv.org/abs/2308.01317,,,,,,,,Unspecified unreleased,"""dataset of over 1,000,000 chest radiographs from 5 hospitals in India, and 4 hospitals in the USA""",,,,,,,Unknown,"In this work, we present an approach, which we call Embeddings for Language/Image-aligned X-Rays, or ELIXR, that leverages a language-aligned image encoder combined or grafted onto a fixed LLM, PaLM 2, to perform a broad range of chest X-ray tasks. We train this lightweight adapter architecture using images paired with corresponding free-text radiology reports from the MIMIC-CXR dataset. ELIXR achieved state-of-the-art performance on zero-shot chest X-ray (CXR) classification (mean AUC of 0.850 across 13 findings), data-efficient CXR classification (mean AUCs of 0.893 and 0.898 across five findings (atelectasis, cardiomegaly, consolidation, pleural effusion, and pulmonary edema) for 1% (~2,200 images) and 10% (~22,000 images) training data), and semantic search (0.76 normalized discounted cumulative gain (NDCG) across nineteen queries, including perfect retrieval on twelve of them). Compared to existing data-efficient methods including supervised contrastive learning (SupCon), ELIXR required two orders of magnitude less data to reach similar performance. ELIXR also showed promise on CXR vision-language tasks, demonstrating overall accuracies of 58.7% and 62.5% on visual question answering and report quality assurance tasks, respectively. These results suggest that ELIXR is a robust and versatile approach to CXR AI.

ELIXR-B (Xu et al., 2023) is trained with the BLIP-2 method (Li et al., 2022) on a dataset of over 1,000,000 chest radiographs from 5 hospitals in India, and 4 hospitals in the USA. This model tends to perform better on downstream classification tasks.",,,,"United States of America,United States of America,India","PaLM 2,ELIXR-C",,,,,2025-05-01 10:42,,,,,,Industry,,,,,,,Industry,,,,,,,,,
Falcon-180B,Language,Language modeling,Technology Innovation Institute,"Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, MÃ©rouane Debbah, Ã‰tienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, Daniele Mazzotta, Badreddine Noune, Baptiste Pannier, Guilherme Penedo",2023-09-06,The Falcon Series of Open Language Models,https://falconllm.tii.ae/falcon-180b.html; https://arxiv.org/abs/2311.16867,261.0,Training cost,"""It's currently at the top of the Hugging Face Leaderboard for pre-trained Open Large Language Models and is available for both research and commercial use.""

""This model performs exceptionally well in various tasks like reasoning, coding, proficiency, and knowledge tests, even beating competitors like Meta's LLaMA 2.""",180000000000.0,"""Falcon 180B is a super-powerful language model with 180 billion parameters""",3.76e+24,"43,500 petaflop-days per Table 1 of the paper

43500 * 1e15 * 24 * 3600 = 3.76e24


C = 6ND = 6 FLOP/token/parameter * 3.5 trillion tokens * 180 billion parameters = 3.78*10^24 FLOP",RefinedWeb,"""The Falcon series is made of three causal decoder-only models trained on up to 4,096 A100. We assembled a pretraining dataset of 3,500 billion tokens, predominantly sourced from our work on RefinedWeb (Penedo et al., 2023)â€“a massive filtered and deduplicated web dataset""

Training dataset composition is described in Table 3. Falcon was trained for 1 epoch.",2625000000000.0,3.5 trillion tokens * (~3 words per 4 tokens) ~= 2.625 trillion words,4320.0,"Stanford CRFM foundation model ecosystem graph data page https://crfm.stanford.edu/ecosystem-graphs/index.html?asset=Falcon-180B says 9 months, which is the maximum possible amount of time: training began sometime in 2023, and it was released in September. 

However, 6 months is more realistic. That is the length of the gap between Falcon 40B and Falcon 180B. Additionally, the amount of compute is specified in the paper, so there is only one degree of freedom in the uncertain values of training duration and hardware utilization rate. At six months, the utilization is unusually low, so the training was probably not longer than that.",NVIDIA A100 SXM4 40 GB,,Confident,"Falcon 180B is a super-powerful language model with 180 billion parameters, trained on 3.5 trillion tokens. It's currently at the top of the Hugging Face Leaderboard for pre-trained Open Large Language Models and is available for both research and commercial use.

This model performs exceptionally well in various tasks like reasoning, coding, proficiency, and knowledge tests, even beating competitors like Meta's LLaMA 2.

Among closed source models, it ranks just behind OpenAI's GPT 4, and performs on par with Google's PaLM 2 Large, which powers Bard, despite being half the size of the model.",1.0,,Open weights (restricted use),United Arab Emirates,,,,4096.0,0.1892,2025-06-10 15:40,Amazon Web Services,,,4194304.0,"from paper (https://arxiv.org/pdf/2311.16867.pdf):

Batch size 2048 (presumably sequences) per Table 16. Warmed up using smaller batches for first 100B tokens.

""All Falcon models are pretrained with a 2,048 sequence length""

2048*2048 = 4194304",Government,,3.76e+24,3.76e+24,17694720.0,Unreleased,"""Falcon 180b can be commercially used but under very restrictive conditions, excluding any ""hosting use""."" https://huggingface.co/blog/falcon-180b",Government,$10340911.71,"Estimated training compute: 3.76e24
FLOPs at 100% utilization, based on GPU-hours: 4320 * 4096 * 3600 * 3.12e14 = 1.987e25
3.76e24 / 1.987e25 = 0.1892",BF16,3254975.4289709157,"Reported,Operation counting",,,,4
Baichuan2-13B,Language,Chat,Baichuan,"Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong Yan, Fan Yang, Fei Deng, Feng Wang, Feng Liu, Guangwei Ai, Guosheng Dong, Haizhou Zhao, Hang Xu, Haoze Sun, Hongda Zhang, Hui Liu, Jiaming Ji, Jian Xie, JunTao Dai, Kun Fang, Lei Su, Liang Song, Lifeng Liu, Liyun Ru, Luyao Ma, Mang Wang, Mickel Liu, MingAn Lin, Nuolan Nie, Peidong Guo, Ruiyang Sun, Tao Zhang, Tianpeng Li, Tianyu Li, Wei Cheng, Weipeng Chen, Xiangrong Zeng, Xiaochuan Wang, Xiaoxi Chen, Xin Men, Xin Yu, Xuehai Pan, Yanjun Shen, Yiding Wang, Yiyu Li, Youxin Jiang, Yuchen Gao, Yupeng Zhang, Zenan Zhou, Zhiying Wu",2023-09-06,Baichuan 2: Open Large-scale Language Models,"https://huggingface.co/baichuan-inc/Baichuan2-13B-Base, https://arxiv.org/abs/2309.10305",,,,13000000000.0,,2.03e+23,"They describe the dataset as having 2.6T tokens, but the checkpoint graph makes it clear that's also the number of tokens the model was trained on.

13b * 2.6t * 6 = 2.03e23",,"2.6 trillion tokens, bilingual.

paper/model card don't give breakdown between English and Chinese",2275000000000.0,"2.6T tokens, or ~2.3T words assuming that the dataset is roughly even English (0.75 words/token) and Chinese (1 word/token)

1.3T Chinese tokens * (1 word/token) = 1.3T Chinese words
1.3T English tokens * (0.75 words/token) = 0.975T English words
total: 2.275T, or ~2.3T",,,,,Confident,,1.0,,Open weights (restricted use),China,,,,1024.0,0.5772,2025-06-10 16:11,,,,,,Industry,,,,,Unreleased,"Baichuan community license, restrictive commercial: https://huggingface.co/baichuan-inc/Baichuan2-13B-Base",Industry,,"""By integrating these strategies, our system is
capable of training Baichuan 2-7B and Baichuan
2-13B models efficiently on 1,024 NVIDIA A800
GPUs, achieving a computational efficiency that
exceeds 180 TFLOPS.""

180 TFLOPS / 311.84 TFLOPS = 57.7%",,,Operation counting,,,,
TigerBot-70B,Language,"Chat,Language generation,Language modeling/generation,Question answering",Tigerobo,"Ye Chen, Wei Cai, Liangmin Wu, Xiaowei Li, Zhanxuan Xin, Cong Fu",2023-09-06,TigerBot: An Open Multilingual Multitask LLM,"https://github.com/TigerResearch/TigerBot/blob/main/README_en.md, https://arxiv.org/abs/2312.08688",,,"Outperforms Llama 2:

""We use 10 mainstream benchmark test sets in the industry to evaluate the model's reading comprehension, reasoning, world knowledge, common sense Q&A, mathematics and coding capabilities, including: mmlu, arc, squad_v2, squad, mrqa, web_questions, openbook_qa, commonsense_qa, trivia_qa, wiki_qa. The evaluation results are shown in the figure below. The comprehensive capabilities of Tigerbot-70b are better than Llama-2-70b, and also ahead of large open source models at home and abroad.""

https://github.com/TigerResearch/TigerBot/wiki/TigerBot%E2%80%9070B%E5%8F%91%E5%B8%83%EF%BC%81",70000000000.0,70B,1.02e+24,"~1.02e24

Tigerobo did ~2.1e23 additional pre-training. We estimated Llama 2 was trained on 8.1e23 FLOP.",,"""Tigerbot-70b is further pre-trained on the foundation of Llama-2-70b using high-quality multi-language data of 300 billion tokens. ""

""We collected data from Chinese books, the internet, and encyclopedia-type data based on the distribution of GPT3 pretraining data, and filtered the data through source quality control and tf-idf soft deduplication. From 20TB of data, we filtered down to 2TB, maintaining the proportion of language and categories. On this basis, we randomly sampled 100G of data and released it open source.""",300000000000.0,,,,NVIDIA A100,,Confident,"(translated from https://github.com/TigerResearch/TigerBot/wiki/TigerBot%E2%80%9070B%E5%8F%91%E5%B8%83%EF%BC%81)

We are pleased to release Tigerbot-70b, which continues to be open source and free for commercial use, including:

Tigerbot-70b-base: Continuing pre-training on the basis of Llama-2-70b, the model's comprehensive capabilities are better than Llama-2-70b in 10 mainstream benchmark tests such as mmlu, reaching SOTA in the industry.

a. Using high-quality multi-lingual data of 300 billion tokens,

b. The algorithm uses GQA, flash-attn, RoPE, holistic-training and other technologies,

c. The training uses tensor/pipeline-partition technology, and the computing efficiency reaches the SOTA reported in the Llama-2 paper.",,,Open weights (restricted use),China,Llama 2-70B,1,70b * 300b * 6 = 126000*10^18 = 1.26*10^23,512.0,,2025-05-16 10:30,,Paper on TigerBot-70B,,4000000.0,"from paper:

""We pretrained TigerBot models using a global batch size (GBS) of 4M tokens, while fine-tuned models with a GBS as small as 100â€“400k tokens""

It's also based on pretrained Llama 2, which also used a batch size of 4M",Industry,,,,,Open source,"Apache 2.0 https://github.com/TigerResearch/TigerBot/blob/main/README_en.md

but it's also a Llama 2 finetune.

training code here: https://github.com/TigerResearch/TigerBot/tree/main/train 

They released a 5% sample of training data: "" On this basis, we randomly sampled 100G of data and released it open source.""",Industry,,,,406871.9286213644,Operation counting,,,,
360 Smart Brain,"Multimodal,Language,Image generation","Language generation,Chat,Image generation",360 Security Technology,,2023-09-04,The 360 â€‹â€‹Brain Model is now open to the public,https://www.hayo.com/article/64f68f1d8578eea6c7ec663f,,,,,"""hundreds of billions"" https://www.hayo.com/article/650babb37c769bcba319ed83",,"hundreds of billions of parameters, trained on over 1T tokens, per this: https://www.hayo.com/article/650babb37c769bcba319ed83

vague report from a Google-translated article, though",,"hundreds of billions of parameters, trained on over 1T tokens, per this: https://www.hayo.com/article/650babb37c769bcba319ed83",1000000000000.0,,,,,,Likely,"According to news on September 5, the 360 â€‹â€‹Smart Brain large model will be open to the public from now on and will be fully accessible to the 360 â€‹â€‹â€œFamily Bucketâ€.

360 Zhi Nao will be open to the public on five major platforms. Users can download the â€œ360 Zhi Naoâ€ App through the 360 â€‹â€‹Zhi Nao official website and major application stores.",,,,China,,,,,,2024-09-05 14:08,,,,,,Industry,,,,,,,Industry,,,,,,,,,
Swift,Robotics,Helicopter driving,Intel Labs,"Elia Kaufmann, Leonard Bauersfeld, Antonio Loquercio, Matthias MÃ¼ller, Vladlen Koltun, Davide Scaramuzza ",2023-08-30,Champion-level drone racing using deep reinforcement learning,https://www.nature.com/articles/s41586-023-06419-4,101.0,SOTA improvement,"""Our work marks the first time, to our knowledge, that an autonomous mobile robot achieved world-champion-level performance in a real-world competitive sport.""",56804.0,"The control network is an MLP with input dimension 31, two hidden layers of size 128, and an output of dimension 4.
(31+1)*128+(128+1)*128+(128+1)*4 = 21124

Gate detector is a 6 layer U-net with 
8*(3^3*3+1) + 16*(3^2*8+1) + 16*(3^2*16+1) + 16*(5^2*16+1) + 16*(7^2*16+1) + 16*(7^2*16+1) = 35680

35680 + 21124 = 56804",5.337e+16,"Policies are trained for a total of 1â€‰Ã—â€‰108 environment interactions, which takes 50â€‰min on a workstation (i9 12900K, RTX 3090, 32â€‰GB RAM DDR5). Fine-tuning is performed for 2â€‰Ã—â€‰107 environment interactions.

35.58 TFLOPS * 50 min * 60 s/min * 0.50 utilization = 5.337*10^16 FLOP",,,,,0.8,"50 minutes (training details, page 8)",NVIDIA GeForce RTX 3090,Reinforcement learning,Likely,"First-person view (FPV) drone racing is a televised sport in which professional competitors pilot high-speed aircraft through a 3D circuit. Each pilot sees the environment from the perspective of their drone by means of video streamed from an onboard camera. Reaching the level of professional pilots with an autonomous drone is challenging because the robot needs to fly at its physical limits while estimating its speed and location in the circuit exclusively from onboard sensors1. Here we introduce Swift, an autonomous system that can race physical vehicles at the level of the human world champions. The system combines deep reinforcement learning (RL) in simulation with data collected in the physical world. Swift competed against three human champions, including the world champions of two international leagues, in real-world head-to-head races. Swift won several races against each of the human champions and demonstrated the fastest recorded race time. This work represents a milestone for mobile robotics and machine intelligence2, which may inspire the deployment of hybrid learning-based solutions in other physical systems.",,,Unreleased,"Multinational,United States of America",,,,1.0,,2025-06-04 17:35,,,,,,Industry,,,,,Unreleased,"""Pseudocode for Swift detailing the training process and algorithms can be found in the file â€˜pseudocode.zipâ€™ on Zenodo at https://doi.org/10.5281/zenodo.7955278. To safeguard against potential misuse, the full source code associated with this research will not be made publicly available.""",Industry,,,,382.113280348051,Hardware,,,,
ABAB,Language,Language generation,MiniMax,,2023-08-30,,"https://kr-asia.com/baidus-ernie-bot-among-eight-chinese-llm-products-approved-for-public-launch, https://api.minimax.chat/",,,,,"""hundreds of billions"" per https://api.minimax.chat/",,,Unspecified unreleased,,,,,,,,Unknown,"MiniMax has released three foundational model architectures: text-to-visual, text-to-audio, and text-to-text. The startup has also introduced a self-developed general LLM â€œABABâ€, named after the sound of baby babble.",,,API access,China,,,,,,2025-01-06 13:59,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
Jais,Language,"Language modeling,Chat,Translation","Cerebras Systems,Mohamed bin Zayed University of Artificial Intelligence (MBZUAI),Inception","Neha Sengupta, Sunil Kumar Sahu, Bokang Jia, Satheesh Katipomu, Haonan Li, Fajri Koto, Osama Mohammed Afzal, Samta Kamboj, Onkar Pandit, Rahul Pal, Lalit Pradhan, Zain Muhammad Mujahid, Massa Baali, Alham Fikri Aji, Zhengzhong Liu, Andy Hock, Andrew Feldman, Jonathan Lee, Andrew Jackson, Preslav Nakov, Timothy Baldwin, Eric Xing",2023-08-29,Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models,https://arxiv.org/abs/2308.16149,24.0,SOTA improvement,SOTA at Arabic language tasks.,13000000000.0,"""With 13 billion parameters, they demonstrate better knowledge and reasoning capabilities in Arabic""",3.08e+22,C = 6ND = 6 * 13 billion params * 395 billion tokens = 3.081e+22 FLOP,"Abu El-Khair,Aranews,ArabicText 2022,C4 Arabic,Arabic Wikipedia,ArabicNews 2020,Maktabah,United Nations Parallel Corpus,The Pile,Books3,arXiv,PubMed Central,WebText2,English Wikipedia,FreeLaw,PubMed Abstracts,DeepMind Mathematics,Project Gutenberg,BookCorpus2,EuroParl,PhilPapers,YouTube Subtitles,NIH Grant Abstracts,Enron Emails,GitHub","It was pretrained on 395 billion tokens, including 116 billion Arabic tokens, 232 billion English tokens, and 46 billion tokens of code.
The Arabic data consists of 72 billion tokens, which was augmented by 18 billion tokens of translated English text and then upsampled 1.6 times to reach 116 billion tokens.
The English data is sampled from the Pile dataset and consists of 232 billion tokens.
The code data consists of 46 billion tokens sampled from GitHub.",300000000000.0,395B tokens ~= 300B words,600.0,2023 June 25 to July 18 = 25 days = 600 hours,,,Confident,"We introduce Jais and Jais-chat, new state-of-the-art Arabic-centric foundation and instruction-tuned open generative large language models (LLMs). The models are based on the GPT-3 decoder-only architecture and are pretrained on a mixture of Arabic and English texts, including source code in various programming languages. With 13 billion parameters, they demonstrate better knowledge and reasoning capabilities in Arabic than any existing open Arabic and multilingual models by a sizable margin, based on extensive evaluation. Moreover, the models are competitive in English compared to English-centric open models of similar size, despite being trained on much less English data. We provide a detailed description of the training, the tuning, the safety alignment, and the evaluation of the models. We release two open versions of the model â€”the foundation Jais model, and an instruction-tuned Jais-chat variantâ€” with the aim of promoting research on Arabic LLMs.",,,Open weights (unrestricted),"Multinational,United Arab Emirates,United States of America",,,,,,2025-06-05 17:01,,,,3932160.0,"""After packing, we used a global batch size of 1,920 sequences of 2,048 tokens each. ""","Industry,Academia",checked,,,,Unreleased,"apache 2.0
https://huggingface.co/inceptionai/jais-13b","Industry,Academia",,,,,Operation counting,inceptionai,,,
Refact-1.6B,Language,Language modeling/generation,Refact AI,"Oleg Klimov, Sergey Vakhreev",2023-08-29, Refact-1.6B ,https://huggingface.co/smallcloudai/Refact-1_6B-fim,,,,1600000000.0,1.6B,1.152e+22,"6ND = 6 * 1.6B * 1.2T = 11520000000000000000000 = 1.152e22
citation ""model trained for 1.2T tokens. ""
alternative
flops = (64) * (27770 * 10**9) * (28 * 24 * 3600) * (0.3) = 1.2898787328e+21
(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)
Precision: bfloat16
GPUs 64 NVidia A5000
Training time 28 days
27.77 TFLOPS - peak flop from https://www.techpowerup.com/gpu-specs/rtx-a5000.c3748",RedPajama,"""For the base model, we used our own dataset that contains code with permissive licenses only, and open text datasets. Filtering is the key to success of this model:

    We only used text in English
    Only topics related to computer science
    Applied heavy deduplication

The text to code proportion was 50:50, model trained for 1.2T tokens. """,1200000000000.0,"1.2T from ""The text to code proportion was 50:50, model trained for 1.2T tokens. """,672.0,"from ""Model Stats""",NVIDIA RTX A5000,Self-supervised learning,Likely,,,,Open weights (restricted use),United Kingdom of Great Britain and Northern Ireland,,,,6.0,,2025-05-09 11:32,,,,,,Industry,,,,4032.0,Unreleased,"OpenRAIL-M license, responsible use restrictions: https://bigscience.huggingface.co/blog/bigscience-openrail-m",Industry,,,,2742.1059640734707,"Operation counting,Hardware",,,,
PeptideBERT,Biology,Proteins,Carnegie Mellon University (CMU),"Chakradhar Guntuboina, Adrita Das, Parisa Mollaei, Seongwon Kim, and Amir Barati Farimani",2023-08-28,PeptideBERT: A language Model based on Transformers for Peptide Property Prediction,https://arxiv.org/abs/2309.03099,,SOTA improvement,"""Our model has achieved state of the art (SOTA) for predicting Hemolysis, which is a task for determining peptideâ€™s potential to induce red blood cell lysis.""",,,4.9e+16,"""Compute for fine-tuning ProtBERT: 1 NVidia GeForce GTX 1080Ti, 30 epochs, batch size 32, model trained for individual tasks with training time ranging from 58-116 minutes, assuming 
from Table 1 we have 244 minutes
11.34e12 FLOPs and 0.3 utilization rate FLOP = 244 min * 60 sec/min * 11.34e12 FLOP/sec *0.3 = 4.9e16 FLOP,",,,21700000001.0,"Pretraining:
217,000,000 sequences Ã— 100 residues = 2.17 Ã— 10Â¹â° tokens

Fine-tuning sequences:
9,316 + 29,892 + 17,185 = 56,393 sequences
56,393 Ã— 100 residues = 5.64 Ã— 10â¶ tokens

Total tokens:
2.17 Ã— 10Â¹â° + 5.64 Ã— 10â¶ â‰ˆ 2.17 Ã— 10Â¹â°",4.1,244 minues from Table 1,NVIDIA GeForce GTX 1080 Ti,,Confident,"Recent advances in Language Models have enabled the protein modeling community with a powerful tool since protein sequences can be represented as text. Specifically, by taking advantage of Transformers, sequence-to-property prediction will be amenable without the need for explicit structural data. In this work, inspired by recent progress in Large Language Models (LLMs), we introduce PeptideBERT, a protein language model for predicting three key properties of peptides (hemolysis, solubility, and non- fouling). The PeptideBert utilizes the ProtBERT pretrained transformer model with 12 attention heads and 12 hidden layers. We then finetuned the pretrained model for the three downstream tasks. Our model has achieved state of the art (SOTA) for predicting Hemolysis, which is a task for determining peptideâ€™s potential to induce red blood cell lysis. Our PeptideBert non-fouling model also achieved remarkable accuracy in predicting peptideâ€™s capacity to resist non-specific interactions. This model, trained predominantly on shorter sequences, benefits from the dataset where negative examples are largely associated with insoluble peptides. Codes, models, and data used in this study are freely available at: https://github.com/ChakradharG/PeptideBERT",30.0,,Open weights (unrestricted),United States of America,ProtBERT-UniRef,49805280000000000,"""Compute for fine-tuning ProtBERT: 1 NVidia GeForce GTX 1080Ti, 30 epochs, batch size 32, model trained for individual tasks with training time ranging from 58-116 minutes, assuming 
from Table 1 we have 244 minutes
11.34e12 FLOPs and 0.3 utilization rate FLOP = 244 min * 60 sec/min * 11.34e12 FLOP/sec *0.3 = 4.9e16 FLOP,",1.0,,2025-04-30 10:04,,,,,,Academia,,,,4.0,Open source,"MIT (models, training, inference): https://github.com/ChakradharG/PeptideBERT ",Academia,,,,272.95021398005304,Hardware,,,,
MathGPT,Language,Quantitative reasoning,TAL Education Group (Xueersi),,2023-08-24,TALâ€™s MathGPT launches public beta: your next-gen math assistant,https://www.gizmochina.com/2023/08/24/mathgpt-launch-public-beta-next-gen-math-assistant/,,,,,,,,,,,,,,,,Unknown,"During its 20th-anniversary event, TAL Education Group launched the public beta testing of its innovative mathematical large model, MathGPT. This LLM model is designed primarily for global mathematics enthusiasts and research institutions, marking a significant milestone as Chinaâ€™s first large model tailored for mathematics.",,,Hosted access (no API),China,,,,,,2024-09-05 14:08,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
HyperCLOVA X,Language,"Language modeling/generation,Chat,Search,Translation,Code generation,Question answering,Quantitative reasoning",NAVER,"Kang Min Yoo, Jaegeun Han, Sookyo In, Heewon Jeon, Jisu Jeong, Jaewook Kang, Hyunwook Kim, Kyung-Min Kim, Munhyong Kim, Sungju Kim, Donghyun Kwak, Hanock Kwak, Se Jung Kwon, Bado Lee, Dongsoo Lee, Gichang Lee, Jooho Lee, Baeseong Park, Seongjin Shin, Joonsang Yu, Seolki Baek, Sumin Byeon, Eungsup Cho, Dooseok Choe, Jeesung Han, Youngkyun Jin, Hyein Jun, Jaeseung Jung, Chanwoong Kim, Jinhong Kim, Jinuk Kim, Dokyeong Lee, Dongwook Park, Jeong Min Sohn, Sujung Han, Jiae Heo, Sungju Hong, Mina Jeon, Hyunhoon Jung, Jungeun Jung, Wangkyo Jung, Chungjoon Kim, Hyeri Kim, Jonghyun Kim, Min Young Kim, Soeun Lee, Joonhee Park, Jieun Shin, Sojin Yang, Jungsoon Yoon, Hwaran Lee, Sanghwan Bae, Jeehwan Cha, Karl Gylleus, Donghoon Ham, Mihak Hong, Youngki Hong, Yunki Hong, Dahyun Jang, Hyojun Jeon, Yujin Jeon, Yeji Jeong, Myunggeun Ji, Yeguk Jin, Chansong Jo, Shinyoung Joo, Seunghwan Jung, Adrian Jungmyung Kim, Byoung Hoon Kim, Hyomin Kim, Jungwhan Kim, Minkyoung Kim, Minseung Kim, Sungdong Kim, Yonghee Kim, Youngjun Kim, Youngkwan Kim, Donghyeon Ko, Dughyun Lee, Ha Young Lee, Jaehong Lee, Jieun Lee, Jonghyun Lee, Jongjin Lee, Min Young Lee, Yehbin Lee, Taehong Min, Yuri Min, Kiyoon Moon, Hyangnam Oh, Jaesun Park, Kyuyon Park, Younghun Park, Hanbae Seo, Seunghyun Seo, Mihyun Sim, Gyubin Son, Matt Yeo, Kyung Hoon Yeom, Wonjoon Yoo et al. (296 additional authors not shown)",2023-08-24,Koreaâ€™s internet giant Naver unveils generative AI services,"https://arxiv.org/abs/2404.01954
https://techcrunch.com/2023/08/24/koreas-internet-giant-naver-unveils-generative-ai-services/, https://www.ncloud.com/solution/featured/hyperclovax",,,"Possible significant usage: ""HyperCLOVA X is available for creators and enterprise customers""",,"Unknown

Previous version had 204B parameters: ""Naver says HyperCLOVA has more than 204 billion parameters, but it did not disclose how many parameters have been trained on the HyperCLOVA X""

Maybe ambiguous whether HyperCLOVA X is a new and separate model? But HyperClova is pretty old.

""HyperCLOVA X is built on HyperCLOVA and improves on the previous LLMs""

https://www.ncloud.com/solution/featured/hyperclovax",,Estimations for 82B model are marked as lower bound estimations,,,,,,,,,Speculative,"South Korean internet search company Naver on Thursday rolled out its own generative artificial intelligence tool, HyperCLOVA X. The companyâ€™s large language model (LLM) offers services such as a ChatGPT-like AI chatbot, CLOVA X, and a generative AI-based search engine, Cue, equivalent to Microsoft Bing. ",,,API access,Korea (Republic of),,,,,,2025-03-24 15:09,,,,,,Industry,,1.476000000000001e+23,,,Unreleased,"API access for HyperCLOVA X is available at CLOVA Studio, a Hyperscale AI development tool optimized for businesses and provided via NAVER Cloud Platform. The chat service is available at https://clovax.naver.com/.",Industry,,,,,,,,,
Qwen-VL,"Multimodal,Language,Vision","Image captioning,Chat,Question answering,Visual question answering",Alibaba,"Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, Jingren Zhou",2023-08-24,"Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond",https://arxiv.org/abs/2308.12966,405.0,SOTA improvement,"""As the results shown, our Qwen-VL and Qwen-VL-Chat both achieve obviously better results compared to previous
generalist models in terms of both two tasks. Specifically, on zero-shot image caption task, Qwen-VL achieves
state-of-the-art performance (i.e., 85.8 CIDEr score) on the Flickr30K karpathy-test split, even outperforms
previous generalist models with much more parameters (e.g., Flamingo-80B with 80B parameters).""",9600000000.0,9.6B total - Table 1,,"Qwen-7B and ViT as base models, trained on 1.5B image-text pairs",,"""Our pre-training dataset is composed of several publicly accessible sources and some in-house data.
We made an effort to clean the dataset of certain patterns. As summarized in Table 2, the original dataset
contains a total of 5 billion image-text pairs, and after cleaning, 1.4 billion data remain, with 77.3% English
(text) data and 22.7% Chinese (text) data.""",1400000000.0,1.4B text-image pairs,,,,,Likely,"We introduce the Qwen-VL series, a set of large-scale vision-language models designed to perceive and understand both text and images. Comprising Qwen-VL and Qwen-VL-Chat, these models exhibit remarkable performance in tasks like image captioning, question answering, visual localization, and flexible interaction. The evaluation covers a wide range of tasks including zero-shot captioning, visual or document visual question answering, and grounding. We demonstrate the Qwen-VL outperforms existing Large Vision Language Models (LVLMs). We present their architecture, training, capabilities, and performance, highlighting their contributions to advancing multimodal artificial intelligence. Code, demo and models are available at https://github.com/QwenLM/Qwen-VL.",1.0,,Open weights (restricted use),China,Qwen-7B,,"50k steps, 30k batch size (table 8)",,,2025-05-28 16:05,,,,,,Industry,,,,,Unreleased,,Industry,,,BF16,,,,,,
airoboros-l2-70b-2.1,Language,"Language modeling/generation,Question answering",,Jon Durbin,2023-08-24,airoboros: using large language models to fine-tune large language models,https://huggingface.co/jondurbin/airoboros-l2-70b-2.1,,,,70000000000.0,same as llama 2 70b,,,airoboros datasets,https://huggingface.co/datasets/jondurbin/airoboros-2.1,,,,,,,Confident,"This is my take on implementing the Self-Instruct paper. The approach is quite heavily modified, and does not use any human-generated seeds.

This updated implementation supports either the /v1/completions endpoint or /v1/chat/completions, which is particularly useful in that it supports gpt-4 and gpt-3.5-turbo (which is 1/10 the cost of text-davinci-003).

Huge thank you to the folks over at a16z for sponsoring the costs associated with building models and associated tools!",,,Open weights (restricted use),,Llama 2-70B,,,,,2025-05-01 10:42,,,,,,,,,,,Open source,"""I am purposingly leaving this license ambiguous (other than the fact you must comply with the Meta original license for llama-2) because I am not a lawyer and refuse to attempt to interpret all of the terms accordingly.

Your best bet is probably to avoid using this commercially due to the OpenAI API usage.""
https://huggingface.co/jondurbin/airoboros-l2-70b-2.1
",,,,,,,jondurbin,,,
PULI GPTrio,Language,"Chat,Translation",Hungarian Research Centre for Linguistics,"Zijian GyÅ‘zÅ‘ Yang, LÃ¡szlÃ³ JÃ¡nos Laki, TamÃ¡s VÃ¡radi & GÃ¡bor PrÃ³szÃ©ky ",2023-08-23,Mono- and Multilingual GPT-3 Models for Hungarian,https://link.springer.com/chapter/10.1007/978-3-031-40498-6_9; https://huggingface.co/NYTK/PULI-GPTrio,,,,6700000000.0,6.7B,5.8e+21,"8 A100s for three months

8 * 312 trillion * 24 * 3600 * 90 * 0.3 (utilization assumption) = 5.8e21",,"Mix of Hungarian, English, and Chinese text",230590476190.0,"adding up column in Table 2.
41.5 billion Hungarian words * assumed 1 token/word +
61.9 billion English words * 4 tokens / 3 words +
98.7 billion Chinese character * assumed 1 token/character +
33GB of github documents * 1 token / 4.2 bytes * 1e9 bytes/GB =
around 230.6 billion tokens

might be slightly off because it's counting non-Chinese tokens in the Chinese data, rather than non-Chinese words, but close.",2200.0,3 months,NVIDIA A100,,Confident,"In recent years, the growth in size of Transformer-based language models has accelerated significantly. Global technology companies are training larger and larger models that require enormous resources and training data. With these experiments, they aim to demonstrate that sufficiently large models with abundant training data can solve any natural language processing task even without fine-tuning. It may not be feasible to compete directly in this race, but there is an opportunity to conduct experiments in the direction of larger models in their shadow. Our aim is to train large language models for Hungarian. According to the knowledge transfer researches, a language model can adapt valuable knowledge from other languages. Furthermore, in order for the model to be able to solve translation tasks, it also needs multilingual knowledge. In our research, we trained a Hungarian monolingual and a Hungarian-English-Chinese trilingual 6.7 billion parameter GPT language model with more than 1TB text data. In our experiments, we also fine-tuned our model with the prompts provided by the Stanford Alpaca dataset. Thus, employing this methodology, an instruct GPT was built, which, as far as we know, is the first multilingual large language model in this region that can follow instructions.",,,Open weights (non-commercial),Hungary,,,,,,2025-02-14 16:16,,,,,,Academia,,,,,Unreleased,License: cc-by-nc-4.0 (non commercial),Academia,,,,,Hardware,,,,
ShapeMol,Biology,Drug discovery,Ohio State University,"Ziqi Chen, Bo Peng, Srinivasan Parthasarathy, Xia Ning
",2023-08-23,Shape-conditioned 3D Molecule Generation via Equivariant Diffusion Models,https://arxiv.org/abs/2308.11890,6.0,,,2700000.0,,2.59999999999998e+19,"1. Hardware: 1x Tesla V100 PCIe 32GB (1.30Ã—10Â¹â´ FLOP/s)
2. Training duration: directly provided - 140 hours total (80h Shape Encoder + 60h Diffusion Model) = 504,000 seconds
3. Utilization: 40%
4. Calculation: 1.30Ã—10Â¹â´ FLOP/s Ã— 1 GPU Ã— 504,000s Ã— 0.4 = 2.60Ã—10Â¹â¹ FLOPs",,,1593653.0,"
1. Training dataset: 1,593,653 molecules
",140.0,,NVIDIA V100,,Confident,"Ligand-based drug design aims to identify novel drug candidates of similar shapes with known active molecules. In this paper, we formulated an in silico shape-conditioned molecule generation problem to generate 3D molecule structures conditioned on the shape of a given molecule. To address this problem, we developed a translation- and rotation-equivariant shape-guided generative model ShapeMol. ShapeMol consists of an equivariant shape encoder that maps molecular surface shapes into latent embeddings, and an equivariant diffusion model that generates 3D molecules based on these embeddings. Experimental results show that ShapeMol can generate novel, diverse, drug-like molecules that retain 3D molecular shapes similar to the given shape condition. These results demonstrate the potential of ShapeMol in designing drug candidates of desired 3D shapes binding to protein target pockets.",,,,United States of America,,,,1.0,,2025-05-01 10:42,,,,,,,,,,,,,,,,,327.57672938451,Hardware,,,,
IDEFICS-80B,"Multimodal,Language,Vision","Language modeling,Image captioning,Visual question answering",Hugging Face,"Hugo Laurencon, Daniel van Strien, Stas Bekman, Leo Tronchon, Lucile Saulnier, Thomas Wang, Siddharth Karamcheti, Amanpreet Singh, Giada Pistilli, Yacine Jernite, Victor Sanh",2023-08-22,Introducing IDEFICS: An Open Reproduction of State-of-the-Art Visual Language Model,https://huggingface.co/blog/idefics,,,,80000000000.0,IDEFICS... comes in two variantsâ€”the base version and the instructed version. Each variant is available at the 9 billion and 80 billion parameter sizes.,1.1593580544e+23,"flops = 512 * 312e12 * 28*24*3600 * 0.3 = 1.159e23
(num gpus) * (peak perforemence) * (time in seconds) * (assumed utilization rate)

""The IDEFICS models were trained on an AWS SageMaker cluster with 8x80GB A100 GPUs nodes and EFA network.
    IDEFICS-80B took ~28 days of training on 64 nodes (512 GPUs).""

https://huggingface.co/HuggingFaceM4/idefics-80b-instruct 

trained on 150B text tokens + images

6ND = 6*734000000000*80*10^9 = 3.5232e+23

","Wikipedia,Public Multimodal Dataset (PMD),LAION,OBELICS","IDEFICS was trained on a mixture of openly available datasets: Wikipedia, Public Multimodal Dataset, and LAION, as well as a new 115B token dataset called OBELICS that we created. OBELICS consists of 141 million interleaved image-text documents scraped from the web and contains 353 million images.",734000000000.0,"IDEFICS was trained on a mixture of openly available datasets: Wikipedia, Public Multimodal Dataset, and LAION, as well as a new 115B token dataset called OBELICS that we created. OBELICS consists of 141 million interleaved image-text documents scraped from the web and contains 353 million images.

See https://huggingface.co/HuggingFaceM4/idefics-80b-instruct
149.6B tokens and 1.582B images in total.

Effective Batch Size (# of tokens)	3.67M
Max Training Steps	200K

3.67*10^6*200000 = 734000000000 tokens = 734B",672.0,"""IDEFICS-80b pretraining

Hardware Type: 512 NVIDIA A100 GPUs
Hours used: 672 hours (28 days)
Cloud Provider: AWS
Compute Region: US-West 2 (288g CO2eq/kWh)
Carbon Emitted: 39,498 kg of CO2eq""",NVIDIA A100,,Confident,"We are excited to release IDEFICS (Image-aware Decoder Enhanced Ã  la Flamingo with Interleaved Cross-attentionS), an open-access visual language model. IDEFICS is based on Flamingo, a state-of-the-art visual language model initially developed by DeepMind, which has not been released publicly. Similarly to GPT-4, the model accepts arbitrary sequences of image and text inputs and produces text outputs.",,,Open weights (non-commercial),"Multinational,United States of America","LLaMA-65B,CLIP ViT-H/14 - LAION-2B",,,512.0,,2025-05-26 20:09,AWS,,,3670000.0,,Industry,checked,,,,Unreleased,"Llama license (non commercial)

https://huggingface.co/HuggingFaceM4/idefics-80b-instruct",Industry,,,BF16,407007.86305450817,"Hardware,Operation counting",HuggingFaceM4,,,
IDEFICS-9B,"Multimodal,Language,Vision","Language modeling,Image captioning,Visual question answering",Hugging Face,"Hugo Laurencon, Daniel van Strien, Stas Bekman, Leo Tronchon, Lucile Saulnier, Thomas Wang, Siddharth Karamcheti, Amanpreet Singh, Giada Pistilli, Yacine Jernite, Victor Sanh",2023-08-22,Introducing IDEFICS: An Open Reproduction of State-of-the-Art Visual Language Model,https://huggingface.co/blog/idefics,,,,9000000000.0,9B,,,"OBELICS,Wikipedia,LAION,Public Multimodal Dataset (PMD)","OBELICS	Unstructured Multimodal Web Documents	114.9B text tokens	353M images 	1 epoch

Wikipedia	Unstructured Multimodal Web Documents	3.192B text tokens 39M images	3 epochs

LAION	Image-Text Pairs	29.9B text tokens 1.120B images	1 epoch

PMD	Image-Text Pairs	1.6B	text tokens 70M images	3 epochs",262000000000.0,"Sequence Length	1024
Effective Batch Size (# of tokens)	1.31M
Max Training Steps	200K

1.31*10^6*200000 = 262000000000 = 262B tokens",350.0,"""IDEFICS-9b pretraining

Hardware Type: 128 NVIDIA A100 GPUs
Hours used: 350 hours
Cloud Provider: AWS
Compute Region: US-West 2 (288g CO2eq/kWh)
Carbon Emitted: 5,160 kg of CO2eq""",NVIDIA A100,,Confident,"IDEFICS (Image-aware Decoder Enhanced Ã  la Flamingo with Interleaved Cross-attentionS) is an open-access reproduction of Flamingo, a closed-source visual language model developed by Deepmind. Like GPT-4, the multimodal model accepts arbitrary sequences of image and text inputs and produces text outputs. IDEFICS is built solely on publicly available data and models.

The model can answer questions about images, describe visual contents, create stories grounded on multiple images, or simply behave as a pure language model without visual inputs.

IDEFICS is on par with the original closed-source model on various image-text benchmarks, including visual question answering (open-ended and multiple choice), image captioning, and image classification when evaluated with in-context few-shot learning. It comes into two variants: a large 80 billion parameters version and a 9 billion parameters version.",,,Open weights (non-commercial),"Multinational,United States of America","CLIP ViT-H/14 - LAION-2B,LLaMA-7B",1,"6ND = 6*262000000000*9*10^9 = 1.4148e+22

Hardware Type: 128 NVIDIA A100 GPUs
Hours used: 350 hours
Cloud Provider: AWS
Compute Region: US-West 2 (288g CO2eq/kWh)
Carbon Emitted: 5,160 kg of CO2eq

128*350*3600*312000000000000*0.3 = 1.5095808e+22

sqrt(1.4148e+22*1.5095808e+22) = 1.4614222e+22",128.0,,2025-05-01 10:42,AWS,,,,,Industry,,,,,,"The model is built on top of two pre-trained models: laion/CLIP-ViT-H-14-laion2B-s32B-b79K and huggyllama/llama-65b. The first was released under an MIT license, while the second was released under a specific non-commercial license focused on research purposes. As such, users should comply with that license by applying directly to Meta's form.

The two pre-trained models are connected to each other with newly initialized parameters that we train. These are not based on any of the two base frozen models forming the composite model. We release the additional weights we trained under an MIT license.",Industry,,,BF16,101751.96576362704,"Operation counting,Hardware",HuggingFaceM4,,,
Dou Bao,Language,"Chat,Language modeling/generation",ByteDance,,2023-08-18,,https://pandaily.com/bytedance-launches-its-first-large-scale-ai-conversation-product-dou-bao/,,,,,,,,,,,,,,,,Unknown,"The first AI conversational app â€œDou Baoâ€ and its web version have recently been launched, with the download channel for Android already open. The â€œDou Baoâ€ is the internal codename â€œGraceâ€ AI project by ByteDance, and currently has functions such as text-based conversation and image-based conversation.",,,Hosted access (no API),China,,,,,,2024-12-08 14:55,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
KwaiYii 13B,Language,Chat,Kuaishou Technology,,2023-08-16,"""KwaiYii"" large-scale language model (KwaiYii)",https://github.com/kwai/KwaiYii,,,"""The KwaiYii-13B-Base pre-training model has excellent general technical base capabilities and has achieved State-Of-The-Art effect under the same model size on most authoritative Chinese/English Benchmarks. For example, the KwaiYii-13B-Base pre-trained model is currently at the leading level of the same model size on MMLU, CMMLU, C-Eval, HumanEval and other benchmarks.""",13000000000.0,13B,,,,,,,,,,,Likely,"""KwaiYii"" is a series of large-scale language models (LLM) independently developed by the Kuaishou AI team from scratch. It currently includes models with multiple parameter sizes and covers pre-training models. (KwaiYii-Base), dialogue model (KwaiYii-Chat). Here we introduce the 13B scale series model KwaiYii-13B.""",,,Unreleased,China,,,,,,2025-05-01 10:42,,,,,,Industry,,,,,,,Industry,,,,,,,,,
VARCO LLM 2.0 base,Language,"Language modeling/generation,Chat,Translation,Question answering",NCSOFT,,2023-08-16,VARCO LLM 2.0 is NCSOFT's large language model that can be applied to the development of natural language processing-based AI services.,"https://ncsoft.github.io/ncresearch/varco-llm-details/
https://aws.amazon.com/marketplace/pp/prodview-d7amr4yxpibew?sr=0-3&ref_=beagle&applicationId=AWSMPContessa",,,,13000000000.0,,1.248e+23,=1600000000000 tokens * 6 FLOP / token / parameter * 13000000000 parameters = 1.248Ã—10^23 FLOP,,"""Our LLM is trained with datasets that are either publicly available for pretraining, collected from the Internet or internally constructed,â€ Jehee Lee, CRO of NCSOFT, told Engadget via email.",1600000000000.0,https://ncsoft.github.io/ncresearch/varco-llm-details/,,,,,Likely,"VARCO LLM 2.0 is NCSOFT's large language model that can be applied to the development of various natural language processing-based AI services such as text generation, question answering, chatbots, summarization, and information extraction. NCSOFT's VARCO LLM 2.0 was developed with our own technology, including data construction, pre-training, instruction tuning and alignment tuning. We evaluated VARCO LLM 2.0 on various NLP tasks and its performance has significantly improved compared to VARCO LLM 1.0, and it boasts the highest performance among other Korean LLMs of similar sizes. In particular, it has been trained to be used in high-level natural language processing applications such as creative writing, summarization, question and answering, chatbots and translation, and shows high performance in related quantitative indicators. For inquiries regarding further performance improvement or collaboration for service applications, please contact us by email (varco_llm@ncsoft.com).

Korean Text Generation : VARCO LLM 2.0 is optimized for Korean natural language generation applications. In particular, it provides more natural and creative responses in understanding user instructions and generating text.",,,API access,Korea (Republic of),,,,,,2025-05-26 19:46,,,,,,Industry,,,,,Unreleased,,Industry,,,,,Operation counting,,,,
DeciCoder-1B,Language,Code generation,Deci AI,DeciAI Research Team,2023-08-15, Model Card for DeciCoder-1b,https://huggingface.co/Deci/DeciCoder-1b,,,,1100000000.0,"1.1B, per model card https://huggingface.co/Deci/DeciCoder-1b",2.9e+21,446b * 1.1b * 6 = 2.9e21,StarCoder,"""DeciCoder was trained on StarCoder Training Dataset, filtered for Python, Java, and Javascript code""",446000000000.0,Total Tokens: 446B ,,,,,Confident,"DeciCoder 1B is a 1 billion parameter decoder-only code completion model trained on the Python, Java, and Javascript subsets of Starcoder Training Dataset. The model uses Grouped Query Attention and has a context window of 2048 tokens. It was trained using a Fill-in-the-Middle training objective. The model's architecture was generated by Deci's proprietary Neural Architecture Search-based technology, AutoNAC.",,,Open weights (unrestricted),Israel,,,,,,2025-02-14 16:17,,,,,,Industry,,,,,Unreleased,"Apache 2.0

data is StarCoder, license unclear",Industry,,,,,Operation counting,,,,
Platypus-70B,Language,Language generation,Boston University,"Ariel N. Lee, Cole J. Hunter, Nataniel Ruiz",2023-08-14,"Platypus: Quick, Cheap, and Powerful Refinement of LLMs","https://arxiv.org/abs/2308.07317, https://platypus-llm.github.io/",117.0,,"SOTA for open-source but not in general (per Table 3)

""As per the Hugging Face Open LLM Leaderboard data dated 8/10/23 (Table 3), our Platypus2-70Binstruct variant has outperformed its competitors, securing the top position with an average score of 73.13""",70000000000.0,fine-tuned from LLaMa 2-70B,,,,"Platypus is fine-tuned Llama 2, so starts with whatever dataset was used to train Llama 2. Then they fine-tuned using Open-Platypus:

""Open-Platypus, a small-scale dataset that consists of a curated sub-selection of public text datasets. The dataset is focused on improving LLMsâ€™ STEM and logic knowledge, and is made up of 11 open-source datasets. It is comprised mainly of human-designed questions, with only 10% of questions generated by an LLM. """,,,,see finetune compute,NVIDIA A100,,Confident,"We present Platypus, a family of fine-tuned and merged Large Language Models (LLMs) that achieves the strongest performance and currently stands at first place in HuggingFace's Open LLM Leaderboard as of the release date of this work. In this work we describe (1) our curated dataset Open-Platypus, that is a subset of other open datasets and which we release to the public (2) our process of fine-tuning and merging LoRA modules in order to conserve the strong prior of pretrained LLMs, while bringing specific domain knowledge to the surface (3) our efforts in checking for test data leaks and contamination in the training data, which can inform future research. Specifically, the Platypus family achieves strong performance in quantitative LLM metrics across model sizes, topping the global Open LLM leaderboard while using just a fraction of the fine-tuning data and overall compute that are required for other state-of-the-art fine-tuned LLMs. In particular, a 13B Platypus model can be trained on a single A100 GPU using 25k questions in 5 hours. This is a testament of the quality of our Open-Platypus dataset, and opens opportunities for more improvements in the field. Project page: this https URL",,,Open weights (non-commercial),United States of America,Llama 2-70B,39540000000000000000,"22 hours on 4 A100 GPUs (""our 70B model using 4 A100s 80GB for 22 hours."")

If FP16, this was around 312 TFLOPS * 88 hours * 40% = 3.954*10^19 FLOP

It would be double this if they used INT8.",,,2024-11-01 10:05,,,,,,Academia,,,,,Open (non-commercial),"Non-Commercial Creative Commons license (CC BY-NC-4.0)

dataset has several licenses but includes non-comm: https://huggingface.co/datasets/garage-bAInd/Open-Platypus

code: https://github.com/arielnlee/Platypus",Academia,,,,,,,,,
Code Llama-34B,Language,Code generation,Meta AI,"Baptiste RoziÃ¨re, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Ellen Tan, Yossef (Yossi) Adi, Jingyu Liu, Tal Remez, JÃ©rÃ©my Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Defossez, Jade Copet, Faisal Azhar, Hugo Touvron, Gabriel Synnaeve, Louis Martin, Nicolas Usunier, Thomas Scialom",2023-08-14,Code Llama: Open Foundation Models for Code,"https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/
https://arxiv.org/abs/2308.12950",1297.0,,"SOTA for open models: ""Moreover, our largest model,
with its 34B parameters, is significantly larger than previous open-source models â€“ GPT-NeoX-20B (Black
et al., 2022) and StarCoder with 15.5B parameters â€“ which allows it to achieve state-of-the-art performances
on HumanEval, MBPP and MultiPL-E among open-source models""",34000000000.0,34B,5.3e+23,"1.22e23 finetune compute, or ~5.3e23 including Llama-2 34B base compute. See finetune compute notes for calculation.
",Unspecified unreleased,"""As shown in Table 1, Code Llama is trained predominantly on a near-deduplicated dataset of publicly available code. We also source 8% of our samples data from natural language datasets related to
code. This dataset contains many discussions about code and code snippets included in natural language questions or answers. To help the model retain natural language understanding skills, we also sample a small proportion of our batches from a natural language dataset""",600000000000.0,"Llama 2 used 2T tokens, and ""We train Code Llama on 500B additional tokens and Code Llama - Python further on 100B tokens""

2T + 500B + 100B = 2600000000000",,,NVIDIA A100 SXM4 80 GB,,Confident,"We release Code Llama, a family of large language models for code based on Llama 2 providing state-of-the-art performance among open models, infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks. We provide multiple flavors to cover a wide range of applications: foundation models (Code Llama), Python specializations (Code Llama - Python), and instruction-following models (Code Llama - Instruct) with 7B, 13B and 34B parameters each. All models are trained on sequences of 16k tokens and show improvements on inputs with up to 100k tokens. 7B and 13B Code Llama and Code Llama - Instruct variants support infilling based on surrounding content. Code Llama reaches state-of-the-art performance among open models on several code benchmarks, with scores of up to 53% and 55% on HumanEval and MBPP, respectively. Notably, Code Llama - Python 7B outperforms Llama 2 70B on HumanEval and MBPP, and all our models outperform every other publicly available model on MultiPL-E. We release Code Llama under a permissive license that allows for both research and commercial use.",,,Open weights (restricted use),United States of America,Llama 2-34B,1,"Training the nine Code Llama models took 400k A100-hours across all the models, per model card. It's nine models because there are three base models at 7B, 13B, 34B, and then Instruct and Python models across all three sizes. I'll calculate for Code Llama Python-34B since it's the most trained.

Code Llama-base is trained from Llama 2 with 500B tokens: ""We train Code Llama on 500B tokens during the initial phase, starting from the 7B, 13B, and 34B versions of Llama 2""

Code Llama-Python required an additional 100B tokens in fine-tuning: ""We train Code Llama on 500B additional tokens and Code Llama - Python further on 100B tokens.""
Code Llama-Instruct is fine-tuned on 5B tokens: ""For Code Llama - Instruct, we train with a batch size of 524,288 tokens and on approx. 5B tokens in total.""

34B * (500B+100B) * 6 = 1.22e23

Using the hardware method, we get 400,000 * 3600 * 312 trillion * 0.3 = 1.32e23 for the whole family. Using 34/(34+13+7) as the proportion of compute used for the 34B model, we get 0.63 * 1.32e23 = 8.5e22. (this is rounding down instruct tuning to 0).

Token*params estimate is probably better; seems like evidence of utilization > 0.3.",,,2024-11-25 15:29,,,,4000000.0,"Llama 2 pretraining used 4M batches. I believe the sentence below refers to the training from Llama 2 -> Code Llama-base. 

""We use a batch size of 4M tokens which are presented as sequences of 4,096 tokens each.""

Subsequent fine-tuning batch sizes are 500k-1M. 

""For Code Llama - Instruct, we train with a batch size of 524,288 tokens and on approx. 5B tokens in total... For long context fine-tuning (LCFT)... the batch size is set to 2M tokens for model sizes 7B and 13B and to 1M tokens for model size 34B, respectively. Training lasts for 10,000 gradient steps by default."" ",Industry,,,,,Unreleased,"Llama 2 license. can't use outputs to train models.

https://github.com/meta-llama/llama/blob/main/LICENSE",Industry,,,,,Operation counting,,,,
Japanese-LM-3.6B,Language,Language modeling,LINE Corporation,"Shun Kiyono, Sho Takase, Toshinori Sato",2023-08-14,japanese-large-lm-3.6b,"https://engineering.linecorp.com/ja/blog/3.6-billion-parameter-japanese-language-model, https://huggingface.co/line-corporation/japanese-large-lm-3.6b",,,,3600000000.0,3.6B,,"1.7B model was trained on 4000 A100-hours:

""Regarding the time required to build this model, for example, the 1.7B model is converted to A100 80GB and takes about 4000 GPU hours""

4000 * 3600 * 312 teraFLOPS * 0.3 =1.35e21

speculative, the 3.6B model may be about 2x that, but not clear it was trained on the same number of epochs

Using C=6ND, C = 6 * 3.6e9 * 650GB * 111e6 word/GB / 1 word/token. 1.6e21 FLOP.

If we assume 2 epochs, 3.2e21 FLOP. Certainly <1e23 FLOP.
",,"""Our training corpus consists of the Japanese portions of publicly available corpus such as C4, CC-100, and Oscar. We also incorporated the Web texts crawled by in-house system. The total size of our training corpus is about 650 GB.""

https://huggingface.co/line-corporation/japanese-large-lm-3.6b",72000000000.0,"650GB per huggingface

our guide says 111M Japanese words per GB, which would be ~72B words

https://docs.google.com/document/d/1G3vvQkn4x_W71MKg0GmHVtzfd9m0y3_Ofcoew0v902Q/edit#heading=h.ieihc08p8dn0",,,NVIDIA A100,,Likely,"(google translated)

In this article, we will introduce the 3.6 Billion (3.6 Billion) and 1.7 Billion (1.7 Billion) parameters that we trained and published in Japanese. While introducing the language models (hereinafter referred to as the 3.6B model and 1.7B model, respectively), we will share the know-how on language model construction that we have gained along the way.",,,Open weights (unrestricted),Japan,,,,,,2024-09-05 14:08,,,,,,Industry,,,,,Unreleased,"Apache 2.0 for weights
https://huggingface.co/line-corporation/japanese-large-lm-3.6b",Industry,,,,,,,,,
Code Llama-7B,Language,Code generation,Meta AI,"Baptiste RoziÃ¨re, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Ellen Tan, Yossef (Yossi) Adi, Jingyu Liu, Tal Remez, JÃ©rÃ©my Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Defossez, Jade Copet, Faisal Azhar, Hugo Touvron, Gabriel Synnaeve, Louis Martin, Nicolas Usunier, Thomas Scialom",2023-08-14,Code Llama: Open Foundation Models for Code,"https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/
https://arxiv.org/abs/2308.12950",1297.0,,,7000000000.0,7B,1.1e+23,"2.5e22 finetune compute + 8.4e22 base compute for Llama 2-7B, for ~1.1e23 compute overall

Table 26: ""In aggregate, training all 12 Code Llama models required 1400K GPU hours of computation on hardware of type A100-80GB""
Suggests all versions took a combined 4.7e23 FLOPs:
3.12e14 * 1400000 * 3600 * 0.3 = 4.7e23

Assuming this refers to the finetune compute only, agrees with our finetune estimate if compute is proportional to parameter count:
7 / (7+13+34+70) = 0.056
0.056 * 4.7e23 = 2.65e22
",Unspecified unreleased,"""As shown in Table 1, Code Llama is trained predominantly on a near-deduplicated dataset of
publicly available code. We also source 8% of our samples data from natural language datasets related to
code. This dataset contains many discussions about code and code snippets included in natural language
questions or answers. To help the model retain natural language understanding skills, we also sample a small
proportion of our batches from a natural language dataset""",600000000000.0,"Llama 2 used 2T tokens, and ""We train Code Llama on 500B additional tokens and Code Llama - Python further on 100B tokens""

2T + 500B + 100B = 2600000000000",,,NVIDIA A100 SXM4 80 GB,,Confident,"We release Code Llama, a family of large language models for code based on Llama 2 providing state-of-the-art performance among open models, infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks. We provide multiple flavors to cover a wide range of applications: foundation models (Code Llama), Python specializations (Code Llama - Python), and instruction-following models (Code Llama - Instruct) with 7B, 13B and 34B parameters each. All models are trained on sequences of 16k tokens and show improvements on inputs with up to 100k tokens. 7B and 13B Code Llama and Code Llama - Instruct variants support infilling based on surrounding content. Code Llama reaches state-of-the-art performance among open models on several code benchmarks, with scores of up to 53% and 55% on HumanEval and MBPP, respectively. Notably, Code Llama - Python 7B outperforms Llama 2 70B on HumanEval and MBPP, and all our models outperform every other publicly available model on MultiPL-E. We release Code Llama under a permissive license that allows for both research and commercial use.",,,Open weights (restricted use),United States of America,Llama 2-7B,2,"Code Llama-base is trained from Llama 2 with 500B tokens: ""We train Code Llama on 500B tokens during the initial phase, starting from the 7B, 13B, and 34B versions of Llama 2""

Code Llama-Python required an additional 100B tokens in fine-tuning: ""We train Code Llama on 500B additional tokens and Code Llama - Python further on 100B tokens.""
Code Llama-Instruct is fine-tuned on 5B tokens: ""For Code Llama - Instruct, we train with a batch size of 524,288 tokens and on approx. 5B tokens in total.""

7B * (500B+100B) * 6 = 2.5e22

Table 26: ""In aggregate, training all 12 Code Llama models required 1400K GPU hours of computation on hardware of type A100-80GB""
Suggests all versions took a combined 4.7e23 FLOPs:
3.12e14 * 1400000 * 3600 * 0.3 = 4.7e23

Assuming this refers to the finetune compute only, agrees with our finetune estimate if compute is proportional to parameter count:
7 / (7+13+34+70) = 0.056
0.056 * 4.7e23 = 2.65e22
",,,2024-11-25 15:29,,,,4000000.0,,Industry,,,,,Unreleased,"Llama 2 license. can't use outputs to train models.

https://github.com/meta-llama/llama/blob/main/LICENSE",Industry,,,,,Operation counting,,,,
Code Llama-13B,Language,Code generation,Meta AI,"Baptiste RoziÃ¨re, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Ellen Tan, Yossef (Yossi) Adi, Jingyu Liu, Tal Remez, JÃ©rÃ©my Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Defossez, Jade Copet, Faisal Azhar, Hugo Touvron, Gabriel Synnaeve, Louis Martin, Nicolas Usunier, Thomas Scialom",2023-08-14,Code Llama: Open Foundation Models for Code,"https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/
https://arxiv.org/abs/2308.12950",,,,13000000000.0,13B,,,Unspecified unreleased,,600000000000.0,"""We train Code Llama on 500B additional tokens and Code Llama - Python further on 100B tokens""",,,NVIDIA A100 SXM4 80 GB,,Confident,"We release Code Llama, a family of large language models for code based on Llama 2 providing state-of-the-art performance among open models, infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks. We provide multiple flavors to cover a wide range of applications: foundation models (Code Llama), Python specializations (Code Llama - Python), and instruction-following models (Code Llama - Instruct) with 7B, 13B and 34B parameters each. All models are trained on sequences of 16k tokens and show improvements on inputs with up to 100k tokens. 7B and 13B Code Llama and Code Llama - Instruct variants support infilling based on surrounding content. Code Llama reaches state-of-the-art performance among open models on several code benchmarks, with scores of up to 53% and 55% on HumanEval and MBPP, respectively. Notably, Code Llama - Python 7B outperforms Llama 2 70B on HumanEval and MBPP, and all our models outperform every other publicly available model on MultiPL-E. We release Code Llama under a permissive license that allows for both research and commercial use.",,,Open weights (restricted use),United States of America,Llama 2-13B,4,600000000000*13*10^9*6 = 4.68e+22,,,2025-05-01 10:42,,,,4000000.0,,Industry,,,,,Unreleased,"Llama 2 license. can't use outputs to train models.

https://github.com/meta-llama/llama/blob/main/LICENSE",Industry,,,,,Operation counting,,,,
Japanese StableLM Base Alpha 7B,Language,"Language modeling/generation,Translation",Stability AI,"Meng Lee, Fujiki Nakamura, Makoto Shing, Paul McCann, Takuya Akiba, Naoki Orii",2023-08-10,"Japanese StableLM, Marking Entry into International Language Model  Market",https://stability.ai/news/stability-ai-new-jplm-japanese-language-model-stablelm,,,"best open-source Japanese LM: ""It stands as the top-performing publicly available Japanese language model, according to a benchmark suite against four sets of other Japanese LMs.""",7000000000.0,7B,3.15e+22,"7b params, 750b tokens
7b * 750b * 6 = 3.15e22",,Japanese language sources + RedPajama,750000000000.0,750B tokens,,,,,Confident,"Japanese StableLM is a 7 billion-parameter general-purpose language model. It stands as the top-performing publicly available Japanese language model, according to a benchmark suite against four sets of other Japanese LMs.

Japanese StableLM Base Alpha 7B will be released under the commercially available Apache License 2.0. Japanese StableLM Instruct Alpha 7B is a model created for research purposes and is released exclusively for research use. For details, please refer to the Hugging Face Hub page.",,,Open weights (unrestricted),United Kingdom of Great Britain and Northern Ireland,,,,,,2025-05-27 10:35,,,,,,Industry,,,,,Unreleased,"Apache 2.0 for weights
data appears to be open with unclear licenses: https://huggingface.co/stabilityai/japanese-stablelm-base-alpha-7b",Industry,,,,,Operation counting,stabilityai,,,
Baichuan2-53B,Language,"Language modeling/generation,Chat",Baichuan,,2023-08-09,Chinese AI startup Baichuan rolls out third LLM in four months,https://technode.com/2023/08/09/chinese-ai-startup-baichuan-rolls-out-third-llm-in-four-months/,,,,53000000000.0,,8.268e+23,"Given that it was announced at a similar time to the other Baichuan2 models, this assumes that the dataset size is the same at 2.6T tokens while the parameter count was scaled up. This would be consistent with many other model releases, such as Meta's Llama models.
53b * 2.6t * 6 = 8.268e23
",,,,,,,,,Likely,"On Tuesday, four-month-old AI startup Baichuan Intelligent Technology unveiled its first closed-source model equipped with 53 billion parameters. Following the Chinese companyâ€™s rapid release of two open-source large language models since its founding in April, the new model demonstrates the firmâ€™s fast pace in delivering pre-trained models for larger parameters. The freshly introduced model, Baichuan-53B, is mainly for corporate clients and focused on text generation. A ChatGPT-like chat service built on the model entered internal testing on the same day the model was launched, its official website shows, with plans for the firm to publicly launch APIs and associated components next month.",,,Unreleased,China,,,,,,2025-05-23 13:56,,,,,,Industry,,,,,Unreleased,,Industry,,,,,Operation counting,,,,
Claude Instant,Language,"Language modeling,Chat",Anthropic,,2023-08-09,Releasing Claude Instant 1.2,https://www.anthropic.com/news/releasing-claude-instant-1-2,,,"Possibly widely used, seems to only be available via API and not Anthropic's chat interface. 
Used by one company for customer service: https://www.anthropic.com/news/prompt-engineering-for-business-performance",,"speculatively, Anthropic charges 1/10 as much for Claude Instant as Claude 2, so it may have around 1/10 the parameters (Claude 2 parameters are not public info)

https://cdn.sanity.io/files/4zrzovbb/website/90df03aed08b794ab03c5a7bf28b2ad9cf26cf3c.pdf",,,Unspecified unreleased,"Assuming this refers to Instant 1.2, this has a knowledge cutoff date of January 2023, according to https://github.com/HaoooWang/llm-knowledge-cutoff-dates?tab=readme-ov-file and https://docsbot.ai/models/claude-instant-1-2.",,,,,,,Unknown,"Businesses working with Claude can now access our latest version of Claude Instant, version 1.2, available through our API. Claude Instant is our faster, lower-priced yet still very capable model, which can handle a range of tasks including casual dialogue, text analysis, summarization, and document comprehension.

Claude Instant 1.2 incorporates the strengths of our latest model Claude 2 in real-world use cases and shows significant gains in key areas like math, coding, reasoning, and safety. It generates longer, more structured responses and follows formatting instructions better. Instant 1.2 also shows improvements in quote extraction, multilingual capabilities, and question answering.",,,API access,United States of America,,,,,,2025-05-12 21:18,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
CALM,Robotics,Animal (human/non-human) imitation,"NVIDIA,Technion - Israel Institute of Technology","Chen Tessler, Yoni Kasten, Yunrong Guo, Shie Mannor, Gal Chechik, Xue Bin Peng",2023-08-06,CALM: Conditional Adversarial Latent Models for Directable Virtual Characters,https://research.nvidia.com/labs/par/calm/,9.0,,,,,,The total pre-training of the networks involved 5 billion environment steps. The low-level policy operates at 30Hz while the high-level policy operates at 6Hz.,,"160 diverse motion clips totaling 30 minutes, from a motion capture dataset. These include motions like walking, running, sword strikes, etc.",,,,,NVIDIA A100,Reinforcement learning,Unknown,"In this work, we present Conditional Adversarial Latent Models (CALM),
an approach for generating diverse and directable behaviors for user-controlled
interactive virtual characters. Using imitation learning, CALM learns a representation of movement that captures the complexity and diversity of human
motion, and enables direct control over character movements. The approach
jointly learns a control policy and a motion encoder that reconstructs key
characteristics of a given motion without merely replicating it. The results
show that CALM learns a semantic motion representation, enabling control
over the generated motions and style-conditioning for higher-level task training. Once trained, the character can be controlled using intuitive interfaces,
akin to those found in video games.",,,,"United States of America,Israel",,,,,,2025-05-09 11:32,,,,,,"Industry,Academia",,,,,,,"Industry,Academia",,,,,,,,,
SS-pLM,Biology,Protein or nucleotide language model (pLM/nLM),"Nostrum Biodiscovery,Barcelona Supercomputing Center,Institucio Catalana de Recerca i Estudis AvancÃ§ats","Yaiza Serrano, Sergi Roda, Victor Guallar, Alexis Molina",2023-08-06,Efficient and accurate sequence generation with small-scale protein language models,https://www.biorxiv.org/content/10.1101/2023.08.04.551626.abstract,3.0,,,14800000.0,From Table 3,2.28096e+19,"1. Hardware setup: 4x NVIDIA A30 GPUs (1.50Ã—10Â¹â´ FLOP/s per GPU in FP16)
2. Training duration: 1 day (86,400 seconds) - directly provided
3. Utilization rate: 40%
4. Calculation: 4 GPUs Ã— 1.50Ã—10Â¹â´ FLOP/s Ã— 86,400s Ã— 0.40 = 2.07Ã—10Â¹â¹ FLOPs

4*1.7e+14*0.4*86400s=2.3e+19",UniRef50,,525000001.0,Total Datapoints = 1.75 Ã— 10â¶ sequences Ã— 300 tokens/sequence = 5.25 Ã— 10â¸ tokens,24.0,,NVIDIA A30 PCIe,,Confident,"Large Language Models (LLMs) have demonstrated exceptional capabilities in understanding contextual relationships, outperforming traditional methodologies in downstream tasks such as text generation and sentence classification. This success has been mirrored in the realm of protein language models (pLMs), where proteins are encoded as text via their amino acid sequences. However, the training of pLMs, which involves tens to hundreds of millions of sequences and hundreds of millions to billions of parameters, poses a significant computational challenge.

In this study, we introduce a Small-Scale Protein Language Model (SS-pLM), a more accessible approach that requires training on merely millions of representative sequences, reducing the number of trainable parameters to 14.8M. This model significantly reduces the computational load, thereby democratizing the use of foundational models in protein studies. We demonstrate that the performance of our model, when fine-tuned to a specific set of sequences for generation, is comparable to that of larger, more computationally demanding pLM.",,,,"Spain,Spain,Spain",,,,4.0,,2025-05-01 10:42,,,,,,"Government,Research collective",,,,,,,"Government,Research collective",,,,1312.1138696054595,Hardware,,,,
StableLM-Base-Alpha-7B,Language,Language modeling,Stability AI,,2023-08-05,,https://huggingface.co/stabilityai/stablelm-base-alpha-7b-v2,,,,6890209280.0,,4.5e+22,"""StableLM-Base-Alpha-7B-v2 is pre-trained using a multi-stage context length extension schedule following similar work (Nijkamp et al. 2023); first pre-training at a context length of 2048 for 1 trillion tokens, then fine-tuning at a context length of 4096 for another 100B tokens""

6890209280 params * 1.1 trillion tokens * 6 = 4.5e22

alternatively: ""StableLM-Base-Alpha-7B-v2 was trained on the Stability AI cluster - occupying 384 NVIDIA A100 40GB GPUs across AWS P4d instances. Training took approximately 16.33 days to complete across both stages.""

312 teraflops * 384 * 16.33 * 24 * 3600 * 0.3 = 5.07e22","RefinedWeb,RedPajama-Data,The Pile","""The first pre-training stage relies on 1 trillion tokens sourced from a mix of the public Falcon RefinedWeb extract (Penedo et al., 2023), RedPajama-Data (Together Computer 2023, The Pile (Gao et al., 2020), and internal datasets with web text sampled at a rate of 71%.

In the second stage, we include the StarCoder (Li et al., 2023) dataset and down sample web text to 55% while increasing sampling proportions of naturally long text examples in the aforementioned sources.""",750000000000.0,1 trillion tokens,392.0,16.33 days,NVIDIA A100 SXM4 40 GB,,Confident,"StableLM-Base-Alpha-7B-v2 is a 7 billion parameter decoder-only language model pre-trained on diverse English datasets. This model is the successor to the first StableLM-Base-Alpha-7B model, addressing previous shortcomings through the use of improved data sources and mixture ratios.",1.0,,Open weights (unrestricted),United Kingdom of Great Britain and Northern Ireland,,,,,,2025-05-27 10:50,,,,,,Industry,,,,,Unreleased,"CC BY-SA (permissive):

https://creativecommons.org/licenses/by-sa/4.0/",Industry,,,,,Operation counting,stabilityai,,,
GGNN,Biology,"Proteins,Protein interaction prediction","Westlake University,Tsinghua University,Toyota Technological Institute at Chicago","Fang Wu, Lirong Wu, Dragomir Radev, Jinbo Xu and Stan Z. Li",2023-08-05,Integration of pre-trained protein language models into geometric deep learning networks,https://www.nature.com/articles/s42003-023-05133-1,19.0,SOTA improvement,"""In this work, we integrate the knowledge learned by well-trained protein language models into several state-of-the-art geometric networks and evaluate a variety of protein representation learning benchmarks, including protein-protein interface prediction, model quality assessment, protein-protein rigid-body docking, and binding affinity prediction. Our findings show an overall improvement of 20% over baselines.""",,"ESM-2 650M is used as the main PLM, they run ablations with versions up to 3B. Unclear how many parameters are are in the geometric graph neural network module.",7.56e+21,"ESM-2 650M is very likely the majority of FLOPs, since they only used 2 A100s (ESM-2 650M used 512 V100s for 8 days). As such I'm reporting the compute from ESM-2 650M here only.",,,,,,,NVIDIA A100 SXM4 80 GB,,Confident,"Geometric deep learning has recently achieved great success in non-Euclidean domains, and learning on 3D structures of large biomolecules is emerging as a distinct research area. However, its efficacy is largely constrained due to the limited quantity of structural data. Meanwhile, protein language models trained on substantial 1D sequences have shown burgeoning capabilities with scale in a broad range of applications. Several preceding studies consider combining these different protein modalities to promote the representation power of geometric neural networks but fail to present a comprehensive understanding of their benefits. In this work, we integrate the knowledge learned by well-trained protein language models into several state-of-the-art geometric networks and evaluate a variety of protein representation learning benchmarks, including protein-protein interface prediction, model quality assessment, protein-protein rigid-body docking, and binding affinity prediction. Our findings show an overall improvement of 20% over baselines. Strong evidence indicates that the incorporation of protein language modelsâ€™ knowledge enhances geometric networksâ€™ capacity by a significant margin and can be generalized to complex tasks.",,,Unreleased,"China,China,United States of America",ESM2-650M,,,2.0,,2025-06-02 15:28,,,,,,"Academia,Academia,Academia",,,,,Open source,"MIT license
https://github.com/smiles724/GNN-Bottleneck","Academia,Academia,Academia",,,,1590.4764725839211,Other,,,,
Weblab-10B,Language,Language modeling/generation,Matsuo Lab, Takeshi Kojima,2023-08-04, weblab-10b,https://huggingface.co/matsuo-lab/weblab-10b,,,,10000000000.0,,3.6e+22,"6ND = 10B*600B * 6 = 3.6e22"" The model was trained on around 600B tokens from a mixture of the following corpora.""

See also: https://weblab.t.u-tokyo.ac.jp/en/100%E5%84%84%E3%83%91%E3%83%A9%E3%83%A1%E3%83%BC%E3%82%BF%E3%82%B5%E3%82%A4%E3%82%BA%E3%83%BB%E6%97%A5%E8%8B%B12%E3%83%B6%E5%9B%BD%E8%AA%9E%E5%AF%BE%E5%BF%9C%E3%81%AE%E5%A4%A7%E8%A6%8F%E6%A8%A1/","The Pile,Japanese C4","""The model was trained on around 600B tokens from a mixture of the following corpora.
    Japanese C4
    The Pile
""",256131000000.0,"Japanese C4 is 821 GB; The Pile is 825 GB.
There are ~111 million tokens per GB for Japanese text and ~200M for English.
(821 * 111M) + (825 * 200) = 256,131,000,000 tokens

Model saw 600B tokens, so ~ 2.34 epochs
",,,,Self-supervised learning,Confident,,2.34,,Open weights (non-commercial),Japan,,,,,,2025-05-09 11:32,,,,,,,,,,,,"cc-by-nc-4.0, non commercial",,,,,,Operation counting,,,,
YuLan-Chat-2 (13B),Language,Chat,Renmin University of China,,2023-08-02,YuLan-Chat: An Open-Source Bilingual Chatbot,https://github.com/RUC-GSAI/YuLan-Chat,,,"""[Aug. 18, 2023] Our YuLan-Chat-2-13B achieves the 5th position of OpenCompass benchmark!""",13000000000.0,,,,,,,,,,,,Confident,"YuLan-Chat models are chat-based large language models, which are developed by the researchers in GSAI, Renmin University of China (YuLan, which represents Yulan Magnolia, is the campus flower of Renmin University of China). The newest version is developed by continually-pretraining and instruction-tuning LLaMA-2 with high-quality English and Chinese data. The model has the following technical characteristics:
Due to continued pre-training on high-quality Chinese-English bilingual data, the language ability of the model has been improved.
To well support Chinese and longer inputs and outputs, we expand the original vocabulary with Chinese words and extend the maximum length of LLaMA-2. It can support 8k context now.
To well activate the bilingual instruction following capacity, we construct high-quality bilingual instructions, and perform multi-stage instruction-tuning.",,,,China,LLaMA-13B,,"""Due to continued pre-training on high-quality Chinese-English bilingual data, the language ability of the model has been improved.""

Token count not stated",,,2024-09-05 14:08,,,,,,Academia,,,,,,,Academia,,,,,,,,,
OpenFlamingo,Image generation,"Image generation,Text-to-image","University of Washington,Stanford University,Allen Institute for AI,Hebrew University of Jerusalem,Columbia University,Google DeepMind,University of California Santa Barbara (UCSB),Research Center Juelich","Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, Ludwig Schmidt",2023-08-02,OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models,https://arxiv.org/abs/2308.01390,,,,9000000000.0,"""We introduce OpenFlamingo, a family of autoregressive vision-language models ranging from 3B to 9B parameters.""",,9B model is trained using 64 A100-80GBs in 16bf. Length of training not stated. Might be possible to calculate training compute using operations counting method.,"LAION-2B,MMC4 / Multimodal C4",,17400000000.0,"""OpenFlamingo models were trained for 60M interleaved (MMC4) examples1 and 120M LAION-2B examples.""
Table 2 and Figure 4 provide details on the length of each example. MMC4 has median of 2 images and 256 text tokens per sample, while LAION-2B has 1 and 17, respectively.

Suggests a total of around 240M images and 17.4B text tokens. Prediction task is next (text) token prediction; images are only used for conditioning.",,,NVIDIA A100 SXM4 80 GB,,Confident,"We introduce OpenFlamingo, a family of autoregressive vision-language models ranging from 3B to 9B parameters. OpenFlamingo is an ongoing effort to produce an open-source replication of DeepMind's Flamingo models. On seven vision-language datasets, OpenFlamingo models average between 80 - 89% of corresponding Flamingo performance. This technical report describes our models, training data, hyperparameters, and evaluation suite. We share our models and code at this https URL.",,,Open weights (unrestricted),"United States of America,United States of America,United States of America,Israel,United States of America,United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France,United States of America",,,,64.0,,2025-01-17 15:09,,,,,,"Academia,Academia,Research collective,Academia,Academia,Industry,Academia",,,,,Open source,"MIT. code and weights:
https://github.com/mlfoundations/open_flamingo?tab=readme-ov-file","Academia,Academia,Research collective,Academia,Academia,Industry,Academia",,,,50898.64745173198,,,,,
JIANG,Language,Language modeling,K.D. Feddersen (KDF),"Qinhua Duan, Wenchao Gu, Yujia Chen, Wenxin Mao, Zewen Tian, Hui Cao",2023-08-01,JIANG: Chinese Open Foundation Language Model,https://arxiv.org/abs/2308.00624,0.0,,,,They show a chart with different 400M models they trained to refine the design. Main model probably has more but they don't specify.,4.03e+22,"""The training was conducted using 96 A100 80G GPUs, and the entire process took approximately 52 days.""

312 teraflop/s * 96 * 52 * 24 * 3600 * 0.3 = 4e22",,"""The model is trained on a large quantity of textual data including both English and Chinese and use a standard optimizer.""

Mostly from Chinese internet, and The Pile and GitHub.",467000000000.0,"467B tokens (inferred from Table 1).

It's a mix of Chinese and English text, I'll use our standard 1:1 token:words ratio for Chinese.",1200.0,52 days,NVIDIA A100 SXM4 80 GB,,Confident,"With the advancements in large language model technology, it has showcased capabilities that come close to those of human beings across various tasks. This achievement has garnered significant interest from companies and scientific research institutions, leading to substantial investments in the research and development of these models. While numerous large models have emerged during this period, the majority of them have been trained primarily on English data. Although they exhibit decent performance in other languages, such as Chinese, their potential remains limited due to factors like vocabulary design and training corpus. Consequently, their ability to fully express their capabilities in Chinese falls short. To address this issue, we introduce the model named JIANG (Chinese pinyin of ginger) specifically designed for the Chinese language. We have gathered a substantial amount of Chinese corpus to train the model and have also optimized its structure. The extensive experimental results demonstrate the excellent performance of our model.",,,Open weights (unrestricted),China,,,,96.0,,2025-02-14 16:18,,,,6000000.0,"""During the training process, we employed a large batch size of 6 million tokens to enhance the modelâ€™s stability""",Industry,,,,,,apache 2.0,Industry,,,,76349.67141784582,Hardware,,,,
Vicuna-7B-v1.5,Language,"Language modeling/generation,Chat","Large Model Systems Organization,University of California (UC) Berkeley",,2023-08-01,Vicuna Model Card,https://huggingface.co/lmsys/vicuna-7b-v1.5,,,,7000000000.0,,,"6*7*10^9*370*10^6 = 1.554e+19

unsure about amount of epochs ->  lower bound, Likely confidence",Vicuna ShareGPT Dataset,,370000000.0,"""The training data is around 125K conversations collected from ShareGPT.com.""""
370M tokens (Table 14)

https://arxiv.org/pdf/2306.05685""",,,,,Likely,"Vicuna v1.5 is fine-tuned from Llama 2 with supervised instruction fine-tuning. The training data is around 125K conversations collected from ShareGPT.com. See more details in the ""Training Details of Vicuna Models"" section in the appendix of this paper.
https://arxiv.org/abs/2306.05685",,,Open weights (restricted use),"United States of America,United States of America",Llama 2-7B,,,,,2025-05-01 10:42,,,,,,"Academia,Academia",,1.554e+19,,,Open source,"apache 2 for code https://github.com/lm-sys/FastChat

llama 2 for weights","Academia,Academia",,,,,Operation counting,,,,
Vicuna-13B-v1.5,Language,"Language modeling/generation,Chat","Large Model Systems Organization,University of California (UC) Berkeley",,2023-08-01,Vicuna Model Card,https://huggingface.co/lmsys/vicuna-13b-v1.5,,,,13000000000.0,,,"6 FLOP / parameter / token * 13*10^9 parameters * 370*10^6 tokens = 2.886e+19 FLOP

unsure about amount of epochs ->  lower bound, Likely confidence",Vicuna ShareGPT Dataset,,370000000.0,"""The training data is around 125K conversations collected from ShareGPT.com.""""
370M tokens (Table 14)

https://arxiv.org/pdf/2306.05685""",,,,,Likely,"Vicuna v1.5 is fine-tuned from Llama 2 with supervised instruction fine-tuning. The training data is around 125K conversations collected from ShareGPT.com. See more details in the ""Training Details of Vicuna Models"" section in the appendix of this paper.
https://arxiv.org/abs/2306.05685",,,Open weights (restricted use),"United States of America,United States of America",Llama 2-13B,,,,,2025-05-01 10:42,,,,,,"Academia,Academia",,2.886e+19,,,Open source,"apache 2 for code https://github.com/lm-sys/FastChat

llama 2 for weights","Academia,Academia",,,,,Operation counting,,,,
bilingual-gpt-neox-4b,Language,"Language generation,Translation",rinna,"Tianyu Zhao, Toshiaki Wakatsuki, Akio Kaga, Koh Mitsuda, Kei Sawada",2023-07-31,Release of Pre-Trained Models for the Japanese Language,"https://huggingface.co/rinna/bilingual-gpt-neox-4b

https://arxiv.org/abs/2404.01657",,,,3800000000.0,3.8 billion,1.2e+22,3.8 billion params * 524b tokens * 6 = 1.2e22,"Japanese CC-100,Japanese C4,The Pile,RedPajama",,,,,,,,Likely,This repository provides an English-Japanese bilingual GPT-NeoX model of 3.8 billion parameters.,,,Open weights (unrestricted),Japan,,,,,,2025-02-14 16:18,,,,,,Industry,,,,,Unreleased,"MIT for weights. open data, multiple licenses: https://huggingface.co/rinna/bilingual-gpt-neox-4b",Industry,,,,,Operation counting,,,,
RT-2,Robotics,Robotic manipulation,Google DeepMind,"Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, Pete Florence, Chuyuan Fu, Montse Gonzalez Arenas, Keerthana Gopalakrishnan, Kehang Han, Karol Hausman, Alexander Herzog, Jasmine Hsu, Brian Ichter, Alex Irpan, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Lisa Lee, Tsang-Wei Edward Lee, Sergey Levine, Yao Lu, Henryk Michalewski, Igor Mordatch, Karl Pertsch, Kanishka Rao, Krista Reymann, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Pierre Sermanet, Jaspiar Singh, Anikait Singh, Radu Soricut, Huong Tran, Vincent Vanhoucke, Quan Vuong, Ayzaan Wahid, Stefan Welker, Paul Wohlhart, Jialin Wu, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, Brianna Zitkovich",2023-07-28,RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control,https://arxiv.org/abs/2307.15818,691.0,SOTA improvement,"""We compare our method to multiple state-of-the-art baselines that challenge different aspects of our method. All of the baselines use the exact same robotic data... Here, on average, both instantiations of RT-2 perform similarly, resulting in âˆ¼2x improvement over the next two baselines, RT-1 and MOO, and âˆ¼6x better than the other baselines""

Top10 recent paper from Sebastian Sartor 2025-05-14",55000000000.0,"""We train two specific instantiations of RT-2 that leverage pre-trained VLMs: (1) RT-2-PaLI-X is built from 5B and 55B PaLI-X (Chen et al., 2023a), and (2) RT-2-PaLM-E is built from 12B PaLM-E (Driess et al., 2023).""

55B and 12B have similar overall performance",,"""""For RT-2-PaLI-X-55B, we use learning rate 1e-3 and batch size 2048 and co-fine-tune the model for 80K gradient steps""
Sequence length not stated",RT-1,"""The vision-language datasets are based on the dataset mixtures from Chen et al. (2023b) and Driess et al. (2023). The bulk of this data consists of the WebLI dataset, which is around 10B image-text pairs across 109 languages, filtered to the top 10% scoring cross-modal similarity examples to give
1B training examples""
""The robotics dataset is based on the dataset from Brohan et al. (2022).""

Chen et al and Driess et al are the original Pali-X and Palm-E papers.  image-text web data

Brohan et al is the RT-1 paper",,,,,,,Confident,"We study how vision-language models trained on Internet-scale data can be incorporated directly into end-to-end robotic control to boost generalization and enable emergent semantic reasoning. Our goal is to enable a single end-to-end trained model to both learn to map robot observations to actions and enjoy the benefits of large-scale pretraining on language and vision-language data from the web. To this end, we propose to co-fine-tune state-of-the-art vision-language models on both robotic trajectory data and Internet-scale vision-language tasks, such as visual question answering. In contrast to other approaches, we propose a simple, general recipe to achieve this goal: in order to fit both natural language responses and robotic actions into the same format, we express the actions as text tokens and incorporate them directly into the training set of the model in the same way as natural language tokens. We refer to such category of models as vision-language-action models (VLA) and instantiate an example of such a model, which we call RT-2. Our extensive evaluation (6k evaluation trials) shows that our approach leads to performant robotic policies and enables RT-2 to obtain a range of emergent capabilities from Internet-scale training. This includes significantly improved generalization to novel objects, the ability to interpret commands not present in the robot training data (such as placing an object onto a particular number or icon), and the ability to perform rudimentary reasoning in response to user commands (such as picking up the smallest or largest object, or the one closest to another object). We further show that incorporating chain of thought reasoning allows RT-2 to perform multi-stage semantic reasoning, for example figuring out which object to pick up for use as an improvised hammer (a rock), or which type of drink is best suited for someone who is tired (an energy drink).",,,Unreleased,"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France",PaLI-X,,"""For RT-2-PaLI-X-55B, we use learning rate 1e-3 and batch size 2048 and co-fine-tune the model for 80K gradient steps""

",,,2025-06-11 21:39,,,,,,Industry,checked,,,,Unreleased,"no model weights or training code releases are mentioned
https://robotics-transformer2.github.io/",Industry,,,,,,,,,
Zi Yue,Language,Chat,NetEase,,2023-07-28,NetEase Youdao launches first large model in education,https://www.chinadaily.com.cn/a/202307/28/WS64c3226ea31035260b8190a4.html,,,"""Zhou Feng, CEO of NetEase Youdao, told China Daily that, in terms of translation ability, Zi Yue has ""already surpassed ChatGPT in an all-around way"" and is better than most major smart translators in the market.

""The biggest opportunity brought by the large model is individualized teaching and studying, which offers students personalized analysis and tutoring while guiding them in exploring answers on their own, just like human teachers"", Zhou said.""",,,,,,,,,,,,,Unknown,"As Open AI's ChatGPT takes the tech world by storm, Chinese educational technology firm NetEase Youdao launched its large model, along with up to six applications, on Thursday, which marked the birth of one of China's first large models in the education sector.",,,,China,,,,,,2024-09-05 14:08,,,,,,Industry,,,,,,,Industry,,,,,,,,,
Zi Yue 2.0,Language,Chat,NetEase,,2023-07-28,"NetEase Youdao Upgrades 'Ziyue' Foundation Model to 2.0 Ver, Encompassing More Subjects & Teaching Areas",https://m.aastocks.com/en/usq/news/comment.aspx?source=AAFN&id=NOW.1317044&catg=1,,,,,,,,,,,,,,,,Unknown,"Chinese media reported that Youdao (DAO.US)     has released the 2.0 upgrade for its Ziyue educational foundation model. Youdao also launched Youdao Speed Reading (literal translation of ""æœ‰é“é€Ÿè®€""), new-generation virtual personality verbal language trainer, AI home tutors, and Youdao-branded new-generation intelligent hardware applications.

It is reported that Ziyue 2.0 has been upgraded in the knowledge question and answering ability within the education scene, with it expanding to more subjects and teaching areas. The amount of educational data has been largely expanded, the model's context window has been upgraded to 16,000 tokens, and new Agent and retrieval enhancement capabilities have been added.",,,,China,,,,,,2024-09-05 14:08,,,,,,Industry,,,,,,,Industry,,,,,,,,,
BELLE-Llama2-13B-chat-0.4M,Language,Language modeling/generation,KE Holdings Inc. (â€œBeikeâ€),,2023-07-27,,https://huggingface.co/BELLE-2/BELLE-Llama2-13B-chat-0.4M,,,,13000000000.0,,,,,,,,,,,,Confident,"This model is obtained by fine-tuning the complete parameters using 0.4M Chinese instruction data on the original Llama2-13B-chat. We firmly believe that the original Llama2-chat exhibits commendable performance post Supervised Fine-Tuning (SFT) and Reinforcement Learning with Human Feedback (RLHF). Our pursuit continues to be the further enhancement of this model using Chinese instructional data for fine-tuning, with an aspiration to facilitate stable and high-quality Chinese language outputs.",,,Open weights (non-commercial),China,Llama 2-13B,,,,,2025-05-01 10:42,,,,,,Industry,,,,,,"License: LLaMA 2
Please refer License of Meta LLaMA Currently only for learning and communication. Please strictly observe the use restrictions of LLaMA. The LLaMA model does not allow the release of the complete model weights after adjustment, but can publish the diff of the original model. Therefore, we use XOR in the file room to ensure that talents with the authorization of the original model of LLaMA can transform the model released by this project into a usable format. ",Industry,,,,,,,,,
AudioLM,Audio,Audio generation,Google Research,"ZalÃ¡n Borsos, RaphaÃ«l Marinier, Damien Vincent, Eugene Kharitonov, Olivier Pietquin, Matt Sharifi, Dominik Roblek, Olivier Teboul, David Grangier, Marco Tagliasacchi, Neil Zeghidour",2023-07-26,AudioLM: a Language Modeling Approach to Audio Generation,https://arxiv.org/abs/2209.03143,446.0,SOTA improvement,"Compared to other systems without text supervision, AudioLM achieves the highest
sWUGGY scores across both splits. Similarly, it also attains the
highest score in the sBLIMP metric, improving by 8% relative
over the previous state-of-the-art (CPC-BERT [59]).",1500000000.0,"""We use identical decoder-only Transformers in
all stages, with 12 layers, 16 attention heads, embedding
dimension of 1024, feed-forward layer dimension of 4096
and dropout of 0.1, together with T5-style relative positional
embeddings [38], resulting in a model parameter size of
0.3B per stage.""

Three stages (figure 2), and 300M per stage. Plus 600M parameters for w2v-BERT XL, so 1.5B total",3.9e+18,"""We train each stage on 16 TPUv4s with batch size of 256 for 1M steps.""

That's for the 900M-param transformers

If there's 256 passes in each batch, then using 6ND that's 900m * 256m * 6 = 1.3e18. sanity check: 16 tpu4s is 4.4e15 FLOP/s. 1.3e18 FLOP / 4.4e15 FLOP/s is 295 seconds. adjusting for utilization it would be ~1000 seconds or 15 minutes? probably too short, so 1.3e18 seems too low.

upd there are 3 stages -> 1.3e18*3 = 3.9e+18 (Speculative due to reasoning above)",LibriLight,,820800000.0,"60k hours of English speech
13680*60000 = 820800000 words

https://docs.google.com/document/d/1G3vvQkn4x_W71MKg0GmHVtzfd9m0y3_Ofcoew0v902Q/edit#heading=h.sxcem9l5k3ce",,,Google TPU v4,Self-supervised learning,Speculative,"We introduce AudioLM, a framework for high-quality audio generation with long-term consistency. AudioLM maps the input audio to a sequence of discrete tokens and casts audio generation as a language modeling task in this representation space. We show how existing audio tokenizers provide different trade-offs between reconstruction quality and long-term structure, and we propose a hybrid tokenization scheme to achieve both objectives. Namely, we leverage the discretized activations of a masked language model pre-trained on audio to capture long-term structure and the discrete codes produced by a neural audio codec to achieve high-quality synthesis. By training on large corpora of raw audio waveforms, AudioLM learns to generate natural and coherent continuations given short prompts. When trained on speech, and without any transcript or annotation, AudioLM generates syntactically and semantically plausible speech continuations while also maintaining speaker identity and prosody for unseen speakers. Furthermore, we demonstrate how our approach extends beyond speech by generating coherent piano music continuations, despite being trained without any symbolic representation of music.",,,Unreleased,"Multinational,United States of America,Canada,Switzerland",,,,,,2025-05-30 11:42,,,,,,Industry,checked,,,,Unreleased,,Industry,,,,,Operation counting,,,,
WizardLM 13B v1.2,Language,"Language modeling/generation,Question answering","Microsoft,Peking University",,2023-07-25,,https://huggingface.co/WizardLMTeam/WizardLM-13B-V1.2,,,,13000000000.0,13B,,,Evol Instruct,,130000000.0,"~70K rows (instruction + output)
~7K chars each 
(https://huggingface.co/datasets/WizardLMTeam/WizardLM_evol_instruct_70k)

assuming avg 5 characters per word, 4/3 tokens per word:

(70000 * 7000 * 4/3) / 5 ~130M tokens 
number of epochs is unknown",,,,,Confident,,,,Open weights (restricted use),"United States of America,Multinational,India,Belgium,China",Llama 2-13B,,,,,2025-05-12 16:00,,,,,,"Industry,Academia",,,,,,"llama2 license

https://huggingface.co/WizardLMTeam/WizardLM-13B-V1.2","Industry,Academia",,,,,,WizardLMTeam,,,
RFdiffusion,Biology,"Protein generation,Protein folding prediction","University of Washington,Columbia University,Ecole Normale SupÃ¨rieure,University of Cambridge,Massachusetts Institute of Technology (MIT),Seoul National University","Joseph L. Watson, David Juergens, Nathaniel R. Bennett, Brian L. Trippe, Jason Yim, Helen E. Eisenach, Woody Ahern, Andrew J. Borst, Robert J. Ragotte, Lukas F. Milles, Basile I. M. Wicky, Nikita Hanikel, Samuel J. Pellock, Alexis Courbet, William Sheffler, Jue Wang, Preetham Venkatesh, Isaac Sappington, Susana VÃ¡zquez Torres, Anna Lauko, Valentin De Bortoli, Emile Mathieu, Sergey Ovchinnikov, Regina Barzilay, Tommi S. Jaakkola, Frank DiMaio, Minkyung Baek, David Baker",2023-07-23,De novo design of protein structure and function with RFdiffusion,https://www.nature.com/articles/s41586-023-06415-8,,,,,,,,"PDB (Protein Data Bank), AlphaFold database (AFDB)","""We generate training inputs by noising structures sampled from the Protein Data Bank (PDB) for up to 200 steps""

""RF was trained on a mixture of datasets including 1) monomer/homo-oligomer structures in the PDB, 2) hetero-oligomer structures in the PDB (date cutoff August 2nd, 2021), 3) AlphaFold2 structural models having plDDT > 0.758, and 4) negative protein-protein interaction examples generated by random pairing. The training examples were sampled from each database with a ratio of 2:1:4:1.""",2293760000.0,"Table 3 (Supplementary materials):

Initial Training:

crop size 256
25600 examples per epoch
200 epochs

Fine tuning:

crop size 384
25600 examples per epoch
100 epochs

256*25600*200+384*25600*100 = 2293760000 tokens ~ 2.3B tokens",672.0,"""RoseTTAFold was trained for 4 weeks on 64 V100 GPUs on
Microsoft Azure.""

4*7*24=672 hours",NVIDIA V100,,Confident,"There has been considerable recent progress in designing new proteins using deep-learning methods1,2,3,4,5,6,7,8,9. Despite this progress, a general deep-learning framework for protein design that enables solution of a wide range of design challenges, including de novo binder design and design of higher-order symmetric architectures, has yet to be described. Diffusion models10,11 have had considerable success in image and language generative modelling but limited success when applied to protein modelling, probably due to the complexity of protein backbone geometry and sequenceâ€“structure relationships. Here we show that by fine-tuning the RoseTTAFold structure prediction network on protein structure denoising tasks, we obtain a generative model of protein backbones that achieves outstanding performance on unconditional and topology-constrained protein monomer design, protein binder design, symmetric oligomer design, enzyme active site scaffolding and symmetric motif scaffolding for therapeutic and metal-binding protein design. We demonstrate the power and generality of the method, called RoseTTAFold diffusion (RFdiffusion), by experimentally characterizing the structures and functions of hundreds of designed symmetric assemblies, metal-binding proteins and protein binders. The accuracy of RFdiffusion is confirmed by the cryogenic electron microscopy structure of a designed binder in complex with influenza haemagglutinin that is nearly identical to the design model. In a manner analogous to networks that produce images from user-specified inputs, RFdiffusion enables the design of diverse functional proteins from simple molecular specifications.",,,Open weights (unrestricted),"United States of America,United States of America,France,United Kingdom of Great Britain and Northern Ireland,United States of America,Korea (Republic of)",RoseTTAFold All-Atom (RFAA),5,125000000000000*64*672*3600*0.3=5.80608e+21,64.0,,2025-05-01 10:42,,,,,,"Academia,Academia,Academia,Academia,Academia,Academia",,,,,Unreleased,"Code for running RFdiffusion has been released on GitHub, free for
academic, personal and commercial use at https://github.com/Rosetta-
Commons/RFdiffusion. It is also available as a Google Colab notebook,
accessible through GitHub.","Academia,Academia,Academia,Academia,Academia,Academia",,,,38182.48764202212,Hardware,,,,
YAYI-13B-Llama2,Language,Language modeling/generation,Yayi (Wenge),,2023-07-22,wenge-research,https://github.com/wenge-research/YAYI,,,,13000000000.0,,,"base model compute: 1.6e23 FLOP
fine-tune compute: 3.12e+18 FLOP",Yayi domain,"""This training data covers five key domains: media publicity, public opinion analysis, public safety, financial risk control, and urban governance, encompassing over a hundred natural language instruction tasks.""",20000000.0,"""For this open-source release, we have made available a training dataset containing 50,000 samples, which can be downloaded from our Huggingface data repository.""

50000 samples * 400 tokens per sample (assumption based on hf repo) = 20000000 tokens",,,,,Likely,"The Yayi model is fine-tuned on millions of high-quality domain data manually constructed. The training data covers five major fields, including media publicity, public opinion analysis, public safety, financial risk control, and urban governance, and hundreds of natural language instruction tasks. In the iterative process from pre-training initialization weights to domain models, we gradually enhanced the basic Chinese capabilities and domain analysis capabilities of the Yayi model, and added multi-round dialogue and some plug-in capabilities. ",2.0,,Open weights (non-commercial),China,Llama 2-13B,3120000000000000000,6 FLOP / parameter / token * 13 * 10^9 parameters * 20000000 tokens * 2 epochs = 3.12e+18 FLOP,,,2025-05-01 10:42,,,,,,Industry,,,,,,"""The code in this project is open-source under the Apache-2.0 license, the data follows the CC BY-NC 4.0 license, and the usage of YaYi series model weights must adhere to the Model License.""

https://huggingface.co/wenge-research/yayi-13b-llama2",Industry,,,,,Operation counting,wenge-research,,,
YAYI-7B-Llama2,Language,Language modeling/generation,Yayi (Wenge),,2023-07-22,wenge-research,https://github.com/wenge-research/YAYI,,,,7000000000.0,,,"base model compute: 8.4e+22 FLOP
fine-tune compute: 1.68e+18 FLOP",Yayi domain,"""This training data covers five key domains: media publicity, public opinion analysis, public safety, financial risk control, and urban governance, encompassing over a hundred natural language instruction tasks.""",20000000.0,"""For this open-source release, we have made available a training dataset containing 50,000 samples, which can be downloaded from our Huggingface data repository.""

50000 samples * 400 tokens per sample (assumption based on hf repo) = 20000000 tokens",,,,,Likely,"The Yayi model is fine-tuned on millions of high-quality domain data manually constructed. The training data covers five major fields, including media publicity, public opinion analysis, public safety, financial risk control, and urban governance, and hundreds of natural language instruction tasks. In the iterative process from pre-training initialization weights to domain models, we gradually enhanced the basic Chinese capabilities and domain analysis capabilities of the Yayi model, and added multi-round dialogue and some plug-in capabilities. ",2.0,,Open weights (non-commercial),China,Llama 2-7B,1680000000000000000,6 FLOP / parameter / token * 7 * 10^9 parameters * 20000000 tokens * 2 epochs = 1.68e+18 FLOP,,,2025-05-01 10:42,,,,,,Industry,,,,,,"""The code in this project is open-source under the Apache-2.0 license, the data follows the CC BY-NC 4.0 license, and the usage of YaYi series model weights must adhere to the Model License.""

https://huggingface.co/wenge-research/yayi-7b-llama2",Industry,,,,,Operation counting,wenge-research,,,
Stable Beluga 1,Language,Language generation,Stability AI,,2023-07-21,"Meet Stable Beluga 1 and Stable Beluga 2, Our Large and Mighty Instruction Fine-Tuned Language Models",https://huggingface.co/stabilityai/StableBeluga1-Delta,,,"#4 on Open LLM leaderboard (in July, it's much lower now):

""These Stable Beluga results were evaluated by Stability AI researchers and independently reproduced by Hugging Face on July 21st, 2023, and published in their leaderboard.

As of July 27th, 2023, Stable Beluga 2 is the very best model (#1) on the leaderboard, and Stable Beluga 1 is #4""",65200000000.0,65.2B,,,,,,,,,,,Confident,"Stability AI and its CarperAI lab proudly announce Stable Beluga 1 and its successor Stable Beluga 2 (formerly codenamed FreeWilly), two powerful new, open access, Large Language Models (LLMs). Both models demonstrate exceptional reasoning ability across varied benchmarks. Stable Beluga 1 leverages the original LLaMA 65B foundation model and was carefully fine-tuned with a new synthetically-generated dataset using Supervised Fine-Tune (SFT) in standard Alpaca format. Similarly, Stable Beluga 2 leverages the LLaMA 2 70B foundation model to achieve industry-leading performance.",,,Open weights (non-commercial),United Kingdom of Great Britain and Northern Ireland,LLaMA-65B,,"Fine-tuned on a 600k dataset. Not sure how many epochs. https://stability.ai/news/stable-beluga-large-instruction-fine-tuned-models

""The training for the Stable Beluga models was directly inspired by the methodology pioneered by Microsoft in its paper: ""Orca: Progressive Learning from Complex Explanation Traces of GPT-4.â€ While our data generation process is similar, we differ in our data sources. Our variant of the dataset, containing 600,000 data points (roughly 10% of the dataset size the original Orca paper used)...""",,,2024-09-05 14:08,,,,,,Industry,,,,,Unreleased,non-comm license https://huggingface.co/stabilityai/StableBeluga1-Delta,Industry,,,,,,,,,
Stable Beluga 2,Language,Language generation,Stability AI,,2023-07-20,"Meet Stable Beluga 1 and Stable Beluga 2, Our Large and Mighty Instruction Fine-Tuned Language Models","https://stability.ai/news/stable-beluga-large-instruction-fine-tuned-models, https://huggingface.co/stabilityai/StableBeluga2",,,"#1 on Open LLM leaderboard (in July, it's much lower now):

""These Stable Beluga results were evaluated by Stability AI researchers and independently reproduced by Hugging Face on July 21st, 2023, and published in their leaderboard.

As of July 27th, 2023, Stable Beluga 2 is the very best model (#1) on the leaderboard, and Stable Beluga 1 is #4""",70000000000.0,fine-tuned from Llama 2-70B,,,Stable Beluga Instruction Dataset,"""The training for the Stable Beluga models was directly inspired by the methodology pioneered by Microsoft in its paper: ""Orca: Progressive Learning from Complex Explanation Traces of GPT-4.â€ While our data generation process is similar, we differ in our data sources.

Our variant of the dataset, containing 600,000 data points (roughly 10% of the dataset size the original Orca paper used), was created synthetically using high-quality instructions from the following datasets created by Enrico Shippole:

COT Submix Original

NIV2 Submix Original

FLAN 2021 Submix Original

T0 Submix Original""",,,,,,,Likely,"Stability AI and its CarperAI lab proudly announce Stable Beluga 1 and its successor Stable Beluga 2 (formerly codenamed FreeWilly), two powerful new, open access, Large Language Models (LLMs). Both models demonstrate exceptional reasoning ability across varied benchmarks. Stable Beluga 1 leverages the original LLaMA 65B foundation model and was carefully fine-tuned with a new synthetically-generated dataset using Supervised Fine-Tune (SFT) in standard Alpaca format. Similarly, Stable Beluga 2 leverages the LLaMA 2 70B foundation model to achieve industry-leading performance.",,,Open weights (non-commercial),United Kingdom of Great Britain and Northern Ireland,Llama 2-70B,,"Fine-tuned on a 600k dataset. Not sure how many epochs. https://stability.ai/news/stable-beluga-large-instruction-fine-tuned-models

""The training for the Stable Beluga models was directly inspired by the methodology pioneered by Microsoft in its paper: ""Orca: Progressive Learning from Complex Explanation Traces of GPT-4.â€ While our data generation process is similar, we differ in our data sources. Our variant of the dataset, containing 600,000 data points (roughly 10% of the dataset size the original Orca paper used)...""",,,2025-06-10 16:30,,,,,,Industry,,,,,Unreleased,non-comm: https://huggingface.co/stabilityai/StableBeluga2/blob/main/LICENSE.txt,Industry,,,,,,,,,
EXAONE 2.0,"Multimodal,Image generation,Language,Biology,Vision","Language modeling,Image generation,Visual question answering",LG AI Research,,2023-07-19,LG AI Research Develops Foundation Model Using Amazon SageMaker,https://aws.amazon.com/solutions/case-studies/lg-ai-research-case-study/,,,,300000000000.0,300 billion,,,Unspecified unreleased,"From KoreaTimes (https://www.koreatimes.co.kr/www/tech/2023/12/129_355258.html)

""EXAONE 2.0 studied about 45 million specialized documents and 350 million images, including patents and papers secured through partnerships.

Considering that much of the existing expertise data is in English, EXAONE 2.0 is developed as a bilingual model that can understand and answer both in Korean and English at the same time. It also learns over four times more data than the previous model.""",,,,,,,Speculative,,,,Unreleased,Korea (Republic of),,,,,,2025-01-27 10:45,,,,,,Industry,,1.08045e+24,,,,,Industry,,,,,,,,,
Llama 2-70B,Language,Language modeling,Meta AI,"Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, Thomas Scialom
",2023-07-18,Llama 2: Open Foundation and Fine-Tuned Chat Models,"https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/
https://arxiv.org/abs/2307.09288",8056.0,"Historical significance,Significant use,Highly cited,Training cost",Model has been open-sourced and frequently downloaded. The paper claims that Llama 2 is the current best open-source chat model as of its release date.,70000000000.0,"Llama has been released in 7B, 13B, 34B, and 70B variants.",8.1e+23,"""Pretraining utilized a cumulative 3.3M GPU hours of computation on hardware of type A100-80GB"" of which 1720320 GPU hours were used to train the 70B model.

311.84 BF16 TFLOP/s * 1720320 hours * 0.40 utilization = 7.725e+23 FLOP.

Alternatively: the model was trained for 1 epoch on 2 trillion tokens and has 70B parameters. C = 6ND = 6*70B*2T = 8.4e+23 FLOP.",Llama 2 dataset,"2 trillion tokens of publicly available text, with no text from Meta's products.
""Our training corpus includes a new mix of data from publicly available sources, which does not include data from Metaâ€™s products or services. We made an effort to remove data from certain sites known to contain a high volume of personal information about private individuals. We trained on 2 trillion tokens of data as this provides a good performanceâ€“cost trade-off, up-sampling the most factual sources in an effort to increase knowledge and dampen hallucinations.""
Knowledge cutoff date is September 2022, according to https://huggingface.co/meta-llama/Llama-2-70b. ",1500000000000.0,2 trillion tokens ~= 1.5 trillion words,1728.0,"Model was trained from January 2023 to July 2023, which is six months. However, the training run duration did not take up this whole period. According to a Meta employee interviewed by Epoch, Llama 2 34B and 70B were trained on different clusters, with overlapping training periods. Based on an estimate of 1000 GPUs, it would have taken 72 days.",NVIDIA A100 SXM4 80 GB,Supervised,Confident,"In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closedsource models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.",1.0,,Open weights (restricted use),United States of America,,,,1000.0,0.4192,2025-05-29 01:21,,Metaâ€™s Research Super Cluster,,4000000.0,,Industry,,,,1720320.0,Unreleased,"Llama 2 license. can't use outputs to train models.

https://github.com/meta-llama/llama/blob/main/LICENSE",Industry,$1099604.99,"8.1e23 FLOPs based on 6NC method.
Table 2 reports 1720320 A100 hours, which at 100% utilization would give 1720320 * 3600 * 3.12e14 = 1.932e24 FLOP
Utilization: 8.1e23 / 1.932e24 = 0.4192",,795557.0703894598,"Hardware,Operation counting",meta-llama,,,9
Llama 2-7B,Language,Language modeling,Meta AI,"Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, Thomas Scialom
",2023-07-18,Llama 2: Open Foundation and Fine-Tuned Chat Models,https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/,8056.0,"Historical significance,Significant use,Highly cited",Model has been open-sourced and frequently downloaded. The paper claims that Llama 2 is the current best open-source chat model as of its release date.,7000000000.0,"Llama has been released in 7B, 13B, and 70B variants.",8.4e+22,"Trained on 2 trillion tokens per Table 1. 

C = 6ND = 6 FLOP / token / parameter * 7B parameters * 2T tokens = 8.4e+22 FLOP.

Also, 7B model was trained on 184320 GPU-hours

312 trillion * 184320 GPU-hours * 3600 sec/hour * 0.3 [utilization] = 6.21e22 FLOP",Llama 2 dataset,"2 trillion tokens of publicly available text, with no text from Meta's products.
""Our training corpus includes a new mix of data from publicly available sources, which does not include data from Metaâ€™s products or services. We made an effort to remove data from certain sites known to contain a high volume of personal information about private individuals. We trained on 2 trillion tokens of data as this
provides a good performanceâ€“cost trade-off, up-sampling the most factual sources in an effort to increase knowledge and dampen hallucinations.""
Knowledge cutoff date is September 2022, according to https://huggingface.co/meta-llama/Llama-2-70b. ",1500000000000.0,2 trillion tokens ~= 1.5T words,,,NVIDIA A100 SXM4 80 GB,Supervised,Confident,"In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closedsource models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.",1.0,,Open weights (restricted use),United States of America,,,,,,2025-06-03 14:38,,Metaâ€™s Research Super Cluster,,4000000.0,,Industry,,,,184320.0,Unreleased,"Llama 2 license. can't use outputs to train models.

https://github.com/meta-llama/llama/blob/main/LICENSE",Industry,$114259.39,,,,"Hardware,Operation counting",,,,
GPT3-2.7B (FlashAttention-2),Language,Language modeling/generation,"Stanford University,Princeton University",Tri Dao,2023-07-18,FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning,https://arxiv.org/pdf/2307.08691,1178.0,"Highly cited,Historical significance",,2700000000.0,,,"8x A100 SXMs used, but no indication of dataset size, how long training took, or compute costs",,,,,,,NVIDIA A100 SXM4 80 GB,,Confident,"Scaling Transformers to longer sequence lengths has been a major problem in the last several years, promising to improve performance in language modeling and high-resolution image understanding, as well as to unlock new applications in code, audio, and video generation. The attention layer is the main bottleneck in scaling to longer sequences, as its runtime and memory increase quadratically in the sequence length. FlashAttention exploits the asymmetric GPU memory hierarchy to bring significant memory saving (linear instead of quadratic) and runtime speedup (2-4\times compared to optimized baselines), with no approximation. However, FlashAttention is still not nearly as fast as optimized matrix-multiply (GEMM) operations, reaching only 25-40\% of the theoretical maximum FLOPs/s. We observe that the inefficiency is due to suboptimal work partitioning between different thread blocks and warps on the GPU, causing either low-occupancy or unnecessary shared memory reads/writes. We propose FlashAttention-2, with better work partitioning to address these issues. In particular, we (1) tweak the algorithm to reduce the number of non-matmul FLOPs (2) parallelize the attention computation, even for a single head, across different thread blocks to increase occupancy, and (3) within each thread block, distribute the work between warps to reduce communication through shared memory. These yield around 2\times speedup compared to FlashAttention, reaching 50-73\% of the theoretical maximum FLOPs/s on A100 and getting close to the efficiency of GEMM operations. We empirically validate that when used end-to-end to train GPT-style models, FlashAttention-2 reaches training speed of up to 225 TFLOPs/s per A100 GPU (72\% model FLOPs utilization).",,,Unreleased,"United States of America,United States of America",,,,8.0,72.0,2025-06-19 12:44,,,,,,"Academia,Academia",,,,,,,"Academia,Academia",,,,6364.456563115679,,,,,
RetNet,Language,Language modeling,"Microsoft Research,Tsinghua University","Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, Furu Wei",2023-07-17,Retentive Network: A Successor to Transformer for Large Language Models,https://arxiv.org/abs/2307.08621,203.0,,,6700000000.0,"Table 2

They later mention testing the memory and throughput of a 13B-parameter model, but it doesn't sound like they trained it long enough to test its perplexity.",4.02e+21,"C = 6ND = 6 * 6.7 billion * 100 billion
""We train the models with 512 AMD MI200 GPUs.""

Another rough method: Table 4 shows throughput is 8642.2 words per second during training on an 8xA100 setup, with batch size of 8192. This suggests 2543.1 hours to go through the full 100B tokens ~=  75B words.
(8 * 3.12e14) * (2543.1 * 3600) * (0.3) = 6.9e21 FLOPs

(low confidence in this as actual training setup used different hardware and much larger batch size, but same OOM is encouraging)",,"""The training corpus is a curated compilation of The Pile, C4, and The Stack""",100000000000.0,,,,,,Confident,"In this work, we propose Retentive Network (RetNet) as a foundation architecture for large language models, simultaneously achieving training parallelism, low-cost inference, and good performance. We theoretically derive the connection between recurrence and attention. Then we propose the retention mechanism for sequence modeling, which supports three computation paradigms, i.e., parallel, recurrent, and chunkwise recurrent. Specifically, the parallel representation allows for training parallelism. The recurrent representation enables low-cost O(1) inference, which improves decoding throughput, latency, and GPU memory without sacrificing performance. The chunkwise recurrent representation facilitates efficient long-sequence modeling with linear complexity, where each chunk is encoded parallelly while recurrently summarizing the chunks. Experimental results on language modeling show that RetNet achieves favorable scaling results, parallel training, low-cost deployment, and efficient inference. The intriguing properties make RetNet a strong successor to Transformer for large language models.",,,Unreleased,"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,China",,,,,,2025-02-14 16:19,,,,4000000.0,4M,"Industry,Academia",,,,,Unreleased,"MIT for code

looks like model code not train code: https://github.com/microsoft/unilm/tree/master/retnet ","Industry,Academia",,,,,Operation counting,,,,
BaiLing,Multimodal,"Language modeling/generation,Question answering,Visual question answering",Ant Group,,2023-07-15,,https://www.antgroup.com/en/technology/new-tech-Technology-Antfocuses-Tabcomnent-detail/20241028001,,,,,,,,Unspecified unreleased,,,,,,,,Unknown,"Ant Groupâ€™s BaiLing foundation model has made significant advancements in computing power, security, and knowledge processing. It has established a computing cluster with tens of thousands of heterogeneous accelerator cards, integrated security capabilities ranging from detection to defense, and the ability to process trillions of tokens.",,,Unreleased,China,,,,,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
CryoChains,Biology,Cryo-EM image reconstruction,"University of California Santa Barbara (UCSB),Stanford University","Bongjin Koo, Julien Martel, Ariana Peck, Axel Levy, FrÃ©dÃ©ric Poitevin, Nina Miolane",2023-07-15,CryoChains: Heterogeneous Reconstruction of Molecular Assembly of Semi-flexible Chains from Cryo-EM Images,https://arxiv.org/abs/2306.07274,,,,,,,,,,50000.0,"Then, 50, 000 training and 5, 000 test images are
generated with the noise âˆ’20 dB",,,,,Confident,"Cryogenic electron microscopy (cryo-EM) has transformed structural biology by allowing to reconstruct 3D biomolecular structures up to near-atomic resolution. However, the 3D reconstruction process remains challenging, as the 3D structures may exhibit substantial shape variations, while the 2D image acquisition suffers from a low signal-to-noise ratio, requiring to acquire very large datasets that are time-consuming to process. Current reconstruction methods are precise but computationally expensive, or faster but lack a physically-plausible model of large molecular shape variations. To fill this gap, we propose CryoChains that encodes large deformations of biomolecules via rigid body transformation of their chains, while representing their finer shape variations with the normal mode analysis framework of biophysics. Our synthetic data experiments on the human GABA\textsubscript{B} and heat shock protein show that CryoChains gives a biophysically-grounded quantification of the heterogeneous conformations of biomolecules, while reconstructing their 3D molecular structures at an improved resolution compared to the current fastest, interpretable deep learning method.",,,,"United States of America,United States of America",,,,,,2025-05-01 10:42,,,,,,"Academia,Academia",,,,,,,"Academia,Academia",,,,,,,,,
ChatRhino,Language,Chat,JD.com,,2023-07-13,JD.com Introduces ChatRhino: Empowering Industry Innovations with an Advanced Large Language Model,https://jdcorporateblog.com/jd-com-introduces-chatrhino-empowering-industry-innovations-with-an-advanced-large-language-model/,,,,100000000000.0,"""ChatRhino sets a new benchmark as a 100-billion-parameter model"", could be substantially rounded",,,,"Mix of general and supply chain data: ""By combining 70% generalized data with 30% native intelligent supply chain data, JDâ€™s latest AI model offers targeted solutions for real industry challenges across sectors such as retail, logistics, finance, health, and city""",,,,,,,Confident,"JD.com today unveiled its ChatRhino (Yanxi in Chinese) large language model (LLM) on its 2023 JDDiscovery tech summit, tailored to serve various industries. By combining 70% generalized data with 30% native intelligent supply chain data, JDâ€™s latest AI model offers targeted solutions for real industry challenges across sectors such as retail, logistics, finance, health, and city. Building upon the success of the billion-parameter model K-PLUG launched in 2021 and the 10-billion-parameter model Vega introduced in 2022, JDâ€™s ChatRhino sets a new benchmark as a 100-billion-parameter model.",,,,China,,,,,,2024-09-05 14:08,,,,,,Industry,,,,,,,Industry,,,,,,,,,
Uni-RNA-L8,Biology,Protein or nucleotide language model (pLM/nLM),DP Technology,"Xi Wang, Ruichu Gu, Zhiyuan Chen, Yongge Li, Xiaohong Ji, Guolin Ke, Han Wen",2023-07-12,UNI-RNA: UNIVERSAL PRE-TRAINED MODELS REVOLUTIONIZE RNA RESEARCH,https://www.biorxiv.org/content/10.1101/2023.07.11.548588v1.abstract,20.0,,,25000000.0,Table 8: Model architecture parameters of different Uni-RNA models,,"""We trained our proposed network on 128 A100""",,,50000000000.0,"Table 8: Model architecture parameters of different Uni-RNA models

Sequences are capped at 4096 length, but no average sequence length is given. Estimating at 500 tokens per sequence. 

100000000*500=50000000000",,,NVIDIA A100,,Likely,"RNA molecules play a crucial role as intermediaries in diverse biological processes. Attaining a profound understanding of their function can substantially enhance our comprehension of lifeâ€™s activities and facilitate drug development for numerous diseases. The advent of high-throughput sequencing technologies makes vast amounts of RNA sequence data accessible, which contains invaluable information and knowledge. However, deriving insights for further application from such an immense volume of data poses a significant challenge. Fortunately, recent advancements in pre-trained models have surfaced as a revolutionary solution for addressing such challenges owing to their exceptional ability to automatically mine and extract hidden knowledge from massive datasets. Inspired by the past successes, we developed a novel context-aware deep learning model named Uni-RNA that performs pre-training on the largest dataset of RNA sequences at the unprecedented scale to date. During this process, our model autonomously unraveled the obscured evolutionary and structural information embedded within the RNA sequences. As a result, through fine-tuning, our model achieved the state-of-the-art (SOTA) performances in a spectrum of downstream tasks, including both structural and functional predictions. Overall, Uni-RNA established a new research paradigm empowered by the large pre-trained model in the field of RNA, enabling the community to unlock the power of AI at a whole new level to significantly expedite the pace of research and foster groundbreaking discoveries.",,,Unreleased,China,,,,128.0,,2025-05-01 10:42,,,,,,Industry,,,,,,,Industry,,,,101844.9122337359,,,,,
Med-PaLM,"Medicine,Language",Question answering,"Google Research,National Library of Medicine,DeepMind","Karan Singhal, Shekoofeh Azizi, Tao Tu, S. Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, Perry Payne, Martin Seneviratne, Paul Gamble, Chris Kelly, Abubakr Babiker, Nathanael SchÃ¤rli, Aakanksha Chowdhery, Philip Mansfield, Dina Demner-Fushman, Blaise AgÃ¼era y Arcas, Dale Webster, Greg S. Corrado, Yossi Matias, Katherine Chou, Juraj Gottweis, Nenad Tomasev, Yun Liu, Alvin Rajkomar, Joelle Barral, Christopher Semturs, Alan Karthikesalingam & Vivek Natarajan",2023-07-12,Large language models encode clinical knowledge,https://www.nature.com/articles/s41586-023-06291-2,,,,540000000000.0,"""We performed instruction prompt tuning on Flan-PaLM 540B with a soft prompt length of 100 to produce Med-PaLM. We froze the rest of the model, and used an embedding dimension of 18432 as in PaLM [1], which resulted in 1.84M trainable parameters.""
(from supplementary materials)",,,MultiMedQA,"Primary Dataset: Med-PaLM was trained using MultiMedQA, a composite benchmark comprising seven datasets, which span medical examinations, research, and consumer medical questions. It includes:
MedQA
MedMCQA
PubMedQA
MMLU (clinical topics)
HealthSearchQA
LiveQA
MedicationQA",,"from supplementary materials ""We used a batch size of 32 across all runs and ran training for 200 steps.""
they also mention average question length of 25 words, but I am not sure if that applies to all datapoints

MedMCQA (https://proceedings.mlr.press/v174/pal22a/pal22a.pdf, Table 2) has on average 12.77+ 2.69+67.52 = 82.98 tokens per datapoint
",,,,,Confident,"Large language models (LLMs) have demonstrated impressive capabilities, but the bar for clinical applications is high. Attempts to assess the clinical knowledge of models typically rely on automated evaluations based on limited benchmarks. Here, to address these limitations, we present MultiMedQA, a benchmark combining six existing medical question answering datasets spanning professional medicine, research and consumer queries and a new dataset of medical questions searched online, HealthSearchQA. We propose a human evaluation framework for model answers along multiple axes including factuality, comprehension, reasoning, possible harm and bias. In addition, we evaluate Pathways Language Model1 (PaLM, a 540-billion parameter LLM) and its instruction-tuned variant, Flan-PaLM2 on MultiMedQA. Using a combination of prompting strategies, Flan-PaLM achieves state-of-the-art accuracy on every MultiMedQA multiple-choice dataset (MedQA3, MedMCQA4, PubMedQA5 and Measuring Massive Multitask Language Understanding (MMLU) clinical topics6), including 67.6% accuracy on MedQA (US Medical Licensing Exam-style questions), surpassing the prior state of the art by more than 17%. However, human evaluation reveals key gaps. To resolve this, we introduce instruction prompt tuning, a parameter-efficient approach for aligning LLMs to new domains using a few exemplars. The resulting model, Med-PaLM, performs encouragingly, but remains inferior to clinicians. We show that comprehension, knowledge recall and reasoning improve with model scale and instruction prompt tuning, suggesting the potential utility of LLMs in medicine. Our human evaluations reveal limitations of todayâ€™s models, reinforcing the importance of both evaluation frameworks and method development in creating safe, helpful LLMs for clinical applications.",,,Unreleased,"Multinational,United States of America,Canada,Switzerland,United States of America,United Kingdom of Great Britain and Northern Ireland",Flan-PaLM 540B,,,,,2025-05-01 10:42,,,,,,"Industry,Government,Industry",,,,,Unreleased,,"Industry,Government,Industry",,,,,,,,,
Uni-RNA-L-24,Biology,Protein or nucleotide language model (pLM/nLM),DP Technology,"Xi Wang, Ruichu Gu, Zhiyuan Chen, Yongge Li, Xiaohong Ji, Guolin Ke, Han Wen",2023-07-12,Uni-RNA: Universal Pre-Trained Models Revolutionize RNA Research,https://www.biorxiv.org/content/10.1101/2023.07.11.548588v1.abstract,20.0,,,400000000.0,Table 8: Model architecture parameters of different Uni-RNA models,,"""We trained our proposed network on 128 A100""",,,250000000000.0,"Table 8: Model architecture parameters of different Uni-RNA models

Sequences are capped at 4096 length, but no average sequence length is given. Estimating at 500 tokens per sequence. 

500000000*500=250000000000",,,Intel A100,,Likely,"RNA molecules play a crucial role as intermediaries in diverse biological processes. Attaining a profound understanding of their function can substantially enhance our comprehension of lifeâ€™s activities and facilitate drug development for numerous diseases. The advent of high-throughput sequencing technologies makes vast amounts of RNA sequence data accessible, which contains invaluable information and knowledge. However, deriving insights for further application from such an immense volume of data poses a significant challenge. Fortunately, recent advancements in pre-trained models have surfaced as a revolutionary solution for addressing such challenges owing to their exceptional ability to automatically mine and extract hidden knowledge from massive datasets. Inspired by the past successes, we developed a novel context-aware deep learning model named Uni-RNA that performs pre-training on the largest dataset of RNA sequences at the unprecedented scale to date. During this process, our model autonomously unraveled the obscured evolutionary and structural information embedded within the RNA sequences. As a result, through fine-tuning, our model achieved the state-of-the-art (SOTA) performances in a spectrum of downstream tasks, including both structural and functional predictions. Overall, Uni-RNA established a new research paradigm empowered by the large pre-trained model in the field of RNA, enabling the community to unlock the power of AI at a whole new level to significantly expedite the pace of research and foster groundbreaking discoveries.",,,Unreleased,China,,,,128.0,,2025-06-16 11:04,,,,,,Industry,,,,,Unreleased,,Industry,,,,763.8368417530193,,,,,
Uni-RNA-L12,Biology,Protein or nucleotide language model (pLM/nLM),DP Technology,"Xi Wang, Ruichu Gu, Zhiyuan Chen, Yongge Li, Xiaohong Ji, Guolin Ke, Han Wen",2023-07-12,Uni-RNA: Universal Pre-Trained Models Revolutionize RNA Research,https://www.biorxiv.org/content/10.1101/2023.07.11.548588v1.abstract,20.0,,,85000000.0,Table 8: Model architecture parameters of different Uni-RNA models,,"""We trained our proposed network on 128 A100""",,,50000000000.0,"Table 8: Model architecture parameters of different Uni-RNA models

Sequences are capped at 4096 length, but no average sequence length is given. Estimating at 500 tokens per sequence. 

100000000*500=",,,NVIDIA A100,,Likely,"RNA molecules play a crucial role as intermediaries in diverse biological processes. Attaining a profound understanding of their function can substantially enhance our comprehension of lifeâ€™s activities and facilitate drug development for numerous diseases. The advent of high-throughput sequencing technologies makes vast amounts of RNA sequence data accessible, which contains invaluable information and knowledge. However, deriving insights for further application from such an immense volume of data poses a significant challenge. Fortunately, recent advancements in pre-trained models have surfaced as a revolutionary solution for addressing such challenges owing to their exceptional ability to automatically mine and extract hidden knowledge from massive datasets. Inspired by the past successes, we developed a novel context-aware deep learning model named Uni-RNA that performs pre-training on the largest dataset of RNA sequences at the unprecedented scale to date. During this process, our model autonomously unraveled the obscured evolutionary and structural information embedded within the RNA sequences. As a result, through fine-tuning, our model achieved the state-of-the-art (SOTA) performances in a spectrum of downstream tasks, including both structural and functional predictions. Overall, Uni-RNA established a new research paradigm empowered by the large pre-trained model in the field of RNA, enabling the community to unlock the power of AI at a whole new level to significantly expedite the pace of research and foster groundbreaking discoveries.",,,Unreleased,China,,,,128.0,,2025-06-16 11:04,,,,,,Industry,,,,,Unreleased,,Industry,,,,101844.9122337359,,,,,
Uni-RNA-L16,Biology,Protein or nucleotide language model (pLM/nLM),DP Technology,"Xi Wang, Ruichu Gu, Zhiyuan Chen, Yongge Li, Xiaohong Ji, Guolin Ke, Han Wen",2023-07-12,Uni-RNA: Universal Pre-Trained Models Revolutionize RNA Research,https://www.biorxiv.org/content/10.1101/2023.07.11.548588v1.abstract,20.0,,,169000000.0,"Table 8: Model architecture parameters of different Uni-RNA models

Estimating sequence length at 500 tokens.

500000000*500=250000000000",,"""We trained our proposed network on 128 A100""",,,250000000000.0,Table 8: Model architecture parameters of different Uni-RNA models,,,,,Likely,"RNA molecules play a crucial role as intermediaries in diverse biological processes. Attaining a profound understanding of their function can substantially enhance our comprehension of lifeâ€™s activities and facilitate drug development for numerous diseases. The advent of high-throughput sequencing technologies makes vast amounts of RNA sequence data accessible, which contains invaluable information and knowledge. However, deriving insights for further application from such an immense volume of data poses a significant challenge. Fortunately, recent advancements in pre-trained models have surfaced as a revolutionary solution for addressing such challenges owing to their exceptional ability to automatically mine and extract hidden knowledge from massive datasets. Inspired by the past successes, we developed a novel context-aware deep learning model named Uni-RNA that performs pre-training on the largest dataset of RNA sequences at the unprecedented scale to date. During this process, our model autonomously unraveled the obscured evolutionary and structural information embedded within the RNA sequences. As a result, through fine-tuning, our model achieved the state-of-the-art (SOTA) performances in a spectrum of downstream tasks, including both structural and functional predictions. Overall, Uni-RNA established a new research paradigm empowered by the large pre-trained model in the field of RNA, enabling the community to unlock the power of AI at a whole new level to significantly expedite the pace of research and foster groundbreaking discoveries.",,,Unreleased,China,,,,,,2025-06-16 11:04,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
Claude 2,Language,"Language modeling,Chat,Language modeling/generation,Question answering",Anthropic,,2023-07-11,,"https://www.anthropic.com/index/claude-2, https://www-files.anthropic.com/production/images/Model-Card-Claude-2.pdf",,Historical significance,,,,3.866e+24,https://colab.research.google.com/drive/1MdPuhS4Emaf23VXYZ-ooExDW-5GXZkw0#scrollTo=Ds0Q5X8aMnOY,Unspecified unreleased,"From model card: ""Claude models are trained on a proprietary mix of publicly available information from the Internet, datasets
that we license from third party businesses, and data that our users affirmatively share or that crowd workers provide. Some of the human feedback data used to finetune Claude was made public [12] alongside our RLHF [2] and red-teaming [4] research.
Claude 2â€™s training data cuts off in early 2023, and roughly 10 percent of the data included was non-English.""
This has a knowledge cutoff date of January 2024 - which I assume means January 1, 2024 - https://www.youreverydayai.com/knowledge-cutoff-what-it-is-and-why-it-matters-for-large-language-models/.",,,,,,,Speculative,,,,API access,United States of America,,,,,,2025-05-12 19:44,,,,,,Industry,checked,,1.22e+26,,Unreleased,,Industry,,,,,"Benchmarks,Hardware",,,,3
Emu1 (BAAI),"Vision,Multimodal,Language","Image generation,Text autocompletion,Text-to-image,Visual question answering,Image captioning,Language modeling/generation","Beijing Academy of Artificial Intelligence / BAAI,Tsinghua University,Peking University","Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun Huang, Xinlong Wang",2023-07-11,Generative Pretraining in Multimodality,https://arxiv.org/abs/2307.05222,,,,14000000000.0,"""The total number of parameters of Emu is 14B and is trained end-to-end.""

"" We leverage pretrained EVA-CLIP (Sun et al., 2023), LLaMA (Touvron et al., 2023) and Stable Diffusion (Rombach et al., 2022) to initialize the Visual Encoder, the Multimodal Modeling LLM and the Visual Decoder, respectively.""",2.70000000001e+21,"""We train the model on 128 NVIDIA 80G-A100 GPUs for 10k steps with around 82M samples (150B tokens in total), and the pretraining takes approximately 2 days.""
https://www.wolframalpha.com/input?i=128*312+TFLOPS+*+2+days+*+0.4","LAION-2B,WebVid-10M,LAION-COCO,YT-Storyboard-1B,MMC4 / Multimodal C4","""We pretrain Emu with web-scale data across modalities in various forms, including image-text pairs (LAION-2B (Schuhmann et al., 2022), LAION-COCO (lai, b)), interleaved images-text data
(MMC4 (Zhu et al., 2023b)), video-text pairs (WebVid-10M (Bain et al., 2021)), and our collected interleaved video-text data (YT-Storyboard-1B). """,,,48.0,"""We train the model on 128 NVIDIA 80G-A100 GPUs for 10k steps with around 82M samples (150B tokens in total), and the pretraining takes approximately 2 days.""",NVIDIA A100 SXM4 80 GB,,Confident,"We present Emu, a Transformer-based multimodal foundation model, which can seamlessly generate images and texts in multimodal context. This omnivore model can take in any single-modality or multimodal data input indiscriminately (e.g., interleaved image, text and video) through a one-model-for-all autoregressive training process. First, visual signals are encoded into embeddings, and together with text tokens form an interleaved input sequence. Emu is then end-to-end trained with a unified objective of classifying the next text token or regressing the next visual embedding in the multimodal sequence. This versatile multimodality empowers the exploration of diverse pretraining data sources at scale, such as videos with interleaved frames and text, webpages with interleaved images and text, as well as web-scale image-text pairs and video-text pairs. Emu can serve as a generalist multimodal interface for both image-to-text and text-to-image tasks, and supports in-context image and text generation. Across a broad range of zero-shot/few-shot tasks including image captioning, visual question answering, video question answering and text-to-image generation, Emu demonstrates superb performance compared to state-of-the-art large multimodal models. Extended capabilities such as multimodal assistants via instruction tuning are also demonstrated with impressive performance.",,,Open weights (non-commercial),"China,China,China","Stable Diffusion (LDM-KL-8-G),EVA-01,LLaMA-13B",,,128.0,,2025-06-12 13:11,,,,,,"Academia,Academia,Academia",,,,6144.0,Unreleased,"Apache 2.0
https://github.com/baaivision/Emu/tree/main/Emu1

Llama license (mentioned in github repo)
https://huggingface.co/BAAI/Emu2

training code release is still in their todo list","Academia,Academia,Academia",,,,101847.18028115285,Hardware,BAAI,,,
Baichuan 1-13B,Language,"Language modeling/generation,Question answering",Baichuan,,2023-07-11,"Baichuan-13B
",https://github.com/baichuan-inc/Baichuan-13B/blob/main/README_EN.md,,,,13264901120.0,,9.36e+22,13b parameters * 1.2t tokens * 6 FLOP / parameter / token = 9.36e22 FLOP,,,1400000000000.0,,,,,,Confident,"Baichuan-13B is an open-source, commercially available large-scale language model developed by Baichuan Intelligent Technology following Baichuan-7B, containing 13 billion parameters. It achieves the best results of the same size on both authoritative Chinese and English benchmarks. This release includes two versions: pre-training (Baichuan-13B-Base) and alignment (Baichuan-13B-Chat).",,,Open weights (restricted use),China,,,,,,2025-06-11 17:43,,,,,,Industry,,,,,Open source,"Community License for Baichuan-13B Model (usage restrictions, need to apply for commercial license)
https://huggingface.co/baichuan-inc/Baichuan-13B-Base

""Open source, free and available for commercial use: Baichuan-13B is not only fully open to academic research, but developers can also use it commercially for free, just by applying for and obtaining an official commercial license via email.""

repo is under Apache 2.0
https://github.com/baichuan-inc/Baichuan-13B/tree/main",Industry,,,,,Operation counting,baichuan-inc,,,
TeleChat,Language,"Chat,Language modeling/generation,Text summarization,Translation",China Telecom,,2023-07-07,,https://m.thepaper.cn/baijiahao_23766944,,,"""In addition, TeleChat-E, the educational version of the large model based on TeleChat, ranks seventh on C-Eval, the global comprehensive examination evaluation list for large models. The top few include well-known large models such as GP4 and ChatGPT""",,,,,,"(Google translated from https://m.thepaper.cn/baijiahao_23766944) ""TeleChat uses a large amount of high-quality Chinese and English corpus for pre-training, and uses tens of millions of question and answer data for fine-tuning""",,,,,,,Unknown,"(Google translated) At China Telecom's ""Computing and Network IntegrationÂ·Sunac Future"" sub-forum, China Telecom Digital Intelligence Technology Branch (hereinafter referred to as: Telecom Zhike) officially released China Telecom's large language model TeleChat and demonstrated the large model empowering data Products in three directions: middle platform, intelligent customer service and intelligent government affairs.",,,Unreleased,China,,,,,,2025-06-12 16:14,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
Pangu 3.0,"Multimodal,Language,Image generation,Vision","Language modeling/generation,Image generation",Huawei,,2023-07-07,"Reshaping Industries with AI: Huawei Cloud Launches Pangu Models 3.0 and Ascend AI Cloud Services
",https://www.huaweicloud.com/intl/en-us/news/20230707180809498.html,,,,100000000000.0,"100B? I think the five foundation models are all included in the same system, instead of being five different variants of Pangu, but that's not very clear. I think that's implied by ""All of these capability sets will be made available to customers and partners, and will be consistent regardless of the model size"". 

https://www.huaweicloud.com/intl/en-us/news/20230707180809498.html 
""Huawei Cloud Pangu models were designed to focus on the practical needs of specific industry scenarios. The newly-launched Pangu Models 3.0 use a ""5+N+X"" three-layer architecture.

The L0 layer consists of five foundation models: NLP, CV, multimodal, prediction, and scientific computing, which provide general skills to power an endless possibility of industry-specific applications. Pangu Models 3.0 are available in different sizes: 10 billion parameters, 38 billion parameters, 71 billion parameters, and 100 billion parameters, meeting diverse customer needs and different standards on latency and response times. Brand-new capability sets are also provided, such as knowledge-based Q&A, copywriting, and code generation for the Pangu NLP model; and image generation and understanding for the Pangu multimodal model. All of these capability sets will be made available to customers and partners, and will be consistent regardless of the model size (number of parameters).

The L1 layer consists of N industry-tailored models. Huawei Cloud can provide customers with industry models it has trained on open industry datasets, including Pangu models for government, finance, manufacturing, mining, and meteorology. Alternatively, customers can train their own models using their own datasets based on Huawei's L0 or L1 Pangu models.

The L2 layer provides pre-trained models for specific industry scenarios and tasks, such as intelligent government hotline, intelligent branch assistant, lead compound screening, conveyor belt foreign object detection, and typhoon trajectory prediction. These models can be quickly deployed off-the-shelf.""

",,,,,,,,,,,Likely,"Huawei Cloud Pangu models were designed to focus on the practical needs of specific industry scenarios. The newly-launched Pangu Models 3.0 use a ""5+N+X"" three-layer architecture.

The L0 layer consists of five foundation models: NLP, CV, multimodal, prediction, and scientific computing, which provide general skills to power an endless possibility of industry-specific applications. Pangu Models 3.0 are available in different sizes: 10 billion parameters, 38 billion parameters, 71 billion parameters, and 100 billion parameters, meeting diverse customer needs and different standards on latency and response times. Brand-new capability sets are also provided, such as knowledge-based Q&A, copywriting, and code generation for the Pangu NLP model; and image generation and understanding for the Pangu multimodal model. All of these capability sets will be made available to customers and partners, and will be consistent regardless of the model size (number of parameters).",,,API access,China,,,,,,2025-01-13 17:11,,,,,,Industry,,,,,,,Industry,,,,,,,,,
InternLM,Language,Language modeling,"Shanghai AI Lab,SenseTime",,2023-07-06,,https://github.com/InternLM/InternLM-techreport/blob/main/InternLM.pdf,,SOTA improvement,"(from Google-translated page) ""In addition to using academic datasets to evaluate InternLM, we also use human examinations to assess its capabilities. InternLM can achieve good scores on examination benchmarks such as MMLU, AGIEval, C-Eval, and GAOKAO-bench that cover different languages and subjects, scoring higher than ChatGPT on multiple benchmarks""",104000000000.0,"""We present InternLM, a multilingual foundational language model with 104B parameters""",9.984e+23,6 * 104b * 1.6T = 9.984e23,,,1600000000000.0,"""InternLM is pre-trained on a large corpora with 1.6T tokens""",,Training performance for the open-source InternLM-7B: https://github.com/InternLM/InternLM/blob/main/doc/en/train_performance.md,NVIDIA A100 SXM4 80 GB,,Confident,"Pre-training a bilingual 100B Foundation model on data with over a trillion tokens, the model exhibits excellent performance in scenarios such as Chinese, English, and coding due to the appropriate data ratio. Based on the foundation model, the application of high-quality human annotated dialogue data combined with RLHF technology enables the InternLM large language model to respond to complex commands during human interaction, while also demonstrating responses in line with human morality and values.",,,Unreleased,"China,Hong Kong,China",,,,,,2025-06-10 16:28,,,,,,"Academia,Industry",checked,,,,Unreleased,Though they released 7b and 20b models (https://github.com/InternLM/InternLM/tree/main/model_cards) 100b model is not found,"Academia,Industry",,,,,Operation counting,,,,6
CodeGen2.5,Language,Code generation,Salesforce,"Erik Nijkamp, Hiroaki Hayashi, Yingbo Zhou, Caiming Xiong",2023-07-06,"CodeGen2.5: Small, but mighty",https://blog.salesforceairesearch.com/codegen25/,,,,7000000000.0,7B,5.899999999999999e+22,"7B parameters, trained on 1.4T tokens

7 billion * 1.4 trillion * 6 = 5.9e22",StarCoderData,,300000000000.0,"""an epoch constitutes about 300B tokens""",,,,,Confident,"The family of Salesforce CodeGen models is growing with CodeGen2.5 â€“ a small, but mighty model! While there has been a recent trend of large language models (LLM) of increasing size, we show that a small model can obtain surprisingly good performance, when being trained well.  

The key contributions towards productization of these models are:

Releasing CodeGen2.5 LLM with state-of-the-art on HumanEval for 7B parameters.
CodeGen2.5 with 7B is on par with >15B code-generation models (CodeGen1-16B, CodeGen2-16B, StarCoder-15B), less than half the size.
Featuring robust infill sampling, that is, the model can â€œreadâ€ text of both the left and right hand size of the current position.
Optimized for fast sampling under Flash attention for optimized serving and local deployment on personal machines.
Permissively licensed in Apache 2.0.",4.66,,Open weights (unrestricted),United States of America,,,,,,2025-02-14 16:20,,,,,,Industry,,,,,,apache 2.0,Industry,,,,,Operation counting,,,,
xTrimoPGLM -100B,Biology,"Proteins,Protein or nucleotide language model (pLM/nLM)","Tsinghua University,BioMap Research","Bo Chen, Xingyi Cheng, Yangli-ao Geng, Shen Li, Xin Zeng, Boyan Wang, Jing Gong, Chiming Liu, Aohan Zeng, Yuxiao Dong, Jie Tang, Le Song",2023-07-06,xTrimoPGLM: Unified 100B-Scale Pre-trained Transformer for Deciphering the Language of Protein,https://www.biorxiv.org/content/10.1101/2023.07.05.547496v4,65.0,"SOTA improvement,Training cost","""Our extensive experiments reveal that xTrimoPGLM significantly outperforms other advanced baselines in diverse protein understanding tasks (13 out of 15 tasks across four categories)""",100000000000.0,"Abstract: ""training xTrimoPGLM at an unprecedented scale of 100 billion
parameters and 1 trillion training tokens""",6.2e+23,"""xTrimoPGLM-100B is trained on a cluster of 96 DGX-A100 GPU (8Ã—40G) servers in FP16 precision from January 18 to June 30, 2023. During this time, xTrimoPGLM-100B has consumed 1 trillion tokens from the dataset consisting of Uniref90 and ColAbFoldDB. As of the current date, xTrimoPGLM-100B continues its pre-training process to pass through as many tokens as possible""

6 * 100 billion params * 1T tokens = 6e23

8*96 * 312 trillion * 163 days * 24 * 3600 * 0.3 ~= 1e24

directly given in the paper (Table 9, or Table 4 in new version): 6.2E+23 ",UniRef50,,,~24M protein sequences,3912.0,163 days,NVIDIA A100 SXM4 40 GB,Self-supervised learning,Confident,"Protein language models have shown remarkable success in learning biological information from protein sequences. However, most existing models are limited by either autoencoding or autoregressive pre-training objectives, which makes them struggle to handle protein understanding and generation tasks concurrently. This paper proposes a unified protein language model, xTrimoPGLM, to address these two types of tasks simultaneously through an innovative pre-training framework. Our key technical contribution is an exploration of the compatibility and the potential for joint optimization of the two types of objectives, which has led to a strategy for training xTrimoPGLM at an unprecedented scale of 100 billion parameters and 1 trillion training tokens. Our extensive experiments reveal that xTrimoPGLM significantly outperforms other advanced baselines in diverse protein understanding tasks (13 out of 15 tasks across four categories) and generates novel protein sequences which are structurally similar to natural ones. Furthermore, using the same xTrimoPGLM framework, we train an antibody-specific model (xTrimoPGLM-Ab) using 1 billion parameters. This model set a new record in predicting antibody naturalness and structures, both essential to the field of antibody-based drug design, and demonstrated a significantly faster inference speed than AlphaFold2. These results highlight the substantial capability and versatility of xTrimoPGLM in understanding and generating protein sequences.",,,Unreleased,"China,China",,,,768.0,,2025-05-28 16:05,,,,2097152.0,"""We employ batches of 2,048 sequences, each 1,024 tokens in length""","Academia,Industry",,,,3004416.0,Unreleased,,"Academia,Industry",$1818526.29,,FP16,611151.12765533,"Reported,Operation counting,Hardware",,,,9
NEC LLM 13B,Language,"Language modeling/generation,Chat",NEC Laboratories,,2023-07-06,NECã€130å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ä¸–ç•Œãƒˆãƒƒãƒ—ã‚¯ãƒ©ã‚¹ã®æ—¥æœ¬èªžæ€§èƒ½ã‚’æœ‰ã™ã‚‹è»½é‡ãªLLMã‚’é–‹ç™º,https://jpn.nec.com/press/202307/20230706_02.html,,,,13000000000.0,13B,,"""NEC's LLM relieved this time was also trained using 512 GPUs installed on NEC's AI supercomputer""
from https://jpn.nec.com/rd/technologies/202308/index.html",,,,,,,,,Confident,,,,,United States of America,,,,512.0,,2024-09-05 14:08,,,,,,Industry,,,,,,,Industry,,,,,,,,,
Pangu-Weather,Earth science,Weather forecasting,Huawei,"Kaifeng Bi, Lingxi Xie, Hengheng Zhang, Xin Chen, Xiaotao Gu, Qi Tian",2023-07-05,Accurate medium-range global weather forecasting with 3D neural networks,"https://www.nature.com/articles/s41586-023-06185-3, https://www.huaweicloud.com/intl/en-us/news/20230707180809498.html,
https://www.huawei.com/en/news/2023/7/pangu-ai-model-nature-publish",197.0,SOTA improvement,"""In meteorology, the Pangu Meteorology Model (or Pangu-Weather) is the first AI model to have surpassed state-of-the-art numerical weather prediction (NWP) methods in terms of accuracy. The prediction speed is also several orders of magnitude faster. In the past, predicting the trajectory of a typhoon over 10 days took 4 to 5 hours of simulation on a high-performance cluster of 3,000 servers. Now, the Pangu model can do it in 10 seconds on a single GPU of a single server, and with more accurate results.""

https://www.huaweicloud.com/intl/en-us/news/20230707180809498.html",256000000.0,"4*64 million = 256M params

""We trained four deep networks with lead times (the time difference
between input and output) at 1â€‰h, 3â€‰h, 6â€‰h and 24â€‰h, respectively... 

This modification increases the number of bias parameters by a factor of 527, with each 3D deep network containing approximately 64â€‰million parameters.""",3.98e+22,"""Each of the four deep networks was trained for 100 epochs, and
each of them takes approximately 16â€‰days on a cluster of 192 NVIDIA
Tesla-V100 GPUs.""

192 * 4 * 16 * 24 * 3600 * 125 teraflops * 0.3 utilization = 3.98e22",ERA5,"""We used a single point in time for both input and output. The time resolution
of the ERA5 data is 1â€‰h; in the training subset (1979â€“2017), there were
as many as 341,880 time points, the amount of training data in one
epoch""",,"""We used a single point in time for both input and output. The time resolution
of the ERA5 data is 1â€‰h; in the training subset (1979â€“2017), there were
as many as 341,880 time points, the amount of training data in one
epoch... We fed all included weather variables, including 13 layers of upper-air
variables and the surface variables""

341,880 is the number of hours in ~40 years. But there's lots of data for each hour.",1536.0,"4*16 = 64 days
""Each of the four deep networks was trained for 100 epochs, andeach of them takes approximately 16â€‰days on a cluster of 192 NVIDIA Tesla-V100 GPUs.""
",NVIDIA V100,,Confident,"Weather forecasting is important for science and society. At present, the most accurate forecast system is the numerical weather prediction (NWP) method, which represents atmospheric states as discretized grids and numerically solves partial diferential equations that describe the transition between those states1 . However, this procedure is computationally expensive. Recently, artifcial-intelligence-based methods2 have shown potential in accelerating weather forecasting by orders of magnitude, but the forecast accuracy is still signifcantly lower than that of NWP methods. Here we introduce an artifcial-intelligence-based method for accurate, medium-range global weather forecasting. We show that three-dimensional deep networks equipped with
Earth-specifc priors are efective at dealing with complex patterns in weather data, and that a hierarchical temporal aggregation strategy reduces accumulation errors in medium-range forecasting. Trained on 39â€‰years of global data, our program, Pangu-Weather, obtains stronger deterministic forecast results on reanalysis data in all tested variables when compared with the worldâ€™s best NWP system, the operational integrated forecasting system of the European Centre for Medium-Range Weather Forecasts (ECMWF)3
. Our method also works well with extreme weather forecasts and ensemble forecasts. When initialized with reanalysis data, the accuracy of tracking
tropical cyclones is also higher than that of ECMWF-HRES.",100.0,,Open weights (non-commercial),China,,,"Possibly based on Pangu 3? Pangu-Weather is mentioned in the Pangu 3 announcement. But the architecture description doesn't seem to resemble Pangu 3. So it seems like Pangu-Weather is one of the higher-level models that can be attached to Pangu 3. 

https://www.huaweicloud.com/intl/en-us/news/20230707180809498.html 
",192.0,,2025-02-14 16:20,,,,,,Industry,,,,,Unreleased,"Models and code here: https://github.com/198808xc/Pangu-Weather 

Commercial use forbidden",Industry,$51279.02,,,114593.38832967015,Hardware,,,,
LongNet,Language,Language modeling/generation,"Microsoft,Xiâ€™an Jiaotong University","Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, Nanning Zheng, Furu Wei
",2023-07-05,"LongNet: Scaling Transformers to 1,000,000,000 Tokens",https://arxiv.org/abs/2307.02486,112.0,,Very long sequence length (1b),2700000000.0,2.7B,4.86e+21,"2.7B params * 300B tokens * 6 = 4.86e21

Note: not sure if there are very long sequences in the training data that would affect this calculation. Per paper, complexity of their attention mechanism scales linearly with sequence length.",The Stack,"2.7B model uses 300B tokens from The Stack, others only use 40B.",300000000000.0,"2.7B model uses 300B tokens from The Stack, others only use 40B.",,,,,Confident,"Scaling sequence length has become a critical demand in the era of large language models. However, existing methods struggle with either computational complexity or model expressivity, rendering the maximum sequence length restricted. To address this issue, we introduce LongNet, a Transformer variant that can scale sequence length to more than 1 billion tokens, without sacrificing the performance on shorter sequences. Specifically, we propose dilated attention, which expands the attentive field exponentially as the distance grows. LongNet has significant advantages: 1) it has a linear computation complexity and a logarithm dependency between any two tokens in a sequence; 2) it can be served as a distributed trainer for extremely long sequences; 3) its dilated attention is a drop-in replacement for standard attention, which can be seamlessly integrated with the existing Transformer-based optimization. Experiments results demonstrate that LongNet yields strong performance on both long-sequence modeling and general language tasks. Our work opens up new possibilities for modeling very long sequences, e.g., treating a whole corpus or even the entire Internet as a sequence.",1.0,,,"United States of America,Multinational,India,Belgium,China",,,,,,2025-02-14 16:20,,,,,,"Industry,Academia",,,,,,,"Industry,Academia",,,,,Operation counting,,,,
Stable Diffusion XL (SDXL),Image generation,"Image generation,Text-to-image",Stability AI,"Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas MÃ¼ller, Joe Penna, Robin Rombach",2023-07-04,SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis,https://arxiv.org/abs/2307.01952,1165.0,Significant use,Looks like this is now the main/flagship Stable Diffusion model,3400000000.0,"""...result in a model size of 2.6B parameters in the UNet, see Tab. 1. The text encoders have a total size of 817M parameters.""",,,Unspecified unreleased,,,,,,,,Speculative,"We present SDXL, a latent diffusion model for text-to-image synthesis. Compared to previous versions of Stable Diffusion, SDXL leverages a three times larger UNet backbone: The increase of model parameters is mainly due to more attention blocks and a larger cross-attention context as SDXL uses a second text encoder. We design multiple novel conditioning schemes and train SDXL on multiple aspect ratios. We also introduce a refinement model which is used to improve the visual fidelity of samples generated by SDXL using a post-hoc image-to-image technique. We demonstrate that SDXL shows drastically improved performance compared the previous versions of Stable Diffusion and achieves results competitive with those of black-box state-of-the-art image generators. In the spirit of promoting open research and fostering transparency in large model training and evaluation, we provide access to code and model weights at this https URL",,,Open weights (non-commercial),United Kingdom of Great Britain and Northern Ireland,,,,,,2025-06-02 12:12,,,,,,Industry,,,,,Unreleased,"SDXL 0.9 Research License:
https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9

MIT license for inference code, not sure if training code is here:
https://github.com/Stability-AI/generative-models/tree/main",Industry,,,,,,stabilityai,,,
Multilingual-E5-large,Language,Semantic embedding,Microsoft,"Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, Furu Wei",2023-06-30,Multilingual E5 Text Embeddings: A Technical Report,"https://arxiv.org/abs/2402.05672
https://huggingface.co/intfloat/multilingual-e5-large",,,,560000000.0,560M from https://huggingface.co/intfloat/multilingual-e5-large,3.370752e+18,"6ND = 6*560000000*(1000000000+1600000*2 epochs) = 3.370752e+18

confidence 'likely"" because pre-training epochs are unknown","mC4,Wikipedia,Stack Exchange,xP3,CC-News",table 2 - multiple data,1000160000.0,"Pre-training: Table 1 around 1B text pairs in different languages
Fine-tuning: 1.6M 

total: 1000000000+160000 = 1000160000 text pairs ",,,,,Likely,"This technical report presents the training methodology and evaluation results of the open-source multilingual E5 text embedding models, released in mid-2023. Three embedding models of different sizes (small / base / large) are provided, offering a balance between the inference efficiency and embedding quality. The training procedure adheres to the English E5 model recipe, involving contrastive pre-training on 1 billion multilingual text pairs, followed by fine-tuning on a combination of labeled datasets. Additionally, we introduce a new instruction-tuned embedding model, whose performance is on par with state-of-the-art, English-only models of similar sizes. Information regarding the model release can be found at https://github.com/microsoft/unilm/tree/master/e5 ",,,Open weights (unrestricted),"United States of America,Multinational,India,Belgium",,,,,,2025-02-14 16:20,,,,,,Industry,,,,,Unreleased,"weights: https://github.com/microsoft/unilm/blob/master/e5/README.md

license (MIT):
https://github.com/microsoft/unilm/blob/master/LICENSE",Industry,,,,,Operation counting,,,,
Honghu Graphic,"Multimodal,Vision,Image generation","Text-to-image,Image generation",China Unicom,,2023-06-28,,https://medium.com/@sdatokens/china-unicom-released-a-graphic-model-that-can-realize-with-text-pictures-video-clips-llm-ai-d50afd965a5b,,,,2000000000.0,"""The model now has 800 million training parameters and 2 billion training parameters in two versions, which can fulfill functions such as text-to-picture and picture-to-picture, said staff members of the company."" https://www.chinadaily.com.cn/a/202309/02/WS64f2715aa310d2dce4bb3865.html",,,,,,,,,,,Likely,"From https://www.chinadaily.com.cn/a/202309/02/WS64f2715aa310d2dce4bb3865.html:

""Among them, China Unicom is showcasing the Honghu Graphic Grand Model 1.0, its first large model for operators' value-added business.

The model now has 800 million training parameters and 2 billion training parameters in two versions, which can fulfill functions such as text-to-picture and picture-to-picture, said staff members of the company.""",,,,China,,,,,,2024-09-05 14:08,,,,,,Industry,,,,,,,Industry,,,,,,,,,
ERNIE 3.5,Language,"Language modeling,Language modeling/generation",Baidu,,2023-06-27,Introducing ERNIE 3.5: Baiduâ€™s Knowledge-Enhanced Foundation Model Takes a Giant Leap Forward,http://research.baidu.com/Blog/index-view?id=185,,SOTA improvement,SOTA scores on AGIEval and MMLU. See article in China Science Daily: https://mp.weixin.qq.com/s/QVdkmofRSTgjQ7UOFX7s1g,,,,,,,,,,,,,Unknown,,,,Hosted access (no API),China,,,,,,2025-05-28 17:08,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
HyenaDNA,Biology,Protein or nucleotide language model (pLM/nLM),"Stanford University,Harvard University,Mila - Quebec AI (originally Montreal Institute for Learning Algorithms),University of Montreal / UniversitÃ© de MontrÃ©al","Eric Nguyen, Michael Poli, Marjan Faizi, Armin W. Thomas, Callum Birch Sykes, Michael Wornow, Aman Patel, Clayton Rabideau, Stefano Massaroli, Yoshua Bengio, Stefano Ermon, Stephen A. Baccus, Christopher RÃ©",2023-06-27,HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution,https://arxiv.org/abs/2306.15794,123.0,SOTA improvement,"""On fine-tuned benchmarks from the Nucleotide Transformer, HyenaDNA reaches state-of-the-art (SotA) on 12 of 18 datasets using a model with orders of magnitude less parameters and pretraining data.1 On the GenomicBenchmarks, HyenaDNA surpasses SotA on 7 of 8 datasets on average by +10 accuracy points, and by as much as +20 accuracy points on enhancer identification.""",6600000.0,"Table A.1 shows details, largest experiment is on far right.",1.811e+21,"8 Nvidia A100 (80GB) GPUs, ~4 weeks
(4 * 7 * 24 * 3600) seconds * (8 * 3.12e14) FLOP/sec * 0.3 (utilization) = 1.811e21",Human Reference Genome (GRCh38/hg38),"""For pretraining, we use a single human reference genome [...] For the test set, we use chromosomes 14 and X, exclusively"" 
Human genome is ~3.2B nucleotide pairs, 14 and X are ~101M and 154M respectively.",2945000000.0,"Human genome is ~3.2B nucleotide pairs, 14 and X are ~101M and 154M respectively. Largest run sees 2T tokens, which implies ~679 epochs.",672.0,"""For example, the largest model with context length 1M was trained on 2T tokens over 4 weeks.""",NVIDIA A100,,Confident,"Genomic (DNA) sequences encode an enormous amount of information for gene regulation and protein synthesis. Similar to natural language models, researchers have proposed foundation models in genomics to learn generalizable features from unlabeled genome data that can then be fine-tuned for downstream tasks such as identifying regulatory elements. Due to the quadratic scaling of attention, previous Transformer-based genomic models have used 512 to 4k tokens as context (<0.001% of the human genome), significantly limiting the modeling of long-range interactions in DNA. In addition, these methods rely on tokenizers or fixed k-mers to aggregate meaningful DNA units, losing single nucleotide resolution where subtle genetic variations can completely alter protein function via single nucleotide polymorphisms (SNPs). Recently, Hyena, a large language model based on implicit convolutions was shown to match attention in quality while allowing longer context lengths and lower time complexity. Leveraging Hyena's new long-range capabilities, we present HyenaDNA, a genomic foundation model pretrained on the human reference genome with context lengths of up to 1 million tokens at the single nucleotide-level - an up to 500x increase over previous dense attention-based models. HyenaDNA scales sub-quadratically in sequence length (training up to 160x faster than Transformer), uses single nucleotide tokens, and has full global context at each layer. We explore what longer context enables - including the first use of in-context learning in genomics. On fine-tuned benchmarks from the Nucleotide Transformer, HyenaDNA reaches state-of-the-art (SotA) on 12 of 18 datasets using a model with orders of magnitude less parameters and pretraining data. On the GenomicBenchmarks, HyenaDNA surpasses SotA on 7 of 8 datasets on average by +10 accuracy points.",679.12,,Open weights (restricted use),"United States of America,United States of America,Canada,Canada",,,,8.0,,2025-06-02 12:17,,,,64000000.0,"Table A.1 indicates largest model saw sequence length of 1M, and that batch sizes range from 64-1024. In section 3.2: ""Our sequence length schedule starts at L1 = 64, then doubles the window at each stage while keeping the global batch size constant."" I assume smallest batch size was used for largest sequence length.","Academia,Academia,Academia,Academia",,,,,Open source,"BSD 3-Clause License for weights (prohibits others from using the name of the copyright holder)
https://huggingface.co/LongSafari/hyenadna-large-1m-seqlen-hf

training code: Apache 2.0
https://github.com/HazyResearch/hyena-dna","Academia,Academia,Academia,Academia",$5000.00,,FP16,6367.433640556241,Hardware,LongSafari,,,
RWKV-4 World (7B),Language,"Language modeling/generation,Chat",RWKV Foundation,,2023-06-26,RWKV-4 World,https://huggingface.co/BlinkDL/rwkv-4-world,,,,7393000000.0,"7B
Table 2 https://arxiv.org/pdf/2305.13048",,,"The Pile,RedPajama,OSCAR,Wikipedia","""RWKV-4 trained on 100+ world languages (70% English, 15% multilang, 15% code).

World = Some_Pile + Some_RedPajama + Some_OSCAR + All_Wikipedia + All_ChatGPT_Data_I_can_find""",,,,,,,Confident,"Model Description
RWKV-4 trained on 100+ world languages (70% English, 15% multilang, 15% code).

World = Some_Pile + Some_RedPajama + Some_OSCAR + All_Wikipedia + All_ChatGPT_Data_I_can_find

XXXtuned = finetune of World on MC4, OSCAR, wiki, etc.",,,Open weights (unrestricted),Multinational,,,,,,2024-09-05 14:08,,,,,,Research collective,,,,,Open source,Apache 2.0: https://huggingface.co/BlinkDL/rwkv-4-world,Research collective,,,,,,,,,
Kosmos-2,"Language,Vision,Multimodal","Visual question answering,Image captioning,Named entity recognition,Character recognition,Document representation",Microsoft,"Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, Furu Wei",2023-06-26,Kosmos-2: Grounding Multimodal Large Language Models to the World,https://arxiv.org/abs/2306.14824,492.0,,,1600000000.0,1.6B,4.5500354284e+20,"""We train the model on 256 V100 GPUs and the training takes approximately one day to complete""
""We train KOSMOS-2 for 60k steps, equivalent to around 25 billion tokens""

GPU-time method
(256) * (1.3e14) * (24 * 3600) * (0.3) = 8.626176e20
(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)

Parameter-data method
6ND = 6*25B*1.6B = 2.4e20

Used geometric mean of two estimates.",GRIT,"""We train the model on newly added grounded image-text pairs, monomodal text
corpora, image-caption pairs, and interleaved image-text data. Our training process involves a
batch size of 419K tokens, consisting of 185K tokens from text corpora, 215K tokens from original
and grounded image-caption pairs, and 19K tokens from interleaved data. We train KOSMOS-2
for 60k steps, equivalent to around 25 billion tokens""",,"text and images
 ""We train KOSMOS-2 for 60k steps, equivalent to around 25 billion tokens""
""We train the model on newly added grounded image-text pairs, monomodal text corpora, image-caption pairs, and interleaved image-text data. Our training process involves a batch size of 419K tokens, consisting of 185K tokens from text corpora, 215K tokens from original and grounded image-caption pairs, and 19K tokens from interleaved data. We train KOSMOS-2
for 60k steps, equivalent to around 25 billion tokens""
",24.0,""" We train the model on 256 V100 GPUs and the training takes approximately one day to complete""",NVIDIA V100,Self-supervised learning,Likely,"We introduce Kosmos-2, a Multimodal Large Language Model (MLLM), enabling new capabilities of perceiving object descriptions (e.g., bounding boxes) and grounding text to the visual world. Specifically, we represent refer expressions as links in Markdown, i.e., ``[text span](bounding boxes)', where object descriptions are sequences of location tokens. Together with multimodal corpora, we construct large-scale data of grounded image-text pairs (called GrIT) to train the model. In addition to the existing capabilities of MLLMs (e.g., perceiving general modalities, following instructions, and performing in-context learning), Kosmos-2 integrates the grounding capability into downstream applications. We evaluate Kosmos-2 on a wide range of tasks, including (i) multimodal grounding, such as referring expression comprehension, and phrase grounding, (ii) multimodal referring, such as referring expression generation, (iii) perception-language tasks, and (iv) language understanding and generation. This work lays out the foundation for the development of Embodiment AI and sheds light on the big convergence of language, multimodal perception, action, and world modeling, which is a key step toward artificial general intelligence. Code and pretrained models are available at this https://github.com/microsoft/unilm/tree/master/kosmos-2",,,Open weights (unrestricted),"United States of America,Multinational,India,Belgium",,,,256.0,,2025-06-12 12:37,,,,,,Industry,,,,6144.0,Open source,"data: https://huggingface.co/datasets/zzliang/GRIT
weights and code, includes training: https://github.com/microsoft/unilm/tree/master/kosmos-2
license for full repo (MIT):
https://github.com/microsoft/unilm/blob/master/LICENSE

MIT license
https://huggingface.co/microsoft/kosmos-2-patch14-224",Industry,,,,152821.8105810948,"Operation counting,Hardware",microsoft,,,
Llama-2-Chinese 13B,Language,"Language modeling/generation,Question answering,Code generation,Quantitative reasoning",FlagAlpha,,2023-06-25,,https://github.com/FlagAlpha/Llama2-Chinese,,,,13000000000.0,,,,,"Several Chinese datasets:

https://github.com/FlagAlpha/Llama2-Chinese#-%E4%B8%AD%E6%96%87%E6%95%B0%E6%8D%AE",,,,,,,Likely,"Welcome to the Llama Chinese community! We are an advanced technical community focusing on the optimization and upper-level construction of the Llama model in Chinese. *Based on large-scale Chinese data, the Chinese capabilities of the Llama2 model are continuously iteratively upgraded starting from pre-training* . We warmly welcome developers and researchers who are passionate about large model LLM to join us.",,,Open weights (unrestricted),China,Llama 2-13B,,They did both additional pre-training in Chinese (token count not stated) and LoRA fine-tuning.,,,2024-09-16 13:01,,,,,,Research collective,,,,,Open (non-commercial),"license isn't clear, it's not on github (where the code is) but HF says apache:
https://huggingface.co/FlagAlpha/Llama2-Chinese-7b-Chat",Research collective,,,,,,,,,
Inflection-1,Language,Language modeling,Inflection AI,,2023-06-23,Inflection-1 technical memo,https://inflection.ai/assets/Inflection-1.pdf,,,"""Inflection-1 outperforms models trained with at most the same amount of compute as PaLM-540B on MMLU and the other benchmarks in Table 1.""",,,1.0001e+24,"<= 2.5e24

They define two ""compute classes"", one for models with more compute than PaLM 540B, i.e. GPT-4 and PaLM 2, and one for models with as much compute or less, i.e. GPT-3.5, Chinchilla, LLaMA, and Inflection-1.

PaLM 540B required 2.5e24 FLOP to train (confirmed by Google)",,"""Inflection-1 was trained using thousands of NVIDIA H100 GPUs on a very large dataset.""",,,,,NVIDIA H100 SXM5 80GB,,Speculative,"Large language models (LLMs) based on the Transformer architecture have been shown to possess a range of advanced capabilities in language generation and understanding. These capabilities have paved the way for deployment of LLMs in products like OpenAIâ€™s Chat-GPT and Googleâ€™s Bard. At Inflection AI, our mission is to create personal AIs for everyone, and in May 2023 we released Pi (pi.ai) â€“ an LLM-based personal AI which is designed to be empathetic, useful, and safe. In this work we introduce the foundation model powering Pi, dubbed Inflection-1, and evaluate its performance characteristics across a variety of benchmarks.",,,Hosted access (no API),United States of America,,,,,,2025-06-10 16:03,,,,,,Industry,checked,,2.4999999999999997e+24,,Unreleased,,Industry,,,,,Comparison with other models,,,,
MPT-30B,Language,"Language generation,Code generation",MosaicML,,2023-06-22,,https://huggingface.co/mosaicml/mpt-30b,,,,30000000000.0,30B,1.8900000000001e+23,"According to their blog post,
""MPT-30B FLOPs ~= 6 * 30e9 [params] * 1.05e12 [tokens] = 1.89e23 FLOPs""","mC4,C4,RedPajama,The Stack",https://www.databricks.com/sites/default/files/inline-images/open-source-foundations-models-1.png,1050000000000.0,"~4T tokens across sources, but only trained on 1.05T of these",278.4,30B: 512x H100-80gb for 11.6 days,NVIDIA H100 SXM5 80GB,,Confident,"MPT-30B is a decoder-style transformer pretrained from scratch on 1T tokens of English text and code. This model was trained by MosaicML.

MPT-30B is part of the family of Mosaic Pretrained Transformer (MPT) models, which use a modified transformer architecture optimized for efficient training and inference.

MPT-30B comes with special features that differentiate it from other LLMs, including an 8k token context window (which can be further extended via finetuning; see MPT-7B-StoryWriter), support for context-length extrapolation via ALiBi, and efficient inference + training via FlashAttention. It also has strong coding abilities thanks to its pretraining mix. MPT models can also be served efficiently with both standard HuggingFace pipelines and NVIDIA's FasterTransformer. The size of MPT-30B was also specifically chosen to make it easy to deploy on a single GPUâ€”either 1xA100-80GB in 16-bit precision or 1xA100-40GB in 8-bit precision.",1.0,,Open weights (unrestricted),United States of America,,,,512.0,0.372,2025-06-10 15:50,Databricks,,,4096000.0,"last two batch sizes were 3,456,000 and 4,096,000, but 4,096,000 only used for last 5% of training

""To build 8k support into MPT-30B efficiently, we first pre-trained on 1T tokens using sequences that were 2k tokens long, and then trained for an additional 50B tokens using sequences that were 8k tokens long...

The model was trained in three stages using the MosaicML Platform: (i) First it was trained on 440 A100-40GBs with a batch size of 1760. (ii) Then, on 216 A100-40GBs with a batch size of 1728. (iii) Training was completed on 256 H100-80GBs with a batch size of 512 with 8k context length and 50B tokens""


",Industry,,,,142541.0,Open source,"apache 2.0 for weights.

pretrain code here: https://github.com/mosaicml/llm-foundry/tree/main/scripts/train/yamls/pretrain ",Industry,,"6NC gives 1.89e23 FLOPs needed to train
GPU details give 11.6 days on 512 H100s: 11.6 * 24 * 3600 * 512 * 9.9e14 = 5.08e23
1.89e23 / 5.08e23 = 0.3720",,713231.9794598748,Operation counting,mosaicml,,,
Vicuna-33B-v1.3,Language,"Language modeling/generation,Chat","Large Model Systems Organization,University of California (UC) Berkeley",,2023-06-22,Vicuna Model Card,https://huggingface.co/lmsys/vicuna-33b-v1.3,,,,33000000000.0,,,lower bound from Vicuna-13B-v1.3 estimation,Vicuna ShareGPT Dataset,,370000000.0,"""The training data is around 125K conversations collected from ShareGPT.com.""
370M tokens (Table 14)

https://arxiv.org/pdf/2306.05685",,,,,Confident,"Vicuna v1.3 is fine-tuned from LLaMA with supervised instruction fine-tuning. The training data is around 125K conversations collected from ShareGPT.com. See more details in the ""Training Details of Vicuna Models"" section in the appendix of this paper.",,,Open weights (restricted use),"United States of America,United States of America",LLaMA-33B,,,,,2025-05-01 10:42,,,,,,"Academia,Academia",,8.658e+19,,,Open source,"Llama license for merged weights
https://huggingface.co/lmsys/vicuna-33b-v1.3

apache 2 for code https://github.com/lm-sys/FastChat","Academia,Academia",,,,,Operation counting,,,,
Vicuna-13B-v1.3,Language,"Language modeling/generation,Chat","Large Model Systems Organization,University of California (UC) Berkeley",,2023-06-22,Vicuna Model Card,https://huggingface.co/lmsys/vicuna-13b-v1.3,,,,13000000000.0,,,,Vicuna ShareGPT Dataset,,370000000.0,"""The training data is around 125K conversations collected from ShareGPT.com.""""
370M tokens (Table 14)

https://arxiv.org/pdf/2306.05685""",,""" The training is done with 8x A100 GPUs. The longest single training run takes around 2 days.""",NVIDIA A100,,Confident,"Vicuna v1.3 is fine-tuned from LLaMA with supervised instruction fine-tuning. The training data is around 125K conversations collected from ShareGPT.com. See more details in the ""Training Details of Vicuna Models"" section in the appendix of this paper.
https://arxiv.org/abs/2306.05685",3.0,,Open weights (restricted use),"United States of America,United States of America",LLaMA-13B,86580000000000000000,6*13*10^9*370*10^6*3 = 8.658e+19,8.0,,2025-05-01 10:42,,,,,,"Academia,Academia",,,,,Open source,"llama license for merged weights
https://huggingface.co/lmsys/vicuna-13b-v1.3

apache 2 for code https://github.com/lm-sys/FastChat","Academia,Academia",,,,6368.142673748882,Operation counting,,,,
Vicuna-7B-v1.3,Language,"Language modeling/generation,Chat","Large Model Systems Organization,University of California (UC) Berkeley",,2023-06-22,Vicuna Model Card,https://huggingface.co/lmsys/vicuna-7b-v1.3,,,,7000000000.0,,,,Vicuna ShareGPT Dataset,,370000000.0,"""The training data is around 125K conversations collected from ShareGPT.com.""""
370M tokens (Table 14)

https://arxiv.org/pdf/2306.05685""",48.0,""" The training is done with 8x A100 GPUs. The longest single training run takes around 2 days.""",NVIDIA A100,,Confident,"Vicuna v1.3 is fine-tuned from LLaMA with supervised instruction fine-tuning. The training data is around 125K conversations collected from ShareGPT.com. See more details in the ""Training Details of Vicuna Models"" section in the appendix of this paper.
https://arxiv.org/abs/2306.05685",3.0,,Open weights (restricted use),"United States of America,United States of America",LLaMA-7B,46620000000000000000,"6*7*10^9*370*10^6*3 = 4.662e+19

312000000000000*8*48*3600*0.3 = 1.2939264e+20

""The longest single training run takes around 2 days."" -> it is likely to be slightly less than 48 hours",8.0,,2025-05-01 10:42,,,,,,"Academia,Academia",,,,,Open source,"llama license for merged weights
https://huggingface.co/lmsys/vicuna-7b-v1.3

apache 2 for code https://github.com/lm-sys/FastChat","Academia,Academia",,,,6368.142673748882,"Operation counting,Hardware",,,,
RoboCat,Robotics,Robotic manipulation,"Google DeepMind,Google","Konstantinos Bousmalis, Giulia Vezzani, Dushyant Rao, Coline Devin, Alex X. Lee, Maria Bauza, Todor Davchev, Yuxiang Zhou, Agrim Gupta, Akhil Raju, Antoine Laurens, Claudio Fantacci, Valentin Dalibard, Martina Zambelli, Murilo Martins, Rugile Pevceviciute, Michiel Blokzijl, Misha Denil, Nathan Batchelor, Thomas Lampe, Emilio Parisotto, Konrad Å»oÅ‚na, Scott Reed, Sergio GÃ³mez Colmenarejo, Jon Scholz, Abbas Abdolmaleki, Oliver Groth, Jean-Baptiste Regli, Oleg Sushkov, Tom RothÃ¶rl, JosÃ© Enrique Chen, Yusuf Aytar, Dave Barker, Joy Ortiz, Martin Riedmiller, Jost Tobias Springenberg, Raia Hadsell, Francesco Nori, Nicolas Heess",2023-06-20,RoboCat: A Self-Improving Foundation Agent for Robotic Manipulation,https://arxiv.org/abs/2306.11706,33.0,SOTA improvement,Top10 recent paper from Sebastian Sartor 2025-05-14,1180000000.0,"""Most of the experimental results are based on models with a 1.18B-parameter decoder-only transformer (Vaswani et al., 2017) with 24 layers, an embedding size of 2048, and a post-attention feedforward hidden size of 8196."" page 8",,,,"""We use a diverse and large number of datasets for training RoboCat. These include data from agent experience, human demonstrations and self-generated data, on both simulated and real-world robot environments. See Section 3.4 for details on our datasets.""",,,,,,,Speculative,"The ability to leverage heterogeneous robotic experience from different robots and tasks to quickly master novel skills and embodiments has the potential to transform robot learning. Inspired by recent advances in foundation models for vision and language, we propose a foundation agent for robotic manipulation. This agent, named RoboCat, is a visual goal-conditioned decision transformer capable of consuming multi-embodiment action-labelled visual experience. This data spans a large repertoire of motor control skills from simulated and real robotic arms with varying sets of observations and actions. With RoboCat, we demonstrate the ability to generalise to new tasks and robots, both zero-shot as well as through adaptation using only 100--1000 examples for the target task. We also show how a trained model itself can be used to generate data for subsequent training iterations, thus providing a basic building block for an autonomous improvement loop. We investigate the agent's capabilities, with large-scale evaluations both in simulation and on three different real robot embodiments. We find that as we grow and diversify its training data, RoboCat not only shows signs of cross-task transfer, but also becomes more efficient at adapting to new tasks.",,,Unreleased,"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France,United States of America",,,,,,2025-06-11 21:39,,,,,,"Industry,Industry",,,,,Unreleased,https://deepmind.google/discover/blog/robocat-a-self-improving-robotic-agent/,"Industry,Industry",,,,,,,,,
GigaGAN,Image generation,"Text-to-image,Image generation","POSTECH,Carnegie Mellon University (CMU),Adobe","Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain Paris, Taesung Park",2023-06-19,Scaling up GANs for Text-to-Image Synthesis,https://arxiv.org/abs/2303.05511,,,,1000000000.0,1B (Table 2),3.8680312e+22,"""GigaGAN and SD-v1.5 require 4,783 and 6,250 A100 GPU days""

312*10^12 FLOP / sec / GPU * 4783 GPU-days * 24 hours / day * 3600 sec / hour * 0.3 [assumed utilization] = 3.8680312e+22 FLOP","LAION-2B,COYO-700M,Unspecified unreleased",""" For text-to-image synthesis, we train our models on the union of LAION2B-en [88] and COYO-700M [8] datasets, with the exception of the 128-to1024 upsampler model trained on Adobeâ€™s internal Stock images.""",980000000.0,"[IMAGES]

0.98B of training images (table 2)",,,,,Confident,"The recent success of text-to-image synthesis has taken the world by storm and captured the general public's imagination. From a technical standpoint, it also marked a drastic change in the favored architecture to design generative image models. GANs used to be the de facto choice, with techniques like StyleGAN. With DALL-E 2, auto-regressive and diffusion models became the new standard for large-scale generative models overnight. This rapid shift raises a fundamental question: can we scale up GANs to benefit from large datasets like LAION? We find that naÃvely increasing the capacity of the StyleGAN architecture quickly becomes unstable. We introduce GigaGAN, a new GAN architecture that far exceeds this limit, demonstrating GANs as a viable option for text-to-image synthesis. GigaGAN offers three major advantages. First, it is orders of magnitude faster at inference time, taking only 0.13 seconds to synthesize a 512px image. Second, it can synthesize high-resolution images, for example, 16-megapixel pixels in 3.66 seconds. Finally, GigaGAN supports various latent space editing applications such as latent interpolation, style mixing, and vector arithmetic operations.",,,Unreleased,"Korea (Republic of),United States of America,United States of America",,,,,,2025-05-14 15:01,,,,,,"Academia,Academia,Industry",,,,,Unreleased,,"Academia,Academia,Industry",,,,,Hardware,,,,
Pix2Struct-Large,Vision,"Image captioning,Visual question answering","Google Research,University of Cambridge","Kenton Lee, Mandar Joshi, Iulia Turc, Hexiang Hu, Fangyu Liu, Julian Eisenschlos, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang, Kristina Toutanova",2023-06-15,Pix2Struct: Screenshot Parsing as Pretraining for Visual Language Understanding,https://arxiv.org/abs/2210.03347,,,,1300000000.0,1.3B,1.7380147e+20,"""The large model is pretrained for 170K steps with a batch size of 1024""
""The decoder sequence length is 128 tokens""

6 FLOP / token / parameter * 1.3 * 10^9 parameters * 128 tokens / sample* 1024 samples / step * 170000 steps = 1.7380147e+20 FLOP",C4,"""The pretraining data is constructed from URLs in the C4 corpus. We collect 80M (about one third of the total number of documents) pairs of screenshots paired with their HTML source.""",,,,,,,Likely,"Visually-situated language is ubiquitous -- sources range from textbooks with diagrams to web pages with images and tables, to mobile apps with buttons and forms. Perhaps due to this diversity, previous work has typically relied on domain-specific recipes with limited sharing of the underlying data, model architectures, and objectives. We present Pix2Struct, a pretrained image-to-text model for purely visual language understanding, which can be finetuned on tasks containing visually-situated language. Pix2Struct is pretrained by learning to parse masked screenshots of web pages into simplified HTML. The web, with its richness of visual elements cleanly reflected in the HTML structure, provides a large source of pretraining data well suited to the diversity of downstream tasks. Intuitively, this objective subsumes common pretraining signals such as OCR, language modeling, image captioning. In addition to the novel pretraining strategy, we introduce a variable-resolution input representation and a more flexible integration of language and vision inputs, where language prompts such as questions are rendered directly on top of the input image. For the first time, we show that a single pretrained model can achieve state-of-the-art results in six out of nine tasks across four domains: documents, illustrations, user interfaces, and natural images.",,,Open weights (unrestricted),"Multinational,United States of America,Canada,Switzerland,United Kingdom of Great Britain and Northern Ireland",,,,128.0,,2025-06-01 14:51,,,,,,"Industry,Academia",,,,,Open source,"Apache 2.0
https://github.com/google-research/pix2struct","Industry,Academia",,,,,Operation counting,,,,
WizardCoder-15.5B,Language,Code generation,Microsoft,"Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, Daxin Jiang",2023-06-14,WizardCoder: Empowering Code Large Language Models with Evol-Instruct,https://arxiv.org/abs/2306.08568,449.0,,"""It surpasses all other open-source Code LLMs by a substantial margin. Moreover, our model even outperforms the largest closed LLMs, Anthropicâ€™s Claude and Googleâ€™s Bard, on HumanEval and HumanEval+.""

however, worse than GPT-4 and GPT-3.5 (as noted in Figure 1) ",15500000000.0,15.5B,1.12e+23,1.12e23 base compute (StarCoder estimate) + 1.95e19 finetune compute (see below) ~= 1.12e23,Evol Instruct,"synthetic code data:

""To construct the training dataset, we initialized it with the 20K
 instruction-following dataset called Code Alpaca5. We iteratively employ the Evol-Instruct technique on this dataset consisting of 20,000 samples to produce evolved data""",,"""The evolved dataset consists of approximately 78k samples""

Not sure how big the samples are.",,,,,Likely,"Code Large Language Models (Code LLMs), such as StarCoder, have demonstrated exceptional performance in code-related tasks. However, most existing
models are solely pre-trained on extensive raw code data without instruction finetuning. In this paper, we introduce WizardCoder, which empowers Code LLMs
with complex instruction fine-tuning, by adapting the Evol-Instruct method to
the domain of code. Through comprehensive experiments on four prominent
code generation benchmarks, namely HumanEval, HumanEval+, MBPP, and DS1000, we unveil the exceptional capabilities of our model. It surpasses all other
open-source Code LLMs by a substantial margin. Moreover, our model even
outperforms the largest closed LLMs, Anthropicâ€™s Claude and Googleâ€™s Bard, on
HumanEval and HumanEval+. Our code, model weights, and data are public at
https://github.com/nlpxucan/WizardLM.",,,Open weights (restricted use),"United States of America,Multinational,India,Belgium",StarCoder,19503513600000000000,"""The StarCoder [11] serves as our basic foundation model. The evolved dataset consists of approximately 78k samples. To fine-tune the basic models, we employ specific configurations, including a
batch size of 512, a sequence length of 2048, 200 fine-tuning steps, 30 warmup steps, a learning rate
of 2e-5, a Cosine learning rate scheduler, and fp16 mixed precision.""

512*2048*200 = 209,715,200 training tokens

209715200 * 15.5B * 6 = 1.95e19",,,2025-05-16 10:30,,,,,,Industry,,,,,Open source,"commercial, responsible use restrictions: https://github.com/nlpxucan/WizardLM/blob/main/WizardCoder/MODEL_WEIGHTS_LICENSE

code is apache: https://github.com/nlpxucan/WizardLM/blob/main/WizardCoder/CODE_LICENSE
training code here: https://github.com/nlpxucan/WizardLM/blob/main/WizardCoder/src/train_wizardcoder.py 

data non-commercial: https://github.com/nlpxucan/WizardLM/blob/main/WizardCoder/DATA_LICENSE",Industry,,,,,Operation counting,,,,
BELLE-LLaMA-7B-2M-enc ,Language,Language modeling/generation,KE Holdings Inc. (â€œBeikeâ€),,2023-06-12,,https://huggingface.co/BelleGroup/BELLE-LLaMA-7B-2M-enc,,,,7000000000.0,,,,Stanford-Alpaca,,2000000.0,"I am not sure if they report tokens, words, or documents",,,,,Confident,"BELLE-LLAMA-7B-2M-enc is based on LLAMA 7B and finetuned with 2M Chinese data combined with 50,000 pieces of English data from the open source Stanford-Alpaca, resulting in good Chinese instruction understanding and response generation capabilities.

The code of Chinese data generation and other detailed information can be found in our Github project repository: https://github.com/LianjiaTech/BELLE.",3.0,,Open weights (non-commercial),China,LLaMA-7B,,,,,2024-11-27 15:26,,,,,,Industry,,,,,,"GNU General Public License v3.0 
Please refer License of Meta LLaMA Currently only for learning and communication. Please strictly observe the use restrictions of LLaMA. The LLaMA model does not allow the release of the complete model weights after adjustment, but can publish the diff of the original model. Therefore, we use XOR in the file room to ensure that talents with the authorization of the original model of LLaMA can transform the model released by this project into a usable format. ",Industry,,,,,,,,,
BELLE-LLaMA-13B-2M-enc,Language,Language modeling/generation,KE Holdings Inc. (â€œBeikeâ€),,2023-06-12,,https://huggingface.co/BelleGroup/BELLE-LLaMA-13B-2M-enc,,,,13000000000.0,,,,Stanford-Alpaca,,2000000.0,"I am not sure if they report tokens, words, or documents",,,,,Confident,"BELLE-LLAMA-13B-2M-enc is based on LLAMA 13B and finetuned with 2M Chinese data combined with 50,000 pieces of English data from the open source Stanford-Alpaca, resulting in good Chinese instruction understanding and response generation capabilities.

The code of Chinese data generation and other detailed information can be found in our Github project repository: https://github.com/LianjiaTech/BELLE.",3.0,,Open weights (non-commercial),China,LLaMA-13B,,,,,2025-05-01 10:42,,,,,,Industry,,,,,,"License: GNU General Public License v3.0 
Please refer License of Meta LLaMA Currently only for learning and communication. Please strictly observe the use restrictions of LLaMA. The LLaMA model does not allow the release of the complete model weights after adjustment, but can publish the diff of the original model. Therefore, we use XOR in the file room to ensure that talents with the authorization of the original model of LLaMA can transform the model released by this project into a usable format. ",Industry,,,,,,,,,
Wu Dao Aquila-7B,Language,"Chat,Code generation",Beijing Academy of Artificial Intelligence / BAAI,,2023-06-10,,"https://spectrum.ieee.org/china-chatgpt-wu-dao

https://huggingface.co/BAAI/Aquila-7B

https://github.com/FlagAI-Open/FlagAI/blob/master/examples/Aquila/README_en.md",,,,7000000000.0,,,,,,,,,,NVIDIA A100,,Confident,"Who said all large-language models (LLMs) necessarily need to be large? In Chinaâ€™s case, LLMs are currently downsizing in their size and number of parameters. According to sources, this is because the country is now focusing on enabling Chinese startups and smaller entities to build their own generative AI applications. As part of this downscaling trend, in June the Beijing Academy of Artificial Intelligence (BAAI) introduced Wu Dao 3.0, a series of open-source LLMs.

Based on interviews with high-ranking, anonymous sources involved in the project, IEEE Spectrum can report that Wu Dao 3.0 builds on the academyâ€™s work with Wu Dao 2.0, a sparse, multimodal generative AI modelâ€”as has been widely reported about version 2.0â€”with 1.75 trillion parameters. Although there is no single set of parameters for Wu Dao 3.0 (itâ€™s a range of models with a variety of parameter counts) all are well below the 1.75 trillion high-water mark that version 2.0 set.",,,Open weights (restricted use),China,,,,,,2025-06-12 10:54,,,,,,Academia,,,,,Open source,"Apache 2.0 for code: https://huggingface.co/BAAI/Aquila-7B
BAAI license for weights, commercial but restrictions around rights/PRC laws: https://huggingface.co/BAAI/Aquila-7B/resolve/main/BAAI%20Aquila%20Model%20License%20Agreement.pdf",Academia,,,,,,BAAI,,,
Wu Dao Aquila-33B,Language,"Chat,Code generation,Language modeling/generation,Question answering,Text summarization",Beijing Academy of Artificial Intelligence / BAAI,,2023-06-10,,"https://spectrum.ieee.org/china-chatgpt-wu-dao

https://huggingface.co/BAAI/Aquila-7B",,,,33000000000.0,33B for largest model: https://huggingface.co/BAAI/Aquila-7B,,,,,,,,,NVIDIA A100,,Confident,"Who said all large-language models (LLMs) necessarily need to be large? In Chinaâ€™s case, LLMs are currently downsizing in their size and number of parameters. According to sources, this is because the country is now focusing on enabling Chinese startups and smaller entities to build their own generative AI applications. As part of this downscaling trend, in June the Beijing Academy of Artificial Intelligence (BAAI) introduced Wu Dao 3.0, a series of open-source LLMs.

Based on interviews with high-ranking, anonymous sources involved in the project, IEEE Spectrum can report that Wu Dao 3.0 builds on the academyâ€™s work with Wu Dao 2.0, a sparse, multimodal generative AI modelâ€”as has been widely reported about version 2.0â€”with 1.75 trillion parameters. Although there is no single set of parameters for Wu Dao 3.0 (itâ€™s a range of models with a variety of parameter counts) all are well below the 1.75 trillion high-water mark that version 2.0 set.",,,Unreleased,China,,,,,,2025-06-12 11:15,,,,,,Academia,,,,,,"""coming soon"" per https://huggingface.co/BAAI/Aquila-7B. This page was published a year ago so seems unlikely it will ever be released.",Academia,,,,,,,,,
PoET,Biology,"Protein generation,Protein or nucleotide language model (pLM/nLM)",OpenProtein.ai,"Timothy F. Truong Jr, Tristan Bepler",2023-06-09,PoET: A generative model of protein families as sequences-of-sequences,https://arxiv.org/abs/2306.06156,18.0,,,57000000.0,,2.30000000000002e+20,"1. Hardware setup: 7x NVIDIA A100 GPUs, 3.12e14 FLOP/s per GPU
2. Training duration: 3 days = 259,200 seconds (directly provided)
3. Utilization rate: 40% (default assumption)
4. Calculation: 3.12e14 FLOP/s Ã— 7 GPUs Ã— 259,200s Ã— 0.4 = 2.3e20 FLOPs",,,87580000001.0,"29,000,000 sets * 10 sequences/set = 290,000,000 sequences
Tokens per sequence = 300 amino acids + 2 tokens = 302 tokens
Total tokens = 290,000,000 sequences * 302 tokens/sequence = 87,580,000,000
Final result: 8.758 Ã— 10Â¹â° datapoints",72.0,,NVIDIA A100,,Confident,"Generative protein language models are a natural way to design new proteins with desired functions. However, current models are either difficult to direct to produce a protein from a specific family of interest, or must be trained on a large multiple sequence alignment (MSA) from the specific family of interest, making them unable to benefit from transfer learning across families. To address this, we propose Protein Evolutionary Transformer (PoET), an autoregressive generative model of whole protein families that learns to generate sets of related proteins as sequences-of-sequences across tens of millions of natural protein sequence clusters. PoET can be used as a retrieval-augmented language model to generate and score arbitrary modifications conditioned on any protein family of interest, and can extrapolate from short context lengths to generalize well even for small families. This is enabled by a unique Transformer layer; we model tokens sequentially within sequences while attending between sequences order invariantly, allowing PoET to scale to context lengths beyond those used during training. In extensive experiments on deep mutational scanning datasets, we show that PoET outperforms existing protein language models and evolutionary sequence models for variant function prediction across proteins of all MSA depths. We also demonstrate PoET's ability to controllably generate new protein sequences.",,,,Singapore,,,,7.0,,2025-05-01 10:42,,,,,,Industry,,,,,,,Industry,,,,5573.738213375315,Hardware,,,,
SYNTERACT,Biology,Protein interaction prediction,University of Delaware,"Logan Hallee, Jason P. Gleghorn",2023-06-09,Protein-Protein Interaction Prediction is Achievable with Large Language Models,https://www.biorxiv.org/content/10.1101/2023.06.07.544109v1.abstract,,,,420000000.0,,,,,,180000001.0,"353,976 pairs Ã— 500 avg amino acids = 176,988,000 tokens â‰ˆ 1.8 Ã— 10â¸ tokens

{pairs calculation: 179,018 + 3,958 + 170,000 = 353,976}",,,,,Confident,"Predicting protein-protein interactions (PPIs) is vital for elucidating fundamental biology, designing peptide therapeutics, and for high-throughput protein annotation. This is particularly relevant in the current biotechnology landscape characterized by the proliferation of protein generative models, which necessitate a high-throughput and generalized PPI predictor for proteins regardless of conventional motifs or known biological functions. Our work addresses this need and provides strong evidence of the utility and reliability of protein language models (pLMs) in learning the PPI objective. We demonstrated that with the use of a sizable balanced dataset, pLMs achieve state-of-the-art performance metrics in PPI prediction on diverse proteins. To generate a dataset that allows for the approximation of these conditions, we implemented a novel synthetic data generation scheme to augment BIOGRID and Negatome datasets. The enhancement of these datasets was then used to fine-tune ProtBERT for PPI prediction to develop a model that we call SYNTERACT (SYNThetic data-driven protein-protein intERACtion Transformer). Our results are compelling, demonstrating 92% accuracy on validated positive and negative interacting pairs derived from 50 different organisms, all of which were excluded from the training phase. In addition to the high metrics, secondary analysis revealed that our synthetic negative data was able to successfully mimic actual negative samples, further reinforcing the integrity of synthetic data additions to PPI datasets. Another notable discovery was the ease in which previously existing PPI datasets could be predicted with simplistic features, calling into question if they can actually inform PPI prediction. We find that the subcellular compartment bias inherent to the compilation of these datasets is learnable with deep learning methods and demonstrate that our approach is not burdened by this disadvantage.",,,,United States of America,ProtBERT-BFD,,,,,2025-05-01 10:42,,,,70.0,,Academia,,,,,,,Academia,,,,,,,,,
MusicGen,Audio,Audio generation,Meta AI,"Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi, Alexandre DÃ©fossez",2023-06-08,Simple and Controllable Music Generation,https://arxiv.org/abs/2306.05284,240.0,SOTA improvement,"""We conduct extensive empirical evaluation, considering both automatic and human studies, showing the proposed approach is superior to the evaluated baselines on a standard text-to-music benchmark""",3359000000.0,"""We train autoregressive transformer models at different sizes: 300M, 1.5B, 3.3B parameters""

Uses EnCodec 32kHz (HF version has 59M params) for audio tokenization.",,"We train the 300M, 1.5B and 3.3B parameter models, using respectively 32, 64 and 96 GPUs, with mixed precision.

Unclear how many epochs used so FLOP calculation is not feasible.",ShutterStock and Pond5 music data collections,"""We use 20K hours of licensed music to train MUSICGEN. Specifically, we rely on an internal dataset of 10K high-quality music tracks, and on the ShutterStock and Pond5 music data collections with respectively 25K and 365K instrument-only music tracks. All datasets consist of full-length music sampled at 32 kHz with metadata composed of a textual description and additional information such as the genre, BPM, and tags.""",,"""We train on 30-second audio crops sampled at random from the full track... We use 20K hours of licensed music""

20000 hours * 60 min/hour * 2 inputs/min = 2400000 input sequences

EnCodec is run at 32kHz but after convolutions has a frame rate of 50 Hz, suggesting 2400000 * 30s * 50/s = 3,600,000,000 audio tokens.

Not confident enough in this calculation to add to database.",,,,,Likely,"We tackle the task of conditional music generation. We introduce MusicGen, a single Language Model (LM) that operates over several streams of compressed discrete music representation, i.e., tokens. Unlike prior work, MusicGen is comprised of a single-stage transformer LM together with efficient token interleaving patterns, which eliminates the need for cascading several models, e.g., hierarchically or upsampling. Following this approach, we demonstrate how MusicGen can generate high-quality samples, while being conditioned on textual description or melodic features, allowing better controls over the generated output. We conduct extensive empirical evaluation, considering both automatic and human studies, showing the proposed approach is superior to the evaluated baselines on a standard text-to-music benchmark. Through ablation studies, we shed light over the importance of each of the components comprising MusicGen. Music samples, code, and models are available at this https URL.",,,Open weights (non-commercial),United States of America,,,,,,2025-05-30 13:56,,,,,,Industry,checked,,,,Open source,"Code is released under MIT, model weights are released under CC-BY-NC 4.0

https://github.com/facebookresearch/audiocraft/blob/main/docs/MUSICGEN.md",Industry,,,FP16,,,,,,
PolySphere-1,Language,"Chat,Language modeling/generation,Japanese language modeling",AI inside,AI Inside,2023-06-08,"AI inside Establishes â€œXResearchâ€ for R&D and Social Implementation of Generative AI and LLM, Providing the Alpha Version of a 14 Billion-Parameter Japanese LLM Service","https://inside.ai/en/news/2023/06/08/aiinside-xresearch/
https://inside.ai/en/news/2023/06/29/polysphere-token/",,,,14000000000.0,"14B from https://inside.ai/en/news/2023/06/08/aiinside-xresearch/
",,,,,,,,,,,Likely,,,,Unreleased,Japan,,,,,,2025-06-09 15:00,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
RedPajama-INCITE-7B-Base,Language,Chat,Together,,2023-06-06,,https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Base,,,"one of the fine-tuned versions is Pareto SOTA for open source. wouldn't consider it SOTA overall. ""RedPajama-INCITE-7B-Instruct is the highest scoring open model on HELM benchmarks, making it ideal for a wide range of tasks. It outperforms LLaMA-7B and state-of-the-art open models such as Falcon-7B (Base and Instruct) and MPT-7B (Base and Instruct) on HELM by 2-9 points.""",6900000000.0,6.9b,4.1e+22,"Trained over 1.001 trillion tokens.
6.9b * 1 trillion * 6 = 4.1e22

",RedPajama (1T),https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T,1001000000000.0,"Authors collected 1.2 trillion token dataset, but only trained on 1.001T of them.",,,NVIDIA V100,,Confident,"We trained 3B and 7B models on the Summit supercomputer, in collaboration with AAI CERC lab at UniversitÃ© de MontrÃ©al, EleutherAI & LAION for compute time on Summit within the INCITE program award ""Scalable Foundation Models for Transferable Generalist AIâ€.

Today we are excited to release the v1 versions of the RedPajama-INCITE family of models, including instruct-tuned and chat versions under the Apache 2.0 license.",1.0,,Open weights (unrestricted),United States of America,,,,3072.0,,2025-02-14 16:21,,,,4000000.0,"""global batch size 4M tokens""",Industry,,,,,Unreleased,"Apache 2.0 for weights

data TBD: https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T",Industry,,,,1834678.6878449956,Operation counting,,,,
LTM-1,Language,Code generation,Magic,,2023-06-06,"LTM-1: an LLM with a 5,000,000 token context window",https://magic.dev/blog/ltm-1,,SOTA improvement,Very long context window - 5M tokens,,,,"Must be below 1e23 FLOP, as it's trained with a single A100.",,,,,,,,,Unknown,"Magicâ€™s LTM-1 enables 50x larger context windows than transformers
Magic's trained a Large Language Model (LLM) thatâ€™s able to take in the gigantic amounts of context when generating suggestions. For our coding assistant, this means Magic can now see your entire repository of code.",,,Unreleased,United States of America,,,,,,2025-05-30 13:25,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
GELU for CIFAR-10,Vision,Image classification,"University of California (UC) Berkeley,Toyota Technological Institute at Chicago","Dan Hendrycks, Kevin Gimpel",2023-06-06,Gaussian Error Linear Units (GELUs),https://arxiv.org/abs/1606.08415,,,,9888.0,https://docs.google.com/spreadsheets/d/1rnsk2ysbAra1UfQWD9TFDki-0T9OmKqBdd7fw6O7QWE/edit?usp=sharing,741600000000.0,6ND = 6*9888*50000*250=741600000000,CIFAR-10,,50000.0,50K - traning examples in MNIST datset,,,NVIDIA GeForce GTX TITAN X,,Speculative,"We propose the Gaussian Error Linear Unit (GELU), a high-performing neural network activation function. The GELU activation function is xÎ¦(x), where Î¦(x) the standard Gaussian cumulative distribution function. The GELU nonlinearity weights inputs by their value, rather than gates inputs by their sign as in ReLUs (x1x>0). We perform an empirical evaluation of the GELU nonlinearity against the ReLU and ELU activations and find performance improvements across all considered computer vision, natural language processing, and speech tasks.",250.0,,,"United States of America,United States of America",,,,,,2025-02-14 16:21,,,,,,"Academia,Academia",,,,,Open source,"https://github.com/hendrycks/GELUs
MIT License","Academia,Academia",,,,,Operation counting,,,,
life2vec,Medicine,Mortality prediction,"Technical University of Denmark,University of Copenhagen","Germans Savcisens, Tina Eliassi-Rad, Lars Kai Hansen, Laust Mortensen, Lau Lilleholt, Anna Rogers, Ingo Zettler, Sune Lehmann",2023-06-05,Using Sequences of Life-events to Predict Human Lives,https://arxiv.org/abs/2306.03009,,,,8400000.0,Appendix B: 8.4m,163905134400000.0,"=6*3252086*8400000=1.639051344 Ã— 10^14

I am not sure about datasize thus Speculative confidence level","Labour Market Account (AMRUN),National Patient Registry (LPR)","We work with the Labour Market Account (AMRUN) [11] and the National Patient Registry (LPR) datasets [13, 40]. Within the Labour Market Account dataset are event data for every resident of Denmark. For Danish residents who have been in contact with secondary of health care services,
primarily hospitals, the events are accounted in the National Patient Registry. We limit ourselves to data recorded in the period from 2008 until the end of 2015. Datasets are pseudonymized prior to our work by de-identifying addresses, Central Person Register numbers (CPRs), and names. Data is
stored within Statistics Denmark, and all access/use of data is logged.",3252086.0,The total number of residents in the filtered dataset is 3 252 086.,,,,,Speculative,"Over the past decade, machine learning has revolutionized computers' ability to analyze text through flexible computational models. Due to their structural similarity to written language, transformer-based architectures have also shown promise as tools to make sense of a range of multi-variate sequences from protein-structures, music, electronic health records to weather-forecasts. We can also represent human lives in a way that shares this structural similarity to language. From one perspective, lives are simply sequences of events: People are born, visit the pediatrician, start school, move to a new location, get married, and so on. Here, we exploit this similarity to adapt innovations from natural language processing to examine the evolution and predictability of human lives based on detailed event sequences. We do this by drawing on arguably the most comprehensive registry data in existence, available for an entire nation of more than six million individuals across decades. Our data include information about life-events related to health, education, occupation, income, address, and working hours, recorded with day-to-day resolution. We create embeddings of life-events in a single vector space showing that this embedding space is robust and highly structured. Our models allow us to predict diverse outcomes ranging from early mortality to personality nuances, outperforming state-of-the-art models by a wide margin. Using methods for interpreting deep learning models, we probe the algorithm to understand the factors that enable our predictions. Our framework allows researchers to identify new potential mechanisms that impact life outcomes and associated possibilities for personalized interventions.",,,Unreleased,"Denmark,Denmark",,,,,,2025-02-14 16:21,,,,,,"Academia,Academia",,,,,,License: Not for public use or distribution,"Academia,Academia",,,,,Operation counting,,,,
Polyglot-Ko-12.8B,Language,"Translation,Language modeling/generation",EleutherAI,"Hyunwoong Ko, Kichang Yang, Minho Ryu, Taekyoon Choi, Seungmu Yang, Jiwung Hyun, Sungho Park, Kyubyong Park",2023-06-04,A Technical Report for Polyglot-Ko: Open-Source Large-Scale Korean Language Models,https://arxiv.org/abs/2306.02254; https://huggingface.co/EleutherAI/polyglot-ko-12.8b,22.0,,,12898631680.0,,1.28e+22,"trained for 167 billion tokens

167b * 12.8b * 6 = 1.28e22",,"""We collaborated with TUNiB to collect a largescale Korean language dataset for our research. The dataset, totaling 1.2TB, was meticulously gathered through our collaborative efforts. Subsequently, we performed preprocessing on this dataset, resulting in 863GB of text data that served as the foundation for our analysis and model training.""",96000000000.0,"863 GB of Korean language data after processing

~111m Korean words per GB, so ~95,793,000,000 or ~96B words.
~1 token per korean word.

https://docs.google.com/document/d/1G3vvQkn4x_W71MKg0GmHVtzfd9m0y3_Ofcoew0v902Q/edit#heading=h.ieihc08p8dn0",,,NVIDIA A100,,Confident,"Polyglot is a pioneering project aimed at enhancing the non-English language performance of multilingual language models. Despite the availability of various multilingual models such as mBERT (Devlin et al., 2019), XGLM (Lin et al., 2022), and BLOOM (Scao et al., 2022), researchers and developers often resort to building monolingual models in their respective languages due to the dissatisfaction with the current multilingual models non-English language capabilities. Addressing this gap, we seek to develop advanced multilingual language models that offer improved performance in non-English languages. In this paper, we introduce the Polyglot Korean models, which represent a specific focus rather than being multilingual in nature. In collaboration with TUNiB, our team collected 1.2TB of Korean data meticulously curated for our research journey. We made a deliberate decision to prioritize the development of Korean models before venturing into multilingual models. This choice was motivated by multiple factors: firstly, the Korean models facilitated performance comparisons with existing multilingual models; and finally, they catered to the specific needs of Korean companies and researchers. This paper presents our work in developing the Polyglot Korean models, which propose some steps towards addressing the non-English language performance gap in multilingual language models.",,,Open weights (unrestricted),"Multinational,United States of America",,,,256.0,,2025-02-14 16:21,,,,554817.0,"from HuggingFace: ""Polyglot-Ko-12.8B was trained for 167 billion tokens over 301,000 steps on 256 A100 GPUs with the GPT-NeoX framework.""

from the paper: ""The overall batch size was maintained through the use of gradient accumulation steps (GAS). The model was trained for a total of 301,000 steps.""

GAS is a technique to train larger batches if you have limited memory. I don't think this text says anything in particular about whether the batch sizes changed over the course of training? 167B / 301k = 554,817",Research collective,,,,,,apache 2.0 for weights,Research collective,,,,203862.2671051655,Operation counting,,,,
Baichuan1-7B,Language,"Language modeling/generation,Question answering",Baichuan,,2023-06-01,Baichuan-7B,https://huggingface.co/baichuan-inc/Baichuan-7B,,,,7000559616.0,,5.04e+22,7b parameters * 1.2t tokens * 6 FLOP / parameter / token = 5.04e22 FLOP,,,1200000000000.0,,,,NVIDIA A800 PCIe 40 GB,,Confident,"Baichuan-7B is an open-source large-scale pre-trained model developed by Baichuan Intelligent Technology. Based on the Transformer architecture, it is a model with 7 billion parameters trained on approximately 1.2 trillion tokens. It supports both Chinese and English, with a context window length of 4096. It achieves the best performance of its size on standard Chinese and English authoritative benchmarks (C-EVAL/MMLU).",,,Open weights (non-commercial),China,,,,1024.0,0.5772,2025-06-11 17:43,,,,,,Industry,,,,,Open source,"https://huggingface.co/baichuan-inc/Baichuan-7B
no clear license

https://github.com/baichuan-inc/Baichuan-7B",Industry,,"""By integrating these strategies, our system is
capable of training Baichuan 2-7B and Baichuan
2-13B models efficiently on 1,024 NVIDIA A800
GPUs, achieving a computational efficiency that
exceeds 180 TFLOPS.""

180 TFLOPS / 311.84 TFLOPS = 57.7%",,509689.7180341823,Operation counting,baichuan-inc,,,
EGNN,Biology,Protein stability prediction,InstaDeep,"Sebastien Boyer, Sam Money-Kyrle, Oliver Bent",2023-05-30,"Predicting protein stability changes under  
multiple amino acid substitutions using equivariant graph neural networks",https://arxiv.org/abs/2305.19801,4.0,,,,,,,,,,600000 datapoints,,,,,Unknown,,,,,United Kingdom of Great Britain and Northern Ireland,,,,,,2025-05-01 10:42,,,,,,Industry,,,,,,,Industry,,,,,,,,,
PaLI-X,"Multimodal,Language,Vision,Video","Image captioning,Video description,Character recognition",Google Research,"Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa, Soravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz, Sebastian Goodman, Xiao Wang, Yi Tay, Siamak Shakeri, Mostafa Dehghani, Daniel Salz, Mario Lucic, Michael Tschannen, Arsha Nagrani, Hexiang Hu, Mandar Joshi, Bo Pang, Ceslee Montgomery, Paulina Pietrzyk, Marvin Ritter, AJ Piergiovanni, Matthias Minderer, Filip Pavetic, Austin Waters, Gang Li, Ibrahim Alabdulmohsin, Lucas Beyer, Julien Amelot, Kenton Lee, Andreas Peter Steiner, Yang Li, Daniel Keysers, Anurag Arnab, Yuanzhong Xu, Keran Rong, Alexander Kolesnikov, Mojtaba Seyedhosseini, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, Radu Soricut",2023-05-29,PaLI-X: On Scaling up a Multilingual Vision and Language Model,https://arxiv.org/abs/2305.18565,155.0,SOTA improvement,"""PaLI-X advances the state-of-the-art on most vision-and-language benchmarks considered (25+ of them).""",55000000000.0,55B (table 1),,,WebLI,"""The main pretraining data for our model is based on WebLI [5], consisting of roughly one billion images with alt-texts from the web and OCR annotations (using the GCP Vision API), covering over 100 languages. In addition to WebLI âŸ¨image, textâŸ© pairs, we introduce here Episodic WebLI data, where each episode corresponds to a set of such pairs. We aim to have each episode contain loosely related images (i.e., they are clustered according to their URL field), so as to encourage attention among examples in an â€œepisodeâ€. We find this new dataset (with 75M episodes and around 400M images in total) important for developing the few-shot capabilities of the model.""",1400000000.0,"1 billion images with alt texts in WebLI, 400m images in Episodic WebLI data",,,,,Likely,"We present the training recipe and results of scaling up PaLI-X, a multilingual vision and language model, both in terms of size of the components and the breadth of its training task mixture. Our model achieves new levels of performance on a wide-range of varied and complex tasks, including multiple image-based captioning and question-answering tasks, image-based document understanding and few-shot (in-context) learning, as well as object detection, video question answering, and video captioning. PaLI-X advances the state-of-the-art on most vision-and-language benchmarks considered (25+ of them). Finally, we observe emerging capabilities, such as complex counting and multilingual object detection, tasks that are not explicitly in the training mix.",,,Unreleased,"Multinational,United States of America,Canada,Switzerland",UL2,,,,,2025-05-30 12:50,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
HuatuoGPT,Language,"Language modeling/generation,Question answering,Chat,Medical diagnosis","Shenzhen Research Institue of Big Data,Chinese University of Hong Kong (CUHK)","Hongbo Zhang, Junying Chen, Feng Jiang, Fei Yu, Zhihong Chen, Jianquan Li, Guiming Chen, Xiangbo Wu, Zhiyi Zhang, Qingying Xiao, Xiang Wan, Benyou Wang, Haizhou Li",2023-05-24,"HuatuoGPT, towards Taming Language Model to Be a Doctor",https://arxiv.org/abs/2305.15075,,,,13000000000.0,HuatuoGPT-13B is trained on Ziya-LLaMA-13B-Pretrain-v1,,,HuatuoGPT-sft-data-v1,"To leverage the best of both distilled data (from ChatGPT) and real-world data (from Doctors), we firstly fine-tune HuatuoGPT using four types of data:
Distilled Instructions from ChatGPT	61,400
Real-world Instructions from Doctors	69,768
Distilled Conversations from ChatGPT	68,888
Real-world Conversations with Doctors	25,986 
(scale is undefined)",55611000.0,"167000000 (mln of  Mandarin Chinese words per GB) *0.333 GB = 55611000

The dataset weoght at HuggingFace: https://huggingface.co/datasets/FreedomIntelligence/HuatuoGPT-sft-data-v1/blob/main/HuatuoGPT_sft_data_v1.jsonl",,,,,Speculative,"In this paper, we present HuatuoGPT, a large language model (LLM) for medical consultation. The core recipe of HuatuoGPT is to leverage both \textit{distilled data from ChatGPT} and \textit{real-world data from doctors} in the supervised fine-tuned stage. The responses of ChatGPT are usually detailed, well-presented and informative while it cannot perform like a doctor in many aspects, e.g. for integrative diagnosis. We argue that real-world data from doctors would be complementary to distilled data in the sense the former could tame a distilled language model to perform like doctors. To better leverage the strengths of both data, we train a reward model to align the language model with the merits that both data bring, following an RLAIF (reinforced learning from AI feedback) fashion. To evaluate and benchmark the models, we propose a comprehensive evaluation scheme (including automatic and manual metrics). Experimental results demonstrate that HuatuoGPT achieves state-of-the-art results in performing medical consultation among open-source LLMs in GPT-4 evaluation, human evaluation, and medical benchmark datasets. It is worth noting that by using additional real-world data and RLAIF, the distilled language model (i.e., HuatuoGPT) outperforms its teacher model ChatGPT in most cases. Our code, data, and models are publicly available at \url{this https URL}. The online demo is available at \url{this https URL}.",,,Open weights (unrestricted),"China,Hong Kong,China",LLaMA-13B,4337658000000000000,55611000*13000000000*6,,,2024-12-02 10:25,,,,,,"Academia,Academia",,,,,Open source,"https://github.com/FreedomIntelligence/HuatuoGPT

Apache 2.0 license","Academia,Academia",,,,,Operation counting,,,,
Goat-7B,Language,Quantitative reasoning,National University of Singapore,"Tiedong Liu, Bryan Kian Hsiang Low",2023-05-23,Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks,https://arxiv.org/abs/2305.14201,64.0,SOTA improvement,"""We introduce Goat, a fine-tuned LLaMA model that significantly outperforms GPT-4 on a range of arithmetic tasks. Fine-tuned on a synthetically generated dataset, Goat achieves state-ofthe-art performance on BIG-bench arithmetic sub-task.""",7000000000.0,7B,,2.78e+22 for base LLaMA-7B,,"Model was fine-tuned from LLaMA-7B.

Fine-tuning dataset is a synthetic math dataset:

""We generate the dataset synthetically using a Python script. The dataset consists of around 1 million question-answer pairs. The answer contains
the proposed CoT as well as the final numerical output. The numbers are randomly generated, hence
ensuring a very low probability of instances being
duplicated, although small numbers may be sampled multiple times. We sample from log space to
ensure the numbers are equally likely to be sampled
from different orders of magnitude, which is similar to the sampling method used by Lee and Kim
(2023). The details of the dataset are presented in
Appendix F.""",,Fine-tune dataset had 1 million question-answer pairs. likely ~10 tokens per pair?,,,NVIDIA A10 PCIe,,Speculative,"We introduce Goat, a fine-tuned LLaMA model that significantly outperforms GPT-4 on a range of arithmetic tasks. Fine-tuned on a synthetically generated dataset, Goat achieves state-of-the-art performance on BIG-bench arithmetic sub-task. In particular, the zero-shot Goat-7B matches or even surpasses the accuracy achieved by the few-shot PaLM-540B. Surprisingly, Goat can achieve near-perfect accuracy on large-number addition and subtraction through supervised fine-tuning only, which is almost impossible with previous pretrained language models, such as Bloom, OPT, GPT-NeoX, etc. We attribute Goat's exceptional performance to LLaMA's consistent tokenization of numbers. To tackle more challenging tasks like large-number multiplication and division, we propose an approach that classifies tasks based on their learnability, and subsequently decomposes unlearnable tasks, such as multi-digit multiplication and division, into a series of learnable tasks by leveraging basic arithmetic principles. We thoroughly examine the performance of our model, offering a comprehensive evaluation of the effectiveness of our proposed decomposition steps. Additionally, Goat-7B can be easily trained using LoRA on a 24GB VRAM GPU, facilitating reproducibility for other researchers. We release our model, dataset, and the Python script for dataset generation.",1.0,,Open weights (non-commercial),Singapore,LLaMA-7B,2020000000000000000,"""Goat-7B can be easily fine-tuned using LoRA on a 24GB VRAM GPU... In particular, the fine-tuning process for a specific arithmetic sub-task, such as 8-digit addition using 100K instances, takes only approximately 1.5 hours on an A10 GPU to achieve near-perfect accuracy""

Info isn't very complete - no timeframe specified for the VRAM GPU, I'm not sure how many tokens are in the fine-tune dataset and they use LoRA. Maybe it's 15 A10-hours total (1M total instances)? But safe to assume it's a small fraction of Llama's pretrain compute.

125 trillion (A10 FLOPs) * 15 * 3600 * 0.3 = 2.02e18",,,2025-05-28 16:05,,,,,,Academia,,,,,Open (non-commercial),"no license noted. perhaps LLaMA 1 license by default (non-comm). repo with finetune (i.e. training since this is a Llama finetune) code
https://github.com/liutiedong/goat",Academia,,,FP16,,,,,,
Guanaco-65B,Language,Chat,University of Washington,"Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer",2023-05-23,QLoRA: Efficient Finetuning of Quantized LLMs,https://arxiv.org/abs/2305.14314; https://github.com/artidoro/qlora,1578.0,,"""Our best model family, which we name Guanaco, outperforms
all previous openly released models on the Vicuna benchmark, reaching 99.3%
of the performance level of ChatGPT while only requiring 24 hours of finetuning
on a single GPU""",65000000000.0,"from Llama-65B 
(also 33B, 13B, 7B variants)",5.5e+23,"Fine-tune of Llama-65B, which appears to have been trained on a ""professional grade GPU"" with 48GB VRAM (likely A6000) for 24 hours.

Fine-tune compute is negligible compared to pretraining (5.5e23 for Llama-65b)",,Fine-tuned on instruction datasets such as GLUE and Super-NaturalInstructions,,,24.0,24 hours,,,Confident,"We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimziers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training.",,,Open weights (non-commercial),United States of America,LLaMA-65B,8000000000000000000,"""using a single professional GPU over 24 hours we achieve 99.3% with our largest model""

no model specified, but if it's an A100, 312 tflop/s * 24 * 3600 * 0.3 utilization = 8e18",,,2025-05-16 10:30,,,,,,Academia,,,,,Open source,"LLaMA license, non-commercial for weights. code is MIT

code: https://github.com/artidoro/qlora/blob/main/scripts/finetune_guanaco_65b.sh ",Academia,,,,,Hardware,,,,
BiomedGPT (182M),"Language,Vision,Medicine","Visual question answering,Medical diagnosis,Image captioning,Image classification,Text summarization,Language modeling/generation,Mortality prediction","Lehigh University,University of Georgia,Samsung Research America,Harvard Medical School,University of Pennsylvania","Kai Zhang, Rong Zhou, Eashan Adhikarla, Zhiling Yan, Yixin Liu, Jun Yu, Zhengliang Liu, Xun Chen, Brian D. Davison, Hui Ren, Jing Huang, Chen Chen, Yuyin Zhou, Sunyang Fu, Wei Liu, Tianming Liu, Xiang Li, Yong Chen, Lifang He, James Zou, Quanzheng Li, Hongfang Liu, Lichao Sun",2023-05-23,BiomedGPT: A Generalist Vision-Language Foundation Model for Diverse Biomedical Tasks,https://arxiv.org/abs/2305.17100,,,,182000000.0,,,"""For pre-training, we used ten 24-GB NVIDIA A5000 GPUs configured for multi-GPU training using DistributedDataParallel (DDP)""",,,,"""BiomedGPT, we curated a large-scale pre-training corpus comprising 592,567 images, approximately 183 million text sentences, 46,408 object-label pairs, and 271,804 image-text pairs """,,,NVIDIA RTX A5000,,Confident,"Traditional biomedical artificial intelligence (AI) models, designed for specific tasks or modalities, often exhibit limited flexibility in real-world deployment and struggle to utilize holistic information. Generalist AI holds the potential to address these limitations due to its versatility in interpreting different data types and generating tailored outputs for diverse needs. However, existing biomedical generalist AI solutions are typically heavyweight and closed source to researchers, practitioners, and patients. Here, we propose BiomedGPT, the first open-source and lightweight vision-language foundation model, designed as a generalist capable of performing various biomedical tasks. BiomedGPT achieved state-of-the-art results in 16 out of 25 experiments while maintaining a computing-friendly model scale. We also conducted human evaluations to assess the capabilities of BiomedGPT in radiology visual question answering, report generation, and summarization. BiomedGPT exhibits robust prediction ability with a low error rate of 3.8% in question answering, satisfactory performance with an error rate of 8.3% in writing complex radiology reports, and competitive summarization ability with a nearly equivalent preference score to human experts. Our method demonstrates that effective training with diverse data can lead to more practical biomedical AI for improving diagnosis and workflow efficiency.",,,,"United States of America,United States of America,United States of America,United States of America,United States of America",,,,10.0,,2025-05-01 10:42,,,,,,"Academia,Academia,Industry,Academia,Academia",,,,,,,"Academia,Academia,Industry,Academia,Academia",,,,4580.161444318493,,,,,
RWKV-4 14B,Language,Language modeling,RWKV Foundation,"Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, Xuzheng He, Haowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bartlomiej Koptyra, Hayden Lau, Krishna Sri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Xiangru Tang, Bolun Wang, Johan S. Wind, Stansilaw Wozniak, Ruichong Zhang, Zhenyuan Zhang, Qihang Zhao, Peng Zhou, Jian Zhu, Rui-Jie Zhu",2023-05-22,RWKV: Reinventing RNNs for the Transformer Era,https://arxiv.org/abs/2305.13048,356.0,,,14000000000.0,14b,2.78e+22,"from HuggingFace page: https://huggingface.co/BlinkDL/rwkv-4-pile-14b

trained for 330B tokens
14 billion * 330 billion * 6 = 2.78e22

paper notes that a forward pass is almost exactly 2x parameters (within 2%): ""Alternative approximations for FLOPs include doubling the parameters which yields similar results within 2% for 14B and a 30% discrepancy for 169M variant."" and that 6*params*tokens is a good approximation because it's not a transformer: ""FLOPs is for a forward pass for one token. It was calculated as 6(V D + 13D2L), which is the
twice (add and multiply) the number of parameters
in linear layers. The backwards pass FLOPs can be
approximated as twice that of the forward pass. So
the total is 6(V D + 13D2L) per token for training
(3x fw FLOPs). It is noteworthy that FLOPs are
independent of the context length, unlike regular
transformers""",The Pile,,330000000000.0,,,,NVIDIA A100 SXM4 80 GB,,Confident,"Transformers have revolutionized almost all natural language processing (NLP) tasks but suffer from memory and computational complexity that scales quadratically with sequence length. In contrast, recurrent neural networks (RNNs) exhibit linear scaling in memory and computational requirements but struggle to match the same performance as Transformers due to limitations in parallelization and scalability. We propose a novel model architecture, Receptance Weighted Key Value (RWKV), that combines the efficient parallelizable training of Transformers with the efficient inference of RNNs. Our approach leverages a linear attention mechanism and allows us to formulate the model as either a Transformer or an RNN, which parallelizes computations during training and maintains constant computational and memory complexity during inference, leading to the first non-transformer architecture to be scaled to tens of billions of parameters. Our experiments reveal that RWKV performs on par with similarly sized Transformers, suggesting that future work can leverage this architecture to create more efficient models. This work presents a significant step towards reconciling the trade-offs between computational efficiency and model performance in sequence processing tasks.",,,,Multinational,,,,,,2024-11-25 14:57,,,,262144.0,"262144 (or 131072?)
""To train the models mentioned, we... switch batch size dynamically between 128 or 256 sequences, each of 1024 tokens""",Research collective,,,,,,,Research collective,,,,,Operation counting,,,,
MMS-1B,Speech,Speech recognition,Meta AI,"Vineel Pratap, Andros Tjandra, Bowen Shi, Paden Tomasello, Arun Babu, Sayani Kundu, Ali Elkahky, Zhaoheng Ni, Apoorv Vyas, Maryam Fazel-Zarandi, Alexei Baevski, Yossi Adi, Xiaohui Zhang, Wei-Ning Hsu, Alexis Conneau, Michael Auli",2023-05-22,"Scaling Speech Technology to 1,000+ Languages","https://arxiv.org/abs/2305.13516
https://huggingface.co/facebook/mms-1b-all",231.0,,,1000000000.0,https://huggingface.co/facebook/mms-1b-all/tree/main,,"from page 13 ""all models were
pre-trained for a total of one million updates on A100 GPUs with 80GB of memory. The MMS (0.3B)
model was trained with an effective batch size of 2.3 hours of data across 48 GPUs and the MMS
(1B) model was trained with an effective batch size of 3.5 hours on 64 GPUs""
","Common Voice,LibriSpeech","from page 13 ""Data. The pre-training data covers about 491K hours in 1,406 languages. This data is drawn from
six training corpora with different characteristics, including the corpora used in XLS-R [Babu et al.,
2022]:
â€¢ MMS-lab-U: 1,362 languages comprising 55K hours (Â§3.1).
â€¢ Multilingual Librispech (MLS): 8 European languages of read books totaling 50K
hours [Pratap et al., 2020c]
â€¢ CommonVoice (CV): 89 languages totaling 8.8 hours of read Wikipedia text; we use v9.0 of
the corpus [Ardila et al., 2020])
â€¢ VoxLingua-107 (VL): 107 languages totaling 5.3K hours of YouTube content [Valk and
AlumÃ¤e, 2020]
â€¢ BABEL (BBL): 17 African and Asian languages totaling about 1K hours of conversational
telephone data [Gales et al., 2014]
â€¢ VoxPopuli (VP): 371K hours of unlabeled speech data in 23 languages derived from Euro-
pean Parliament event recordings [Wang et al., 2021]""",7856000000.0,"491K hours in mono 16 kHz = 7,856,000,000 samples",,,NVIDIA A100 SXM4 80 GB,,Confident,"Expanding the language coverage of speech technology has the potential to improve access to information for many more people. However, current speech technology is restricted to about one hundred languages which is a small fraction of the over 7,000 languages spoken around the world. The Massively Multilingual Speech (MMS) project increases the number of supported languages by 10-40x, depending on the task. The main ingredients are a new dataset based on readings of publicly available religious texts and effectively leveraging self-supervised learning. We built pre-trained wav2vec 2.0 models covering 1,406 languages, a single multilingual automatic speech recognition model for 1,107 languages, speech synthesis models for the same number of languages, as well as a language identification model for 4,017 languages. Experiments show that our multilingual speech recognition model more than halves the word error rate of Whisper on 54 languages of the FLEURS benchmark while being trained on a small fraction of the labeled data. ",7.13,,Open weights (non-commercial),United States of America,,,,64.0,,2024-11-01 10:05,,,,,,Industry,,,,,Open (non-commercial),CC-BY-NC 4.0 (non commercial),Industry,,,,50980.3235369173,,,,,
XuanYuan 2.0,Language,"Language modeling/generation,Chat",Du Xiaoman,"Xuanyu Zhang, Qing Yang, Dongliang Xu",2023-05-19,XuanYuan 2.0: A Large Chinese Financial Chat Model with Hundreds of Billions Parameters,"https://arxiv.org/abs/2305.12002, https://huggingface.co/Duxiaoman-DI/XuanYuan-176B",64.0,,,176200000000.0,176.2B,,Fine-tuned from BLOOM-176B. More details in fine-tuning column,,"Fine-tuned on mix of financial data and general data, including instruction data (figure 1)",366000000000.0,Table 2,,,NVIDIA A100 SXM4 80 GB,,Confident,"In recent years, pre-trained language models have undergone rapid development with the emergence of large-scale models. However, there is a lack of open-sourced chat models specifically designed for the Chinese language, especially in the field of Chinese finance, at the scale of hundreds of billions. To address this gap, we introduce XuanYuan 2.0, the largest Chinese chat model to date, built upon the BLOOM-176B architecture. Additionally, we propose a novel training method called hybrid-tuning to mitigate catastrophic forgetting. By combining general-domain with domain-specific knowledge and integrating the stages of pre-training and fine-tuning, XuanYuan 2.0 is capable of providing accurate and contextually appropriate responses in the Chinese financial domain.",,,Open weights (restricted use),China,BLOOM-176B,1,"13B tokens for fine-tuning, per table 2

176B * 13B * 6 = 1.37e22",,,2024-11-01 10:05,,,,,,Industry,,,,,,looks like commercial + responsible use: https://bigscience.huggingface.co/blog/the-bigscience-rail-license,Industry,,,,,,,,,
ONE-PEACE,"Multimodal,Vision,Speech,Language","Image classification,Speech recognition,Audio question answering,Audio classification","Alibaba,Huazhong University of Science and Technology","Peng Wang, Shijie Wang, Junyang Lin, Shuai Bai, Xiaohuan Zhou, Jingren Zhou, Xinggang Wang, Chang Zhou",2023-05-18,ONE-PEACE: Exploring One General Representation Model Toward Unlimited Modalities,https://arxiv.org/abs/2305.11172v1,76.0,SOTA improvement,""" ONEPEACE achieves leading results in both uni-modal and multi-modal tasks, including image classification (89.8%
accuracy on ImageNet w/o privately labeled data), semantic segmentation (63.0% mIoU on ADE20K), audio-text
retrieval (outperforming previous SOTAs on AudioCaps and Clotho by a large margin), audio classification (91.8%
zero-shot accuracy on ESC-50, 69.7% accuracy on FSD50K, 59.6% accuracy on VGGSound w/o visual information),
audio question answering (86.2% accuracy on AVQA w/o visual information), image-text retrieval (84.1% I2T R@1
on MSCOCO and 97.6% I2T R@1 on Flickr30K w/o intermediate finetuning and ranking), and visual grounding
(89.26%/83.23%/89.27% scores on RefCOCO/+/g test sets).""",4000000000.0,"""we propose ONE-PEACE, a model with 4B parameters""",1.8e+20,"4 billion params * 7.5 billion data * 6 = 1.8e20.

see training dataset size notes. this estimate required some more assumptions than usual.","LAION-2B,LAION-Audio-630K","""For image-text pairs, we use LAION-2B... For audio-text pairs, we mainly use the environmental sound datasets processed by [76].""

looks like there's additional fine-tuning data as well",1600000000.0,"""After these steps, we retain about 1.5 billion image-text pairs""
...
""We also perform simple cleaning on the data, which involves removing samples with text lengths less than 3 or greater than
512, as well as texts containing non-English or emoji characters. Ultimately, we obtain about 2.4 million audio-text pairs, with a total duration of around 8,000 hours""

8000 hours = 480,000 minutes = ~109,440,000 words at 228 wpm

https://docs.google.com/document/d/1G3vvQkn4x_W71MKg0GmHVtzfd9m0y3_Ofcoew0v902Q/edit#heading=h.3pbt0hfgv7pq

Trained on 10 epochs for audio. For text, they train on ""200K steps with a batch size of 32768"" = 6,533,600,000
Adding together, they train on ~ 7.5b data points on a dataset of 1.6b, for ~4.7 epochs on average.",,,,,Speculative,"In this work, we explore a scalable way for building a general representation model toward unlimited modalities. We release ONE-PEACE, a highly extensible model with 4B parameters that can seamlessly align and integrate representations across vision, audio, and language modalities. The architecture of ONE-PEACE comprises modality adapters, shared self-attention layers, and modality FFNs. This design allows for the easy extension of new modalities by adding adapters and FFNs, while also enabling multi-modal fusion through self-attention layers. To pretrain ONE-PEACE, we develop two modality-agnostic pretraining tasks, cross-modal aligning contrast and intra-modal denoising contrast, which align the semantic space of different modalities and capture fine-grained details within modalities concurrently. With the scaling-friendly architecture and pretraining tasks, ONE-PEACE has the potential to expand to unlimited modalities. Without using any vision or language pretrained model for initialization, ONE-PEACE achieves leading results on a wide range of uni-modal and multi-modal tasks, including image classification (ImageNet), semantic segmentation (ADE20K), audio-text retrieval (AudioCaps, Clotho), audio classification (ESC-50, FSD50K, VGGSound), audio question answering (AVQA), image-text retrieval (MSCOCO, Flickr30K), and visual grounding (RefCOCO/+/g). Code is available at this https URL.",4.7,,Open weights (unrestricted),"China,China",,,,,,2025-05-28 16:05,,,,,,"Industry,Academia",,,,,Open source,"Apache 2.0, includes train code
https://github.com/OFA-Sys/ONE-PEACE/tree/main/one_peace ","Industry,Academia",,,BF16,,Operation counting,,,,
LIMA,Language,"Chat,Language modeling/generation","Meta AI,Carnegie Mellon University (CMU),University of Southern California,Tel Aviv University","Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, Omer Levy",2023-05-18,LIMA: Less Is More for Alignment,https://arxiv.org/abs/2305.11206,1081.0,Highly cited,"Citation count obtained from a Google search for ""LIMA: Less Is More for Alignment"" (https://www.google.com/search?q=LIMA%3A+Less+Is+More+for+Alignment&rlz=1C5CHFA_enUS627US627&oq=LIMA%3A+Less+Is+More+for+Alignment&gs_lcrp=EgZjaHJvbWUqBggAEEUYOzIGCAAQRRg7MgYIARBFGDoyBggCEEUYPdIBBzQ4NWowajeoAgCwAgA&sourceid=chrome&ie=UTF-8).",65000000000.0,"""We train LIMA (Less Is More for Alignment) using the following protocol. Starting from LLaMa 65B [Touvron et al., 2023], we fine-tune on our 1,000-example alignment training set,"" according to page 4 of https://arxiv.org/pdf/2305.11206.",5.5000439e+23,"Finetune:  4.39e18 FLOP
Base model: 5.5e+23 FLOP 
Total: 4.39e18+5.5e+23=5.5e+23 FLOP","Stack Exchange,wikiHow,Pushshift Reddit","
",750000.0,"The total amount of training data is roughly 750,000 tokens, split over exactly 1,000 sequences.",,,,,Confident,"Large language models are trained in two stages: (1) unsupervised pretraining from raw text, to learn general-purpose representations, and (2) large scale instruction tuning and reinforcement learning, to better align to end tasks and user preferences. We measure the relative importance of these two stages by training LIMA, a 65B parameter LLaMa language model fine-tuned with the standard supervised loss on only 1,000 carefully curated prompts and responses, without any reinforcement learning or human preference modeling. LIMA demonstrates remarkably strong performance, learning to follow specific response formats from only a handful of examples in the training data, including complex queries that range from planning trip itineraries to speculating about alternate history. Moreover, the model tends to generalize well to unseen tasks that did not appear in the training data. In a controlled human study, responses from LIMA are either equivalent or strictly preferred to GPT-4 in 43% of cases; this statistic is as high as 58% when compared to Bard and 65% versus DaVinci003, which was trained with human feedback. Taken together, these results strongly suggest that almost all knowledge in large language models is learned during pretraining, and only limited instruction tuning data is necessary to teach models to produce high quality output.",15.0,,Unreleased,"United States of America,United States of America,United States of America,Israel",LLaMA-65B,439000000000000000000,"â€œThe total amount of training data is roughly 750,000 tokens, split over exactly 1,000 sequences,â€ according to page 2 of https://arxiv.org/pdf/2305.11206.

â€œWe train LIMA (Less Is More for Alignment) using the following protocol. Starting from LLaMa 65B [Touvron et al., 2023], we fine-tune on our 1,000-example alignment training set. To differentiate between each speaker (user and assistant), we introduce a special end-of-turn token (EOT) at the end of each utterance; this token plays the same role as EOS of halting generation, but avoids conflation with any other meaning that the pretrained model may have imbued into the preexisting EOS token.

We follow standard fine-tuning hyperparameters: we fine-tune for 15 epochs using AdamW [Loshchilov and Hutter, 2017] with 1 = 0.9, 2 = 0.95, and weight decay of 0.1. Without warmup steps, we set the initial learning rate to 1 âˆ’ 5 and linearly decaying to 1 âˆ’ 6 by the end of training. The batch size is set to 32 examples (64 for smaller models), and texts longer than 2048 tokens are trimmed. One notable deviation from the norm is the use of residual dropout;
we follow Ouyang et al. [2022] and apply dropout over residual connections, starting at  = 0.0 at the bottom layer and linearly raising the rate to  = 0.3 at the last layer ( = 0.2 for smaller models). We find that perplexity does not correlate with generation quality, and thus manually select checkpoints between the 5th and the 10th epochs using the held-out 50-example development set,"" according to page 4 of https://arxiv.org/pdf/2305.11206.

Since Llama has a dense transformer architecture, the 6ND approximation yields
Fine-tuning compute
= # of active parameters / forward pass * # of tokens * 6 FLOPS / token * # of epochs
~= 65e9 parameters * 75e4 tokens * 6 FLOPS / token * 15 epochs
= 438,750e13 FLOPS
~= 4.39e18 FLOPS",,,2025-05-23 13:39,,,,,,"Industry,Academia,Academia,Academia",,,,,Unreleased,,"Industry,Academia,Academia,Academia",,,,,Operation counting,,,,
CoEdiT-xxl,Language,Language generation,"University of Minnesota,Grammarly","Vipul Raheja, Dhruv Kumar, Ryan Koo, Dongyeop Kang",2023-05-17,CoEdIT: Text Editing by Task-Specific Instruction Tuning,"https://arxiv.org/abs/2305.09857, https://huggingface.co/grammarly/coedit-large",37.0,SOTA improvement,"""We achieve state-of-the-art performance on multiple text editing tasks: grammatical error correction, text simplification, sentence fusion, iterative text editing, and three stylistic editing
tasks (formality style transfer, neutralization,
and paraphrasing).""",11000000000.0,11B,,finetuned from Flan-T5,,"82k pairs of editing examples:

""we fine-tune a pre-trained
sequence-to-sequence model on a parallel corpus
of instruction-based 82K input-output pairs. The
inputs and outputs are sourced from publicly available corpora for different text editing tasks""

""Our dataset creation is based on the ITERATER+
dataset proposed by Kim et al. (2022) who combined datasets from various text editing tasks (See
Table 1). Their work, in turn, is based on Du et al (2022b), who categorized each edit into MEANINGCHANGED or NON-MEANING-CHANGED.""",3000000.0,"82k pairs of sentences. Roughly 20 words per sentence based on examples but mean length could be higher due to outliers.
40*82k = ~3,000,000",,,NVIDIA A100,,Likely,"We introduce COEDIT, a state-of-the-art text editing system for writing assistance. COEDIT takes instructions from the user specifying the attributes of the desired text, such as ""Make the sentence simpler"" or ""Write it in a more neutral style,"" and outputs the edited text. We present a large language model fine-tuned on a diverse collection of task-specific instructions for text editing (a total of 82K instructions). Our model (1) achieves state-of-the-art performance on various text editing benchmarks, (2) is competitive with publicly available largestsized LLMs trained on instructions while being âˆ¼60x smaller, (3) is capable of generalizing to unseen edit instructions, and (4) exhibits abilities to generalize to composite instructions containing different combinations of edit actions. Through extensive qualitative and quantitative analysis, we show that writers prefer the edits suggested by COEDIT, relative to other stateof-the-art text editing models1.",5.0,,Open weights (non-commercial),"United States of America,United States of America",Flan-T5 11B,1000000000000000000,"""We fine-tune different versions of pre-trained FLANT5 (Chung et al., 2022a) models on the COEDIT dataset. Specifically, we use FLANT5-L (770M parameters), FLANT5-XL (3B parameters), FLANT5-XXL (11B parameters) models.""

""Each model is trained for 5 epochs with early stopping. All models were fine-tuned on A100 GPUs using Deepspeed""

6 * 5 epochs * 3 million words (rough estimate) * 11 billion = 9.9e17 ~= 1e18

",,,2025-05-28 16:05,,,,,,"Academia,Industry",,,,,Open (non-commercial),"cc-by-nc (non commercial) for weights: https://huggingface.co/grammarly/coedit-large
training code/data here with unclear licenses: https://github.com/vipulraheja/coedit","Academia,Industry",,,FP16,,,,,,
WeLM,Language,"Language modeling/generation,Translation",WeChat AI,"Hui Su, Xiao Zhou, Houjin Yu, Xiaoyu Shen, Yuwen Chen, Zilin Zhu, Yang Yu, Jie Zhou",2023-05-16,WeLM: A Well-Read Pre-trained Language Model for Chinese,https://arxiv.org/abs/2209.10372 ,21.0,,,10000000000.0,"""WeLM is trained with 10B parametersâ€",2.484338688e+22,"""The largest model is trained on 128 A100-SXM4-40GB GPUs in about 24 daysâ€, ""All models are trained with FP16 mixed precision.""
3.12e14 * 128 * 24 * 24 * 3600 * 0.3 = 2.48e22
(FLOP/GPU-s) * (GPU) * (days) * (h/day) * (sec/h) * (utilization assumption)",,,262000000000.0,"from the paper ""After all the above filtering process, our corpus contains 262B tokensâ€",576.0,"""The largest model is trained on 128 A100-SXM4-40GB GPUs in about 24 daysâ€, ",NVIDIA A100 SXM4 40 GB,Unsupervised,Confident,"Large Language Models pre-trained with self-supervised learning have demonstrated impressive zero-shot generalization capabilities on a wide spectrum of tasks. In this work, we present WeLM: a well-read pre-trained language model for Chinese that is able to seamlessly perform different types of tasks with zero or few-shot demonstrations. WeLM is trained with 10B parameters by ""reading"" a curated high-quality corpus covering a wide range of topics. We show that WeLM is equipped with broad knowledge on various domains and languages. On 18 monolingual (Chinese) tasks, WeLM can significantly outperform existing pre-trained models with similar sizes and match the performance of models up to 25 times larger. WeLM also exhibits strong capabilities in multi-lingual and code-switching understanding, outperforming existing multilingual language models pre-trained on 30 languages. Furthermore, We collected human-written prompts for a large set of supervised datasets in Chinese and fine-tuned WeLM with multi-prompted training. The resulting model can attain strong generalization on unseen types of tasks and outperform the unsupervised WeLM in zero-shot learning. Finally, we demonstrate that WeLM has basic skills at explaining and calibrating the decisions from itself, which can be promising directions for future research. Our models can be applied from this https://welm.weixin.qq.com/docs/api/",,,,China,,,,128.0,,2025-05-09 11:32,,,,,,Industry,,,,73728.0,,,Industry,,,,101974.27158107296,Hardware,,,,
Med-PaLM 2,"Medicine,Language",Question answering,"Google Research,DeepMind","Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou, Kevin Clark, Stephen Pfohl, Heather Cole-Lewis, Darlene Neal, Mike Schaekermann, Amy Wang, Mohamed Amin, Sami Lachgar, Philip Mansfield, Sushant Prakash, Bradley Green, Ewa Dominowska, Blaise Aguera y Arcas, Nenad Tomasev, Yun Liu, Renee Wong, Christopher Semturs, S. Sara Mahdavi, Joelle Barral, Dale Webster, Greg S. Corrado, Yossi Matias, Shekoofeh Azizi, Alan Karthikesalingam, Vivek Natarajan",2023-05-16,Towards Expert-Level Medical Question Answering with Large Language Models,https://arxiv.org/abs/2305.09617,,SOTA improvement,https://paperswithcode.com/sota/question-answering-on-medqa-usmle ,340000000000.0,from PaLM 2,,,MultiMedQA,"We applied instruction finetuning to the base LLM following the protocol used by Chung et al. [21]. The datasets used included the training splits of MultiMedQAâ€“namely MedQA, MedMCQA, HealthSearchQA, LiveQA and MedicationQA. We trained a â€œunifiedâ€ model, which is optimized for performance across all datasets in MultiMedQA using dataset mixture ratios (proportions of each dataset) reported in Table 3.",,"Dataset Count Mixture ratio
MedQA 10,178 37.5%
MedMCQA 182,822 37.5%
LiveQA 10 3.9%
MedicationQA 9 3.5%
HealthSearchQA 45 17.6%

MedMCQA (https://proceedings.mlr.press/v174/pal22a/pal22a.pdf, Table 2) has on average 12.77+ 2.69+67.52 = 82.98 tokens per datapoint",,,,,Likely,"Recent artificial intelligence (AI) systems have reached milestones in ""grand challenges"" ranging from Go to protein-folding. The capability to retrieve medical knowledge, reason over it, and answer medical questions comparably to physicians has long been viewed as one such grand challenge.
Large language models (LLMs) have catalyzed significant progress in medical question answering; Med-PaLM was the first model to exceed a ""passing"" score in US Medical Licensing Examination (USMLE) style questions with a score of 67.2% on the MedQA dataset. However, this and other prior work suggested significant room for improvement, especially when models' answers were compared to clinicians' answers. Here we present Med-PaLM 2, which bridges these gaps by leveraging a combination of base LLM improvements (PaLM 2), medical domain finetuning, and prompting strategies including a novel ensemble refinement approach.
Med-PaLM 2 scored up to 86.5% on the MedQA dataset, improving upon Med-PaLM by over 19% and setting a new state-of-the-art. We also observed performance approaching or exceeding state-of-the-art across MedMCQA, PubMedQA, and MMLU clinical topics datasets.
We performed detailed human evaluations on long-form questions along multiple axes relevant to clinical applications. In pairwise comparative ranking of 1066 consumer medical questions, physicians preferred Med-PaLM 2 answers to those produced by physicians on eight of nine axes pertaining to clinical utility (p < 0.001). We also observed significant improvements compared to Med-PaLM on every evaluation axis (p < 0.001) on newly introduced datasets of 240 long-form ""adversarial"" questions to probe LLM limitations.
While further studies are necessary to validate the efficacy of these models in real-world settings, these results highlight rapid progress towards physician-level performance in medical question answering.",,,Unreleased,"Multinational,United States of America,Canada,Switzerland,United Kingdom of Great Britain and Northern Ireland",PaLM 2,,,,,2025-05-01 10:42,,,,,,"Industry,Industry",,,,,Unreleased,,"Industry,Industry",,,,,,,,,
OpenCALM,Language,Chat,CyberAgent,Ryosuke Ishigami,2023-05-15,OpenCALM-7B,https://huggingface.co/cyberagent/open-calm-7b,,,,7000000000.0,7B,8.95104e+19,6*2131200000*7000000000=8.95104e+19,"Wikipedia (ja),Japanese CC-100",,2131200000.0,"Wikipedia ja:
size of en wikipedia: 20.28 GB (https://huggingface.co/datasets/wikipedia_
size of japanese wikipedia: 20.28 GB / 6,825,683 english articles  * 1,416,129 japanese articles = 4.2 GB
wikipedia statistics: https://meta.wikimedia.org/wiki/List_of_Wikipedias
111M japanese words per GB * 4.2 GB = 466200000 words

CommonCrawl ja:
here they mention using cc100
cc-100 has 15G of japanese -> 1665000000 words
https://huggingface.co/datasets/cc100


1665000000+466200000=2131200000 (confidence - speculative because we don't know what subset they used)",,,,,Speculative,"OpenCALM is a suite of decoder-only language models pre-trained on Japanese datasets, developed by CyberAgent, Inc.",,,Open weights (unrestricted),Japan,,,"They say ""Library: GPT-NeoX"". could mean it's fine-tuned from GPT-NeoX, or just that it uses the same architecture or something? ",,,2025-02-14 16:22,,,,,,Industry,,,,,Unreleased,"CC BY-SA 4.0 license, commercial",Industry,,,,,Operation counting,,,,
A.X (Adot) 39B,"Language,Speech",Chat,SK Telecom,,2023-05-15,,"https://a.sktelecom.com/
https://m.ddaily.co.kr/page/view/2023092614585090151",,,,39000000000.0,,,the lower bound is taken from A.X (Adot) 18B estimation,,,,,,,,,Speculative,,,,Hosted access (no API),Korea (Republic of),,,,,,2025-03-24 15:23,,,,,,Industry,,1.3500000001e+18,,,Unreleased,,Industry,,,,,,,,,
LMRec,"Language,Recommendation",Language modeling/generation,"NAVER,Naver AI Lab","Kyuyong Shin, Hanock Kwak, Wonjae Kim, Jisu Jeong, Seungjae Jung, Kyung-Min Kim, Jung-Woo Ha, Sang-Woo Lee",2023-05-13,Pivotal Role of Language Modeling in Recommender Systems: Enriching Task-specific and Task-agnostic Representation Learning,https://arxiv.org/abs/2212.03760,,,,210000000.0,210 million (Figure 4),1.9782e+18,210000000*1570000000*6=1.9782e+18,Amazon Review Data,"We use three in-house datasets in order to assess our approach on various applications and add three public datasets that are predominantly evaluated in recommendation communities. The in-house datasets are built from
services of an online booking service (OBS), an online travel agency (OTA), and e-commerce platmform (ECOMM). For public datasets, we select two
categories â€œIndustrial and Scientificâ€ (Scientific) and â€œPrime Pantryâ€ (Pantry) from Amazon review datatsets (Ni et al., 2019) which are two completely
different service domains.",1570000000.0,"Table 3
 1222700000 (in-house dataset) + 347300000 (piblic) = 1,570,000,000 tokens",,,,,Likely,"Recent studies have proposed unified user modeling frameworks that leverage user behavior data from various applications. Many of them benefit from utilizing users' behavior sequences as plain texts, representing rich information in any domain or system without losing generality. Hence, a question arises: Can language modeling for user history corpus help improve recommender systems? While its versatile usability has been widely investigated in many domains, its applications to recommender systems still remain underexplored. We show that language modeling applied directly to task-specific user histories achieves excellent results on diverse recommendation tasks. Also, leveraging additional task-agnostic user histories delivers significant performance benefits. We further demonstrate that our approach can provide promising transfer learning capabilities for a broad spectrum of real-world recommender systems, even on unseen domains and services.",,,,"Korea (Republic of),Korea (Republic of)",,,,,,2025-02-14 16:22,,,,1024.0,table 9,"Industry,Industry",,,,,,,"Industry,Industry",,,,,Operation counting,,,,
InstructBLIP,"Multimodal,Language,Vision","Visual question answering,Chat","Salesforce Research,Hong Kong University of Science and Technology (HKUST),Nanyang Technological University","Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, Steven Hoi",2023-05-11,InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning,https://arxiv.org/abs/2305.06500,1343.0,SOTA improvement,from abstract - SOTA on ScienceQA,13000000000.0,13B form 2.6,1.94e+20,"""All models are trained utilizing 16 Nvidia A100 (40G) GPUs and are completed within 1.5 days.""
16 * 3.12e14 * 1.5 * 24 * 3600 * 0.3 = 1.94e20","COCO,Web CapFilt,NoCaps,Flickr30K Entities,TextCaps,VQAv2,VizWiz,GQA,OKVQA,ScienceQA,OCR-VQA,TextVQA,LLaVA-Instruct-150k","COCO Caption, Web CapFilt, NoCaps, Flickr30K, TextCaps, VQAv2, VizWiz, GQA, Visual Spatial Reasoning, IconQA, OKVQA, A-OKVQA, ScienceQA, Visual Dialog, OCR-VQA, TextVQA, HatefulMemes, LLaVA-Instruct-150K, MSVD-QA, MSRVTT-QA, iVQA",,"""All models are instruction-tuned with a maximum of 60K steps""

""We employ a batch size of 192, 128, and 64 for the 3B, 7B, and 11/13B models, respectively. """,36.0,"""All models are trained utilizing 16 Nvidia A100 (40G) GPUs and are completed within 1.5 days.""",NVIDIA A100 SXM4 40 GB,,Confident,"Large-scale pre-training and instruction tuning have been successful at creating general-purpose language models with broad competence. However, building general-purpose vision-language models is challenging due to the rich input distributions and task diversity resulting from the additional visual input. Although vision-language pretraining has been widely studied, vision-language instruction tuning remains under-explored. In this paper, we conduct a systematic and comprehensive study on vision-language instruction tuning based on the pretrained BLIP-2 models. We gather 26 publicly available datasets, covering a wide variety of tasks and capabilities, and transform them into instruction tuning format. Additionally, we introduce an instruction-aware Query Transformer, which extracts informative features tailored to the given instruction. Trained on 13 held-in datasets, InstructBLIP attains state-of-the-art zero-shot performance across all 13 held-out datasets, substantially outperforming BLIP-2 and larger Flamingo models. Our models also lead to state-of-the-art performance when finetuned on individual downstream tasks (e.g., 90.7% accuracy on ScienceQA questions with image contexts). Furthermore, we qualitatively demonstrate the advantages of InstructBLIP over concurrent multimodal models. All InstructBLIP models are open-sourced at this https URL. ",,,Open weights (non-commercial),"United States of America,Hong Kong,China,Singapore",Vicuna-13B v0,194088960000000000000,"flops = (16) * (312 * 10**12) * (1.5* 24 * 3600) * (0.3) = 1.9e20
(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)

""All models are trained utilizing 16 Nvidia A100 (40G) GPUs and are completed within 1.5 days.""",16.0,,2025-06-06 12:34,,,,,,"Industry,Academia,Academia",,,,576.0,Open (non-commercial),"LlaMA/Vicuna license, non-comm:
https://github.com/salesforce/LAVIS/tree/main/projects/instructblip

research only:
https://huggingface.co/Salesforce/instructblip-vicuna-13b","Industry,Academia,Academia",,,FP16,12748.20334097663,Hardware,Salesforce,,,
ESM-GearNet,Biology,"Proteins,Protein function prediction","Mila - Quebec AI (originally Montreal Institute for Learning Algorithms),University of Montreal / UniversitÃ© de MontrÃ©al,IBM Research,HEC Montreal,CIFAR AI Research","Zuobai Zhang, Chuanrui Wang, Minghao Xu, Vijil Chenthamarakshan, AurÃ©lie Lozano, Payel Das, Jian Tang",2023-05-11,A Systematic Study of Joint Representation Learning on Protein Sequences and Structures,https://arxiv.org/abs/2303.06275,17.0,,,650000000.0,,2.145e+19,"Using 6*N*D with 50 epochs:
6*650000000*110000001*50=2.145e+19", AlphaFold database (AFDB),,110000001.0,"Calculating unique tokens seen in first epoch:
Number of Proteins: 365,000
Average Residues per Protein: 300
Total Datapoints = 365,000 Ã— 300 = 109,500,000 â‰ˆ 1.1 Ã— 10^8 tokens",,,NVIDIA A100,,Likely,"Learning effective protein representations is critical in a variety of tasks in biology such as predicting protein functions. Recent sequence representation learning methods based on Protein Language Models (PLMs) excel in sequence-based tasks, but their direct adaptation to tasks involving protein structures remains a challenge. In contrast, structure-based methods leverage 3D structural information with graph neural networks and geometric pre-training methods show potential in function prediction tasks, but still suffers from the limited number of available structures. To bridge this gap, our study undertakes a comprehensive exploration of joint protein representation learning by integrating a state-of-the-art PLM (ESM-2) with distinct structure encoders (GVP, GearNet, CDConv). We introduce three representation fusion strategies and explore different pre-training techniques. Our method achieves significant improvements over existing sequence- and structure-based methods, setting new state-of-the-art for function annotation. This study underscores several important design choices for fusing protein sequence and structure information. Our implementation is available at this https URL.",50.0,,,"Canada,Canada,United States of America,Multinational,Ireland,United Kingdom of Great Britain and Northern Ireland,Brazil,Switzerland,Canada,Canada",,,,4.0,,2025-05-01 10:42,,,,,,"Academia,Academia,Industry,Academia,Research collective",,,,,,,"Academia,Academia,Industry,Academia,Research collective",,,,3187.050835244157,,,,,
PaLM 2,Language,"Language modeling,Language modeling/generation",Google,"Andrew M. Dai, David R. So, Dmitry Lepikhin, Jonathan H. Clark, Maxim Krikun, Melvin Johnson, Nan Du, Rohan Anil, Siamak Shakeri, Xavier Garcia, Yanping Huang, Yi Tay, Yong Cheng, Yonghui Wu, Yuanzhong Xu, Yujing Zhang, Zachary Nado, Bryan Richter, Alex Polozov, Andrew Nystrom, Fangxiaoyu Feng, Hanzhao Lin, Jacob Austin, Jacob Devlin, Kefan Xiao, Orhan Firat, Parker Riley, Steven Zheng, Yuhuai Wu, Zhongtao Liu, Jiahui Yu, Guy Gur-Ari, Weikang Zhou, Sneha Kudugunta, Sunipa Dev, Frederick Liu, Gustavo Hernandez Abrego, Kelvin Xu, Abe Ittycheriah, Daniel Sohn, John Nham, Le Hou, Siyuan Qiao, Pidong Wang, Zirui Wang, Laurent El Shafey, Hyeontaek Lim, Marcello Maggioni, Michael Isard, Paul Barham, Qiao Zhang, Tao Wang, Yash Katariya, Aurko Roy, Benjamin Lee, Brennan Saeta, Ce Zheng, Hadi Hashemi, Junwhan Ahn, Rajkumar Samuel, Steven Hand, Zhifeng Chen, Kiran Vodrahalli, Aakanksha Chowdhery, Ethan Dyer, Emanuel Taropa, Vlad Feinberg, James Bradbury, Reiner Pope, Wei Li, YaGuang Li, Eric Chu, Jeffrey Hui, Joshua Howland, Vlad Fienber, Aroma Mahendru, Michele Catasta, Vedant Misra, Kevin Robinson, Maysam Moussalem, Sebastian Ruder, Erica Moreira, Eric Ni, Paige Bailey, Lucas Gonzalez, Alexandre Passos, Slav Petrov, Gaurav Mishra, Mark Omernick, Ambrose Slone, Andrea Hu, Colin Cherry, Denny Zhou, Jan Botha, John Wieting, Joshua Maynez, Kathleen Kenealy, Kevin Brooks, Linting Xue, Markus Freitag, Martin Polacek, Pengcheng Yin, Sebastian Gehrmann, Xuezhi Wang, Kathy Meier-Hellstern, Christopher A. Choquette-Choo, Daniel Smilkov, Emily Reif, Alicia Parrish, Alex Castro Ros, ClÃ©ment Crepy, Dasha Valter, Jeremy Hurwitz, Katherine Lee, Mark DÃ­az, Marie Pellat, Matthew Jagielski, Renee Shelby, Shachi Dave",2023-05-10,PaLM 2 Technical Report,https://arxiv.org/abs/2305.10403,1734.0,"SOTA improvement,Training cost,Significant use,Highly cited","Significant use: Gmail and Google Docs have millions of users.
""At I/O today, we announced over 25 new products and features powered by PaLM 2. That means that PaLM 2 is bringing the latest in advanced AI capabilities directly into our products and to people â€” including consumers, developers, and enterprises of all sizes around the world. Here are some examples:
PaLM 2â€™s improved multilingual capabilities are allowing us to expand Bard to new languages, starting today. Plus, itâ€™s powering our recently announced coding update.
Workspace features to help you write in Gmail and Google Docs, and help you organize in Google Sheets are all tapping into the capabilities of PaLM 2 at a speed that helps people get work done better, and faster.""
https://blog.google/technology/ai/google-palm-2-ai-large-language-model/",340000000000.0,"Model Architecture: ""PaLM-2 is a new state-of-the-art language model. We have small, medium, and large variants that use stacked layers based on the Transformer architecture, with varying parameters depending on model size. Further details of model size and architecture are withheld from external publication.""
However, the parameter count was leaked to CNBC: https://www.cnbc.com/2023/05/16/googles-palm-2-uses-nearly-five-times-more-text-data-than-predecessor.html",7.34e+24,"Compute Requirements ""Not reported.""
Paper suggests heuristic of  C=6ND. Based on 340B parameters and 3.6T tokens, training compute would be around 7.3*10^24 FLOP.",,"""The PaLM 2 pre-training corpus is composed of a diverse set of sources: web documents, books, code, mathematics, and conversational data. The pre-training corpus is significantly larger than the corpus used to train PaLM (Chowdhery et al., 2022). PaLM 2 is trained on a dataset that includes a higher percentage of non-English data than previous large language models, which is beneficial for multilingual tasks"" (page 9)

â€œPaLM 2's knowledge cutoff time is mid-2021. Knowledge about events after that time is limited,â€ according to https://ai.google.dev/palm_docs/palm. September 2021 according to https://computercity.com/artificial-intelligence/knowledge-cutoff-dates-llms",2700000000000.0,"""The pre-training corpus is significantly larger than the corpus used to train PaLM"" so greater than 6e+11. According to the leaked documents viewed by CNBC, the corpus was 3.6 trillion tokens or around 2.7*10^12 words.

https://www.cnbc.com/2023/05/16/googles-palm-2-uses-nearly-five-times-more-text-data-than-predecessor.html",,,Google TPU v4,,Likely,"We introduce PaLM 2, a new state-of-the-art language model that has better multilingual and reasoning capabilities and is more compute-efficient than its predecessor PaLM (Chowdhery et al., 2022). PaLM 2 is a Transformer-based model trained using a mixture of objectives similar to UL2 (Tay et al., 2023). Through extensive evaluations on English and multilingual language, and reasoning tasks, we demonstrate that PaLM 2 has significantly improved quality on downstream tasks across different model sizes, while simultaneously exhibiting faster and more efficient inference compared to PaLM. This improved efficiency enables broader deployment while also allowing the model to respond faster, for a more natural pace of interaction. PaLM 2 demonstrates robust reasoning capabilities exemplified by large improvements over PaLM on BIG-Bench and other reasoning tasks. PaLM 2 exhibits stable performance on a suite of responsible AI evaluations, and enables inference-time control over toxicity without additional overhead or impact on other capabilities. Overall, PaLM 2 achieves state-of-the-art performance across a diverse set of tasks and capabilities.",,,API access,United States of America,,,,,,2025-05-12 20:29,,,,,,Industry,checked,,7.34e+24,,Unreleased,,Industry,$4865570.06,,,,Operation counting,,,,2
StarCoder,Language,Code generation,"Hugging Face,ServiceNow,Northeastern University,Mila - Quebec AI (originally Montreal Institute for Learning Algorithms),Carnegie Mellon University (CMU),Johns Hopkins University,Leipzig University,ScaDS.AI,Queen Mary University of London,Roblox,Sea AI Lab,Technion - Israel Institute of Technology,Monash University,CSIRO,Data61,McGill University,Saama,University of British Columbia (UBC),Massachusetts Institute of Technology (MIT),Technical University of Munich,IBM,University of Vermont,UnfoldML,SAP,University of Notre Dame,Columbia University,New York University (NYU),University of Allahabad,Discover Dollar,Toloka,Telefonica,Stanford University,Weizmann Institute of Science,Alan Turing Institute,Wellesley College,EleutherAI,Forschungszentrum Julich","Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, JoÃ£o Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos MuÃ±oz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, Harm de Vries",2023-05-09,StarCoder: may the source be with you!,https://arxiv.org/abs/2305.06161,512.0,SOTA improvement,"""We perform the most comprehensive evaluation of Code LLMs to date and show that StarCoderBase outperforms every open Code LLM that supports multiple programming languages and matches or outperforms the OpenAI code-cushman-001 model. Furthermore, StarCoder outperforms every model that is fine-tuned on Python""",15500000000.0,"""We trained a 15.5B parameter model""",8.46e+22,"FLOP reported here, 8.46e22
https://huggingface.co/bigcode/starcoder


""We trained our model on a GPU cluster with 512 A100 80 GB GPUs... Based on the total number of GPU hours that training took (320,256) and an average power usage of 280W per GPU... The fine-tuned model adds 3.5% of training time""

320256 * 312 tFLOP/s * 3600 * 1.035 * 0.3 (utilization assumption) = 1.12e23",The Stack,"""StarCoderBase is trained on 1 trillion tokens sourced from The Stack (Kocetkov et al., 2022), a large collection of permissively licensed GitHub repositories with inspection tools and an opt-out process""",1000000000000.0,"""StarCoderBase is trained on 1 trillion tokens sourced from The Stack""",625.5,"625.5 hours = 320256 /512
512 GPUs from ""We trained our model on a GPU cluster with 512 A100 80 GB GPUs ""

320256 GPU hours from ""Based on the total number of GPU hours that training took (320,256)""
citations from sections 5.6 and 5.7",NVIDIA A100 SXM4 80 GB,,Confident,"""The BigCode community, an open-scientific collaboration working on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder and StarCoderBase: 15.5B parameter models with 8K context length, infilling capabilities and fast large-batch inference enabled by multi-query attention. StarCoderBase is trained on 1 trillion tokens sourced from The Stack, a large collection of permissively licensed GitHub repositories with inspection tools and an opt-out process. We fine-tuned StarCoderBase on 35B Python tokens, resulting in the creation of StarCoder. We perform the most comprehensive evaluation of Code LLMs to date and show that StarCoderBase outperforms every open Code LLM that supports multiple programming languages and matches or outperforms the OpenAI code-cushman-001 model. Furthermore, StarCoder outperforms every model that is fine-tuned on Python, can be prompted to achieve 40\% pass@1 on HumanEval, and still retains its performance on other programming languages. We take several important steps towards a safe open-access model release, including an improved PII redaction pipeline and a novel attribution tracing tool, and make the StarCoder models publicly available under a more commercially viable version of the Open Responsible AI Model license.""",1.0,,Open weights (restricted use),"Multinational,United States of America,United States of America,United States of America,Canada,United States of America,United States of America,Germany,Germany,United Kingdom of Great Britain and Northern Ireland,United States of America,Singapore,Israel,Australia,Australia,Australia,Canada,United States of America,Canada,United States of America,Germany,United States of America,United States of America,Sweden,Multinational,Germany,United States of America,United States of America,United States of America,India,India,Multinational,Spain,United States of America,Israel,United Kingdom of Great Britain and Northern Ireland,United States of America,Multinational,United States of America,Germany",,,,512.0,0.2272,2025-05-28 16:05,,,,4000000.0,"""The model was trained for 250k iterations, with a batch size of 4M tokens, for a total of one trillion tokens.""","Industry,Industry,Academia,Academia,Academia,Academia,Academia,Academia,Industry,Academia,Academia,Government,Government,Academia,Academia,Academia,Academia,Industry,Academia,Industry,Academia,Academia,Academia,Academia,Industry,Industry,Industry,Academia,Academia,Government,Academia,Research collective,Government",checked,,,320256.0,Unreleased,"some restrictions

https://huggingface.co/spaces/bigcode/bigcode-model-license-agreement

data is The Stack, which has multiple licenses
https://huggingface.co/datasets/bigcode/the-stack-dedup ","Industry,Industry,Academia,Academia,Academia,Academia,Academia,Academia,Industry,Academia,Academia,Government,Government,Academia,Academia,Academia,Academia,Industry,Academia,Industry,Academia,Academia,Academia,Academia,Industry,Industry,Industry,Academia,Academia,Government,Academia,Research collective,Government",$212217.65,"Actual training compute given by https://huggingface.co/bigcode/starcoder as 8.46e22 FLOPs
Stated GPU usage is 320,256 A100-hours for pre-training + 11,208 for fine-tuning.
(320,256 + 11,208) * 3600 * 3.12e14 = 3.723e23 FLOPs at full utilization
Implied utilization: 8.46e22 / 3.723e23 = 0.2272",BF16,407960.6765621668,"Reported,Hardware",,,,
ImageBind,"Multimodal,Vision,Audio,Language,Image generation,Speech","Image classification,Speech recognition,Image generation,Language modeling/generation",Meta AI,"Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, Ishan Misra",2023-05-09,IMAGEBIND: One Embedding Space To Bind Them All,"https://arxiv.org/abs/2305.05665, https://github.com/facebookresearch/ImageBind",607.0,SOTA improvement,"""we set a new state-of-the-art on emergent zero-shot recognition tasks across modalities, outperforming specialist supervised models""",932000000.0,used ViT-Huge 630M as an image/video encoder and OpenCLIP-302m as text encoder,,,"SUN RGB-D,LLVIP,Ego4D,AudioSet",""" For the naturally available paired data, we use
the (video, audio) pairs from the Audioset dataset [19], (image, depth) pairs from the SUN RGB-D dataset [69], (image, thermal) pairs from the LLVIP dataset [32] and (video,
IMU) pairs from the Ego4D dataset [23].""",,,,,"NVIDIA V100,NVIDIA A100",,Likely,"We present ImageBind, an approach to learn a joint embedding across six different modalities - images, text, audio, depth, thermal, and IMU data. We show that all combinations of paired data are not necessary to train such a joint embedding, and only image-paired data is sufficient to bind the modalities together. ImageBind can leverage recent large scale vision-language models, and extends their zero-shot capabilities to new modalities just by using their natural pairing with images. It enables novel emergent applications 'out-of-the-box' including cross-modal retrieval, composing modalities with arithmetic, cross-modal detection and generation. The emergent capabilities improve with the strength of the image encoder and we set a new state-of-the-art on emergent zero-shot recognition tasks across modalities, outperforming specialist supervised models. Finally, we show strong few-shot recognition results outperforming prior work, and that ImageBind serves as a new way to evaluate vision models for visual and non-visual tasks.",64.0,,Open weights (non-commercial),United States of America,ViT-Huge/14,,,,,2025-02-03 12:13,,,,,,Industry,,,,,Open (non-commercial),"Creative commons non-commercial
models and code in this repo: https://github.com/facebookresearch/ImageBind/blob/main/README.md 
train code: https://github.com/facebookresearch/ImageBind/blob/main/imagebind/models/imagebind_model.py ",Industry,,,,,,,,,
Otter,"Multimodal,Language,Vision","Chat,Visual question answering,Image captioning,Image generation,Vision-language generation",Nanyang Technological University,"Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, Ziwei Liu",2023-05-05,Otter: A Multi-Modal Model with In-Context Instruction Tuning,https://arxiv.org/abs/2305.03726,437.0,,,1300000000.0,1.3B from section 3.2:  'This results in approximately 1.3 billion trainable parameters for the Otter model.',,rather low as only 4 RTX 3090 were used.,,custom,,,,,NVIDIA GeForce RTX 3090,,Likely,"Large language models (LLMs) have demonstrated significant universal capabilities as few/zero-shot learners in various tasks due to their pre-training on vast amounts of text data, as exemplified by GPT-3, which boosted to InstrctGPT and ChatGPT, effectively following natural language instructions to accomplish real-world tasks. In this paper, we propose to introduce instruction tuning into multi-modal models, motivated by the Flamingo model's upstream interleaved format pretraining dataset. We adopt a similar approach to construct our MultI-Modal In-Context Instruction Tuning (MIMIC-IT) dataset. We then introduce Otter, a multi-modal model based on OpenFlamingo (open-sourced version of DeepMind's Flamingo), trained on MIMIC-IT and showcasing improved instruction-following ability and in-context learning. We also optimize OpenFlamingo's implementation for researchers, democratizing the required training resources from 1Ã— A100 GPU to 4Ã— RTX-3090 GPUs, and integrate both OpenFlamingo and Otter into Huggingface Transformers for more researchers to incorporate the models into their customized training and inference pipelines. ",,,Open weights (unrestricted),Singapore,,,,4.0,,2024-11-01 10:05,,,,,,Academia,,,,,Open source,MIT: https://github.com/Luodian/Otter,Academia,,,,2789.042117229417,,,,,
CodeGen2,Language,Code generation,Salesforce,"Erik Nijkamp, Hiroaki Hayashi, Caiming Xiong, Silvio Savarese, Yingbo Zhou",2023-05-03,CodeGen2: Lessons for Training LLMs on Programming and Natural Languages,https://arxiv.org/abs/2305.02309,132.0,,,16000000000.0,16B for largest CodeGen2 model,,,Stack v1.1,"""We examine our recipe on four model sizes: 1B, 3.7B, 7B, and 16B, and
refer to them as CodeGen2.
1 For training a subset of the Stack v1.1 (Kocetkov et al., 2022), filtered
with a stronger permissive license guideline, is used""",,,,,,,Confident,"Large language models (LLMs) have demonstrated remarkable abilities in representation learning for program synthesis and understanding tasks. The quality of the learned representations appears to be dictated by the neural scaling laws as a function of the number of model parameters and observations, while imposing upper bounds on the model performance by the amount of available data and compute, which is costly.
In this study, we attempt to render the training of LLMs for program synthesis more efficient by unifying four key components: (1) model architectures, (2) learning methods, (3) infill sampling, and, (4) data distributions. Specifically, for the model architecture, we attempt to unify encoder and decoder-based models into a single prefix-LM. For learning methods, (i) causal language modeling, (ii) span corruption, (iii) infilling are unified into a simple learning algorithm. For infill sampling, we explore the claim of a ""free lunch"" hypothesis. For data distributions, the effect of a mixture distribution and multi-epoch training of programming and natural languages on model performance is explored.
We conduct a comprehensive series of empirical experiments on 1B LLMs, for which failures and successes of this exploration are distilled into five lessons. We will provide a final recipe for training and release CodeGen2 models in size 1B, 3.7B, 7B, and, 16B parameters, along with the training framework as open-source: this https URL.",,,Open weights (unrestricted),United States of America,,,,,,2024-11-01 10:05,,,,,,Industry,,,,,,apache 2.0,Industry,,,,,,,,,
Perfusion,Image generation,Text-to-image,"NVIDIA,Tel Aviv University,Bar-Ilan University","Yoad Tewel, Rinon Gal, Gal Chechik, Yuval Atzmon",2023-05-02,Key-Locked Rank One Editing for Text-to-Image Personalization,https://arxiv.org/abs/2305.01644,128.0,,Pareto frontier performance but not SOTA,,,,"Must be <1e23 FLOP, it's trained with a single A100.",,,,,,,NVIDIA A100,,Unknown,"Text-to-image models (T2I) offer a new level of flexibility by allowing users to guide the creative process through natural language. However, personalizing these models to align with user-provided visual concepts remains a challenging problem. The task of T2I personalization poses multiple hard challenges, such as maintaining high visual fidelity while allowing creative control, combining multiple personalized concepts in a single image, and keeping a small model size. We present Perfusion, a T2I personalization method that addresses these challenges using dynamic rank-1 updates to the underlying T2I model. Perfusion avoids overfitting by introducing a new mechanism that ""locks"" new concepts' cross-attention Keys to their superordinate category. Additionally, we develop a gated rank-1 approach that enables us to control the influence of a learned concept during inference time and to combine multiple concepts. This allows runtime-efficient balancing of visual-fidelity and textual-alignment with a single 100KB trained model, which is five orders of magnitude smaller than the current state of the art. Moreover, it can span different operating points across the Pareto front without additional training. Finally, we show that Perfusion outperforms strong baselines in both qualitative and quantitative terms. Importantly, key-locking leads to novel results compared to traditional approaches, allowing to portray personalized object interactions in unprecedented ways, even in one-shot settings.",,,Unreleased,"United States of America,Israel,Israel",,,,,,2024-11-01 10:05,,,,,,"Industry,Academia,Academia",,,,,Unreleased,,"Industry,Academia,Academia",,,,,,,,,
OpenLLaMA-13B,Language,"Language modeling/generation,Question answering",OpenLM Research,"Xinyang Geng, Hao Liu",2023-05-01,OpenLLaMA: An Open Reproduction of LLaMA,https://github.com/openlm-research/open_llama,,,,13000000000.0,13B,7.8e+22,13b * 1T * 6 = 7.8e22,RedPajama,RedPajama 1T: https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T,1000000000000.0,"1T tokens, or ~750B words",,,Google TPU v4,,Confident,"TL;DR: we are releasing our public preview of OpenLLaMA, a permissively licensed open source reproduction of Meta AIâ€™s LLaMA. We are releasing a series of 3B, 7B and 13B models trained on different data mixtures. Our model weights can serve as the drop in replacement of LLaMA in existing implementations.

In this repo, we present a permissively licensed open source reproduction of Meta AI's LLaMA large language model. We are releasing a series of 3B, 7B and 13B models trained on 1T tokens. We provide PyTorch and JAX weights of pre-trained OpenLLaMA models, as well as evaluation results and comparison against the original LLaMA models. The v2 model is better than the old v1 model trained on a different data mixture.",1.0,,Open weights (unrestricted),United States of America,,,,,,2024-11-25 13:45,,,,,,Research collective,,,,,Unreleased,"Apache-2.0 license
https://github.com/openlm-research/open_llama",Research collective,,,,,Operation counting,,,,
MosaicML Diffusion,Image generation,"Image generation,Text-to-image",Databricks,"Mihir Patel, Erica Ji Yuen, Cory Stephenson, Landan Seguin",2023-04-28,Training Stable Diffusion from Scratch for <$50k with MosaicML,"https://www.databricks.com/blog/stable-diffusion-2
https://www.databricks.com/blog/diffusion
https://github.com/mosaicml/diffusion",,,Included due to high degree of transparency in training process,1289952427.0,"Manually loaded model with following code snippet:

from diffusers import DiffusionPipeline, DPMSolverMultistepScheduler
import torch

repo_id = ""stabilityai/stable-diffusion-2""
pipe = DiffusionPipeline.from_pretrained(repo_id, torch_dtype=torch.float16, revision=""fp16"")

n_params_vae = sum(p.numel() for p in pipe.components['vae'].parameters())
n_params_text_encoder = sum(p.numel() for p in pipe.components['text_encoder'].parameters())
n_params_unet = sum(p.numel() for p in pipe.components['unet'].parameters())
n_params = n_params_vae + n_params_text_encoder + n_params_unet

print(f""Total number of parameters: {n_params}"")",1.07085888e+22,"23,835 A100-hours * 3.12e14 FLOP/GPU-sec * 3600 sec/hour * 0.4 (assumed utilization) = 1.07e22

""Both phases of training ran on 128 NVIDIA A100 GPUs. The first training phase was run for 550k iterations in 1.6 days while the second phase was run for 850k iterations in 4.9 days, for a total of 20,051 A100 hours for training. In addition to the training time, we pre-computed the latents for the VAE and CLIP model to reduce training time and cost when making multiple passes over the dataset. Pre-computing the latents required an additional 3,784 A100 hours, resulting in 23,835 A100 hours in total""

Uses automatic mixed precision, which uses half precision (fp16) in most layers, but fp32 in a few numerically unstable layers like normalization and softmax. Unlike original stable diffusion 2, used half precision LayerNorm and GroupNorm layers.",LAION,"We trained on a subset of LAION-5B that includes samples with English-only captions and an aesthetic score of 4.5+. For the first phase of training, we used all images with resolution >=256x256, amounting to 790 million image-caption samples. For the second phase of training, we only used images with resolution >=512x512, amounting to 300 million image-caption samples.",790000000.0,"For the first phase of training, we used all images with resolution >=256x256, amounting to 790 million image-caption samples. For the second phase of training, we only used images with resolution >=512x512, amounting to 300 million image-caption samples.

Note that second phase data is a strict subset of first stage.",186.0,"""Both phases of training ran on 128 NVIDIA A100 GPUs. The first training phase was run for 550k iterations in 1.6 days while the second phase was run for 850k iterations in 4.9 days, for a total of 20,051 A100 hours for training. In addition to the training time, we pre-computed the latents for the VAE and CLIP model to reduce training time and cost when making multiple passes over the dataset. Pre-computing the latents required an additional 3,784 A100 hours, resulting in 23,835 A100 hours in total""
Assuming pre-computation used all 128 A100s, that would have taken 30 hours. Total time: (1.6 + 4.9)*24 + 30 = 186 hours",NVIDIA A100,Self-supervised learning,Confident,"We've replicated Stable Diffusion 2 for less than $50k, and we've open-sourced the training code so you can too! This is a 3x cost reduction from our last blog post and an 8x reduction from the original Stable Diffusion 2, making training large-scale diffusion models from scratch more accessible than ever before.",1.38,,Unreleased,United States of America,,,,128.0,,2025-05-09 11:32,,,,,,Industry,,,,,,"Model weights not released, training code is open source with Apache 2.0 license",Industry,,,,102015.15602722247,Hardware,,,,
Agile Soccer Robot,Robotics,"Animal (human/non-human) imitation,Sports",Google DeepMind,"Tuomas Haarnoja, Ben Moran, Guy Lever, Sandy H. Huang, Dhruva Tirumala, Markus Wulfmeier, Jan Humplik, Saran Tunyasuvunakool, Noah Y. Siegel, Roland Hafner, Michael Bloesch, Kristian Hartikainen, Arunkumar Byravan, Leonard Hasenclever, Yuval Tassa, Fereshteh Sadeghi, Nathan Batchelor, Federico Casarini, Stefano Saliceti, Charles Game, Neil Sreendra, Kushal Patel, Marlon Gwira, Andrea Huber, Nicole Hurley, Francesco Nori, Raia Hadsell, Nicolas Heess",2023-04-26,Learning Agile Soccer Skills for a Bipedal Robot with Deep Reinforcement Learning,https://arxiv.org/abs/2304.13653,82.0,SOTA improvement,"Likely the best bipedal soccer AI, since it's DeepMind, and related work section just discusses results involving specific soccer skills and quadruped robots:

""Whether bipedal or quadrupedal, navigation represents only a fraction of animal and human capabilities. Motivated by this observation, there is a growing interest in whole body control, i.e. tasks in which the whole body is used in flexible ways to interact with the environment. Examples include climbing (Rudin et al., 2022a), getting-up from the ground (Ma et al., 2023), catching objects (Ma et al., 2023), and mobile manipulation with legs (Cheng et al., 2023). Recently, reinforcement learning has been applied to learn simple soccer skills, including goalkeeping (Huang et al., 2022), ball manipulation on diverse terrains (Bohez et al., 2022; Ji et al., 2023), and shooting (Ji et al.,
2022). These works focus on a narrower set of skills than the 1v1 soccer game, and the quadrupedal platform is inherently more stable and therefore presents an easier learning challenge.""",,,,,,self-play training in simulation,,""". The get-up teacher learns to get up relatively quickly and trained in total for approximately 2.4 Â· 10^8 environment steps,
equivalent to approximately 70 days of simulation time, or 14 hours of wall-clock time. The soccer
teacher was trained for 2 Â· 10^9 environment steps, which took 158 hours of training, equivalent to
approximately 580 days of simulated match""",240.0,"14+158+68 hours:
""Training the get-up and soccer teachers took 14 and 158 hours (6.5 days), respectively, and distillation and self-play
took 68 hours (see Appendix B for details)""",,Reinforcement learning,Unknown,"We investigate whether Deep Reinforcement Learning (Deep RL) is able to synthesize sophisticated and safe movement skills for a low-cost, miniature humanoid robot that can be composed into complex behavioral strategies in dynamic environments. We used Deep RL to train a humanoid robot with 20 actuated joints to play a simplified one-versus-one (1v1) soccer game. We first trained individual skills in isolation and then composed those skills end-to-end in a self-play setting. The resulting policy exhibits robust and dynamic movement skills such as rapid fall recovery, walking, turning, kicking and more; and transitions between them in a smooth, stable, and efficient manner - well beyond what is intuitively expected from the robot. The agents also developed a basic strategic understanding of the game, and learned, for instance, to anticipate ball movements and to block opponent shots. The full range of behaviors emerged from a small set of simple rewards. Our agents were trained in simulation and transferred to real robots zero-shot. We found that a combination of sufficiently high-frequency control, targeted dynamics randomization, and perturbations during training in simulation enabled good-quality transfer, despite significant unmodeled effects and variations across robot instances. Although the robots are inherently fragile, minor hardware modifications together with basic regularization of the behavior during training led the robots to learn safe and effective movements while still performing in a dynamic and agile way. Indeed, even though the agents were optimized for scoring, in experiments they walked 156% faster, took 63% less time to get up, and kicked 24% faster than a scripted baseline, while efficiently combining the skills to achieve the longer term objectives. Examples of the emergent behaviors and full 1v1 matches are available on the supplementary website.",,,Unreleased,"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France",,,,,,2025-06-04 17:25,,,,,,Industry,,,,,Unreleased,"only video demos here
https://sites.google.com/view/op3-soccer",Industry,,,,,,,,,
WizardLM-7B,Language,Language modeling,"Microsoft,Peking University","Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, Daxin Jiang",2023-04-24,WizardLM: Empowering Large Language Models to Follow Complex Instructions,https://arxiv.org/abs/2304.12244,727.0,,"""Labelers prefer WizardLM outputs over outputs from ChatGPT under complex test instructions. On Evol-Instruct testset, WizardLM performs worse than ChatGPT, with a win
rate 12.8% lower than ChatGPT (28.0% vs. 40.8%). However, in the high-difficulty section
of Evol-Instruct test set (difficulty level â‰¥ 8), our WizardLM even outperforms ChatGPT,
with a win rate 7.9% larger than ChatGPT (42.9% vs. 35.0%), that is human annotators even
prefer the output of our model than ChatGPT on those hard questions""

^ note that this is about GPT-3.5",6700000000.0,This is Llama-7b's parameter count,4.02e+22,"""We use pre-trained LLaMA 7B [4] to initialize our model. We adopt Adam optimizer as an initial learning rate of 2 Ã—10âˆ’5, a maximum number of tokens 2048, and the batch size is 8 for each GPU. We train our model on 8 V100 GPUs with Deepspeed Zero-3 for 70 hours on 3 epochs""

Llama-7b was ~4e22. 8*70 V100-hours is ~2e20, so fine-tuning was <1% of base training.",Evol Instruct,"Fine-tuning dataset is made of LLM-generated instructions: ""In this work, we introduce Evol-Instruct, a novel method using LLMs instead of humans to automatically mass-produce open-domain instructions of various difficulty levels, to improve the performance of LLMs""",,,70.0,,NVIDIA V100,,Confident,"Training large language models (LLMs) with open-domain instruction following data brings colossal success. However, manually creating such instruction data is very time-consuming and labor-intensive. Moreover, humans may struggle to produce high-complexity instructions. In this paper, we show an avenue for creating large amounts of instruction data with varying levels of complexity using LLM instead of humans. Starting with an initial set of instructions, we use our proposed Evol-Instruct to rewrite them step by step into more complex instructions. Then, we mix all generated instruction data to fine-tune LLaMA. We call the resulting model WizardLM. Human evaluations on a complexity-balanced test bed and Vicuna's testset show that instructions from Evol-Instruct are superior to human-created ones. By analyzing the human evaluation results of the high complexity part, we demonstrate that outputs from our WizardLM are preferred to outputs from OpenAI ChatGPT. In GPT-4 automatic evaluation, WizardLM achieves more than 90\% capacity of ChatGPT on 17 out of 29 skills. Even though WizardLM still lags behind ChatGPT in some aspects, our findings suggest that fine-tuning with AI-evolved instructions is a promising direction for enhancing LLMs. Our code and data are public at this https URL",,,Open weights (non-commercial),"United States of America,Multinational,India,Belgium,China",LLaMA-7B,,,8.0,,2025-05-12 15:54,,,,,,"Industry,Academia",,,,,Open source,"non commercial for weights 
https://github.com/nlpxucan/WizardLM
code is apache:  https://github.com/nlpxucan/WizardLM/blob/main/WizardLM/CODE_LICENSE 
finetune code: https://github.com/nlpxucan/WizardLM/tree/main/WizardLM#fine-tuning ","Industry,Academia",$46907.43,,,4782.386422758304,Hardware,,,,
Falcon-7B,Language,Language modeling/generation,Technology Innovation Institute,,2023-04-24,Falcon-7B ,https://huggingface.co/tiiuae/falcon-7b,,,,7000000000.0,7B,6.3e+22,"6ND = 6 * 7B * 1.5T = 6.3e22
""Falcon-7B is a 7B parameters causal decoder-only model built by TII and trained on 1,500B tokens of RefinedWeb enhanced with curated corpora. It is made available under the Apache 2.0 license.""

Table 1 from https://arxiv.org/pdf/2311.16867 Falcon paper

730 petaflop-days * 1e15 * 24 * 3600 = 6.3072e+22 FLOPs",RefinedWeb,"""Falcon-7B is a 7B parameters causal decoder-only model built by TII and trained on 1,500B tokens of RefinedWeb enhanced with curated corpora. It is made available under the Apache 2.0 license.""",1500000000000.0,"1125000000000.0 words  assuming 0.75 words per token (1.5T tokens)
""Falcon-7B is a 7B parameters causal decoder-only model built by TII and trained on 1,500B tokens of RefinedWeb enhanced with curated corpora. It is made available under the Apache 2.0 license.""",,"compute divided by flops per seconds
 6.3e22 / 3.e16 = 2100000.0 seconds = 583 hours = 24 days
compute (from other note) = 6.3e22
flops per seconds = (num gpu) * (peak flops) * (assumed utilization rate) = 
384*312e12*0.3 = 3.6e16",NVIDIA A100 SXM4 40 GB,,Confident,,,,Open weights (unrestricted),United Arab Emirates,,,,384.0,,2025-06-10 15:36,,,,,,Government,,,,,,"apache 2.0 for weights, ODC license (similar to apache) for data:
https://huggingface.co/datasets/tiiuae/falcon-refinedweb",Government,,,,306072.7310565314,"Operation counting,Reported",,,,
WizardLM 70B,Language,"Language modeling/generation,Chat","Microsoft,Peking University","Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, Daxin Jiang",2023-04-24,WizardLM: Empowering Large Language Models to Follow Complex Instructions,"https://huggingface.co/WizardLM/WizardLM-70B-V1.0
https://arxiv.org/abs/2304.12244",727.0,,,70000000000.0,70B,,,Evol Instruct,"""Fine-tuning dataset is made of LLM-generated instructions: ""In this work, we introduce Evol-Instruct, a novel method using LLMs instead of humans to automatically mass-produce open-domain instructions of various difficulty levels, to improve the performance of LLMs""",,,,,,,Confident,"Training large language models (LLMs) with open-domain instruction following data brings colossal success. However, manually creating such instruction data is very time-consuming and labor-intensive. Moreover, humans may struggle to produce high-complexity instructions. In this paper, we show an avenue for creating large amounts of instruction data with varying levels of complexity using LLM instead of humans. Starting with an initial set of instructions, we use our proposed Evol-Instruct to rewrite them step by step into more complex instructions. Then, we mix all generated instruction data to fine-tune LLaMA. We call the resulting model WizardLM. Human evaluations on a complexity-balanced test bed and Vicuna's testset show that instructions from Evol-Instruct are superior to human-created ones. By analyzing the human evaluation results of the high complexity part, we demonstrate that outputs from our WizardLM are preferred to outputs from OpenAI ChatGPT. In GPT-4 automatic evaluation, WizardLM achieves more than 90\% capacity of ChatGPT on 17 out of 29 skills. Even though WizardLM still lags behind ChatGPT in some aspects, our findings suggest that fine-tuning with AI-evolved instructions is a promising direction for enhancing LLMs. Our code and data are public at https://github.com/nlpxucan/WizardLM",,,Open weights (restricted use),"United States of America,Multinational,India,Belgium,China",Llama 2-70B,,,,,2025-05-12 15:54,,,,,,"Industry,Academia",,,,,Open source,"Llama 2 license for weights.

code is apache: https://github.com/nlpxucan/WizardLM/blob/main/WizardLM/CODE_LICENSE ","Industry,Academia",,,,,,,,,
ruGPT-3.5 13B,Language,"Chat,Language modeling/generation",Sber,,2023-04-24,ruGPT-3.5 13B,https://huggingface.co/ai-forever/ruGPT-3.5-13B,,,,13000000000.0,13B,1.0699776e+23,"""Model was trained using Deepspeed and Megatron libraries, on 300B tokens dataset for 3 epochs, around 45 days on 512 V100. After that model was finetuned 1 epoch with sequence length 2048 around 20 days on 200 GPU A100 on additional data""

512 GPUs * 125000000000000 FLOPs/s [peak] * 45 days * 24 hours * 3600 s * 0.3 + 200 GPUs * 312000000000000 FLOPs/s [peak for fp16] * 20 days * 24 hours * 3600 s * 0.3 = 1.0699776e+23

they probably used fp16 as in their similar project: https://habr.com/ru/companies/sberdevices/articles/780334/

6ND = 6*13B*300B*3 = 70200*10^18 = 7*10^24",,,300000000000.0,,1080.0,,"NVIDIA A100,NVIDIA Tesla V100 SXM2",,Confident,,3.0,,Open weights (unrestricted),Russia,,,,512.0,,2025-04-30 11:36,,,,,,"Industry,Government",,,,,Unreleased,"MIT license
https://huggingface.co/ai-forever/ruGPT-3.5-13B/discussions","Industry,Government",,,,,"Operation counting,Hardware",ai-forever,,,
MOSS-Moon-003,Language,Code generation,Fudan University,,2023-04-19,,https://huggingface.co/fnlp/moss-moon-003-base,,,,16000000000.0,16B,6.669999999999999e+22,"6.67e22 including pre-training for CodeGen:

""The base language model of MOSS-003, which was initialized with CodeGen and further pre-trained on 100B Chinese tokens and 20B English tokens. The model has seen 700B tokens during pre-training and consumed ~6.67x10^22 FLOPs in total.""
",,"""The base language model of MOSS was pre-trained on ~700B English, Chinese, and code tokens, including the PILE, BigQuery, BigPython, and our private Chinese corpus. The base model was then fine-tuned on multi-turn plugin-augmented conversational data""",700000000000.0,Ignoring tokens from fine-tuning; very likely small relative to pre-training data.,,,,,Confident,"MOSS is an open-sourced plugin-augmented conversational language model. moss-moon models have 16B parameters, allowing users to perform inference on a single A100 GPU or 2 NVIDIA 3090 GPUs with FP16 precision, and on a single NVIDIA 3090 GPU with INT-4/8 precision. The base language model of MOSS was pre-trained on ~700B English, Chinese, and code tokens, including the PILE, BigQuery, BigPython, and our private Chinese corpus. The base model was then fine-tuned on multi-turn plugin-augmented conversational data. Finally, we performed preference-aware training to further improve the model.",,,Open weights (unrestricted),China,CodeGen-Mono 16.1B,1,"""The base language model of MOSS-003, which was initialized with CodeGen and further pre-trained on 100B Chinese tokens and 20B English tokens. The model has seen 700B tokens during pre-training and consumed ~6.67x10^22 FLOPs in total.""


Using the proportion of tokens for fine-tuning against total tokens, we have:
6.67e22 * 120/(120+700) = 9.7e21.

However, the 6.67e22 might be *just* pre-training (this phrasing isn't clear). so that would be
6.67e22 * (120/700) = 1.14e22

alternatively, 16b * 120b * 6 = 1.15e22

I'll go with 1.14e22 but all these numbers are very close",,,2025-02-14 16:25,,,,,,Academia,,,,,Open source,"copyleft (permissive, but derivatives must also be open)
https://github.com/OpenMOSS/MOSS/blob/main/MODEL_LICENSE

finetune code (this model is a finetune): https://github.com/OpenMOSS/MOSS/blob/main/finetune_moss.py ",Academia,,,,,Operation counting,,,,
Claude 1.3,Language,"Language modeling,Chat",Anthropic,,2023-04-18,,https://twitter.com/AnthropicAI/status/1648353600350060545?lang=en,,,100k context window may have been SOTA at the time.,,,,,Unspecified unreleased,,,,,,,,Unknown,"We are offering a new version of our model, Claude-v1.3, that is safer and less susceptible to adversarial attacks.
For businesses using Claude, capabilities in all domains should stay the same or improve as you upgrade from previous versions. We always work to improve safety and performance in tandem.
This new model is already powering the Claude App for Slack and Claude+ in the Poe app, if youâ€™d like to try it out today!",,,API access,United States of America,,,,,,2025-06-11 17:18,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
LLaVA,"Multimodal,Vision,Language","Chat,Question answering,Visual question answering","University of Wisconsin Madison,Microsoft Research,Columbia University","Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee",2023-04-17,Visual Instruction Tuning,https://arxiv.org/abs/2304.08485,2482.0,SOTA improvement,"When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%.",13000000000.0,13B,7.8049e+22,"8 * 3.12e14 * (18 * 3600) * 0.3 = 4.9e19
num gpus * peak flops * time *assumed utilization rate 
""We train all models with 8Ã— A100s. Pretraining on CC-595K completes within 4 hours. Finetuning on Instruct-158K completes within 10 hours. Finetuning on ScienceQA completes within 4 hours."" so 18 hours of time in total.

However, they use Vicuna as their LLM backbone, which used 7.8e22 FLOPs in training. Total FLOPs are then 4.9e19 + 7.8e22 = 7.8049e22",Conceptual Captions (CC3M),"""We pre-train our model on the filtered CC-595K subset for 1 epoch with a learning rate of 2e-3 and a batch size of 128, and fine-tune on the proposed LLaVA-Instruct-158K dataset """,,"595K + 158K = 753K image text pairs
""This results in around 595K image-text pairs""
""We collect 158K unique language-image instruction-following samples in total, including 58K in conversations, 23K in detailed description, and 77k in complex reasoning, respectively. """,10.0,"""We train all models with 8Ã— A100s. Pretraining on CC-595K completes within 4 hours. Finetuning on Instruct-158K completes within 10 hours. Finetuning on ScienceQA completes within 4 hours.""",NVIDIA A100,,Confident,"Instruction tuning large language models (LLMs) using machine-generated instruction-following data has improved zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field. In this paper, we present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data, we introduce LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding.Our early experiments show that LLaVA demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%. We make GPT-4 generated visual instruction tuning data, our model and code base publicly available. ",,,Open weights (unrestricted),"United States of America,United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,United States of America",Vicuna-13B v0,49000000000000000000,"8 * 3.12e14 * (18 * 3600) * 0.3 = 4.9e19
num gpus * peak flops * time *assumed utilization rate 
""We train all models with 8Ã— A100s. Pretraining on CC-595K completes within 4 hours. Finetuning on Instruct-158K completes within 10 hours. Finetuning on ScienceQA completes within 4 hours."" so 18 hours of time in total.",8.0,,2025-05-28 16:05,,,,,,"Academia,Industry,Academia",,,,80.0,Open source,"apache 2.0

train repo: https://github.com/haotian-liu/LLaVA?tab=readme-ov-file#train 

model: https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md ","Academia,Industry,Academia",$42.46,,BF16,6377.509314719864,Hardware,,,,
BELLE-LLaMA-EXT-7B,Language,Language modeling/generation,KE Holdings Inc. (â€œBeikeâ€),"Yunjie Ji, Yan Gong, Yong Deng, Yiping Peng, Qiang Niu, Baochang Ma, Xiangang Li",2023-04-16,Towards Better Instruction Following Language Models for Chinese: Investigating the Impact of Training Data and Evaluation,https://arxiv.org/abs/2304.07854,,,,7000000000.0,,,,"Stanford-Alpaca,Vicuna ShareGPT Dataset,Belle-3.5","Alpaca-3.5-en(Taori et al., 2023), which is released by Stanford Alpaca and consists of 52K instruction-following samples. These samples are
generated by text-davinci-003.

Alpaca-3.5-zh(Cui and Yang, 2023), which is the translated Chinese version of alpaca-3.5-en.

Alpaca-4-en, Alpaca-4-zh(Peng et al., 2023), which are released by LLaMA-GPT4, both containing 52K instruction -following samples. These
samples are generated by GPT-4. To obtain alpaca-4-zh, Peng et al. (2023) first used ChatGPT to translate 52K instructions into Chinese then asked GPT-4 to answer them in Chinese.

ShareGPT(ShareGPT, 2023), which are usershared conversations with ChatGPT, consisting of 8.3K samples. We conduct three steps of data
cleaning (Chiang et al., 2023). Only English and Chinese conversations are kept. Besides, conversations are divided into smaller segments with a
maximum length of 2048 tokens. Finally, we derive 120,009 conversations.

Belle-3.5, which is our own dataset, consisting of instruction-following samples and multiturn conversations. This dataset contains 500,000
samples that are filtered out from 2.3M raw data with the cleaning method mentioned in section 3. To simplify dataset names while conducting experiments under different data settings, we define two functions for identifying the language of the given dataset. zh(d) means the Chinese version of d and en(d) means the English one.
",3400000000.0,"""LLaMA-EXT, which is obtained by extending
the vocabulary of the vanilla LLaMA and further
pre-train on 3.4B Chinese words in which only
word embeddings are updated.""

for Chinese language we assume 1 word ~ 1 token

Batch size 32
Max length 2048
training steps are not reported",,4.284e+20/312000000000000 FLOPs * 8 * 3600 * 0.3 ~ 159 hours ~ 6.6 days,NVIDIA A100 SXM4 80 GB,,Confident,"Recently, significant public efforts have been directed towards developing low-cost models with capabilities akin to ChatGPT, thereby fostering the growth of open-source conversational models. However, there remains a scarcity of comprehensive and in-depth evaluations of these models' performance. In this study, we examine the influence of training data factors, including quantity, quality, and linguistic distribution, on model performance. Our analysis is grounded in several publicly accessible, high-quality instruction datasets, as well as our own Chinese multi-turn conversations. We assess various models using a evaluation set of 1,000 samples, encompassing nine real-world scenarios. Our goal is to supplement manual evaluations with quantitative analyses, offering valuable insights for the continued advancement of open-source chat models. Furthermore, to enhance the performance and training and inference efficiency of models in the Chinese domain, we extend the vocabulary of LLaMA - the model with the closest open-source performance to proprietary language models like GPT-3 - and conduct secondary pre-training on 3.4B Chinese words. We make our model, data, as well as code publicly available.",3.0,,Open weights (non-commercial),China,LLaMA-7B,428400000000000000000,"""We conduct experiments on 8 A100 GPUs, each has 80G memory.""

6ND = 6*7*10^9*3.4*10^9*3 = 4.284e+20",8.0,,2025-05-01 10:42,,,,,,Industry,,,,,,"License: GNU General Public License v3.0 
Please refer License of Meta LLaMA Currently only for learning and communication. Please strictly observe the use restrictions of LLaMA. The LLaMA model does not allow the release of the complete model weights after adjustment, but can publish the diff of the original model. Therefore, we use XOR in the file room to ensure that talents with the authorization of the original model of LLaMA can transform the model released by this project into a usable format. ",Industry,,,BF16,6377.651339424,Operation counting,,,,
Suno Bark Model,"Speech,Audio","Text-to-speech,Audio generation",Suno,,2023-04-15,,"https://github.com/suno-ai/bark
https://huggingface.co/suno/bark",,,,300000000.0,,,,,,,,,,,,Confident,"Bark is a transformer-based text-to-audio model created by Suno. Bark can generate highly realistic, multilingual speech as well as other audio - including music, background noise and simple sound effects. The model can also produce nonverbal communications like laughing, sighing and crying. To support the research community, we are providing access to pretrained model checkpoints, which are ready for inference and available for commercial use.",,,Open weights (unrestricted),United States of America,,,,,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,MIT License,Industry,,,,,,,,,
DINOv2,Vision,Image representation,"Facebook AI Research,INRIA","Maxime Oquab, TimothÃ©e Darcet, ThÃ©o Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, HervÃ© Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, Piotr Bojanowski",2023-04-14,DINOv2: Learning Robust Visual Features without Supervision,https://arxiv.org/abs/2304.07193,1697.0,SOTA improvement,"""Our family of models drastically improves over
the previous state of the art in self-supervised learning and reaches performance comparable with weakly-
supervised features.""
",1140000000.0,1.14B from https://huggingface.co/facebook/dinov2-giant,7.41851136e+21,"table 14

22016 * 3600 * 312 * 10 ** 12 * 3/10 = 7.41851136e+21
gpu hours in seconds * flops of A100 * assumed utilization  rate",,new dataset  - named LVD142M Table 15,142000000.0,new dataset  - named LVD142M Table 15,,,NVIDIA A100 SXM4 40 GB,Self-supervised learning,Confident,"The recent breakthroughs in natural language processing for model pretraining on large quantities of data have opened the way for similar foundation models in computer vision. These models could greatly simplify the use of images in any system by producing all-purpose visual features, i.e., features that work across image distributions and tasks without finetuning. This work shows that existing pretraining methods, especially self-supervised methods, can produce such features if trained on enough curated data from diverse sources. We revisit existing approaches and combine different techniques to scale our pretraining in terms of data and model size. Most of the technical contributions aim at accelerating and stabilizing the training at scale. In terms of data, we propose an automatic pipeline to build a dedicated, diverse, and curated image dataset instead of uncurated data, as typically done in the self-supervised literature. In terms of models, we train a ViT model (Dosovitskiy et al., 2020) with 1B parameters and distill it into a series of smaller models that surpass the best available all-purpose features, OpenCLIP (Ilharco et al., 2021) on most of the benchmarks at image and pixel levels. ",,,Open weights (unrestricted),"United States of America,France",,,,,,2025-05-28 16:05,,,,,,"Industry,Academia",,,,,Open source,"apache 2.0

training code and weights here: https://github.com/facebookresearch/dinov2 ","Industry,Academia",$10203.61,,FP16,,Hardware,,,,
HuaTuo,Language,"Language modeling/generation,Question answering,Medical diagnosis",Harbin Institute of Technology,"Haochun Wang, Chi Liu, , Nuwa Xi, Zewen Qiang, Sendong Zhao, Bing Qin, Ting Liu",2023-04-14,"HuaTuo (åŽé©¼): Tuning LLaMA Model with Chinese Medical Knowledge
",https://arxiv.org/abs/2304.06975,185.0,,,7000000000.0,,,,,,,8000 instruction data (unknown length),2.3,"""We trained the model on an A100-SXM-80GB for a total of 10 epochs, which took approximately 2 hours and 17 minutes.""",NVIDIA A100 SXM4 80 GB,,Confident,"Large Language Models (LLMs), such as the LLaMA model, have demonstrated their effectiveness in various general-domain natural language processing (NLP) tasks. Nevertheless, LLMs have not yet performed optimally in biomedical domain tasks due to the need for medical expertise in the responses. In response to this challenge, we propose HuaTuo (åŽ é©¼), a LLaMA-based model that has been supervised-fine-tuned with generated QA (Question-Answer) instances. The experimental results demonstrate that HuaTuo generates responses that possess more reliable medical knowledge",10.0,,Open weights (non-commercial),China,LLaMA-7B,769392000000000000,"312000000000000 FLOP / GPU / sec [A100 reported, bf16 assumed] * 1 GPU * 137 minutes * 60 sec / minute * 0.3 [assumed utilization] = 7.69392e+17 FLOP",,,2025-06-09 11:09,,,,,,Academia,,,,,Open source,"Apache 2.0
https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese

Academic Free License v3.0 
https://huggingface.co/lovepon/lora-llama-literature
https://huggingface.co/thinksoso/lora-llama-med

Parameter settings and details training process can be found in the wandb log: https://wandb.ai/thinksoso/llama_med/runs/a5wgcnzt/overview?workspace=user-thinksoso",Academia,,,,,Hardware,,,,
Anthropic LM 52B,Language,"Language modeling/generation,Question answering",Anthropic,"Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, Jared Kaplan",2023-04-12,Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback,https://arxiv.org/abs/2204.05862,1695.0,,,52000000000.0,,,,,"The 52B preference model was trained on a mixture of helpfulness and harmlessness (red teaming) datasets collected by Anthropic using crowdworkers conversing with language models in a feedback interface.
The helpfulness dataset contains around 44k comparisons and the harmlessness dataset contains around 42k comparisons. The data consists of multi-turn dialogues where crowdworkers choose the more helpful or more harmful response at each turn.
The model was trained using a technique called preference model pretraining on additional datasets before finetuning on Anthropic's human feedback data.",,,,,,Reinforcement learning,Confident,"We apply preference modeling and reinforcement learning from human feedback (RLHF) to finetune language models to act as helpful and harmless assistants. We find this alignment training improves performance on almost all NLP evaluations, and is fully compatible with training for specialized skills such as python coding and summarization. We explore an iterated online mode of training, where preference models and RL policies are updated on a weekly cadence with fresh human feedback data, efficiently improving our datasets and models. Finally, we investigate the robustness of RLHF training, and identify a roughly linear relation between the RL reward and the square root of the KL divergence between the policy and its initialization. Alongside our main results, we perform peripheral analyses on calibration, competing objectives, and the use of OOD detection, compare our models with human writers, and provide samples from our models using prompts appearing in recent related work.",,,Unreleased,United States of America,,,,,,2025-05-09 11:32,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
Dolly 2.0-12b,Language,Chat,Databricks,"Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell, Matei Zaharia, Reynold Xin",2023-04-12,Free Dolly: Introducing the World's First Truly Open Instruction-Tuned LLM,https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm,,,,12000000000.0,,,,databricks-dolly-15k,Fine-tuned on instruction dataset with 15k examples.,3493333.0,(13.1 MB * 200M english words in GB / 1000) *4/3 = 3493333 tokens,,,,,Confident,"Dolly 2.0 is a 12B parameter language model based on the EleutherAI pythia model family and fine-tuned exclusively on a new, high-quality human generated instruction following dataset, crowdsourced among Databricks employees.

We are open-sourcing the entirety of Dolly 2.0, including the training code, the dataset, and the model weights, all suitable for commercial use. This means that any organization can create, own, and customize powerful LLMs that can talk to people, without paying for API access or sharing data with third parties.",,,Open weights (unrestricted),United States of America,Pythia-12b,251519980000000,"Trained on 15k question-answer examples (so fine-tune compute is probably minor)
6ND=6*12000000*3493333=2.5151998e+14",,,2024-09-05 14:08,,,,,,Industry,,,,,Open source,"""We are open-sourcing the entirety of Dolly 2.0, including the training code, the dataset, and the model weights, all suitable for commercial use.""

under a copyleft license:
https://creativecommons.org/licenses/by-sa/3.0/",Industry,,,,,,,,,
Incoder-6.7B,Language,Code generation,"Facebook AI Research,University of Washington,University of California (UC) Berkeley,Carnegie Mellon University (CMU),Toyota Technological Institute at Chicago","Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Wen-tau Yih, Luke Zettlemoyer, Mike Lewis",2023-04-09,InCoder: A Generative Model for Code Infilling and Synthesis,https://arxiv.org/abs/2204.05999,515.0,SOTA improvement,"""Zero-shot infilling with bidirectional context substantially outperforms approaches based on left-to-right-only models, and on several tasks
obtains performance comparable to state-of-the-art models fine-tuned on the tasks""",6700000000.0,6.7B,3.00001e+21,"per table 5, required 3 zettaflop (3e21) to train.

also, ""INCODER-6.7B was trained on 248 V100 GPUs for 24 days""

hardware method: 125 trillion * 248 * 24 * 24 * 3600 * 0.3 = 2e22. suggests their utilization was quite low, or 24 days was just calendar time.
",,"Code from GitHub and StackOverflow

""To train our models, we collect a corpus of (1) public code with permissive, non-copyleft, opensource licenses from GitHub and GitLab and (2) StackOverflow questions, answers, and comments.
Our primary focus in this paper is on the Python language, but we also include code files from
28 total languages and StackOverflow content from all available languages.""",,"216 GB: ""Our final pre-training corpus contains a total of 159 GB of code, 52 GB of it
in Python, and a total of 57 GB of content from StackOverflow""",576.0,24,NVIDIA V100,,Confident,"Code is seldom written in a single left-to-right pass and is instead repeatedly edited and refined. We introduce InCoder, a unified generative model that can perform program synthesis (via left-to-right generation) as well as editing (via infilling). InCoder is trained to generate code files from a large corpus of permissively licensed code, where regions of code have been randomly masked and moved to the end of each file, allowing code infilling with bidirectional context. Our model is the first generative model that is able to directly perform zero-shot code infilling, which we evaluate on challenging tasks such as type inference, comment generation, and variable re-naming. We find that the ability to condition on bidirectional context substantially improves performance on these tasks, while still performing comparably on standard program synthesis benchmarks in comparison to left-to-right only models pretrained at similar scale. The InCoder models and code are publicly released. this https URL",1.0,,Open weights (non-commercial),"United States of America,United States of America,United States of America,United States of America,United States of America",,,,,,2025-02-14 16:25,,,,,,"Industry,Academia,Academia,Academia,Academia",,,,,Unreleased,"CC-BY-NC 4.0 (non commercial)

data is open: ""To train our models, we collect a corpus of (1) public code with permissive, non-copyleft, opensource licenses from GitHub and GitLab and (2) StackOverflow questions, answers, and comments.""

inference code, not training code in this repo: https://github.com/dpfried/incoder/blob/main/README.md ","Industry,Academia,Academia,Academia,Academia",$3129.08,,,,Reported,,,,
gLM,Biology,Protein or nucleotide language model (pLM/nLM),Harvard University,"Yunha Hwang, Andre L. Cornman, Sergey Ovchinnikov, Peter R. Girguis",2023-04-08,Deep learning of genomic contexts predicts protein co-regulation and function,"https://www.biorxiv.org/content/10.1101/2023.04.07.536042v1.full
https://github.com/y-hwang/gLM",,,,1000000000.0,"""Our model consists of ~1B parameters which is at least a magnitude smaller  compared to state-of-the-art pLMs.""",2.26437e+20,"""The training stage takes several weeks on four NVIDIA A100 GPUs.""

Assumption: 3 weeks, 40% utilization rate, 78 TFLOP peak rate

Estimate: (3*7*24*3600) s * 78e12 FLOP/s *4 GPU * 0.4 = 2.3e20",MGnify,"""seven million metagenomic contig fragments consisting of 15 to 30 genes from the MGnify database""",30800001.0,"Number of datapoints = 30.8 Ã— 10^6

Calculations:
30.8 Ã— 10^6 = 30,800,000 unique datapoints/tokens in first epoch",,,NVIDIA A100,,Likely,"Deciphering the relationship between a gene and its genomic context is fundamental to understanding and engineering biological systems. Machine learning has shown promise in learning latent relationships underlying the sequence-structure-function paradigm from massive protein sequence datasets. However, to date, limited attempts have been made in extending this continuum to include higher order genomic context information. Evolutionary processes dictate the specificity of genomic contexts in which a gene is found across phylogenetic distances, and these emergent genomic patterns can be leveraged to uncover functional relationships between gene products. Here, we trained a genomic language model (gLM) on millions of metagenomic scaffolds to learn the latent functional and regulatory relationships between genes. gLM learns contextualized protein embeddings that capture the genomic context as well as the protein sequence itself, and appears to encode biologically meaningful and functionally relevant information (e.g. phylogeny, enzymatic function). Our analysis of the attention patterns demonstrates that gLM is learning co-regulated functional modules (i.e. operons). Our findings illustrate that gLMâ€™s unsupervised deep learning of the metagenomic corpus is an effective approach to encode functional semantics and regulatory syntax of genes in their genomic contexts, providing a promising avenue for uncovering complex relationships between  genes in a genomic region.",,,Open weights (non-commercial),United States of America,ESM2-650M,,,,,2025-04-30 10:04,,,,,,Academia,,,,,Open (non-commercial),"non-commercial license
train code:
https://github.com/y-hwang/gLM?tab=License-1-ov-file",Academia,,,,,Hardware,,,,
DiffDock-PP,Biology,Protein interaction prediction,"Technical University of Munich,Massachusetts Institute of Technology (MIT)","Mohamed Amine Ketata, Cedrik Laue, Ruslan Mammadov, Hannes StÃ¤rk, Menghua Wu, Gabriele Corso, CÃ©line Marquet, Regina Barzilay, Tommi S. Jaakkola",2023-04-08,DiffDock-PP: Rigid Protein-Protein Docking with Diffusion Models,https://arxiv.org/abs/2304.03889,38.0,,,1620000.0,,3.5382841e+16,"Using 6ND formula with 170 epochs
6*21413000*1620000*170=3.5382841e+16",DIPS,,21413000.0,"42,826 pairs approximated at 500 tokens
42826*500=21413000",,,,,Likely,"Understanding how proteins structurally interact is crucial to modern biology, with applications in drug discovery and protein design. Recent machine learning methods have formulated protein-small molecule docking as a generative problem with significant performance boosts over both traditional and deep learning baselines. In this work, we propose a similar approach for rigid protein-protein docking: DiffDock-PP is a diffusion generative model that learns to translate and rotate unbound protein structures into their bound conformations. We achieve state-of-the-art performance on DIPS with a median C-RMSD of 4.85, outperforming all considered baselines. Additionally, DiffDock-PP is faster than all search-based methods and generates reliable confidence estimates for its predictions. Our code is publicly available at $\texttt{this https URL}$",170.0,,,"Germany,United States of America",,,,,,2025-05-01 10:42,,,,,,"Academia,Academia",,,,,,,"Academia,Academia",,,,,,,,,
EvoMIL,Biology,Virus-host association prediction,"University of Glasgow,Cancer Research UK Beatson Institute","Dan Liu, Francesca Young, David L Robertson, Ke Yuan",2023-04-08,Prediction of virus-host association using protein language models and multiple instance learning,https://www.biorxiv.org/content/10.1101/2023.04.07.536023v1,2.0,,,,,,,,,,"""we collected 4696 associations 412
between 4696 viruses and 498 prokaryotic hosts at the species level; 9595 positive 413
associations from 9595 viruses and 1665 eukaryotic hosts at the species level. """,,,,,Unknown,"Predicting virus-host association is essential to understand how viruses interact with host species, and discovering new therapeutics for viral diseases across humans and animals. Currently, the host of the majority of viruses is unknown. Here, we introduce EvoMIL, a deep learning method that predicts virus-host association at the species level from viral sequence only. The method combines a pre-trained large protein language model and attention-based multiple instance learning to allow protein-orientated predictions. Our results show that protein embeddings capture stronger predictive signals than traditional handcrafted features, including amino acids and DNA k-mers, and physio-chemical properties. EvoMIL binary classifiers achieve AUC values of over 0.95 for all prokaryotic and nearly 0.8 for almost all eukaryotic hosts. In multi-host prediction tasks, EvoMIL achieved median performance improvements of 8.6% in prokaryotic hosts and 1.8% in eukaryotic hosts. Furthermore, EvoMIL estimates the importance of single proteins in the prediction and maps them to an embedding landscape of all viral proteins, where proteins with similar functions are distinctly clustered together.

Author summary Being able to predict which viruses can infect which hosts, and identifying the specific proteins that are involved in these interactions, is crucial for understanding viral diseases and developing more effective treatments. Traditional methods for predicting these interactions rely on handcrafted common features among proteins, overlooking the importance of single proteins. We have developed a new method that combines a protein language model and multiple instance learning to allow host prediction directly from protein sequences, without the need to extract handcrafted features. This method significantly improved multiple host association accuracy and revealed the key proteins involved in virus-host interactions.",,,,"United Kingdom of Great Britain and Northern Ireland,United Kingdom of Great Britain and Northern Ireland",ESM1b,,,,,2025-05-01 10:42,,,,,,Academia,,,,,,,Academia,,,,,,,,,
Cerebras-GPT-13B,Language,Language modeling,Cerebras Systems,"Nolan Dey, Gurpreet Gosal, Zhiming (Charles)Chen, Hemant Khachane, William Marshall, Ribhu Pathria, Marvin Tom, Joel Hestness",2023-04-06,Cerebras-GPT: Open Compute-Optimal Language Models Trained on the Cerebras Wafer-Scale Cluster,https://arxiv.org/abs/2304.03208,84.0,,,13000000000.0,13 billion,2.3e+22,"2.3e22, per table 2",The Pile,,278000000000.0,"371B tokens, or 278B words",,,Cerebras CS-2,,Confident,"We study recent research advances that improve large language models through efficient pre-training and scaling, and open datasets and tools. We combine these advances to introduce Cerebras-GPT, a family of open compute-optimal language models scaled from 111M to 13B parameters. We train Cerebras-GPT models on the Eleuther Pile dataset following DeepMind Chinchilla scaling rules for efficient pre-training (highest accuracy for a given compute budget). We characterize the predictable power-law scaling and compare Cerebras-GPT with other publicly-available models to show all Cerebras-GPT models have state-of-the-art training efficiency on both pre-training and downstream objectives. We describe our learnings including how Maximal Update Parameterization (Î¼P) can further improve large model scaling, improving accuracy and hyperparameter predictability at scale. We release our pre-trained models and code, making this paper the first open and reproducible work comparing compute-optimal model scaling to models trained on fixed dataset sizes. Cerebras-GPT models are available on HuggingFace: this https URL.",0.69,,Open weights (unrestricted),Multinational,,,,,,2025-06-12 14:51,,,,2210000.0,"""For the 13B parameter model, we train with a batch size of 720 sequences of length 2048 tokens for the first 84B tokens. At that point, we observed the gap between validation and train loss growing, indicating that the gradient noise was growing, so we increased the batch size to 1080 sequences for the rest of training.""

batch sizes ramp from 1.47M to 2.21M",Industry,,,,,Unreleased,"Apache 2.0 license
https://huggingface.co/cerebras/Cerebras-GPT-13B",Industry,,,,,Reported,cerebras,,,
Segment Anything Model,Vision,Image segmentation,Meta AI,"Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr DollÃ¡r, Ross Girshick",2023-04-05,Segment Anything,https://arxiv.org/abs/2304.02643,4323.0,Highly cited,,636000000.0,"From Facebook website: https://segment-anything.com/
""How big is the model? The image encoder has 632M parameters.
The prompt encoder and mask decoder have 4M parameters.""",7.8e+21,"""SAM was trained on 256 A100 GPUS for 68 hours. We acknowledge the environmental impact and cost of training
large scale models. The environmental impact of training the released SAM model is approximately 6963 kWh""

68*256 A100-hours = 
17408 hours * 3600 * 312 trillion * 0.4 (utilization assumption for image models)
= 7.82e21

max A100 power is 400W. 6,963,000 watt-hours / 400 watts = 17407.5 hours (so they probably just calculated backwards from power rating, and this doesn't give any info on utilization)",Segment Anything 1B,"""Dataset (Â§5). Our final dataset, SA-1B, includes more than
1B masks from 11M licensed and privacy-preserving images (see Fig. 2). SA-1B, collected fully automatically using the final stage of our data engine, has 400Ã— more masks
than any existing segmentation dataset [66, 44, 117, 60],
and as we verify extensively, the masks are of high quality
and diversity. Beyond its use in training SAM to be robust
and general, we hope SA-1B becomes a valuable resource
for research aiming to build new foundation models.""",1100000000.0,"""SA-1B contains 11M diverse, high-resolution, licensed, and privacy protecting images and 1.1B high-quality segmentation masks.""
segmentation mask is a map that identifies segments in an image",68.0,"""SAM was trained on 256 A100 GPUS for 68 hours""",NVIDIA A100,Supervised,Confident,"We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive -- often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and 11M images at this https URL to foster research into foundation models for computer vision.",2.0,,Open weights (unrestricted),United States of America,ViT-Huge/14,7,see Training Compute notes,256.0,,2025-05-28 16:05,,,,,,Industry,,,,,Unreleased,"Apache 2.0 license
don't see pretrain code in the repo, could be wrong

https://github.com/facebookresearch/segment-anything",Industry,$15888.41,,FP32,204134.84223782964,Hardware,,,,
BELLE-7B-2M,Language,Language modeling/generation,KE Holdings Inc. (â€œBeikeâ€),,2023-04-05,,https://huggingface.co/BelleGroup/BELLE-7B-2M,,,,7000000000.0,,,,Stanford-Alpaca,,,"2M examples
Batch size	64",,,,,Confident,"BELLE is based on Bloomz-7b1-mt and finetuned with 2M Chinese data combined with 50,000 pieces of English data from the open source Stanford-Alpaca, resulting in good Chinese instruction understanding and response generation capabilities.

The code of Chinese data generation and other detailed information can be found in our Github project repository: https://github.com/LianjiaTech/BELLE.

We trained models using datasets of different sizes (200,000, 600,000, 1,000,000, and 2,000,000 samples) for instruction learning, and we obtained different model versions as shown below:",3.0,,Open weights (unrestricted),China,BLOOM-7.1B,,,,,2025-05-01 10:42,,,,,,Industry,,,,,,"Apache 2.0
",Industry,,,,,,,,,
Pythia-12b,Language,"Language modeling/generation,Question answering","EleutherAI,""Booz Allen Hamilton, McLean"",University of Cambridge,""Indraprastha Institute of Information Technology
Delhi"",Stability AI,datasaur.ai,University of Amsterdam","Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, Oskar van der Wal",2023-04-03,Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling,https://arxiv.org/abs/2304.01373,891.0,,,12000000000.0,See Table 1 for non-embedding parameters,2.1590000000001e+22,https://www.wolframalpha.com/input?i=6+FLOP+*+12+billion+*+299892736000,The Pile,,299892736000.0,"""We train all models for 299,892,736,000 â‰ˆ 300B tokens""",,,NVIDIA A100 SXM4 40 GB,,Confident,"How do large language models (LLMs) develop and evolve over the course of training? How do these patterns change as models scale? To answer these questions, we introduce \textit{Pythia}, a suite of 16 LLMs all trained on public data seen in the exact same order and ranging in size from 70M to 12B parameters. We provide public access to 154 checkpoints for each one of the 16 models, alongside tools to download and reconstruct their exact training dataloaders for further study. We intend \textit{Pythia} to facilitate research in many areas, and we present several case studies including novel results in memorization, term frequency effects on few-shot performance, and reducing gender bias. We demonstrate that this highly controlled setup can be used to yield novel insights toward LLMs and their training dynamics. Trained models, analysis code, training code, and training data can be found at",1.0,Pythia-12b,Open weights (unrestricted),"Multinational,United States of America,United States of America,United Kingdom of Great Britain and Northern Ireland,India,United Kingdom of Great Britain and Northern Ireland,United States of America,Netherlands",,,,256.0,0.2659,2025-02-14 16:26,Amazon Web Services,AWS US East,,2097152.0,"""The most notable divergence from standard training procedures is that we use a much larger batch size than what is standard for training small language models... we use a batch size of 1024 samples with a sequence length of 2048 (2,097,152 tokens) for all models""","Research collective,Industry,Academia,Academia,Industry,Industry,Academia",,,,72300.0,Open source,apache 2.0 for model/code/data,"Research collective,Industry,Academia,Academia,Industry,Industry,Academia",,"Ops-counting: 2.159e22 FLOP
Table 5: 72300 A100 hours = 72300 * 3600 * 3.12e14 = 8.121e22 FLOP at max utilization
2.159e22 / 8.1207e22 = 0.2659",,204143.93434948783,Operation counting,,,,
Yiye Qingzhou,"Multimodal,Language,Vision,Speech,Video","Language modeling/generation,Question answering,Visual question answering,Speech recognition,Video description",EFFYIC (è¯†å› æ™ºèƒ½),,2023-04-01,"36æ°ªé¦–å‘ | ä¸“æ³¨ä¼ä¸šçº§å¤§æ¨¡åž‹è½åœ°ï¼Œã€Œè¯†å› æ™ºèƒ½ã€å®Œæˆæ•°åƒä¸‡å…ƒé¦–è½®èžèµ„
","https://36kr.com/p/3062982772761984
https://www.effyic.com/technical_core",,,,540000000000.0,540B,,,Unspecified unreleased,,,,,,,,Confident,,,,,China,,,,,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
Vicuna-13B v0,Language,"Language modeling/generation,Chat","Large Model Systems Organization,University of California (UC) Berkeley","Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, Eric P. Xing",2023-03-30,Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality,"https://lmsys.org/blog/2023-03-30-vicuna/

https://github.com/lm-sys/FastChat/blob/main/docs/vicuna_weights_version.md",,,,13000000000.0,,,"Fine-tuning cost $300, so training compute is ~entirely based on underlying Llama-13B weights, which took 7.8e22 FLOPs

6*13*10^9*207200000 = 1.61616e+19
unsure about amount of epochs ->  lower bound, Likely confidence",Vicuna ShareGPT Dataset,,207200000.0,"70K conversations from ShareGPT.com, a website where users can share their ChatGPT conversations.

they later published a paper with more data (125k convesations = 370M tokens) https://arxiv.org/abs/2306.05685

70*370/125 = 207.2M",,,NVIDIA A100,,Likely,"We introduce Vicuna-13B, an open-source chatbot trained by fine-tuning LLaMA on user-shared conversations collected from ShareGPT. Preliminary evaluation using GPT-4 as a judge shows Vicuna-13B achieves more than 90%* quality of OpenAI ChatGPT and Google Bard while outperforming other models like LLaMA and Stanford Alpaca in more than 90%* of cases. The cost of training Vicuna-13B is around $300. The code and weights, along with an online demo, are publicly available for non-commercial use.",,,Open weights (restricted use),"United States of America,United States of America",LLaMA-13B,,,8.0,,2025-05-01 10:42,,,,,,"Academia,Academia",,1.61616e+19,,,Open source,"Llama 2 license for weights (only delta weights, not merged), Apache for code.

train script: https://github.com/lm-sys/FastChat/blob/main/scripts/train_vicuna_13b.sh ","Academia,Academia",$300.00,,,6380.066243365717,Operation counting,,,,
BloombergGPT,Language,Language modeling,"Bloomberg,Johns Hopkins University","Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, Gideon Mann",2023-03-30,BloombergGPT: A Large Language Model for Finance,https://arxiv.org/abs/2303.17564,556.0,SOTA improvement,"""We validate BloombergGPT on standard LLM benchmarks, open financial benchmarks, and a suite of internal benchmarks that most accurately reflect our intended usage. Our mixed dataset training leads to a model that outperforms existing models on financial tasks by significant margins without sacrificing performance on general LLM benchmarks.""",50558868480.0,,2.36e+23,"2.36e23 per Table 4

(using our usual hardware method, 512 A100s over 53 days would be 512 * 312 teraFLOP/s * 53 * 24 * 3600 * 0.3 = 2.19e23)",,"""To train BloombergGPT, we construct â€œFinPileâ€, a comprehensive dataset consisting of a range of English financial documents including news, filings, press releases, web-scraped financial documents, and social media drawn from the Bloomberg archives. These documents have been acquired through our business process over the past two decades. We augment FinPile with public data widely used to train LLMs. The result is a training corpus that is roughly half domain-specific text and half general-purpose text.""",532000000000.0,"708.9 billion tokens. At 0.75 English words per token, that's 532B words",1270.0,"""~53 days""",NVIDIA A100,Self-supervised learning,Confident,"The use of NLP in the realm of financial technology is broad and complex, with applications ranging from sentiment analysis and named entity recognition to question answering. Large Language Models (LLMs) have been shown to be effective on a variety of tasks; however, no LLM specialized for the financial domain has been reported in literature. In this work, we present BloombergGPT, a 50 billion parameter language model that is trained on a wide range of financial data. We construct a 363 billion token dataset based on Bloomberg's extensive data sources, perhaps the largest domain-specific dataset yet, augmented with 345 billion tokens from general purpose datasets. We validate BloombergGPT on standard LLM benchmarks, open financial benchmarks, and a suite of internal benchmarks that most accurately reflect our intended usage. Our mixed dataset training leads to a model that outperforms existing models on financial tasks by significant margins without sacrificing performance on general LLM benchmarks. Additionally, we explain our modeling choices, training process, and evaluation methodology. We release Training Chronicles (Appendix C) detailing our experience in training BloombergGPT.",0.8,,Unreleased,"United States of America,United States of America",,,,512.0,0.327,2025-05-28 16:06,,,,4200000.0,"""in the first 7,200 steps, we use a batch size of 1,024 (2.1M tokens), then switch to a batch size of 2,048 (4.2M tokens) for the remainder of training.""","Industry,Academia",checked,,,650240.0,Unreleased,,"Industry,Academia",$369586.14,"Table 4 indicates an average of 102e12 FLOP/sec per A100 during training. 102e12 / 312e12 = 0.327. Confirmed by calculating manually with number of steps and time per step.
Note also: ""Since we adopt activation checkpointing to reduce our memory footprint, this costs us an additional 0.33x TFLOPs per iteration due to repeated forward passes.""",BF16,408324.2395754059,"Reported,Hardware",,,,
Vicuna-7B v0,Language,Language modeling/generation,"Large Model Systems Organization,University of California (UC) Berkeley","Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, Eric P. Xing",2023-03-30,Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality,"https://lmsys.org/blog/2023-03-30-vicuna/

https://github.com/lm-sys/FastChat/blob/main/docs/vicuna_weights_version.md",,,,7000000000.0,,,"Fine-tuning cost $140, so unlikely to have contributed significantly. Model is fine-tuned from Llama-7B, which used 4.02e+22

6*7*10^9*207200000 = 8.7024e+18
unsure about amount of epochs -> lower bound, Likely confidence",Vicuna ShareGPT Dataset,,207200000.0,"70K conversations from ShareGPT.com, a website where users can share their ChatGPT conversations.

they later published a paper with more data (125k convesations = 370M tokens) https://arxiv.org/abs/2306.05685

70*370/125 = 207.2M",,,NVIDIA A100,,Likely,,,,Open weights (restricted use),"United States of America,United States of America",LLaMA-7B,,,8.0,,2025-05-01 10:42,,,,,,"Academia,Academia",,8.7024e+18,,,Open source,"Llama 2 license for weights (only delta weights, not merged), Apache for code: 
https://github.com/lm-sys/FastChat?tab=readme-ov-file#model-weights 
https://github.com/lm-sys/FastChat?tab=readme-ov-file#fine-tuning ","Academia,Academia",$140.00,,,6380.066243365717,Operation counting,,,,
VideoMAE V2,Video,Action recognition,"Nanjing University,Shenzhen Institute of Advanced Technology,Shanghai AI Lab","Limin Wang, Bingkun Huang, Zhiyu Zhao, Zhan Tong, Yinan He, Yi Wang, Yali Wang, Yu Qiao",2023-03-29,VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking,https://arxiv.org/abs/2303.16727v2,222.0,SOTA improvement,"""Finally, we successfully train a video ViT model with a
billion parameters, which achieves a new state-of-the-art
performance on the datasets of Kinetics (90.0% on K400
and 89.9% on K600) and Something-Something (68.7% on
V1 and 77.0% on V2).""",1000000000.0,1B,9.7e+21,"finetuned on ViT-g (smaller than ViT-G with 1B params)

""It takes more than two weeks to pre-train a ViT-g model with VideoMAE
on 64 A100 GPUs""

64 * 312 trillion * 2 * 7 * 24 * 3600 * 0.4 (utilization assumption) = 9.7e21",,"""To well support the billion-level ViT model pretraining, we build two large-scale video datasets for our proposed progressive training. For self-supervised pre-training of VideoMAE V2, we build a million-level unlabeled video
dataset by collecting clips from multiple resources such
as Movie, Youtube, Instagram, General Webs, and manual recordings from scripts, and the dataset is termed as
UnlabeledHybrid""",,"1.35 million video clips. Not sure about average length (34 seconds, but that's only reported for Instagram portion).

""In total, there are around 1.35M clips in our mixed dataset and
this is the largest dataset ever used for video masked autoencoding.",336.0,2 weeks,NVIDIA A100 SXM4 80 GB,Self-supervised learning,Confident,"Scale is the primary factor for building a powerful foundation model that could well generalize to a variety of downstream tasks. However, it is still challenging to train video foundation models with billions of parameters. This paper shows that video masked autoencoder (VideoMAE) is a scalable and general self-supervised pre-trainer for building video foundation models. We scale the VideoMAE in both model and data with a core design. Specifically, we present a dual masking strategy for efficient pre-training, with an encoder operating on a subset of video tokens and a decoder processing another subset of video tokens. Although VideoMAE is very efficient due to high masking ratio in encoder, masking decoder can still further reduce the overall computational cost. This enables the efficient pre-training of billion-level models in video. We also use a progressive training paradigm that involves an initial pre-training on a diverse multi-sourced unlabeled dataset, followed by a post-pre-training on a mixed labeled dataset. Finally, we successfully train a video ViT model with a billion parameters, which achieves a new state-of-the-art performance on the datasets of Kinetics (90.0% on K400 and 89.9% on K600) and Something-Something (68.7% on V1 and 77.0% on V2). In addition, we extensively verify the pre-trained video ViT models on a variety of downstream tasks, demonstrating its effectiveness as a general video representation learner. The code and model is available at \url{this https URL}.",1200.0,,Open weights (unrestricted),"China,China,China",ViT-G/14,9,"finetuned on ViT-g (smaller than ViT-G with 1B params)

""It takes more than two weeks to pre-train a ViT-g model with VideoMAE
on 64 A100 GPUs""

64 * 312 trillion * 2 * 7 * 24 * 3600 * 0.4 (utilization assumption) = 9.7e21
",64.0,,2025-05-28 16:06,,,,,,"Academia,Academia",,,,,Open source,"MIT
https://github.com/OpenGVLab/VideoMAEv2/blob/master/LICENSE","Academia,Academia",$18339.97,,FP16,51041.66660009344,Hardware,,,,
ERNIE-ViLG 2.0,Image generation,Image generation,"Baidu,Wuhan University of Science and Technology","Zhida Feng, Zhenyu Zhang, Xintong Yu, Yewei Fang, Lanxin Li, Xuyi Chen, Yuxiang Lu, Jiaxiang Liu, Weichong Yin, Shikun Feng, Yu Sun, Li Chen, Hao Tian, Hua Wu, Haifeng Wang",2023-03-28,ERNIE-ViLG 2.0: Improving Text-to-Image Diffusion Model with Knowledge-Enhanced Mixture-of-Denoising-Experts,https://arxiv.org/abs/2210.15257,,,,24000000000.0,"""we train ERNIEViLG 2.0 and scale up the model size to 24B parameters.""",4.658135e+22,312000000000000*432*3600*320*0.3 = 4.658135e+22,"LAION-400M,Unspecified unreleased",,170000000.0,"""The training data consists of 170M image-text pairs, including publicly available English datasets like LAION [28] and a series of internal Chinese datasets. The image autoencoder is trained on the same set. For images with English captions, we translate them with Baidu Translate API3 to get the Chinese version.""",432.0,"""We train ERNIE-ViLG 2.0 on 320 Tesla A100 GPUs for 18 days.""

18*24=432",NVIDIA A100,,Confident,"Recent progress in diffusion models has revolutionized the popular technology of text-to-image generation. While existing approaches could produce photorealistic high-resolution images with text conditions, there are still several open problems to be solved, which limits the further improvement of image fidelity and text relevancy. In this paper, we propose ERNIE-ViLG 2.0, a large-scale Chinese text-to-image diffusion model, to progressively upgrade the quality of generated images by: (1) incorporating fine-grained textual and visual knowledge of key elements in the scene, and (2) utilizing different denoising experts at different denoising stages. With the proposed mechanisms, ERNIE-ViLG 2.0 not only achieves a new state-of-the-art on MS-COCO with zero-shot FID score of 6.75, but also significantly outperforms recent models in terms of image fidelity and image-text alignment, with side-by-side human evaluation on the bilingual prompt set ViLG-300.",,,API access,"China,China",,,,320.0,,2025-05-01 10:42,,,,,,"Industry,Academia",,,,,Unreleased,https://wenxin.baidu.com/ernie-vilg,"Industry,Academia",,,,255214.01639286996,Hardware,,,,
SigLIP 400M,Vision,"Image classification,Image embedding",Google DeepMind,"Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, Lucas Beyer",2023-03-27,Sigmoid Loss for Language Image Pre-Training,https://arxiv.org/abs/2303.15343,,Significant use,,400000000.0,Table 3,4.9467301e+21,"Operation Counting: 
6ND = 6 FLOP / token / parameter*400*10^6 parameters * 6705000000000 tokens [see Dataset size notes] = 1.6092e+22 FLOP

Hardware:
275000000000000 FLOP/s/GPU * 32 GPUs * 120 hours * 3600 sec / hour * 0.4 = 1.52064e+21 FLOPs

geometric mean (1.6092e+22, 1.52064e+21) = 4.9467301e+21",WebLI,"""We pre-train SigLIP models on the WebLI dataset""",6705000000000.0,"""B/16 ViT for image embeddings and B-sized transformer for text embeddings. The input images are resized to 224Ã—224 resolution.""

""SigLIP performs best at batch size 32 k [image-text pairs]"" 

729 patches (table 3)

""a maximum of 16 text tokens are kept""

9B examples * (16 text tokens + 729 image tokens) = 6.705e+12 total training tokens",120.0,5 days = 120 hours,Google TPU v4,,Confident,"We propose a simple pairwise Sigmoid loss for Language-Image Pre-training (SigLIP). Unlike standard contrastive learning with softmax normalization, the sigmoid loss operates solely on image-text pairs and does not require a global view of the pairwise similarities for normalization. The sigmoid loss simultaneously allows further scaling up the batch size, while also performing better at smaller batch sizes. Combined with Locked-image Tuning, with only four TPUv4 chips, we train a SigLiT model that achieves 84.5% ImageNet zero-shot accuracy in two days. The disentanglement of the batch size from the loss further allows us to study the impact of examples vs pairs and negative to positive ratio. Finally, we push the batch size to the extreme, up to one million, and find that the benefits of growing batch size quickly diminish, with a more reasonable batch size of 32k being sufficient. We release our models at this https URL and hope our research motivates further explorations in improving the quality and efficiency of language-image pre-training.",,,Open weights (unrestricted),"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France",,,,32.0,,2025-06-16 14:46,,,,,,Industry,,,,,Unreleased,"Apache2 license

https://github.com/google-research/big_vision

https://colab.research.google.com/github/google-research/big_vision/blob/main/big_vision/configs/proj/image_text/SigLIP_demo.ipynb

code release is still pending as ""TODO""",Industry,,,,10846.837246253186,"Hardware,Operation counting",,,,
SigLiT,Vision,Image classification,Google DeepMind,"Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, Lucas Beyer",2023-03-27,Sigmoid Loss for Language Image Pre-Training,https://arxiv.org/abs/2303.15343,,,,,,7.6032e+19,275000000000000 FLOP/s *48 hours *4 GPUs *3600 sec / hour *0.4 = 7.6032e+19 FLOP,,"""With a ViT-g/14 [58] model as the vision tower and a Large text tower, we can train at 20 k [image-text pairs] batch size on four chips for 107 k steps in under two days.""",,,48.0,2 days = 48 hours,Google TPU v4,,Confident,"We propose a simple pairwise Sigmoid loss for Language-Image Pre-training (SigLIP). Unlike standard contrastive learning with softmax normalization, the sigmoid loss operates solely on image-text pairs and does not require a global view of the pairwise similarities for normalization. The sigmoid loss simultaneously allows further scaling up the batch size, while also performing better at smaller batch sizes. Combined with Locked-image Tuning, with only four TPUv4 chips, we train a SigLiT model that achieves 84.5% ImageNet zero-shot accuracy in two days. The disentanglement of the batch size from the loss further allows us to study the impact of examples vs pairs and negative to positive ratio. Finally, we push the batch size to the extreme, up to one million, and find that the benefits of growing batch size quickly diminish, with a more reasonable batch size of 32k being sufficient. We release our models at this https URL and hope our research motivates further explorations in improving the quality and efficiency of language-image pre-training.",,,Open weights (unrestricted),"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France",,,,4.0,,2025-05-01 10:42,,,,,,Industry,,,,,,Apache2 license,Industry,,,,1355.8546557816485,Hardware,,,,
EVA-CLIP (EVA-02-CLIP-E/14+)																											,Vision,Image classification,"Beijing Academy of Artificial Intelligence / BAAI,Huazhong University of Science and Technology","Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, Yue Cao",2023-03-27,EVA-CLIP: Improved Training Techniques for CLIP at Scale,https://arxiv.org/abs/2303.15389	,,,,5000000000.0,"5b (table 1(a))

image parameters: 4.4B 
text parameters: 695M",3.4560000000000004e+22,6 FLOP / token / parameter * 5*10^9 parameters * 2304000000000/2 tokens [see dataset size notes] = 3.456e+22 FLOP,LAION-2B,,2304000000000.0,"from table 1(a):
9B samples seen 
image size 224^2
batch size: 144k samples 

9*10^9*(224/14)^2 = 2.304e+12 image tokens 
50% of patches are randomly masked (to account for it when estimating compute)",,,NVIDIA RTX A1000,,Confident,"Contrastive language-image pre-training, CLIP for short, has gained increasing attention for its potential in various scenarios. In this paper, we propose EVA-CLIP, a series of models that significantly improve the efficiency and effectiveness of CLIP training. Our approach incorporates new techniques for representation learning, optimization, and augmentation, enabling EVA-CLIP to achieve superior performance compared to previous CLIP models with the same number of parameters but significantly smaller training costs. Notably, our largest 5.0B-parameter EVA-02-CLIP-E/14+ with only 9 billion seen samples achieves 82.0 zero-shot top-1 accuracy on ImageNet-1K val. A smaller EVA-02-CLIP-L/14+ with only 430 million parameters and 6 billion seen samples achieves 80.4 zero-shot top-1 accuracy on ImageNet-1K val. To facilitate open access and open research, we release the complete suite of EVA-CLIP to the community at this https URL.",,,Open weights (unrestricted),"China,China",,,,144.0,,2025-05-30 14:27,,,,144000.0,,"Academia,Academia",,,,,Unreleased,"https://huggingface.co/QuanSun/EVA-CLIP
MIT license

the code here seems to be only inference code
https://github.com/baaivision/EVA/tree/master/EVA-CLIP","Academia,Academia",,,BF16,17227.329744049177,Operation counting,QuanSun,,,
SigLIP,Vision,"Image classification,Zero-shot image classification",Google DeepMind,"Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, Lucas Beyer",2023-03-27,Sigmoid Loss for Language Image Pre-Training,https://arxiv.org/abs/2303.15343,,,,6300000000.0,"Estimated from compute estimate / 6 / approximate datapoint count
1.14e21/6/30000000000 = 6.3B params",1.1400000000009998e+21,"From Table 1: 5 days of training on 32 TPUv4. Assuming 30% utilization, 16-bit precision. ",WebLI,"""We pre-train SigLIP models on the WebLI dataset [13], using only English image and text pairs.""",30000000000.0,"From ""Pali: A jointly-scaled multilingual language-image model"", ""To train PaLI-17B, we build a new high-volume image-and-language dataset WebLI, which consists of 10 billion images and tens of billions of image-text pairs.""",120.0,Table 1: 5 days,Google TPU v4,,Likely,"We propose a simple pairwise Sigmoid loss for Language-Image Pre-training (SigLIP). Unlike standard contrastive learning with softmax normalization, the sigmoid loss operates solely on image-text pairs and does not require a global view of the pairwise similarities for normalization. The sigmoid loss simultaneously allows further scaling up the batch size, while also performing better at smaller batch sizes. Combined with Locked-image Tuning, with only four TPUv4 chips, we train a SigLiT model that achieves 84.5% ImageNet zero-shot accuracy in two days. The disentanglement of the batch size from the loss further allows us to study the impact of examples vs pairs and negative to positive ratio. Finally, we push the batch size to the extreme, up to one million, and find that the benefits of growing batch size quickly diminish, with a more reasonable batch size of 32k being sufficient. We release our models at this https URL and hope our research motivates further explorations in improving the quality and efficiency of language-image pre-training.",,,,"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France",,,,32.0,,2025-05-01 10:42,,,,32000.0,From Table 1: SigLit and SigLIP results,Industry,,,,3840.0,,,Industry,,,,10846.837246253186,Hardware,,,,
Lightweight Fine-tuning a Pretrained Protein Language Model for Protein Secondary,Biology,"Protein or nucleotide language model (pLM/nLM),Protein folding prediction",Henan University,"Wei Yang, Chun Liu, Zheng Li",2023-03-23,Lightweight Fine-tuning a Pretrained Protein Language Model for Protein Secondary Structure Prediction,https://www.biorxiv.org/content/10.1101/2023.03.22.530066v1.abstract,,,,,,1.8710548688e+22,"1. Hardware setup: 1x NVIDIA GeForce RTX 3090 Ti (1.60 x 10^14 FP16 FLOP/s)

2. Training duration: Directly provided - 2,200 seconds/epoch Ã— 43 epochs = 94,600 seconds

3. Utilization rate: 40%

4. Final calculation: 1.60 Ã— 10^14 FLOP/s Ã— 94,600 seconds Ã— 0.4 = 6.05 Ã— 10^18 FLOPs

Base model: 18704498688000000000000
Total: 18704498688000000000000+6.05 Ã— 10^18=18710548688000000000000",,,10000001.0,"25,792 training chains Ã— 400 residues = 10,316,800 tokens â‰ˆ 1.0e7",27.0,,NVIDIA GeForce RTX 3090 Ti,,Confident,"Pretrained large-scale protein language models, such as ESM-1b and ProtTrans, are becoming the fundamental infrastructure for various protein-related biological modeling tasks. Existing works use mainly pretrained protein language models in feature extraction. However, the knowledge contained in the embedding features directly extracted from a pretrained model is task-agnostic. To obtain task-specific feature representations, a reasonable approach is to fine-tune a pretrained model based on labeled datasets from downstream tasks. To this end, we investigate the fine-tuning of a given pretrained protein language model for protein secondary structure prediction tasks. Specifically, we propose a novel end-to-end protein secondary structure prediction framework involving the lightweight fine-tuning of a pretrained model. The framework first introduces a few new parameters for each transformer block in the pretrained model, then updates only the newly introduced parameters, and then keeps the original pretrained parameters fixed during training. Extensive experiments on seven test sets, namely, CASP12, CASP13, CASP14, CB433, CB634, TEST2016, and TEST2018, show that the proposed framework outperforms existing predictors and achieves new state-of-the-art prediction performance. Furthermore, we also experimentally demonstrate that lightweight fine-tuning significantly outperforms full model fine-tuning and feature extraction in enabling models to predict secondary structures. Further analysis indicates that only a few top transformer blocks need to introduce new parameters, while skipping many lower transformer blocks has little impact on the prediction accuracy of secondary structures.",,,,China,ProtT5-XL-U50,6050000000000000000,,1.0,,2025-05-01 10:42,,,,,,Academia,,,,,,,Academia,,,,493.0421354054348,Hardware,,,,
Sparse Wide GPT-3 Small,Language,"Language modeling/generation,Question answering",Cerebras Systems,"Shreyas Saxena, Vithursan Thangarasa, Abhay Gupta, Sean Lie",2023-03-21,Sparse Iso-FLOP Transformations for Maximizing Training Efficiency,https://arxiv.org/abs/2303.11525,3.0,,,1300000000.0,"GPT-3 Small: 125M
At 90% sparsity â†’ ~1.25B total parameters
At 75% sparsity â†’ 125M * ksw^2 = 125M *(1/(1-0.75)) = 500M total pramters (I assume only 125M are active) ",1.875e+18,"6 FLOP / parameter / token * 125* 10^6 active parameters * 2.5 * 10^9 tokens = 1.875e+18 FLOP

__________________
in the Algorithmic progress paper the estimation was 8.84 Ã— 10^19 FLOP, they assumed WT-103 dataset (not mentioned in the paper) and different number of parameters (1.3* 10^9 - I am unsure where it comes from)",The Pile,the Pile contains 341173367965 tokens,2500000000.0,"2.5B (table 13)
",,,Cerebras CS-2,,Speculative,"Recent research has focused on weight sparsity in deep neural network training to reduce FLOPs, aiming for improved efficiency (test accuracy w.r.t training FLOPs). However, sparse weight training often compromises accuracy, requiring extended training schedules to attain the accuracy of dense models. In contrast, our approach, Sparse Iso-FLOP Transformations (Sparse-IFT), uses sparsity to improve accuracy while maintaining dense model FLOPs. Using a single hyperparameter (i.e., the sparsity level), Sparse-IFTs efficiently replace dense layers, expanding the search space for optimal sparse masks. In addition, dynamic sparse training (DST) with Sparse-IFT models effectively navigate this larger sparse mask-weight space, which is evidenced by a spectral analysis using Ramanujan graph properties. Our study reveals a robust correlation among mask topology, weights, and final performance. Notably, without adjusting any training hyperparameters, replacing dense layers with Sparse-IFT yields significant improvements, such as a +3.5% boost for ResNet-18 on ImageNet and +0.9% for GPT-3 Small on the Open LLM leaderboard. To the best of our knowledge, this is the first work to demonstrate the use of sparsity for improving the accuracy of dense models through a set of simple-to-use sparse transformations. Code is available at: this https URL.",1.0,Sparse Wide GPT-3 Small,Unreleased,Multinational,,,,,,2025-04-21 12:45,,,,,,Industry,,,,,Unreleased,paper has repo but no code for sparse GPT-3: https://github.com/CerebrasResearch/Sparse-IFT ,Industry,,,,,Operation counting,,,,
LightOn Mini,Language,"Language modeling/generation,Chat",LightOn,,2023-03-21,LightOn's Large Language Model of 40 billion parameters: MINI,https://www.lighton.ai/blog/lighton-s-blog-4/lighton-s-large-language-model-of-40-billion-parameters-mini-19,,,,40000000000.0,"""Boasting an impressive 40 billion parameters, Mini is a formidable addition to the growing array of language models available in the market today.""",2.4e+23,6ND aproximation: 6*40B*1T = 2.4e23,,"""The amount of data in Mini corpus is 1 trillion tokens. We mainly used data from the public web to pre-train our model, with strong filtering, toxicity reduction, and deduplication to ensure that only high-quality data is retained.""",1000000000000.0,"""The amount of data in Mini corpus is 1 trillion tokens. We mainly used data from the public web to pre-train our model, with strong filtering, toxicity reduction, and deduplication to ensure that only high-quality data is retained.""  assuming 0.75 words per token - 750000000000.0 words",,,,Self-supervised learning,Confident,,,,Hosted access (no API),France,,,,,,2025-05-09 11:32,,,,,,Industry,,,,,Unreleased,,Industry,,,,,Operation counting,,,,
Firefly,Image generation,Image generation,Adobe,,2023-03-21,"Adobe Unveils Firefly, a Family of new Creative Generative AI",https://news.adobe.com/news/news-details/2023/Adobe-Unveils-Firefly-a-Family-of-new-Creative-Generative-AI/default.aspx,,Significant use,"Integrated into Photoshop. Users generate >200m images within a few months of release:

https://venturebeat.com/ai/adobe-stock-creators-arent-happy-with-firefly-the-companys-commercially-safe-gen-ai-tool/

As of October 2024, users have generated 13B images since March 2023. Paid users get 100 generations per month (and can continue at a slower rate after that). Assuming an average of 100 monthly generations per user, that's around 6.7M monthly average users across 19.5 months.",,,,,Adobe Stock,"""The current Firefly generative AI model is trained on a dataset of licensed content, such as Adobe Stock, and public domain content where copyright has expired.""

https://www.adobe.com/products/firefly.html",,,,,,,Unknown,"Today, Adobe (Nasdaq:ADBE) introduced Adobe Firefly, a new family of creative generative AI models, first focused on the generation of images and text effects. Adobe Firefly will bring even more precision, power, speed and ease directly into Creative Cloud, Document Cloud, Experience Cloud and Adobe Express workflows where content is created and modified. Adobe Firefly will be part of a series of new Adobe Sensei generative AI services across Adobeâ€™s clouds.",,,Hosted access (no API),United States of America,,,,,,2025-05-28 17:00,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
Gen-2,Video,"Video generation,Text-to-video,Image-to-video,Video-to-video",Runway,Gen-2 authors,2023-03-20,"Gen-2: Generate novel videos with text, images or video clips",https://research.runwayml.com/gen2,,SOTA improvement,"Website claims SOTA improvement over Stable Diffusion and Text2Live, paper forthcoming",,,,,,,,,,,,,Unknown,,,,API access,United States of America,,,,,,2025-06-02 11:28,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
PanGu-Î£,Language,"Code generation,Language modeling,Translation,Question answering",Huawei Noah's Ark Lab,"Xiaozhe Ren, Pingyi Zhou, Xinfan Meng, Xinjing Huang, Yadao Wang, Weichao Wang, Pengfei Li, Xiaoda Zhang, Alexander Podolskiy, Grigory Arshinov, Andrey Bout, Irina Piontkovskaya, Jiansheng Wei, Xin Jiang, Teng Su, Qun Liu, Jun Yao",2023-03-20,PanGu-Î£: Towards Trillion Parameter Language Model with Sparse Heterogeneous Computing,https://arxiv.org/abs/2303.10845,48.0,SOTA improvement,"""Our experimental findings show that PanGu-{\Sigma} provides state-of-the-art performance in zero-shot learning of various Chinese NLP downstream tasks.""",1085000000000.0,"""In this work, we present PanGu-Î£ , a large language model with sparse architecture containing 1.085 trillion parameters.""",4.669999999999999e+23,"It has sparse architecture, so we can't use C=6ND.
""We develop PanGu-Î£ model under the framework of MindSpore and train it on a cluster with only 512 Ascend 910 AI Accelerators with 329 billion tokens over 100 days.""
100 days * 512 processors * 320 teraFLOPS/processor * 33% utilization = 4.67e+23 FLOP
https://www.wolframalpha.com/input?i=100+days+*+512+*+320+terahertz+*+0.33",,"""329B tokens in more than 40 natural and programming languages""",246750000000.0,329B tokens ~= 247B words,2400.0,"We develop PanGu-Î£ model under the framework of MindSpore 5
and train it on a cluster with only 512 Ascend 910 AI Accelerators [28] with 329 billion tokens over 100 days.",Huawei Ascend 910,Self-supervised learning,Confident,"The scaling of large language models has greatly improved natural language understanding, generation, and reasoning. In this work, we develop a system that trained a trillion-parameter language model on a cluster of Ascend 910 AI processors and MindSpore framework, and present the language model with 1.085T parameters named PanGu-{\Sigma}. With parameter inherent from PanGu-{\alpha}, we extend the dense Transformer model to sparse one with Random Routed Experts (RRE), and efficiently train the model over 329B tokens by using Expert Computation and Storage Separation(ECSS). This resulted in a 6.3x increase in training throughput through heterogeneous computing. Our experimental findings show that PanGu-{\Sigma} provides state-of-the-art performance in zero-shot learning of various Chinese NLP downstream tasks. Moreover, it demonstrates strong abilities when fine-tuned in application data of open-domain dialogue, question answering, machine translation and code generation.",1.84,,Unreleased,China,,,,512.0,,2025-05-28 16:06,,,,524288.0,"""We train PanGu-Î£ with global batch size of 512 with sequence length of 1024 for each sample""",Industry,,,,1228800.0,Unreleased,,Industry,,,FP16,316521.76523003745,Hardware,,,,10
GPT-4,"Multimodal,Language,Vision","Language modeling,Language modeling/generation,Question answering,Visual question answering",OpenAI,"OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, SimÃ³n Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain et al. (181 additional authors not shown)",2023-03-15,GPT-4 Technical Report,https://arxiv.org/abs/2303.08774,8281.0,"Highly cited,SOTA improvement,Training cost","See the paper, p.1: ""On a suite of traditional NLP benchmarks, GPT-4 outperforms both previous large language models and most state-of-the-art systems (which often have benchmark-specific training or hand-engineering).""",,,2.1e+25,"90% CI: 8.2E+24 to 4.4E+25

NOTE: this is a rough estimate based on public information, much less information than most other systems in the database.

Calculation and confidence intervals here: https://colab.research.google.com/drive/1O99z9b1I5O66bT78r9ScslE_nOj5irN9?usp=sharing",Unspecified unreleased,"Assuming this is the earliest model in the family, the knowledge cutoff date was originally September 2021 - which I assume means September 1, 2021 - according to https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models?tabs=global-standard%2Cstandard-chat-completions#gpt-35.",4900000000000.0,"Speculative. Reported secondhand by online sources such as Semianalysis, but not verified by OpenAI. If total number of tokens seen was 13T, text was repeated for 2 epochs, and text was the majority of tokens, then dataset size roughly is 13T*0.75/2 = 4.9T words.

Note this examines only the text dataset, since GPT-4 was first and foremost a language model. However, the vision component had its own vision dataset, which we believe accounted for a much smaller part of the compute budget.",2280.0,(Speculative) SemiAnalysis conjectures that GPT-4 training took 90-100 days with utilization of 32-36%.,NVIDIA A100 SXM4 40 GB,Self-supervised learning,Speculative,"We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.",2.0,,API access,United States of America,,,,25000.0,0.34,2025-05-12 18:46,,,,,,Industry,checked,,,57000000.0,Unreleased,,Industry,$40586592.58,(Speculative) SemiAnalysis conjectures that GPT-4 had utilization of 32-36%: https://www.semianalysis.com/p/gpt-4-architecture-infrastructure,,19944368.12599428,Hardware,,,,1
Falcon-40B,Language,Language modeling,Technology Innovation Institute,,2023-03-15,Abu Dhabi-based Technology Innovation Institute Introduces Falcon LLM: Foundational Large Language Model (LLM) outperforms GPT-3 with 40 Billion Parameters,https://arxiv.org/abs/2311.16867; https://www.tii.ae/news/abu-dhabi-based-technology-innovation-institute-introduces-falcon-llm-foundational-large,0.0,Historical significance,,40000000000.0,Model comes in 7B and 40B variants.,2.4e+23,"C = 6ND = 6 * 40B * 1000B = 2.4e+23 FLOP (assuming one epoch)

Table 1 from https://arxiv.org/pdf/2311.16867 Falcon paper

2,800 petaflop-days * 1e15 * 24 * 3600 = 2.4192e+23 FLOPs",RefinedWeb,"Falcon-40B was trained on 1,000B tokens of RefinedWeb, a high-quality filtered and deduplicated web dataset which we enhanced with curated corpora. Significant components from our curated copora were inspired by The Pile (Gao et al., 2020).",1000000000000.0,1000B tokens ~= 750B words,1440.0,"""Falcon-40B was trained on AWS SageMaker, on 384 A100 40GB GPUs in P4d instances.""
""Training started in December 2022 and took two months.""",NVIDIA A100,,Confident,,,,Open weights (unrestricted),United Arab Emirates,,,,384.0,0.3864,2025-06-10 15:36,,,,2359296.0,"Batch size 1152 (presumably sequences) per Table 16. Warmed up using smaller batches for first 100B tokens.

""All Falcon models are pretrained with a 2,048 sequence length""

https://arxiv.org/pdf/2311.16867.pdf
",Government,checked,,,552960.0,Unreleased,apache 2.0,Government,$319783.16,"Estimated training compute: 2.4e23 FLOP
FLOPs at 100% utilization, based on GPU-hours: (1440 * 384 * 3600 * 3.12e14) = 6.211e23
2.4e23 / 6.211e23 = 0.3864",BF16,306345.49441527214,"Operation counting,Reported",,,,
LEP-AD,Biology,"Proteins,Protein interaction prediction","King Abdullah University of Science and Technology (KAUST),Karolinska Institute","Anuj Daga, Sumeer Ahmad Khan, David Gomez Cabrero, Robert Hoehndorf, Narsis A. Kiani, Jesper Tegner",2023-03-15,LEP-AD: Language Embedding of Proteins and Attention to Drugs predicts Drug Target Interactions,https://www.biorxiv.org/content/10.1101/2023.03.14.532563v1.full.pdf,1.0,SOTA improvement,"""We report new best-in-class state-of-the-art results compared
to competing methods such as SimBoost, DeepCPI, Attention-DTA, GraphDTA,
and more using multiple datasets, including Davis, KIBA, DTC, Metz, ToxCast,
and STITCH. Finally, we find that a pre-trained model with embedding of proteins
(the LED-AD) outperforms a model using an explicit alpha-fold 3D representation of proteins (e.g., LEP-AD supervised by Alphafold)""",3007381000.0,"Uses ESM-2 3B. Table 2 gives details on the non-ESM layers. The GCN appears to have about 3.31M parameters and the linear layers should have 771k and 3.3M, respectively. So total is ~3.007B",,No indication of the training used here. ESM-2 3B used 3e22.,,Table 1,1244420.0,"Largest dataset appears to be STITCH, at 1244420 drug-target pairs.",,,,,Confident,"Predicting drug-target interactions is a tremendous challenge for drug development and lead optimization. Recent advances include training algorithms to learn drug-target interactions from data and molecular simulations. Here we utilize Evolutionary Scale Modeling (ESM-2) models to establish a Transformer protein language model for drug-target interaction predictions. Our architecture, LEPAD, combines pre-trained ESM-2 and Transformer-GCN models predicting binding affinity values. We report new best-in-class state-of-the-art results compared to competing methods such as SimBoost, DeepCPI, Attention-DTA, GraphDTA, and more using multiple datasets, including Davis, KIBA, DTC, Metz, ToxCast, and STITCH. Finally, we find that a pre-trained model with embedding of proteins (the LED-AD) outperforms a model using an explicit alpha-fold 3D representation of proteins (e.g., LEP-AD supervised by Alphafold). The LEP-AD model
scales favorably in performance with the size of training data. Code available at https://github.com/adaga06/LEP-AD",,,Unreleased,"Saudi Arabia,Sweden",ESM2-3B,,,,,2025-05-28 16:06,,,,,,"Academia,Academia",,,,,Open (non-commercial),https://github.com/adaga06/LEP-AD unclear license,"Academia,Academia",,,FP16,,,,,,
Claude,Language,"Language modeling,Chat",Anthropic,,2023-03-14,Introducing Claude,https://www.anthropic.com/index/introducing-claude,,"Historical significance,SOTA improvement",,,,,,Unspecified unreleased,"Assuming that â€œClaudeâ€ refers to â€œClaude 1â€, this has a knowledge cutoff date of March 2023 - which I assume means March 1, 2023 - https://www.youreverydayai.com/knowledge-cutoff-what-it-is-and-why-it-matters-for-large-language-models/.",,,,,,Reinforcement learning,Unknown,"Claude is a next-generation AI assistant based on Anthropicâ€™s research into training helpful, honest, and harmless AI systems. Accessible through chat interface and API in our developer console, Claude is capable of a wide variety of conversational and text processing tasks while maintaining a high degree of reliability and predictability.",,,API access,United States of America,,,,,,2025-05-28 17:05,,,,,,Industry,checked,,,,Unreleased,,Industry,,,,,,,,,
Alpaca,Language,"Language modeling/generation,Question answering",Stanford University,"Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B. Hashimoto",2023-03-13,"Alpaca: A Strong, Replicable Instruction-Following Model",https://crfm.stanford.edu/2023/03/13/alpaca.html,,,,7000000000.0,7b,,,Stanford-Alpaca,"""We train the Alpaca model on 52K instruction-following demonstrations generated in the style of self-instruct using text-davinci-003""",,,3.0,"""For our initial run, fine-tuning a 7B LLaMA model took 3 hours on 8 80GB A100s, which costs less than $100 on most cloud compute providers.""",NVIDIA A100 SXM4 80 GB,,Confident,"We introduce Alpaca 7B, a model fine-tuned from the LLaMA 7B model on 52K instruction-following demonstrations. On our preliminary evaluation of single-turn instruction following, Alpaca behaves qualitatively similarly to OpenAIâ€™s text-davinci-003, while being surprisingly small and easy/cheap to reproduce (<600$). Checkout our code release on GitHub.
Update: The public demo is now disabled. The original goal of releasing a demo was to disseminate our research in an accessible way. We feel that we have mostly achieved this goal, and given the hosting costs and the inadequacies of our content filters, we decided to bring down the demo.",,,Open weights (non-commercial),United States of America,LLaMA-7B,8087040000000000000,312000000000000*8*3*3600*0.3 = 8.08704e+18,8.0,,2025-05-01 10:42,,,,,,Academia,,,,,Open (non-commercial),"https://github.com/tatsu-lab/stanford_alpaca#fine-tuning

https://huggingface.co/tatsu-lab/alpaca-7b-wdiff/tree/main

they released weights diff with LLaa, not final weights",Academia,$100.00,,,6382.482061713183,Hardware,,,,
VALL-E X,Speech,"Translation,Speech synthesis,Speech recognition",Microsoft,"Ziqiang Zhang, Long Zhou, Chengyi Wang, Sanyuan Chen, Yu Wu, Shujie Liu, Zhuo Chen, Yanqing Liu, Huaming Wang, Jinyu Li, Lei He, Sheng Zhao, Furu Wei",2023-03-07,Speak Foreign Languages with Your Own Voice: Cross-Lingual Neural Codec Language Modeling,https://arxiv.org/abs/2303.03926,131.0,,,700000000.0,"""For the cross-lingual codec language models, Ï†MAR and Ï†MNAR are both 12-layer Transformer decoders with an attention dimension of 1024 and the FFN dimension of 4096.""

These are two parts of the model.

According to Ben's script, that's 353M parameters, or ~700M for both
(https://github.com/bencottier/ml-parameter-count/blob/main/parameter_count.py)",1.2e+21,""" The batch sizes of speech and phonemes on each GPU are 1,400,000 (87.5 seconds) and 3,000, respectively. The maximum learning
rate is 5e-4 with warm-up steps of 32,000. The model is pre-trained on 32 V100 GPUs for 400K step""

6CN:
6 * (1.4M + 3000) * 353M * 400k = 1.2e21

(2 OOM larger than our estimate for VALL-E, and would take ~46 days on the V100 cluster at 30% utilization. Seems perhaps a bit high?)","LibriLight,WenetSpeech","""Our VALL-E X is trained using bilingual speech-transcription (ASR) data. The Chinese ASR data are from WenetSpeech [Zhang et al., 2022a] containing 10,000+ hours of multi-domain labeled speech. The English ASR data are from LibriLight [Kahn et al., 2020] containing about 60,000 hours of unlabeled speech, whose speech data are collected from audiobooks. We train a Kaldi4 ASR model on the labeled Librispeech [Panayotov et al., 2015] dataset to generate the pseudo transcripts for the unlabeled LibriLight speech. To train the speech recognition & translation model for S2ST, we also use additional MT and ST data. The MT data are from AI Challenger , OpenSubtitles2018 and WMT2020, which contain about 13M, 10M, and 50M sentence pairs in conversion, drama, and news domains, respectively.""",151200000000.0,"70k hours, encoded by an 8 layer EnCodec, where each layer generates 75 encodings per second of 24 kHz audio.

70k hr * 3600 sec/hr * 75/sec = 18.9B audio encodings per layer

Ï†MAR is trained to autoregressively predict the each subsequent token in the first layer of encodings. Ï†MNAR is trained to predict the remaining acoustic embedding layers after being shown both the first layer of the current input and the full set of embeddings from a previous sentence.

Based on this, the total number of acoustic codes used in training appears to be 8 * 18.9B = 151,200,000,000",,,NVIDIA V100,,Likely,"We propose a cross-lingual neural codec language model, VALL-E X, for cross-lingual speech synthesis. Specifically, we extend VALL-E and train a multi-lingual conditional codec language model to predict the acoustic token sequences of the target language speech by using both the source language speech and the target language text as prompts. VALL-E X inherits strong in-context learning capabilities and can be applied for zero-shot cross-lingual text-to-speech synthesis and zero-shot speech-to-speech translation tasks. Experimental results show that it can generate high-quality speech in the target language via just one speech utterance in the source language as a prompt while preserving the unseen speaker's voice, emotion, and acoustic environment. Moreover, VALL-E X effectively alleviates the foreign accent problems, which can be controlled by a language ID. Audio samples are available at \url{this https URL}.",,,Unreleased,"United States of America,Multinational,India,Belgium",,,,,,2025-02-14 16:28,,,,,,Industry,,,,,,,Industry,,,,,Operation counting,,,,
PaLM-E,"Robotics,Vision,Language","Visual question answering,Robotic manipulation,Image captioning,Language generation","Google,TU Berlin","Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, Pete Florence",2023-03-06,PaLM-E: An Embodied Multimodal Language Model,https://arxiv.org/abs/2303.03378,1192.0,SOTA improvement,"""Our largest model, PaLM-E-562B with 562B parameters, in addition to being trained on robotics tasks, is a visual-language generalist
with state-of-the-art performance on OK-VQA, and retains
generalist language capabilities with increasing scale.""",562000000000.0,562B,,"Based on Palm-540B and ViT-22B and then trained on robotics data.

",,"""Our three robot environments (Fig. 1) include a Task and Motion Planning (TAMP) domain where a robot has to manipulate (grasp and stack) objects, a table-top pushing environment, and a mobile manipulation domain. In each domain, PaLM-E is trained on expert data from that domain. In many cases, this is a sparse amount of data per task. The TAMP tasks involve large combinatorics over possible plans, and many decision sequences are infeasible. PaLM-E has to generate plans that consist of multiple steps, with complicated decision boundaries. The multi-object tabletop pushing environment is taken from the publicly available Language-Table dataset (Lynch et al., 2022) and is challenging since it includes several objects, large cardinality of language, and complex pushing dynamics""",,,,,,,Likely,"Large language models excel at a wide range of complex tasks. However, enabling general inference in the real world, e.g., for robotics problems, raises the challenge of grounding. We propose embodied language models to directly incorporate real-world continuous sensor modalities into language models and thereby establish the link between words and percepts. Input to our embodied language model are multi-modal sentences that interleave visual, continuous state estimation, and textual input encodings. We train these encodings end-to-end, in conjunction with a pre-trained large language model, for multiple embodied tasks including sequential robotic manipulation planning, visual question answering, and captioning. Our evaluations show that PaLM-E, a single large embodied multimodal model, can address a variety of embodied reasoning tasks, from a variety of observation modalities, on multiple embodiments, and further, exhibits positive transfer: the model benefits from diverse joint training across internet-scale language, vision, and visual-language domains. Our largest model, PaLM-E-562B with 562B parameters, in addition to being trained on robotics tasks, is a visual-language generalist with state-of-the-art performance on OK-VQA, and retains generalist language capabilities with increasing scale.",,,Unreleased,"United States of America,Germany",PaLM (540B),,"Based on Palm-540B and ViT 22B. No compute details given.

""We scale PaLM-E up to 562B parameters, integrating the 540B PaLM (Chowdhery et al., 2022) LLM and the 22B Vision Transformer (ViT) (Dehghani et al., 2023) into, to our knowledge, the largest vision-language model currently reported.""",,,2025-05-30 12:52,,,,,,"Industry,Academia",checked,,,,Unreleased,,"Industry,Academia",,,,,,,,,
Uni-Mol Molecular Model,Biology,"Molecular representation learning,Molecular property prediction,Protein-ligand contact prediction,Drug discovery","Renmin University of China,DP Technology,""AI for Science Institute, Beijing (AISI)""","Gengmo Zhou, Zhifeng Gao, Qiankun Ding, Hang Zheng, Hongteng Xu, Zhewei Wei, Linfeng Zhang, Guolin Ke",2023-03-06,Uni-Mol: A Universal 3D Molecular Representation Learning Framework,https://chemrxiv.org/engage/chemrxiv/article-details/6402990d37e01856dc1d1581,,,,,,5.413824e+18,31330000000000 FLOP / sec / GPU [fp16 asssumed] * 8 GPUs * 20 hours * 3600 sec / hour * 0.3 [assumed utilization] = 5.413824e+18 FLOP,Uni-Mol Molecular pre-train dataset,"""The pretraining datasets we use consist of two parts: one part is a database
collection of 12 million molecules that can be synthesized and purchased (See Table 5), and the other part is taken from a previous work [23], whose molecules are collected from the ZINC [82] and ChemBL [83] databases. After normalizing and duplicating, we obtain 19 million molecules as our pretraining dataset. To generate 3D conformations, we use ETKGD [42] with Merck Molecular Force Field [43] optimization in RDKit [44] to randomly generate 11 conformations for each molecule, totally 209M conformations. In these 11 conformations of one molecule, there is a special flattened 3D conformation (atoms with zero z-axis coordinates) that is directly from the molecular graph. This flattened 3D conformation is used for the cases where RDKit failed to generate 3D conformations, like the peptides in the SIDER task.""",,"""a molecular model pretrained by 209M molecular conformations""

Table 6:
batch size 128 
Max training steps 1M",20.0,"""Molecular pretraining runs on 8 V100 GPUs (32GB memory, the same below), and the training time is about 20 hours.""",NVIDIA Tesla V100 DGXS 32 GB,,Confident,"Molecular representation learning (MRL) has gained tremendous attention due to its critical role in learning from limited supervised data for applications like drug design. In most MRL methods, molecules are treated as 1D sequential tokens or 2D topology graphs, limiting their ability to incorporate 3D information for downstream tasks and, in particular, making it almost impossible for 3D geometry prediction/generation. In this paper, we propose a universal 3D MRL framework, called Uni-Mol, that significantly enlarges the representation ability and application scope of MRL schemes. Uni-Mol contains two pretrained models with the same SE(3) Transformer architecture: a molecular model pretrained by 209M molecular conformations; a pocket model pretrained by 3M candidate protein pocket data. Besides, Uni-Mol contains several finetuning strategies to apply the pretrained models to various downstream tasks. By properly incorporating 3D information, Uni-Mol outperforms SOTA in 14/15 molecular property prediction tasks. Moreover, Uni-Mol achieves superior performance in 3D spatial tasks, including protein-ligand binding pose prediction, molecular conformation generation, etc. The code, model, and data are made publicly available at https://github.com/dptech-corp/Uni-Mol.",,,Open weights (unrestricted),"China,China,China",,,,8.0,,2025-05-01 10:42,,,,,,"Academia,Industry,Government",,,,,Open source,"https://github.com/deepmodeling/Uni-Mol
MIT license","Academia,Industry,Government",,,,3989.67317269014,Hardware,,,,
AudioGen,Audio,Audio generation,"Meta AI,Hebrew University of Jerusalem","Felix Kreuk, Gabriel Synnaeve, Adam Polyak, Uriel Singer, Alexandre DÃ©fossez, Jade Copet, Devi Parikh, Yaniv Taigman, Yossi Adi",2023-03-05,AudioGen: Textually Guided Audio Generation,https://arxiv.org/abs/2209.15352,229.0,SOTA improvement,"""We propose a state-of-the-art auto-regressive audio generation model conditioned on textual descriptions or audio prompts, as evaluated with objective and subjective (human
listeners) scores.""",1000000000.0,"""We trained two sets of ALMs, one with 285M parameters (base) and the other with 1B parameters (large).""",9.5e+21,"""the large model was trained on 128 A100 GPUs for 200k steps (âˆ¼1 week)""
A100s are 312 teraflop/s
128 * 312 trillion * 7 * 24 * 3600 * 0.3 (utilization assumption) = 7.2e21

Text encoding uses T5-Large, which used 2.3e21 FLOP in pre-training per Flan paper: https://arxiv.org/abs/2210.11416 ","AudioSet,AudioCaps","""We use a set of several datasets: AudioSet (Gemmeke et al., 2017), BBC sound effects,
AudioCaps (Kim et al., 2019), Clotho v2 (Drossos et al., 2020), VGG-Sound (Chen et al., 2020),
FSD50K (Fonseca et al., 2021), Free To Use Sounds 2
, Sonniss Game Effects 3
, WeSoundEffects 4
,
Paramount Motion - Odeon Cinematic Sound Effects 5
. All audio files were sampled at 16kHz.
For textual descriptions we use two types of annotations. The first one is multi-label annotations,
available for the datasets: AudioSet, VGG-Sound, FSD50K, Sinniss Game Effects, WeSoundEffects, Paramount Motion - Odeon Cinematic Sound Effects.""",230400000000.0,"""Overall we are left with âˆ¼4k hours for training data.""
mix of speech and other sounds

Training the audio autoencoder uses reconstruction loss on sequence of raw audio samples. Audio files are in 16kHz, so
16k * 4k * 3600 = 230.4B samples

Audio language modelling operates on tokens; ""each second of audio is represented by 500 tokens"". 
4k * 3600 * 500 = 7.2B tokens",168.0,1 week,NVIDIA A100,,Likely,"We tackle the problem of generating audio samples conditioned on descriptive text captions. In this work, we propose AaudioGen, an auto-regressive generative model that generates audio samples conditioned on text inputs. AudioGen operates on a learnt discrete audio representation. The task of text-to-audio generation poses multiple challenges. Due to the way audio travels through a medium, differentiating ``objects'' can be a difficult task (e.g., separating multiple people simultaneously speaking). This is further complicated by real-world recording conditions (e.g., background noise, reverberation, etc.). Scarce text annotations impose another constraint, limiting the ability to scale models. Finally, modeling high-fidelity audio requires encoding audio at high sampling rate, leading to extremely long sequences. To alleviate the aforementioned challenges we propose an augmentation technique that mixes different audio samples, driving the model to internally learn to separate multiple sources. We curated 10 datasets containing different types of audio and text annotations to handle the scarcity of text-audio data points. For faster inference, we explore the use of multi-stream modeling, allowing the use of shorter sequences while maintaining a similar bitrate and perceptual quality. We apply classifier-free guidance to improve adherence to text. Comparing to the evaluated baselines, AudioGen outperforms over both objective and subjective metrics. Finally, we explore the ability of the proposed method to generate audio continuation conditionally and unconditionally. Samples: this https URL",,,Open weights (non-commercial),"United States of America,Israel",,,,,,2025-02-14 16:30,,,,,,"Industry,Academia",checked,,,,Open source,"MIT license, but non-commercial for weights: https://github.com/facebookresearch/audiocraft/blob/main/LICENSE_weights

training info: https://github.com/facebookresearch/audiocraft/blob/main/docs/AUDIOGEN.md ","Industry,Academia",$9429.74,,,,Hardware,,,,
Flan UL2,Language,Language generation,Google Brain,"Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won Chung, Siamak Shakeri, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Denny Zhou, Neil Houlsby, Donald Metzler",2023-03-03,A New Open Source Flan 20B with UL2,"https://www.yitay.net/blog/flan-ul2-20b, https://arxiv.org/abs/2205.05131",253.0,,"""We compare Flan-UL2 20B with other models in the Flan series. We report relative improvements over Flan-T5-XXL. Generally, Flan-UL2 outperforms Flan-T5 XXL on all four setups with an overall decent performance lift of +3.2% relative improvement""",19500000000.0,19.5B per https://www.yitay.net/blog/flan-ul2-20b,,,"C4,Flan",UL2 was pre-trained on C4. Flan UL2 is then instruction-tuned using the Flan dataset,,,,,,,Confident,"Flan-UL2 is an encoder decoder model based on the T5 architecture. It uses the same configuration as the UL2 model released earlier last year. It was fine tuned using the ""Flan"" prompt tuning and dataset collection.",,,Open weights (unrestricted),United States of America,UL2,,"From paper (https://arxiv.org/pdf/2205.05131.pdf), they say that ""we opt to train UL2 for another 100K steps"" with Flan instruction tuning. Not sure what the batch size is here though. In other parts of the paper they use batch sizes of 128 or 1024.

Either way, small compared to pre-training compute.",,,2024-11-01 10:05,,,,,,Industry,,,,,Unreleased,Apache license. https://github.com/google-research/google-research/tree/master/ul2,Industry,,,,,,,,,
DiT-XL/2,Image generation,Image generation,"New York University (NYU),University of California (UC) Berkeley","William Peebles, Saining Xie",2023-03-02,Scalable Diffusion Models with Transformers,https://arxiv.org/abs/2212.09748,960.0,SOTA improvement,"""our largest DiT-XL/2 models outperform all prior diffusion models on the classconditional ImageNet 512Ã—512 and 256Ã—256 benchmarks,
achieving a state-of-the-art FID of 2.27 on the latter.""",675000000.0,"675M

Table 4: ""Parameter and flop counts exclude the VAE model which contains 84M parameters across the encoder and decoder.""
675e6 not including VAE (likely frozen), 759e6 including VAE",6e+20,"~6e20, based on eyeballing Figure 9. It's between 1e11 and 1e12 gigaflop (1 gigaflop = 1e9 flop), and about 80% of the way towards 1e12 on a log scale. 10^0.8 is about 6. 

3M iterations with a batch size of 256.

""Compute. We implement all models in JAX [1] and train
them using TPU-v3 pods. DiT-XL/2, our most computeintensive model, trains at roughly 5.7 iterations/second on a
TPU v3-256 pod with a global batch size of 256""
256*123000000000000 FLOPs/s * 800000 training steps / 5.7 iterations/second * 0.3 = 1.3258105e+21",ImageNet,,,didn't state which ImageNet set,,,Google TPU v3,,Confident,"We explore a new class of diffusion models based on the transformer architecture. We train latent diffusion models of images, replacing the commonly-used U-Net backbone with a transformer that operates on latent patches. We analyze the scalability of our Diffusion Transformers (DiTs) through the lens of forward pass complexity as measured by Gflops. We find that DiTs with higher Gflops -- through increased transformer depth/width or increased number of input tokens -- consistently have lower FID. In addition to possessing good scalability properties, our largest DiT-XL/2 models outperform all prior diffusion models on the class-conditional ImageNet 512x512 and 256x256 benchmarks, achieving a state-of-the-art FID of 2.27 on the latter.",,,Open weights (non-commercial),"United States of America,United States of America",Stable Diffusion (LDM-KL-8-G),,,,,2025-06-01 16:46,,,,,,"Academia,Academia",,,,,Open (non-commercial),"CC-BY-NC
https://github.com/facebookresearch/DiT?tab=readme-ov-file

https://www.wpeebles.com/DiT.html","Academia,Academia",$111048.20,,,,"Hardware,Other",,,,
Cohere Command,Language,Language modeling/generation,Cohere,,2023-03-01,"World-class AI, at your command",https://cohere.com/models/command,,,,52000000000.0,"52B for larger version

https://aws.amazon.com/bedrock/cohere-command-embed/

Cohere Command has had a few different sizes over time and is continuously updated, but there's been a 52B version since at least March 2023: https://twitter.com/percyliang/status/1638236921754443776",,,,,,,,"https://docs.cohere.com/docs/environmental-impact

2696.16 kg carbon for base-light and 6689.76 kg carbon for embed-english. Nothing listed for the large model. 

It's possible to back out GPU-hours using this calculator, though it varies by region and Cohere doesn't specify the region.

https://mlco2.github.io/impact/",Google TPU v4,,Speculative,,,,API access,Canada,,,,,,2024-09-05 14:08,,,,,,Industry,checked,,,,Unreleased,,Industry,,,,,,,,,
Palmyra Large 20B,Language,Language modeling,Writer,Sam Julien / Writer,2023-03-01,,https://huggingface.co/Writer/palmyra-large,,,,20000000000.0,20B parameters for Palmyra Large. There is also a 43B version called Palmyra X according to HELM.,9.6e+22,"""Palmyra-Large is a 20B parameters causal decoder-only model built by Writer and trained on +800B tokens of Palmyra-Index-Data enhanced with curated corpora.""

I'm not sure if the 800B is how many tokens the model was trained on, or the size of the dataset. But the dataset linked on HuggingFace has 1T tokens, so 800B as tokens trained is more likely.

20B*800B*6 = 9.6e22",Palmyra dataset,"Writer's custom dataset, English text",750000000000.0,"1 trillion tokens, or 750B words: https://huggingface.co/datasets/Writer/palmyra-data-index",,,,,Speculative,"Palmyra Large was primarily pre-trained with English text. Note that there is still a trace amount of non-English data present within the training corpus that was accessed through CommonCrawl. A causal language modeling (CLM) objective was utilized during the process of the model's pretraining. Similar to GPT-3, Palmyra Large is a member of the same family of models that only contain a decoder. As a result, it was pre-trained utilizing the objective of self-supervised causal language modeling.",0.8,,Open weights (unrestricted),United States of America,,,,,,2025-01-06 15:28,,,,,,Industry,,,,,Unreleased,"Apache for weights.

data ""available on request"" https://huggingface.co/datasets/Writer/palmyra-data-index",Industry,,,,,Operation counting,,,,
gpt-sw3-40b,Language,"Language modeling/generation,Chat",AI Sweden,AI Sweden,2023-03-01,gpt-sw3-40b,https://huggingface.co/AI-Sweden-Models/gpt-sw3-40b,,,,40000000000.0,40B,7.68e+22,"aproximation 6ND = 6*320E9*40e9 = 7.68e22
""GPT-SW3 has been trained on a dataset containing 320B tokens in Swedish, Norwegian, Danish, Icelandic, English, and programming code.""","The Pile,Common Crawl,mC4,Wikipedia", https://huggingface.co/AI-Sweden-Models/gpt-sw3-40b#composition,,"320B tokens
""GPT-SW3 has been trained on a dataset containing 320B tokens in Swedish, Norwegian, Danish, Icelandic, English, and programming code.""""",,,,Self-supervised learning,Confident,,,,Open weights (restricted use),Sweden,,,,,,2025-05-09 11:32,,,,,,,,,,,Unreleased,"license, commercial, some ethical restrictions: 
https://huggingface.co/AI-Sweden-Models/gpt-sw3-40b/blob/main/LICENSE",,,,,,Operation counting,,,,
Kosmos-1,"Multimodal,Language,Vision","Visual question answering,Image captioning,Language modeling/generation,Chat,Question answering,Document classification,Image classification,Visual puzzles",Microsoft,"Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Barun Patra, Qiang Liu, Kriti Aggarwal, Zewen Chi, Johan Bjorck, Vishrav Chaudhary, Subhojit Som, Xia Song, Furu Wei",2023-03-01,Language Is Not All You Need: Aligning Perception with Language Models,https://arxiv.org/abs/2302.14045,,,,1600000000.0,"""The total number of parameters of KOSMOS-1 is about 1.6B""
""The MLLM component has 24 layers with 2,048 hidden dimensions, 8,192 FFN intermediate size, and 32 attention heads, resulting in about 1.3B parameters.""",3.456e+21,"6 FLOP / parameter / token * 1.6 * 10^9 parameters [assuming all parameters were updated each step including vision encoder] * 360 * 10^9 tokens = 3.456e+21 FLOP

""Likely"" confidence because the number of trained parameters could be slightly less","The Pile,Common Crawl,LAION-2B,LAION-400M,COYO-700M,Conceptual Captions (CC3M)",,360000000000.0,"""We use a batch size of 1.2 million tokens (0.5 million tokens from text corpora, 0.5 million tokens from image-caption pairs, and 0.2 million tokens from interleaved data) and train KOSMOS-1 for
300k steps, corresponding to about 360 billion tokens.""",,,,,Likely,"A big convergence of language, multimodal perception, action, and world modeling is a key step toward artificial general intelligence. In this work, we introduce Kosmos-1, a Multimodal Large Language Model (MLLM) that can perceive general modalities, learn in context (i.e., few-shot), and follow instructions (i.e., zero-shot). Specifically, we train Kosmos-1 from scratch on web-scale multimodal corpora, including arbitrarily interleaved text and images, image-caption pairs, and text data. We evaluate various settings, including zero-shot, few-shot, and multimodal chain-of-thought prompting, on a wide range of tasks without any gradient updates or finetuning. Experimental results show that Kosmos-1 achieves impressive performance on (i) language understanding, generation, and even OCR-free NLP (directly fed with document images), (ii) perception-language tasks, including multimodal dialogue, image captioning, visual question answering, and (iii) vision tasks, such as image recognition with descriptions (specifying classification via text instructions). We also show that MLLMs can benefit from cross-modal transfer, i.e., transfer knowledge from language to multimodal, and from multimodal to language. In addition, we introduce a dataset of Raven IQ test, which diagnoses the nonverbal reasoning capability of MLLMs.",,,Unreleased,"United States of America,Multinational,India,Belgium",ViT-G/14 (LiT),,,,,2025-06-01 15:20,,,,1200000.0,,Industry,,,,,Unreleased,"MIT license
https://github.com/microsoft/unilm/tree/master/kosmos-1

(repo refers to code and weights of Kosmos-2, not Kosmos-1)",Industry,,,,,Operation counting,,,,
CodeGen-Mono 16.1B,Language,"Code generation,Code autocompletion",Salesforce,"Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, Caiming Xiong",2023-02-27,CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis,https://arxiv.org/abs/2203.13474,764.0,,"Not as good as code-davinci-001 or code-davinci-002, per Table 1",16100000000.0,16.1B parameters,,,"The Pile,Big Query,BigPython","""The family of CODEGEN models is trained sequentially on three datasets: The Pile, BigQuery, and BigPython.""",568200000000.0,Table 5,,,Google TPU v4,,Likely,"""Program synthesis strives to generate a computer program as a solution to a given problem specification, expressed with input-output examples or natural language descriptions. The prevalence of large language models advances the state-of-the-art for program synthesis, though limited training resources and data impede open access to such models. To democratize this, we train and release a family of large language models up to 16.1B parameters, called CODEGEN, on natural language and programming language data, and open source the training library JAXFORMER. We show the utility of the trained model by demonstrating that it is competitive with the previous state-of-the-art on zero-shot Python code generation on HumanEval. We further investigate the multi-step paradigm for program synthesis, where a single program is factorized into multiple prompts specifying subproblems. To this end, we construct an open benchmark, Multi-Turn Programming Benchmark (MTPB), consisting of 115 diverse problem sets that are factorized into multi-turn prompts. Our analysis on MTPB shows that the same intent provided to CODEGEN in multi-turn fashion significantly improves program synthesis over that provided as a single turn. We make the training library JAXFORMER and model checkpoints available as open source contribution: this https URL.""",4.18,,Open weights (unrestricted),United States of America,,,,,,2024-11-01 10:05,,,,,,Industry,,,,,,apache 2.0,Industry,,,,,,,,,
MsPBRsP,Biology,Protein interaction prediction,Zhengzhou University,"Yuguang Li,  Shuai Lu, Xiaofei Nan, Shoutao Zhang, Qinglei Zhou",2023-02-27,MsPBRsP: Multi-scale Protein Binding Residues Prediction Using Language Model,https://www.biorxiv.org/content/10.1101/2023.02.26.528265v1.abstract,,,,,,,"
",,,1860001.0,"Total Residues: 2,068,793
Training Set (90%): 0.9 Ã— 2,068,793 = 1,861,913 residues
Final Data Amount: 1.86M residues",,,NVIDIA GeForce RTX 2080,,Confident,"Accurate prediction of protein binding residues (PBRs) from sequence is important for the understanding of cellular activity and helpful for the design of novel drug. However, experimental methods are time-consuming and expensive. In recent years, a lot of computational predictors based on machine learning and deep learning models are proposed to reduce such consumption. But those methods often use MSA tools such as PSI-BLAST or NetSurfP to generate some statistical features and enter them into predictive models as necessary supplementary input. The input generation process normally takes long time, and there is no standard to specify which and how many statistic results should be provided to a prediction model. In addition, prediction of PBRs relies on residue local context, but the most appropriate scale is undetermined. Most works pre-selected certain residue features as input and a scale size based on expertise for certain type of PBRs. In this study, we propose a general tool-free end-to-end framework that can be applied to all types of PBRs, Multi-scale Protein Binding Residues Prediction using language model (MsPBRsP). We adopt a pre-trained language model ProtTrans to save the large consumption caused by MSA tools, and use protein sequence alone as input to our model. To ease scale size uncertainty, we construct multi-size windows in attention layer and multi-size kernels in convolutional layer. We test our framework on various benchmark datasets including PBRs from protein-protein, protein-nucleotide, protein-small ligand, heterodimer, homodimer and antibody-antigen interactions. Compared with existing state-of-the-art methods, MsPBRsP achieves superior performance with less running time and higher prediction rates on every PBRs prediction task. Specifically, we boost F1 score by 27.1% and AUPRC score by 7.6% on NSP448 dataset and decrease running time from over 10 minutes to under 0.1s on average. The source code and datasets are available at https://github.com/biolushuai/MsPBRsP-for-multiple-PBRs-prediction.",,,,China,,,,1.0,,2025-05-01 10:42,,,,,,Academia,,,,,,,Academia,,,,235.6905104555972,Hardware,,,,
LLaMA-65B,Language,"Language modeling,Code generation",Meta AI,"Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample",2023-02-24,LLaMA: Open and Efficient Foundation Language Models,https://arxiv.org/abs/2302.13971,8872.0,"Historical significance,Highly cited",Widely-used foundation model that has been adapted for others such as Alpaca.,65200000000.0,"Model card, table 1: https://github.com/facebookresearch/llama/blob/53011c3d7946dadb8274a4c5c7586ab54edf792d/MODEL_CARD.md",5.5e+23,"1.4e12 tokens * 6.52e10 parameters * 6 FLOP/token/parameter = 5.5e23 FLOP

Compared to 2048 A100 GPUs each with 311.84 TFLOPS maximum performance for 21 days, this implies 47% utilization.
https://www.wolframalpha.com/input?i=5.5*10%5E23+FLOP+%2F+%282048+*+311.84+teraFLOPS+*+21+days%29","CCNet,GitHub,Wikipedia,books,arXiv,Stack Exchange","The model was trained using the following source of data: CCNet [67%], C4 [15%], GitHub [4.5%], Wikipedia [4.5%], Books [4.5%], ArXiv [2.5%], Stack Exchange[2%]. The Wikipedia and Books domains include data in the following languages: bg, ca, cs, da, de, en, es, fr, hr, hu, it, nl, pl, pt, ro, ru, sl, sr, sv, uk. See the paper for more details about the training set and corresponding preprocessing.",1340000000000.0,"Table 1 indicates that 1.4T tokens involved sampling sub-datasets at more or less than one epoch. Correcting for this:

(1.1 epoch * 3.3TB) + (1.06 epoch * 0.783TB) + ... = 1.4T tokens
5.24 epoch-TBs = 1.4T tokens
5.24 epoch-TB * 1000 GB/TB * 200M token/GB = 1.4T tokens
1.05T epoch*token = 1.4T tokens
1 epoch = 1.34T tokens
",500.0,"""When training a 65B-parameter model, our code processes around 380 tokens/sec/GPU on 2048 A100 GPU with 80GB of RAM. This means that training over our dataset containing 1.4T tokens takes approximately 21 days.""",NVIDIA A100,Supervised,Confident,"We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.",1.09,LLaMA-65B,Open weights (non-commercial),United States of America,,,,2048.0,0.4746,2025-05-29 10:29,,,,4000000.0,,Industry,checked,,,1024000.0,Unreleased,"""we are releasing our model under a noncommercial license focused on research use cases"" https://ai.meta.com/blog/large-language-model-llama-meta-ai/",Industry,$576476.49,https://www.wolframalpha.com/input?i=5.5*10%5E23+FLOP+%2F+%282048+*+311.84+teraFLOPS+*+21+days%29,,1634534.091472035,Operation counting,,,,7
Anthropic LM 175B,Language,"Language modeling/generation,Question answering",Anthropic,"Deep Ganguli, Amanda Askell, Nicholas Schiefer, Thomas I. Liao, KamilÄ— LukoÅ¡iÅ«tÄ—, Anna Chen, Anna Goldie, Azalia Mirhoseini, Catherine Olsson, Danny Hernandez, Dawn Drain, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jackson Kernion, Jamie Kerr, Jared Mueller, Joshua Landau, Kamal Ndousse, Karina Nguyen, Liane Lovitt, Michael Sellitto, Nelson Elhage, Noemi Mercado, Nova DasSarma, Oliver Rausch, Robert Lasenby, Robin Larson, Sam Ringer, Sandipan Kundu, Saurav Kadavath, Scott Johnston, Shauna Kravec, Sheer El Showk, Tamera Lanham, Timothy Telleen-Lawton, Tom Henighan, Tristan Hume, Yuntao Bai, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, Christopher Olah, Jack Clark, Samuel R. Bowman, Jared Kaplan",2023-02-15,The Capacity for Moral Self-Correction in Large Language Models,https://arxiv.org/abs/2302.07459,132.0,,,175000000000.0,175B,,,,,,,,,,Reinforcement learning,Confident,"We test the hypothesis that language models trained with reinforcement learning from human feedback (RLHF) have the capability to ""morally self-correct"" -- to avoid producing harmful outputs -- if instructed to do so. We find strong evidence in support of this hypothesis across three different experiments, each of which reveal different facets of moral self-correction. We find that the capability for moral self-correction emerges at 22B model parameters, and typically improves with increasing model size and RLHF training. We believe that at this level of scale, language models obtain two capabilities that they can use for moral self-correction: (1) they can follow instructions and (2) they can learn complex normative concepts of harm like stereotyping, bias, and discrimination. As such, they can follow instructions to avoid certain kinds of morally harmful outputs. We believe our results are cause for cautious optimism regarding the ability to train language models to abide by ethical principles.",,,Unreleased,United States of America,,,,,,2025-05-09 11:32,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
BASIC-L + Lion,Vision,Image classification,"Google,University of California Los Angeles (UCLA)","Xiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Yao Liu, Hieu Pham, Xuanyi Dong, Thang Luong, Cho-Jui Hsieh, Yifeng Lu, Quoc V. Le",2023-02-13,Symbolic Discovery of Optimization Algorithms,https://arxiv.org/abs/2302.06675v4,239.0,SOTA improvement,"""On vision-language contrastive learning, we achieve 88.3% zero-shot and 91.1% fine-tuning accuracy on ImageNet, surpassing the previous best results by 2% and 0.1%, respectively""",3070000000.0,parameter count of original BASIC-L,,"This model is BASIC-L retrained with a different optimizer, Lion. Lion seems more compute-efficient, so we should expect compute to be less than BASIC-L.",,,,,,,,,Confident,"We present a method to formulate algorithm discovery as program search, and apply it to discover optimization algorithms for deep neural network training. We leverage efficient search techniques to explore an infinite and sparse program space. To bridge the large generalization gap between proxy and target tasks, we also introduce program selection and simplification strategies. Our method discovers a simple and effective optimization algorithm, Lion (EvoLved Sign Momentum). It is more memory-efficient than Adam as it only keeps track of the momentum. Different from adaptive optimizers, its update has the same magnitude for each parameter calculated through the sign operation. We compare Lion with widely used optimizers, such as Adam and Adafactor, for training a variety of models on different tasks. On image classification, Lion boosts the accuracy of ViT by up to 2% on ImageNet and saves up to 5x the pre-training compute on JFT. On vision-language contrastive learning, we achieve 88.3% zero-shot and 91.1% fine-tuning accuracy on ImageNet, surpassing the previous best results by 2% and 0.1%, respectively. On diffusion models, Lion outperforms Adam by achieving a better FID score and reducing the training compute by up to 2.3x. For autoregressive, masked language modeling, and fine-tuning, Lion exhibits a similar or better performance compared to Adam. Our analysis of Lion reveals that its performance gain grows with the training batch size. It also requires a smaller learning rate than Adam due to the larger norm of the update produced by the sign function. Additionally, we examine the limitations of Lion and identify scenarios where its improvements are small or not statistically significant. Lion is also successfully deployed in production systems such as Google search ads CTR model.",,,Unreleased,"United States of America,United States of America",,,,,,2025-05-30 12:54,,,,,,"Industry,Academia",,,,4350.0,Open source,"Apache 2.0
https://github.com/google/automl/tree/master/lion","Industry,Academia",,,BF16,,,,,,
ViT-22B,Vision,"Object detection,Image classification",Google,"Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, Rodolphe Jenatton, Lucas Beyer, Michael Tschannen, Anurag Arnab, Xiao Wang, Carlos Riquelme, Matthias Minderer, Joan Puigcerver, Utku Evci, Manoj Kumar, Sjoerd van Steenkiste, Gamaleldin F. Elsayed, Aravindh Mahendran, Fisher Yu, Avital Oliver, Fantine Huot, Jasmijn Bastings, Mark Patrick Collier, Alexey Gritsenko, Vighnesh Birodkar, Cristina Vasconcelos, Yi Tay, Thomas Mensink, Alexander Kolesnikov, Filip PavetiÄ‡, Dustin Tran, Thomas Kipf, Mario LuÄiÄ‡, Xiaohua Zhai, Daniel Keysers, Jeremiah Harmsen, Neil Houlsby",2023-02-10,Scaling Vision Transformers to 22 Billion Parameters,https://arxiv.org/abs/2302.05442v1,428.0,SOTA improvement,"""The largest
ViT-22B sets the new SOTA on the challenging ObjectNet test set""",21743000000.0,"21.743B, Table 1",1.93248e+23,"""ViT-22B was trained using 256 visual tokens per image, where each token represents a 14 Ã— 14 patch extracted from 224 Ã— 224 sized images. ViT-22B is trained for 177k steps with batch size of 65k: approximately 3 epochs""

""ViT-22B was trained on 1024 TPU V4 chips for 177K steps""

""Using these techniques, ViT-22B processes 1.15k tokens per second per core during training (forward and backward pass) on TPUv4 [...] ViT-22Bâ€™s model flops utilization (MFU) is 54.9%""

256 * 177k * 65k = 2.945T tokens

So training time is 2.945T tokens / (1.15k * 2 * 1024) tokens/s = 1.25M seconds = 347.4 hours

So 1024 TPUv4 chips for 1.25M seconds at 54.9% MFU:
1024 * 2.75e14 * 1.25M * 0.549 = 1.93248e23",JFT-4B,"""Dataset. ViT-22B is trained on a version of JFT (Sun et al., 2017), extended to around 4B images (Zhai et al.,
2022a). These images have been semi-automatically annotated with a class-hierarchy of 30k labels""",4000000000.0,"""Dataset. ViT-22B is trained on a version of JFT (Sun et al., 2017), extended to around 4B images (Zhai et al.,
2022a). These images have been semi-automatically annotated with a class-hierarchy of 30k labels""",347.4,"""Using these techniques, ViT-22B processes 1.15k tokens per second per core during training (forward and backward pass)""
From model card we know they trained with 1024 TPUv4 chips, and there are 2 cores per chip. Total number of tokens was 177K steps * 65k images/step * 256 tokens/image = 2.945T tokens

So training time is 2.945T tokens / (1.15k * 2 * 1024) tokens/s = 1.25M seconds = 347.4 hours",Google TPU v4,,Confident,"The scaling of Transformers has driven breakthrough capabilities for language models. At present, the largest large language models (LLMs) contain upwards of 100B parameters. Vision Transformers (ViT) have introduced the same architecture to image and video modelling, but these have not yet been successfully scaled to nearly the same degree; the largest dense ViT contains 4B parameters (Chen et al., 2022). We present a recipe for highly efficient and stable training of a 22B-parameter ViT (ViT-22B) and perform a wide variety of experiments on the resulting model. When evaluated on downstream tasks (often with a lightweight linear model on frozen features), ViT-22B demonstrates increasing performance with scale. We further observe other interesting benefits of scale, including an improved tradeoff between fairness and performance, state-of-the-art alignment to human visual perception in terms of shape/texture bias, and improved robustness. ViT-22B demonstrates the potential for ""LLM-like"" scaling in vision, and provides key steps towards getting there.",2.9,,Unreleased,United States of America,,,,1024.0,0.549,2025-05-28 16:06,,,,,,Industry,,,,,Unreleased,don't see it here: https://github.com/google-research/vision_transformer?tab=readme-ov-file#available-vit-models ,Industry,$285555.57,,BF16,347446.80145890714,Hardware,,,,
ProteinDT,"Biology,Language",Proteins,"University of California (UC) Berkeley,California Institute of Technology,University of Toronto,University of Wisconsin Madison,Texas A&M,NVIDIA,Mila - Quebec AI (originally Montreal Institute for Learning Algorithms)","Shengchao Liu, Yanjing Li, Zhuoxinran Li, Anthony Gitter, Yutao Zhu, Jiarui Lu, Zhao Xu, Weili Nie, Arvind Ramanathan, Chaowei Xiao, Jian Tang, Hongyu Guo, and Anima Anandkumar",2023-02-09,A Text-guided Protein Design Framework,https://arxiv.org/abs/2302.04611,,SOTA improvement,"""Compared to six state-of-the-art protein sequence representation methods, ProteinDT can obtain consistently superior performance on four of six benchmark tasks.""",,,,,UniProtKB,They extract a subset of 441K protein-text pairs,197000001.0,"Total amino acids: 197,000,000 residues

Final calculation: 1.97 Ã— 10â¸ datapoints

Value = 197,000,000 = 1.97e8",,,,,Unknown,"Current AI-assisted protein design mainly utilizes protein sequential and structural information. Meanwhile, there exists tremendous knowledge curated by humans in the text format describing proteinsâ€™ high-level functionalities. Yet, whether the incorporation of such text data can help protein design tasks has not been explored. To bridge this gap, we propose ProteinDT, a multi-modal framework that leverages textual descriptions for protein design. ProteinDT consists of three subsequent steps: ProteinCLAP which aligns the representation of two modalities, a facilitator that generates the protein representation from the text modality, and a decoder that creates the protein sequences from the representation. To train ProteinDT, we construct a large dataset, SwissProtCLAP, with 441K text and protein pairs. We quantitatively verify the effectiveness of ProteinDT on three challenging tasks: (1) over 90% accuracy for text-guided protein generation; (2) best hit ratio on 10 zero-shot text-guided protein editing tasks; (3) superior performance on four out of six protein property prediction benchmarks.",,,Open weights (unrestricted),"United States of America,United States of America,Canada,United States of America,United States of America,United States of America,Canada",SciBERT,,,,,2025-06-04 17:57,,,,,,"Academia,Academia,Academia,Academia,Academia,Industry,Academia",,,,,Open source,"MIT license
https://github.com/chao1224/ProteinDT","Academia,Academia,Academia,Academia,Academia,Industry,Academia",,,FP16,,,,,,
Gen-1,Video,"Video generation,Text-to-video",Runway,"Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, Anastasis Germanidis",2023-02-06,Structure and Content-Guided Video Synthesis with Diffusion Models,https://arxiv.org/abs/2302.03011,377.0,SOTA improvement,,,,,,,,,,,,,,Unknown,"Text-guided generative diffusion models unlock powerful image creation and editing tools. While these have been extended to video generation, current approaches that edit the content of existing footage while retaining structure require expensive re-training for every input or rely on error-prone propagation of image edits across frames. In this work, we present a structure and content-guided video diffusion model that edits videos based on visual or textual descriptions of the desired output. Conflicts between user-provided content edits and structure representations occur due to insufficient disentanglement between the two aspects. As a solution, we show that training on monocular depth estimates with varying levels of detail provides control over structure and content fidelity. Our model is trained jointly on images and videos which also exposes explicit control of temporal consistency through a novel guidance method. Our experiments demonstrate a wide variety of successes; fine-grained control over output characteristics, customization based on a few reference images, and a strong user preference towards results by our model.",,,API access,United States of America,,,,,,2025-06-02 11:18,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
ProteinSGM,Biology,Protein design,University of Toronto,"Jin Sub Lee, Jisun Kim, Philip M. Kim",2023-02-04,ProteinSGM: Score-based generative modeling for de novo protein design,https://www.biorxiv.org/content/10.1101/2022.07.13.499967v2.abstract,,,,,,3.024e+19,"""The model is trained with a single NVIDIA V100 GPU using a batch size of 8 and learning rate 1Ã—10âˆ’4 for 2 million iterations, which consumes approximately 7 days."" Assume FP16 precision and 40% utilization. 
1.3e+14*0.4*604800s=3.0e+19",,,14001.0,"Total Proteins: 14,987
Training Set = 14,987 Ã— 0.95 = 14,238
Final Estimate = 1.4 Ã— 10^4 datapoints",168.0,,NVIDIA V100,,Confident,"The generation of de novo protein structures with predefined function and properties remains a challenging problem in protein design. Diffusion models, a novel state-of-the-art class of generative models, have recently shown astounding empirical performance in image synthesis. Here we use image-based representations of protein structure to develop ProteinSGM, a score-based diffusion model that produces realistic de novo proteins and can inpaint plausible backbones and domains into structures of predefined length. With unconditional generation, we show that ProteinSGM can generate native-like protein structures, surpassing the performance of previously reported generative models. We experimentally validate some de novo designs and observe strong structural consistency with generated backbones. Finally, we apply conditional generation to de novo protein design by formulating it as an image inpainting problem, allowing precise and modular design of protein structure.",,,,Canada,,,,1.0,,2025-05-01 10:42,,,,,,Academia,,,,,,,Academia,,,,329.03896884421727,Hardware,,,,
UniPi,"Multimodal,Video,Robotics,Vision",Video generation,"Google DeepMind,Massachusetts Institute of Technology (MIT),University of California (UC) Berkeley,Georgia Institute of Technology,University of Alberta","Yilun Du, Mengjiao Yang, Bo Dai, Hanjun Dai, Ofir Nachum, Joshua B. Tenenbaum, Dale Schuurmans, Pieter Abbeel",2023-01-31,Learning Universal Policies via Text-Guided Video Generation,https://arxiv.org/abs/2302.00111,146.0,,,,"Appears to be a composite model, not sure about the total parameter count.

""We use T5-XXL [22] to process input prompts which consists of 4.6 billion parameters. For combinatorial and multi-task generalization experiments on simulated robotic manipulation, we train a first-frame conditioned video diffusion models on 10x48x64 videos (skipping every 8 frames) with 1.7B parameters and a temporal super resolution of 20x48x64 (skipping every 4 frames) with 1.7B parameters. The resolution of the videos are chosen so that the objects being manipulated (e.g., blocks being moved around) are clearly visible in the video. For the real world video results, we finetune the 16x40x24 (1.7B), 32x40x24 (1.7B), 32x80x48 (1.4B), and 32x320x192 (1.2B) temporal
super resolution models pretrained on the data used by [19].""",,"UniPi was trained on 256 TPUv4 for an unknown duration, which could just about be >10^23 if the training time was 3 months and utilization was 33%. On balance, probably training compute is below 1e23 FLOP.","LAION-400M,Unspecified unreleased","""Our training data consists of an internet-scale pretraining dataset and a smaller real-world robotic dataset. The pretraining dataset uses the same data as [19], which consists of 14 million video-text pairs, 60 million image-text pairs, and the publicly available LAION-400M image-text dataset. The robotic dataset is adopted from the Bridge dataset [29] with 7.2k video-text pairs, where we use the task IDs as texts. We partition the 7.2k video-text pairs into train (80%) and test (20%) splits. We pretrain UniPi on the pretraining dataset followed by finetuning on the train split of the Bridge data.""",,,,,Google TPU v4,,Unknown,"A goal of artificial intelligence is to construct an agent that can solve a wide variety of tasks. Recent progress in text-guided image synthesis has yielded models with an impressive ability to generate complex novel images, exhibiting combinatorial generalization across domains. Motivated by this success, we investigate whether such tools can be used to construct more general-purpose agents. Specifically, we cast the sequential decision making problem as a text-conditioned video generation problem, where, given a text-encoded specification of a desired goal, a planner synthesizes a set of future frames depicting its planned actions in the future, after which control actions are extracted from the generated video. By leveraging text as the underlying goal specification, we are able to naturally and combinatorially generalize to novel goals. The proposed policy-as-video formulation can further represent environments with different state and action spaces in a unified space of images, which, for example, enables learning and generalization across a variety of robot manipulation tasks. Finally, by leveraging pretrained language embeddings and widely available videos from the internet, the approach enables knowledge transfer through predicting highly realistic video plans for real robots.",,,Open weights (unrestricted),"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France,United States of America,United States of America,United States of America,Canada",,,,,,2024-11-01 10:05,,,,,,"Industry,Academia,Academia,Academia,Academia",,,,,Open source,"https://github.com/flow-diffusion/AVDC
MIT License","Industry,Academia,Academia,Academia,Academia",,,,,,,,,
Flan T5-XXL + BLIP-2,"Multimodal,Language,Vision","Vision-language generation,Chat,Visual question answering",Salesforce Research,"Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi",2023-01-30,BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models,"https://arxiv.org/abs/2301.12597, https://huggingface.co/Salesforce/blip2-flan-t5-xl",2880.0,Highly cited,,12100000000.0,"12.1B, per Table 2. 

only 108M trainable params (i.e. params trained during the BLIP process)",,"fine-tuned from Flan-T5 XXL (11B) and ViT-g

fine-tuning compute:

""using a single 16-A100(40G) machine, our largest model with
ViT-g and FlanT5-XXL requires less than 6 days for the first
stage and less than 3 days for the second stage.""

16 * 9 days * 24 * 3600 * 312 teraflops * 0.3 ~= 1.2e21","COCO,LAION-400M","""We use the same pre-training dataset as BLIP with 129M images in total, including COCO (Lin
et al., 2014), Visual Genome (Krishna et al., 2017), CC3M (Sharma et al., 2018), CC12M (Changpinyo et al.,
2021), SBU (Ordonez et al., 2011), and 115M images from the LAION400M dataset""",,"""We use the same pre-training dataset as BLIP with 129M images in total""",200.0,"""less than 6 days for the first
stage and less than 3 days for the second stage""
9*24 is 216, rounding down a bit is 200 hours",NVIDIA A100 SXM4 40 GB,,Confident,"The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions.",,,Open weights (unrestricted),United States of America,Flan-T5 11B,1,"ViT-g is the other base model.

""using a single 16-A100(40G) machine, our largest model with
ViT-g and FlanT5-XXL requires less than 6 days for the first
stage and less than 3 days for the second stage.""

16 * 9 days * 24 * 3600 * 312 teraflops * 0.3 ~= 1.2e21",,,2025-05-28 16:06,,,,,,Industry,,,,,Open source,"BSD license (commercial)
https://github.com/salesforce/LAVIS/tree/main/projects/blip2",Industry,$99690.25,,BF16,,,,,,
BLIP-2 (Q-Former),"Vision,Language","Visual question answering,Image captioning",Salesforce Research,"Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi",2023-01-30,BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models,https://arxiv.org/abs/2301.12597,2880.0,SOTA improvement,"""BLIP-2 achieves state-of-the-art performance on various vision-language tasks""",1480000000.0,"Q-Former has 188M params. The BLIP-2 system overall has ""54x fewer trainable parameters"" than Flamingo80B.",1.20000000001e+21,https://www.wolframalpha.com/input?i=312+teraFLOPS+*+16+*+200+hours+*+0.33,"COCO,LAION-400M,Conceptual Captions (CC3M),Conceptual Captions 12M (CC12M),VisualGenome,SBU","""We use the same pre-training dataset as
BLIP with 129M images in total, including COCO (Lin
et al., 2014), Visual Genome (Krishna et al., 2017),
CC3M (Sharma et al., 2018), CC12M (Changpinyo et al.,
2021), SBU (Ordonez et al., 2011), and 115M images from
the LAION400M dataset (Schuhmann et al., 2021).""",,,200.0,"""For example, using
a single 16-A100(40G) machine, our largest model with
ViT-g and FlanT5-XXL requires less than 6 days for the first
stage and less than 3 days for the second stage.""
9 days = 216 hours",NVIDIA A100 SXM4 40 GB,,Confident,"The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions.",,,Open weights (unrestricted),United States of America,,,,16.0,,2025-05-28 16:06,,,,,,Industry,,,,3200.0,Open source,"https://github.com/salesforce/LAVIS/tree/main/projects/blip2 includes training and inference code

models here: https://huggingface.co/models?other=blip-2 ",Industry,$1960.82,,FP16,12776.908953102382,Hardware,,,,
KeAP,Biology,"Proteins,Protein representation learning","The University of Hong Kong,ByteDance,JancsiTech,OPPO HealthLab","Hong-Yu Zhou, Yunxiang Fu, Zhicheng Zhang, Cheng Bian, Yizhou Yu",2023-01-30,Protein Representation Learning via Knowledge Enhanced Primary Structure Modeling,https://arxiv.org/abs/2301.13154,6.0,,,,,,,ProteinKG25,,1525000000.0,"""ProteinKG25 (Zhang et al., 2022) provides a knowledge graph that consists of approximately five
million triplets, with nearly 600k protein, 50k attribute terms, and 31 relation terms included""

Assuming the relation and attribute terms are ~5 tokens and proteins are ~300 tokens

5000000*305=1525000000",,,,,Likely,"Protein representation learning has primarily benefited from the remarkable development of language models (LMs). Accordingly, pre-trained protein models also suffer from a problem in LMs: a lack of factual knowledge. The recent solution models the relationships between protein and associated knowledge terms as the knowledge encoding objective. However, it fails to explore the relationships at a more granular level, i.e., the token level. To mitigate this, we propose Knowledge-exploited Auto-encoder for Protein (KeAP), which performs token-level knowledge graph exploration for protein representation learning. In practice, non-masked amino acids iteratively query the associated knowledge tokens to extract and integrate helpful information for restoring masked amino acids via attention. We show that KeAP can consistently outperform the previous counterpart on 9 representative downstream applications, sometimes surpassing it by large margins. These results suggest that KeAP provides an alternative yet effective way to perform knowledge enhanced protein representation learning.",,,,"Hong Kong,China,China,China,China",,,,,,2025-05-01 10:42,,,,,,"Academia,Industry,Industry,Industry",,,,,,,"Academia,Industry,Industry,Industry",,,,,,,,,
Genie-SCOPe (bio),Biology,Protein design,Columbia University,"Yeqing Lin, Mohammed AlQuraishi",2023-01-29,"Generating Novel, Designable, and Diverse Protein Structures by Equivariantly Diffusing Oriented Residue Clouds",https://arxiv.org/abs/2301.12485,,,,4100000.0,"""RFDiffusion also contains around 14 times more parameters than Genie (59.8M versus 4.1M).""",1.81149696e+21,"""For Genie-SCOPe, we train the model using data parallelism on 12 A100
Nvidia GPUs with an effective batch size of 48. ""
12*3.1e+14*0.4*1209600s=1.8e+21",SCOPe 2.07,,1753200.0,"SCOPe Dataset: 8,766 domains Ã— 200 residues = 1,753,200 residues",336.0,"""We train Genie for 50,000 epochs (~9 days). For ",NVIDIA A100,,Confident,"Proteins power a vast array of functional processes in living cells. The capability to create new proteins with designed structures and functions would thus enable the engineering of cellular behavior and development of protein-based therapeutics and materials. Structure-based protein design aims to find structures that are designable (can be realized by a protein sequence), novel (have dissimilar geometry from natural proteins), and diverse (span a wide range of geometries). While advances in protein structure prediction have made it possible to predict structures of novel protein sequences, the combinatorially large space of sequences and structures limits the practicality of search-based methods. Generative models provide a compelling alternative, by implicitly learning the low-dimensional structure of complex data distributions. Here, we leverage recent advances in denoising diffusion probabilistic models and equivariant neural networks to develop Genie, a generative model of protein structures that performs discrete-time diffusion using a cloud of oriented reference frames in 3D space. Through in silico evaluations, we demonstrate that Genie generates protein backbones that are more designable, novel, and diverse than existing models. This indicates that Genie is capturing key aspects of the distribution of protein structure space and facilitates protein design with high success rates. Code for generating new proteins and training new versions of Genie is available at https://github. com/aqlaboratory/genie",30000.0,,Open weights (unrestricted),United States of America,,,,12.0,,2025-06-13 15:28,,,,,,Academia,,,,,Open source,"Apache 2.0
https://github.com/aqlaboratory/genie",Academia,,,,9582.89511749987,Hardware,,,,
RESP AI,Biology,"Drug discovery,Proteins",University of California San Diego,"Jonathan Parkinson, Ryan Hard, Wei Wang",2023-01-28,The RESP AI model accelerates the identification of tight-binding antibodies,https://www.nature.com/articles/s41467-023-36028-8#Sec28,,,,,,,,,"The raw sequence read data generated for this study has been uploaded
to the Sequence Read Archive (SRA) database under accession
code PRJNA813220. The antibody sequence data used to train the
autoencoder used in this study are available in the cAbRep database
[https://cab-rep.c2b2.columbia.edu/]. The construction of the cAbRep
database is described in Guo et al.",792000000.0,"""The end result of this process was thus a library of roughly 6 million sequences, half of which are human B-cell receptors and the other half of which are not. The autoencoder model is thus trained both to encode an input sequence and to embed information about typical features observed in true antibody sequences. The autoencoder was implemented using the
PyTorch library in Python 3.6.9 and trained on the full 6 million
sequence dataset until convergence.""

sequence length: 132 (amino acids)

6000000*132=792000000 tokens",,,,,Confident,"High-affinity antibodies are often identified through directed evolution, which may require many iterations of mutagenesis and selection to find an optimal candidate. Deep learning techniques hold the potential to accelerate this process but the existing methods cannot provide the confidence interval or uncertainty needed to assess the reliability of the predictions. Here we present a pipeline called RESP for efficient identification of high affinity antibodies. We develop a learned representation trained on over 3 million human B-cell receptor sequences to encode antibody sequences. We then develop a variational Bayesian neural network to perform ordinal regression on a set of the directed evolution sequences binned by off-rate and quantify their likelihood to be tight binders against an antigen. Importantly, this model can assess sequences not present in the directed evolution library and thus greatly expand the search space to uncover the best sequences for experimental evaluation. We demonstrate the power of this pipeline by achieving a 17-fold improvement in the KD of the PD-L1 antibody Atezolizumab and this success illustrates the potential of RESP in facilitating general antibody development.",8.0,,Open weights (unrestricted),United States of America,,,,,,2025-05-01 10:42,,,,,,Academia,,,,,Open source,"""The code used in this study is available online at https://github.com/
Wang-lab-UCSD/RESP (https://doi.org/10.5281/zenodo.7508853),
together with instructions on how to reproduce all key computational
experiments.""",Academia,,,,,,,,,
DDPM-IP (CelebA),Image generation,"Image generation,Text-to-image",Utrecht University,"Mang Ning, Enver Sangineto, Angelo Porrello, Simone Calderara, Rita Cucchiara",2023-01-27,Input Perturbation Reduces Exposure Bias in Diffusion Models,https://arxiv.org/abs/2301.11706v3,39.0,SOTA improvement,"""For instance, on CelebA 64Ã—64, we achieve a new state-of-theart FID score of 1.27, while saving 37.5% of the training time""",295000000.0,"295M for CelebA model, per Table 9",3.5e+20,"""We use Pytorch 1.8 (Paszke et al., 2019) and trained all the models on different NVIDIA Tesla V100s (16G memory). In
more detail, we use 2 GPUs to train the models on CIFAR10 for 2 days, and 4 GPUs to train the models on ImageNet 32Ã—32
for 34 days. For LSUN tower 64Ã—64, CelebA 64Ã—64 and FFHQ 128Ã—128, we used 16 GPUs to train the models for 3 days,
5 days and 4 days, respectively""

5*16 V100-days for CelebA.

5 * 16 * 24 * 3600 * 125 teraflops * 0.4 ~= 3.5e20",CelebA,,203000.0,,120.0,5 days,NVIDIA V100,,Likely,"Denoising Diffusion Probabilistic Models have shown an impressive generation quality, although their long sampling chain leads to high computational costs. In this paper, we observe that a long sampling chain also leads to an error accumulation phenomenon, which is similar to the exposure bias problem in autoregressive text generation. Specifically, we note that there is a discrepancy between training and testing, since the former is conditioned on the ground truth samples, while the latter is conditioned on the previously generated results. To alleviate this problem, we propose a very simple but effective training regularization, consisting in perturbing the ground truth samples to simulate the inference time prediction errors. We empirically show that, without affecting the recall and precision, the proposed input perturbation leads to a significant improvement in the sample quality while reducing both the training and the inference times. For instance, on CelebA 64Ã—64, we achieve a new state-of-the-art FID score of 1.27, while saving 37.5% of the training time. The code is publicly available at this https URL",681.0,,Open weights (unrestricted),Netherlands,,,,,,2025-06-02 15:25,,,,,,Academia,,,,,Open source,MIT license,Academia,$390.49,,FP16,,Hardware,,,,
MusicLM,Audio,Audio generation,Google,"Andrea Agostinelli, Timo I. Denk, ZalÃ¡n Borsos, Jesse Engel, Mauro Verzetti, Antoine Caillon, Qingqing Huang, Aren Jansen, Adam Roberts, Marco Tagliasacchi, Matt Sharifi, Neil Zeghidour, Christian Frank",2023-01-26,MusicLM: Generating Music From Text,https://arxiv.org/abs/2301.11325,329.0,SOTA improvement,"""We demonstrate that our method outperforms baselines on MusicCaps, a hand-curated, high-quality
dataset of 5.5k music-text pairs prepared by musicians.""",860000000.0,"""We use decoder-only Transformers for modeling the semantic stage and the acoustic stages of AudioLM. The models
share the same architecture, composed of 24 layers, 16 attention heads, an embedding dimension of 1024, feed-forward
layers of dimensionality 4096, dropout of 0.1, and relative
positional embeddings (Raffel et al., 2020), resulting in
430M parameters per stage.""

""stage"" seems to mean semantic + acoustic, so 860M total",,,Free Music Archive,"""We train SoundStream and w2v-BERT on the Free Music
Archive (FMA) dataset (Defferrard et al., 2017), whereas
the tokenizers and the autoregressive models for the semantic and acoustic modeling stages are trained on a dataset containing five million audio clips, amounting to 280k hours of
music at 24 kHz. Each of the stages is trained with multiple passes over the training data""",,>280k hours,,,,,Confident,"We introduce MusicLM, a model generating high-fidelity music from text descriptions such as ""a calming violin melody backed by a distorted guitar riff"". MusicLM casts the process of conditional music generation as a hierarchical sequence-to-sequence modeling task, and it generates music at 24 kHz that remains consistent over several minutes. Our experiments show that MusicLM outperforms previous systems both in audio quality and adherence to the text description. Moreover, we demonstrate that MusicLM can be conditioned on both text and a melody in that it can transform whistled and hummed melodies according to the style described in a text caption. To support future research, we publicly release MusicCaps, a dataset composed of 5.5k music-text pairs, with rich text descriptions provided by human experts.",,,Unreleased,United States of America,W2v-BERT,,also MuLan and SoundStream,,,2025-05-30 11:46,,,,,,Industry,checked,,,,Unreleased,,Industry,,,,,,,,,
MoLFormer-XL,Biology,"Protein generation,Protein folding prediction",IBM,"Jerret Ross, Brian Belgodere, Vijil Chenthamarakshan, Inkit Padhi, Youssef Mroueh, Payel Das",2023-01-25,An AI foundation model that learns the grammar of molecules,"https://research.ibm.com/blog/molecular-transformer-discovery
https://www.nature.com/articles/s42256-022-00580-7",,,,,,4.5090000000000007e+20,125000000000000*(208*16+1*12)*3600*0.3 = 4.509e+20,"PubChem,ZINC 15",,49236000000.0,"""MoLFormer-XL has been pretrained on 1.1 billion molecules represented as machine-readable strings of text.""

15K steps

""PubChem+ZINC (>1 billion data points) datasets""

mean token amount (table 4 from supplementary materials): 44.76

44.76*1.1*10^9 = 49236000000 tokens",208.0,"""Together, both techniques raised our per-GPU processing costs from 50 molecules to 1,600 molecules, allowing us to get away with 16 GPUs instead of 1,000. By eliminating hundreds of unnecessary GPUs, we consumed 61 times less energy and still had a trained model in five days.â€¯""

""Our pretraining task consists of training on the full dataset to 4 epochs. Training a single epoch of just PubChem on a single NVIDIA
V100 GPU would take approximately 60 hours. Utilizing Distributed Data Parallel, pre-training on the full PubChem dataset
alone took approx. 22 hours on 16 NVIDIA V100 GPUs this averages to about 5.5 hours per epoch. The speed up achieved by parallelizing training to 16 GPUs gave us a factor of 10.9. Pre-training for 4 epochs on the combined PubChem+ZINC datasets took approx 208 hours on a 16 NVIDIA V100 GPUs which averages to about 52 hours of compute for a single epoch. All
fine-tunning tasks were able to be performed on single GPUs (either V100 or A100) and completed in approx. 12 hours.""",NVIDIA V100,,Confident,"Large pretrained models are fast becoming AIâ€™s Swiss Army knife. Once limited to summarizing text and translating languages, they can now write code, compose music, and answer obscure questions at length.

Now thereâ€™s a new skill to add to their repertoire: the ability to infer the shapes and properties of molecules to predict how they might behave and to propose entirely new ones.

Most molecular models need estimates or measurements of a moleculeâ€™s 3D shape to accurately predict many of its properties. Chemists can extract this information through simulations or lab experiments, but itâ€™s an imperfect, expensive process that can take months to years. Perhaps unsurprisingly, we have detailed structures for only a few million molecules out of the trillions upon trillions potentially out there.

But now, there could be a way to eliminate this bottleneck in the discovery process with the help of AI. Introducing MoLFormer-XL, the latest addition to the MoLFormer family of foundation models for molecular discovery. MoLFormer-XL has been pretrained on 1.1 billion molecules represented as machine-readable strings of text. From these simple and accessible chemical representations, it turns out that a transformer can extract enough information to infer a moleculeâ€™s form and function.",4.0,,Open weights (unrestricted),United States of America,,,,16.0,,2025-05-01 10:42,,,,,,Industry,,,,,Open source,"Python codes for MoLFormer training and fine-tuning, and Python notebooks for MoLFormer attention visualization, as well as instances of pretrained models, are available at https://github.com/IBM/molformer. 

Apache 2.0",Industry,,,,9583.748775717228,Hardware,,,,
GPT-2+Active-SGD (WT2),Language,Language modeling,University of Montreal / UniversitÃ© de MontrÃ©al,"Davood Wadi, Marc Fredette, Sylvain Senecal",2023-01-24,Read the Signs Towards Invariance to Gradient Descentâ€™s Hyperparameter Initialization,https://arxiv.org/abs/2301.10133,0.0,,,124000000.0,,2.976e+17,"6 FLOP/parameter/token * 124000000.00 parameters * 2000000 tokens * 200 epochs [assumed: not reported for WT2, but reported for WT103 minigpt2 training -> ""Likely"" confidence] = 2.976e+17 FLOP

",WikiText-2,,,,,"""We use a heterogeneous cluster with each node comprised of 4 x
NVIDIA P100 Pascal (12G or 16G HBM2 memory) or 4 x NVIDIA V100 Volta (32G HBM2 memory). To simulate common practitionersâ€™ limited access to GPUs, we train the WikiText-2, WikiText-103, PASCAL VOC, and CIFAR-10
experiments on 1 GPU""","NVIDIA P100,NVIDIA V100",,Likely,"We propose ActiveLR, an optimization meta algorithm that localizes the learning rate, Î±, and adapts them at each epoch according to whether the gradient at each epoch changes sign or not. This sign-conscious algorithm is aware of whether from the previous step to the current one the update of each parameter has been too large or too small and adjusts the Î± accordingly. We implement the Active version (ours) of widely used and recently published gradient descent optimizers, namely SGD with momentum, AdamW, RAdam, and AdaBelief. Our experiments on ImageNet, CIFAR-10, WikiText-103, WikiText-2, and PASCAL VOC using different model architectures, such as ResNet and Transformers, show an increase in generalizability and training set fit, and decrease in training time for the Active variants of the tested optimizers. The results also show robustness of the Active variant of these optimizers to different values of the initial learning rate. Furthermore, the detrimental effects of using large mini-batch sizes are mitigated. ActiveLR, thus, alleviates the need for hyper-parameter search for two of the most commonly tuned hyper-parameters that require heavy time and computational costs to pick. We encourage AI researchers and practitioners to use the Active variant of their optimizer of choice for faster training, better generalizability, and reducing carbon footprint of training deep neural networks.",200.0,GPT-2+Active-SGD,Unreleased,Canada,,,,1.0,,2025-05-01 10:42,,,,,,Academia,,,,,Unreleased,,Academia,,,,,,,,,
Adaptive Agent,Games,Open ended play,DeepMind,"Adaptive Agent Team, Jakob Bauer, Kate Baumli, Satinder Baveja, Feryal Behbahani, Avishkar Bhoopchand, Nathalie Bradley-Schmieg, Michael Chang, Natalie Clay, Adrian Collister, Vibhavari Dasagi, Lucy Gonzalez, Karol Gregor, Edward Hughes, Sheleem Kashem, Maria Loks-Thompson, Hannah Openshaw, Jack Parker-Holder, Shreya Pathak, Nicolas Perez-Nieves, Nemanja Rakicevic, Tim RocktÃ¤schel, Yannick Schroecker, Jakub Sygnowski, Karl Tuyls, Sarah York, Alexander Zacherl, Lei Zhang",2023-01-18,Human-Timescale Adaptation in an Open-Ended Task Space,https://arxiv.org/abs/2301.07608,89.0,,,533000000.0,Table D.9,2.8e+21,"""AdA was implemented using JAX (Bradbury et al., 2018) and the DeepMind JAX Ecosystem (Babuschkin et al., 2020) and trained on 64 Google TPUv3 devices. The wall-clock time for training this version of AdA from scratch was approximately 5 weeks: 1 week to train the teacher, and 4 weeks to train AdA""

64 * 123 teraflop/s * 35 days * 24 * 3600 * 0.4 = 9.5e21

This might be for all single-agent experiments in the paper, or just for the 76M model in Table D.1, I'm not sure.

In Table E.2, the 533M-param model takes 2e20 FLOP to go through 5B learner steps, and was trained on 70B steps in total (Table 1). That would be 2.8e21 for 70B steps. That might be an underestimate because there are also teacher(?) steps.",,"RL in XLand 2.0 task space, ""an environment supporting procedural generation of diverse 3D worlds and
multi-player games""",,,840.0,5 weeks. Possible that this is for multiple models,Google TPU v3,Reinforcement learning,Speculative,"Foundation models have shown impressive adaptation and scalability in supervised and self-supervised learning problems, but so far these successes have not fully translated to reinforcement learning (RL). In this work, we demonstrate that training an RL agent at scale leads to a general in-context learning algorithm that can adapt to open-ended novel embodied 3D problems as quickly as humans. In a vast space of held-out environment dynamics, our adaptive agent (AdA) displays on-the-fly hypothesis-driven exploration, efficient exploitation of acquired knowledge, and can successfully be prompted with first-person demonstrations. Adaptation emerges from three ingredients: (1) meta-reinforcement learning across a vast, smooth and diverse task distribution, (2) a policy parameterised as a large-scale attention-based memory architecture, and (3) an effective automated curriculum that prioritises tasks at the frontier of an agent's capabilities. We demonstrate characteristic scaling laws with respect to network size, memory length, and richness of the training task distribution. We believe our results lay the foundation for increasingly general and adaptive RL agents that perform well across ever-larger open-ended domains.",,,Unreleased,United Kingdom of Great Britain and Northern Ireland,,,,,,2025-06-16 10:47,,,,,,Industry,,,,,Unreleased,,Industry,,,,,Hardware,,,,
Ankh_large,Biology,"Protein generation,Proteins,Protein or nucleotide language model (pLM/nLM)","Technical University of Munich,Columbia University","Ahmed Elnaggar, Hazem Essam, Wafaa Salah-Eldin, Walid Moustafa, Mohamed Elkerdawy, Charlotte Rochereau, Burkhard Rost",2023-01-16,Ankh: Optimized Protein Language Model Unlocks General-Purpose Modelling,https://arxiv.org/abs/2301.06568,59.0,SOTA improvement,"""On average, Ankh improved the PLM SOTA performance by 4.8%""",1900000000.0,"Figure 1 indicates 1.15B parameters, but both the huggingface model and a replication (https://huggingface.co/ElnaggarLab/ankh-large and https://www.biorxiv.org/content/10.1101/2023.07.05.547496v1.full.pdf) indicate 1.9B parameters.
Notebook for counting params: https://colab.research.google.com/drive/1EGI5_vDl4pOBUukJexMHQR16BFKJe4a5?usp=sharing",6.5e+21,"Table 9 from here: https://www.biorxiv.org/content/10.1101/2023.07.05.547496v1.full.pdf

Can also be manually estimated based on the details in Table 11 and 4.6.1 Exp 4. 14B residues * 68 epochs = 952B tokens seen in forward passes. However, only 20% of tokens are masked as individual targets; other tokens in consecutive spans are collapsed into single-token targets to reduce computations. For masking rate of 20%, the average sequence will have 36% as many targets as input tokens under this strategy. This is the relevant number of backward passes:
(2 * 952B * 19B) + (4 * 952B * 0.36 * 19B) = 6.22e22

36% figure verified here: https://colab.research.google.com/drive/1ETsmp_KRMK8kIRA5kdfcO9QiPK28cBQ6?usp=sharing ",UniRef50,"""We build upon the same results by pre-training our baseline on UniRef50.""",14000000000.0,"Pretrained over UniRef50; 45M proteins and 14B amino acids, per Table 2

952B tokens from Table 9 at:
https://www.biorxiv.org/content/10.1101/2023.07.05.547496v1
(This is total tokens over multiple epochs)",,,Google TPU v4,,Confident,"As opposed to scaling-up protein language models (PLMs), we seek improving performance via protein-specific optimization. Although the proportionality between the language model size and the richness of its learned representations is validated, we prioritize accessibility and pursue a path of data-efficient, cost-reduced, and knowledge-guided optimization. Through over twenty experiments ranging from masking, architecture, and pre-training data, we derive insights from protein-specific experimentation into building a model that interprets the language of life, optimally. We present Ankh, the first general-purpose PLM trained on Googleâ€™s TPU-v4 surpassing the state-of-the-art performance with fewer parameters (<10% for pre-training, <7% for inference, and <30% for the embedding dimension). We provide a representative range of structure and function benchmarks where Ankh excels. We further provide a protein variant generation analysis on High-N and One-N input data scales where Ankh succeeds in learning protein evolutionary conservation-mutation trends and introducing functional diversity while retaining key structural-functional characteristics. We dedicate our work to promoting accessibility to research innovation via attainable resources.",68.0,,Open weights (non-commercial),"Germany,United States of America",,,,64.0,,2025-06-06 12:39,,,,524288.0,Table 11,"Academia,Academia",,,,,Unreleased,"cc non-commercial:
https://github.com/agemagician/Ankh/blob/main/LICENSE.md

cc-by-nc for weigths:
https://huggingface.co/ElnaggarLab/ankh-large
","Academia,Academia",$4802.40,,,21727.518178781516,"Operation counting,Third-party estimation",ElnaggarLab,,,
Nucleotide Transformer,Biology,Protein or nucleotide language model (pLM/nLM),"NVIDIA,Technical University of Munich","Hugo Dalla-Torre, Liam Gonzalez, Javier Mendoza Revilla, Nicolas Lopez Carranza, Adam Henryk Grzywaczewski, Francesco Oteri, Christian Dallago, Evan Trop, Hassan Sirelkhatim, Guillaume Richard, Marcin Skwark, Karim Beguir,
Marie Lopez, Thomas Pierrot",2023-01-15,"The Nucleotide Transformer: Building and Evaluating Robust
Foundation Models for Human Genomics",https://www.biorxiv.org/content/10.1101/2023.01.11.523679v1.full.pdf,22.0,SOTA improvement,"""We show that the representations alone match or outperform specialized
methods on 11 of 18 prediction tasks, and up to 15 after fine-tuning.""",2500000000.0,"""We built four distinct foundation language models of different sizes, ranging from 500M up to 2.5B parameters""",8.08e+21,"""Training the largest parameter model required a total of 128 GPUs across 16 compute nodes for 28 days""

In repo, they default to jnp.float32, but recommend fp16 or bp16 for activations in the docstring. JAX defaults to TF32 so they should be utilizing tensor cores.

Assuming 1.56e14 FLOP/s for 32-bit calculations and 0.3 utilization rate

Estimate: 1.56e14 FLOP/s * 128 GPUs * 28 days * 24 h/day * 3600 s/h * 0.3 utilization rate = 1.45e22

Or, with 6ND:
""the model processed a total of 300B tokens during training""
6 * 300B * 2.5B = 4.5e21

Geometric mean: sqrt(1.45e22 * 4.5e21) = 8.08e21","Human Reference Genome (GRCh38/hg38),1000 Genomes Project","""pre-trained them on three different datasets encompassing the Human
reference genome, a collection of 3,200 diverse human genomes, and 850 genomes from several species""
",300000000000.0,"Largest dataset is the 1000 Genome dataset, with 3202 genomes for a total of 20.5 trillion nucleotides. However, for each dataset training was run until the model saw 300B tokens.",672.0,"""Training the largest parameter model required a total of 128 GPUs across 16 compute nodes for 28 days""",NVIDIA A100,Self-supervised learning,Likely,"Closing the gap between measurable genetic information and observable traits is a longstanding challenge in genomics. Yet, the prediction of molecular phenotypes from DNA sequences alone remains limited and inaccurate, often driven by the scarcity of annotated data and the inability to transfer learnings between prediction tasks. Here, we present an extensive study of foundation models pre-trained on DNA sequences, named the Nucleotide Transformer, integrating information from 3,202 diverse human genomes, as well as 850 genomes from a wide range of species, including model and non-model organisms. These transformer models yield transferable, context-specific representations of nucleotide sequences, which allow for accurate molecular phenotype prediction even in low-data settings. We show that the representations alone match or outperform specialized methods on 11 of 18 prediction tasks, and up to 15 after fine-tuning. Despite no supervision, the transformer models learnt to focus attention on key genomic elements, including those that regulate gene expression, such as enhancers. Lastly, we demonstrate that utilizing model representations alone can improve the prioritization of functional genetic variants. The training and application of foundational models in genomics explored in this study provide a widely applicable stepping stone to bridge the gap of accurate molecular phenotype prediction from DNA sequence alone.
",1.0,,Open weights (non-commercial),"United States of America,Germany",,935000000000000000,"""All fine-tuning runs were performed on a single node with eight A100 GPUs. [...] On average, a fine-tuning run lasted 20 minutes for the 500M parameter models, and 50 minutes for the 2.5B parameter models.""

Estimate: 78e12 FLOP/s * 8 GPUs * 50 min * 60 seconds * 0.5 utilization rate = 9.36e17",128.0,,2025-06-01 16:53,,,,,,"Industry,Academia",,,,86016.0,Unreleased,"Attribution-NonCommercial-ShareAlike 4.0 International license
https://github.com/instadeepai/nucleotide-transformer

In this repository, you will find the following:

Inference code for our models
Pre-trained weights for all 9 NT models and 2 SegmentNT models
Instructions for using the code and pre-trained models","Industry,Academia",$51064.71,,"TF32,FP32",102249.42137571,"Operation counting,Hardware",,,,
DreamerV3,"Multimodal,Games",Open ended play,"DeepMind,University of Toronto","Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, Timothy Lillicrap ",2023-01-10,Mastering Diverse Domains through World Models,https://arxiv.org/abs/2301.04104v1,645.0,SOTA improvement,"Using the same hyperparameters across all domains, DreamerV3 outperforms specialized model-free and model-based algorithms in a wide range of benchmarks and data-efficiency regimes. Applied out of the box, DreamerV3 also learns to obtain diamonds in the popular video game Minecraft from scratch given sparse rewards, a long-standing challenge in artificial intelligence for which previous approaches required human data or domain-specific heuristics.",200000000.0,Table B1,2.2032e+20,"16 environment instances, each with 1 V100 running for 17 days (table A1) - it's not entirely clear if the GPU days already account for multiple environment instances.

Assuming no:
Compute: 17*24*60*60*125000000000000*0.3=5.508e+19
Assuming yes:
Compute: 17*24*60*60*16*125000000000000*0.3=8.8128e+20

Geometric mean: 220320000000000000000",,Table A1 with the largest experiment being Minecraft (17 GPU days),100000000.0,,6528.0,,NVIDIA V100,,Likely,"General intelligence requires solving tasks across many domains. Current reinforcement learning algorithms carry this potential but are held back by the resources and knowledge required to tune them for new tasks. We present DreamerV3, a general and scalable algorithm based on world models that outperforms previous approaches across a wide range of domains with fixed hyperparameters. These domains include continuous and discrete actions, visual and low-dimensional inputs, 2D and 3D worlds, different data
budgets, reward frequencies, and reward scales. We observe favorable scaling properties of DreamerV3, with larger models directly translating to higher data-efficiency and final performance. Applied out of the box, DreamerV3 is the first algorithm to collect diamonds in Minecraft from scratch without human data or curricula, a long-standing challenge in artificial intelligence. Our general algorithm makes reinforcement learning broadly applicable and allows scaling to hard decision making problems
",,,Unreleased,"United Kingdom of Great Britain and Northern Ireland,Canada",,,,16.0,,2025-05-30 11:06,,,,,,"Industry,Academia",,,,,Open source,"Apache 2.0
https://github.com/danijar/dreamerv3","Industry,Academia",,,BF16,9586.950671364422,,,,,
SantaCoder,Language,Code generation,"Hugging Face,ServiceNow,Massachusetts Institute of Technology (MIT),Wellesley College,Saama,EleutherAI,Huawei Noah's Ark Lab,Carnegie Mellon University (CMU)","Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, Logesh Kumar Umapathi, Carolyn Jane Anderson, Yangtian Zi, Joel Lamy Poirier, Hailey Schoelkopf, Sergey Troshin, Dmitry Abulkhanov, Manuel Romero, Michael Lappert, Francesco De Toni, Bernardo GarcÃ­a del RÃ­o, Qian Liu, Shamik Bose, Urvashi Bhattacharyya, Terry Yue Zhuo, Ian Yu, Paulo Villegas, Marco Zocca, Sourab Mangrulkar, David Lansky, Huu Nguyen, Danish Contractor, Luis Villa, Jia Li, Dzmitry Bahdanau, Yacine Jernite, Sean Hughes, Daniel Fried, Arjun Guha, Harm de Vries, Leandro von Werra",2023-01-09,SantaCoder: don't reach for the stars!,https://arxiv.org/abs/2301.03988,163.0,,"""Our best model outperforms previous open-source multilingual code generation models (InCoder-6.7B and CodeGen-Multi-2.7B) in both left-to-right generation and infilling on the Java, JavaScript, and Python portions of MultiPL-E, despite being a substantially smaller model""",1100000000.0,1.1B,2.1e+21,Each training run takes 3.1 days to complete on 96 Tesla V100 GPUs for a total of 1.05 Ã— 10^21 FLOPs. The final model described in Section 6.2 uses twice the amount of compute.,The Stack v1.1,"""The base training dataset for the experiments in this paper contains 268 GB of Python, Java and JavaScript files from The Stack v1.1 (Kocetkov et al., 2022) after removing data from optout requests, near-deduplication, PII-redaction (see Section 4)""",,268 GB,150.0,"Their initial training runs took 3.1 days. The final training run was run for twice as many iterations with ""all other hyper-parameters the same"" and used twice as much compute as this. So likely 6 days or ~150 hours, but they don't explicitly say whether they used the same hardware.",NVIDIA V100,,Confident,"The BigCode project is an open-scientific collaboration working on the responsible development of large language models for code. This tech report describes the progress of the collaboration until December 2022, outlining the current state of the Personally Identifiable Information (PII) redaction pipeline, the experiments conducted to de-risk the model architecture, and the experiments investigating better preprocessing methods for the training data. We train 1.1B parameter models on the Java, JavaScript, and Python subsets of The Stack and evaluate them on the MultiPL-E text-to-code benchmark. We find that more aggressive filtering of near-duplicates can further boost performance and, surprisingly, that selecting files from repositories with 5+ GitHub stars deteriorates performance significantly. Our best model outperforms previous open-source multilingual code generation models (InCoder-6.7B and CodeGen-Multi-2.7B) in both left-to-right generation and infilling on the Java, JavaScript, and Python portions of MultiPL-E, despite being a substantially smaller model. All models are released under an OpenRAIL license at this https URL.",,,Open weights (restricted use),"Multinational,United States of America,United States of America,United States of America,United States of America,United States of America,Multinational,United States of America,China,United States of America",,,,,,2025-02-14 16:51,,,,,,"Industry,Industry,Academia,Academia,Research collective,Industry,Academia",checked,,,,Unreleased,"license - commercial, usage restrictions against things like discrimination and misinformation 
https://huggingface.co/spaces/bigcode/license","Industry,Industry,Academia,Academia,Research collective,Industry,Academia",,,,,Hardware,,,,
VALL-E,"Audio,Speech","Speech synthesis,Text-to-speech",Microsoft,"Chengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang, Long Zhou, Shujie Liu, Zhuo Chen, Yanqing Liu, Huaming Wang, Jinyu Li, Lei He, Sheng Zhao, Furu Wei",2023-01-05,Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers,https://arxiv.org/abs/2301.02111,471.0,SOTA improvement,"""VALL-E significantly outperforms
the state-of-the-art zero-shot TTS system [Casanova et al., 2022b] in terms of speech naturalness and
speaker similarity, with +0.12 comparative mean option score (CMOS) and +0.93 similarity mean
option score (SMOS) improvement on LibriSpeech""",353000000.0,"""Both the AR model and the NAR model have the same transformer architecture with 12
layers, 16 attention heads, an embedding dimension of 1024, a feed-forward layer dimension of 4096, and a dropout of 0.1""

Ben's script says that's 353M parameters, using n_block 12, d_model 1024, d_ff 4096, encoder only False

https://github.com/bencottier/ml-parameter-count/blob/main/parameter_count.py",1.01e+19,"""The models are trained using 16 NVIDIA TESLA V100 32GB GPUs with a batch size of 6k acoustic
tokens per GPU for 800k steps""

353M * 800k * 6k * 6 = 1.01e19

16 V100s is 2080 teraFLOP or 2e15 FLOP so 1e19 would take 1.5 hours at 100% utilization or ~5 hours at 30%. Is that plausible?",LibriLight,"""60K hours of English speech with over 7000 unique speakers.""",820800000.0,"60k hours
~13,680 words/hour * 60,000 = 820800000 words
https://docs.google.com/document/d/1G3vvQkn4x_W71MKg0GmHVtzfd9m0y3_Ofcoew0v902Q/edit#heading=h.3pbt0hfgv7pq",,,NVIDIA V100,Supervised,Speculative,"We introduce a language modeling approach for text to speech synthesis (TTS). Specifically, we train a neural codec language model (called Vall-E) using discrete codes derived from an off-the-shelf neural audio codec model, and regard TTS as a conditional language modeling task rather than continuous signal regression as in previous work. During the pre-training stage, we scale up the TTS training data to 60K hours of English speech which is hundreds of times larger than existing systems. Vall-E emerges in-context learning capabilities and can be used to synthesize high-quality personalized speech with only a 3-second enrolled recording of an unseen speaker as an acoustic prompt. Experiment results show that Vall-E significantly outperforms the state-of-the-art zero-shot TTS system in terms of speech naturalness and speaker similarity. In addition, we find Vall-E could preserve the speaker's emotion and acoustic environment of the acoustic prompt in synthesis. See this https URL for demos of our work.",,,Unreleased,"United States of America,Multinational,India,Belgium",,,,,,2025-06-04 17:46,,,,,,Industry,checked,,,,Unreleased,onlly demos https://www.microsoft.com/en-us/research/project/vall-e-x/vall-e/,Industry,$11.41,,,,Operation counting,,,,
DNA Fine-Tuned Language Model (DFLM),Biology,Protein or nucleotide language model (pLM/nLM),Tongji University,"Ying He , Qinhu Zhang , Siguo Wang , Zhanheng Chen , Zhen Cui ,
Zhen-Hao Guo, and De-Shuang Huang",2023-01-02,Predicting the Sequence Specificities of DNA-Binding Proteins by DNA Fine-Tuned Language Model With Decaying Learning Rates,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9751352,4.0,,,,,3.5e+18,"""The pre-training of Human genome language model is
resource-intensive (about 7-10 days on 2 NVIDIA TITAN X GPU).""

Assuming FP32 and 30% utilization

Estimate: (10*24*3600) s * 6.7e12 FLOP/s * 2 * 0.3 = 3.5e18",Human Reference Genome (GRCh38/hg38),"""DFLM mainly used two types of data sets, the unlabeled large-scale Genome Reference Consortium Human Build 38(GRCh38/hg38) [1] and about 500 labeled ChIP-seq data set [2]. The unlabeled large-scale genome data set hg38 was used to train a general Human Genome language model. The ChIP-seq data sets were used to train the DBP fine-tuning language model and classification model.""",,,,,NVIDIA TITAN Xp,,Confident,"DNA-binding proteins (DBPs) play vital roles in the regulation of biological systems. Although there are already many deep learning methods for predicting the sequence specificities of DBPs, they face two challenges as follows. Classic deep learning methods for DBPs prediction usually fail to capture the dependencies between genomic sequences since their commonly used one-hot codes are mutually orthogonal. Besides, these methods usually perform poorly when samples are inadequate. To address these two challenges, we developed a novel language model for mining DBPs using human genomic data and ChIP-seq datasets with decaying learning rates, named DNA Fine-tuned Language Model (DFLM). It can capture the dependencies between genome sequences based on the context of human genomic data and then fine-tune the features of DBPs tasks using different ChIP-seq datasets. First, we compared DFLM with the existing widely used methods on 69 datasets and we achieved excellent performance. Moreover, we conducted comparative experiments on complex DBPs and small datasets. The results show that DFLM still achieved a significant improvement. Finally, through visualization analysis of one-hot encoding and DFLM, we found that one-hot encoding completely cut off the dependencies of DNA sequences themselves, while DFLM using language models can well represent the dependency of DNA sequences. Source code are available at: https://github.com/Deep-Bioinfo/DFLM",,,Unreleased,China,,,,,,2025-04-30 10:04,,,,,,Academia,,,,,Open source,"MIT for code. not sure there are weights here?
https://github.com/Deep-Bioinfo/DFLM",Academia,,,,,Hardware,,,,
Hybrid H3-2.7B,Language,Language modeling/generation,"Stanford University,University at Buffalo","Daniel Y. Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, Christopher RÃ©",2022-12-28,Hungry Hungry Hippos: Towards Language Modeling with State Space Models,https://arxiv.org/abs/2212.14052,271.0,SOTA improvement,Results table shows SOTA performance for some benchmarks,2700000000.0,2.7B,6.48e+21,"6 FLOP/token/parameter * 400000000000 training tokens * 2700000000 parameters = 6.48e+21 FLOP

___________________
in the algorithmic progress paper the estimation was 8.49 Ã— 10^20 based on the assumption of WT-103 dataset and 509 epochs",The Pile,,400000000000.0,"""We train hybrid models at sizes 125M, 355M, 1.3B, and 2.7B on the Pile [21] for 400B tokens""",,"""All models were trained on either a single 16xA100-40GB node or a cluster of 8xA100-80GB nodes.""",NVIDIA A100 SXM4 80 GB,,Likely,"State space models (SSMs) have demonstrated state-of-the-art sequence modeling performance in some modalities, but underperform attention in language modeling. Moreover, despite scaling nearly linearly in sequence length instead of quadratically, SSMs are still slower than Transformers due to poor hardware utilization. In this paper, we make progress on understanding the expressivity gap between SSMs and attention in language modeling, and on reducing the hardware barrier between SSMs and attention. First, we use synthetic language modeling tasks to understand the gap between SSMs and attention. We find that existing SSMs struggle with two capabilities: recalling earlier tokens in the sequence and comparing tokens across the sequence. To understand the impact on language modeling, we propose a new SSM layer, H3, that is explicitly designed for these abilities. H3 matches attention on the synthetic languages and comes within 0.4 PPL of Transformers on OpenWebText. Furthermore, a hybrid 125M-parameter H3-attention model that retains two attention layers surprisingly outperforms Transformers on OpenWebText by 1.0 PPL. Next, to improve the efficiency of training SSMs on modern hardware, we propose FlashConv. FlashConv uses a fused block FFT algorithm to improve efficiency on sequences up to 8K, and introduces a novel state passing algorithm that exploits the recurrent properties of SSMs to scale to longer sequences. FlashConv yields 2Ã— speedup on the long-range arena benchmark and allows hybrid language models to generate text 2.4Ã— faster than Transformers. Using FlashConv, we scale hybrid H3-attention language models up to 2.7B parameters on the Pile and find promising initial results, achieving lower perplexity than Transformers and outperforming Transformers in zero- and few-shot learning on a majority of tasks in the SuperGLUE benchmark.",509.02,Hybrid H3-2.7B,Open weights (unrestricted),"United States of America,United States of America",,,,8.0,,2025-05-28 16:06,,,,,,"Academia,Academia",,,,,Unreleased,"apache 2.0
repo (weights and inference only): https://github.com/HazyResearch/H3/blob/main/README.md ","Academia,Academia",,,BF16,6393.151008587547,Operation counting,,,,
OPT-IML (175B),Language,Language modeling,Meta AI,"Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov, Daniel Simig, Ping Yu, Kurt Shuster, Tianlu Wang, Qing Liu, Punit Singh Koura, Xian Li, Brian O'Horo, Gabriel Pereyra, Jeff Wang, Christopher Dewan, Asli Celikyilmaz, Luke Zettlemoyer, Ves Stoyanov",2022-12-22,OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization,https://arxiv.org/abs/2212.12017,236.0,,,175000000000.0,,4.3e+23,"fine-tuned from OPT-175B (4.3e23) with an estimate 2.1e21 FLOP for fine-tuning. 
""During fine-tuning, our models saw approximately 2 billion tokens, which is only 0.6% of the pre-training budget of OPT""",OPT-IML Bench,"(fine-tuning dataset) ""To this end, we create OPT-IML Bench: a large benchmark for Instruction MetaLearning (IML) of 2000 NLP tasks consolidated into task categories from 8 existing benchmarks""",2000000000.0,"""During fine-tuning, our models saw approximately 2 billion tokens, which is only 0.6% of the pre-training budget of OPT""",72.0,Table 3,NVIDIA A100,,Likely,"Recent work has shown that fine-tuning large pre-trained language models on a collection of tasks described via instructions, a.k.a. instruction-tuning, improves their zero and few-shot generalization to unseen tasks. However, there is a limited understanding of the performance trade-offs of different decisions made during the instruction-tuning process. These decisions include the scale and diversity of the instruction-tuning benchmark, different task sampling strategies, fine-tuning with and without demonstrations, training using specialized datasets for reasoning and dialogue, and finally, the fine-tuning objectives themselves. In this paper, we characterize the effect of instruction-tuning decisions on downstream task performance when scaling both model and benchmark sizes. To this end, we create OPT-IML Bench: a large benchmark for Instruction Meta-Learning (IML) of 2000 NLP tasks consolidated into task categories from 8 existing benchmarks, and prepare an evaluation framework to measure three types of model generalizations: to tasks from fully held-out categories, to held-out tasks from seen categories, and to held-out instances from seen tasks. Through the lens of this framework, we first present insights about instruction-tuning decisions as applied to OPT-30B and further exploit these insights to train OPT-IML 30B and 175B, which are instruction-tuned versions of OPT. OPT-IML demonstrates all three generalization abilities at both scales on four different evaluation benchmarks with diverse tasks and input formats -- PromptSource, FLAN, Super-NaturalInstructions, and UnifiedSKG. Not only does it significantly outperform OPT on all benchmarks but is also highly competitive with existing models fine-tuned on each specific benchmark. We release OPT-IML at both scales, together with the OPT-IML Bench evaluation framework.",,,Open weights (non-commercial),United States of America,OPT-175B,2,"""We fine-tune all 30B models on 64 40GB A100s, and 175B models on 128 40GB A100s"", no timeframe specified

fine-tuned on 2B tokens. 2B * 175B * 6 = 2.1e21

",128.0,,2025-05-16 10:30,,,,125.0,Table 3,Industry,,,,9216.0,Unreleased,"unclear license
https://huggingface.co/facebook/opt-iml-30b",Industry,,,,102304.08471008124,Operation counting,,,,
CaLM,Biology,"Protein or nucleotide language model (pLM/nLM),Protein embedding",University of Oxford,"Carlos Outeiral, Charlotte M. Deane",2022-12-19,Codon language embeddings provide strong signals for protein engineering,https://www.biorxiv.org/content/10.1101/2022.12.15.519894v1.full.pdf,8.0,SOTA improvement,"""We show that large language models trained on codons, instead of amino acid sequences, provide high-quality representations that outperform comparable state-of-the-art models across a variety of tasks. In some tasks, like species recognition, prediction of protein and transcript abundance, or melting point estimation, we show that a language model trained on codons outperforms every other published protein language model, including some that contain over 50 times more parameters"" [Abstract]",86000000.0,"""We trained a large language model with 86M parameters""",2.9e+19,"""4 NVIDIA Quadro RTX4000 GPUs for 40 days""

Calculation assuming FP32, utilization 30%:
= (40 * 24 * 3600) s * 7.1e12 FLOP/s * 0.3 * 4 GPU = 2.999808e+19

alternative calculation:
""Gradients were accumulated to an effective batch size of 1,000 examples, or approximately 256,000 tokens. ""
""(66,000 gradient steps, 14 full epochs)""

256000*66000*14*86000000*6=1.220567e+20",European Nucleotide Archive (ENA),"""The training set was constructed from the European Nucleotide Archive [39], with significant preprocessing to limit redundancy and save computational cost.""",2304000000.0,"""a dataset of 9M non-redundant and diverse cDNA sequences identified from whole-genome sequencing""

""Gradients were accumulated to an effective batch size of 1,000 examples, or approximately 256,000 tokens. ""

9000000*256000/1000=2304000000 tokens",960.0,"""The model reported in this work was trained on 4 NVIDIA Quadro
RTX4000 GPUs for 40 days (66,000 gradient steps, 14 full epochs)""",NVIDIA Quadro RTX 4000,,Likely,"Protein representations from deep language models have yielded state-of-the-art performance across many tasks in computational protein engineering. In recent years, progress has primarily focused on parameter count, with recent modelsâ€™ capacities surpassing the size of the very datasets they were trained on. Here, we propose an alternative direction. We show that large language models trained on codons, instead of amino acid sequences, provide high-quality representations that outperform comparable state-of-the-art models across a variety of tasks.
In some tasks, like species recognition, prediction of protein and transcript abundance, or melting point estimation, we show that a language model trained on codons outperforms every other published protein language model, including some that contain over 50 times more parameters. These results suggest that, in addition to commonly studied scale and model complexity, the information content of biological data provides an orthogonal direction to improve the power of machine learning in biology.",14.0,,Unreleased,United Kingdom of Great Britain and Northern Ireland,,,,4.0,,2025-06-02 14:59,,,,,,Academia,,,,3840.0,Open source,"BSD-3-Clause license
https://github.com/oxpig/CaLM",Academia,,,,1278.886496016633,"Hardware,Operation counting",,,,
text-embedding-ada-002,Language,Language modeling,OpenAI,,2022-12-15,New and improved embedding model,https://openai.com/index/new-and-improved-embedding-model/,,,,,,,,,,,,,,,,Unknown,,,,,United States of America,,,,,,2025-05-01 10:42,,,,,,Industry,,,,,,,Industry,,,,,,,,,
RT-1,Robotics,Robotic manipulation,Google,"Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Tomas Jackson, Sally Jesmonth, Nikhil J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Kuang-Huei Lee, Sergey Levine, Yao Lu, Utsav Malla, Deeksha Manjunath, Igor Mordatch, Ofir Nachum, Carolina Parada, Jodilyn Peralta, Emily Perez, Karl Pertsch, Jornell Quiambao, Kanishka Rao, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Kevin Sayed, Jaspiar Singh, Sumedh Sontakke, Austin Stone, Clayton Tan, Huong Tran, Vincent Vanhoucke, Steve Vega, Quan Vuong, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, Brianna Zitkovich",2022-12-13,RT-1: Robotics Transformer for Real-World Control at Scale,https://arxiv.org/abs/2212.06817,678.0,SOTA improvement,"""Across each category, we find that RT-1 outperforms the prior
models significantly. On seen tasks, RT-1 is able to perform 97% of the more than 200 instructions successfully, which is 25% more than BC-Z and 32% more than Gato. On unseen tasks, RT-1
shows it is capable of generalizing to novel instructions, performing 76% of the never-before-seen
instructions, 24% more than the next best baseline""

Top10 recent paper from Sebastian Sartor 2025-05-14",35000000.0,"""we also limit the size of the model compared to
the original publication, which was 1.2B parameters (resulting in on robot inference time of 1.9s),
to be of similar size to RT-1 (37M parameters for Gato vs. 35M for RT-1""

16M params for image tokenizer, 19M for the transformer",,,RT-1,"""We utilize a dataset that we gathered over the course of 17 months with a fleet of 13 robots, containing
âˆ¼130k episodes and over 700 tasks""

Episode is an example of robot following instructions",,,,,,Reinforcement learning,Confident,"By transferring knowledge from large, diverse, task-agnostic datasets, modern machine learning models can solve specific downstream tasks either zero-shot or with small task-specific datasets to a high level of performance. While this capability has been demonstrated in other fields such as computer vision, natural language processing or speech recognition, it remains to be shown in robotics, where the generalization capabilities of the models are particularly critical due to the difficulty of collecting real-world robotic data. We argue that one of the keys to the success of such general robotic models lies with open-ended task-agnostic training, combined with high-capacity architectures that can absorb all of the diverse, robotic data. In this paper, we present a model class, dubbed Robotics Transformer, that exhibits promising scalable model properties. We verify our conclusions in a study of different model classes and their ability to generalize as a function of the data size, model size, and data diversity based on a large-scale data collection on real robots performing real-world tasks. The project's website and videos can be found at this http URL",,,Open weights (unrestricted),United States of America,,,,,,2025-06-11 21:40,,,,,,Industry,,,,,Open source,"weights and code here, apache 2.0:
https://github.com/google-research/robotics_transformer

data here, also apache: 
https://github.com/google-deepmind/open_x_embodiment",Industry,,,,,,,,,
TranceptEve,Biology,"Proteins,Protein pathogenicity prediction","University of Oxford,Harvard Medical School","Pascal Notin, Lood Van Niekerk, Aaron W Kollasch, Daniel Ritter, Yarin Gal, Debora S. Marks",2022-12-10,TranceptEVE: Combining Family-specific and Family-agnostic Models of Protein Sequences for Improved Fitness Prediction,https://www.biorxiv.org/content/10.1101/2022.12.07.519495v1,,SOTA improvement,"""Besides its broader application scope, it achieves state-of- the-art performance for mutation effects prediction, both in terms of correlation with experimental assays and with clinical annotations from ClinVar.""",,,,,ProteinGym,,,,,,,,Unknown,"Modeling the fitness landscape of protein sequences has historically relied on training models on family-specific sets of homologous sequences called Multiple Sequence Alignments. Many proteins are however difficult to align or have shallow alignments which limits the potential scope of alignment-based methods. Not subject to these limitations, large protein language models trained on non-aligned sequences across protein families have achieved increasingly high predictive performance â€“ but have not yet fully bridged the gap with their alignment-based counterparts. In this work, we introduce TranceptEVE â€“ a hybrid method between family-specific and family-agnostic models that seeks to build on the relative strengths from each approach. Our method gracefully adapts to the depth of the alignment, fully relying on its autoregressive transformer when dealing with shallow alignments and leaning more heavily on the family-specifc models for proteins with deeper alignments. Besides its broader application scope, it achieves state-of-the-art performance for mutation effects prediction, both in terms of correlation with experimental assays and with clinical annotations from ClinVar.",,,Unreleased,"United Kingdom of Great Britain and Northern Ireland,United States of America",Tranception,,,,,2025-06-02 15:19,,,,,,"Academia,Academia",,,,,Unreleased,https://www.pascalnotin.com/publication/trancepteve/,"Academia,Academia",,,,,,,,,
Whisper v2,Speech,Speech recognition,OpenAI,"Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, Ilya Sutskever",2022-12-05,Robust Speech Recognition via Large-Scale Weak Supervision,"https://huggingface.co/openai/whisper-large-v2

https://arxiv.org/abs/2212.04356",2240.0,,,1550000000.0,1550M,1.1e+23,"""Compared to the Whisper large model, the large-v2 model is trained for 2.5x more epochs with added regularization for improved performance.""

We (roughly) estimated Whisper v1 as 4.65e22. 2.5x that is 1.16e23 or ~1.1e23",Unspecified unreleased,"""The models are trained on 680,000 hours of audio and the corresponding transcripts collected from the internet. 65% of this data (or 438,000 hours) represents English-language audio and matched English transcripts, roughly 18% (or 126,000 hours) represents non-English audio and English transcripts, while the final 17% (or 117,000 hours) represents non-English audio and the corresponding transcript. This non-English data represents 98 different languages.""",9302400000.0,"""When scaled to 680,000 hours of multilingual and multitask
supervision, the resulting models generalize well
to standard benchmarks and are often competitive
with prior fully supervised results but in a zeroshot transfer setting without the need for any finetuning.""


13,680 words/h (estimate) * 680,000h = 9,302,400,000 words",,,,Self-supervised learning,Likely,"Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labelled data, Whisper models demonstrate a strong ability to generalise to many datasets and domains without the need for fine-tuning.

Whisper was proposed in the paper Robust Speech Recognition via Large-Scale Weak Supervision by Alec Radford et al. from OpenAI. The original code repository can be found here.

Compared to the Whisper large model, the large-v2 model is trained for 2.5x more epochs with added regularization for improved performance.",7.5,,Open weights (unrestricted),United States of America,,,,,,2025-05-16 10:30,,,,1024.0,Table 18,Industry,,,,,Unreleased,"Apache 2.0 for weights

code for v1 is MIT: https://github.com/openai/whisper",Industry,,,,,Comparison with other models,,,,
Vega v2,Language,"Language modeling,Question answering,Word sense disambiguation","Wuhan University,JD Explore Academy,Shanghai AI Lab,Nanyang Technological University,Washington University in St Louis,Chongqing University of Posts and Telecommunications,University of Sydney","Qihuang Zhong, Liang Ding, Yibing Zhan, Yu Qiao, Yonggang Wen, Li Shen, Juhua Liu, Baosheng Yu, Bo Du, Yixin Chen, Xinbo Gao, Chunyan Miao, Xiaoou Tang, Dacheng Tao",2022-12-04,Toward Efficient Language Model Pretraining and Downstream Adaptation via Self-Evolution: A Case Study on SuperGLUE,https://arxiv.org/abs/2212.01853,39.0,SOTA improvement,Highest score at SuperGLUE leaderboard version 2.0 in terms of RTE (Recognizing Textual Entailment-Accuracy),6000000000.0,,7.76e+22,"Pretraining took 1 month on 320 A100 (Section 3.1)
720*60*60*320*312000000000000*0.3=7.7635584e+22",,,,,720.0,,NVIDIA A100,,Confident,"This technical report briefly describes our JDExplore d-team's Vega v2 submission on the SuperGLUE leaderboard. SuperGLUE is more challenging than the widely used general language understanding evaluation (GLUE) benchmark, containing eight difficult language understanding tasks, including question answering, natural language inference, word sense disambiguation, coreference resolution, and reasoning. [Method] Instead of arbitrarily increasing the size of a pretrained language model (PLM), our aim is to 1) fully extract knowledge from the input pretraining data given a certain parameter budget, e.g., 6B, and 2) effectively transfer this knowledge to downstream tasks. To achieve goal 1), we propose self-evolution learning for PLMs to wisely predict the informative tokens that should be masked, and supervise the masked language modeling (MLM) process with rectified smooth labels. For goal 2), we leverage the prompt transfer technique to improve the low-resource tasks by transferring the knowledge from the foundation model and related downstream tasks to the target task. [Results] According to our submission record (Oct. 2022), with our optimized pretraining and fine-tuning strategies, our 6B Vega method achieved new state-of-the-art performance on 4/8 tasks, sitting atop the SuperGLUE leaderboard on Oct. 8, 2022, with an average score of 91.3.",,,Unreleased,"China,China,Singapore,Australia",,,,320.0,,2025-06-06 14:16,,,,,,"Academia,Academia,Academia,Academia",,,,,Unreleased,,"Academia,Academia,Academia,Academia",,,,255862.7534697245,,,,,
Transformer + GFM,Language,Language modeling,Nanjing University,"Hao Yu, Jianxin Wu",2022-12-01,"""Compressing Transformers: Features Are Low-Rank, but Weights Are Not""",https://cs.nju.edu.cn/wujx/paper/AAAI2023_AFM.pdf,,,,185200000.0,"185.2M (Table 4)

""We implemented our methods based on fairseq (Ott et al. 2019). The original transformer model follows the architectural choice described in Baevski and Auli (2018), which includes 16 decoder blocks and sinusoidal position embeddings in the input layer. Each MHSA module has 8 heads and adaptive input representations have three bands of size 20K, 40K and 200K. The embedding layer and FFNâ€™s hidden-state have dimensions of 1024 and
4096, respectively. We sampled 4K sentences to form the proxy dataset D. Then, we reduced the parameters by 15%, 20% and 25%.""",7.736941599999998e+18,"7.30e18 FLOP [base transformer] + 4.3694162e+17 FLOP [GFM] = 7.7369416e+18 FLOP

_______________
Estimations from the Algorithmic progress paper (upd - a100 gpu were assumed while the paper reports 3090 gpus): 

SOURCE: Compression of Baevski et al. transformer, impute

During the GFM process, we removed layer dropout and
trained on 8 GPUs. We limited the number of tokens per
GPU to a maximum threshold 1536, which means each GPU
processes 1536 tokens using the same model parameters. We
accumulated gradient updates over 8 batches before committing a parameter update following Ott et al. (2018). We set
the batch size as 32 and trained 1000 updates. 

Estimate based on Baevski: 
They had 7.30E+18 FLOP, this has an additional training process using GFM
1000 steps, with approx 1 sec/step = 1000s

https://epochai.org/blog/estimating-training-compute
0.277777 hours, 8 GPUs, NVIDIA A100 (guess), FP16, 30% = 7.4e17 FLOP

7.30e18 + 7.4e17 = 8039999999999999000.00 FLOP ",WikiText-103,"""We also evaluate our approach in the WikiText-103 (Merity et al. 2017) dataset. WikiText-103 is composed of shuffled Wikipedia articles where the context carries across sentences.""",,,,"
",NVIDIA GeForce RTX 3090,,Confident,"Transformer and its variants achieve excellent results in various computer vision and natural language processing tasks, but high computational costs and reliance on large training datasets restrict their deployment in resource-constrained settings. Low-rank approximation of model weights has been
effective in compressing CNN models, but its application to transformers has been less explored and is less effective. Existing methods require the complete dataset to finetune compressed models, which are both time-consuming and data-hungry. This paper reveals that the features (i.e., activations) are low-rank, but model weights are surprisingly not
low-rank. Hence, AAFM is proposed, which adaptively determines the compressed model structure and locally compresses each linear layerâ€™s output features rather than the model weights. A second stage, GFM, optimizes the entire compressed network holistically. Both AAFM and GFM only
use few training samples without labels, that is, they are fewshot, nsupervised, fast and effective. For example, with only 2K images without labels, 33% of the parameters are removed in DeiT-B with 18.8% relative throughput increase, but only a 0.23% accuracy loss for ImageNet recognition. The proposed
methods are successfully applied to the language modeling task in NLP, too. Besides, the few-shot compressed models generalize well in downstream tasks.",,Transformer + GFM,Unreleased,China,FAIRSEQ Adaptive Inputs,436941620000000000,"6 FLOP/token/parameter * 185000000 parameters * 103000000 tokens = 1.1433e+17 FLOP

or 

0.6 hours - reported training time for another model in the paper, I assumed, this training was similar in time length

0.6 hours * 3600 sec / hour * 8 GPUs * 35580000000000 FLOP/s [assumed precision fp16] * 0.3 [assumed utilization] = 1.8444672e+17 FLOP

or 

""During the GFM process, we removed layer dropout and trained on 8 GPUs. We limited the number of tokens per GPU to a maximum threshold 1536, which means each GPU processes 1536 tokens using the same model parameters. We accumulated gradient updates over 8 batches before committing a parameter update following Ott et al. (2018). We set
the batch size as 32 and trained 1000 updates.""

8 GPUs * 1536 max tokens / GPU * 32 tokens / batch * 1000 updates * 185200000 parameters * 6 FLOP / token / parameter = 4.3694162e+17 FLOP

the last estimation is likely to be the most correct (with minimum assumptions) ",8.0,,2025-03-28 12:14,,,,,,Academia,,,,1.0,Unreleased,,Academia,,,,5597.371669502683,"Operation counting,Hardware",,,,
ZymCTRL,Biology,Protein generation,"Basecamp Research,Friedrich-Alexander-UniversitÃ¤t,University of Girona","Geraldene Munsamy, Sebastian Lindner, Philipp Lorenz, Noelia Ferruz",2022-12-01,ZymCTRL: a conditional language model for the controllable generation of artificial enzymes,https://www.mlsb.io/papers_2022/ZymCTRL_a_conditional_language_model_for_the_controllable_generation_of_artificial_enzymes.pdf,11.0,,,738000000.0,"""ZymCTRL contains 36 layers totalling 738M parameters""",5.05e+21,"""We trained for 179,000 steps on 48 NVIDIA A100s 80GB for about 15,000 GPU hours""

15000  * 3600 * 312 teraFLOPS * 0.3 (utilization assumption) = 5.05e21",BRENDA,"""ZymCTRL was trained on the BRENDA database, a dataset of 37M enzyme sequences classified according to their enzymatic class""",13033458285.0,"36,276,604 sequences after filtering, and training uses 90% of these. From figure 6, average sequence is 399.2 amino acids long.
36,276,604 * 0.9 * 399.2 = 13.0B amino acids",,,NVIDIA A100,,Confident,"The design of custom-tailored proteins has the potential to provide novel and
groundbreaking solutions in many fields, including molecular medicine or environmental sciences. Among protein classes, enzymes are particularly attractive because their complex active sites can accelerate chemical transformations by several orders of magnitude. Since enzymes are biodegradable nanoscopic materials, they hold an unmatched promise as sustainable, large-scale industrial catalysts. Motivated by the enormous success of language models in designing novel yet nature-like proteins, we hypothesised that an enzyme-specific language model could provide new opportunities to design purpose-built artificial enzymes. Here, we describe ZymCTRL, a conditional language model trained on the BRENDA database of enzymes, which generates enzymes of a specific enzymatic class upon a user prompt. ZymCTRL generates artificial enzymes distant from natural ones while their intended functionality matches predictions from orthogonal methods. We release the model to the community.",8.0,,Open weights (unrestricted),"United Kingdom of Great Britain and Northern Ireland,Germany,Spain",,,,48.0,,2025-06-11 18:24,,,,,,"Industry,Academia,Academia",,,,,Unreleased,"Apache 2.0
https://huggingface.co/AI4PD/ZymCTRL","Industry,Academia,Academia",,,,38381.97716230412,Hardware,AI4PD,,,
DeepNash,Games,Stratego,DeepMind,"Julien Perolat, Bart de Vylder, Daniel Hennes, Eugene Tarassov, Florian Strub, Vincent de Boer, Paul Muller, Jerome T. Connor, Neil Burch, Thomas Anthony, Stephen McAleer, Romuald Elie, Sarah H. Cen, Zhe Wang, Audrunas Gruslys, Aleksandra Malysheva, Mina Khan, Sherjil Ozair, Finbarr Timbers, Toby Pohlen, Tom Eccles, Mark Rowland, Marc Lanctot, Jean-Baptiste Lespiau, Bilal Piot, Shayegan Omidshafiei, Edward Lockhart, Laurent Sifre, Nathalie Beauguerlange, Remi Munos, David Silver, Satinder Singh, Demis Hassabis, Karl Tuyls",2022-12-01,Mastering the game of Stratego with model-free multiagent reinforcement learning,https://www.science.org/stoken/author-tokens/ST-887/full,147.0,SOTA improvement,"DeepNash beat existing state-of-the-art AI methods in Stratego and achieved a year-to-date (2022) and all-time top-three ranking on the Gravon games platform, competing with human expert players.",,,,"""The final agent was trained using 768 MXUâ€™s (matrix multiplication unit) for Learners and
256 MXUâ€™s for Actors (using 256 TPUâ€™s in total).""
Some more details in Table S1 (in supplementary materials)",,,,"768 * 7.21M trajectories? (Table S1)

768 * 7.21M = 5,537,280,000

https://www.science.org/doi/suppl/10.1126/science.add4679/suppl_file/science.add4679_sm.pdf",,,,Reinforcement learning,Unknown,"We introduce DeepNash, an autonomous agent that plays the imperfect information game Stratego at a human expert level. Stratego is one of the few iconic board games that artificial intelligence (AI) has not yet mastered. It is a game characterized by a twin challenge: It requires long-term strategic thinking as in chess, but it also requires dealing with imperfect information as in poker. The technique underpinning DeepNash uses a game-theoretic, model-free deep reinforcement learning method, without search, that learns to master Stratego through self-play from scratch. DeepNash beat existing state-of-the-art AI methods in Stratego and achieved a year-to-date (2022) and all-time top-three ranking on the Gravon games platform, competing with human expert players.",,,Unreleased,United Kingdom of Great Britain and Northern Ireland,,,,,,2025-05-30 10:57,,,,,,Industry,,,,,Unreleased,"the training code used to be here but not anymore

https://github.com/google-deepmind/open_spiel/tree/master/open_spiel/python/algorithms/rnad",Industry,,,,,,,,,
GPT-3.5 Turbo,Language,"Language modeling/generation,Question answering,Chat",OpenAI,"John Schulman, Barret Zoph, Christina Kim, Jacob Hilton, Jacob Menick, Jiayi Weng, Juan Felipe Ceron Uribe, Liam Fedus, Luke Metz, Michael Pokorny, Rapha Gontijo Lopes, Shengjia Zhao, Arun Vijayvergiya, Eric Sigler, Adam Perelman, Chelsea Voss, Mike Heaton, Joel Parish, Dave Cummings, Rajeev Nayak, Valerie Balcom, David Schnurr, Tomer Kaftan, Chris Hallacy, Nicholas Turley, Noah Deutsch, Vik Goel, Jonathan Ward, Aris Konstantinidis, Wojciech Zaremba, Long Ouyang, Leonard Bogdonoff, Joshua Gross, David Medina, Sarah Yoo, Teddy Lee, Ryan Lowe, Dan Mossing, Joost Huizinga, Roger Jiang, Carroll Wainwright, Diogo Almeida, Steph Lin, Marvin Zhang, Kai Xiao, Katarina Slama, Steven Bills, Alex Gray, Jan Leike, Jakub Pachocki, Phil Tillet, Shantanu Jain, Greg Brockman, Nick Ryder, Alex Paino, Qiming Yuan, Clemens Winter, Ben Wang, Mo Bavarian, Igor Babuschkin, Szymon Sidor, Ingmar Kanitscheider, Mikhail Pavlov, Matthias Plappert, Nik Tezak, Heewoo Jun, William Zhuk, Vitchyr Pong, Lukasz Kaiser, Jerry Tworek, Andrew Carr, Lilian Weng, Sandhini Agarwal, Karl Cobbe, Vineet Kosaraju, Alethea Power, Stanislas Polu, Jesse Han, Raul Puri, Shawn Jain, Benjamin Chess, Christian Gibson, Oleg Boiko, Emy Parparita, Amin Tootoonchian, Kyle Kosic, Christopher Hesse",2022-11-30,"A fast, inexpensive model for simple tasks",https://platform.openai.com/docs/models/gpt-3.5-turbo,,"Historical significance,Significant use","https://www.reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01/

was the default free model in ChatGPT, so likely one of the most popular models in existence",20000000000.0,20B parameters according to Table 1 in Microsoft's CODEFUSION paper: https://arxiv.org/pdf/2310.17680.pdf,,,Unspecified unreleased,"Knowledge cutoff date was originally September 2021 - which I assume means September 1, 2021 - according to https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models?tabs=global-standard%2Cstandard-chat-completions#gpt-35.",,,,,,,Speculative,"GPT-3.5 Turbo models can understand and generate natural language or code and have been optimized for chat using the Chat Completions API but work well for non-chat tasks as well. As of July 2024, use gpt-4o-mini in place of GPT-3.5 Turbo, as it is cheaper, more capable, multimodal, and just as fast. GPT-3.5 Turbo is still available for use in the API.",,,API access,United States of America,,,,,,2025-05-12 18:39,Azure AI,,,,,Industry,,,,,Unreleased,available on API: https://platform.openai.com/docs/models/gpt-3-5-turbo ,Industry,,,,,,,,,
ALM 1.0,Language,Language modeling,Beijing Academy of Artificial Intelligence / BAAI,,2022-11-28,ALM 1.0,https://github.com/FlagAI-Open/FlagAI/blob/master/examples/ALM/README.md,,SOTA improvement,SOTA results on Arabic-language benchmark ALUE.,335000000.0,335M parameters: https://github.com/FlagAI-Open/FlagAI/blob/master/examples/ALM/README.md,,,ArabicText 2022,"""ALM-1.0 uses the largest open-source Arabic text dataset ArabicText 2022. You can check ArabicText 2022 for more information.""",,,,,,,Speculative,,,,Hosted access (no API),China,,,,,,2025-05-28 17:52,,,,,,Academia,,,,,Unreleased,"It seems to have only inference and finetuning codes, no weights or pretraining code

https://github.com/FlagAI-Open/FlagAI/tree/master/examples/ALM",Academia,,,,,,,,,
GPT-3.5,Language,Language modeling,OpenAI,,2022-11-28,,https://platform.openai.com/docs/models/gpt-3-5,,"Historical significance,Significant use,SOTA improvement,Training cost",,,"Parameter count may be 175B based on OpenAI's statements that text-davinci-003 is in the GPT-3.5 series of models. It was also stated to be 175B in the Microsoft CODEFUSION paper, but the paper was reportedly retracted because the authors did not know the parameter count.",2.578e+24,https://colab.research.google.com/drive/1QSxa8YCWjEBQU7mrXLhw6TP1VX5oqgdW#scrollTo=Gt6Z6oZ26clI,,"Knowledge cutoff date was originally September 2021 - which I assume means September 1, 2021 - according to https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models?tabs=global-standard%2Cstandard-chat-completions#gpt-35.",,,,,NVIDIA A100 SXM4 40 GB,Reinforcement learning,Speculative,,,,API access,United States of America,,,,,,2025-05-12 18:39,,,,,,Industry,,,9.899999999999999e+24,,Unreleased,,Industry,$4625550.75,,,,"Comparison with other models,Benchmarks",,,,1
DiT-XL/2 + Discriminator Guidance,Image generation,"Image generation,Text-to-image","Korea Advanced Institute of Science and Technology (KAIST),NAVER","Dongjun Kim, Yeongmin Kim, Se Jung Kwon, Wanmo Kang, Il-Chul Moon",2022-11-28,Refining Generative Process with Discriminator Guidance in Score-based Diffusion Models,https://arxiv.org/abs/2211.17091v4,68.0,SOTA improvement,"""Using our algorithm, we achive state-of-the-art results on ImageNet 256x256 with FID 1.83 and recall 0.64, similar to the validation data's FID (1.68) and recall (0.66)""",,,,"This is a finetune of DiT-XL/2, so its compute won't be much higher.",,,,,,,NVIDIA A100,,Unknown,"The proposed method, Discriminator Guidance, aims to improve sample generation of pre-trained diffusion models. The approach introduces a discriminator that gives explicit supervision to a denoising sample path whether it is realistic or not. Unlike GANs, our approach does not require joint training of score and discriminator networks. Instead, we train the discriminator after score training, making discriminator training stable and fast to converge. In sample generation, we add an auxiliary term to the pre-trained score to deceive the discriminator. This term corrects the model score to the data score at the optimal discriminator, which implies that the discriminator helps better score estimation in a complementary way. Using our algorithm, we achive state-of-the-art results on ImageNet 256x256 with FID 1.83 and recall 0.64, similar to the validation data's FID (1.68) and recall (0.66). We release the code at this https URL.",10.0,,Unreleased,"Korea (Republic of),Korea (Republic of)",DiT-XL/2,,,,,2025-05-30 13:24,,,,,,"Academia,Industry",,,,,Open (non-commercial),"Attribution-NonCommercial-ShareAlike 4.0 International
https://github.com/alsdudrla10/DG?tab=readme-ov-file

I cannot see checkpoints / model weights in the repo","Academia,Industry",,,FP16,,,,,,
Discriminator Guidance,Image generation,Image generation,"Korea Advanced Institute of Science and Technology (KAIST),NAVER","Dongjun Kim, Yeongmin Kim, Se Jung Kwon, Wanmo Kang, Il-Chul Moon",2022-11-28,Refining Generative Process with Discriminator Guidance in Score-based Diffusion Models,https://arxiv.org/abs/2211.17091v4,68.0,SOTA improvement,"""Using our algorithm, we achive state-of-the-art results on ImageNet 256x256 with FID 1.83 and recall 0.64, similar to the validation data's FID (1.68) and recall (0.66).""
https://paperswithcode.com/paper/refining-generative-process-with",,,2.1570000001e+20,481 hours * 312 TFLOPS (A100) * 40% utilization,,,,,481.0,Table 6,NVIDIA A100 PCIe,,Confident,"The proposed method, Discriminator Guidance, aims to improve sample generation of pre-trained diffusion models. The approach introduces a discriminator that gives explicit supervision to a denoising sample path whether it is realistic or not. Unlike GANs, our approach does not require joint training of score and discriminator networks. Instead, we train the discriminator after score training, making discriminator training stable and fast to converge. In sample generation, we add an auxiliary term to the pre-trained score to deceive the discriminator. This term corrects the model score to the data score at the optimal discriminator, which implies that the discriminator helps better score estimation in a complementary way. Using our algorithm, we achive state-of-the-art results on ImageNet 256x256 with FID 1.83 and recall 0.64, similar to the validation data's FID (1.68) and recall (0.66).",10.0,,Open weights (non-commercial),"Korea (Republic of),Korea (Republic of)",,,,,,2025-05-28 16:08,,,,,,"Academia,Industry",,,,,Open (non-commercial),"https://github.com/alsdudrla10/DG
Attribution-NonCommercial-ShareAlike 4.0 International

checkpoints and train code here: https://github.com/alsdudrla10/DG/blob/main/README.md ","Academia,Industry",$337.88,,FP16,,Hardware,,,,
CICERO,Games,Diplomacy,Meta AI,"Anton Bakhtin, Noam Brown, Emily Dinan, Gabriele Farina, Colin Flaherty, Daniel Fried, Andrew Goff, Jonathan Gray, Hengyuan Hu, Athul Paul Jacob, Mojtaba Komeili, Karthik Konath, Minae Kwon, Adam Lerer, Mike Lewis, Alexander H. Miller, Sasha Mitts, Adithya Renduchintala, Stephen Roller, Dirk Rowe, Weiyan Shi, Joe Spisak, Alexander Wei, David Wu, Hugh Zhang, Markus Zijlstra",2022-11-22,Human-level play in the game of Diplomacy by combining language models with strategic reasoning,https://www.science.org/doi/10.1126/science.ade9097,274.0,SOTA improvement,"""We introduce Cicero, the first AI agent to achieve human-level performance in Diplomacy""",,"""We took R2C2 (22) as our base model â€“ a 2.7B parameter Transformer-based (23) encoder-decoder model pre-trained on text from the Internet using a BART de-noising objective (24).""",,,WebDiplomacy,,,"""We obtained a dataset of 125,261 games of Diplomacy played online at webDiplomacy.net. Of these, 40,408 games contained dialogue, with a total of 12,901,662 messages exchanged between players. Player accounts were de-identified and automated redaction of personally identifiable information (PII) was performed by webDiplomacy. We refer to this dataset hereafter as WebDiplomacy .""",,,,,Unknown,,,,Open weights (non-commercial),United States of America,,,,,,2025-05-28 16:08,,,,,,Industry,,,,,Open source,"creative commons (non comm) for model weights, MIT for code
https://github.com/facebookresearch/diplomacy_cicero?tab=readme-ov-file#license-for-model-weights",Industry,,,FP16,,,,,,
CLUE,Recommendation,Recommender system,"Naver Clova,Naver AI Lab","Kyuyong Shin, Hanock Kwak, Su Young Kim, Max Nihlen Ramstrom, Jisu Jeong, Jung-Woo Ha, Kyung-Min Kim",2022-11-22,Scaling Law for Recommendation Models: Towards General-purpose User Representations,https://arxiv.org/abs/2111.11294,,,,160000000.0,160m,3.456e+18,"using grpah reader on Figure 2 b for the upper right point: 
0.04 PF-days * 10^15 FLOPS/s* 3600s*24h=3456000000000000000

counting operations: =160000000.00*50743870312*6=4.871411549952 Ã— 10^19

""The computation (PF-days) is calculated as 6 Ã— # of parameters Ã— batch size Ã— # of training steps Ã— sequence length divided by one PF-day = 8.64 Ã— 10^19. We train all models for 100,000 steps.""

",,,50743870312.0,"Pre-training:
We construct a sufficiently large-scale dataset with more
than 50B (50741643225) behavior tokens collected over 2 years from search
engine and e-commerce platform.
Number of behavior tokens 50,741,643,225 (Appendix 1)
Downstream:
Books: We collect 1,298,489 review logs of 100,000 unique
users and 504,572 unique books.
Clothing: We collect 928,598 review logs of 100,000 unique
users and 314,943 unique clothing, shoes, and jewelry.

50741643225+1298489+928598=50743870312",168.0,"We shuffle the dataset at every epoch and train the model for
8 epochs, where the transfer performance begins to plateau.
The total training time takes 7 days on 64 V100 GPUs. 

",NVIDIA V100,,Likely,"Recent advancement of large-scale pretrained models such as BERT, GPT-3, CLIP, and Gopher, has shown astonishing achievements across various task domains. Unlike vision recognition and language models, studies on general-purpose user representation at scale still remain underexplored. Here we explore the possibility of general-purpose user representation learning by training a universal user encoder at large scales. We demonstrate that the scaling law is present in user representation learning areas, where the training error scales as a power-law with the amount of computation. Our Contrastive Learning User Encoder (CLUE), optimizes task-agnostic objectives, and the resulting user embeddings stretch our expectation of what is possible to do in various downstream tasks. CLUE also shows great transferability to other domains and companies, as performances on an online experiment shows significant improvements in Click-Through-Rate (CTR). Furthermore, we also investigate how the model performance is influenced by the scale factors, such as training data size, model capacity, sequence length, and batch size. Finally, we discuss the broader impacts of CLUE in general.",8.0,,,"Korea (Republic of),Korea (Republic of)",,,,64.0,,2025-02-17 14:58,,,,256.0,"CLUE is trained with 160M parameters, sequence length (128), and batch size (256).","Industry,Industry",,,,,,,"Industry,Industry",,,,38389.67061575342,"Reported,Operation counting",,,,
scFormer,Biology,Representation learning,"University of Toronto,Vector Institute,University Health Network,Microsoft Research","Haotian Cui, Chloe Wang, Hassaan Maan, Nan Duan, Bo Wang",2022-11-22,scFormer: A Universal Representation Learning Approach for Single-Cell Data Using Transformers,https://www.biorxiv.org/content/10.1101/2022.11.20.517285v1.abstract,3.0,,,,,,,,,33585600.0,"Each cell has M genes, not specified precisely but set to 1200 for UMAP plots. 

Total cells: 3005+7982+17001=27988

Estimated training tokens: 27988*1200=33585600",,,,,Speculative,,30.0,,,"Canada,Canada,Canada,United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational",,,,,,2025-05-01 10:42,,,,,,"Academia,Academia,Industry",,,,,,,"Academia,Academia,Industry",,,,,,,,,
AR-LDM,Image generation,Text-to-image,"Alibaba,University of Waterloo,Vector Institute","Xichen Pan, Pengda Qin, Yuhong Li, Hui Xue, Wenhu Chen",2022-11-20,Synthesizing Coherent Story with Auto-Regressive Latent Diffusion Models,https://arxiv.org/abs/2211.10950,51.0,SOTA improvement,"The first latent diffusion model for coherent visual story synthesizing.
""Quantitative results show that AR-LDM achieves SoTA FID scores on PororoSV, FlintstonesSV, and the newly introduced challenging dataset VIST containing natural images""",1500000000.0,Table 1,5.1e+20,8 NVIDIA A100 GPUs for 8 days,,,,"PororoSV, FlintstonesSV and VIST. All storytelling datasets, sizes would be possible to look up.",194.0,8 NVIDIA A100 GPUs for 8 days,NVIDIA A100,,Confident,"Conditioned diffusion models have demonstrated state-of-the-art text-to-image synthesis capacity. Recently, most works focus on synthesizing independent images; While for real-world applications, it is common and necessary to generate a series of coherent images for story-stelling. In this work, we mainly focus on story visualization and continuation tasks and propose AR-LDM, a latent diffusion model auto-regressively conditioned on history captions and generated images. Moreover, AR-LDM can generalize to new characters through adaptation. To our best knowledge, this is the first work successfully leveraging diffusion models for coherent visual story synthesizing. Quantitative results show that AR-LDM achieves SoTA FID scores on PororoSV, FlintstonesSV, and the newly introduced challenging dataset VIST containing natural images. Large-scale human evaluations show that AR-LDM has superior performance in terms of quality, relevance, and consistency.",50.0,,Unreleased,"China,Canada,Canada",Stable Diffusion (LDM-KL-8-G),,,,,2025-02-06 20:08,,,,,,"Industry,Academia,Academia",,,,,Open (non-commercial),"no weights, no license. training and inference code here:
https://github.com/xichenpan/ARLDM","Industry,Academia,Academia",$745.84,,,,Hardware,,,,
Fusion in Encoder,Language,Question answering,Samsung,"Akhil Kedia, Mohd Abbas Zaidi, Haejun Lee",2022-11-18,FiE: Building a Global Probability Space by Leveraging Early Fusion in Encoder for Open-Domain Question Answering,https://arxiv.org/abs/2211.10147,8.0,SOTA improvement,"""Using our proposed method, we outperform the current state-of-the-art method by 2.5 Exact Match score on the Natural Question dataset while using only 25% of parameters and 35% of the latency during inference, and 4.4 Exact Match on WebQuestions dataset""",330000000.0,330M,1.3e+20,"""The experiments were run on 8x80GB Nvidia A100s with 800GB RAM and 4x32-core CPUs, and each experiment took around 1 day for NQ and 2 days for TriviaQA with large models. Inference was run on the same system, and took 2 minutes.""

2 days * 24 * 3600 * 8 * 312 teraflop/s * 0.3 utilization = 1.3e20",TriviaQA,,,79k per table 11 (probably number of question-answer pairs),48.0,2 days,NVIDIA A100 SXM4 80 GB,,Likely,"Generative models have recently started to outperform extractive models in Open Domain Question Answering, largely by leveraging their decoder to attend over multiple encoded passages and combining their information. However, generative models tend to be larger than extractive models due to the need for a decoder, run slower during inference due to auto-regressive decoder beam search, and their generated output often suffers from hallucinations. We propose to extend transformer encoders with the ability to fuse information from multiple passages, using global representation to provide cross-sample attention over all tokens across samples. Furthermore, we propose an alternative answer span probability calculation to better aggregate answer scores in the global space of all samples. Using our proposed method, we outperform the current state-of-the-art method by 2.5 Exact Match score on the Natural Question dataset while using only 25% of parameters and 35% of the latency during inference, and 4.4 Exact Match on WebQuestions dataset. When coupled with synthetic data augmentation, we outperform larger models on the TriviaQA dataset as well. The latency and parameter savings of our method make it particularly attractive for open-domain question answering, as these models are often compute-intensive.",,,Unreleased,Korea (Republic of),,,,,,2025-06-02 11:54,,,,,,Industry,,,,,Unreleased,"""The source code is based on the original implementation of FiD (Izacard and Grave, 2021b), which can be found at their Github. The modeling for the fused Electra model was implemented using
HuggingFace (Wolf et al., 2020) by modifying the ElectraModel.",Industry,$233.06,,,,Hardware,,,,
Galactica,"Language,Biology","Language modeling,Question answering",Meta AI,"Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, Robert Stojnic",2022-11-16,Galactica: A Large Language Model for Science,https://arxiv.org/abs/2211.09085,599.0,SOTA improvement,"""We outperform existing models on a range of scientific tasks. On technical knowledge probes such as LaTeX equations, Galactica outperforms the latest GPT-3 by 68.2% versus 49.0%. Galactica also performs well on reasoning, outperforming Chinchilla on mathematical MMLU by 41.3% to 35.7%, and PaLM 540B on MATH""",120000000000.0,"""The largest 120B model we train runs on a single NVIDIA A100 node""",3.24e+23,"Authors state the model is trained on 450b tokens. Using 6 FLOP/token/parameter, this is 6*120b*450b = 3.24e23",Galactica Corpus,"""Our corpus consists of 106 billion tokens from papers, reference material, encyclopedias and other scientific sources. We combine natural language sources, such as papers and textbooks, and natural sequences, such as protein sequences and chemical formulae. We process LATEX where we can capture it, and also include academic code to capture computational science""",106000000000.0,"""Total dataset size = 106 billion tokens""",,,NVIDIA A100 SXM4 80 GB,Self-supervised learning,Likely,"Information overload is a major obstacle to scientific progress. The explosive growth in scientific literature and data has made it ever harder to discover useful insights in a large mass of information. Today scientific knowledge is accessed through search engines, but they are unable to organize scientific knowledge alone. In this paper we introduce Galactica: a large language model that can store, combine and reason about scientific knowledge. We train on a large scientific corpus of papers, reference material, knowledge bases and many other sources. We outperform existing models on a range of scientific tasks. On technical knowledge probes such as LaTeX equations, Galactica outperforms the latest GPT-3 by 68.2% versus 49.0%. Galactica also performs well on reasoning, outperforming Chinchilla on mathematical MMLU by 41.3% to 35.7%, and PaLM 540B on MATH with a score of 20.4% versus 8.8%. It also sets a new state-of-the-art on downstream tasks such as PubMedQA and MedMCQA dev of 77.6% and 52.9%. And despite not being trained on a general corpus, Galactica outperforms BLOOM and OPT-175B on BIG-bench. We believe these results demonstrate the potential for language models as a new interface for science. We open source the model for the benefit of the scientific community.",4.0,,Open weights (non-commercial),United States of America,,,,128.0,,2025-05-16 10:30,,,,2000000.0,"Table 1: batch size 2M, warmup 1.1B (out of 450B tokens)",Industry,checked,,,,Unreleased,"cc-by-nc (non-commercial): https://huggingface.co/facebook/galactica-120b 

repo but no training code: https://github.com/paperswithcode/galai/blob/main/README.md ",Industry,$591076.89,,,102386.13451047534,Operation counting,,,,16
Luminous Sparse,Language,Language modeling/generation,"Aleph Alpha,Graphcore",,2022-11-16,,https://www.graphcore.ai/posts/graphcore-and-aleph-alpha-demonstrate-80-sparsified-ai-model,,,,2600000000.0,,,,,,,,,,,,Confident,"Graphcore and our partner Aleph Alpha are unveiling a significant advance in AI compute efficiency, with the sparsification of a 13bn parameter model down to just 2.6bn parameters.

The advanced technique, which removes around 80% of the modelâ€™s weights while retaining most of its capabilities, utilizes the IPUâ€™s support for point sparse matrix multiplications - a characteristic of its made-for-AI architecture.",,,Hosted access (no API),"Germany,United Kingdom of Great Britain and Northern Ireland",,,,,,2024-09-15 20:20,,,,,,"Industry,Industry",,,,,Unreleased,,"Industry,Industry",,,,,,,,,
EVA-01,Vision,"Image classification,Object detection,Semantic segmentation","Beijing Academy of Artificial Intelligence / BAAI,Huazhong University of Science and Technology,Zhejiang University (ZJU),Beijing Institute of Technology","Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang, Yue Cao",2022-11-14,EVA: Exploring the Limits of Masked Visual Representation Learning at Scale,https://arxiv.org/abs/2211.07636,509.0,SOTA improvement,"from abstract 'Via this pretext task, we can efficiently scale up EVA to one billion parameters, and sets new records on a broad range of representative vision downstream tasks, such as image recognition, video action recognition, object detection, instance segmentation and semantic segmentation without heavy supervised training.'",1011000000.0,1011M from table 3,1.501e+22,"flops = (128) * (3.12e14) * (14.5 * 24 * 3600) * (0.3) = 1.501e22
(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)

from Table 3, time and num gpus, GPU model is on page 4 (A100), precision is fp16 and likely utilizes tensor cores","ImageNet21k,COCO,Conceptual Captions 12M (CC12M),Conceptual Captions (CC3M)","from table 3 : ImageNet-21K, CC12M, CC3M, Object365, COCO, ADE",29600000.0,from table 3: 29.6M images,348.0,from Table 3 14.5 days = 348 hours,NVIDIA A100 SXM4 40 GB,Self-supervised learning,Confident,"We launch EVA, a vision-centric foundation model to explore the limits of visual representation at scale using only publicly accessible data. EVA is a vanilla ViT pre-trained to reconstruct the masked out image-text aligned vision features conditioned on visible image patches. Via this pretext task, we can efficiently scale up EVA to one billion parameters, and sets new records on a broad range of representative vision downstream tasks, such as image recognition, video action recognition, object detection, instance segmentation and semantic segmentation without heavy supervised training. Moreover, we observe quantitative changes in scaling EVA result in qualitative changes in transfer learning performance that are not present in other models. For instance, EVA takes a great leap in the challenging large vocabulary instance segmentation task: our model achieves almost the same state-of-the-art performance on LVISv1.0 dataset with over a thousand categories and COCO dataset with only eighty categories. Beyond a pure vision encoder, EVA can also serve as a vision-centric, multi-modal pivot to connect images and text. We find initializing the vision tower of a giant CLIP from EVA can greatly stabilize the training and outperform the training from scratch counterpart with much fewer samples and less compute, providing a new direction for scaling up and accelerating the costly training of multi-modal foundation models. To facilitate future research, we release all the code and billion-scale model.",150.0,,Open weights (unrestricted),"China,China,China,China",,,,128.0,,2025-05-28 16:08,,,,,,"Academia,Academia,Academia,Academia",,,,44544.0,Open source,MIT: https://github.com/baaivision/EVA pretrain code: https://github.com/baaivision/EVA/tree/master/EVA-01/eva ,"Academia,Academia,Academia,Academia",$29374.47,,FP16,102390.69476171424,Hardware,,,,
AltCLIP_M9,"Multimodal,Language,Vision","Language modeling/generation,Chat,Visual question answering,Image generation",Beijing Academy of Artificial Intelligence / BAAI,"Zhongzhi Chen, Guang Liu, Bo-Wen Zhang, Fulong Ye, Qinghong Yang, Ledell Wu",2022-11-12,AltCLIP: Altering the Language Encoder in CLIP for Extended Language Capabilities,https://arxiv.org/abs/2211.06679,63.0,SOTA improvement,"""We set new state-of-the-art performances on a bunch of tasks including ImageNet-CN, Flicker30kCN, COCO-CN and XTD""",,,,,"Conceptual Captions (CC3M),LAION-400M,TSL2019,OPUS,WuDao Corpora,LAION-2B",,,,,,,,Unknown,"In this work, we present a conceptually simple and effective method to train a strong bilingual/multilingual multimodal representation model. Starting from the pre-trained multimodal representation model CLIP released by OpenAI, we altered its text encoder with a pre-trained multilingual text encoder XLM-R, and aligned both languages and image representations by a two-stage training schema consisting of teacher learning and contrastive learning. We validate our method through evaluations of a wide range of tasks. We set new state-of-the-art performances on a bunch of tasks including ImageNet-CN, Flicker30k-CN, COCO-CN and XTD. Further, we obtain very close performances with CLIP on almost all tasks, suggesting that one can simply alter the text encoder in CLIP for extended capabilities such as multilingual understanding. Our models and code are available at this https URL.",10.0,,Open weights (unrestricted),China,CLIP (ViT L/14@336px),,,,,2025-05-28 16:08,,,,,,Academia,,,,,Open source,https://github.com/FlagAI-Open/FlagAI/blob/master/examples/AltCLIP/altclip_ft_bmtrain.py Apache-2.0 license,Academia,,,FP32,,,,,,
Innovative Drug-like Molecule Generation from,Biology,Drug discovery,"University of Pittsburgh,Carnegie Mellon University (CMU)","Haotian Zhang, Linxiaoyi Wan",2022-11-12,Innovative Drug-like Molecule Generation from Flow-based Generative Model,https://arxiv.org/abs/2211.06566,,,,,,,,,,,"Total data points = 50,000 binding structures 

Previous estimate: 20 atoms/ligand = 1,000,000 data points
[Final estimate: 1.0e6]",,,,,Unknown,"To design a drug given a biological molecule by using deep learning methods, there are many successful models published recently. People commonly used generative models to design new molecules given certain protein. LiGAN was regarded as the baseline of deep learning model which was developed on convolutional neural networks. Recently, GraphBP showed its ability to predict innovative ""real"" chemicals that the binding affinity outperformed with traditional molecular docking methods by using a flow-based generative model with a graph neural network and multilayer perception. However, all those methods regarded proteins as rigid bodies and only include a very small part of proteins related to binding. However, the dynamics of proteins are essential for drug binding. Based on GraphBP, we proposed to generate more solid work derived from protein data bank. The results will be evaluated by validity and binding affinity by using a computational chemistry algorithm.",,,,"United States of America,United States of America",GraphBP,,,,,2025-05-01 10:42,,,,,,"Academia,Academia",,,,,,,"Academia,Academia",,,,,,,,,
InternImage,Vision,"Image classification,Object detection,Image segmentation","Shanghai AI Lab,Tsinghua University,Nanjing University,SenseTime,Chinese University of Hong Kong (CUHK)","Wenhai Wang, Jifeng Dai, Zhe Chen, Zhenhang Huang, Zhiqi Li, Xizhou Zhu, Xiaowei Hu, Tong Lu, Lewei Lu, Hongsheng Li, Xiaogang Wang, Yu Qiao",2022-11-10,InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions,https://arxiv.org/abs/2211.05778,463.0,SOTA improvement,"""InternImage-H achieved a new record 65.4 mAP on COCO test-dev and 62.9 mIoU on ADE20K, outperforming current leading CNNs and ViTs""",1080000000.0,"1.08B, table 1",2.408e+21,"InternImage-H is pre-trained on a 427 million joint dataset of public Laion-400M [61], YFCC-15M [62], and CC12M [63] for 30 epochs, and then fine-tuned the model on ImageNet-1K for 20 epochs. ImageNet-1K has 1,281,167 images.

Table 2 says InternImage-H uses 188 GFLOP per forward pass at 224 resolution, and 1478 GFLOP at 640

Table 7 indicates training InternImage-H was done at a scale of ""224/640"" so presumably there was pretraining at 224x224 resolution and then some fine-tuning at 640x640. It's not clear how much training was done at each resolution, but typically this is a small fraction of total training (e.g. Noisy Student finds it sufficient to train for 350 epochs at smaller resolution, and then fine-tune at the higher resolution for 1.5 epochs). We'll ignore the additional FLOPs from high resolution training.

Total training FLOPs:
188e9 FLOP/image * (427M images * 30 epochs) + (1.281M images * 20 epochs) * 3 (additional FLOPs for backward pass) = 2.408e21","LAION-400M,Conceptual Captions 12M (CC12M),ImageNet-1k","""To further explore the capability of our model and match the large-scale private data used in previous methods [16, 20, 59], we adopt M3I
Pre-training [60], a unified pre-training approach available
for both unlabeled and weakly-labeled data, to pre-train
InternImage-H on a 427 million joint dataset of public
Laion-400M [61], YFCC-15M [62], and CC12M [63] for
30 epochs, and then we fine-tune the model on ImageNet1K for 20 epochs.""",427000000.0,,,,,,Confident,"Compared to the great progress of large-scale vision transformers (ViTs) in recent years, large-scale models based on convolutional neural networks (CNNs) are still in an early state. This work presents a new large-scale CNN-based foundation model, termed InternImage, which can obtain the gain from increasing parameters and training data like ViTs. Different from the recent CNNs that focus on large dense kernels, InternImage takes deformable convolution as the core operator, so that our model not only has the large effective receptive field required for downstream tasks such as detection and segmentation, but also has the adaptive spatial aggregation conditioned by input and task information. As a result, the proposed InternImage reduces the strict inductive bias of traditional CNNs and makes it possible to learn stronger and more robust patterns with large-scale parameters from massive data like ViTs. The effectiveness of our model is proven on challenging benchmarks including ImageNet, COCO, and ADE20K. It is worth mentioning that InternImage-H achieved a new record 65.4 mAP on COCO test-dev and 62.9 mIoU on ADE20K, outperforming current leading CNNs and ViTs. The code will be released at this https URL.",30.0,,Open weights (unrestricted),"China,China,China,Hong Kong,China,Hong Kong,China",,,,,,2025-06-06 11:10,,,,,,"Academia,Academia,Academia,Industry,Academia",,,,,Open source,"https://github.com/OpenGVLab/InternImage
MIT license

https://huggingface.co/OpenGVLab/internimage_h_jointto22k_384","Academia,Academia,Academia,Industry,Academia",,,FP16,,Operation counting,OpenGVLab,,,
Mogrifier RLSTM (WT2),Language,Language modeling,DeepMind,GÃ¡bor Melis,2022-11-03,Circling Back to Recurrent Models of Language,https://arxiv.org/abs/2211.01848,0.0,SOTA improvement,"""On top of these improvements, the RLSTM
outperformed the LSTM by a small margin, and we established a new state of the art on both datasets""",35000000.0,Table 1,1.4e+17,6ND = 6*35000000*2666667*250 = 1.4000002e+17,WikiText-2,,,,,,,,Confident,"Just because some purely recurrent models suffer from being hard to optimize and inefficient on today's hardware, they are not necessarily bad models of language. We demonstrate this by the extent to which these models can still be improved by a combination of a slightly better recurrent cell, architecture, objective, as well as optimization. In the process, we establish a new state of the art for language modelling on small datasets and on Enwik8 with dynamic evaluation.",250.0,Mogrifier RLSTM (WT2),Unreleased,United Kingdom of Great Britain and Northern Ireland,,,,,,2025-02-14 16:53,,,,,,Industry,,,,,Unreleased,,Industry,,,,,Operation counting,,,,
BLOOMZ-176B,Language,"Translation,Language modeling/generation",Hugging Face,"Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir Radev, Alham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, Colin Raffel",2022-11-03,Crosslingual Generalization through Multitask Finetuning,"https://arxiv.org/abs/2211.01786, https://huggingface.co/bigscience/bloomz",242.0,SOTA improvement,"""Finetuning on multilingual tasks with English prompts further improves performance on English and non-English tasks leading to various state-of-the-art zero-shot results.""

Table 1",176000000000.0,176B,,"fine-tuned from BLOOM-176B

1.37e22 fine-tune compute",xP3,"""In addition, we introduce xP3, a composite of supervised datasets in 46 languages with English and machine-translated prompts. 

https://huggingface.co/datasets/bigscience/xP3",20000000000.0,"per https://huggingface.co/datasets/bigscience/xP3, 94,941,936 KB or 94GB 

if approx 200M words per GB, that's ~20B words (rougher estimate because it's multilingual)

https://docs.google.com/document/d/1G3vvQkn4x_W71MKg0GmHVtzfd9m0y3_Ofcoew0v902Q/edit#heading=h.ieihc08p8dn0",,,,,Likely,"Multitask prompted finetuning (MTF) has been shown to help large language models generalize to new tasks in a zero-shot setting, but so far explorations of MTF have focused on English data and models. We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0. We find finetuning large multilingual language models on English tasks with English prompts allows for task generalization to non-English languages that appear only in the pretraining corpus. Finetuning on multilingual tasks with English prompts further improves performance on English and non-English tasks leading to various state-of-the-art zero-shot results. We also investigate finetuning on multilingual tasks with prompts that have been machine-translated from English to match the language of each dataset. We find training on these machine-translated prompts leads to better performance on human-written prompts in the respective languages. Surprisingly, we find models are capable of zero-shot generalization to tasks in languages they have never intentionally seen. We conjecture that the models are learning higher-level capabilities that are both task- and language-agnostic. In addition, we introduce xP3, a composite of supervised datasets in 46 languages with English and machine-translated prompts. Our code, datasets and models are freely available at this https URL.",,,Open weights (unrestricted),"Multinational,United States of America",BLOOM-176B,1,"""We use publicly available pretrained BLOOM models ranging from 560 million to 176 billion parameters. BLOOM models are large decoder-only language models pretrained for around 350 billion tokens with an architecture similar to GPT-3
(Brown et al., 2020). We finetune the models for an additional 13 billion tokens with loss only being
computed on target tokens.""

13B * 176B * 6",,,2024-09-11 12:49,,,,,,Industry,,,,,Open source,"Apache 2.0
train/eval code: https://github.com/bigscience-workshop/xmtf?tab=readme-ov-file#train-models 
weights: https://huggingface.co/bigscience/bloomz ",Industry,,,,,,,,,
mT0-13B,Language,"Translation,Language modeling/generation","Hugging Face,BigScience","Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir Radev, Alham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, Colin Raffel",2022-11-03,Crosslingual Generalization through Multitask Finetuning,https://arxiv.org/abs/2211.01786,242.0,SOTA improvement,"""Finetuning on multilingual tasks with English prompts further improves performance on English and non-English tasks leading to various state-of-the-art zero-shot results.""

Table 1",13000000000.0,13B,,"fine-tuned from mT5

1.37e22 fine-tune compute",xP3,"""In addition, we introduce xP3, a composite of supervised datasets in 46 languages with English and machine-translated prompts. 

https://huggingface.co/datasets/bigscience/xP3",20000000000.0,"per https://huggingface.co/datasets/bigscience/xP3, 94,941,936 KB or 94GB 

if approx 200M words per GB, that's ~20B words (rougher estimate because it's multilingual)

https://docs.google.com/document/d/1G3vvQkn4x_W71MKg0GmHVtzfd9m0y3_Ofcoew0v902Q/edit#heading=h.ieihc08p8dn0",,,,,Confident,"Multitask prompted finetuning (MTF) has been shown to help large language models generalize to new tasks in a zero-shot setting, but so far explorations of MTF have focused on English data and models. We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0. We find finetuning large multilingual language models on English tasks with English prompts allows for task generalization to non-English languages that appear only in the pretraining corpus. Finetuning on multilingual tasks with English prompts further improves performance on English and non-English tasks leading to various state-of-the-art zero-shot results. We also investigate finetuning on multilingual tasks with prompts that have been machine-translated from English to match the language of each dataset. We find training on these machine-translated prompts leads to better performance on human-written prompts in the respective languages. Surprisingly, we find models are capable of zero-shot generalization to tasks in languages they have never intentionally seen. We conjecture that the models are learning higher-level capabilities that are both task- and language-agnostic. In addition, we introduce xP3, a composite of supervised datasets in 46 languages with English and machine-translated prompts. Our code, datasets and models are freely available at this https URL.",,,Open weights (unrestricted),"Multinational,United States of America,Multinational,France",mT5-XXL,1,"""We finetune the models for an additional 13 billion tokens with loss only being computed on target tokens...
For finetuning mT5, we follow the same procedure as described above for BLOOM, except that inputs are fed into the encoder and thus are not space-separated from targets.""

13B * 13B * 6 = 1.01e21",,,2024-09-05 14:08,,,mT0-13B,,,"Industry,Research collective",,,,,Unreleased,apache 2.0,"Industry,Research collective",,,,,,,,,
eDiff-I,Image generation,"Image generation,Text-to-image",NVIDIA,"Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Qinsheng Zhang, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, Bryan Catanzaro, Tero Karras, Ming-Yu Liu",2022-11-02,eDiff-I: Text-to-Image Diffusion Models with an Ensemble of Expert Denoisers,https://arxiv.org/abs/2211.01324,669.0,SOTA improvement,"SOTA zero-shot FID on COCO 2014, Table 1

May be significantly used, via Nvidia Picasso: https://www.nvidia.com/en-us/gpu-cloud/picasso/",9100000000.0,"9.1B for config D, Table 1",5.46e+19,"6ND = 6*9100000000*1000000000=5.46e+19 (likely, might change because of several epochs / dataset division)

""The base model was trained using 256 NVIDIA A100 GPUs, while the two super-resolution models were trained with 128 NVIDIA A100 GPUs each"" 
no info on duration",Unspecified unreleased,"""We use a collection of public and proprietary datasets to train our model. To ensure high-quality training data, we apply heavy filtering using a pretrained CLIP model to measure the image-text alignment score as well as an aesthetic scorer to rank the image quality""",1000000000.0,"""The final dataset to train our model contains about one billion text-image pairs""",,,NVIDIA A100,,Likely,"Large-scale diffusion-based generative models have led to breakthroughs in text-conditioned high-resolution image synthesis. Starting from random noise, such text-to-image diffusion models gradually synthesize images in an iterative fashion while conditioning on text prompts. We find that their synthesis behavior qualitatively changes throughout this process: Early in sampling, generation strongly relies on the text prompt to generate text-aligned content, while later, the text conditioning is almost entirely ignored. This suggests that sharing model parameters throughout the entire generation process may not be ideal. Therefore, in contrast to existing works, we propose to train an ensemble of text-to-image diffusion models specialized for different synthesis stages. To maintain training efficiency, we initially train a single model, which is then split into specialized models that are trained for the specific stages of the iterative generation process. Our ensemble of diffusion models, called eDiff-I, results in improved text alignment while maintaining the same inference computation cost and preserving high visual quality, outperforming previous large-scale text-to-image diffusion models on the standard benchmark. In addition, we train our model to exploit a variety of embeddings for conditioning, including the T5 text, CLIP text, and CLIP image embeddings. We show that these different embeddings lead to different behaviors. Notably, the CLIP image embedding allows an intuitive way of transferring the style of a reference image to the target text-to-image output. Lastly, we show a technique that enables eDiff-I's ""paint-with-words"" capability. A user can select the word in the input text and paint it in a canvas to control the output, which is very handy for crafting the desired image in mind. The project page is available at this https URL",,,Unreleased,United States of America,,,,,,2025-06-06 12:49,,,,,,Industry,,,,,Unreleased,,Industry,,,,,Operation counting,,,,
GearNet,Biology,"Proteins,Protein function prediction,Protein fold classification","Mila - Quebec AI (originally Montreal Institute for Learning Algorithms),University of Montreal / UniversitÃ© de MontrÃ©al,University of Cambridge,IBM Research,HEC Montreal,CIFAR AI Research","Zuobai Zhang, Minghao Xu, Arian Jamasb, Vijil Chenthamarakshan, AurÃ©lie Lozano, Payel Das, Jian Tang",2022-11-01,Protein Representation Learning by Geometric Structure Pretraining,https://arxiv.org/abs/2203.06125,177.0,,,,,,, AlphaFold database (AFDB),,240000001.0,"805,000 proteins Ã— 300 residues/protein = 241,500,000 datapoints (2.415 Ã— 10^8)
Breakdown:
1. Initial protein count: 365,000 + 440,000 = 805,000
2. Final calculation: 805,000 Ã— 300 = 241,500,000",,,NVIDIA A100,,Confident,"Learning effective protein representations is critical in a variety of tasks in biology such as predicting protein function or structure. Existing approaches usually pretrain protein language models on a large number of unlabeled amino acid sequences and then finetune the models with some labeled data in downstream tasks. Despite the effectiveness of sequence-based approaches, the power of pretraining on known protein structures, which are available in smaller numbers only, has not been explored for protein property prediction, though protein structures are known to be determinants of protein function. In this paper, we propose to pretrain protein representations according to their 3D structures. We first present a simple yet effective encoder to learn the geometric features of a protein. We pretrain the protein graph encoder by leveraging multiview contrastive learning and different self-prediction tasks. Experimental results on both function prediction and fold classification tasks show that our proposed pretraining methods outperform or are on par with the state-of-the-art sequence-based methods, while using much less pretraining data. Our implementation is available at this https URL.",50.0,,,"Canada,Canada,United Kingdom of Great Britain and Northern Ireland,United States of America,Multinational,Ireland,United Kingdom of Great Britain and Northern Ireland,Brazil,Switzerland,Canada,Canada",,,,4.0,,2025-05-01 10:42,,,,,,"Academia,Academia,Academia,Industry,Academia,Research collective",,,,,,,"Academia,Academia,Academia,Industry,Academia,Research collective",,,,3200.635667063614,,,,,
Taiyi-Stable Diffusion,Image generation,"Image generation,Text-to-image",IDEA CCNL,"Jiaxing Zhang, Ruyi Gan, Junjie Wang, Yuxiang Zhang, Lin Zhang, Ping Yang, Xinyu Gao, Ziwei Wu, Xiaoqun Dong, Junqing He, Jianheng Zhuo, Qi Yang, Yongfeng Huang, Xiayu Li, Yanghan Wu, Junyu Lu, Xinyu Zhu, Weifeng Chen, Ting Han, Kunhao Pan, Rui Wang, Hao Wang, Xiaojun Wu, Zhongshen Zeng, Chongpei Chen",2022-10-31,Fengshenbang 1.0: Being the Foundation of Chinese Cognitive Intelligence,https://arxiv.org/abs/2209.02970,87.0,,"The first open-source, Chinese version of Stable Diffusion. Possibly historically significant.",1000000000.0,,5.1e+22,"Fine-tuning: 32 NVIDIA A100 GPUs for 100 hours
32 * 312e12 * 30% * 100 * 60 * 60 = 1.078272e+21 FLOP

Base model: Stable Diffusion, 5e+22 FLOP",Wukong,"We use Noah-Wukong(100M) å’Œ Zero(23M) as our dataset, and take the image and text pairs with CLIP Score (based on IDEA-CCNL/Taiyi-CLIP-RoBERTa-102M-ViT-L-Chinese) greater than 0.2 as our Training set.",20000000.0,"""trained on 20M filtered Chinese image-text pairs""",100.0,32 NVIDIA A100 GPUs for 100 hours,NVIDIA A100,,Confident,"Nowadays, foundation models become one of fundamental infrastructures in artificial intelligence, paving ways to the general intelligence. However, the reality presents two urgent challenges: existing foundation models are dominated by the English-language community; users are often given limited resources and thus cannot always use foundation models. To support the development of the Chinese-language community, we introduce an open-source project, called Fengshenbang, which leads by the research center for Cognitive Computing and Natural Language (CCNL). Our project has comprehensive capabilities, including large pre-trained models, user-friendly APIs, benchmarks, datasets, and others. We wrap all these in three sub-projects: the Fengshenbang Model, the Fengshen Framework, and the Fengshen Benchmark. An open-source roadmap, Fengshenbang, aims to re-evaluate the open-source community of Chinese pre-trained large-scale models, prompting the development of the entire Chinese large-scale model community. We also want to build a user-centered open-source ecosystem to allow individuals to access the desired models to match their computing resources. Furthermore, we invite companies, colleges, and research institutions to collaborate with us to build the large-scale open-source model-based ecosystem. We hope that this project will be the foundation of Chinese cognitive intelligence.",,,Open weights (restricted use),China,Stable Diffusion (LDM-KL-8-G),,,32.0,,2025-02-14 13:51,,,,,,Academia,,,,,,https://huggingface.co/spaces/CompVis/stable-diffusion-license,Academia,$113638.30,,,25605.65555200996,Hardware,,,,
Transformer-XL + PowerSGD + L-Greco,Language,Language modeling,"Institute of Science and Technology Austria (ISTA),Neural Magic","Mohammadreza Alimohammadi, Ilia Markov, Elias Frantar, Dan Alistarh",2022-10-31,L-GreCo: An Efficient and General Framework for Layerwise-Adaptive Gradient Compression,https://arxiv.org/abs/2210.17357,4.0,,,,,1.3150368e+18,"35580000000000 FLOP / sec [fp16 precision assumed] * 15400 sec [see training time notes] * 8 GPUs * 0.3 [assumed utilization] = 1.3150368e+18 FLOP 

_______

They used similar estimation (4.14 Ã— 10^17 FLOP) in the Algorithmic Progress paper - not sure how exactly it was calculated ",WikiText-103,,,,4.3,40k steps [table 7] * 0.385 sec/step [figure 3] = 15400 sec = 4.28 hours,NVIDIA GeForce RTX 3090,,Confident,"Data-parallel distributed training of deep neural networks (DNN) has gained very widespread adoption, but can still experience communication bottlenecks. To address this issue, entire families of compression mechanisms have been developed, including quantization, sparsification, and low-rank approximation, some of which are seeing significant practical adoption. Despite this progress, almost all known compression schemes apply compression uniformly across DNN layers, although layers are heterogeneous in terms of parameter count and their impact on model accuracy. In this work, we provide a general framework for adapting the degree of compression across the model's layers dynamically during training, improving the overall compression, while leading to substantial speedups, without sacrificing accuracy. Our framework, called L-GreCo, is based on an adaptive algorithm, which automatically picks the optimal compression parameters for model layers guaranteeing the best compression ratio while satisfying an error constraint. Extensive experiments over image classification and language modeling tasks shows that L-GreCo is effective across all existing families of compression methods, and achieves up to 2.5Ã— training speedup and up to 5Ã— compression improvement over efficient implementations of existing approaches, while recovering full accuracy. Moreover, L-GreCo is complementary to existing adaptive algorithms, improving their compression ratio by 50% and practical throughput by 66%.",,Transformer-XL + PowerSGD + L-Greco,Unreleased,"Austria,United States of America",,,,8.0,,2025-04-07 13:07,,,TransformerXL + PowerSGD + L-Greco,,,"Academia,Industry",,,,,Open source,"Apache 2. looks like just code: https://github.com/LGrCo/L-GreCo/tree/master/Transformer-XL 

","Academia,Industry",,,,5601.237152002178,Hardware,,,,
EnCodec,Audio,Audio generation,Meta AI,"Alexandre DÃ©fossez, Jade Copet, Gabriel Synnaeve, Yossi Adi",2022-10-24,High Fidelity Neural Audio Compression,"https://arxiv.org/abs/2210.13438, ",440.0,SOTA improvement,""" Finally, our best model, EnCodec, reaches state-of-the-art scores for speech and for
music at 1.5, 3, 6, 12 kbps at 24 kHz, and at 6, 12, and 24 kbps for 48 kHz with stereo channels.""",,,,"""We train all models for 300 epochs, with one epoch being 2,000 updates with the Adam optimizer with a batch size of 64 examples of 1 second each, a learning rate of 3 Â· 10âˆ’4 , Î²1 = 0.5, and Î²2 = 0.9. All the models are traind using 8 A100 GPUs""","DNS,Common Voice,AudioSet,FSD50K,Jamendo","""We train EnCodec on 24 kHz monophonic across diverse domains, namely: speech, noisy speech, music and
general audio while we train the fullband stereo EnCodec on only 48 kHz music. For speech, we use the clean speech segments from DNS Challenge 4 (Dubey et al., 2022) and the Common Voice dataset (Ardila et al., 2019).
For general audio, we use on AudioSet (Gemmeke et al., 2017) together with FSD50K (Fonseca et al., 2021).
For music, we rely on the Jamendo dataset (Bogdanov et al., 2019) for training and evaluation and we further
evaluate our models on music using a proprietary music dataset.""",,"~17k hours total, per Table A.1",,,NVIDIA A100,,Unknown,"We introduce a state-of-the-art real-time, high-fidelity, audio codec leveraging neural networks. It consists in a streaming encoder-decoder architecture with quantized latent space trained in an end-to-end fashion. We simplify and speed-up the training by using a single multiscale spectrogram adversary that efficiently reduces artifacts and produce high-quality samples. We introduce a novel loss balancer mechanism to stabilize training: the weight of a loss now defines the fraction of the overall gradient it should represent, thus decoupling the choice of this hyper-parameter from the typical scale of the loss. Finally, we study how lightweight Transformer models can be used to further compress the obtained representation by up to 40%, while staying faster than real time. We provide a detailed description of the key design choices of the proposed model including: training objective, architectural changes and a study of various perceptual loss functions. We present an extensive subjective evaluation (MUSHRA tests) together with an ablation study for a range of bandwidths and audio domains, including speech, noisy-reverberant speech, and music. Our approach is superior to the baselines methods across all evaluated settings, considering both 24 kHz monophonic and 48 kHz stereophonic audio. Code and models are available at this http URL.",300.0,,Open weights (non-commercial),United States of America,,,,,,2024-11-01 10:04,,,,,,Industry,,,,,Open source,"MIT for repo in general, non commercial weights. Dataset is in repo.
https://github.com/facebookresearch/audiocraft/blob/main/docs/ENCODEC.md",Industry,,,,,,,,,
Tk-Instruct,Language,Instruction interpretation,"University of Washington,Arizona State University,Allen Institute for AI","Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Gary Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Maitreya Patel, Kuntal Kumar Pal, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Shailaja Keyur Sampat, Savan Doshi, Siddhartha Mishra, Sujan Reddy, Sumanta Patro, Tanay Dixit, Xudong Shen, Chitta Baral, Yejin Choi, Noah A. Smith, Hannaneh Hajishirzi, Daniel Khashabi",2022-10-24,Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks,https://arxiv.org/abs/2204.07705,,,,11000000000.0,11B,,,Super-natural Instructions,,5120091466.0,"56.6 words*1616 tasks + (1,048,576 tokens per batch/1,024 examples per batch)*5M instances = 5120091465.6

In total, the dataset includes 1616 tasks
and 5M instances. On average, each instruction is
paired with 2.8 positive and 2.4 negative examples.
The average definition length is 56.6 in words.",20.0,"1000 steps * 1024 examples = 10^6 examples per 4 hours
5M instances -> 20 hours of total training time
""These experiments are run on Google V3-256 TPUs using a batch size of 1,048,576 tokens (1,024 examples), a constant learning rate of 1e-5 and a total of 1000 steps. Each training run takes 4 hours to complete.""",Google TPU v3,,Speculative,"How well can NLP models generalize to a variety of unseen tasks when provided with task instructions? To address this question, we first introduce Super-NaturalInstructions, a benchmark of 1,616 diverse NLP tasks and their expert-written instructions. Our collection covers 76 distinct task types, including but not limited to classification, extraction, infilling, sequence tagging, text rewriting, and text composition. This large and diverse collection of tasks enables rigorous benchmarking of cross-task generalization under instructions -- training models to follow instructions on a subset of tasks and evaluating them on the remaining unseen ones. Furthermore, we build Tk-Instruct, a transformer model trained to follow a variety of in-context instructions (plain language task definitions or k-shot examples). Our experiments show that Tk-Instruct outperforms existing instruction-following models such as InstructGPT by over 9% on our benchmark despite being an order of magnitude smaller. We further analyze generalization as a function of various scaling parameters, such as the number of observed tasks, the number of instances per task, and model sizes. We hope our dataset and model facilitate future progress towards more general-purpose NLP models.",,,Open weights (unrestricted),"United States of America,United States of America,United States of America",T5-11B,337926036729600000000,=5120091466*11B*6=3.379 Ã— 10^20,,,2024-12-02 10:24,,,,1048576.0,"batch size of 1,048,576 tokens (1,024 examples)","Academia,Academia,Research collective",,,,,Open source,"https://instructions.apps.allenai.org/

https://github.com/yizhongw/Tk-Instruct
https://huggingface.co/models?search=tk-instruct-","Academia,Academia,Research collective",,,,,Operation counting,,,,
DiffSBDD (CrossDocked),Biology,Drug discovery,"Ecole Polytechnique FÂ´edÂ´erale de Lausanne (EPFL),University of Cambridge,Cornell University,Chinese Academy of Mathematics and System Science,University of Rome,Microsoft Research,University of Oxford,AITHYRA Institute","Arne Schneuing, Charles Harris, Yuanqi Du, Kieran Didi, Arian Jamasb, Ilia Igashov, Weitao Du, Carla Gomes, Tom Blundell, Pietro Lio, Max Welling, Michael Bronstein, Bruno Correia",2022-10-24,Structure-based Drug Design with Equivariant Diffusion Models,https://arxiv.org/abs/2210.13695,141.0,,,,,2.69568e+20,""" For
CrossDocked, 100 training epochs take approximately 6 h/8 h in the CÎ± case and 48 h/60 h per 100
epochs on a single NVIDIA A100 GPU with all atom pocket representation""

312000000000000*0.4*60 hours*10=2.695680e+20",,,100000.0,"""We use the CrossDocked dataset [26] with 100,000 high-quality protein-ligand pairs for training and
100 proteins for testing""",600.0,,NVIDIA A100,,Confident,"Structure-based drug design (SBDD) aims to design small-molecule ligands that bind with high affinity and specificity to pre-determined protein targets. Generative SBDD methods leverage structural data of drugs in complex with their protein targets to propose new drug candidates. These approaches typically place one atom at a time in an autoregressive fashion using the binding pocket as well as previously added ligand atoms as context in each step. Recently a surge of diffusion generative models has entered this domain which hold promise to capture the statistical properties of natural ligands more faithfully. However, most existing methods focus exclusively on bottom-up de novo design of compounds or tackle other drug development challenges with task-specific models. The latter requires curation of suitable datasets, careful engineering of the models and retraining from scratch for each task. Here we show how a single pre-trained diffusion model can be applied to a broader range of problems, such as off-the-shelf property optimization, explicit negative design, and partial molecular design with inpainting. We formulate SBDD as a 3D-conditional generation problem and present DiffSBDD, an SE(3)-equivariant diffusion model that generates novel ligands conditioned on protein pockets. Our in silico experiments demonstrate that DiffSBDD captures the statistics of the ground truth data effectively. Furthermore, we show how additional constraints can be used to improve the generated drug candidates according to a variety of computational metrics. These results support the assumption that diffusion models represent the complex distribution of structural data more accurately than previous methods, and are able to incorporate additional design objectives and constraints changing nothing but the sampling strategy.",1000.0,,Open weights (unrestricted),"Switzerland,United Kingdom of Great Britain and Northern Ireland,United States of America,China,Italy,United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,United Kingdom of Great Britain and Northern Ireland,Austria",,,,1.0,,2025-06-16 11:56,,,,,,"Academia,Academia,Academia,Academia,Academia,Industry,Academia,Research collective",,,,,Open source,"Code Availability. Our source codes are publicly available at https://github.com/arneschneuing/ (MIT license)
DiffSBDD. Model weights can be downloaded from Zenodo: https://zenodo.org/records/8183747 (Creative Commons Attribution 4.0 International)","Academia,Academia,Academia,Academia,Academia,Industry,Academia,Research collective",,,,439.7260888751819,Hardware,,,,
U-PaLM (540B),Language,Language generation,Google,"Yi Tay, Jason Wei, Hyung Won Chung, Vinh Q. Tran, David R. So, Siamak Shakeri, Xavier Garcia, Huaixiu Steven Zheng, Jinfeng Rao, Aakanksha Chowdhery, Denny Zhou, Donald Metzler, Slav Petrov, Neil Houlsby, Quoc V. Le, Mostafa Dehghani",2022-10-20,Transcending Scaling Laws with 0.1% Extra Compute,https://arxiv.org/abs/2210.11399,61.0,SOTA improvement,"""We show that U-PaLM 540B outperforms PaLM 540B on 21 out of 26 tasks. Given that PaLM is
the SOTA language model on these tasks, this makes U-PaLM the new state-of-the-art on these tasks.""

performance improvement equivalent to 2x training efficiency: ""Impressively, at 540B scale, we show an approximately 2x computational savings rate where U-PaLM achieves the same performance as the final PaLM 540B model at around half its computational budget """,540000000000.0,,2.53e+24,"""The total number of extra tokens we train on for the 540B
model is approximately 1.3 Billion which constitutes 0.16% extra computation... Training an U-PaLM 540B model only consumes 512 TPUv4 chips and finishes in about 5 days which is considered to be lightweight.""

original PaLM was 2.527e+24. adding 0.16% is ~2.53e24",,"""To keep things consistent, we train this model with the same data mixture as PaLM and do not rely on additional sources of data (labeled or unlabeled).""",,,120.0,5 days,Google TPU v4,,Confident,"Scaling language models improves performance but comes with significant computational costs. This paper proposes UL2R, a method that substantially improves existing language models and their scaling curves with a relatively tiny amount of extra compute. The key idea is to continue training a state-of-the-art large language model (e.g., PaLM) on a few more steps with UL2's mixture-of-denoiser objective. We show that, with almost negligible extra computational costs and no new sources of data, we are able to substantially improve the scaling properties of large language models on downstream metrics. In this paper, we continue training PaLM with UL2R, introducing a new set of models at 8B, 62B, and 540B scale which we call U-PaLM. Impressively, at 540B scale, we show an approximately 2x computational savings rate where U-PaLM achieves the same performance as the final PaLM 540B model at around half its computational budget (i.e., saving âˆ¼4.4 million TPUv4 hours). We further show that this improved scaling curve leads to 'emergent abilities' on challenging BIG-Bench tasks -- for instance, U-PaLM does much better than PaLM on some tasks or demonstrates better quality at much smaller scale (62B as opposed to 540B). Overall, we show that U-PaLM outperforms PaLM on many few-shot setups, i.e., English NLP tasks (e.g., commonsense reasoning, question answering), reasoning tasks with chain-of-thought (e.g., GSM8K), multilingual tasks (MGSM, TydiQA), MMLU and challenging BIG-Bench tasks. Finally, we provide qualitative examples showing the new capabilities of U-PaLM for single and multi-span infilling.",,,Unreleased,United States of America,PaLM (540B),4,"""The total number of extra tokens we train on for the 540B
model is approximately 1.3 Billion which constitutes 0.16% extra computation... Training an U-PaLM 540B model only consumes 512 TPUv4 chips and finishes in about 5 days which is considered to be lightweight.""

PaLM was 2.5e24
0.16% of that is 4e21",512.0,,2025-05-16 10:30,,,,,,Industry,,,,61440.0,Unreleased,,Industry,,,,174161.11557264757,Comparison with other models,,,,
LMSI-Palm,Language,"Language generation,Language modeling/generation,Question answering","Google,University of Illinois Urbana-Champaign (UIUC)","Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, Jiawei Han",2022-10-20,Large Language Models Can Self-Improve,https://arxiv.org/abs/2210.11610,417.0,SOTA improvement,"""We show that our approach improves the general reasoning ability of a 540B-parameter LLM (74.4%->82.1% on GSM8K, 78.2%->83.0% on DROP, 90.0%->94.4% on OpenBookQA, and 63.4%->67.9% on ANLI-A3) and achieves state-of-the-art-level performance, without any ground truth label.""",540000000000.0,540B,,"(fine-tuned from Palm-540B, which was 2.52e24)",GSM8K,Trained on chain-of-thought PaLM output from several datasets of questions that require reasoning. See section 4,,,,,,,Confident,"Large Language Models (LLMs) have achieved excellent performances in various tasks. However, fine-tuning an LLM requires extensive supervision. Human, on the other hand, may improve their reasoning abilities by self-thinking without external inputs. In this work, we demonstrate that an LLM is also capable of self-improving with only unlabeled datasets. We use a pre-trained LLM to generate ""high-confidence"" rationale-augmented answers for unlabeled questions using Chain-of-Thought prompting and self-consistency, and fine-tune the LLM using those self-generated solutions as target outputs. We show that our approach improves the general reasoning ability of a 540B-parameter LLM (74.4%->82.1% on GSM8K, 78.2%->83.0% on DROP, 90.0%->94.4% on OpenBookQA, and 63.4%->67.9% on ANLI-A3) and achieves state-of-the-art-level performance, without any ground truth label. We conduct ablation studies and show that fine-tuning on reasoning is critical for self-improvement.",,,Unreleased,"United States of America,United States of America",PaLM (540B),,"""To reduce the training burden, we sample 5k examples from the non-football and football partition of the DROP dataset, and sample 5k examples from ANLI-A2 and ANLI-A3. For each dataset, we fine-tune the model for 10k steps with a learning rate of 5eâˆ’5
and a batch size of 32."" Not sure about sequence length",,,2025-06-04 17:33,,,,,,"Industry,Academia",,,,,Unreleased,,"Industry,Academia",,,,,,,,,
Flan-PaLM 540B,Language,Language modeling/generation,Google,"Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, Jason Wei",2022-10-20,Scaling Instruction-Finetuned Language Models,https://arxiv.org/abs/2210.11416,2506.0,"Highly cited,SOTA improvement",">1k cites

""Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU.""",540000000000.0,540B,2.4999999999999997e+24,"0.2% greater than Palm 540B, which used 2.5e24",Flan,"Various instruction examples for many tasks:

""Our final set of finetuning tasks is sourced from a combination of tasks from FLAN, T0, Natural Instructions, along with some dialog, program synthesis, and chain-of-thought reasoning tasks, as described in Figure 2. We provide specific pointers and citations in Table 24. All data sources are publicly
available. We also remove all MMLU tasks from Natural Instructions to preserve its role as a broad benchmark of 57 held-out tasks for evaluation. In total, there are 1,836 tasks."" ",,,37.0,"""we only use 0.2% of the pre-training compute to instruction-finetune Flan-PaLM 540B (approximately 512 v4 TPU chips for 37 hours)""",Google TPU v4,,Confident,"Finetuning language models on a collection of datasets phrased as instructions has been shown to improve model performance and generalization to unseen tasks. In this paper we explore instruction finetuning with a particular focus on (1) scaling the number of tasks, (2) scaling the model size, and (3) finetuning on chain-of-thought data. We find that instruction finetuning with the above aspects dramatically improves performance on a variety of model classes (PaLM, T5, U-PaLM), prompting setups (zero-shot, few-shot, CoT), and evaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation). For instance, Flan-PaLM 540B instruction-finetuned on 1.8K tasks outperforms PALM 540B by a large margin (+9.4% on average). Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. We also publicly release Flan-T5 checkpoints, which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.",,,Unreleased,United States of America,PaLM (540B),5,"5.6e21 per Table 2

""we only use 0.2% of the pre-training compute to instruction-finetune Flan-PaLM 540B (approximately 512 v4 TPU chips for 37 hours)""

512 * 37 * 3600 * 275 teraflops * 0.3 = 5.6e21 (so 30% utilization was correct)",512.0,0.3,2025-05-16 10:30,,,,,,Industry,,,,18944.0,Unreleased,,Industry,,"Estimated training compute: 2.5e24
FLOPs at 100% utilization, based on GPU-hours: 37 * 512 * 3600 * 3.12e14 = 2.128e22
3.76e24 / 1.987e25 = 0.1892",,174161.11557264757,"Reported,Hardware",,,,
Flan-T5 11B,Language,Language modeling/generation,Google,"Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, Jason Wei",2022-10-20,Scaling Instruction-Finetuned Language Models,"https://arxiv.org/abs/2210.11416, https://huggingface.co/google/flan-t5-xxl",2506.0,Highly cited,,11000000000.0,11B,3.3e+22,"Table 2: 0.2% greater than T5 xxl, which used 3.3e22 FLOP",,"Various instruction examples for many tasks:

""Our final set of finetuning tasks is sourced from a combination of tasks from FLAN, T0, Natural Instructions, along with some dialog, program synthesis, and chain-of-thought reasoning tasks, as described in Figure 2. We provide specific pointers and citations in Table 24. All data sources are publicly
available. We also remove all MMLU tasks from Natural Instructions to preserve its role as a broad benchmark of 57 held-out tasks for evaluation. In total, there are 1,836 tasks."" ",100000000000.0,"""For T5 models without instruction finetuning, we use LM-adapted models, which were produced by training T5 on 100B additional tokens from C4 on a standard language modeling objective""",,,Google TPU v4,,Confident,"Finetuning language models on a collection of datasets phrased as instructions has been shown to improve model performance and generalization to unseen tasks. In this paper we explore instruction finetuning with a particular focus on (1) scaling the number of tasks, (2) scaling the model size, and (3) finetuning on chain-of-thought data. We find that instruction finetuning with the above aspects dramatically improves performance on a variety of model classes (PaLM, T5, U-PaLM), prompting setups (zero-shot, few-shot, CoT), and evaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation). For instance, Flan-PaLM 540B instruction-finetuned on 1.8K tasks outperforms PALM 540B by a large margin (+9.4% on average). Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. We also publicly release Flan-T5 checkpoints, which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.",,,Open weights (unrestricted),United States of America,T5-11B,76000000000000000000,"7.6e19, per Table 2",,,2024-12-22 14:24,,,,,,Industry,,,,,Unreleased,"apache 2.0 license

https://github.com/google-research/t5x/blob/main/docs/models.md#flan-t5-checkpoints",Industry,$98374.29,,,,Reported,,,,
GPT-2 + Progressive LRD,Language,Language modeling/generation,"Huawei,Huawei Noah's Ark Lab","Habib Hajimolahoseini, Walid Ahmed, Mehdi Rezagholizadeh, Vahid Partovinia, Yang Liu",2022-10-12,Strategies for Applying Low Rank Decomposition to Transformer-Based Models,https://neurips2022-enlsp.github.io/papers/paper_33.pdf,,,,31000000.0,"Table 2
They compressed GPT-2 (124M parameters) to 31M parameters",7.936e+20,"base model training compute estimation: 7.936 Ã— 10^20 FLOP

__________
In the Algorithmic progress paper the estimation was 6.2 Ã— 10^19 FLOP",,They compressed pre-trained GPT-2. Possibly without finetuning on additional data?,,,,,,,Speculative,"Low rank decomposition decomposes each fully-connected layer of the transformer modules into two smaller layers using Singular Value Decomposition. The state-of-the-art techniques usually apply LRD in a single-shot, where all of the layers are decomposed simultaneously. In this paper, we propose and compare different strategies for applying low rank decomposition to compress pre-trained transformer based models. These strategies include: layer-by-layer and progressive decomposition. We observe that progressive low rank decomposition, in which the rank is decreased incrementally results in a higher accuracy after decomposition comparing to single-shot and layer-by-layer low rank decomposition. Furthermore, in contrast with many of state-of-the-art compression methods where intensive pre-training of the compressed model is necessary, we show that progressive LRD can provide promising performance by compressing the model in the fine-tuning stage.",,Progressive LRD,Unreleased,"China,China",GPT-2 (124M),,,,,2025-04-21 12:52,,,Progressive LRD,,,"Industry,Industry",,,,,Unreleased,,"Industry,Industry",,,,,Comparison with other models,,,,
GenSLM,Biology,Protein or nucleotide language model (pLM/nLM),"University of Chicago,NVIDIA,Harvard University,Cerebras Systems,Technical University of Munich,California Institute of Technology","Maxim Zvyagin, Alexander Brace, Kyle Hippe, Yuntian Deng, Bin Zhang, Cindy Orozco Bohorquez, Austin Clyde, Bharat Kale, Danilo Perez-Rivera, Heng Ma, Carla M. Mann, Michael Irvin, J. Gregory Pauloski, Logan Ward, Valerie Hayot, Murali Emani, Sam Foreman, Zhen Xie, Diangen Lin, Maulik Shukla, Weili Nie, Josh Romero, Christian Dallago, Arash Vahdat, Chaowei Xiao, Thomas Gibbs, Ian Foster, James J. Davis, Michael E. Papka, Thomas Brettin, Rick Stevens, Anima Anandkumar, Venkatram Vishwanath, Arvind Ramanathan",2022-10-11,GenSLMs: Genome-scale language models reveal SARS-CoV-2 evolutionary dynamics,https://www.biorxiv.org/content/biorxiv/early/2022/10/11/2022.10.10.511571.full.pdf,58.0,SOTA improvement,"""Together, these capabilities go beyond state-of-the-art techniques
for global-scale whole genome surveillance of pandemic-causing
viruses and address a critical infrastructure need for the global
public health organization"" - SOTA improvement on very specific task",25000000000.0,See Table 3,1.42e+21,"See Table 3
Overall ZettaFlops 1.42","SARS-CoV-2 genome dataset,BV-BRC","SARS-CoV-2 genome datasets from multiple sources:

""we used >1.5 million high-quality BV-BRC SARSCoV-2 complete genome sequences""

""We also utilized a dataset collected by the Houston Methodist Hospital System - one of the largest single-institution collections of SARS-CoV-2 genome sequences in the United States. [...]  Sequences with >256 ambiguous characters were discarded, leaving 16,545 total sequences""

Prokaryotic gene sequence dataset from BV-BRC:
""To allow for better generalization and avoid overfitting of the models to the SARS-CoV-2 data, we used >110 million unique prokaryotic gene sequences from BV-BRC""",56000000001.0,"110,000,000 sequences * 512 tokens/sequence = 56,320,000,000 tokens (5.6e10)",,,,,Confident,"Our work seeks to transform how new and emergent variants of pandemic causing viruses, specially SARS-CoV-2, are identified and classified. By adapting large language models (LLMs) for genomic data, we build genome-scale language models (GenSLMs) which can learn the evolutionary landscape of SARS-CoV-2 genomes. By pretraining on over 10 million prokaryotic gene sequences, and then finetuning a SARS-CoV-2 specific model on 1.5 million genomes, we show that GenSLM can accurately and rapidly identify variants of concern. Thus, to our knowledge, GenSLM represents one of the first whole genome scale foundation models which can generalize to other prediction tasks. We demonstrate the scaling of GenSLMs on both GPU-based supercomputers and AI-hardware accelerators, achieving over 1.54 zettaflops in training runs. We present initial scientific insights gleaned from examining GenSLMs in tracking the evolutionary dynamics of SARS-CoV-2, noting that its full potential on large biological data is yet to be realized.",,,Open weights (unrestricted),"United States of America,United States of America,United States of America,Multinational,Germany,United States of America",,,,,,2025-06-02 14:26,,,,,,"Academia,Industry,Academia,Industry,Academia,Academia",checked,,,,Open source,"MIT license
https://github.com/ramanathanlab/genslm","Academia,Industry,Academia,Industry,Academia,Academia",,,FP16,,Reported,,,,
Instruct-GPT + Mind's Eye,Language,Quantitative reasoning,"Google,Dartmouth College","Ruibo Liu, Jason Wei, Shixiang Shane Gu, Te-Yen Wu, Soroush Vosoughi, Claire Cui, Denny Zhou, Andrew M. Dai",2022-10-11,Mind's Eye: Grounded Language Model Reasoning through Simulation,https://arxiv.org/abs/2210.05359,65.0,,Best at their proposed Utopia benchmark (maybe a bit one-off?),176500000000.0,"Two models: a LM that converts the input to code for a physics simulator, and a foundation model (InstructGPT)

""the resulting models have 0.3B and 1.5B parameters (used as default)""

InstructGPT is 175B. 175B+1.5B = 176.5B",,"""Training of the JAX-based text-to-code LMs runs on TPU-v3 Pods. The learning rates we use for training 0.3B and 1.5B LMs on C4 are {3.0e-4, 1.8e-4}, which are switched to {1.8e-4, 0.5e-4} when fine-tuning on the text-code pairs. We use cosine annealing to control learning rate over time with fixed warm-up steps (3k).""","Utopia,C4","""We propose a new multi-task physics alignment dataset, UTOPIA, whose aim is to benchmark how well current LMs can understand and reason over some basic laws of physics (Â§2). The dataset contains 39 sub-tasks covering six common scenes that involve understanding basic principles of physics (e.g., conservation of momentum in elastic collisions), and all the ground-truth answers are automatically generated by a physics engine""

Also C4

""Besides fine-tuning on the dataset with text-code pairs, we also pre-train the model on the C4 dataset """,,,,,Google TPU v3,,Confident,"Successful and effective communication between humans and AI relies on a shared experience of the world. By training solely on written text, current language models (LMs) miss the grounded experience of humans in the real-world -- their failure to relate language to the physical world causes knowledge to be misrepresented and obvious mistakes in their reasoning. We present Mind's Eye, a paradigm to ground language model reasoning in the physical world. Given a physical reasoning question, we use a computational physics engine (DeepMind's MuJoCo) to simulate the possible outcomes, and then use the simulation results as part of the input, which enables language models to perform reasoning. Experiments on 39 tasks in a physics alignment benchmark demonstrate that Mind's Eye can improve reasoning ability by a large margin (27.9% zero-shot, and 46.0% few-shot absolute accuracy improvement on average). Smaller language models armed with Mind's Eye can obtain similar performance to models that are 100x larger. Finally, we confirm the robustness of Mind's Eye through ablation studies.",,,Unreleased,"United States of America,United States of America",InstructGPT 175B,,,,,2025-06-16 15:33,,,,,,"Industry,Academia",,,,,Unreleased,,"Industry,Academia",,,,,,,,,
Diplodocus,Games,Diplomacy,"Meta AI,Massachusetts Institute of Technology (MIT)","Anton Bakhtin, David J Wu, Adam Lerer, Jonathan Gray, Athul Paul Jacob, Gabriele Farina, Alexander H Miller, Noam Brown",2022-10-11,Mastering the Game of No-Press Diplomacy via Human-Regularized Reinforcement Learning and Planning,https://arxiv.org/abs/2210.05492,33.0,SOTA improvement,"SOTA Improvement in no-press Diplomacy
""In a 200-game no-press Diplomacy tournament involving 62 human participants spanning skill levels from beginner to expert, two Diplodocus agents both achieved a higher average score than all other participants who played more than two games, and ranked first and third according to an Elo ratings model. """,,may be estimated from https://github.com/facebookresearch/diplomacy_cicero?tab=readme-ov-file,,,,"""we train the architecture described in Appendix F on a dataset of roughly 46000 online Diplomacy games provided by webdiplomacy.net.""
then self-play training",,"""we train the architecture described in Appendix F on a dataset of roughly 46000 online Diplomacy games provided by webdiplomacy.net.""
then self-play training",,,,,Unknown,"No-press Diplomacy is a complex strategy game involving both cooperation and competition that has served as a benchmark for multi-agent AI research. While self-play reinforcement learning has resulted in numerous successes in purely adversarial games like chess, Go, and poker, self-play alone is insufficient for achieving optimal performance in domains involving cooperation with humans. We address this shortcoming by first introducing a planning algorithm we call DiL-piKL that regularizes a reward-maximizing policy toward a human imitation-learned policy. We prove that this is a no-regret learning algorithm under a modified utility function. We then show that DiL-piKL can be extended into a self-play reinforcement learning algorithm we call RL-DiL-piKL that provides a model of human play while simultaneously training an agent that responds well to this human model. We used RL-DiL-piKL to train an agent we name Diplodocus. In a 200-game no-press Diplomacy tournament involving 62 human participants spanning skill levels from beginner to expert, two Diplodocus agents both achieved a higher average score than all other participants who played more than two games, and ranked first and third according to an Elo ratings model.",,,Open weights (non-commercial),"United States of America,United States of America",,,,,,2024-11-01 10:04,,,,,,"Industry,Academia",,,,,Open source,"creative commons (non comm) for model weights, MIT for code
https://github.com/facebookresearch/diplomacy_cicero?tab=readme-ov-file#license-for-model-weights","Industry,Academia",,,,,,,,,
DiffDock,Biology,Proteins,Massachusetts Institute of Technology (MIT),"Gabriele Corso, Hannes StÃ¤rk, Bowen Jing, Regina Barzilay, Tommi Jaakkola",2022-10-04,"DiffDock: Diffusion Steps, Twists, and Turns for Molecular Docking","https://arxiv.org/abs/2210.01776, https://docs.nvidia.com/bionemo-framework/latest/models/diffdock.html",313.0,SOTA improvement,"""DiffDock obtains a 38% top-1 success rate (RMSD<2A) on PDBBind, significantly outperforming the previous state-of-the-art of traditional docking (23%) and deep learning (20%) methods""",20240000.0,"""For determining the hyperparameters of DIFFDOCKâ€™s score model, we trained
smaller models (3.97 million parameters) that fit into 48GB of GPU RAM before scaling it up to the final model (20.24 million parameters) that was trained on four 48GB GPUs""

There's a separate 4.77M ""confidence model"" that helps make predictions along with the score model",7.2e+19,"""We trained our final score model on four 48GB RTX A6000 GPUs for 850 epochs (around 18 days).""

4 * 38.7 teraflops * 18 days * 24 * 3600 * 0.3 = 7.2e19

https://www.techpowerup.com/gpu-specs/rtx-a6000.c3686",PDB (Protein Data Bank),"""We evaluate our method on the complexes from PDBBind [Liu et al., 2017], a large collection of protein-ligand structures collected from PDB [Berman et al., 2003], which was used with time-based splits to benchmark many previous works""
",17000.0,"""We employ the time-split of PDBBind proposed by Stark et al. [2022] with 17k complexes from 2018 or earlier for training/validation and 363 test structures from 2019 with no ligand overlap with the training complexes""",432.0,18 days,NVIDIA RTX A6000,,Likely,"Predicting the binding structure of a small molecule ligand to a protein -- a task known as molecular docking -- is critical to drug design. Recent deep learning methods that treat docking as a regression problem have decreased runtime compared to traditional search-based methods but have yet to offer substantial improvements in accuracy. We instead frame molecular docking as a generative modeling problem and develop DiffDock, a diffusion generative model over the non-Euclidean manifold of ligand poses. To do so, we map this manifold to the product space of the degrees of freedom (translational, rotational, and torsional) involved in docking and develop an efficient diffusion process on this space. Empirically, DiffDock obtains a 38% top-1 success rate (RMSD<2A) on PDBBind, significantly outperforming the previous state-of-the-art of traditional docking (23%) and deep learning (20%) methods. Moreover, while previous methods are not able to dock on computationally folded structures (maximum accuracy 10.4%), DiffDock maintains significantly higher precision (21.7%). Finally, DiffDock has fast inference times and provides confidence estimates with high selective accuracy.",850.0,,Open weights (unrestricted),United States of America,,,,,,2025-06-06 11:01,,,,,,Academia,,,,,Open source,"MIT license
https://github.com/gcorso/DiffDock",Academia,,,,,Hardware,,,,
NMST+GPT-2,Language,Language modeling,New York University (NYU),"Eugene Choi, Cheolhyoung Lee, Kyunghyun Cho",2022-10-03,A Non-monotonic Self-terminating Language Model,https://arxiv.org/abs/2210.00660,0.0,,,124000000.0,124M (the same as GPT-2),1.20380928e+20,"assuming that GPT-2 (124M) compute is similar to GPT-2 (117M) compute, which is 120000000000000000000 FLOP = 1.2 Ã— 10^20 FLOP

120000000000000000000 FLOP + 380928000000000000 FLOP = 1.204Ã—10^20 FLOP ",WikiText-103,,,"500, 000 steps
""For computational efficiency, we bucket the dataset into sequences of similar
lengths, and each batch contains a maximum of 1,024 total tokens.""

500000*1024 = 512 000 000 updates

512 000 000 / 103 000 000 = 4.97087378641 epochs",,,,,Confident,"Recent large-scale neural autoregressive sequence models have shown impressive performances on a variety of natural language generation tasks. However, their generated sequences often exhibit degenerate properties such as non-termination, undesirable repetition, and premature termination, when generated with decoding algorithms such as greedy search, beam search, top-k sampling, and nucleus sampling. In this paper, we focus on the problem of non-terminating sequences resulting from an incomplete decoding algorithm. We first define an incomplete probable decoding algorithm which includes greedy search, top-k sampling, and nucleus sampling, beyond the incomplete decoding algorithm originally put forward by Welleck et al. (2020). We then propose a non-monotonic self-terminating language model, which significantly relaxes the constraint of monotonically increasing termination probability in the originally proposed self-terminating language model by Welleck et al. (2020), to address the issue of non-terminating sequences when using incomplete probable decoding algorithms. We prove that our proposed model prevents non-terminating sequences when using not only incomplete probable decoding algorithms but also beam search. We empirically validate our model on sequence completion tasks with various architectures.",4.97,NMST+GPT-2,Unreleased,United States of America,GPT-2 (124M),380928000000000000,6 FLOP / token / parameter * 512 000 000 tokens * 124 000 000 parameters = 3.80928 Ã— 10^17 FLOP,,,2025-03-31 11:15,,,NMST+GPT-2,,,Academia,,,,,Open source,BSD 3-Clause License. Looks like just training code  https://github.com/nyu-dl/non-monotonic-self-terminating-lm,Academia,,,,,,,,,
GemNet-OC ,Materials science,"Molecular simulation,Molecular property prediction,Atomistic simulations,Molecular representation learning,Materials design","Technical University of Munich,Carnegie Mellon University (CMU),Facebook AI Research","Johannes Gasteiger, Muhammed Shuaibi, Anuroop Sriram, Stephan GÃ¼nnemann, Zachary Ulissi, C. Lawrence Zitnick, Abhishek Das",2022-09-30,GemNet-OC: Developing Graph Neural Networks for Large and Diverse Molecular Simulation Datasets,https://arxiv.org/abs/2204.02782,,,,,,5.4000000000000007e+20,125000000000000 FLOP / GPU / sec [bf16 assumed] * 4000 GPU-hours [inferring from Figure 2] * 3600 sec / hour * 0.3 [asssumed utilization] = 5.4e+20 FLOP,Open Catalyst 2020 (OC20),,9817363519.0,"""trained for a max number of epochs (4) or until the learning rate has been exhaustively stepped, whichever comes first""

from the Table 1:

âˆ¼134 M samples (133 934 018)
avg size 73.3 (7-225)

133 934 018 * 73.3 = 9817363519.4 atoms (~tokens)",,"""the analyses and models presented in this work required more than
16 000 GPU days of training overall""

Gemnet-OC took ~4*10^3 GPU-hours based in Figure 2",NVIDIA V100,,Likely,"Recent years have seen the advent of molecular simulation datasets that are orders of magnitude larger and more diverse. These new datasets differ substantially in four aspects of complexity: 1. Chemical diversity (number of different elements), 2. system size (number of atoms per sample), 3. dataset size (number of data samples), and 4. domain shift (similarity of the training and test set). Despite these large differences, benchmarks on small and narrow datasets remain the predominant method of demonstrating progress in graph neural networks (GNNs) for molecular simulation, likely due to cheaper training compute requirements. This raises the question -- does GNN progress on small and narrow datasets translate to these more complex datasets? This work investigates this question by first developing the GemNet-OC model based on the large Open Catalyst 2020 (OC20) dataset. GemNet-OC outperforms the previous state-of-the-art on OC20 by 16% while reducing training time by a factor of 10. We then compare the impact of 18 model components and hyperparameter choices on performance in multiple datasets. We find that the resulting model would be drastically different depending on the dataset used for making model choices. To isolate the source of this discrepancy we study six subsets of the OC20 dataset that individually test each of the above-mentioned four dataset aspects. We find that results on the OC-2M subset correlate well with the full OC20 dataset while being substantially cheaper to train on. Our findings challenge the common practice of developing GNNs solely on small datasets, but highlight ways of achieving fast development cycles and generalizable results via moderately-sized, representative datasets such as OC-2M and efficient models such as GemNet-OC. Our code and pretrained model weights are open-sourced.",4.0,,Open weights (unrestricted),"Germany,United States of America,United States of America",,,,,,2025-05-12 11:42,,,,,,"Academia,Academia,Industry",,,,4000.0,Open source,"MIT license

https://github.com/facebookresearch/fairchem/tree/main/src/fairchem/core","Academia,Academia,Industry",,,,,Hardware,,,,
Make-A-Video,Video,"Video generation,Text-to-video",Meta AI,"Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, Devi Parikh, Sonal Gupta, Yaniv Taigman",2022-09-29,Make-A-Video: Text-to-Video Generation without Text-Video Data,https://arxiv.org/abs/2209.14792,962.0,SOTA improvement,,,,,,"LAION,WebVid-10M,HD-VILA-100M",,,,,,,Self-supervised learning,Unknown,"We propose Make-A-Video -- an approach for directly translating the tremendous recent progress in Text-to-Image (T2I) generation to Text-to-Video (T2V). Our intuition is simple: learn what the world looks like and how it is described from paired text-image data, and learn how the world moves from unsupervised video footage. Make-A-Video has three advantages: (1) it accelerates training of the T2V model (it does not need to learn visual and multimodal representations from scratch), (2) it does not require paired text-video data, and (3) the generated videos inherit the vastness (diversity in aesthetic, fantastical depictions, etc.) of today's image generation models. We design a simple yet effective way to build on T2I models with novel and effective spatial-temporal modules. First, we decompose the full temporal U-Net and attention tensors and approximate them in space and time. Second, we design a spatial temporal pipeline to generate high resolution and frame rate videos with a video decoder, interpolation model and two super resolution models that can enable various applications besides T2V. In all aspects, spatial and temporal resolution, faithfulness to text, and quality, Make-A-Video sets the new state-of-the-art in text-to-video generation, as determined by both qualitative and quantitative measures.",,,Hosted access (no API),United States of America,,,,,,2025-05-30 13:52,,,,,,Industry,checked,,,,Unreleased,,Industry,,,,,,,,,
Sparrow,Language,"Chat,Language modeling/generation,Question answering",DeepMind,"Amelia Glaese, Nat McAleese, Maja TrÄ™bacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, Lucy Campbell-Gillingham, Jonathan Uesato, Po-Sen Huang, Ramona Comanescu, Fan Yang, Abigail See, Sumanth Dathathri, Rory Greig, Charlie Chen, Doug Fritz, Jaume Sanchez Elias, Richard Green, SoÅˆa MokrÃ¡, Nicholas Fernando, Boxi Wu, Rachel Foley, Susannah Young, Iason Gabriel, William Isaac, John Mellor, Demis Hassabis, Koray Kavukcuoglu, Lisa Anne Hendricks, Geoffrey Irving",2022-09-28,Improving alignment of dialogue agents via targeted human judgements,https://arxiv.org/abs/2209.14375,437.0,,,70000000000.0,70B,,,,,,,,,,,Confident,"We present Sparrow, an information-seeking dialogue agent trained to be more helpful, correct, and harmless compared to prompted language model baselines. We use reinforcement learning from human feedback to train our models with two new additions to help human raters judge agent behaviour. First, to make our agent more helpful and harmless, we break down the requirements for good dialogue into natural language rules the agent should follow, and ask raters about each rule separately. We demonstrate that this breakdown enables us to collect more targeted human judgements of agent behaviour and allows for more efficient rule-conditional reward models. Second, our agent provides evidence from sources supporting factual claims when collecting preference judgements over model statements. For factual questions, evidence provided by Sparrow supports the sampled response 78% of the time. Sparrow is preferred more often than baselines while being more resilient to adversarial probing by humans, violating our rules only 8% of the time when probed. Finally, we conduct extensive analyses showing that though our model learns to follow our rules it can exhibit distributional biases.",,,Unreleased,United Kingdom of Great Britain and Northern Ireland,Chinchilla,,"Few clues from paper. Not clear how much fine-tuning data in total. Also they freeze some layers during fine-tuning:

""In all cases when fine-tuning, we freeze the bottom 64 transformer layers of Chinchilla, and
only fine-tune the final 16 layers; this allows sharing of the frozen layers between the rule model,
preference models, and the base LM/policy when reranking and during reinforcement learning
training, resulting in a reduced memory footprint (fig. 8).""",,,2025-06-16 10:46,,,,,,Industry,checked,,,,Unreleased,,Industry,,,,,,,,,
Whisper,Speech,Speech recognition,OpenAI,"Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, Ilya Sutskever",2022-09-21,Robust Speech Recognition via Large-Scale Weak Supervision,"https://cdn.openai.com/papers/whisper.pdf

https://arxiv.org/abs/2212.04356",2240.0,SOTA improvement,,1550000000.0,Table 1,4.2072663e+21,See figure 9,Unspecified unreleased,,9302400000.0,"""When scaled to 680,000 hours of multilingual and multitask
supervision, the resulting models generalize well
to standard benchmarks and are often competitive
with prior fully supervised results but in a zeroshot transfer setting without the need for any finetuning.""


13,680 words/h * 680,000h = 9,302,400,000 words",,,,Self-supervised learning,Likely,"We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zeroshot transfer setting without the need for any finetuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.",3.0,,Open weights (unrestricted),United States of America,,,,,,2025-05-28 16:09,,,,256.0,Table 17,Industry,checked,,,,Unreleased,"MIT for weights:
https://github.com/openai/whisper

the repo looks like just inference code to me. also, this paper says it's just inference code and they reproduced their version of Whisper through other means: https://arxiv.org/pdf/2309.13876 ",Industry,,,FP16,,Hardware,,,,
DistilProtBert,Biology,"Proteins,Protein folding prediction",Bar-Ilan University,"Yaron Geffen, Yanay Ofran, Ron Unger",2022-09-18,DistilProtBert: a distilled protein language model used to distinguish between real proteins and their randomly shuffled counterparts,https://academic.oup.com/bioinformatics/article/38/Supplement_2/ii95/6701995,23.0,,,230000000.0,"""we were able to reduce the number of DistilProtBert parameters by almost half, to 230 M""",1.9000000000000003e+20,"""Pretraining was done on five v100 32-GB Nvidia GPUs from a DGX
cluster with a local batch size of 16 examples... The model was trained for three epochs using mixed precision and dynamic padding. Every epoch run took approximately 4 days, resulting in total pretraining time of 12 days""

5 * 125 teraFLOP/s * 12 * 24 * 3600 * 0.3 (assumed utilization) = 1.9e20",UniRef50,"""DistilProtBert was pretrained on 43 M sequences from UniRef50 with length ranging from 20 to 512 amino acids""",13000000001.0,"43,000,000 sequences Ã— 300 amino acids = 12,900,000,000 (1.29e10) data points

Calculation:
43,000,000 Ã— 300 = 12,900,000,000",288.0,12 days,NVIDIA Tesla V100 DGXS 32 GB,,Confident,"Recently, deep learning models, initially developed in the field of natural language processing (NLP), were applied successfully to analyze protein sequences. A major drawback of these models is their size in terms of the number of parameters needed to be fitted and the amount of computational resources they require. Recently, 'distilled' models using the concept of student and teacher networks have been widely used in NLP. Here, we adapted this concept to the problem of protein sequence analysis, by developing DistilProtBert, a distilled version of the successful ProtBert model. Implementing this approach, we reduced the size of the network and the running time by 50%, and the computational resources needed for pretraining by 98% relative to ProtBert model. Using two published tasks, we showed that the performance of the distilled model approaches that of the full model. We next tested the ability of DistilProtBert to distinguish between real and random protein sequences. The task is highly challenging if the composition is maintained on the level of singlet, doublet and triplet amino acids. Indeed, traditional machine-learning algorithms have difficulties with this task. Here, we show that DistilProtBert preforms very well on singlet, doublet and even triplet-shuffled versions of the human proteome, with AUC of 0.92, 0.91 and 0.87, respectively. Finally, we suggest that by examining the small number of false-positive classifications (i.e. shuffled sequences classified as proteins by DistilProtBert), we may be able to identify de novo potential natural-like proteins based on random shuffling of amino acid sequences.",3.0,,Open weights (unrestricted),Israel,,,,,,2025-06-11 18:08,,,,,,Academia,,,,,Open source,"MIT license
https://github.com/yarongef/DistilProtBert

MIT license
https://huggingface.co/yarongef/DistilProtBert",Academia,,,,,Hardware,yarongef,,,
CLIP ViT-H/14 - LAION-2B,Vision,Image classification,LAION,"Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade W Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa R Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, Jenia Jitsev",2022-09-15, Model Card for CLIP ViT-H/14 - LAION-2B ,https://huggingface.co/laion/CLIP-ViT-H-14-laion2B-s32B-b79K,,,,986000000.0,986M params from https://huggingface.co/laion/CLIP-ViT-H-14-laion2B-s32B-b79K,7.8410000000001e+22,"""A CLIP ViT-H/14 model trained with the LAION-2B English subset of LAION-5B (https://laion.ai/blog/laion-5b/) using OpenCLIP (https://github.com/mlfoundations/open_clip).""
Per https://laion.ai/blog/large-openclip/ H/14 used 824 A100s, trained 42 samples per gpu-second, saw 32B samples
compute = 33% * 311.84 TFLOPS * (32 billion / 42) seconds = 7.841e22 FLOP",LAION-2B,"""A CLIP ViT-H/14 model trained with the LAION-2B English subset of LAION-5B (https://laion.ai/blog/laion-5b/) using OpenCLIP (https://github.com/mlfoundations/open_clip).""",2000000000.0,"2B size of LAION-2B
input is image text pair
""A CLIP ViT-H/14 model trained with the LAION-2B English subset of LAION-5B (https://laion.ai/blog/laion-5b/) using OpenCLIP (https://github.com/mlfoundations/open_clip).""",,,,,Confident,"Model Description
A CLIP ViT-H/14 model trained with the LAION-2B English subset of LAION-5B (https://laion.ai/blog/laion-5b/) using OpenCLIP (https://github.com/mlfoundations/open_clip).

Model training done by Romain Beaumont on the stability.ai cluster.

Uses
As per the original OpenAI CLIP model card, this model is intended as a research output for research communities. We hope that this model will enable researchers to better understand and explore zero-shot, arbitrary image classification. We also hope it can be used for interdisciplinary studies of the potential impact of such model.

The OpenAI CLIP paper includes a discussion of potential downstream impacts to provide an example for this sort of analysis. Additionally, the LAION-5B blog (https://laion.ai/blog/laion-5b/) and upcoming paper include additional discussion as it relates specifically to the training dataset.",,,Open weights (unrestricted),"Multinational,Germany",,,,,,2025-02-14 17:00,,,,,,Research collective,,,,,Unreleased,"MIT license for weights

dataset is LAION which is open source:
https://laion.ai/blog/laion-5b/",Research collective,,,,,Hardware,,,,
NeMO Megatron GPT 20B,Language,"Language modeling/generation,Question answering",NVIDIA,,2022-09-15, NeMo Megatron-GPT 20B ,https://huggingface.co/nvidia/nemo-megatron-gpt-20B,,,,20000000000.0,20B,,"If trained for one epoch on the Pile, would be
6 * 20B * 220B = 2.64e22
No indication of actual number of tokens seen",The Pile,,341173367965.0,"Size of the pile is around 220B tokens:
825 GB * 200 words/GB * 4/3 token/word = 220B

as per figure 4 from https://arxiv.org/pdf/2204.06745
the Pile contains 341173367965 tokens

Possible they trained for less than one epoch.",,,,Self-supervised learning,Confident,"Megatron-GPT 20B is a transformer-based language model. GPT refers to a class of transformer decoder-only models similar to GPT-2 and 3 while 20B refers to the total trainable parameter count (20 Billion) [1, 2].",,,Open weights (unrestricted),United States of America,,,,,,2025-05-09 11:32,,,,,,Industry,,,,,Unreleased,"CC BY 4.0: https://creativecommons.org/licenses/by/4.0/

commercial, no restrictions other than to give credit",Industry,,,,,,,,,
PaLI,"Language,Vision,Multimodal","Visual question answering,Language modeling/generation",Google,"Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver, Nan Ding, Keran Rong, Hassan Akbari, Gaurav Mishra, Linting Xue, Ashish Thapliyal, James Bradbury, Weicheng Kuo, Mojtaba Seyedhosseini, Chao Jia, Burcu Karagol Ayan, Carlos Riquelme, Andreas Steiner, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, Radu Soricut",2022-09-14,PaLI: A Jointly-Scaled Multilingual Language-Image Model,https://arxiv.org/abs/2209.06794v4,567.0,SOTA improvement,"""PaLI achieves state-of-the-art in multiple vision and language tasks
(such as captioning, visual question-answering, scene-text understanding)""",16900000000.0,"3.9b Image Encoder, 
14b Multimodal Encoder-Decoder",1.69e+23,"Pre-training the ViT component involved 1.1 million steps (they train over 1M steps but run the last 100k twice and then average the two resulting models). Batch size is 16384 and the inputs are 224x224. Table 8 indicates a forward pass with ViT-e/14 on a 224 image takes 1980 GFLOPs, so total training compute for the ViT-e/14 model is:
1980e9 * 16384 * 1.1 million * 3 (account for backward passes) = 1.07e23

In the ""overal model"" section, they then say: ""The largest model, PaLI-17B, is pretrained using 1,024 GCP-TPUv4 chips for 7 days"". It is then trained for another 3 days on 512 chips at higher resolution. 

I assume the stated TPUv4 training does not include the ViT pretraining, since it amounts to fewer FLOPs than we estimate above for the ViT.

275 teraFLOP/s * ((1024 * 7) + (512 * 3)) * 24 * 3600 * 0.3 (utilization assumption) = 6.2e22

Total: 1.07e23 + 6.2e22 = 1.69e23",WebLI,"""we introduce WebLI, a multilingual imagelanguage dataset built from images and texts available on the public web... Due to the abundance of multilingual content on the internet, the collection process for the WebLI dataset can be scaled to cover 10 billion images and 12 billion alt-texts. In addition to annotation with web text, we use publicly available automatic service to extract OCR annotations on all images, resulting in 29 billion image-OCR pairs. To balance quality and retain scale, we filter the dataset to the highest quality subset retaining only the top 10% scoring of the original WebLI image-text pairs (about 1B examples), which we use to train PaLI""",1600000000.0,"""During training, the model passes over 1.6B images, one epoch over the entire pretraining dataset""",240.0,10,Google TPU v4,,Likely,"Effective scaling and a flexible task interface enable large language models to excel at many tasks. We present PaLI (Pathways Language and Image model), a model that extends this approach to the joint modeling of language and vision. PaLI generates text based on visual and textual inputs, and with this interface performs many vision, language, and multimodal tasks, in many languages. To train PaLI, we make use of large pre-trained encoder-decoder language models and Vision Transformers (ViTs). This allows us to capitalize on their existing capabilities and leverage the substantial cost of training them. We find that joint scaling of the vision and language components is important. Since existing Transformers for language are much larger than their vision counterparts, we train a large, 4-billion parameter ViT (ViT-e) to quantify the benefits from even larger-capacity vision models. To train PaLI, we create a large multilingual mix of pretraining tasks, based on a new image-text training set containing 10B images and texts in over 100 languages. PaLI achieves state-of-the-art in multiple vision and language tasks (such as captioning, visual question-answering, scene-text understanding), while retaining a simple, modular, and scalable design.",1.0,,Unreleased,United States of America,,,,1024.0,,2025-05-16 10:30,,,,,,Industry,checked,,,172032.0,Unreleased,,Industry,$50878.11,,,348601.5921270125,"Operation counting,Hardware",,,,
BEIT-3,"Multimodal,Vision,Language","Object detection,Semantic segmentation,Image classification,Visual question answering,Image captioning,Language generation",Microsoft,Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks,2022-08-22,Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks,https://arxiv.org/abs/2208.10442,563.0,SOTA improvement,"from abstract: 'In this work, we introduce a general-purpose multimodal foundation model BEiT-3, which achieves state-of-the-art transfer performance on both vision and vision-language tasks.'",1900000000.0,1.9B from Table 2,7e+19,"from Table 11, 1M training steps with batch size 6144. 
From Table 2 we have that model have 1.9B parameters.
Model is VIT","ImageNet21k,COCO,English Wikipedia,""BookCorpus (BooksCorpus, Toronto Book Corpus)""",from Table 3,,"from Table 3
21M pairs image text,
14M images,160GB documents",,,,,Likely,"A big convergence of language, vision, and multimodal pretraining is emerging. In this work, we introduce a general-purpose multimodal foundation model BEiT-3, which achieves state-of-the-art transfer performance on both vision and vision-language tasks. Specifically, we advance the big convergence from three aspects: backbone architecture, pretraining task, and model scaling up. We introduce Multiway Transformers for general-purpose modeling, where the modular architecture enables both deep fusion and modality-specific encoding. Based on the shared backbone, we perform masked ""language"" modeling on images (Imglish), texts (English), and image-text pairs (""parallel sentences"") in a unified manner. Experimental results show that BEiT-3 obtains state-of-the-art performance on object detection (COCO), semantic segmentation (ADE20K), image classification (ImageNet), visual reasoning (NLVR2), visual question answering (VQAv2), image captioning (COCO), and cross-modal retrieval (Flickr30K, COCO). ",,,Open weights (unrestricted),"United States of America,Multinational,India,Belgium",,,,,,2025-06-04 17:44,,,,,,Industry,,,,,Unreleased,"apache 2.0
https://github.com/microsoft/unilm/tree/master/beit3

It seems that there are no pre-training code, only fine-tuning code",Industry,,,,,Operation counting,,,,
PaLM-SayCan,Robotics,Robotic manipulation,Google,"Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan, Andy Zeng",2022-08-16,"Do As I Can, Not As I Say: Grounding Language in Robotic Affordances",https://arxiv.org/abs/2204.01691,1446.0,,Top10 recent paper from Sebastian Sartor 2025-05-14,540000000000.0,"540B from PaLM-540B.

",,"""The RL model is trained using 16 TPUv3 chips and for about 100 hours, as well as a pool of 3000 CPU workers to collect episodes and another 3000 CPU workers to compute target Q-values.""

1600 hours * 3600 * 123 teraflops * 0.4 (assumed utilization) = 2.83e20

PaLM itself uses 2.5272e+24, so additional training is negligible.",,,,,100.0,The RL model is trained using 16 TPUv3 chips and for about 100 hours,Google TPU v3,Reinforcement learning,Likely,"Large language models can encode a wealth of semantic knowledge about the world. Such knowledge could be extremely useful to robots aiming to act upon high-level, temporally extended instructions expressed in natural language. However, a significant weakness of language models is that they lack real-world experience, which makes it difficult to leverage them for decision making within a given embodiment. For example, asking a language model to describe how to clean a spill might result in a reasonable narrative, but it may not be applicable to a particular agent, such as a robot, that needs to perform this task in a particular environment. We propose to provide real-world grounding by means of pretrained skills, which are used to constrain the model to propose natural language actions that are both feasible and contextually appropriate. The robot can act as the language model's ""hands and eyes,"" while the language model supplies high-level semantic knowledge about the task. We show how low-level skills can be combined with large language models so that the language model provides high-level knowledge about the procedures for performing complex and temporally-extended instructions, while value functions associated with these skills provide the grounding necessary to connect this knowledge to a particular physical environment. We evaluate our method on a number of real-world robotic tasks, where we show the need for real-world grounding and that this approach is capable of completing long-horizon, abstract, natural language instructions on a mobile manipulator. The project's website and the video can be found at this https URL.",,,Unreleased,United States of America,PaLM (540B),283000000000000000000,"I think PaLM-540B wasn't actually involved in training and the authors trained a separate RL model. 

""The RL model is trained using 16 TPUv3 chips and for about 100 hours, as well as a pool of 3000 CPU workers to collect episodes and another 3000 CPU workers to compute target Q-values.""

Ignoring CPU calculations:
1600 hours * 3600 * 123 teraflops * 0.4 (assumed utilization) = 2.83e20",16.0,,2025-06-16 14:21,,,,,,Industry,checked,,,,Open source,"We open source a version of SayCan that works with a simulated tabletop environment. [tabletop saycan]

https://github.com/google-research/google-research/tree/master/saycan

License: Apache 2.0",Industry,,,,7053.483004299183,,,,,
Luminous-supreme,Language,Language generation,Aleph Alpha,,2022-08-15,Model Card Luminous,https://docs.aleph-alpha.com/docs/introduction/model-card/,,,,70000000000.0,"""~70B""",3.5461e+23,"""~839000h"" GPU-hours on A100s, per Environmental Impact section of model card.

312 trillion * 839000 * 3600 * 0.3 = 2.8e23

6ND = 6*70B*1069.30B = 4.49106e+23

sqrt(2.8e23*4.49106e+23) = 3.54612... Ã— 10^23

reported here:
167TFLOPS
https://docs.aleph-alpha.com/docs/Deprecated%20Luminous/Deprecated-Luminous/model-card/",,"""The Luminous family has been trained on a dataset compiled of sources in English, German, French, Spanish and Italian...""

more details in model card 
https://docs.aleph-alpha.com/docs/introduction/model-card/",1069300000000.0,"from the table

Total Size:
2.77 + 0.79 + 0.18 + 0.07 + 0.06 + 0.02 = 3.89 TB
Tokens:
761.41B + 217.15B + 49.47B + 19.29B + 16.49B + 5.49B = 1069.30B tokens",2016.0,Approximately 12 weeks = 2016 hours,"NVIDIA A100 SXM4 40 GB,NVIDIA A100 SXM4 80 GB",,Confident,"The Luminous series is a family of large language models. Large language models are powerful technological tools that can process and produce text. These capabilities emerge during model â€œtrainingâ€ where the model is exposed to significant amounts of human text data. Similar to a person who deliberately absorbs information while reading a whole library and half of the internet, large language models acquire structural understanding (and not necessarily also knowledge) of language and accumulated information about the world.

The Luminous family currently consists of three vanilla models, which differ in complexity and ability. They are, from the smallest to the largest, luminous-base, luminous-extended and luminous-supreme. All Luminous models are trained in the five most commonly spoken European languages: English, German, French, Italian and Spanish.",,,API access,Germany,,,,512.0,,2024-11-18 15:56,,,,,,Industry,,,,,Unreleased,,Industry,,,,,"Hardware,Operation counting",,,,
Luminous-extended,Language,Language modeling/generation,Aleph Alpha,,2022-08-15,,https://docs.aleph-alpha.com/docs/Deprecated%20Luminous/Deprecated-Luminous/model-card/,,,,30000000000.0,~30B (~42B with multi-modality),1.0019457e+23,"311840000000000*360000*3600*0.3 = 1.2124339e+23

6ND = 6*30*10^9*460000000000 = 8.28e+22

sqrt(8.28e+22*1.2124339e+23) = 1.0019457e+23",,"""""""The Luminous family has been trained on a dataset compiled of sources in English, German, French, Spanish and Italian...""""

more details in model card 
https://docs.aleph-alpha.com/docs/introduction/model-card/""",460000000000.0,"~460B tokens
230000 iterations",1344.0,~ 8 weeks = 56*24 = 1344 hours,NVIDIA A100 SXM4 40 GB,,Confident,"Aleph Alpha luminous-extended is the second largest model which is faster and cheaper than Luminous-supreme. the model can perform information extraction, language simplification and has multi-capable image description capability. You can try Aleph Alpha models with predefined examples for free. Go to at the Jumpstart page on their site and click through the examples on Classification and Labelling, Generation, Information Extraction, Translation and Conversion and Multimodal. Aleph Alpha are based in Europe, which allows customers with sensitive data to process their information in compliance with European regulations for data protection and security on a sovereign, European computing infrastructure.",,,API access,Germany,,,,512.0,,2025-05-01 10:42,,,,,,Industry,,,,360000.0,Unreleased,,Industry,,,,410393.6048104728,"Operation counting,Hardware",,,,
Luminous-base,Language,Language modeling/generation,Aleph Alpha,,2022-08-15,,https://docs.aleph-alpha.com/docs/Deprecated%20Luminous/Deprecated-Luminous/model-card/,,,,13000000000.0,~13B (~15B with multi-modality),3.1673782e+22,"311840000000000*95000*3600*0.3 = 3.1994784e+22

6ND = 6*13*10^9*402000000000 = 3.1356e+22

sqrt(3.1356e+22* 3.1994784e+22) = 3.1673782e+22",,"""""""The Luminous family has been trained on a dataset compiled of sources in English, German, French, Spanish and Italian...""""

more details in model card 
https://docs.aleph-alpha.com/docs/introduction/model-card/""",402000000000.0,"192000 iterations
~402B tokens",1344.0,~ 8 weeks = 56*24 = 1344 hours,NVIDIA A100 SXM4 40 GB,,Confident,"Luminous-base is our smallest model. That makes it our fastest and cheapest model. Therefore, it is best used for use-cases where speed is important and costs should be low. This may include tasks like classification and labelling.",,,API access,Germany,,,,128.0,,2025-05-01 10:42,,,,,,Industry,,,,95000.0,Unreleased,,Industry,,,,102598.4012026182,"Operation counting,Hardware",,,,
PeTriBERT,Biology,Protein generation,"University of Montpellier,BionomeeX","Baldwin Dumortier, Antoine Liutkus, ClÃ©ment CarrÃ©, Gabriel Krouk",2022-08-13,PeTriBERT : Augmenting BERT with tridimensional encoding for inverse protein folding and design,https://www.biorxiv.org/content/10.1101/2022.08.10.503344v1.abstract,9.0,,,40000000.0,,1e+20,"1. Hardware setup: 8x NVIDIA Tesla V100 SXM2 32GB GPUs (1.25 x 10^14 FLOP/s per GPU)

2. Training duration: 70 hours (directly provided) = 252,000 seconds

3. Utilization rate: 40%

4. Final calculation:
1.25 x 10^14 FLOP/s/GPU Ã— 8 GPUs Ã— 252,000 seconds Ã— 0.4 = 1.0 x 10^20 FLOPs",,,297000001.0,"Training sequence data points = 290,000 proteins Ã— 1,024 tokens/protein = 297,160,000 tokens (~2.97Ã—10â¸)",70.0,,NVIDIA Tesla V100 DGXS 32 GB,,Confident,"Protein is biology workhorse. Since the recent break-through of novel folding methods, the amount of available structural data is increasing, closing the gap between data-driven sequence-based and structure-based methods. In this work, we focus on the inverse folding problem that consists in predicting an amino-acid primary sequence from protein 3D structure. For this purpose, we introduce a simple Transformer model from Natural Language Processing augmented 3D-structural data. We call the resulting model PeTriBERT: Proteins embedded in tridimensional representation in a BERT model. We train this small 40-million parameters model on more than 350 000 proteins sequences retrieved from the newly available AlphaFoldDB database. Using PetriBert, we are able to in silico generate totally new proteins with a GFP-like structure. These 9 of 10 of these GFP structural homologues have no ressemblance when blasted on the whole entry proteome database. This shows that PetriBert indeed capture protein folding rules and become a valuable tool for de novo protein design.",,,,"France,France",,,,8.0,,2025-05-01 10:42,,,,,,Industry,,,,,,,Industry,,,,4007.928551099969,Hardware,,,,
M3GNet,Materials science,"Atomistic simulations,Molecular simulation",University of California San Diego,"Chi Chen, Shyue Ping Ong",2022-08-11,A Universal Graph Deep Learning Interatomic Potential for the Periodic Table,https://arxiv.org/abs/2202.02450,,,,,,,,Materials Project,"""Utilizing the largely untapped dataset of more than 187,000 energies, 16,000,000 forces and
1,600,000 stresses from structural relaxations performed by the Materials Project since its
inception in 2011, we trained a universal material graph with three-body interactions neural
network (M3GNet) IAP for 89 elements of the periodic table with low energy, force, and stress errors.""",,,,,,,Unknown,"Interatomic potentials (IAPs), which describe the potential energy surface of atoms, are a fundamental input for atomistic simulations. However, existing IAPs are either fitted to narrow chemistries or too inaccurate for general applications. Here, we report a universal IAP for materials based on graph neural networks with three-body interactions (M3GNet). The M3GNet IAP was trained on the massive database of structural relaxations performed by the Materials Project over the past 10 years and has broad applications in structural relaxation, dynamic simulations and property prediction of materials across diverse chemical spaces. About 1.8 million materials were identified from a screening of 31 million hypothetical crystal structures to be potentially stable against existing Materials Project crystals based on M3GNet energies. Of the top 2000 materials with the lowest energies above hull, 1578 were verified to be stable using DFT calculations. These results demonstrate a machine learning-accelerated pathway to the discovery of synthesizable materials with exceptional properties.",,,Open weights (unrestricted),United States of America,,,,,,2025-05-01 10:42,,,,,,Academia,,,,,Open source,"https://github.com/materialsvirtuallab/m3gnet

BSD 3-Clause ",Academia,,,,,,,,,
BlenderBot 3,Language,Chat,"McGill University,Meta AI,Mila - Quebec AI (originally Montreal Institute for Learning Algorithms)","Kurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju, Eric Michael Smith, Stephen Roller, Megan Ung, Moya Chen, Kushal Arora, Joshua Lane, Morteza Behrooz, William Ngan, Spencer Poff, Naman Goyal, Arthur Szlam, Y-Lan Boureau, Melanie Kambadur, Jason Weston",2022-08-10,BlenderBot 3: a deployed conversational agent that continually learns to responsibly engage,"https://arxiv.org/abs/2208.03188, https://github.com/facebookresearch/ParlAI/blob/main/parlai/zoo/bb3/model_card.md

training code: https://parl.ai/projects/bb3/ ",218.0,SOTA improvement,"""Human evaluations show its superiority to existing open-domain dialogue agents, including its predecessors""",175000000000.0,,4.3e+23,(taken from OPT-175 base),BlenderBot 3 Data,"Fine-tuned from OPT-175B.

""The fine-tuning data for BB3 comprises roughly 4 million source/target examples spread across the various training modules. This corresponds to around 1.13B training tokens. When fine-tuning the OPT-based BB3 models, we additionally included 600k examples ( 170m tokens) of pre-training data to help with training stability. Table 16 and Table 17 enumerate the breakdown by module.""
",1300000000.0,,,,NVIDIA A100 SXM4 40 GB,,Likely,"We present BlenderBot 3, a 175B parameter dialogue model capable of open-domain conversation with access to the internet and a long-term memory, and having been trained on a large number of user defined tasks. We release both the model weights and code, and have also deployed the model on a public web page to interact with organic users. This technical report describes how the model was built (architecture, model and training scheme), and details of its deployment, including safety mechanisms. Human evaluations show its superiority to existing open-domain dialogue agents, including its predecessors (Roller et al., 2021; Komeili et al., 2022). Finally, we detail our plan for continual learning using the data collected from deployment, which will also be publicly released. The goal of this research program is thus to enable the community to study ever-improving responsible agents that learn through interaction.",,,Open weights (non-commercial),"Canada,United States of America,Canada",OPT-175B,1,"""The 30B and 175B parameter BlenderBot 3 models were each trained for one epoch of the training data
on 64 (30B) or 128 (175B) x 40gb A100 GPUs; we found that the model (especially the 175B version)
overfit significantly when seeing the training data more than once. The 175B model was trained with
a batch size of 2^18 and the 30B model was trained with a batch size of 2^19, resulting in roughly 5600
updates and 2800 updates respectively.""

175b params * 5600 * 2^18 * 6 = 1.5e21
",128.0,,2025-05-16 10:30,,,,262144.0,"Note that this is batch size for fine-tuning. Blenderbot is based on OPT-175B which had batch size 2M.

""The 175B model was trained with a batch size of 2^18""
2^18 = 262144","Academia,Industry,Academia",,,,,Open source,"weights have a non-commercial license, must go through request form: https://docs.google.com/forms/d/e/1FAIpQLSfRzw8xVzxaxgRyuodTZtkcYADAjzYjN5gcxx6DMa4XaGwwhQ/viewform

meanwhile training code is here. repo is MIT-licensed https://github.com/facebookresearch/ParlAI/blob/main/parlai/scripts/train_model.py ","Academia,Industry,Academia",,,,102609.82584809876,Operation counting,,,,
RNA-FM,Biology,RNA structure prediction,"Chinese University of Hong Kong (CUHK),Fudan University,Shanghai AI Lab,Harbin Institute of Technology,University of Electronic Science and Technology of China,Massachusetts Institute of Technology (MIT),Harvard University,Shanghai Zelixir Biotech,CUHK Shenzhen Research Institute","Jiayang Chen, Zhihang Hu, Siqi Sun, Qingxiong Tan, Yixuan Wang, Qinze Yu, Licheng Zong, Liang Hong, Jin Xiao, Tao Shen, Irwin King, Yu Li",2022-08-08,Interpretable RNA Foundation Model from Unannotated Data for Highly Accurate RNA Structure and Function Predictions,https://arxiv.org/abs/2204.00300,78.0,,,,,2.59999999999998e+21,"1. Hardware setup: 8x NVIDIA A100 GPUs (3.12E14 FLOP/s per GPU)

2. Training duration: 30 days estimated (2,592,000 seconds)

3. Utilization rate: 40%

4. Calculation: 3.12E14 FLOP/s Ã— 8 GPUs Ã— 2,592,000 seconds Ã— 0.4 = 2.59E21 FLOPs",,,23500000001.0,"23 million sequences Ã— 1024 tokens per sequence:
23,000,000 Ã— 1,024 = 23 Ã— 10â¶ Ã— 1.024 Ã— 10Â³ = 2.35 Ã— 10Â¹â° tokens",720.0,,NVIDIA A100,,Confident,"Non-coding RNA structure and function are essential to understanding various biological processes, such
as cell signaling, gene expression, and post-transcriptional regulations. These are all among the core problems
in the RNA field. With the rapid growth of sequencing technology, we have accumulated a massive amount
of unannotated RNA sequences. On the other hand, expensive experimental observatory results in only limited
numbers of annotated data and 3D structures. Hence, it is still challenging to design computational methods for
predicting their structures and functions. The lack of annotated data and systematic study causes inferior performance. To resolve the issue, we propose a novel RNA foundation model (RNA-FM) to take advantage of all
the 23 million non-coding RNA sequences through self-supervised learning. Within this approach, we discover
that the pre-trained RNA-FM could infer sequential and evolutionary information of non-coding RNAs without
using any labels. Furthermore, we demonstrate RNA-FMâ€™s effectiveness by applying it to the downstream secondary/3D structure prediction, SARS-CoV-2 genome structure and evolution prediction, protein-RNA binding
preference modeling, and gene expression regulation modeling. The comprehensive experiments show that the
proposed method improves the RNA structural and functional modelling results significantly and consistently.
Despite only being trained with unlabelled data, RNA-FM can serve as the foundational model for the field.",,,Open weights (unrestricted),"Hong Kong,China,China,China,China,China,United States of America,United States of America,China,China",,,,8.0,,2025-06-13 15:09,,,,,,"Academia,Academia,Academia,Academia,Academia,Academia,Academia,Industry",,,,,Open source,"MIT license
https://github.com/ml4bio/RNA-FM

Apache 2.0
https://huggingface.co/cuhkaih/rnafm","Academia,Academia,Academia,Academia,Academia,Academia,Academia,Industry",,,,6413.399753905654,Hardware,cuhkaih,,,
FastSpeech 2,Speech,Text-to-speech,"Zhejiang University (ZJU),Microsoft Research Asia","Yi Ren, Chenxu Hu, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, Tie-Yan Liu",2022-08-08,FastSpeech 2: Fast and High-Quality End-to-End Text to Speech,https://arxiv.org/abs/2006.04558,,,,27000000.0,27M,2.2977e+18,125000000000000*17.02*3600*0.3 = 2.2977e+18,LJSpeech,"LJSpeech contains 13,100 English audio clips (about 24 hours) and corresponding text transcripts. We split the dataset into three sets: 12,228 samples for training, 349 samples (with document title LJ003) for validation
and 523 samples (with document title LJ001 and LJ002) for testing.",,,,"17.02 h (Table 2)

We train FastSpeech 2 on 1 NVIDIA V100 GPU, with batchsize of 48 sentences.",NVIDIA V100,,Likely,"Non-autoregressive text to speech (TTS) models such as FastSpeech can synthesize speech significantly faster than previous autoregressive models with comparable quality. The training of FastSpeech model relies on an autoregressive teacher model for duration prediction (to provide more information as input) and knowledge distillation (to simplify the data distribution in output), which can ease the one-to-many mapping problem (i.e., multiple speech variations correspond to the same text) in TTS. However, FastSpeech has several disadvantages: 1) the teacher-student distillation pipeline is complicated and time-consuming, 2) the duration extracted from the teacher model is not accurate enough, and the target mel-spectrograms distilled from teacher model suffer from information loss due to data simplification, both of which limit the voice quality. In this paper, we propose FastSpeech 2, which addresses the issues in FastSpeech and better solves the one-to-many mapping problem in TTS by 1) directly training the model with ground-truth target instead of the simplified output from teacher, and 2) introducing more variation information of speech (e.g., pitch, energy and more accurate duration) as conditional inputs. Specifically, we extract duration, pitch and energy from speech waveform and directly take them as conditional inputs in training and use predicted values in inference. We further design FastSpeech 2s, which is the first attempt to directly generate speech waveform from text in parallel, enjoying the benefit of fully end-to-end inference. Experimental results show that 1) FastSpeech 2 achieves a 3x training speed-up over FastSpeech, and FastSpeech 2s enjoys even faster inference speed; 2) FastSpeech 2 and 2s outperform FastSpeech in voice quality, and FastSpeech 2 can even surpass autoregressive models. Audio samples are available at this https URL.",,,Unreleased,"China,China",,,,1.0,,2025-05-01 10:42,,,,,,"Academia,Industry",,,,17.0,Unreleased,,"Academia,Industry",,,,330.3605642465138,Hardware,,,,
SGPT BE 5.8B,Language,"Semantic search,Semantic embedding,Entity embedding",Peking University,Niklas Muennighoff,2022-08-05,SGPT: GPT Sentence Embeddings for Semantic Search,https://arxiv.org/abs/2202.08904,,,,5800000000.0,5.8B,,,MS MARCO,"""For asymmetric search, we train
on MS-MARCO [28]. We limit the model sequence length to 300 tokens during both training and
inference""",,,60.0,"""For SGPT-BE symmetric search training took 21 hours, while asymmetric
training took 60 hours for the 5.8B model""",NVIDIA A100 SXM4 40 GB,,Confident,"Decoder transformers have continued increasing in scale reaching hundreds of billions of parameters. Due to their scale the same decoder sets state-of-the-art results on various language tasks via prompting or fine-tuning. Yet, these large foundation models remain unusable for the related fields of semantic search and sentence embeddings. This prevents possibly new state-of-the-art results and forces organizations to train and maintain separate models. To this end, we propose SGPT to use decoders for sentence embeddings and semantic search via prompting or fine-tuning. At 5.8 billion parameters SGPT improves on the previously best sentence embeddings by a margin of 7% and outperforms a concurrent method with 175 billion parameters as measured on the BEIR search benchmark. Code, models and result files are freely available at this https URL.",,,Open weights (non-commercial),China,GPT-J-6B,161740800000000000000,312000000000000 FLOP / GPU / sec [bf16 assumed] * 8 GPUs * 60 hours * 3600 sec / hour * 0.3 [assumed utilization] = 1.617408e+20 FLOP,8.0,,2025-06-04 17:06,Oracle,,,,,Academia,,,,,Open source,"no clear license
https://huggingface.co/Muennighoff/SGPT-5.8B-weightedmean-msmarco-specb-bitfit

MIT license
https://github.com/Muennighoff/sgpt",Academia,,,,6413.828235359284,Hardware,Muennighoff,,,
GLM-130B,Language,"Language modeling/generation,Translation",Tsinghua University,"Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Peng Zhang, Yuxiao Dong, Jie Tang",2022-08-04,GLM-130B: An Open Bilingual Pre-trained Model,https://keg.cs.tsinghua.edu.cn/glm-130b/posts/glm-130b/,989.0,SOTA improvement,"""GLM-130B achieves an accuracy of 80.2% on zero-shot LAMBADA (En), while 76.2% for GPT-3 175B and 77.9% for the SOTA offered by PaLM 540B.""",130000000000.0,Dense model,3.5490054945e+23,"""96 NVIDIA A100 (40G * 8) servers for 2 months""

312 TFLOPS/GPU * 96 servers * 8 GPU/server * 2 months * 32.5% utilization = 4.037e23

utilization rate - citation from the paper: ""we report hardware FLOPs utilization (HFU) of 43.3% and model FLOPs utilization (MFU) of 32.5% due to re-materialization.""

Aligns pretty well with 6ND:
6 * 400B * 130B = 3.12E23

Geometric mean: sqrt(4.037e23 * 3.12e23) = 3.549e23","The Pile,WuDao Corpora","""The pre-training data includes 1.2T Pile (train split) (Gao et al., 2020) English, 1.0T Chinese WudaoCorpora (Yuan et al., 2021), and 250G Chinese corpora (including online forums, encyclopedia, and
QA) we crawl from the web, which form a balanced composition of English and Chinese contents""",400000000000.0,"400B ""We completed the 400B-token training and evaluation of GLM-130B in July, and subsequently released the model and pre-training details in August 2022. ""  from https://arxiv.org/pdf/2406.12793

""As of July 3rd, 2022, GLM-130B has been trained on over 400 billion text tokens (200B each for Chinese and English)""",1440.0,"""During the 60-day access to the cluster, we manage to train GLM-130B for 400 billion tokens""
60 days * 24 = 1,440 hours",NVIDIA A100 SXM4 40 GB,,Confident,"GLM-130B (ICLR 2023) is an open bilingual (English & Chinese) bidirectional dense model with 130 billion parameters, pre-trained using the General Language Model (GLM) algorithm1. It is designed to support inference tasks with the 130B parameters on a single A100 (40G * 8) or V100 (32G * 8) server. As of July 3rd, 2022, GLM-130B has been trained on over 400 billion text tokens (200B each for Chinese and English) ",1.0,GLM-130B,Open weights (non-commercial),China,,,,768.0,0.325,2025-05-28 16:09,,,,8650752.0,,Academia,checked,,,1105920.0,Unreleased,non commercial license. looks like inference but not training code: https://github.com/THUDM/GLM-130B/blob/main/MODEL_LICENSE,Academia,$820296.63,"""Following the calculation in (Chowdhery et al., 2022), we report hardware FLOPs utilization (HFU) of 43.3% and model FLOPs utilization (MFU) of 32.5% due to re-materialization.""",FP16,615741.2226117075,"Operation counting,Hardware",,,,12
AlexaTM 20B,Language,"Language modeling,Translation,Question answering",Amazon,"Saleh Soltan, Shankar Ananthakrishnan, Jack FitzGerald, Rahul Gupta, Wael Hamza, Haidar Khan, Charith Peris, Stephen Rawls, Andy Rosenbaum, Anna Rumshisky, Chandana Satya Prakash, Mukund Sridhar, Fabian Triefenbach, Apurv Verma, Gokhan Tur, Prem Natarajan",2022-08-02,AlexaTM 20B: Few-Shot Learning Using a Large-Scale Multilingual Seq2Seq Model,https://arxiv.org/abs/2208.01448,73.0,SOTA improvement,The Abstract reports SOTA improvement on multiple benchmarks.,19750000000.0,See Table 1 on p.3 of the paper,2.04374016e+23,"Training throughput is reported as 154 TFLOP/s - see p.5 of the paper.
""We relied on an internal and optimized version of DeepSpeed that we have since open-sourced (Chiu & Zheng, 2022) to obtain training throughput of up to 154 TFLOPS/GPU on 16 AWS p4d.24xlarge compute instances.""

Accelerator compute days are reported as 15,360 days - see Table 17 on p.18 of the paper.","mC4,Wikipedia",,1319000000000.0,"See Table 2 on p.3 of the paper.

119B Wikipedia tokens + 1.2T mC4 tokens = 1319000000000 tokens",2880.0,"See p.5 of the paper: ""We trained AlexaTM 20B for 120 days on 128 A100 GPUs...""",NVIDIA A100,,Confident,"In this work, we demonstrate that multilingual large-scale sequence-to-sequence (seq2seq) models, pre-trained on a mixture of denoising and Causal Language Modeling (CLM) tasks, are more efficient few-shot learners than decoder-only models on various tasks. In particular, we train a 20 billion parameter multilingual seq2seq model called Alexa Teacher Model (AlexaTM 20B) and show that it achieves state-of-the-art (SOTA) performance on 1-shot summarization tasks, outperforming a much larger 540B PaLM decoder model. AlexaTM 20B also achieves SOTA in 1-shot machine translation, especially for low-resource languages, across almost all language pairs supported by the model (Arabic, English, French, German, Hindi, Italian, Japanese, Marathi, Portuguese, Spanish, Tamil, and Telugu) on Flores-101 dataset. We also show in zero-shot setting, AlexaTM 20B outperforms GPT3 (175B) on SuperGLUE and SQuADv2 datasets and provides SOTA performance on multilingual tasks such as XNLI, XCOPA, Paws-X, and XWinograd. Overall, our results present a compelling case for seq2seq models as a powerful alternative to decoder-only models for Large-scale Language Model (LLM) training. ",,,API access,United States of America,,,,128.0,0.4935,2025-05-28 16:09,Amazon Web Services,,,2000000.0,"""We trained AlexaTM 20B for 120 days on 128 A100 GPUs for the total of 500k updates with the accumulated batch size of 2 million tokens""",Industry,,,,368640.0,Unreleased,https://aws.amazon.com/about-aws/whats-new/2022/11/alexatm-20b-model-available-sagemaker-jumpstart/?nc1=h_ls,Industry,$267943.21,"Training througput reported at 154 TFLOPs/GPU, vs 312 TFLOPs/GPU for the A100s they use
154e12 / 312e12 = 0.4935",BF16,102628.10792703854,Hardware,,,,17
ProtGPT2,Biology,"Proteins,Protein generation,Protein or nucleotide language model (pLM/nLM)",University of Bayreuth,"Noelia Ferruz, Steffen Schmidt, Birte HÃ¶cker ",2022-07-27,ProtGPT2 is a deep unsupervised language model for protein design,https://www.nature.com/articles/s41467-022-32007-7,367.0,,,738000000.0,"""Here, we introduce ProtGPT2, an autoregressive Transformer model with 738 million parameters capable of generating de novo protein sequences in a high-throughput fashion.""",4.1e+21,"""The model trained on 128 NVIDIA A100s in 4 days""

128 * 4 * 24 * 3600 * 312 trillion FLOP/s * 0.3 = 4.1e21

5.3E+21 from Table 9 https://www.biorxiv.org/content/10.1101/2023.07.05.547496v1",UniRef50,"""We took Uniref50 version 2021_04 as the dataset for training, containing 49,874,565 sequences""",,,96.0,,NVIDIA A100,Unsupervised,Confident,"Protein design aims to build novel proteins customized for specific purposes, thereby holding the potential to tackle many environmental and biomedical problems. Recent progress in Transformer-based architectures has enabled the implementation of language models capable of generating text with human-like capabilities. Here, motivated by this success, we describe ProtGPT2, a language model trained on the protein space that generates de novo protein sequences following the principles of natural ones. The generated proteins display natural amino acid propensities, while disorder predictions indicate that 88% of ProtGPT2-generated proteins are globular, in line with natural sequences. Sensitive sequence searches in protein databases show that ProtGPT2 sequences are distantly related to natural ones, and similarity networks further demonstrate that ProtGPT2 is sampling unexplored regions of protein space. AlphaFold prediction of ProtGPT2-sequences yields well-folded non-idealized structures with embodiments and large loops and reveals topologies not captured in current structure databases. ProtGPT2 generates sequences in a matter of seconds and is freely available.",,,Open weights (unrestricted),Germany,,,,128.0,,2025-05-09 11:32,,,,,,Academia,,,,12288.0,,"apache

https://huggingface.co/nferruz/ProtGPT2",Academia,,,,102641.82162383666,"Hardware,Third-party estimation",,,,
GPT-NeoX-Japanese,Language,Language modeling/generation,Abeja,"Shinya Otani, Takayoshi Makabe, Anuj Arora, Kyo Hattori ",2022-07-27,"This repository provides a 2.7B-parameter Japanese GPT-NeoX-based model. The model was trained by ABEJA, Inc","https://huggingface.co/abeja/gpt-neox-japanese-2.7b, https://tech-blog.abeja.asia/entry/abeja-gpt-project-202207",,,,2700000000.0,2.7B,,,"Japanese CC-100,Wikipedia (ja)","""The model was trained on Japanese CC-100, Japanese Wikipedia, and Japanese OSCAR.""",,,,,,,Confident,"https://huggingface.co/docs/transformers/model_doc/gpt_neox_japanese

We introduce GPT-NeoX-Japanese, which is an autoregressive language model for Japanese, trained on top of https://github.com/EleutherAI/gpt-neox. Japanese is a unique language with its large vocabulary and a combination of hiragana, katakana, and kanji writing scripts. To address this distinct structure of the Japanese language, we use a special sub-word tokenizer. We are very grateful to tanreinama for open-sourcing this incredibly helpful tokenizer. Following the recommendations from Googleâ€™s research on PaLM, we have removed bias parameters from transformer blocks, achieving better model performance. Please refer this article in detail.

Development of the model was led by Shinya Otani, Takayoshi Makabe, Anuj Arora, and Kyo Hattori from ABEJA, Inc.. For more information on this model-building activity, please refer here (ja).",,,Open weights (unrestricted),Japan,GPT-NeoX-20B,,actually 2.7B,,,2024-09-05 14:08,,,,,,Industry,,,,,,MIT license,Industry,,,,,,,,,
OmegaPLM,Biology,"Proteins,Protein folding prediction","Massachusetts Institute of Technology (MIT),Westlake University","Ruidong Wu, Fan Ding, Rui Wang, Rui Shen, Xiwen Zhang, Shitong Luo, Chenpeng Su, Zuofan Wu, Qi Xie, Bonnie Berger, Jianzhu Ma, Jian Peng",2022-07-22,High-resolution de novo structure prediction from primary sequence,https://www.biorxiv.org/content/10.1101/2022.07.21.500999v1,268.0,Historical significance,"""Here, we introduce OmegaFold, the first computational method to successfully predict high-resolution protein structure from a single primary sequence alone. Using a new combination of a protein language model that allows us to make predictions from single sequences and a geometry-inspired transformer model trained on protein structures, OmegaFold outperforms RoseTTAFold and achieves similar prediction accuracy to AlphaFold2 on recently released structures""",670000000.0,"""Our model contains 66 layers with around 670 million parameters without sharing parameters, which doubles the layer count of ESM-1b but roughly retains the parameter count.""",1.03514112e+22,"""OmegaPLM is implemented in PyTorch (44) and trained for 2,560 GPU Nvidia A100 80G days."" 
""Default precision format in Nvidia A100 GPUs is set to TensorFloat-32 for matrix operations.""

Assume 0.3 utilization for language model

Estimate: (2560 * 24 * 3600) s * 156e12 FLOP/s * 0.3 * = 1.04e22",UniRef50,"""After pretraining on sequences in UniRef50 (dated at 2021/04)""",8960000001.0,"Number of sequences: 35 x 10^6
Sequence length: 512
Total data points: 35 x 10^6 x 512 = 1.792 x 10^10 tokens

First stage crop size: 256
First stage data points: 35 x 10^6 x 256 = 8.96 x 10^9 tokens

Additional structural data: ~110,000 sequences = 7.68 x 10^7 tokens

Final estimate: 8.96 x 10^9 tokens",,"2,560 GPU Nvidia A100 80G days",NVIDIA A100 SXM4 80 GB,,Confident,"Recent breakthroughs have used deep learning to exploit evolutionary information in multiple sequence alignments (MSAs) to accurately predict protein structures. However, MSAs of homologous proteins are not always available, such as with orphan proteins or fast-evolving proteins like antibodies, and a protein typically folds in a natural setting from its primary amino acid sequence into its three-dimensional structure, suggesting that evolutionary information and MSAs should not be necessary to predict a proteinâ€™s folded form. Here, we introduce OmegaFold, the first computational method to successfully predict high-resolution protein structure from a single primary sequence alone. Using a new combination of a protein language model that allows us to make predictions from single sequences and a geometry-inspired transformer model trained on protein structures, OmegaFold outperforms RoseTTAFold and achieves similar prediction accuracy to AlphaFold2 on recently released structures. OmegaFold enables accurate predictions on orphan proteins that do not belong to any functionally characterized protein family and antibodies that tend to have noisy MSAs due to fast evolution. Our study fills a much-encountered gap in structure prediction and brings us a step closer to understanding protein folding in nature.",,,Unreleased,"United States of America,China",,,,,,2025-05-30 13:27,,,,2097152.0,"""[...] each batch contains 4,096 sequences and each sequence is padded or cropped to 512 residues"" 4096 * 512 = 2097152","Academia,Academia",,,,61440.0,Unreleased,,"Academia,Academia",$52400.32,,TF32,,Hardware,,,,
ESM2-15B,Biology,"Proteins,Protein or nucleotide language model (pLM/nLM),Protein folding prediction","Meta AI,New York University (NYU),Stanford University,Massachusetts Institute of Technology (MIT)","Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, Allan dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Salvatore Candido, Alexander Rives",2022-07-21,Evolutionary-scale prediction of atomic-level protein structure with a language model,"https://www.science.org/doi/abs/10.1126/science.ade2574
https://www.biorxiv.org/content/10.1101/2022.07.20.500902v2",636.0,SOTA improvement,"""The resulting ESM-2 model family significantly outperforms previously state-of-the-art ESM-1b (a âˆ¼650 million parameter model) at a comparable number of parameters, and on structure prediction benchmarks it also outperforms other recent protein language models""",15000000000.0,"""we train models up to 15B parameters""",7.350000000009999e+22,"from xTrimoPGLM paper Table 9 (https://www.biorxiv.org/content/10.1101/2023.07.05.547496v1): 5.1e22 FLOP

from Arb Research (https://arbresearch.com/files/gen_bio.pdf): ""ESM-2-15B: 270000 updates x 3.2M batch size x 15 B â€œconnectionsâ€ x 6. : 7.8e22 FLOP

from the paper's Supplementary Materials: 
""We trained each model over 512 NVIDIA V100 GPUs. ESM2 700M took 8 days to train. The 3B parameter LM took 30 days. The 15B model took 60 days.""
60 days x 512 V100s x an imputed 30% utilization"": 1e23 FLOP

Geometric mean: 7.35e22",UniRef50,"""UniRef50, September 2021 version, is used for the training of ESM models""",12000000000.0,"Section A.1.1:
""This allowed ESM-2 models to train on over 60M protein sequences.""

Average protein sequence is 200 tokens, per https://epoch.ai/blog/biological-sequence-models-in-the-context-of-the-ai-directives#fn:4 
60M * 200 = 12B tokens

Epochs: 15B model used 270k steps at 3.2M token batch size
270k * 3.2M / 12B = 72",1440.0,,NVIDIA V100,Unsupervised,Confident,"""Recent advances in machine learning have leveraged evolutionary information in multiple sequence alignments to predict protein structure. We demonstrate direct inference of full atomic-level protein structure from primary sequence using a large language model. As language models of protein sequences are scaled up to 15 billion parameters, an atomic-resolution picture of protein structure emerges in the learned representations. This results in an order-of-magnitude acceleration of high-resolution structure prediction, which enables large-scale structural characterization of metagenomic proteins. We apply this capability to construct the ESM Metagenomic Atlas by predicting structures for >617 million metagenomic protein sequences, including >225 million that are predicted with high confidence, which gives a view into the vast breadth and diversity of natural proteins.""",72.0,,Open weights (unrestricted),"United States of America,United States of America,United States of America,United States of America",,,,512.0,,2025-05-09 11:32,,,,,,"Industry,Academia,Academia,Academia",,,,,Unreleased,"MIT weights, CC BY 4.0 data
https://github.com/facebookresearch/esm?tab=readme-ov-file#available-esmssd

may just be inference code in the repo^","Industry,Academia,Academia,Academia",$163467.82,,,307966.61145938886,"Hardware,Third-party estimation",,,,
YuYan 11B,Language,Language modeling/generation,"Hong Kong Baptist University,NetEase","Gongzheng Li, Yadong Xi, Jingzhen Ding, Duan Wang, Ziyang Luo, Rongsheng Zhang, Bai Liu, Changjie Fan, Xiaoxi Mao, Zeng Zhao",2022-07-15,Easy and Efficient Transformer: Scalable Inference Solution For Large NLP Model,https://arxiv.org/abs/2104.12470,8.0,,,11000000000.0,https://huggingface.co/FUXI/yuyan-11b,,,,,,,,,"NVIDIA A100 PCIe,NVIDIA GeForce RTX 2080 Ti 11GB",Self-supervised learning,Confident,"Recently, large-scale transformer-based models have been proven to be effective over various tasks across many domains. Nevertheless, applying them in industrial production requires tedious and heavy works to reduce inference costs. To fill such a gap, we introduce a scalable inference solution: Easy and Efficient Transformer (EET), including a series of transformer inference optimization at the algorithm and implementation levels. First, we design highly optimized kernels for long inputs and large hidden sizes. Second, we propose a flexible CUDA memory manager to reduce the memory footprint when deploying a large model. Compared with the state-of-the-art transformer inference library (Faster Transformer v4.0), EET can achieve an average of 1.40-4.20x speedup on the transformer decoder layer with an A100 GPU",,,,"Hong Kong,China,China",,,,,,2025-05-09 11:32,,,,,,"Academia,Industry",,,,,,,"Academia,Industry",,,,,,,,,
Transformer-XL + RMT,Language,Language modeling,"Moscow Institute of Physics and Technology,AIRI Artificial Intelligence Research Institute","Aydar Bulatov, Yuri Kuratov, Mikhail S. Burtsev",2022-07-14,Recurrent Memory Transformer,https://arxiv.org/abs/2207.06881,85.0,,,247000000.0,"""WikiText-103 experiments use 16-layer Transformers (10 heads, 410 hidden size, 2100 intermediate FF)""",6.7839792e+18,"6 FLOP / parameter / token * 200000 steps * 128 tokens per batch * 60 batches per step * 247000000 parameters= 2.276352e+18 FLOP

312000000000000 FLOP / sec / GPU * 2 GPUs * 30 hours [could be less, upper bound] * 3600 sec / hour * 0.3 [assumed utilization] = 2.02176e+19 FLOP

sqrt(2.276352e+18*2.02176e+19) = 6.7839792e+18 FLOP",WikiText-103,"""200,000 steps for WikiText-103""
"" Model size and training parameters are selected to match Transformer-XL paper. For Wikitext-103 an input context length was set to 150 tokens""

batch_size 60 (from https://github.com/booydar/LM-RMT/blob/main/pytorch/run_wt103_base.sh)

200000*128*60/103000000 = 15 epochs ",103000000.0,,30.0,"In most of the WikiText-103 experiments, we used 2 NVIDIA A100 80Gb GPUs,
training time varied from 10 to 30 hours depending on sequence length, memory size, and number of BPTT unrolls",NVIDIA A100 SXM4 80 GB,,Speculative,"Transformer-based models show their effectiveness across multiple domains and tasks. The self-attention allows to combine information from all sequence elements into context-aware representations. However, global and local information has to be stored mostly in the same element-wise representations. Moreover, the length of an input sequence is limited by quadratic computational complexity of self-attention.
In this work, we propose and study a memory-augmented segment-level recurrent Transformer (RMT). Memory allows to store and process local and global information as well as to pass information between segments of the long sequence with the help of recurrence.
We implement a memory mechanism with no changes to Transformer model by adding special memory tokens to the input or output sequence. Then the model is trained to control both memory operations and sequence representations processing.
Results of experiments show that RMT performs on par with the Transformer-XL on language modeling for smaller memory sizes and outperforms it for tasks that require longer sequence processing. We show that adding memory tokens to Tr-XL is able to improve its performance. This makes Recurrent Memory Transformer a promising architecture for applications that require learning of long-term dependencies and general purpose in memory processing, such as algorithmic tasks and reasoning.",15.0,Transformer-XL + RMT,Unreleased,"Russia,Russia",,,,2.0,,2025-04-21 20:09,,,Transformer-XL + RMT,,,"Academia,Research collective",,,,,Open source,Apache 2: https://github.com/booydar/LM-RMT,"Academia,Research collective",,,,1604.2428268807498,"Operation counting,Hardware",,,,
Rita-XLarge,Biology,"Proteins,Protein or nucleotide language model (pLM/nLM)","LightOn,Harvard University,University of Oxford","Daniel Hesslow, Niccolo Zanichelli, Pascal Notin, Iacopo Poli, Debora Marks ",2022-07-14,RITA: a Study on Scaling Up Generative Protein Sequence Models,https://arxiv.org/abs/2205.05789,70.0,,,1200000000.0,"""with up to 1.2 billion parameters""",8.64e+20,"""The models were trained for a total training time of over 25 thousand Nvidia-V100 GPU hours""

125 teraFLOP/s (uncertain which V100 model, tensor performance varies from 112-130tFLOP/s) * 25000 * 3600 * 0.3 (utilization) = 3.4e+21"" <- the total compute for several models

For the biggest (XL) from Figure 1:
10 PF-days = 10*10^15*24*3600 FLOPs = 8.64e+20 FLOPs <- total compute just for the XL model","UniRef100,MGnify,Metaclust","""We focus on three different pre-training corpora: UniRef100 (The UniProt Consortium, 2020), MGnify (Mitchell et al., 2020) and Metaclust (Steinegger & Soding Â¨ , 2018), each providing a sufficient amount of tokens for model pretraining without having to repeat the data.""",300000000001.0,"150 billion amino acids Ã— 2 (primary + reversed sequences) = 300 billion datapoints
Final estimate: 3.0e11 datapoints",,,NVIDIA V100,,Confident,"In this work we introduce RITA: a suite of autoregressive generative models for protein sequences, with up to 1.2 billion parameters, trained on over 280 million protein sequences belonging to the UniRef-100 database. Such generative models hold the promise of greatly accelerating protein design. We conduct the first systematic study of how capabilities evolve with model size for autoregressive transformers in the protein domain: we evaluate RITA models in next amino acid pre- diction, zero-shot fitness, and enzyme function prediction, showing benefits from increased scale. We release the RITA models openly, to the benefit of the research community.",,,Open weights (unrestricted),"France,United States of America,United Kingdom of Great Britain and Northern Ireland",,,,,,2025-04-30 10:04,,,,,,"Industry,Academia,Academia",,,,,Unreleased,"MIT license
https://github.com/lightonai/RITA","Industry,Academia,Academia",,,,,Reported,,,,
Delphi,Language,Chat,"Allen Institute for AI,University of Washington","Liwei Jiang, Jena D. Hwang, Chandra Bhagavatula, Ronan Le Bras, Jenny Liang, Jesse Dodge, Keisuke Sakaguchi, Maxwell Forbes, Jon Borchardt, Saadia Gabriel, Yulia Tsvetkov, Oren Etzioni, Maarten Sap, Regina Rini, Yejin Choi",2022-07-12,Can Machines Learn Morality? The Delphi Experiment,https://arxiv.org/abs/2110.07574,,,,11000000000.0,,,, Commonsense Norm Bank,"To teach Delphi, we compile a new dataset, COMMONSENSE NORM BANK (or NORM BANK in short), which contains 1.7 million examples of descriptive judgments on everyday situations.2 All of these examples are drawn from existing datasets to cover diverse aspects of social norms and ethics.
The relevant data sources for this paper include SOCIAL CHEMISTRY (Forbes et al., 2020) for social norms and commonsense moral judgments, the commonsense morality subsection of ETHICS (Hendrycks et al., 2021a) for additional moral judgments, MORAL STORIES (Emelin et al., 2021) for contextualized moral judgments in simple commonsense stories, and SOCIAL BIAS INFERENCE CORPUS (Sap et al., 2020) for unjust social biases such as racism and sexism",,Training on the proposed COMMONSENSE NORM BANK is carried out for 400k gradient updates,72.0,"We train Delphi using TPU v3-32 and evaluate it using TPU v3-8, with model parallelisms of 32 and 8 respectively, on Google Cloud Virtual Machines. Training Delphi on COMMONSENSE NORM BANK for 4 epochs takes approximately 72 hours.",Google TPU v3,,Speculative,"As AI systems become increasingly powerful and pervasive, there are growing concerns about machines' morality or a lack thereof. Yet, teaching morality to machines is a formidable task, as morality remains among the most intensely debated questions in humanity, let alone for AI. Existing AI systems deployed to millions of users, however, are already making decisions loaded with moral implications, which poses a seemingly impossible challenge: teaching machines moral sense, while humanity continues to grapple with it.
To explore this challenge, we introduce Delphi, an experimental framework based on deep neural networks trained directly to reason about descriptive ethical judgments, e.g., ""helping a friend"" is generally good, while ""helping a friend spread fake news"" is not. Empirical results shed novel insights on the promises and limits of machine ethics; Delphi demonstrates strong generalization capabilities in the face of novel ethical situations, while off-the-shelf neural network models exhibit markedly poor judgment including unjust biases, confirming the need for explicitly teaching machines moral sense.
Yet, Delphi is not perfect, exhibiting susceptibility to pervasive biases and inconsistencies. Despite that, we demonstrate positive use cases of imperfect Delphi, including using it as a component model within other imperfect AI systems. Importantly, we interpret the operationalization of Delphi in light of prominent ethical theories, which leads us to important future research questions.",4.0,,Hosted access (no API),"United States of America,United States of America",Unicorn,1689600000000000000,'=6*11B*400k gradient updates*16 (batch size)*4 epochs=1.6896 Ã— 10^18,,,2024-12-02 10:24,,,,16.0,,"Research collective,Academia",,,,,Unreleased,https://delphi.allenai.org/,"Research collective,Academia",,,,,Operation counting,,,,
BLOOM-176B,Language,"Language modeling,Translation,Code generation","Hugging Face,BigScience","Margaret Mitchell, Giada Pistilli, Yacine Jernite, Ezinwanne Ozoani, Marissa Gerchick, Nazneen Rajani, Sasha Luccioni, Irene Solaiman, Maraim Masoud, Somaieh Nikpoor, Carlos MuÃ±oz Ferrandis, Stas Bekman, Christopher Akiki, Danish Contractor, David Lansky, Angelina McMillan-Major, Tristan Thrush, Suzana IliÄ‡, GÃ©rard Dupont, Shayne Longpre, Manan Dey, Stella Biderman, Douwe Kiela, Emi Baylor, Teven Le Scao, Aaron Gokaslan, Julien Launay, Niklas Muennighoff",2022-07-11,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model,https://arxiv.org/abs/2211.05100,1984.0,"Historical significance,Highly cited","Was the largest open-source model at the time. 1000+ researchers, many from important orgs such as Microsoft and NVIDIA.

https://huggingface.co/bigscience/bloom",176247271424.0,"See ""Technical Specifications"" on Hugging Face:
https://huggingface.co/bigscience/bloom",3.65664e+23,"https://bigscience.huggingface.co/blog/bloom Blog post says 117 days.

384 A100 GPUs * 314 TFLOPS throughput per GPU * 117 days * 0.3 (utilization assumption) = 3.65664e23
https://www.wolframalpha.com/input?i=384+*+314+TFLOPS+*+117+days+*+0.3",BigScience ROOTS Corpus,"In total, 1.6 terabytes of pre-processed text was converted into 350 billion unique tokens as BLOOM's training datasets.
arXiv:2210.15424

""BLOOM was trained on the ROOTS corpus (LaurenÂ¸con et al., 2022), a composite collection
of 498 Hugging Face datasets (Lhoest et al., 2021) amounting to 1.61 terabytes of text that
span 46 natural languages and 13 programming languages. A high-level overview of this
dataset can be seen in Figure 3, while a detailed itemized list of every language along with
its linguistic genus, family and macroarea is presented in Table 1""",379000000000.0,"Table 3.5 https://arxiv.org/pdf/2211.05100

366B (pretrain) + 13B (finetune) = 379B  tokens total ",2808.0,117 days * 24 hours/day,NVIDIA A100 SXM4 80 GB,Self-supervised learning,Confident,"Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",1.0,,Open weights (restricted use),"Multinational,United States of America,Multinational,France",,,,384.0,0.5,2025-06-11 11:30,,,,4194304.0,Table 3. 2048*2048,"Industry,Research collective",checked,,,1078272.0,Unreleased,responsible use restrictions: https://bigscience.huggingface.co/blog/the-bigscience-rail-license,"Industry,Research collective",$901068.66,"""We were able to achieve 156 TFLOPs in our fastest configuration with A100 GPUs, attaining our objective of half of the theoretical peak performance of 312 TFLOPs (in float32 or bfloat16).""
156/312 = 0.5",BF16,308035.2013244814,Hardware,,,,9
NLLB,Language,Translation,Meta AI,"Marta R. Costa-jussÃ , James Cross, Onur Ã‡elebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loic Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco (Paco) GuzmÃ¡n, Philipp Koehn, Alexandre Mourachko, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, Jeff Wang",2022-07-06,No Language Left Behind: Scaling Human-Centered Machine Translation,https://research.facebook.com/publications/no-language-left-behind/,924.0,SOTA improvement,"""Our model achieves an improvement of 44% BLEU relative to the previous state-of-the-art""",54500000000.0,"Section 8.2.4: ""The model has a total of 54.5B parameters
and FLOPs similar to that of a 3.3B dense model""",1.751113728e+22,"Section 8.8:
"" To train NLLB-200, a cumulative
of 51968 GPU hours of computation was performed on hardware of type A100-SXM-80GB""
See also Table 48

Section 8.2.4 states they use FP16

NVIDIA Datasheet states 312TFLOPS for FP16
https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-nvidia-us-2188504-web.pdf

Assuming 0.3 utilization:

312e12*3600*51968*0.3

Also:
""Our final model is a Transformer
encoder-decoder model in which we replace the Feed Forward Network (FFN) layer in
every 4th Transformer block with a Sparsely Gated Mixture of Experts layer containing 128
experts. We use model dimension 2048, FFN dimension 8192, 16 attention heads, 24 encoder
layers and 24 decoder layers. We use Pre-LayerNorm (Xiong et al., 2020) as described in
Section 6.1.1. We share the embedding weights of the encoder input embedding, decoder
input embedding and decoder output embedding layers. We use an overall dropout of 0.3,
attention dropout 0.1 and EOM with peom=0.2. The model has a total of 54.5B parameters
and FLOPs similar to that of a 3.3B dense model.""",,,360000000000.0,"[WORDS]

Section 8.2.2: ""As we prepare to train on the final 202 language dataset comprising of over 18B sentence
pairs and 2440 language directions""

18B sentences * 20 words/sentence",,,NVIDIA A100 SXM4 80 GB,Self-supervised learning,Confident,"Driven by the goal of eradicating language barriers on a global scale, machine translation has solidified itself as a key focus of artificial intelligence research today. However, such efforts have coalesced around a small subset of languages, leaving behind the vast majority of mostly low-resource languages. What does it take to break the 200 language barrier while ensuring safe, high quality results, all while keeping ethical considerations in mind? In No Language Left Behind, we took on this challenge by first contextualizing the need for low-resource language translation support through exploratory interviews with native speakers. Then, we created datasets and models aimed at narrowing the performance gap between low and high-resource languages. More specifically, we developed a conditional compute model based on Sparsely Gated Mixture of Experts that is trained on data obtained with novel and effective data mining techniques tailored for low-resource languages. We propose multiple architectural and training improvements to counteract overfitting while training on thousands of tasks. Critically, we evaluated the performance of over 40,000 different translation directions using a human-translated benchmark, Flores-200, and combined human evaluation with a novel toxicity benchmark covering all languages in Flores-200 to assess translation safety. Our model achieves an improvement of 44% BLEU relative to the previous state-of-the-art, laying important groundwork towards realizing a universal translation system. Finally, we open source all contributions described in this work, accessible at https://github.com/facebookresearch/fairseq/tree/nllb.",,,Open weights (unrestricted),United States of America,,,,,,2025-06-18 13:17,,,,1000000.0,"""We train the model for 300k steps using the 4 phase curriculum
described in Section 8.2.3. We use an effective batch size of 1M tokens per update.""",Industry,,,,59168.0,Open source,"MIT

train code: https://github.com/facebookresearch/fairseq/blob/nllb/examples/nllb/modeling/README.md ",Industry,$50667.25,,FP16,,Hardware,,,,
BLOOM-7.1B,Language,"Language modeling/generation,Translation","Hugging Face,BigScience","Margaret Mitchell, Giada Pistilli, Yacine Jernite, Ezinwanne Ozoani, Marissa Gerchick, Nazneen Rajani, Sasha Luccioni, Irene Solaiman, Maraim Masoud, Somaieh Nikpoor, Carlos MuÃ±oz Ferrandis, Stas Bekman, Christopher Akiki, Danish Contractor, David Lansky, Angelina McMillan-Major, Tristan Thrush, Suzana IliÄ‡, GÃ©rard Dupont, Shayne Longpre, Manan Dey, Stella Biderman, Douwe Kiela, Emi Baylor, Teven Le Scao, Aaron Gokaslan, Julien Launay, Niklas Muennighoff",2022-07-05,BigScience Language Open-science Open-access Multilingual (BLOOM) Language Model,https://arxiv.org/abs/2211.05100,404.0,,,7070000000.0,,1.5014556e+22,6 FLOP / token / parameter * 7070000000 parameters * 354000000000 tokens = 1.501668 Ã— 10^22 FLOP,BigScience ROOTS Corpus,,354000000000.0,"Table 3 https://arxiv.org/pdf/2211.05100

341B (pretrain) + 13B (finetune) = 354B  tokens total ",,,,,Confident,"Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",1.0,BLOOM-7.1B,Open weights (restricted use),"Multinational,United States of America,Multinational,France",,,,,,2025-03-31 11:24,,,,,,"Industry,Research collective",,,,,Unreleased,"no harmful use: https://bigscience.huggingface.co/blog/the-bigscience-rail-license

https://huggingface.co/bigscience/bloom-7b1","Industry,Research collective",,,,,Operation counting,bigscience,,,
CodeT5-large,Language,Code generation,Salesforce,"Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, Steven C.H. Hoi ",2022-07-05,CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning,https://arxiv.org/abs/2207.01780,190.0,SOTA improvement,"""Our method not only achieves new SOTA results on the challenging APPS benchmark, but also shows strong zero-shot transfer capability with new SOTA results on the simpler MBPP benchmark.""",770000000.0,"""We pretrain a CodeT5-large model (770M) from scratch following T5-largeâ€™s architecture""",2.72e+21,"""We perform our experiments on a kubernetes with 16 A100-40G GPUs on Google Cloud Platform and the total pretraining duration is around 21 days""

16 * 312tFLOP/s * 21 * 24 * 3600 * 0.3 (utilization assumption) = 2.72e21",GitHub,"""We enlarge the Python pretraining dataset using the recently released
large-scale Github Code dataset5. We have compiled public, non-personal information from GitHub consisting of permissively licensed Python code (e.g. â€œmitâ€, â€œapache-2â€, â€œbsd-3-clauseâ€, â€œbsd-2- 126clauseâ€, â€œcc0-1.0â€, â€œunlicenseâ€, â€œiscâ€). The resulting Python dataset (GCPY) has 10.5B tokens and is 10x larger than the CodeSearchNet (CSN) corpus [Husain et al., 2019] used in the original CodeT5 [Wang et al., 2021]""",10500000000.0,10.5b tokens,504.0,21 days,NVIDIA A100,,Likely,"""Program synthesis or code generation aims to generate a program that satisfies a problem specification. Recent approaches using large-scale pretrained language models (LMs) have shown promising results, yet they have some critical limitations. In particular, they often follow a standard supervised fine-tuning procedure to train a code generation model only from the pairs of natural-language problem descriptions and ground-truth programs. Such paradigm largely ignores some important but potentially useful signals in the problem specification such as unit tests, which thus often results in poor performance when solving complex unseen coding tasks. To address the limitations, we propose ""CodeRL"", a new framework for program synthesis tasks through pretrained LMs and deep reinforcement learning (RL). Specifically, during training, we treat the code-generating LM as an actor network, and introduce a critic network that is trained to predict the functional correctness of generated programs and provide dense feedback signals to the actor. During inference, we introduce a new generation procedure with a critical sampling strategy that allows a model to automatically regenerate programs based on feedback from example unit tests and critic scores. For the model backbones, we extended the encoder-decoder architecture of CodeT5 with enhanced learning objectives, larger model sizes, and better pretraining data. Our method not only achieves new SOTA results on the challenging APPS benchmark, but also shows strong zero-shot transfer capability with new SOTA results on the simpler MBPP benchmark.""",150.0,,Open weights (unrestricted),United States of America,,,,,,2025-06-06 11:04,,,,,,Industry,,,,,Open source,"BSD-3-Clause license
https://github.com/salesforce/CodeT5

https://huggingface.co/Salesforce/codet5-large",Industry,$4478.15,,FP16,,Hardware,Salesforce,,,
Minerva (540B),Language,Quantitative reasoning,Google,"Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, Vedant Misra",2022-06-29,Solving Quantitative Reasoning Problems with Language Models,https://arxiv.org/abs/2206.14858,585.0,SOTA improvement,,540350000000.0,"""To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model (PaLM).""

Our approach is to start with the PaLM pretrained decoder-only transformer language models Chowdhery et al. (2022), and further train (finetune) them on our mathematical dataset using an autoregressive objective.
Table 2 contains the main model and training hyperparameters.

See Table 2",2.7415e+24,"Minerva was fine-tuned from PaLM using the same hardware. Assume the same model FLOPs utilization rate for pre-training and fine-tuning.
""the 540B model was trained for 29 days on a v4-1024""

PaLM pretraining time: 6144 TPU for 1200 hours + 3072 TPU for 336 hours = @8404992 TPU-hours
Minerva finetuning time: 1024 TPU for 696 hours = 712704 TPU-hours
So fine-tuning added 8.5% more compute.

Minerva total compute = PaLM pretraining compute * (712704+8404992)/(8404992) = 2.7415*10^24 FLOP
https://www.wolframalpha.com/input?i=%28712704%2B8404992%29%2F%288404992%29+*+2.5272*10%5E24

Palm pretraining: 2.5272e+24",arXiv,"PaLM, finetuned on arxiv",26000000000.0,"""Our models were trained on a dataset of 38.5B tokens"" + PaLM

upd 38.5B tokens - sie of the dataset, the model saw 26B tokens in 399k steps (see Table 2)",696.0,,Google TPU v4,Self-supervised learning,Confident,"Language models have achieved remarkable performance on a wide range of tasks that require natural language understanding. Nevertheless, state-of-the-art models have generally struggled with tasks that require quantitative reasoning, such as solving mathematics, science, and engineering problems at the college level. To help close this gap, we introduce Minerva, a large language model pretrained on general natural language data and further trained on technical content. The model achieves state-of-the-art performance on technical benchmarks without the use of external tools. We also evaluate our model on over two hundred undergraduate-level problems in physics, biology, chemistry, economics, and other sciences that require quantitative reasoning, and find that the model can correctly answer nearly a third of them.",,,Unreleased,United States of America,PaLM (540B),2,,1024.0,,2025-06-18 13:15,,,,,,Industry,checked,,,712704.0,Unreleased,,Industry,,,,349199.866571188,Hardware,,,,
DALL-E mega,Image generation,"Text-to-image,Image generation",Craiyon,"Dayma, Boris and Patil, Suraj and Cuenca, Pedro and Saifullah, Khalid and Abraham, Tanishq and LÃª Kháº¯c, PhÃºc and Melas, Luke and Ghosh, Ritobrata",2022-06-28,DALLÂ·E Mega Model Card,"https://huggingface.co/dalle-mini/dalle-mega
https://github.com/borisdayma/dalle-mini",,,,,,2.285273088e+22,"flops = (128) * (1.23e14) * (1344 * 3600) * (0.3) = 2.3e22
(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)
from https://huggingface.co/dalle-mini/dalle-mega#environmental-impact",,possible the same data as DALL-E mini,2000000000.0,possible the same data as DALL-E mini,1344.0,from https://huggingface.co/dalle-mini/dalle-mega#environmental-impact,Google TPU v3,,Confident,"Model Description: This is a model that can be used to generate images based on text prompts. As the model developers wrote in the project report about DALLÂ·E mini, â€œOpenAI had the first impressive model for generating images with DALLÂ·E. DALLÂ·E mini is an attempt at reproducing those results with an open-source model.â€",,,Open weights (unrestricted),United States of America,,,,128.0,,2025-02-17 11:00,,,,,,Industry,,,,172032.0,Open source,"apache 2.0
training journal: https://wandb.ai/dalle-mini/dalle-mini/reports/DALL-E-Mega-Training--VmlldzoxODMxMDI2#training-parameters ",Industry,,,,56489.47168105598,Hardware,,,,
ProGen2-xlarge,Biology,"Proteins,Protein generation,Protein or nucleotide language model (pLM/nLM)","Salesforce Research,Columbia University,Johns Hopkins University","Erik Nijkamp, Jeffrey Ruffolo, Eli N. Weinstein, Nikhil Naik, Ali Madani
",2022-06-27,ProGen2: Exploring the Boundaries of Protein Language Models,https://arxiv.org/abs/2206.13517,199.0,SOTA improvement,"""ProGen2 models show state-of-the-art performance in capturing the distribution of observed evolutionary sequences, generating novel viable sequences, and pre- dicting protein fitness without additional finetuning.""",6400000000.0,"""We introduce a suite of protein language models, named ProGen2, that are scaled up to 6.4B parameters""",1.35e+22,"Estimate 1:
""350,000 steps x 1m batch size x 6.4 B â€œconnectionsâ€ x 6"" - Arb Research (https://arbresearch.com/files/gen_bio.pdf)
Steps and batches from Table 1. 
FLOP estimate: 1.3e22

Table 9 from here: https://www.biorxiv.org/content/10.1101/2023.07.05.547496v1.full.pdf
FLOP estimate: 1.4e22

Geometric mean = 1.35e22 FLOP","UniRef90,BFD30","""The standard PROGEN2 models are pretrained on a mixture of Uniref90 (Suzek et al., 2015) and BFD30 (Steinegger & SÃ¶ding, 2018) databases""",350000000000.0,350B from Table 9 https://www.biorxiv.org/content/10.1101/2023.07.05.547496v1,,,Google TPU v3,Self-supervised learning,Confident,"Attention-based models trained on protein sequences have demonstrated incredible success at classification and generation tasks relevant for artificial intelligence- driven protein design. However, we lack a sufficient understanding of how very large-scale models and data play a role in effective protein model development. We introduce a suite of protein language models, named ProGen2, that are scaled up to 6.4B parameters and trained on different sequence datasets drawn from over a billion proteins from genomic, metagenomic, and immune repertoire databases. ProGen2 models show state-of-the-art performance in capturing the distribution of observed evolutionary sequences, generating novel viable sequences, and predicting protein fitness without additional finetuning. As large model sizes and raw numbers of protein sequences continue to become more widely accessible, our results suggest that a growing emphasis needs to be placed on the data distribution provided to a protein sequence model. We release the ProGen2 models and code at https://github.com/salesforce/progen.",,,Open weights (unrestricted),"United States of America,United States of America,United States of America",,,,,,2025-05-09 11:32,,,,,,"Industry,Academia,Academia",,,,,Unreleased,"BSD license (permissive)
https://github.com/salesforce/progen?tab=BSD-3-Clause-1-ov-file#readme","Industry,Academia,Academia",$11850.18,,,,"Hardware,Third-party estimation",,,,
ProGen2-base,Biology,Protein or nucleotide language model (pLM/nLM),"Salesforce Research,Columbia University,Johns Hopkins University","Erik Nijkamp, Jeffrey Ruffolo, Eli N. Weinstein, Nikhil Naik, Ali Madani
",2022-06-27,ProGen2: Exploring the Boundaries of Protein Language Models,https://arxiv.org/abs/2206.13517,,,,764000000.0,764M (Table1),1.1e+21,"Table 9 from here: https://www.biorxiv.org/content/10.1101/2023.07.05.547496v1.full.pdf
FLOP estimate: 1.1E+21

6 FLOP / parameter / token * 764*10^6 parameters * 200*10^9 tokens = 9.168e+20 FLOP","UniRef90,BFD30","""The standard PROGEN2 models are pretrained on a mixture of Uniref90 (Suzek et al., 2015) and BFD30 (Steinegger & SÃ¶ding, 2018) databases""",200000000000.0,"200B from Table 9 https://www.biorxiv.org/content/10.1101/2023.07.05.547496v1

from Table 1 of the original paper: 
batch size 500k
total steps: 400,000

400,000*500000 / 200000000000 = 1 epoch",,,Google TPU v3,,Confident,"Attention-based models trained on protein sequences have demonstrated incredible success at classification and generation tasks relevant for artificial intelligence- driven protein design. However, we lack a sufficient understanding of how very large-scale models and data play a role in effective protein model development. We introduce a suite of protein language models, named ProGen2, that are scaled up to 6.4B parameters and trained on different sequence datasets drawn from over a billion proteins from genomic, metagenomic, and immune repertoire databases. ProGen2 models show state-of-the-art performance in capturing the distribution of observed evolutionary sequences, generating novel viable sequences, and predicting protein fitness without additional finetuning. As large model sizes and raw numbers of protein sequences continue to become more widely accessible, our results suggest that a growing emphasis needs to be placed on the data distribution provided to a protein sequence model. We release the ProGen2 models and code at https://github.com/salesforce/progen.",1.0,,Open weights (unrestricted),"United States of America,United States of America,United States of America",,,,,,2025-05-01 10:42,,,,,,"Industry,Academia,Academia",,,,,,"Our code and models are BSD-3 licensed
https://github.com/salesforce/progen","Industry,Academia,Academia",,,,,"Operation counting,Third-party estimation",,,,
CodeWhisperer,Language,Code generation,Amazon,,2022-06-24,"Introducing Amazon CodeWhisperer, the ML-powered coding companion",https://aws.amazon.com/blogs/machine-learning/introducing-amazon-codewhisperer-the-ml-powered-coding-companion/,,,Possibly widely used; deployed on AWS,,,,,Unspecified unreleased,,,,,,,,Unknown,"We are excited to announce Amazon CodeWhisperer, a machine learning (ML)-powered service that helps improve developer productivity by providing code recommendations based on developersâ€™ natural comments and prior code. With CodeWhisperer, developers can simply write a comment that outlines a specific task in plain English, such as â€œupload a file to S3.â€ Based on this, CodeWhisperer automatically determines which cloud services and public libraries are best suited for the specified task, builds the specific code on the fly, and recommends the generated code snippets directly in the IDE.",,,Hosted access (no API),United States of America,,,,,,2024-09-05 14:08,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
YaLM,Language,"Language modeling,Chat",Yandex,"Mikhail Khrushchev, Ruslan Vasilev, Alexey Petrov, Nikolay Zinov",2022-06-23,Yandex Publishes YaLM 100B. Itâ€™s the Largest GPT-Like Neural Network in Open Source,https://medium.com/yandex/yandex-publishes-yalm-100b-its-the-largest-gpt-like-neural-network-in-open-source-d1df53d0e9a6,,,,100000000000.0,100B,2.2e+23,"""It took us 65 days to train the model on a pool of 800 A100 graphics cards and 1.7 TB of online texts, books, and countless other sources.""","The Pile,YaLM Russian Dataset","""25% The Pile â€” open English dataset by Eleuther AI team

75% Texts in Russian collected by our team (percentages of the whole dataset are given)""

https://github.com/yandex/YaLM-100B?tab=readme-ov-file",300000000000.0,"1.7TB of data 300B tokens â€“ from github https://github.com/yandex/YaLM-100B
I've assumed that 1 token correspond to 1 word in russian language.",1560.0,65 days,NVIDIA A100,Self-supervised learning,Likely,,,,Open weights (unrestricted),Russia,,,,800.0,,2025-05-16 10:30,,,,,,Industry,checked,,,1248000.0,Unreleased,"Apache 2.0 for weights.

training details, but no code: https://medium.com/yandex/yandex-publishes-yalm-100b-its-the-largest-gpt-like-neural-network-in-open-source-d1df53d0e9a6 ",Industry,,,,641997.2949584174,Hardware,,,,
Parti,Image generation,"Text-to-image,Image generation",Google Research,"Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, Yonghui Wu",2022-06-22,Scaling Autoregressive Models for Content-Rich Text-to-Image Generation,https://arxiv.org/abs/2206.10789v1,880.0,SOTA improvement,"""Second, we achieve consistent quality improvements by scaling the encoder-decoder Transformer model up to 20B parameters, with a new state-of-the-art zero-shot FID score of 7.23 and finetuned FID score of 3.22 on MS-COCO""",20000000000.0,"Abstract: ""we achieve consistent quality improvements
by scaling the encoder-decoder Transformer model up to 20B parameters""",5.09607936e+23,"Calculated from architecture. Does not take into account the encoding and decoding of text and images, only the transformer stack.

Table 1 shows for the 20B model
16 encoder layers
64 decoder layers
Dmodel = 4096
Dhidden = 16384
Num heads = 64

Just below table 1:
""We use a maximum length of text tokens of 128, and the length of image tokens are fixed to 1024""

I take the length of the sequence to be 100 for the encoder stack and 1024 for the decoder stack.

Section 3, Training: ""a total
of 450,000 steps and final ratio of 0.025. We use a global batch size of 8192 during training.""

6* 20B parameters * (1024+128) sequence length*450000 steps*8192 batch size= 5.096079e+23","LAION-400M,FIT400M,JFT-4B",,4800000000.0,,,,Google TPU v4,Self-supervised learning,Confident,"We present the Pathways Autoregressive Text-to-Image (Parti) model, which generates high-fidelity photorealistic images and supports content-rich synthesis involving complex compositions and world knowledge. Parti treats text-to-image generation as a sequence-to-sequence modeling problem, akin to machine translation, with sequences of image tokens as the target outputs rather than text tokens in another language. This strategy can naturally tap into the rich body of prior work on large language models, which have seen continued advances in capabilities and performance through scaling data and model sizes. Our approach is simple: First, Parti uses a Transformer-based image tokenizer, ViT-VQGAN, to encode images as sequences of discrete tokens. Second, we achieve consistent quality improvements by scaling the encoder-decoder Transformer model up to 20B parameters, with a new state-of-the-art zero-shot FID score of 7.23 and finetuned FID score of 3.22 on MS-COCO. Our detailed analysis on Localized Narratives as well as PartiPrompts (P2), a new holistic benchmark of over 1600 English prompts, demonstrate the effectiveness of Parti across a wide variety of categories and difficulty aspects. We also explore and highlight limitations of our models in order to define and exemplify key areas of focus for further improvements. See https://parti.research.google/ for high-resolution images.",,,Unreleased,"Multinational,United States of America,Canada,Switzerland",,,,,,2025-06-18 13:04,,,,,,Industry,checked,,,,Unreleased,"""For these reasons, we have decided not to release our Parti models, code, or data for public use without further safeguards in place""
https://sites.research.google/parti/",Industry,$344852.95,,BF16,,Operation counting,,,,6
CodeGeeX,Language,Code generation,"Zhipu AI,Tsinghua University",,2022-06-22,,https://github.com/THUDM/CodeGeeX,,,,13000000000.0,"""We introduce CodeGeeX, a large-scale multilingual code generation model with 13 billion parameters""",6.630000000001e+22,"Assume 1 epoch on 850B tokens.
C=6DN=6*850B*13B
https://www.wolframalpha.com/input?i=6+*+13+billion+*+850+billion",The Pile,"Mix of open and scraped data:

""Our training data contains two parts. The first part is from open-sourced code datasets, The Pile and CodeParrot. The Pile contains a subset of code corpus that collects public repositories with more than 100 stars from GitHub, from which we select codes in 23 popular programming languages. The second part is supplementary data directly scrapped from the public GitHub repositories that do not appear in previous datasets, including Python, Java and C++.""",850000000000.0,"As of June 22, 2022, CodeGeeX has been trained on more than 850 billion tokens",156.1,"Assume 30% utilization on 1536 Ascend 910 calculating in FP16.
https://www.wolframalpha.com/input?i=6+*+13+billion+*+850+billion+FLOP+%2F+%280.30*1536*256+TFLOPS%29
If they used INT8 precision, the training time would be half of this.",Huawei Ascend 910,Self-supervised learning,Confident,"We introduce CodeGeeX, a large-scale multilingual code generation model with 13 billion parameters, pre-trained on a large code corpus of more than 20 programming languages. As of June 22, 2022, CodeGeeX has been trained on more than 850 billion tokens on a cluster of 1,536 Ascend 910 AI Processors.",,,Open weights (restricted use),"China,China",,,,,,2025-05-09 11:32,,,,6291456.0,Table 3. 2048 * 3072,"Industry,Academia",checked,,,,Open source,"Training code is Apache 2.0. pretrain code here:
https://github.com/THUDM/CodeGeeX/blob/main/scripts/pretrain_codegeex.sh 
Model has a restricted license: https://github.com/THUDM/CodeGeeX/blob/main/MODEL_LICENSE

data partially open (The Pile)","Industry,Academia",,,,,Operation counting,,,,
Unified-IO (XL),"Multimodal,Vision,Language","Object detection,Language modeling/generation,Image generation,Visual question answering,Image classification,Image captioning,Text classification,Text summarization,Question answering","Allen Institute for AI,University of Washington","Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, Aniruddha Kembhavi",2022-06-17,"Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks",https://arxiv.org/abs/2206.08916,324.0,,"Milestone in generality: ""Unified-IO is the first model capable of performing all 7 tasks on the GRIT benchmark and produces strong results across 16 diverse benchmarks like NYUv2-Depth, ImageNet, VQA2.0, OK-VQA, Swig, VizWizGround, BoolQ, and SciTail, with no task-specific fine-tuning""",2925000000.0,"2952M, Table 2",3.5e+21,"1M steps, batch size 1024. Sequence length may be 128-256:
""We use a maximum of 256 and 128 text tokens for inputs and outputs respectively, and a maximum
length of 576 (i.e. 24 Ã— 24 patch encoding from a 384 Ã— 384 image) for image inputs and 256 (i.e.
16 Ã— 16 latent codes from a 256 Ã— 256 image) for image outputs

6 * 1 million * 1024 * 128 * 2.9 billion = 2.3e21
6 * 1 million * 1024 * 256 * 2.9 billion = 4.6e21
average is 3.5e21

No hardware details.","Wikipedia,C4,ImageNet21k,Conceptual Captions 12M (CC12M),Conceptual Captions (CC3M),COCO","""we gather 95 vision, language, and multi-modal datasets from 62 publicly available data sources as targets for our model to learn during multi-task training""

Number of epochs= 1,000,000Ã—1024 / 130,000,000 â‰ˆ 7.88",,,,,Google TPU v4,,Speculative,"We propose UNIFIED-IO, a model that performs a large variety of AI tasks spanning classical computer vision tasks, including pose estimation, object detection, depth estimation and image generation, vision-and-language tasks such as region captioning and referring expression, to natural language processing tasks such as question answering and paraphrasing. Developing a single unified model for such a large variety of tasks poses unique challenges due to the heterogeneous inputs and outputs pertaining to each task, including RGB images, per-pixel maps, binary masks, bounding boxes, and language. We achieve this unification by homogenizing every supported input and output into a sequence of discrete vocabulary tokens. This common representation across all tasks allows us to train a single transformer-based architecture, jointly on over 90 diverse datasets in the vision and language fields. UNIFIED-IO is the first model capable of performing all 7 tasks on the GRIT benchmark and produces strong results across 16 diverse benchmarks like NYUv2-Depth, ImageNet, VQA2.0, OK-VQA, Swig, VizWizGround, BoolQ, and SciTail, with no task-specific fine-tuning. Code and demos for UNIFIED-IO are available at: unified-io.allenai.org",7.88,,Open weights (unrestricted),"United States of America,United States of America",,,,,,2025-05-01 10:42,,,,,,"Research collective,Academia",,,,,Unreleased,"Apache 2.0, looks like model and inference code: https://github.com/allenai/unified-io-inference","Research collective,Academia",,,,,Operation counting,,,,
CoCa,Vision,"Image classification,Visual question answering,Image captioning",Google Research,"Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, Yonghui Wu",2022-06-14,CoCa: Contrastive Captioners are Image-Text Foundation Models,https://arxiv.org/abs/2205.01917v2,1056.0,SOTA improvement,"""Notably on ImageNet classification, CoCa obtains 86.3% zero-shot top-1 accuracy, 90.6% with a frozen encoder and learned classification head, and new state-of-the-art 91.0% top-1 accuracy on ImageNet with a finetuned encoder.""",2100000000.0,"""Our largest CoCa model (""CoCa"" in short) follows the ViT-giant setup in [21] with 1B-parameters in the image encoder and 2.1B-parameters altogether with the text decoder""",7.3e+22,"""Pretraining CoCa takes about 5 days on 2,048 CloudTPUv4 chips""

275 teraFLOP/s * 2048 * 5 * 24 * 3600 * 0.3 (assumed utilization) = 7.3e22","JFT-3B,ALIGN","""CoCa is pretrained from scratch in a single stage on both webscale alt-text data and annotated images by treating all labels simply as texts. We use the JFT-3B dataset [21] with label names as the paired texts, and the ALIGN dataset [13] with noisy alt-texts.""",4800000000.0,"JFT is 3 billion captioned images, ALIGN is 1.8 billion captioned images

""we use a batch size of 65,536 image-text pairs, where half of each batch comes from JFT and ALIGN, respectively. All models are trained on the
combined contrastive and captioning objectives in Eq.(4) for 500k steps, roughly corresponding to 5 epochs on JFT and 10 epochs on ALIGN.""

(5*3b+10*1.8b)/4.8b=6.875 epochs on average",120.0,5 days,Google TPU v4,Self-supervised learning,Confident,"Exploring large-scale pretrained foundation models is of significant interest in computer vision because these models can be quickly transferred to many downstream tasks. This paper presents Contrastive Captioner (CoCa), a minimalist design to pretrain an image-text encoder-decoder foundation model jointly with contrastive loss and captioning loss, thereby subsuming model capabilities from contrastive approaches like CLIP and generative methods like SimVLM. In contrast to standard encoder-decoder transformers where all decoder layers attend to encoder outputs, CoCa omits cross-attention in the first half of decoder layers to encode unimodal text representations, and cascades the remaining decoder layers which cross-attend to the image encoder for multimodal image-text representations. We apply a contrastive loss between unimodal image and text embeddings, in addition to a captioning loss on the multimodal decoder outputs which predicts text tokens autoregressively. By sharing the same computational graph, the two training objectives are computed efficiently with minimal overhead. CoCa is pretrained end-to-end and from scratch on both web-scale alt-text data and annotated images by treating all labels simply as text, seamlessly unifying natural language supervision for representation learning. Empirically, CoCa achieves state-of-the-art performance with zero-shot transfer or minimal task-specific adaptation on a broad range of downstream tasks, spanning visual recognition (ImageNet, Kinetics-400/600/700, Moments-in-Time), crossmodal retrieval (MSCOCO, Flickr30K, MSR-VTT), multimodal understanding (VQA, SNLI-VE, NLVR2), and image captioning (MSCOCO, NoCaps). Notably on ImageNet classification, CoCa obtains 86.3% zero-shot top-1 accuracy, 90.6% with a frozen encoder and learned classification head, and new state-of-the-art 91.0% top-1 accuracy on ImageNet with a finetuned encoder.",6.88,,Unreleased,"Multinational,United States of America,Canada,Switzerland",,,,2048.0,,2025-05-09 11:32,,,,,"65,536 image-text pairs",Industry,,,,,Unreleased,,Industry,$78043.38,,,698633.0659558588,Hardware,,,,
MetaLM,"Multimodal,Language,Vision","Language modeling,Visual question answering,Language modeling/generation",Microsoft Research,"Yaru Hao, Haoyu Song, Li Dong, Shaohan Huang, Zewen Chi, Wenhui Wang, Shuming Ma, Furu Wei",2022-06-13,Language Models are General-Purpose Interfaces,https://arxiv.org/abs/2206.06336v1,90.0,SOTA improvement,"Abstract: ""Experimental results across various language-only and vision-language benchmarks show that our model outperforms or is competitive with specialized models on finetuning, zero-shot generalization, and few-shot learning.""",,,,,The Pile,,,,,,,Self-supervised learning,Unknown,"Foundation models have received much attention due to their effectiveness across a broad range of downstream applications. Though there is a big convergence in terms of architecture, most pretrained models are typically still developed for specific tasks or modalities. In this work, we propose to use language models as a general-purpose interface to various foundation models. A collection of pretrained encoders perceive diverse modalities (such as vision, and language), and they dock with a language model that plays the role of a universal task layer. We propose a semi-causal language modeling objective to jointly pretrain the interface and the modular encoders. We subsume the advantages and capabilities from both causal and non-causal modeling, thereby combining the best of two worlds. Specifically, the proposed method not only inherits the capabilities of in-context learning and open-ended generation from causal language modeling, but also is conducive to finetuning because of the bidirectional encoders. More importantly, our approach seamlessly unlocks the combinations of the above capabilities, e.g., enabling in-context learning or instruction following with finetuned encoders. Experimental results across various language-only and vision-language benchmarks show that our model outperforms or is competitive with specialized models on finetuning, zero-shot generalization, and few-shot learning.",,,Unreleased,"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational",,,,,,2025-06-01 16:17,,,,,,Industry,,,,,Unreleased,"I don't see neither code nor weights here
https://github.com/microsoft/unilm/tree/master/metalm",Industry,,,,,,,,,
EGRU (WT2),Language,Language modeling,"Ruhr University Bochum,Technische UniversitÃ¤t Dresden,University of London","Anand Subramoney, Khaleelulla Khan Nazeer, Mark SchÃ¶ne, Christian Mayr, David Kappel",2022-06-13,Efficient recurrent architectures through activity sparsity and sparse back-propagation through time,https://arxiv.org/abs/2206.06178v3,13.0,,SOTA for recurrent networks but not in general,74000000.0,"74M
Table 3
",2.22e+18,6 FLOP / token / parameter * 74000000 parameters * 2000000 tokens * 2500 epochs = 2.22 Ã— 10^18 FLOP,WikiText-2,,2000000.0,"""All EGRU models were trained for 2500 epochs""

size of WT2: 2M tokens

Table S6
batch size 128
seq length 67",,,NVIDIA A100,,Confident,"Recurrent neural networks (RNNs) are well suited for solving sequence tasks in resource-constrained systems due to their expressivity and low computational requirements. However, there is still a need to bridge the gap between what RNNs are capable of in terms of efficiency and performance and real-world application requirements. The memory and computational requirements arising from propagating the activations of all the neurons at every time step to every connected neuron, together with the sequential dependence of activations, contribute to the inefficiency of training and using RNNs. We propose a solution inspired by biological neuron dynamics that makes the communication between RNN units sparse and discrete. This makes the backward pass with backpropagation through time (BPTT) computationally sparse and efficient as well. We base our model on the gated recurrent unit (GRU), extending it with units that emit discrete events for communication triggered by a threshold so that no information is communicated to other units in the absence of events. We show theoretically that the communication between units, and hence the computation required for both the forward and backward passes, scales with the number of events in the network. Our model achieves efficiency without compromising task performance, demonstrating competitive performance compared to state-of-the-art recurrent network models in real-world tasks, including language modeling. The dynamic activity sparsity mechanism also makes our model well suited for novel energy-efficient neuromorphic hardware. Code is available at this https URL.",2500.0,EGRU (WT2),Unreleased,"Germany,Germany,United Kingdom of Great Britain and Northern Ireland",,,,,,2025-03-31 11:37,,,,8576.0,128 * 67 = 8576,"Academia,Academia,Academia",,,,,Open source,"Apache 2.0: https://github.com/Efficient-Scalable-Machine-Learning/EvNN
train: https://github.com/Efficient-Scalable-Machine-Learning/EvNN/blob/main/benchmarks/lm/train.py ","Academia,Academia,Academia",,,,,Operation counting,,,,
BIG-G 137B,Language,Language modeling/generation,Google,"Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R. Brown, Adam Santoro, Aditya Gupta, AdriÃ  Garriga-Alonso, Agnieszka Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander W. Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Parrish, Allen Nie, Aman Hussain, Amanda Askell, Amanda Dsouza, Ambrose Slone, Ameet Rahane, Anantharaman S. Iyer, Anders Andreassen, Andrea Madotto, Andrea Santilli, Andreas StuhlmÃ¼ller, Andrew Dai, Andrew La, Andrew Lampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta, Anna Gottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul Menezes, Arun Kirubarajan, Asher Mullokandov, Ashish Sabharwal, Austin Herrick, Avia Efrat, Aykut Erdem, Ayla KarakaÅŸ, B. Ryan Roberts, Bao Sheng Loe, Barret Zoph, BartÅ‚omiej Bojanowski, Batuhan Ã–zyurt, Behnam Hedayatnia, Behnam Neyshabur, Benjamin Inden, Benno Stein, Berk Ekmekci, Bill Yuchen Lin, Blake Howald, Bryan Orinion, Cameron Diao, Cameron Dour, Catherine Stinson, Cedrick Argueta, CÃ©sar Ferri RamÃ­rez, Chandan Singh, Charles Rathkopf, Chenlin Meng, Chitta Baral, Chiyu Wu, Chris Callison-Burch, Chris Waites, Christian Voigt, Christopher D. Manning, Christopher Potts, Cindy Ramirez, Clara E. Rivera, Clemencia Siro, Colin Raffel, Courtney Ashcraft, Cristina Garbacea, Damien Sileo, Dan Garrette, Dan Hendrycks, Dan Kilman, Dan Roth, Daniel Freeman, Daniel Khashabi, Daniel Levy, Daniel MoseguÃ­ GonzÃ¡lez, Danielle Perszyk, Danny Hernandez, Danqi Chen, Daphne Ippolito et al. (351 additional authors not shown)",2022-06-09,Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models,https://arxiv.org/abs/2206.04615,1394.0,,,137000000000.0,"137B. Table App.1
",5.6e+23,"""BIG-G models were trained at Google. We use 13 dense decoder-only Transformer models (Vaswani
et al., 2017) with gated activation layers (Dauphin et al., 2017) and GELU activations based on the LaMDA
architectures (Thoppilan et al., 2022). These models were trained on a dataset consisting of a mixture of web
documents, code, dialog, and Wikipedia data, with approximately three billion documents tokenized to 2.8
trillion BPE tokens using a 32k-token SentencePiece vocabulary""

Appendix:

""We use a pre-training batch size of 262k tokens for all models...""

2.6M steps for 137B, per Table App.1. So trained on 2.6M * 262k = 681B tokens (~0.25 epochs)
681B * 137B * 6 = 5.6e23",GLaM dataset,"""These models were trained on a dataset consisting of a mixture of web
documents, code, dialog, and Wikipedia data, with approximately three billion documents tokenized to 2.8
trillion BPE tokens using a 32k-token SentencePiece vocabulary""",681000000000.0,"Full dataset is comprised of 2.8 trillion tokens, but calculation based on batch size and steps suggests model was trained on only 681 billion tokens.",,,,,Confident,"Language models demonstrate both quantitative improvement and new qualitative capabilities with increasing scale. Despite their potentially transformative impact, these new capabilities are as yet poorly characterized. In order to inform future research, prepare for disruptive new model capabilities, and ameliorate socially harmful effects, it is vital that we understand the present and near-future capabilities and limitations of language models. To address this challenge, we introduce the Beyond the Imitation Game benchmark (BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 450 authors across 132 institutions. Task topics are diverse, drawing problems from linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, and beyond. BIG-bench focuses on tasks that are believed to be beyond the capabilities of current language models. We evaluate the behavior of OpenAI's GPT models, Google-internal dense transformer architectures, and Switch-style sparse transformers on BIG-bench, across model sizes spanning millions to hundreds of billions of parameters. In addition, a team of human expert raters performed all tasks in order to provide a strong baseline. Findings include: model performance and calibration both improve with scale, but are poor in absolute terms (and when compared with rater performance); performance is remarkably similar across model classes, though with benefits from sparsity; tasks that improve gradually and predictably commonly involve a large knowledge or memorization component, whereas tasks that exhibit ""breakthrough"" behavior at a critical scale often involve multiple steps or components, or brittle metrics; social bias typically increases with scale in settings with ambiguous context, but this can be improved with prompting.",1.0,,Unreleased,United States of America,,,,,,2025-05-16 10:30,,,,262000.0,,Industry,,,,,Unreleased,,Industry,,,,,Operation counting,,,,
DITTO,Language,Language modeling/generation,"Tsinghua University,Apple,Westlake University,Chinese University of Hong Kong (CUHK)","Jin Xu, Xiaojiang Liu, Jianhao Yan, Deng Cai, Huayang Li, Jian Li",2022-06-06,Learning to Break the Loop: Analyzing and Mitigating Repetitions for Neural Text Generation,https://arxiv.org/abs/2206.02369,53.0,SOTA improvement,"Achieves SOTA on CNN/DailyMail by fine-tuning and improving on BART-large, which is SOTA",750000000.0,"""We train a Transformer model (750M parameters, similar to GPT-2 Large)""

""Specifically, we use a 16-layer Transformer with 8 attention heads, hidden size 1024 and fully-connected dimension 4096.""",3.31776e+18,6 FLOP / token / parameter * 160000 steps * 3 samples per batch * 1536 tokens per sample *  750000000 parameters = 3.31776 Ã— 10^18 FLOP,WikiText-103,"""We first train the baseline model with standard maximum likelihood (MLE) for a maximum of 150k updates with a batch size of 3 samples. The maximum length of each training sample is 1,536 tokens. Then, we fine-tune the model that has the best validation perplexity with DITTO training for 10k steps.""

160000*3*1536 / 103000000 = 7.158 epochs",103000000.0,,,,NVIDIA V100,,Confident,"While large-scale neural language models, such as GPT2 and BART, have achieved impressive results on various text generation tasks, they tend to get stuck in undesirable sentence-level loops with maximization-based decoding algorithms (\textit{e.g.}, greedy search). This phenomenon is counter-intuitive since there are few consecutive sentence-level repetitions in human corpora (e.g., 0.02\% in Wikitext-103). To investigate the underlying reasons for generating consecutive sentence-level repetitions, we study the relationship between the probabilities of the repetitive tokens and their previous repetitions in the context. Through our quantitative experiments, we find that 1) Language models have a preference to repeat the previous sentence; 2) The sentence-level repetitions have a \textit{self-reinforcement effect}: the more times a sentence is repeated in the context, the higher the probability of continuing to generate that sentence; 3) The sentences with higher initial probabilities usually have a stronger self-reinforcement effect. Motivated by our findings, we propose a simple and effective training method \textbf{DITTO} (Pseu\underline{D}o-Repet\underline{IT}ion Penaliza\underline{T}i\underline{O}n), where the model learns to penalize probabilities of sentence-level repetitions from pseudo repetitive data. Although our method is motivated by mitigating repetitions, experiments show that DITTO not only mitigates the repetition issue without sacrificing perplexity, but also achieves better generation quality. Extensive experiments on open-ended text generation (Wikitext-103) and text summarization (CNN/DailyMail) demonstrate the generality and effectiveness of our method.",7.16,DITTO,Unreleased,"China,United States of America,China,Hong Kong,China",,,,8.0,,2025-05-28 16:09,,,,,,"Academia,Industry,Academia,Academia",,,,,Open source,"open code
https://github.com/Jxu-Thu/DITTO

training: https://github.com/Jxu-Thu/DITTO/blob/main/train.py ","Academia,Industry,Academia,Academia",,,FP16,4816.802908983849,Operation counting,,,,
Diffusion-GAN,Image generation,"Image generation,Text-to-image","UT Austin,Microsoft","Zhendong Wang, Huangjie Zheng, Pengcheng He, Weizhu Chen, Mingyuan Zhou",2022-06-05,Diffusion-GAN: Training GANs with Diffusion,https://arxiv.org/abs/2206.02262v4,164.0,SOTA improvement,"""We demonstrate the advantages of Diffusion-GAN over strong GAN
baselines on various datasets, showing that it can produce more realistic images with higher stability and data efficiency than state-of-the-art GANs.""",,,,"Must be <1e23 FLOP, all experiments were done with 4 or 8 V100s.","CIFAR-10,LSUN Bedroom,AFHQ,LSUN Church,STL-10,FFHQ","They experimented with the following datasets: ""CIFAR-10 (Krizhevsky, 2009), STL-10 (Coates et al., 2011), LSUN-Bedroom (Yu et al., 2015), LSUN-Church
(Yu et al., 2015), AFHQ(Cat/Dog/Wild) (Choi et al., 2020), and FFHQ (Karras et al., 2019)""",,,,,NVIDIA V100,,Unknown,"Generative adversarial networks (GANs) are challenging to train stably, and a promising remedy of injecting instance noise into the discriminator input has not been very effective in practice. In this paper, we propose Diffusion-GAN, a novel GAN framework that leverages a forward diffusion chain to generate Gaussianmixture distributed instance noise. Diffusion-GAN consists of three components, including an adaptive diffusion process, a diffusion timestep-dependent discriminator, and a generator. Both the observed and generated data are diffused by the same adaptive diffusion process. At each diffusion timestep, there is a different noise-to-data ratio and the timestep-dependent discriminator learns to distinguish the diffused real data from the diffused generated data. The generator learns from the discriminatorâ€™s feedback by backpropagating through the forward diffusion chain, whose length is adaptively adjusted to balance the noise and data levels. We theoretically show that the discriminatorâ€™s timestep-dependent strategy gives consistent and helpful guidance to the generator, enabling it to match the true data distribution. We demonstrate the advantages of Diffusion-GAN over strong GAN baselines on various datasets, showing that it can produce more realistic images with higher stability and data efficiency than state-of-the-art GANs.",,,Open weights (unrestricted),"United States of America,United States of America,Multinational,India,Belgium",,,,,,2025-06-02 15:22,,,,,,"Academia,Industry",,,,,Open source,"MIT license
https://github.com/Zhendong-Wang/Diffusion-GAN

https://huggingface.co/zhendongw/diffusion-gan/tree/main","Academia,Industry",,,FP16,,,zhendongw,,,
CRL,Biology,"Protein folding prediction,Protein classification,Protein-ligand binding affinity prediction,Protein structure similarity prediction",Ulm University,"Pedro Hermosilla, Timo Ropinski",2022-05-31,Contrastive Representation Learning for 3D Protein Structures,https://arxiv.org/abs/2205.15675,42.0,,,,,,"""All networks were
trained for 550 K training steps, resulting in 6 days of training""",PDB (Protein Data Bank),,476362.0,"""We collected
476, 362 different protein chains, each composed of at least 25 of amino acids.""",144.0,,,,Confident,"Learning from 3D protein structures has gained wide interest in protein modeling and structural bioinformatics. Unfortunately, the number of available structures is orders of magnitude lower than the training data sizes commonly used in computer vision and machine learning. Moreover, this number is reduced even further, when only annotated protein structures can be considered, making the training of existing models difficult and prone to over-fitting. To address this challenge, we introduce a new representation learning framework for 3D protein structures. Our framework uses unsupervised contrastive learning to learn meaningful representations of protein structures, making use of proteins from the Protein Data Bank. We show, how these representations can be used to solve a large variety of tasks, such as protein function prediction, protein fold classification, structural similarity prediction, and protein-ligand binding affinity prediction. Moreover, we show how fine-tuned networks, pre-trained with our algorithm, lead to significantly improved task performance, achieving new state-of-the-art results in many tasks.",,,,Germany,,,,,,2025-05-01 10:42,,,,,,,,,,,,,,,,,,,,,,
CogVideo,Video,"Video generation,Text-to-video","Tsinghua University,Beijing Academy of Artificial Intelligence / BAAI","Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, Jie Tang",2022-05-29,CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers,https://arxiv.org/abs/2205.15868,356.0,Historical significance,The world's largest and first opensource large-scale pre-trained text-to-video model.,9400000000.0,,,,Unspecified unreleased,,5400000.0,"""trained on 5.4 million text-video pairs""",,,,,Speculative,"Large-scale pretrained transformers have created milestones in text (GPT-3) and text-to-image (DALL-E and CogView) generation. Its application to video generation is still facing many challenges: The potential huge computation cost makes the training from scratch unaffordable; The scarcity and weak relevance of text-video datasets hinder the model understanding complex movement semantics. In this work, we present 9B-parameter transformer CogVideo, trained by inheriting a pretrained text-to-image model, CogView2. We also propose multi-frame-rate hierarchical training strategy to better align text and video clips. As (probably) the first open-source large-scale pretrained text-to-video model, CogVideo outperforms all publicly available models at a large margin in machine and human evaluations.",,,Open weights (unrestricted),"China,China",CogView2,30456000000000000000,"6ND = 6*9400000000*5400000=3.0456e+17

(number of epochs is unknown)",,,2025-05-28 16:09,,,,,,"Academia,Academia",checked,,,,Open source,"https://github.com/THUDM/CogVideo
Apache 2
train code: https://github.com/THUDM/CogVideo/blob/CogVideo/pretrain_cogvideo.py ","Academia,Academia",,,FP16,,Operation counting,,,,
Tranception,Biology,"Proteins,Protein pathogenicity prediction","University of Oxford,Harvard Medical School,Cohere","Pascal Notin, Mafalda Dias, Jonathan Frazer, Javier Marchena-Hurtado, Aidan Gomez, Debora S. Marks, Yarin Gal",2022-05-27,Tranception: protein fitness prediction with autoregressive transformers and inference-time retrieval,https://arxiv.org/abs/2205.13760,139.0,SOTA improvement,"""We introduce Tranception, a novel transformer architecture leveraging autoregressive predictions and retrieval of homologous sequences at inference to achieve state-of-the-art fitness prediction performance. Given its markedly higher performance on multiple mutants, robustness to shallow alignments and ability to score indels, our approach offers significant gain of scope over existing approaches.""",700000000.0,"""Our largest transformer model, Tranception L, has 700M parameters and is trained on UniRef100 (Suzek et al., 2014)""",7.240000000000001e+21,"Trained using 64 A100 GPUs for two weeks.
64 * 312 teraFLOP/s * 14 days * 24 hours/day * 3600 seconds/hour * 0.3 utilization (assumption)
= 7.24e21",UniRef100,"""We therefore train our final model (700M parameters) on UniRef100""",75000000001.0,"Total tokens = Number of Sequences Ã— Average Sequence Length
249,000,000 Ã— 300 = 74,700,000,000 â‰ˆ 7.5 Ã— 10Â¹â° tokens",336.0,2 weeks,NVIDIA A100,Self-supervised learning,Confident,"The ability to accurately model the fitness landscape of protein sequences is critical to a wide range of applications, from quantifying the effects of human variants on disease likelihood, to predicting immune-escape mutations in viruses and designing novel biotherapeutic proteins. Deep generative models of protein sequences trained on multiple sequence alignments have been the most successful approaches so far to address these tasks. The performance of these methods is however contingent on the availability of sufficiently deep and diverse alignments for reliable training. Their potential scope is thus limited by the fact many protein families are hard, if not impossible, to align. Large language models trained on massive quantities of non-aligned protein sequences from diverse families address these problems and show potential to eventually bridge the performance gap. We introduce Tranception, a novel transformer architecture leveraging autoregressive predictions and retrieval of homologous sequences at inference to achieve state-of-the-art fitness prediction performance. Given its markedly higher performance on multiple mutants, robustness to shallow alignments and ability to score indels, our approach offers significant gain of scope over existing approaches. To enable more rigorous model testing across a broader range of protein families, we develop ProteinGym -- an extensive set of multiplexed assays of variant effects, substantially increasing both the number and diversity of assays compared to existing benchmarks.",,,Open weights (unrestricted),"United Kingdom of Great Britain and Northern Ireland,United States of America,Canada",,,,64.0,,2025-06-06 11:26,,,,,,"Academia,Academia,Industry",,,,21504.0,Open source,"MIT license

https://github.com/OATML-Markslab/Tranception

https://huggingface.co/OATML-Markslab/Tranception_Large","Academia,Academia,Industry",$15247.44,,,51390.67413498394,Hardware,OATML-Markslab,,,
TRIMELMext (247M),Language,"Language modeling/generation,Translation",Princeton University,"Zexuan Zhong, Tao Lei, Danqi Chen",2022-05-25,Training Language Models with Memory Augmentation,https://arxiv.org/abs/2205.12674,115.0,,,247000000.0,,3.12e+19,"Table 9: 
73728 tokens per update * 286000 updates =21086208000 tokens

Using 6ND formula:
6 * 247000000 * 21086208000 = 3.124976e+19",WikiText-103,,103000000.0,,,,,,Confident,"Recent work has improved language models (LMs) remarkably by equipping them with a non-parametric memory component. However, most existing approaches only introduce mem-ories at testing time or represent them using a separately trained encoder, resulting in suboptimal training of the language model. In this work, we present TRIME, a novel yet simple training approach designed for training LMs with memory augmentation. Our approach uses a training objective that directly takes in-batch examples as accessible memory. We also present new methods for memory construction and data batching, which are used for adapting to different sets of memories--local, long-term, and external memory--at testing time. We evaluate TRIME on multiple language modeling and machine translation benchmarks and show that it is able to achieve significant improvements across all the settings. Concretely, TRIME reduces the perplexity from 18.70 to 15.37 on WIKITEXT-103, by effectively leveraging a large memory set from the training corpus. Compared to standard LM training, TRIME adds negligible computational overhead and is compatible with different neural architectures, making it a versatile solution for training memory-augmented LMs.",204.72,TRIMELMext (247M),Open weights (non-commercial),United States of America,,,,,,2025-03-31 10:23,,,,,,Academia,,,,,Open (non-commercial),"code and weights, no clear license: https://github.com/princeton-nlp/TRIME?tab=readme-ov-file ",Academia,,,,,,,,,
Imagen,Image generation,"Text-to-image,Image generation",Google Brain,"Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J Fleet, Mohammad Norouzi",2022-05-23,Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding,https://arxiv.org/abs/2205.11487,4580.0,"Significant use,SOTA improvement,Highly cited",,7762000000.0,"2B 64x64 generation model, 600M 64->256 super-resolution model, 400M 256->1024 super-resolution model
Uses encodings from a frozen T5-XXL, which should be included in total parameter count. Loading the model directly, there are 4,762,310,656 parameters in the encoder.
2B + 4.762B + 600M + 400M = 7.762 billion

here they claim it is 3B parameters: https://arxiv.org/pdf/2407.15811",1.4600000000000002e+22,"256 TPU v4 chips for 64x64, for 4 days
128 TPU v4 chips for 64->256, for 2 days
128 TPU v4 chips for 256->1024, for 2 days

256 TPUs * 275 teraFLOPS/TPU * 4 days + 2 * (128 TPUs * 275 teraFLOPS/TPU * 2 days) * 40% utilization = 1.46e+22 FLOP","LAION-400M,Unspecified unreleased",,860000000.0,"[IMAGE-TEXT PAIRS]
""We train on a combination of internal datasets, with â‰ˆ 460M
image-text pairs, and the publicly available Laion dataset [61], with â‰ˆ 400M image-text pairs.""",96.0,4 days,Google TPU v4,Self-supervised learning,Likely,"We present Imagen, a text-to-image diffusion model with an unprecedented degree of photorealism and a deep level of language understanding. Imagen builds on the power of large transformer language models in understanding text and hinges on the strength of diffusion models in high-fidelity image generation. Our key discovery is that generic large language models (e.g. T5), pretrained on text-only corpora, are surprisingly effective at encoding text for image synthesis: increasing the size of the language model in Imagen boosts both sample fidelity and image-text alignment much more than increasing the size of the image diffusion model. Imagen achieves a new state-of-the-art FID score of 7.27 on the COCO dataset, without ever training on COCO, and human raters find Imagen samples to be on par with the COCO data itself in image-text alignment. To assess text-to-image models in greater depth, we introduce DrawBench, a comprehensive and challenging benchmark for text-to-image models. With DrawBench, we compare Imagen with recent methods including VQ-GAN+CLIP, Latent Diffusion Models, and DALL-E 2, and find that human raters prefer Imagen over other models in side-by-side comparisons, both in terms of sample quality and image-text alignment.",,,API access,United States of America,,,,256.0,,2025-05-14 15:32,,,,,,Industry,checked,,,24576.0,Unreleased,,Industry,$7915.82,,,87371.9285545078,Hardware,,,,
improved U-Net for chest X-ray images segmentation,"Medicine,Vision","Visual question answering,Medical diagnosis","Henan University of Technology,Nanyang Central Hospital","Wufeng Liu, Jiaxin Luo, Yan Yang, Wenlian Wang, Junkui Deng & Liang Yu ",2022-05-23,Automatic lung segmentation in chest X-ray images using improved U-Net,https://www.nature.com/articles/s41598-022-12743-y,,,,,,,,"NIH Chest X-ray Dataset,JSRT Dataset,Montgomery County (MC) Dataset","NIH Chest X-ray Dataset (sample of 2,785 manually labeled images)

JSRT Dataset (247 CXR images)

Montgomery County (MC) Dataset (138 CXR images)",,"""The batch size is set to 64. Max epochs are set to 70.""

""image size was adjusted to 256 * 256""",,,"NVIDIA GeForce RTX 3060,Intel Core i5-11600K",,Unknown,"The automatic segmentation of the lung region for chest X-ray (CXR) can help doctors diagnose many lung diseases. However, extreme lung shape changes and fuzzy lung regions caused by serious lung diseases may incorrectly make the automatic lung segmentation model. We improved the U-Net network by using the pre-training Efficientnet-b4 as the encoder and the Residual block and the LeakyReLU activation function in the decoder. The network can extract Lung field features efficiently and avoid the gradient instability caused by the multiplication effect in gradient backpropagation. Compared with the traditional U-Net model, our method improves about 2.5% dice coefficient and 6% Jaccard Index for the two benchmark lung segmentation datasets. Our model improves about 5% dice coefficient and 9% Jaccard Index for the private lung segmentation datasets compared with the traditional U-Net model. Comparative experiments show that our method can improve the accuracy of lung segmentation of CXR images and it has a lower standard deviation and good robustness.",70.0,,,"China,China",,,,,,2025-05-01 10:42,,,,,,"Academia,Government",,,,,,,"Academia,Government",,,,,,,,,
LSTM+GraB,Language,Language modeling/generation,Cornell University,"Yucheng Lu, Wentao Guo, Christopher De Sa",2022-05-22,GraB: Finding Provably Better Data Permutations than Random Reshuffling,https://arxiv.org/abs/2205.10733,14.0,,,,,,Definitely <1e23 FLOP: all experiments used an NVIDIA GeForce RTX 2080 Ti GPU.,WikiText-2,""" (3) training LSTM on wikitext-2 [Merity et al., 2017]""
",,,,,,,Unknown,"Random reshuffling, which randomly permutes the dataset each epoch, is widely adopted in model training because it yields faster convergence than with-replacement sampling. Recent studies indicate greedily chosen data orderings can further speed up convergence empirically, at the cost of using more computation and memory. However, greedy ordering lacks theoretical justification and has limited utility due to its non-trivial memory and computation overhead. In this paper, we first formulate an example-ordering framework named herding and answer affirmatively that SGD with herding converges at the rate O(Tâˆ’2/3) on smooth, non-convex objectives, faster than the O(n1/3Tâˆ’2/3) obtained by random reshuffling, where n denotes the number of data points and T denotes the total number of iterations. To reduce the memory overhead, we leverage discrepancy minimization theory to propose an online Gradient Balancing algorithm (GraB) that enjoys the same rate as herding, while reducing the memory usage from O(nd) to just O(d) and computation from O(n2) to O(n), where d denotes the model dimension. We show empirically on applications including MNIST, CIFAR10, WikiText and GLUE that GraB can outperform random reshuffling in terms of both training and validation performance, and even outperform state-of-the-art greedy ordering while reducing memory usage over 100Ã—.",50.0,LSTM+GraB,Unreleased,United States of America,,,,,,2024-09-05 14:08,,,,,,Academia,,,,,Open source,MIT for code. LSTM script here: https://github.com/EugeneLYC/GraB/blob/main/neurips22/examples/nlp/word_language_model/lstm_wiki.sh,Academia,,,,,,,,,
SimCSE,Language,Semantic embedding,"Princeton University,Tsinghua University","Tianyu Gao, Xingcheng Yao, Danqi Chen",2022-05-18,SimCSE: Simple Contrastive Learning of Sentence Embeddings,https://arxiv.org/abs/2104.08821,2795.0,"Highly cited,SOTA improvement",,,,,,,"""Training details. We start from pre-trained checkpoints of BERT (Devlin et al., 2019) (uncased) or RoBERTa (Liu et al., 2019) (cased) and take the [CLS] representation as the sentence embedding (see Â§6.3 for comparison between different pooling methods). We train unsupervised SimCSE on 106 randomly sampled sentences from English Wikipedia, and train supervised SimCSE on the combination of MNLI and SNLI datasets (314k). More training details can be found in Appendix A""",,,,,,,Unknown,"This paper presents SimCSE, a simple contrastive learning framework that greatly advances state-of-the-art sentence embeddings. We first describe an unsupervised approach, which takes an input sentence and predicts itself in a contrastive objective, with only standard dropout used as noise. This simple method works surprisingly well, performing on par with previous supervised counterparts. We find that dropout acts as minimal data augmentation, and removing it leads to a representation collapse. Then, we propose a supervised approach, which incorporates annotated pairs from natural language inference datasets into our contrastive learning framework by using ""entailment"" pairs as positives and ""contradiction"" pairs as hard negatives. We evaluate SimCSE on standard semantic textual similarity (STS) tasks, and our unsupervised and supervised models using BERT base achieve an average of 76.3% and 81.6% Spearman's correlation respectively, a 4.2% and 2.2% improvement compared to the previous best results. We also show -- both theoretically and empirically -- that the contrastive learning objective regularizes pre-trained embeddings' anisotropic space to be more uniform, and it better aligns positive pairs when supervised signals are available.",3.0,,Open weights (unrestricted),"United States of America,China",RoBERTa Large,,,,,2025-06-02 11:06,,,,,,"Academia,Academia",,,,,Open source,"Unclear lisense: https://huggingface.co/princeton-nlp/sup-simcse-roberta-large

MIT license: 
https://github.com/princeton-nlp/SimCSE?tab=readme-ov-file","Academia,Academia",,,FP16,,,princeton-nlp,,,
Gato,"Multimodal,Robotics,Games,Language","Atari,Image captioning,Chat,Robotic manipulation",DeepMind,"Scott Reed, Konrad Å»oÅ‚na, Emilio Parisotto, Sergio GÃ³mez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai GimÃ©nez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom Eccles, Jake Bruce, Ali Razavi, Ashley Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell, Oriol Vinyals, Mahyar Bordbar, Nando de Freitas",2022-05-12,A Generalist Agent,https://arxiv.org/abs/2205.06175,668.0,SOTA improvement,"SOTA at Meta-World MT50 tasks (96.6%) page 14, section 5.5",1180000000.0,"""This section focuses on in-simulation evaluation.
Figure 10 compares the full 1.18B parameter Gato"" p.10",4.02e+21,"256 (16x16x) TPUv3 chips x 123e12 FLOPS/chip x 4 days x 86400 seconds/day * 0.4 utilization = 4.35e21 FLOPs

Similar value by 6NC:
6 * 524288000000 * 1.18B = 3.71e21

Using geometric mean:
sqrt(4.35e21 * 3.71e21) = 4.02e21",,"Table 1 lists the datasets used. Note that actual training appears to have used fewer than the 1.5T control tokens + unspecified number of vision/language tokens: ""Training of the model is performed on a 16x16 TPU v3 slice for 1M steps with batch size 512 and token sequence length L = 1024, which takes about 4 days.""
1M * 512 * 1024 = 524,288,000,000 tokens in training",524288000000.0,,96.0,4 days,Google TPU v3,,Confident,"Inspired by progress in large-scale language modeling, we apply a similar approach towards building a single generalist agent beyond the realm of text outputs. The agent, which we refer to as Gato, works as a multi-modal, multi-task, multi-embodiment generalist policy. The same network with the same weights can play Atari, caption images, chat, stack blocks with a real robot arm and much more, deciding based on its context whether to output text, joint torques, button presses, or other tokens. In this report we describe the model and the data, and document the current capabilities of Gato.",,,Unreleased,United Kingdom of Great Britain and Northern Ireland,,,,256.0,,2025-06-18 13:00,,,,,,Industry,checked,,,,Unreleased,,Industry,$3523.06,,,113097.25585951032,"Hardware,Operation counting",,,,
UL2,Language,Language modeling/generation,"Google Research,Google Brain","Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Neil Houlsby, Donald Metzler",2022-05-10,Unifying Language Learning Paradigms,https://arxiv.org/abs/2205.05131v1,253.0,SOTA improvement,"""by scaling our model up to 20B parameters, we achieve SOTA
performance on 50 well-established supervised NLP tasks""",20000000000.0,Taken from Directory of LLMs,1.2e+23,"Trained on 1T tokens
20B * 1T * 6 = 1.2e23 

Second source: Section 5.1 says model was trained on 512 TPUv4 chips, and took slightly over 1 month
512 * 2.75e14 * 31 * 24 * 3600 * 0.3 = 1.13e23",C4,'The model is trained on a total of 1 trillion tokens on C4 (2 million steps).',1000000000000.0,1T tokens,744.0,"around 31 days from 'Pre-training took approximately slight more than one month for about 1 trillion
tokens.' from section 5.1
so around 31*24 = 744
",Google TPU v4,,Confident,"Existing pre-trained models are generally geared towards a particular class of problems. To date, there seems to be still no consensus on what the right architecture and pre-training setup should be. This paper presents a unified framework for pre-training models that are universally effective across datasets and setups. We begin by disentangling architectural archetypes with pre-training objectives -- two concepts that are commonly conflated. Next, we present a generalized and unified perspective for self-supervision in NLP and show how different pre-training objectives can be cast as one another and how interpolating between different objectives can be effective. We then propose Mixture-of-Denoisers (MoD), a pre-training objective that combines diverse pre-training paradigms together. We furthermore introduce a notion of mode switching, wherein downstream fine-tuning is associated with specific pre-training schemes. We conduct extensive ablative experiments to compare multiple pre-training objectives and find that our method pushes the Pareto-frontier by outperforming T5 and/or GPT-like models across multiple diverse setups. Finally, by scaling our model up to 20B parameters, we achieve SOTA performance on 50 well-established supervised NLP tasks ranging from language generation (with automated and human evaluation), language understanding, text classification, question answering, commonsense reasoning, long text reasoning, structured knowledge grounding and information retrieval. Our model also achieve strong results at in-context learning, outperforming 175B GPT-3 on zero-shot SuperGLUE and tripling the performance of T5-XXL on one-shot summarization. We release Flax-based T5X model checkpoints for the 20B model at \url{this https URL}.",,,Open weights (unrestricted),"Multinational,United States of America,Canada,Switzerland,United States of America",,,,512.0,0.2993,2025-05-28 16:09,,,,65536.0,"""We pre-train all models for 500K steps with a batch size of 128 and a sequence length of 512 inputs and 512 targets using the C4 corpus. The total approximate tokens seen during pre-training is approximately 32 billion tokens.""

500k*128*512 ~= 32B
128*512=65,536","Industry,Industry",checked,,,380928.0,Unreleased,"Apache 2.0

https://huggingface.co/google/ul2","Industry,Industry",$126785.76,"""Pretraining took slightly longer than 1 month"" to train on 1T tokens using 512 TPUv4 chips.
ops-counting method gives 1.20e23 FLOP needed to train, and 512 TPUv4s running at max FLOPs for 1.1 months is:
1.1 * 30 * 24 * 3600 * 512 * 2.75e14 = 4.01e23 FLOP
1.2e23 / 4.01e23 = 0.29925",BF16,174794.45309829456,"Hardware,Operation counting",google,,,18
ASE,Robotics,Animal (human/non-human) imitation,"NVIDIA,University of California (UC) Berkeley","Xue Bin Peng, Yunrong Guo, Lina Halper, Sergey Levine, Sanja Fidler",2022-05-05,ASE: Large-Scale Reusable Adversarial Skill Embeddings for Physically Simulated Characters,https://arxiv.org/abs/2205.01906,56.0,,,,,4.4928e+19,"Training was done using the Isaac Gym simulator on an NVIDIA V100 GPU. The model was trained on over 10 billion samples, which equates to 10 years of simulated experience time. Training took around 10 days on a single GPU.
1.3e14 * 10 * 24 * 3600 * 0.4 = 4.49e19",,"The model is trained on a dataset of 187 motion clips, about 30 minutes of human motion capture data, depicting locomotion and sword wielding motions.",,,240.0,Training took around 10 days on a single GPU.,NVIDIA Tesla V100 PCIe 16 GB,Reinforcement learning,Likely,"The incredible feats of athleticism demonstrated by humans are made possible in part by a vast repertoire of general-purpose motor skills, acquired through years of practice and experience. These skills not only enable humans to perform complex tasks, but also provide powerful priors for guiding their behaviors when learning new tasks. This is in stark contrast to what is common practice in physics-based character animation, where control policies are most typically trained from scratch for each task. In this work, we present a large-scale data-driven framework for learning versatile and reusable skill embeddings for physically simulated characters. Our approach combines techniques from adversarial imitation learning and unsupervised reinforcement learning to develop skill embeddings that produce life-like behaviors, while also providing an easy to control representation for use on new downstream tasks. Our models can be trained using large datasets of unstructured motion clips, without requiring any task-specific annotation or segmentation of the motion data. By leveraging a massively parallel GPU-based simulator, we are able to train skill embeddings using over a decade of simulated experiences, enabling our model to learn a rich and versatile repertoire of skills. We show that a single pre-trained model can be effectively applied to perform a diverse set of new tasks. Our system also allows users to specify tasks through simple reward functions, and the skill embedding then enables the character to automatically synthesize complex and naturalistic strategies in order to achieve the task objectives.",,,,"United States of America,United States of America",,,,,,2025-05-09 11:32,,,,,,"Industry,Academia",,,,,,,"Industry,Academia",,,,,Hardware,,,,
DeBERTaV3large + KEAR,Language,"Question answering,Language modeling/generation",Microsoft,"Yichong Xu, Chenguang Zhu, Shuohang Wang, Siqi Sun, Hao Cheng, Xiaodong Liu, Jianfeng Gao, Pengcheng He, Michael Zeng, Xuedong Huang",2022-05-04,Human Parity on CommonsenseQA: Augmenting Self-Attention with External Attention,https://arxiv.org/abs/2112.03254v3,50.0,SOTA improvement,"""The proposed system, Knowledgeable External Attention for commonsense Reasoning (KEAR), reaches human parity on the open CommonsenseQA research benchmark with an accuracy of 89.4\% in comparison to the human accuracy of 88.9\%.""

SOTA per https://paperswithcode.com/sota/common-sense-reasoning-on-commonsenseqa",418000000.0,"DeBERTaV3-large had 418M params, per Table 2",,this is a fine-tuned version of DeBERTaV3-large,,"""We present details of the 17 datasets that we use for training data retrieval in Table 1. All the datasets are multiple-choice or classification datasets related to commonsense reasoning, and we include dataset details in the appendix.""",,,,,,,Confident,"Most of today's AI systems focus on using self-attention mechanisms and transformer architectures on large amounts of diverse data to achieve impressive performance gains. In this paper, we propose to augment the transformer architecture with an external attention mechanism to bring external knowledge and context to bear. By integrating external information into the prediction process, we hope to reduce the need for ever-larger models and increase the democratization of AI systems. We find that the proposed external attention mechanism can significantly improve the performance of existing AI systems, allowing practitioners to easily customize foundation AI models to many diverse downstream applications. In particular, we focus on the task of Commonsense Reasoning, demonstrating that the proposed external attention mechanism can augment existing transformer models and significantly improve the model's reasoning capabilities. The proposed system, Knowledgeable External Attention for commonsense Reasoning (KEAR), reaches human parity on the open CommonsenseQA research benchmark with an accuracy of 89.4\% in comparison to the human accuracy of 88.9\%.",10.0,,Unreleased,"United States of America,Multinational,India,Belgium",DeBERTaV3large,,,,,2025-05-30 14:11,,,,,,Industry,,,,,Open source,"no attributed license 
https://github.com/microsoft/KEAR?tab=readme-ov-file",Industry,,,,,,,,,
OPT-175B,Language,"Language modeling,Chat,Language modeling/generation,Question answering",Meta AI,"Susan Zhangâˆ— , Stephen Rollerâˆ— , Naman Goyalâˆ— , Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ottâ€  , Sam Shleiferâ€  , Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer",2022-05-02,OPT: Open Pre-trained Transformer Language Models,https://arxiv.org/abs/2205.01068,2932.0,"Significant use,Highly cited",https://ai.meta.com/blog/opt-175b-large-language-model-applications/,175000000000.0,"""In line with Meta AIâ€™s commitment to open science, we are sharing Open Pretrained Transformer (OPT-175B), a language model with 175 billion parameters trained on publicly available data sets""",4.3e+23,"https://github.com/facebookresearch/metaseq/blob/main/projects/OPT/chronicles/final_update.md

""As of yesterday, at 12:46pm PST on January 6, our 175B model finally completed its training run on 300B tokens. This required ~4.30E+23 FLOPs of compute""","The Pile,""BookCorpus (BooksCorpus, Toronto Book Corpus)"",CC-Stories,Pushshift Reddit","""The pre-training corpus contains a concatenation
of datasets used in RoBERTa (Liu et al., 2019b),
the Pile (Gao et al., 2021a), and PushShift.io Reddit (Baumgartner et al., 2020; Roller et al., 2021)""
...
""RoBERTa We included the BookCorpus (Zhu et al., 2015) and Stories (Trinh and Le, 2018) subsets of the RoBERTa corpus and utilized an updated version of CCNews, containing news stories crawled through September 28, 2021. This CCNews v2 corpus was preprocessed the same way as the original RoBERTa CCNews (Liu et al., 2019b).

The Pile We included a subset of the Pile (Gao et al., 2021a), including: CommonCrawl, DM Mathematics, Project Gutenberg, HackerNews, OpenSubtitles, OpenWebText2, USPTO and Wikipedia. Other subsets of the Pile were eliminated
...

PushShift.io Reddit We included a subset of the Pushshift.io corpus produced by Baumgartner et al. (2020) and previously used by Roller et al. (2021). To convert the conversational trees
into language-model-accessible documents, we extracted the longest chain of comments in each thread and discarded all other paths in the tree. This reduced the corpus by about 66%.",180000000000.0,"""The training data contains 180B tokens corresponding to 800 GB of data""

1 token ~ 0.75 words",793.5,"4.3*10^23 FLOP / (147 TFLOPS) = 813000 A100-hours
https://www.wolframalpha.com/input?i=4.3*10%5E23+FLOP+%2F+%28147+TFLOPS%29

""As of yesterday, at 12:46pm PST on January 6, our 175B model finally completed its training run on 300B tokens. This required ~4.30E+23 FLOPs of compute, or roughly ~33 days of continuous training on 1024 80GB A100s (assuming no hardware issues, no numerical instabilities, etc.).""",NVIDIA A100 SXM4 80 GB,Self-supervised learning,Confident,"Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3,1 while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models",1.67,OPT-175B,Open weights (non-commercial),United States of America,,,,1024.0,0.4712,2025-06-11 11:36,,,,2000000.0,Table 1,Industry,,,,812544.0,Open source,"non-commercial for weights:
https://github.com/facebookresearch/metaseq/blob/main/projects/OPT/MODEL_LICENSE.md

training code (MIT) https://github.com/facebookresearch/metaseq/blob/main/docs/training.md ",Industry,$731667.61,"""[...] which enabled training OPT-175B on 992 80GB A100 GPUs, reaching 147 TFLOP/s utilization per GPU.

Peak FLOP/s per A100: 312 TFLOP/s
147/312 = 0.4712",FP16,822708.6888139298,Reported,,,,6
Flamingo,"Multimodal,Vision,Language,Video","Visual question answering,Image captioning",DeepMind,"Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, Karen Simonyan",2022-04-29,Flamingo: a Visual Language Model for Few-Shot Learning,https://arxiv.org/abs/2204.14198,2473.0,"Highly cited,SOTA improvement","""For tasks lying anywhere on this spectrum, a single Flamingo model can achieve a new state of the art with few-shot learning, simply by prompting the model with task-specific examples. On numerous benchmarks, Flamingo outperforms models fine-tuned on thousands of times more task-specific data.""",80000000000.0,"""We obtain three models, Flamingo-3B, Flamingo-9B and Flamingo-80B""

"" The Flamingo-80B model builds on top of the frozen Chinchilla 70B language model [42]. Starting from the very first layer and before every seventh transformer blocks, we add a GATED XATTN-DENSE layer attending to the visual inputs; this accounts for 10B additional learned parameters. For simplicity, we refer to this model as simply Flamingo throughout the paper""",2.1897200000000104e+23,"1536 TPU v4 chips for 15 days. Assuming 40% utilization:
C = 1536 TPU * 275*10^12 FLOP/s/TPU * 15 day * 86400 s/day * 0.40 = 2.2*10^23 FLOP

""All training and evaluation was performed on TPUv4 instances. The largest model containing 80 billion parameters is trained on QUSV chips for 15 days and sharded across 16 devices.""

""All trained parameters and optimizer accumulators are stored and updated in float32; all activations and gradients are computed in bfloat16 after downcasting of parameters from float32 to bfloat16""","MultiModal MassiveWeb,LTIP,VTP,ALIGN",,,"Flamingo was trained on a mixture of web-scraped datasets:
43M pages of text with interleaved images (MultiModal MassiveWeb dataset)
312M image-text pairs (LTIP dataset)
27M video-text pairs (VTP dataset)
1.8B image-alt text pairs (ALIGN dataset)

Training dataset size is at least 2.1 billion.",360.0,1536 TPU v4 chips for 15 days,Google TPU v4,Supervised,Confident,"Building models that can be rapidly adapted to novel tasks using only a handful of annotated examples is an open challenge for multimodal machine learning research. We introduce Flamingo, a family of Visual Language Models (VLM) with this ability. We propose key architectural innovations to: (i) bridge powerful pretrained vision-only and language-only models, (ii) handle sequences of arbitrarily interleaved visual and textual data, and (iii) seamlessly ingest images or videos as inputs. Thanks to their flexibility, Flamingo models can be trained on large-scale multimodal web corpora containing arbitrarily interleaved text and images, which is key to endow them with in-context few-shot learning capabilities. We perform a thorough evaluation of our models, exploring and measuring their ability to rapidly adapt to a variety of image and video tasks. These include open-ended tasks such as visual question-answering, where the model is prompted with a question which it has to answer; captioning tasks, which evaluate the ability to describe a scene or an event; and close-ended tasks such as multiple-choice visual question-answering. For tasks lying anywhere on this spectrum, a single Flamingo model can achieve a new state of the art with few-shot learning, simply by prompting the model with task-specific examples. On numerous benchmarks, Flamingo outperforms models fine-tuned on thousands of times more task-specific data.",,,Unreleased,United Kingdom of Great Britain and Northern Ireland,Chinchilla,,,1536.0,,2025-05-28 16:10,,,,,,Industry,checked,,,552960.0,Unreleased,,Industry,$183423.16,,BF16,524511.829594388,Hardware,,,,
CogView2,Image generation,Image generation,"Tsinghua University,Beijing Academy of Artificial Intelligence / BAAI","Ming Ding, Wendi Zheng, Wenyi Hong, Jie Tang",2022-04-28,CogView2: Faster and Better Text-to-Image Generation via Hierarchical Transformers,https://arxiv.org/abs/2204.14217,,,,6000000000.0,"The backbone of our pretrained CogLM is a Transformer with Sandwich LayerNorm [3]. The model has 6 billion parameters (48 layers, hidden size 3072, 48 attention heads)",2.265e+22,"""The model has 6 billion parameters (48 layers, hidden size 3072, 48 attention heads), trained for 300,000 iterations in FP16 with batch size 4,096""

Using 6ND formula:

6*6000000000*300000*4096*512=2.2649242e+22",,"""Our dataset for pretraining contains about 30 million text-image pairs""",,,,,,,Confident,"The development of the transformer-based text-to-image models are impeded by its slow generation and complexity for high-resolution images. In this work, we put forward a solution based on hierarchical transformers and local parallel auto-regressive generation. We pretrain a 6B-parameter transformer with a simple and flexible self-supervised task, Cross-modal general language model (CogLM), and finetune it for fast super-resolution. The new text-to-image system, CogView2, shows very competitive generation compared to concurrent state-of-the-art DALL-E-2, and naturally supports interactive text-guided editing on images.",,,,"China,China",,,,,,2025-03-31 21:42,,,,,,"Academia,Academia",,,,,,,"Academia,Academia",,,,,,,,,
GraphBP,Biology,Drug discovery,"Texas A&M,Fujitsu","Meng Liu, Youzhi Luo, Kanji Uchino, Koji Maruhashi, Shuiwang Ji",2022-04-19,Generating 3D Molecules for Target Protein Binding,https://arxiv.org/abs/2204.09410,109.0,,,,,,,,,15000001.0,"500,000 complexes Ã— 30 atoms/complex = 15,000,000 total data points (1.5e7)",,,,,Speculative,,,,,"United States of America,Japan",,,,,,2025-05-01 10:42,,,,,,"Academia,Industry",,,,,,,"Academia,Industry",,,,,,,,,
Sparse all-MLP,Language,Language modeling,Meta AI,"Ping Yu, Mikel Artexte, Myle Ott, Sam Shleifer, Hongyu Gong, Ves Stoyanov, Xian Li",2022-04-14,Efficient Language Modeling with Sparse all-MLP,https://arxiv.org/abs/2203.06850,11.0,SOTA improvement,"Abstract:
""Our model also outperforms
the RoBERTa-Large model on several English tasks of the GLUE benchmark by 0.3% on average while handling 99 more languages.""",9410000000.0,"Table 2: ""In Section 4.4, we run our large model (9.41B parameters)""",5.32224e+20,"112 hours on 32 V100 GPUs
assumed 0.33 util rate

112 hours *3600 seconds / hour *0.33 utilization *32 gpus *125000000000000 FLOPs=532224000000000000000
","RoBERTa dataset,CC100",,100000000000.0,100B tokens (Table 2) so 75B words.,112.0,,NVIDIA V100,Self-supervised learning,Confident,"All-MLP architectures have attracted increasing interest as an alternative to attention-based models. In NLP, recent work like gMLP shows that all-MLPs can match Transformers in language modeling, but still lag behind in downstream tasks. In this work, we analyze the limitations of MLPs in expressiveness, and propose sparsely activated MLPs with mixture-of-experts (MoEs) in both feature and input (token) dimensions. Such sparse all-MLPs significantly increase model capacity and expressiveness while keeping the compute constant. We address critical challenges in incorporating conditional computation with two routing strategies. The proposed sparse all-MLP improves language modeling perplexity and obtains up to 2Ã— improvement in training efficiency compared to both Transformer-based MoEs (GShard, Switch Transformer, Base Layers and HASH Layers) as well as dense Transformers and all-MLPs. Finally, we evaluate its zero-shot in-context learning performance on six downstream tasks, and find that it surpasses Transformer-based MoEs and dense Transformers.",,,Unreleased,United States of America,,,,,,2025-06-18 12:57,,,,,,Industry,,,,,Unreleased,,Industry,,,,,Hardware,,,,
Stable Diffusion (LDM-KL-8-G),Image generation,"Image generation,Text-to-image","Runway,Ludwig Maximilian University","Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, BjÃ¶rn Ommer",2022-04-13,High-Resolution Image Synthesis with Latent Diffusion Models,https://arxiv.org/abs/2112.10752,10764.0,"Significant use,Highly cited",,1450000000.0,See Table 2,5e+22,"""I get 5e22 FLOP. 150k hours on A100 [1] gives 150*10^3 hours * 3600 seconds/hour * 3.12E+14 peak performance of A100 * 0.33 utilisation = 5e22  FLOP""

[1] https://twitter.com/EMostaque/status/1563870674111832066",LAION-400M,"Depends on the specific task; see sec 4

""we train a 1.45B parameter
KL-regularized LDM conditioned on language prompts on
LAION-400M""",400000000.0,,585.9,"total chip-hours divided by number of GPUs
150k/256",NVIDIA A100,Self-supervised learning,Confident,"By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at this https URL .",,,Open weights (restricted use),"United States of America,Germany",,,,256.0,,2025-06-18 12:54,,,,,,"Industry,Academia",,,,150000.0,Open source,"OpenRAIL license for weights
https://huggingface.co/CompVis/stable-diffusion-v-1-4-original

""The Responsible AI License allows users to take advantage of the model in a wide range of settings (including free use and redistribution) as long as they respect the specific use case restrictions outlined, which correspond to model applications the licensor deems ill-suited for the model or are likely to cause harm""

MIT license for code
https://github.com/CompVis/stable-diffusion","Industry,Academia",$111248.22,,,205764.21634205984,Hardware,CompVis,,,
STT Conformer-Transducer XL,Speech,Speech recognition,NVIDIA,,2022-04-12,,https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/stt_en_conformer_transducer_xlarge,,,"SOTA for open-source speech recognition, per OpenAI's Whisper paper: https://cdn.openai.com/papers/whisper.pdf",600000000.0,600M,,,"NeMo ASRSET,LibriSpeech,Fisher,Switchboard,VoxPopuli","""All the models in this collection are trained on a composite dataset (NeMo ASRSET) comprising of several thousand hours of English speech:

Librispeech 960 hours of English speech
Fisher Corpus
...""",,"Tens of millions of words, at ~12k words/hour (~200 words per minute)",,,,,Likely,,,,Open weights (unrestricted),United States of America,,,,,,2024-09-30 14:54,,,,,,Industry,,,,,Unreleased,"CC-BY-4.0:

https://huggingface.co/nvidia/stt_en_conformer_transducer_xlarge",Industry,,,,,,,,,
BERT-RBP,Biology,"Proteins,Protein interaction prediction",Waseda University,"Keisuke Yamada, Michiaki Hamada",2022-04-07,Prediction of RNAâ€“protein interactions using a nucleotide language model,https://academic.oup.com/bioinformaticsadvances/article/2/1/vbac023/6564689,36.0,SOTA improvement,"""Our model outperformed state-of-the-art prediction models using the eCLIP-seq data of 154 RBPs"" [Abstract] - SOTA improvement on a very specific task",110000000.0,"Base model is BERT base (110M parameters), pre-trained on human reference genome (DNABert: https://academic.oup.com/bioinformatics/article/37/15/2112/6128680)",1.4e+20,"See DNABert entry:

""Since the pre-training of DNABERT model is resource-intensive (about 25â€‰days on 8 NVIDIA 2080Ti GPUs)""

Assuming FP16 and 30% utilization

Calculation = (25 * 24 *3600) s * 2.7e13 FLOP/s per GPU * 8 GPUs * 0.3 utilization = 1.4e20 FLOP",RBPSuite,"See DNABert entry: ""We generated training data from human genome [...]"" [2.2.2 Pre-training]

""An eCLIP-seq dataset previously generated from the ENCODE3 database by Pan et al. (2020) was used. The original dataset consisted of 154 RBP sets with up to 60 000 positive RNA sequences that bind to the corresponding RBP and the same number of negative sequences."" [2.2 Data preparation]",78000001.0,"RBPs: 154
Sequences per RBP: 15,000
Total sequences = 154 Ã— 15,000 = 2,310,000
Tokens per sequence = 34
Total tokens = 2,310,000 Ã— 34 = 78,540,000
Final estimate: 7.8 Ã— 10â· tokens",,,,,Confident,"Motivation
The accumulation of sequencing data has enabled researchers to predict the interactions between RNA sequences and RNA-binding proteins (RBPs) using novel machine learning techniques. However, existing models are often difficult to interpret and require additional information to sequences. Bidirectional encoder representations from transformer (BERT) is a language-based deep learning model that is highly interpretable. Therefore, a model based on BERT architecture can potentially overcome such limitations.

Results
Here, we propose BERT-RBP as a model to predict RNAâ€“RBP interactions by adapting the BERT architecture pretrained on a human reference genome. Our model outperformed state-of-the-art prediction models using the eCLIP-seq data of 154 RBPs. The detailed analysis further revealed that BERT-RBP could recognize both the transcript region type and RNA secondary structure only based on sequence information. Overall, the results provide insights into the fine-tuning mechanism of BERT in biological contexts and provide evidence of the applicability of the model to other RNA-related problems.",3.0,,Open weights (non-commercial),Japan,DNABERT,22000000000000000,"""The models were trained on four NVIDIA Tesla V100 GPUs (128
GB memory). The training of one RBP model using 19 200 samples
took <10 min.""

Calculation assuming FP16 and 30% utlization and NVIDIA Tesla V100 SMX2 model: 
10 min * 60 sec/min * 3.1e13 FLOP/s * 4 GPU * 0.3 utilization = 2.2e16",,,2025-04-30 10:04,,,,,,Academia,,,,,Open (non-commercial),"No clear license: https://github.com/kkyamada/bert-rbp

train script: https://github.com/kkyamada/bert-rbp/blob/master/examples/run_finetune.py 

data also doesn't have a clear license: http://www.csbio.sjtu.edu.cn/bioinf/RBPsuite/dataset_new.html",Academia,,,,,Hardware,,,,
DALLÂ·E 2,Image generation,"Text-to-image,Image generation",OpenAI,"Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen",2022-04-06,Hierarchical Text-Conditional Image Generation with CLIP Latents,https://cdn.openai.com/papers/dall-e-2.pdf,5470.0,"Highly cited,SOTA improvement",,3500000000.0,"""Our decoder architecture is the 3.5 billion parameter GLIDE model""",3.3695784e+23,"Decoder architecture is similar to Imagen (1.46E+22), but trained on 1.6e9 datapoints (Table 3) rather than Imagen's 5.1e9 datapoints.

DALL-E 2 uses two models as priors. I estimate the prior model's FLOP as 6*N*D = 6 * 1e9 * 4096 * 1e6 = 2.5e19 FLOP. However, this seems low compared to CLIP.

So it may be possible to estimate DALL-E 2's compute by analogy to Imagen, but there is a lot of uncertainty and more research would be needed.

here (https://arxiv.org/pdf/2407.15811) they claim the DALL-E.2 model was trained on the equivalent of 5208.3 days on 8*A100 GPUs:

312000000000000 FLOP / sec / GPU * 8 GPUs * 5208.3 days * 24 hours / day * 3600 sec / hour * 0.3 [assumed utilization] = 3.3695784e+23 FLOP","CLIP,DALL-E","https://aimlapi.com/models/openai-dall-e-2-api says the model's knowledge cutoff date is September 2021, which I assume means September 1, 2021.",650000000.0,"""When training the encoder, we sample from the CLIP [39] and DALL-E [40] datasets (approximately 650M images in total) with equal probability""",,,,Self-supervised learning,Speculative,"Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter
are computationally more efficient and produce higher-quality samples.",,,API access,United States of America,,,,,,2025-05-23 14:01,,,,,,Industry,checked,,,,Unreleased,,Industry,,,,,Third-party estimation,,,,11
PaLM (540B),Language,"Language modeling,Code generation,Translation",Google Research,"Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev,, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta ,Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, Noah Fiedel",2022-04-04,PaLM: Scaling Language Modeling with Pathways,https://arxiv.org/abs/2204.02311,5064.0,"Highly cited,SOTA improvement,Training cost","Demonstrates continued benefits of scaling, as well as discontinuous improvements in performance",540350000000.0,"""To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model (PaLM).""",2.5272e+24,"See Table 20.

6144 TPUv4 for 1200 hours + 3072 TPUv4 for 336 hours.
Equivalent to 6144 TPUv4 for 1368 hours.

46.2% model FLOPs utilization

""The 540B-parameter PaLM model sustained a remarkable 57.8% of the peak hardware floating point performance over 50 days while training on TPU v4 supercomputers. "" https://cloud.google.com/blog/topics/systems/tpu-v4-enables-performance-energy-and-co2e-efficiency-gains","Wikipedia,GLaM dataset,LaMBDA dataset,GitHub",,585000000000.0,"""The PaLM pretraining dataset consists of a high-quality corpus of 780 billion tokens that represent a wide range of natural language use cases.""

1 token ~ 0.75 words",1536.0,"6144 TPUv4 for 1200 hours + 3072 TPUv4 for 336 hours.

Equivalent to 6144 TPUv4 for 1368 hours.",Google TPU v4,Self-supervised learning,Confident,"Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.",1.0,,Unreleased,"Multinational,United States of America,Canada,Switzerland",,,,6144.0,0.462,2025-05-29 10:23,,,,4000000.0,"""For the largest model, we use batch size 512 (1M tokens) until step 50k, then double it to 1024 (2M tokens) until step 115k, and finally double again it to 2048 (4M tokens) until training is complete at step 255k""",Industry,checked,,,8404992.0,Unreleased,,Industry,$2945949.76,"""In Section 4, we describe how we were able to scale pipeline-free training of PaLM 540B to 6144 chips across two TPU v4 Pods while achieving very high efficiency of 46.2% in model FLOPs utilization (observed throughput relative to theoretical max throughput) and 57.8% in hardware FLOPs utilization.""",,2099215.6984531507,Hardware,,,,1
NoPos,Language,Language modeling/generation,"Tel Aviv University,University of Washington,Intel Labs,Meta AI","Adi Haviv, Ori Ram, Ofir Press, Peter Izsak, Omer Levy",2022-03-30,Transformer Language Models without Positional Encodings Still Learn Positional Information,https://arxiv.org/abs/2203.16634,83.0,,,1300000000.0,"1.3B

""The baseline model in this setting follows the 1.3B parameter architecture of Brown et al. (2020), also known as GPT-3 XL: 24 transformer layers with 2048 model dimensions, 8192 feed-forward dimensions, and 32 attention heads.""",2.09664e+19,6 FLOP / token / parameter * 1.3 * 10^9 parameters * 256 000 tokens per batch [table 5] * 10500 steps [table 5] = 2.09664e+19 FLOP ,The Pile (NoPos subset),,21000000000.0,,,,,,Confident,"Causal transformer language models (LMs), such as GPT-3, typically require some form of positional encoding, such as positional embeddings. However, we show that LMs without any explicit positional encoding are still competitive with standard models, and that this phenomenon is robust across different datasets, model sizes, and sequence lengths. Probing experiments reveal that such models acquire an implicit notion of absolute positions throughout the network, effectively compensating for the missing information. We conjecture that causal attention enables the model to infer the number of predecessors that each token can attend to, thereby approximating its absolute position. Our findings indicate that causal LMs might derive positional awareness not only from the explicit positioning mechanism, but also from the effects of the causal mask.",0.12,NoPos,Unreleased,"Israel,United States of America,Multinational,United States of America,United States of America",,,,,,2025-03-31 12:54,,,,256000.0,Table 5,"Academia,Academia,Industry,Industry",,,,,Unreleased,,"Academia,Academia,Industry,Industry",,,,,Operation counting,,,,
Chinchilla,Language,Language modeling,DeepMind,"Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, Laurent Sifre",2022-03-29,Training Compute-Optimal Large Language Models,https://arxiv.org/abs/2203.15556,1486.0,SOTA improvement,"Proposes new scaling law, with good empirical results",70000000000.0,"""We test this hypothesis by training a predicted compute-optimal model, \chinchilla, that uses the same compute budget as \gopher but with 70B parameters and 4Ã— more more data. \chinchilla uniformly and significantly outperforms \Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks.""",5.76e+23,"""Both Chinchilla and Gopher have been trained for the same number of FLOPs but differ in the size of the model and the number of training tokens.""

We see the number of flops in table 3","MassiveWeb,C4","MassiveWeb, Books, C4, News, Github, Wikipedia (Table A1)",1050000000000.0,"Table 1 shows Chinchilla was training on 1.4 trillion tokens

1 token ~ 0.75 words",,,"Google TPU v4,Google TPU v3",Self-supervised learning,Confident,"We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over \nummodels language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, \chinchilla, that uses the same compute budget as \gopher but with 70B parameters and 4Ã— more more data. \chinchilla uniformly and significantly outperforms \Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that \chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, \chinchilla reaches a state-of-the-art average accuracy of 67.5\% on the MMLU benchmark, greater than a 7\% improvement over \gopher.",1.0,Chinchilla,Unreleased,United Kingdom of Great Britain and Northern Ireland,,,,,,2025-05-29 11:34,,,,3000000.0,"Table 1. ""1.5M â†’ 3M""",Industry,checked,,,,Unreleased,,Industry,,,BF16,,Reported,,,,4
MemSizer (language modeling),Language,Language modeling,"Meta AI,Chinese University of Hong Kong (CUHK)","Yizhe Zhang, Deng Cai",2022-03-23,Linearizing Transformer with Key-Value Memory,https://arxiv.org/abs/2203.12644,6.0,,,357000000.0,"357M
Table 3

""Following Kasai et al. (2021), we choose similar hyperparameters to prior work (Baevski and Auli, 2019; Fan et al., 2020): 32 layers, 8 heads, 128 head dimensions, 1024 model dimensions, 4096 fully connected dimensions and
dropout (Srivastava et al., 2014) and layer dropout rates of 0.2. """,7.3e+18,"6 FLOP / token / parameter * 357000000 parameters * 103000000 tokens * 1 epoch [assumed for lower bound] = 2.20626e+17 FLOP

also to consider: 
 7.3 Ã— 10^18 FLOP 
SOURCE: Impute based on Baevski and Auli 2019

""We generally follow the optimization method from Baevski and Auli (2019), with a slight modification for some hyperparameters including learning rate (we use 10âˆ’4), which shows better convergence. ""

NOTES: Probably correct within an order of 2 or so",WikiText-103,,103000000.0,,,,,,Likely,"Efficient transformer variants with linear time complexity have been developed to mitigate the quadratic computational overhead of the vanilla transformer. Among them are low-rank projection methods such as Linformer and kernel-based Transformers. Despite their unique merits, they usually suffer from a performance drop comparing with the vanilla transformer on many sequence generation tasks, and often fail to obtain computation gain when the generation is short. We propose MemSizer, an approach towards closing the performance gap while improving the efficiency even with short generation. It projects the source sequences into lower dimension representations like Linformer, while enjoying efficient recurrent-style incremental computation similar to kernel-based transformers. This yields linear computation time and constant memory complexity at inference time. MemSizer also employs a lightweight multi-head mechanism which renders the computation as light as a single-head model. We demonstrate that MemSizer provides an improved balance between efficiency and accuracy over the vanilla transformer and other efficient transformer variants in three typical sequence generation tasks, including machine translation, abstractive text summarization, and language modeling.",,MemSizer,Unreleased,"United States of America,Hong Kong,China",,,,,,2025-05-01 10:42,,,MemSizer,,,"Industry,Academia",,2.20626e+17,,,Open (non-commercial),"code, no license: https://github.com/jcyk/memsizer 
wt103 train: https://github.com/jcyk/memsizer/blob/main/lm_wikitext-103.sh ","Industry,Academia",,,,,Operation counting,,,,
"Segatron-XL large, M=384 + HCP",Language,Language modeling,"Microsoft Research,University of Waterloo","He Bai, Tong Wang, Alessandro Sordoni, Peng Shi",2022-03-21,Better Language Model with Hypernym Class Prediction,https://arxiv.org/abs/2203.10692,14.0,SOTA improvement,"""Empirically, this curriculum learning strategy consistently improves perplexity over various large, highly-performant state-of-the-art Transformer-based models on two datasets, WikiText-103 and ARXIV""",257000000.0,"257M
(Table 1)

18 layers, 16 heads, hidden size
1024 batch size 128.",2.65e+19,6 FLOP / token / parameter * 257000000 parameters * 384 tokens/batch Ã— 128 [batch size] * 350000 steps = 2.6527334e+19 FLOP,WikiText-103,,103000000.0,"103000000 tokens - size of wikitext 103

Training steps: 350,000 steps 
Sequence length: 384 tokens
Batch size: 128
Tokens per batch = 384 tokens/batch Ã— 128 = 49,152 tokens/batch
Total tokens = 49,152 tokens/batch Ã— 350,000 steps = 17.2 billion tokens 

17200000000/103000000 = 167 epochs",,,,,Confident,"Class-based language models (LMs) have been long devised to address context sparsity in n-gram LMs. In this study, we revisit this approach in the context of neural LMs. We hypothesize that class-based prediction leads to an implicit context aggregation for similar words and thus can improve generalization for rare words. We map words that have a common WordNet hypernym to the same class and train large neural LMs by gradually annealing from predicting the class to token prediction during training. Empirically, this curriculum learning strategy consistently improves perplexity over various large, highly-performant state-of-the-art Transformer-based models on two datasets, WikiText-103 and Arxiv. Our analysis shows that the performance improvement is achieved without sacrificing performance on rare words. Finally, we document other attempts that failed to yield empirical gains, and discuss future directions for the adoption of class-based LMs on a larger scale.",167.02,"""Segatron-XL large, M=384 + HCP""",Unreleased,"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada",,,,,,2025-03-31 13:27,,,,,,"Industry,Academia",,,,,Open (non-commercial),"code, no license
https://github.com/richardbaihe/robustlm","Industry,Academia",,,,,Operation counting,,,,
Transformer Large + HCP,Language,Language modeling,"University of Waterloo,Microsoft Research","He Bai, Tong Wang, Alessandro Sordoni, Peng Shi",2022-03-21,Better Language Model with Hypernym Class Prediction,https://arxiv.org/abs/2203.10692,14.0,,other model in this paper has better performance,257000000.0,257M,6.06e+18,"80k steps (Table 1)
""The input lengths are 150 for the base model and 384 for the large model.""
""large model: 18 layers, 16 heads, hidden size 1024 batch size 128""

6 FLOP / token / parameter * 257000000 parameters * 384 tokens per sequence * 128 sequences per batch * 80000 steps = 6.06339072 Ã— 10^18 FLOP ",WikiText-103,,103000000.0,,,,,,Confident,"Class-based language models (LMs) have been long devised to address context sparsity in n-gram LMs. In this study, we revisit this approach in the context of neural LMs. We hypothesize that class-based prediction leads to an implicit context aggregation for similar words and thus can improve generalization for rare words. We map words that have a common WordNet hypernym to the same class and train large neural LMs by gradually annealing from predicting the class to token prediction during training. Empirically, this curriculum learning strategy consistently improves perplexity over various large, highly-performant state-of-the-art Transformer-based models on two datasets, WikiText-103 and Arxiv. Our analysis shows that the performance improvement is achieved without sacrificing performance on rare words. Finally, we document other attempts that failed to yield empirical gains, and discuss future directions for the adoption of class-based LMs on a larger scale.",38.18,Transformer Large + HCP,Unreleased,"Canada,United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational",,,,,,2025-03-31 13:55,,,,,,"Academia,Industry",,,,,Open (non-commercial),"code, no license: https://github.com/richardbaihe/robustLM ","Academia,Industry",,,,,Operation counting,,,,
ViT-G (model soup),Vision,Image classification,"University of Washington,Columbia University,Google,Meta AI,Tel Aviv University","Mitchell Wortsman, Gabriel Ilharco, Samir Yitzhak Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari S. Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, Ludwig Schmidt",2022-03-10,Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time,https://arxiv.org/abs/2203.05482v3,703.0,SOTA improvement,"""When fine-tuning large pre-trained models such as CLIP, ALIGN, and a ViT-G pre-trained on JFT, our soup recipe provides significant improvements over the best model in a hyperparameter sweep on ImageNet. The resulting ViT-G model, which attains 90.94% top-1 accuracy on ImageNet, achieved a new state of the art.""",1843000000.0,This is from the original ViT-G paper,3.4e+21,"This is a fine-tuned version of ViT-G, which required 3.4e21 to train per PCD/Akronomicon.

Fine-tuning compute is likely minor in comparision:
""Models are fine-tuned at a batch size of 512 for either 10,000 or 20,000 steps (approximately 4 or 8 epochs)... all models are fine-tuned at 518 Ã— 518 resolution""
At 20k steps, we have (518^2) * 512 * 20k = 2.75e12 pixels seen in fine-tuning, compared to (224^2) * 32768 * 5M = 8.22e15 in pre-training.",,,,,,,,,Confident,"The conventional recipe for maximizing model accuracy is to (1) train multiple models with various hyperparameters and (2) pick the individual model which performs best on a held-out validation set, discarding the remainder. In this paper, we revisit the second step of this procedure in the context of fine-tuning large pre-trained models, where fine-tuned models often appear to lie in a single low error basin. We show that averaging the weights of multiple models fine-tuned with different hyperparameter configurations often improves accuracy and robustness. Unlike a conventional ensemble, we may average many models without incurring any additional inference or memory costs -- we call the results ""model soups."" When fine-tuning large pre-trained models such as CLIP, ALIGN, and a ViT-G pre-trained on JFT, our soup recipe provides significant improvements over the best model in a hyperparameter sweep on ImageNet. The resulting ViT-G model, which attains 90.94% top-1 accuracy on ImageNet, achieved a new state of the art. Furthermore, we show that the model soup approach extends to multiple image classification and natural language processing tasks, improves out-of-distribution performance, and improves zero-shot performance on new downstream tasks. Finally, we analytically relate the performance similarity of weight-averaging and logit-ensembling to flatness of the loss and confidence of the predictions, and validate this relation empirically. Code is available at this https URL.",8.0,,Open weights (non-commercial),"United States of America,United States of America,United States of America,United States of America,Israel",,,,,,2025-02-19 12:57,,,,,,"Academia,Academia,Industry,Industry,Academia",,,,,Unreleased,"no license
code here, may just be inference code: https://github.com/mlfoundations/model-soups ","Academia,Academia,Industry,Industry,Academia",,,,,Operation counting,,,,
GPT3-6.7B + muP,Language,Language modeling/generation,"Microsoft,OpenAI","Greg Yang, Edward J. Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ryder, Jakub Pachocki, Weizhu Chen, Jianfeng Gao",2022-03-07,Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer,https://arxiv.org/abs/2203.03466,120.0,,,6700000000.0,,1.28e+22,"""we outperform published numbers of the 6.7B GPT-3 model, with
tuning cost only 7% of total pretraining cost.""

GPT-3 6.7B reported training compute is 1.2e+22 FLOP 

1.2e+22 FLOP * 1.07 = 1.284e+22 FLOP

6 FLOP / parameter / token * 6660000000 parameters * 300000000000 tokens = 1.1988 Ã— 10^22 FLOP

",,"Figure 15
Page 11 of https://arxiv.org/pdf/2203.03466 says this is the GPT-3 6.7B model tuned with ÂµTransfer, so I assume it the same knowledge cutoff dates as the GPT-3 models, which is December 31, 2019.",300000000000.0,,,,,,Confident,"Hyperparameter (HP) tuning in deep learning is an expensive process, prohibitively so for neural networks (NNs) with billions of parameters. We show that, in the recently discovered Maximal Update Parametrization (muP), many optimal HPs remain stable even as model size changes. This leads to a new HP tuning paradigm we call muTransfer: parametrize the target model in muP, tune the HP indirectly on a smaller model, and zero-shot transfer them to the full-sized model, i.e., without directly tuning the latter at all. We verify muTransfer on Transformer and ResNet. For example, 1) by transferring pretraining HPs from a model of 13M parameters, we outperform published numbers of BERT-large (350M parameters), with a total tuning cost equivalent to pretraining BERT-large once; 2) by transferring from 40M parameters, we outperform published numbers of the 6.7B GPT-3 model, with tuning cost only 7% of total pretraining cost. A Pytorch implementation of our technique can be found at this http URL and installable via `pip install mup`.",1.0,GPT3-6.7B + muP,Unreleased,"United States of America,Multinational,India,Belgium,United States of America",,840000000000000000000,"""we outperform published numbers of the 6.7B GPT-3 model, with
tuning cost only 7% of total pretraining cost.""

GPT-3 6.7B reported training compute is 1.2e+22 FLOP 

1.2e+22 FLOP * 0.07 = 8.4e+20 FLOP",,,2025-05-12 15:29,,,GPT3-6.7B + muP,,,"Industry,Industry",,,,,Unreleased,"their repo is open: https://github.com/microsoft/mup

The technique is open, not the model. GPT-3 isn't open so it wouldn't be possible for people to recreate GPT-3 + muP with this code","Industry,Industry",,,,,"Comparison with other models,Operation counting",,,,
MegaSyn,Medicine,Drug discovery,Collaborations Pharmaceuticals,"Fabio Urbina, Filippa Lentzos, CÃ©dric Invernizzi, Sean Ekins",2022-03-07,Dual Use of Artificial Intelligence-powered Drug Discovery,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9544280/,148.0,Historical significance,"Notable example of an AI model having a potential dual use for bio/chemical weapons:

""To narrow the universe of molecules we chose to drive the generative model towards compounds like the nerve agent VX, one of the most toxic chemical warfare agents developed during the 20th centuryâ€”a few salt-sized grains of VX, (6â€“10 mg)5, is sufficient to kill a person. Nerve agents such as Novichoks have also been in the headlines recently6.

In less than 6 hours after starting on our in-house server, our model generated forty thousand molecules that scored within our desired threshold. In the process, the AI designed not only VX, but many other known chemical warfare agents that we identified through visual confirmation with structures in public chemistry databases. Many new molecules were also designed that looked equally plausible. These new molecules were predicted to be more toxic based on the predicted LD50 in comparison to publicly known chemical warfare agents (Figure 1). This was unexpected as the datasets we used for training the AI did not include these nerve agents. The virtual molecules even occupied a region of molecular property space that was entirely separate to the many thousands of molecules in the organism-specific LD50 model, which is mainly made up of pesticides, environmental toxins, and drugs (Figure 1). By inverting the use of our machine learning models, we had transformed our innocuous generative model from a helpful tool of medicine to a generator of likely deadly molecules.""",,"model details here: https://chemrxiv.org/engage/chemrxiv/article-details/61551803d1fc335b7cf8fd45

""The variational autoencoder utilizes an encoder-decoder architecture to map chemical space into a latent vector 34. The encoder is composed of 3 LSTM layers of 512 units each followed by a linear layer of 64 units (the latent space).
Our decoder is comprised of 3 LSTM layers of 512 units each with dropout of 0.2 between
all layers""",,,ChEMBL,"https://chemrxiv.org/engage/chemrxiv/article-details/61551803d1fc335b7cf8fd45

""The initial model is trained on ChEMBL 28â€™s ~2 million compounds""",,,,,,,Unknown,An international security conference explored how artificial intelligence (AI) technologies for drug discovery could be misused for de novo design of biochemical weapons. A thought experiment evolved into a computational proof.,,,Hosted access (no API),United States of America,,,,,,2025-06-04 17:18,,,,,,Industry,,,,,Unreleased,"""We can use MegaSyn in fee for service work for you.
We can provide an annual license for you to access this software on your own server.
We provide maintenance and customization options.""
https://www.collaborationspharma.com/megasyn",Industry,,,,,,,,,
RQ-Transformer (3.8B params ImageNet dataset),"Vision,Image generation",Text-to-image,"Kakao,POSTECH","Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, Wook-Shin Han",2022-03-03,Autoregressive Image Generation using Residual Quantization,https://arxiv.org/abs/2203.01941,189.0,,,3822000000.0,3822M from Table 5 - ImageNet rows,2.9113344e+20,"""We use eight NVIDIA A100 GPUs to train RQ-Transformer of 1.4B parameters, and four GPUs to train RQ-Transformers of other sizes. The training time is <9 days for LSUN-cat, LSUN-bedroom, <4.5 days for ImageNet, and CC-3M, and <1 day for LSUN-church and FFHQ. ""

Taken literally, suggests 3.8B model used fewer GPUs than 1.4B. This seems likely to be a typo, so I assume details are meant to be given for largest model (3.8B).

(8) * (3.12e14) * (4.5 * 24 * 3600) * (0.3) = 2.911e20
(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)",ImageNet,"Table 5 
",1200000.0,size of ImageNet,108.0,"States 4.5 days for ImageNet for 1.4B model, but this is probably a typo. I expect these details were given for the largest model.

""We use eight NVIDIA A100 GPUs to train RQ-Transformer of 1.4B parameters, and four GPUs to trainRQ-Transformers of other sizes. The training time is <9 days for LSUN-cat, LSUN-bedroom, <4.5 days for ImageNet, and CC-3M, and <1 day for LSUN-church and FFHQ. """,NVIDIA A100,,Likely,"For autoregressive (AR) modeling of high-resolution images, vector quantization (VQ) represents an image as a sequence of discrete codes. A short sequence length is important for an AR model to reduce its computational costs to consider long-range interactions of codes. However, we postulate that previous VQ cannot shorten the code sequence and generate high-fidelity images together in terms of the rate-distortion trade-off. In this study, we propose the two-stage framework, which consists of Residual-Quantized VAE (RQ-VAE) and RQ-Transformer, to effectively generate high-resolution images. Given a fixed codebook size, RQ-VAE can precisely approximate a feature map of an image and represent the image as a stacked map of discrete codes. Then, RQ-Transformer learns to predict the quantized feature vector at the next position by predicting the next stack of codes. Thanks to the precise approximation of RQ-VAE, we can represent a 256Ã—256 image as 8Ã—8 resolution of the feature map, and RQ-Transformer can efficiently reduce the computational costs. Consequently, our framework outperforms the existing AR models on various benchmarks of unconditional and conditional image generation. Our approach also has a significantly faster sampling speed than previous AR models to generate high-quality images. ",50.0,,,"Korea (Republic of),Korea (Republic of)",,,,4.0,,2025-02-17 11:09,,,,,,"Industry,Academia",,,,432.0,,,"Industry,Academia",,,,3218.0027181984096,Hardware,,,,
PolyCoder,Language,Code generation,Carnegie Mellon University (CMU),"Frank F. Xu, Uri Alon, Graham Neubig, Vincent J. Hellendoorn",2022-02-26,A Systematic Evaluation of Large Language Models of Code,https://arxiv.org/abs/2202.13169,510.0,SOTA improvement,"""In the C programming language, PolyCoder outperforms
all models including Codex""",2700000000.0,2.7B for largest model,1.1e+21,"""We use GPT-NeoX toolkit 11 to
train the model efficiently in parallel with 8 Nvidia RTX 8000 GPUs on a single machine. The wall
time used to train the largest 2.7B model is about 6 weeks""

8 * 130 TFLOP/s * 6 * 7 * 24 * 3600 * 0.3 (utilization) ~= 1.1e21",,"Code scraped from GitHub. ""249GB of code across 12 programming languages on a single machine.""",,"249GB

They trained on 39B tokens per Table 3, but I'm not sure how many epochs that is. May be <1. ",1000.0,6 weeks,NVIDIA Quadro RTX 8000,,Likely,"Large language models (LMs) of code have recently shown tremendous promise in completing code and synthesizing code from natural language descriptions. However, the current state-of-the-art code LMs (e.g., Codex (Chen et al., 2021)) are not publicly available, leaving many questions about their model and data design decisions. We aim to fill in some of these blanks through a systematic evaluation of the largest existing models: Codex, GPT-J, GPT-Neo, GPT-NeoX-20B, and CodeParrot, across various programming languages. Although Codex itself is not open-source, we find that existing open-source models do achieve close results in some programming languages, although targeted mainly for natural language modeling. We further identify an important missing piece in the form of a large open-source model trained exclusively on a multi-lingual corpus of code. We release a new model, PolyCoder, with 2.7B parameters based on the GPT-2 architecture, which was trained on 249GB of code across 12 programming languages on a single machine. In the C programming language, PolyCoder outperforms all models including Codex. Our trained models are open-source and publicly available at this https URL, which enables future research and application in this area.",,,Open weights (unrestricted),United States of America,,,,,,2025-05-28 18:01,,,,,,Academia,checked,,,,Unreleased,"MIT license for model weights
https://huggingface.co/NinedayWang/PolyCoder-2.7B

It seems that there is no pretraining code here: https://github.com/VHellendoorn/Code-LMs",Academia,,,FP16,,Hardware,NinedayWang,,,
ST-MoE,Language,Language modeling/generation,"Google,Google Brain,Google Research","Barret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yanping Huang, Jeff Dean, Noam Shazeer, William Fedus",2022-02-17,ST-MoE: Designing Stable and Transferable Sparse Expert Models,https://arxiv.org/abs/2202.08906v2,117.0,SOTA improvement,"""ST-MoE-32B improves the current state-of-the-art on the test server submissions for both ARC Easy (92.7 â†’ 94.8) and ARC Challenge (81.4 â†’ 86.5).""",269000000000.0,269B. it's called ST-MoE-32B because it's equivalent to a 32B dense model.,2.9000000000000005e+23,"The paper claims ""scaling a sparse model to 269B parameters, with a computational cost comparable to a 32B dense encoder-decoder"". If this is true for training cost, then 6*32e9*1.5e12 = 2.9e23",C4,"""The pre-training dataset used to train our Sparse 32B model is a mix of C4 (Raffel et al., 2019) and the dataset introduced in GLaM (Du et al., 2021).""",1500000000000.0,"""We pre-train for 1.5T tokens on a mixture of English-only C4 dataset (Raffel et al., 2019) and the dataset from GLaM (Du et al., 2021) summarized in Appendix E""
",,,,,Likely,"Scale has opened new frontiers in natural language processing -- but at a high cost. In response, Mixture-of-Experts (MoE) and Switch Transformers have been proposed as an energy efficient path to even larger and more capable language models. But advancing the state-of-the-art across a broad set of natural language tasks has been hindered by training instabilities and uncertain quality during fine-tuning. Our work focuses on these issues and acts as a design guide. We conclude by scaling a sparse model to 269B parameters, with a computational cost comparable to a 32B dense encoder-decoder Transformer (Stable and Transferable Mixture-of-Experts or ST-MoE-32B). For the first time, a sparse model achieves state-of-the-art performance in transfer learning, across a diverse set of tasks including reasoning (SuperGLUE, ARC Easy, ARC Challenge), summarization (XSum, CNN-DM), closed book question answering (WebQA, Natural Questions), and adversarially constructed tasks (Winogrande, ANLI R3).",0.84,,Unreleased,"United States of America,United States of America,Multinational,United States of America,Canada,Switzerland",,,,,,2025-05-28 16:10,,,,1000000.0,""" We use 1M tokens per batch""","Industry,Industry,Industry",,,,,Open source,"Apache License 2.0
Code for our models is available at https://github.com/tensorflow/mesh/blob/master/mesh_tensorflow/transformer/moe.py","Industry,Industry,Industry",,,BF16,,Operation counting,,,,10
Midjourney V1,Image generation,Image generation,Midjourney,,2022-02-15,,,,"Significant use,Historical significance","Significant historical usage and controversy as one of the first generative AI art models. For example:
https://www.nytimes.com/2022/09/02/technology/ai-artificial-intelligence-artists.html",,,,,Unspecified unreleased,,,,,,,,Unknown,,,,Hosted access (no API),United States of America,,,,,,2025-01-06 13:27,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
MuZero VP9,Video,Video compression,DeepMind,"Amol Mandhane, Anton Zhernov, Maribeth Rauh, Chenjie Gu, Miaosen Wang, Flora Xue, Wendy Shang, Derek Pang, Rene Claus, Ching-Han Chiang, Cheng Chen, Jingning Han, Angie Chen, Daniel J. Mankowitz, Jackson Broshear, Julian Schrittwieser, Thomas Hubert, Oriol Vinyals, Timothy Mann",2022-02-14,MuZero with Self-competition for Rate Control in VP9 Video Compression,https://arxiv.org/abs/2202.06626,38.0,,,,,,"Definitely < 1e23 FLOP: ""All experiments in this paper are run using Google Cloud TPUs [Google, 2018]. The learner processes use two 3rd generation TPUs, and the actor processes use four 2nd generation TPUs for 15 hours.""",,,,,,,,,Unknown,"Video streaming usage has seen a significant rise as entertainment, education, and business increasingly rely on online video. Optimizing video compression has the potential to increase access and quality of content to users, and reduce energy use and costs overall. In this paper, we present an application of the MuZero algorithm to the challenge of video compression. Specifically, we target the problem of learning a rate control policy to select the quantization parameters (QP) in the encoding process of libvpx, an open source VP9 video compression library widely used by popular video-on-demand (VOD) services. We treat this as a sequential decision making problem to maximize the video quality with an episodic constraint imposed by the target bitrate. Notably, we introduce a novel self-competition based reward mechanism to solve constrained RL with variable constraint satisfaction difficulty, which is challenging for existing constrained RL methods. We demonstrate that the MuZero-based rate control achieves an average 6.28% reduction in size of the compressed videos for the same delivered video quality level (measured as PSNR BD-rate) compared to libvpx's two-pass VBR rate control policy, while having better constraint satisfaction behavior.",,,Unreleased,United Kingdom of Great Britain and Northern Ireland,,,,,,2025-06-16 10:42,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
LaMDA,Language,Language modeling,Google,"Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Kathleen Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak, Ed Chi, Quoc Le",2022-02-10,LaMDA: Language Models for Dialog Applications,https://arxiv.org/abs/2201.08239,1375.0,Historical significance,,137000000000.0,"""LaMDA is a family of Transformer-based neural language models specialized for dialog, which have up to 137B parameters""",3.55e+23,"""The total FLOPS is 56.5% * 123 TFLOPS/s * 1024 chips * 57.7 days
= 3.55E+23""
From https://arxiv.org/pdf/2201.08239.pdf p.18
",Infiniset,"LaMDA's underlying dataset is called 'Infiniset', and besides the dialogue also involves common crawl, wikipedia, a mixture of english and non-english web documents, and data from programming-related sites (so LaMDA models can also dabble in code).",1560000000000.0,"""and are pre-trained on 1.56T words of public dialog data and web text""",1385.0,57.7 days * 24,Google TPU v3,Self-supervised learning,Confident,"We present LaMDA: Language Models for Dialog Applications. LaMDA is a family of Transformer-based neural language models specialized for dialog, which have up to 137B parameters and are pre-trained on 1.56T words of public dialog data and web text. While model scaling alone can improve quality, it shows less improvements on safety and factual grounding. We demonstrate that fine-tuning with annotated data and enabling the model to consult external knowledge sources can lead to significant improvements towards the two key challenges of safety and factual grounding. The first challenge, safety, involves ensuring that the model's responses are consistent with a set of human values, such as preventing harmful suggestions and unfair bias. We quantify safety using a metric based on an illustrative set of human values, and we find that filtering candidate responses using a LaMDA classifier fine-tuned with a small amount of crowdworker-annotated data offers a promising approach to improving model safety. The second challenge, factual grounding, involves enabling the model to consult external knowledge sources, such as an information retrieval system, a language translator, and a calculator. We quantify factuality using a groundedness metric, and we find that our approach enables the model to generate responses grounded in known sources, rather than responses that merely sound plausible. Finally, we explore the use of LaMDA in the domains of education and content recommendations, and analyze their helpfulness and role consistency.",,,Unreleased,United States of America,,,,1024.0,0.565,2025-05-16 10:30,,,,256000.0,"""All models were trained with 256K tokens per batch""",Industry,checked,,,1418240.0,Unreleased,,Industry,$229949.99,"""We used the Lingvo framework [94] for training and achieved 123 TFLOPS/sec with 56.5% FLOPS utilization""",,453306.7251313544,Hardware,,,,6
ProteinBERT,Biology,"Proteins,Protein generation","Hebrew University of Jerusalem,Ben-Gurion University of the Negev,Deep Trading","Nadav Brandes, Dan Ofer, Yam Peleg, Nadav Rappoport, Michal Linial",2022-02-10,ProteinBERT: a universal deep-learning model of protein sequence and function,https://academic.oup.com/bioinformatics/article/38/8/2102/6502274,386.0,SOTA improvement,"""ProteinBERT obtains near state-of-the-art performance, and sometimes exceeds it, on multiple benchmarks covering diverse protein properties (including protein structure, post-translational modifications and biophysical attributes)""",16000000.0,"""Altogether, it includes âˆ¼16M trainable parameters, making it substantially smaller than other protein language models""",6.5e+19,"""Pretraining speed on a single GPU (Nvidia Quadro RTX 5000) was 280 protein records per second. We trained the model for 28â€‰days over âˆ¼670M records""

28 * 24 * 3600 * 89 TFLOP/s * 0.3 (assumed utilization) = 6.5e19
https://www.wolframalpha.com/input?i=28+days+*+89+TFLOP%2Fs+*+0.3",UniRef90,,32000000001.0,"Number of proteins: 106,000,000
Average protein length: 300 amino acids

Total unique tokens = 106,000,000 Ã— 300 = 31,800,000,000 â‰ˆ 3.2e10 tokens",672.0,28 days,NVIDIA Quadro RTX 5000,Self-supervised learning,Confident,"Self-supervised deep language modeling has shown unprecedented success across natural language tasks, and has recently been repurposed to biological sequences. However, existing models and pretraining methods are designed and optimized for text analysis. We introduce ProteinBERT, a deep language model specifically designed for proteins. Our pretraining scheme combines language modeling with a novel task of Gene Ontology (GO) annotation prediction. We introduce novel architectural elements that make the model highly efficient and flexible to long sequences. The architecture of ProteinBERT consists of both local and global representations, allowing end-to-end processing of these types of inputs and outputs. ProteinBERT obtains near state-of-the-art performance, and sometimes exceeds it, on multiple benchmarks covering diverse protein properties (including protein structure, post-translational modifications and biophysical attributes), despite using a far smaller and faster model than competing deep-learning methods. Overall, ProteinBERT provides an efficient framework for rapidly training protein predictors, even with limited labeled data.",6.4,,Open weights (unrestricted),"Israel,Israel,United States of America",,,,1.0,,2025-05-30 12:55,,,,26008.0,"Supplementary materials: ""During pretraining we used batch sizes of 128, 64 or 32 for episodes of 128, 512 or 1,024 tokens, respectively"" Since they seem to be used in equal parts, taking geometric mean: ((128*128)*(64*512)*(32*1024))**(1/3) = 26,008","Academia,Academia,Industry",,,,,Open source,"MIT license

https://github.com/nadavbra/protein_bert","Academia,Academia,Industry",,,,254.28806247515055,Hardware,,,,
GPT-NeoX-20B,Language,Language modeling/generation,EleutherAI,"Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, Samuel Weinbach",2022-02-09,GPT-NeoX-20B: An Open-Source Autoregressive Language Model,https://arxiv.org/abs/2204.06745,688.0,Historical significance,,20000000000.0,,9.31627008e+22,Trained for 3 months on 96 A100s (according to correspondence with author). Let's say 0.4 utilization rate.,The Pile,,341173367965.0,"""In aggregate, the Pile consists of over 825GiB of raw text data""

Figure 4",2160.0,"see other notes
",NVIDIA A100 SXM4 40 GB,Self-supervised learning,Confident,"We introduce GPT-NeoX-20B, a 20 billion parameter autoregressive language model trained on the Pile, whose weights will be made freely and openly available to the public through a permissive license. It is, to the best of our knowledge, the largest dense autoregressive model that has publicly available weights at the time of submission. In this work, we describe \model{}'s architecture and training and evaluate its performance on a range of language-understanding, mathematics, and knowledge-based tasks. We find that GPT-NeoX-20B is a particularly powerful few-shot reasoner and gains far more in performance when evaluated five-shot than similarly sized GPT-3 and FairSeq models. We open-source the training and evaluation code, as well as the model weights, at this https URL.",1.4,GPT-NeoX-20B,Open weights (unrestricted),"Multinational,United States of America",,,,96.0,,2025-06-18 12:51,,,,3150000.0,"""we opt to use the same batch size as OpenAIâ€™s 175B modelâ€“approximately 3.15M tokens, or 1538 contexts of 2048 tokens each, and train for a total of 150,000 steps""",Research collective,,,,207360.0,Open source,Apache 2.0. training code: https://github.com/EleutherAI/gpt-neox ,Research collective,$184272.81,"Previously given as 0.3750, but no source found.",FP16,77269.91251696396,Hardware,,,,13
RETRO-7B,Language,Language modeling/generation,DeepMind,"Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego de Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero,Karen Simonyan, Jack W. Raeâ€¡, Erich Elsenâ€¡ and Laurent Sifre",2022-02-07,Improving language models by retrieving from trillions of tokens,https://arxiv.org/abs/2112.04426,817.0,SOTA improvement,"""Our largest model obtains state-of-the-art results on a range of downstream evaluation
datasets including Wikitext103""",7500000000.0,"""Retro provides a constant gain for models ranging from 150M to 7B parameters, and Retro can be improved at evaluation time by increasing the database size and the number of retrieved neighbours. """,1.68e+22,C=6ND = 6 * 7e9 * 400e9 = 1.7e22 ,WikiText-103,,419430400000.0,"""we train for 419,430,400,000 training tokens"" ~= 315B words.",,,,Self-supervised learning,Confident,"We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a 2 trillion token database, our Retrieval-Enhanced Transformer (RETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25Ã— fewer parameters. After fine-tuning, RETRO performance translates to downstream knowledge-intensive tasks such as question answering. RETRO combines a frozen Bert retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training. We typically train RETRO from scratch, yet can also rapidly RETROfit pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale.",,RETRO-7B,Unreleased,United Kingdom of Great Britain and Northern Ireland,,,,,,2025-06-18 12:49,,,,,,Industry,,,,,Unreleased,,Industry,,,,,Operation counting,,,,
AlphaCode,Language,Code generation,DeepMind,"Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, RÃ©mi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson d'Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, Oriol Vinyals",2022-02-02,Competition-Level Code Generation with AlphaCode,https://arxiv.org/abs/2203.07814,1013.0,SOTA improvement,,41100000000.0,41.1B. Table 3,1.63944e+23,"Figure 7 (a) shows a maximum training compute budget of approx 23000 TPU-days per model.
23000 days * 24 h/day * 3600 sec/h * 2.75e14 FLOP/s * 0.3 utilization = 1.64e23 FLOP","CodeContests,Unspecified unreleased","Looks like evaluation data is released but not pretraining data:

""We use large transformer language models to generate code, pre-training them
on selected GitHub code and fine-tuning on our curated set of competitive programming problems...
A core part of developing our system was ensuring that submissions are rigorously evaluated and
that evaluation problems are truly unseen during training, so difficult problems cannot be solved
by copying from the training set. Towards this goal, we release a new training and evaluation
competitive programming dataset, CodeContests""",,Appendix part A has answers for pretraining.,147.2,"Figure 7 (a) shows that the models were trained for around 23000 TPU-days. We know they trained on TPUv4s, and in appendix D.1 they say they have 3750 TPUv4 and TPUv4i. Assuming they trained only on the 3750 TPUv4s, that suggests 23000 / 3750 = 6.13 days, or 147.2 hours.",Google TPU v4,Self-supervised learning,Confident,"Programming is a powerful and ubiquitous problem-solving tool. Developing systems that can assist programmers or even generate programs independently could make programming more productive and
accessible, yet so far incorporating innovations in AI has proven challenging. Recent large-scale language models have demonstrated an impressive ability to generate code, and are now able to complete
simple programming tasks. However, these models still perform poorly when evaluated on more complex, unseen problems that require problem-solving skills beyond simply translating instructions into
code. For example, competitive programming problems which require an understanding of algorithms
and complex natural language remain extremely challenging. To address this gap, we introduce AlphaCode, a system for code generation that can create novel solutions to these problems that require deeper
reasoning. In simulated evaluations on recent programming competitions on the Codeforces platform,
AlphaCode achieved on average a ranking of top 54.3% in competitions with more than 5,000 participants. We found that three key components were critical to achieve good and reliable performance:
(1) an extensive and clean competitive programming dataset for training and evaluation, (2) large and
efficient-to-sample transformer-based architectures, and (3) large-scale model sampling to explore the
search space, followed by filtering based on program behavior to a small set of submissions.
",,,Unreleased,United Kingdom of Great Britain and Northern Ireland,,,,3750.0,,2025-06-18 12:47,,,,4718592.0,"2304 token sequences, 2048 batch size. 2304 * 2048 = 4718592

trained on 967B tokens and 205k steps. 967B/205k = 4717073, so seems they didn't do warmup",Industry,checked,,,,Unreleased,,Industry,,,BF16,1283001.2678531131,Hardware,,,,11
DARK,Biology,"Protein generation,Proteins",University College London (UCL),"Lewis Moffat, Shaun M. Kandathil, David T. Jones",2022-01-28,Design in the DARK: Learning Deep Generative Models for De Novo Protein Design,https://www.biorxiv.org/content/10.1101/2022.01.27.478087v1.full,25.0,,,,"""DARK3, uses a Transformer decoder with 12 layers, 12 heads, and a feedforward dimension of 768.""",9.7e+18,"""Rounding up to the nearest day, if we were to re-perform DARK from nothing to having a trained DARK3 it would take 12 days when parallelized across ten V100 GPUS. Of that time, model training constitutes just over 3 days and only requires 1 GPU""

3 * 24 * 3600 * 125 teraFLOP/s * 0.3 (utilization) = 9.7e18",,"Iteratively trained by generating synthetic data: ""To build this tool we develop a framework, called DARK, that trains the underlying generative model on an iteratively expanding set of synthetic sequences""",50000001.0,"500,000 sequences Ã— 100 amino acids = 50,000,000 data points",,,NVIDIA V100,Unsupervised,Speculative,"The design of novel protein sequences is providing paths towards the development of novel therapeutics and materials. At the forefront is the challenging field of de novo protein design, which looks to design protein sequences unlike those found in nature using general design methodologies. In this work, we develop a tool for de novo design, based on a deep generative sequence model, that rapidly samples novel protein sequences with diverse and ordered structures. To build this tool we develop a framework, called DARK, that trains the underlying generative model on an iteratively expanding set of synthetic sequences. The resulting model generalizes where models trained on natural sequences struggle and greatly improves on the efficiency of comparable sampling-based approaches. We further show how it can generate high quality candidates for de novo design problems and aid in the development of further novel design methods, in all, providing another step, amongst others, towards truly automated and intelligent protein design.",,,,United Kingdom of Great Britain and Northern Ireland,,,,,,2025-05-09 11:32,,,,,,Academia,,,,,,,Academia,,,,,Hardware,,,,
InstructGPT 175B,Language,Language modeling/generation,OpenAI,"Long Ouyang, Pamela Mishkin, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright,John Schulman Amanda Askell, Fraser Kelton Peter Welinder, Luke Miller Maddie Simens Paul Christiano,Ryan Lowe,Chong Zhang Jacob Hilton, Sandhini Agarwal Katarina Slama Alex Ray, Jan Leike",2022-01-27,Training language models to follow instructions with human feedback,https://arxiv.org/pdf/2203.02155,9228.0,"Historical significance,Highly cited",,175000000000.0,"""We train three model sizes (1.3B, 6B, and 175B parameters)""",3.19181e+23,"""training our 175B PPO-ptx model requires 60 petaflops/s-days, compared to 3,640 petaflops/s-days for GPT-3 (Brown et al., 2020)""

60/3640 = +1.65% to base model compute

base model was reported 3.14e+23 FLOP

3.14e+23 * 1.0165 = 319181000000000000000000",,"Page 1 of https://arxiv.org/pdf/2203.02155 says InstructGPT models were instruction fine-tuned from GPT-3, so I assume these models have the same knowledge cutoff dates as the GPT-3 models, which is December 31, 2019.",374000033207.0,"Table 6 - describes **number of prompts**

26584 + 6623 = 33207

This is added to GPT-3 dataset size.",,,,Self-supervised learning,Confident,"Making language models bigger does not inherently make them better at following a userâ€™s intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback (RLHF). We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.",,,API access,United States of America,GPT-3 175B (davinci),5,,,,2025-05-23 14:03,,,,,,Industry,checked,,,,Unreleased,"used to be accessible via API, now deprecated",Industry,,,,,Reported,,,,
InstructGPT 6B,Language,Language modeling/generation,OpenAI,"Long Ouyang, Pamela Mishkin, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright,John Schulman Amanda Askell, Fraser Kelton Peter Welinder, Luke Miller Maddie Simens Paul Christiano,Ryan Lowe,Chong Zhang Jacob Hilton, Sandhini Agarwal Katarina Slama Alex Ray, Jan Leike",2022-01-27,Training language models to follow instructions with human feedback,https://arxiv.org/abs/2203.02155,9228.0,"Historical significance,Highly cited",,6000000000.0,"""We train three model sizes (1.3B, 6B, and 175B parameters)""",,,,"Page 1 of https://arxiv.org/pdf/2203.02155 says InstructGPT models were instruction fine-tuned from GPT-3, so I assume these models have the same knowledge cutoff dates as the GPT-3 models, which is December 31, 2019.",,,,,,Self-supervised learning,Confident,"Making language models bigger does not inherently make them better at following a userâ€™s intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback (RLHF). We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.",,,API access,United States of America,GPT-3 6.7B,,,,,2025-06-02 10:21,,,,,,Industry,checked,,,,Unreleased,"used to be accessible via API, now deprecated",Industry,,,,,,,,,
InstructGPT 1.3B,Language,Language modeling/generation,OpenAI,"Long Ouyang, Pamela Mishkin, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright,John Schulman Amanda Askell, Fraser Kelton Peter Welinder, Luke Miller Maddie Simens Paul Christiano,Ryan Lowe,Chong Zhang Jacob Hilton, Sandhini Agarwal Katarina Slama Alex Ray, Jan Leike",2022-01-27,Training language models to follow instructions with human feedback,https://arxiv.org/abs/2203.02155,9228.0,"Historical significance,Highly cited",,1300000000.0,"""We train three model sizes (1.3B, 6B, and 175B parameters)""",,,,"Page 1 of https://arxiv.org/pdf/2203.02155 says InstructGPT models were instruction fine-tuned from GPT-3, so I assume these models have the same knowledge cutoff dates as the GPT-3 models, which is December 31, 2019.",,,,,,Self-supervised learning,Confident,"Making language models bigger does not inherently make them better at following a userâ€™s intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback (RLHF). We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.",,,API access,United States of America,GPT-3 XL,,,,,2025-06-02 10:21,,,,,,Industry,checked,,,,Unreleased,"used to be accessible via API, now deprecated",Industry,,,,,,,,,
AbLang (heavy sequences),Biology,"Proteins,Antibody property prediction,Protein or nucleotide language model (pLM/nLM)",University of Oxford,"Tobias H Olsen, Iain H Moal, Charlotte M Deane",2022-01-22,AbLang: an antibody language model for completing antibody sequences,https://academic.oup.com/bioinformaticsadvances/article/2/1/vbac046/6609807,99.0,SOTA improvement,"""AbLang restores residues more accurately and faster than a current state-of-the-art protein language model ESM-1b, emphasizing the benefits and potential of an antibody specific language model"" - SOTA improvement for a very specific task",355000000.0,"""The hyperparameters were selected to be similar to those used
in the RoBERTa paper (Liu et al., 2019).""

Liu et al., 2019 link: https://arxiv.org/pdf/1907.11692.pdf
""We begin by training RoBERTa following the BERTLARGE architecture (L = 24, H = 1024, A = 16, 355M parameters)""",,,Observed Antibody Space (OAS) database,,2290000001.0,"Heavy Chain: 14,126,724 sequences Ã— 160 residues = 2,260,275,840 datapoints
Light Chain: 187,068 sequences Ã— 160 residues = 29,930,880 datapoints
Total: 2,260,275,840 + 29,930,880 = 2,290,206,720 datapoints (2.29B)",,,,,Confident,"Motivation
General protein language models have been shown to summarize the semantics of protein sequences into representations that are useful for state-of-the-art predictive methods. However, for antibody specific problems, such as restoring residues lost due to sequencing errors, a model trained solely on antibodies may be more powerful. Antibodies are one of the few protein types where the volume of sequence data needed for such language models is available, e.g. in the Observed Antibody Space (OAS) database.

Results
Here, we introduce AbLang, a language model trained on the antibody sequences in the OAS database. We demonstrate the power of AbLang by using it to restore missing residues in antibody sequence data, a key issue with B-cell receptor repertoire sequencing, e.g. over 40% of OAS sequences are missing the first 15 amino acids. AbLang restores the missing residues of antibody sequences better than using IMGT germlines or the general protein language model ESM-1b. Further, AbLang does not require knowledge of the germline of the antibody and is seven times faster than ESM-1b.",20.0,,Open weights (unrestricted),United Kingdom of Great Britain and Northern Ireland,,,,,,2025-06-02 14:56,,,,,,Academia,,,,,Unreleased," BSD-3-Clause license
https://github.com/oxpig/AbLang

AbLang is a python package",Academia,,,,,,,,,
Japanese-GPT-1B,Language,Language modeling/generation,rinna,"Kei Sawada, Tianyu Zhao, Makoto Shing, Kentaro Mitsui, Akio Kaga, Yukiya Hono, Toshiaki Wakatsuki, Koh Mitsuda",2022-01-19,"This repository provides a 1.3B-parameter Japanese GPT model. The model was trained by rinna Co., Ltd.",https://huggingface.co/rinna/japanese-gpt-1b,,,,1300000000.0,,,,"Japanese CC-100,Japanese C4,Wikipedia (ja)","""The model was trained on Japanese C4, Japanese CC-100 and Japanese Wikipedia to optimize a traditional language modelling objective. It reaches around 14 perplexity on a chosen validation set from the same data.""",,,,,,,Confident,"Model architecture
A 24-layer, 2048-hidden-size transformer-based language model.

Training
The model was trained on Japanese C4, Japanese CC-100 and Japanese Wikipedia to optimize a traditional language modelling objective. It reaches around 14 perplexity on a chosen validation set from the same data.

Tokenization
The model uses a sentencepiece-based tokenizer. The vocabulary was first trained on a selected subset from the training data using the official sentencepiece training script, and then augmented with emojis and symbols.",,,Open weights (unrestricted),Japan,,,,,,2024-09-05 14:08,,,,,,Industry,,,,,,MIT for weights,Industry,,,,,,,,,
Detic,Vision,"Object detection,Image classification","Meta AI,University of Texas at Austin","Xingyi Zhou, Rohit Girdhar, Armand Joulin, Philipp KrÃ¤henbÃ¼hl, Ishan Misra",2022-01-07,Detecting Twenty-thousand Classes using Image-level Supervision,https://arxiv.org/abs/2201.02605,483.0,SOTA improvement,"""On open-vocabulary COCO, our method outperforms the previous state-of-the-art OVR-CNN [ 72 ] by 5 point with the same detector and data""",88000000.0,"from https://github.com/microsoft/Swin-Transformer Swin-B have 88M, 
from page 8 :  'Training our ResNet50 model takes âˆ¼ 22 hours on 8 V100 GPUs. The large 21K Swin-B model trains in âˆ¼ 24 hours on 32 GPUs.'",2.34399744e+19,"28.26e12* 32 * 24*3600*0.3 =2.34e19 = peak flops * num gpus * num seconds * assumed utilization rate
for Swin-B model from page 8 :  'Training our ResNet50 model takes âˆ¼ 22 hours on 8 V100 GPUs. The large 21K Swin-B model trains in âˆ¼ 24 hours on 32 GPUs.'","ImageNet21k,Conceptual Captions (CC3M),LVIS","table above section 5.1
""We evaluate Detic on the large-vocabulary object detection dataset LVIS [18 ]""
""Image-supervised data. We use two sources of image-supervised data: ImageNet-21K [10] and Conceptual Captions """,16900000.0,"14M + 1.5M + 1.2M + 100K + 100K = 16900000.0
table above section 5.1",24.0,"from page 8 :  'Training our ResNet50 model takes âˆ¼ 22 hours on 8 V100
GPUs. The large 21K Swin-B model trains in âˆ¼ 24 hours on 32 GPUs.'",NVIDIA V100,,Speculative," Current object detectors are limited in vocabulary size due to the small scale of detection datasets. Image classifiers, on the other hand, reason about much larger vocabularies, as their datasets are larger and easier to collect. We propose Detic, which simply trains the classifiers of a detector on image classification data and thus expands the vocabulary of detectors to tens of thousands of concepts. Unlike prior work, Detic does not need complex assignment schemes to assign image labels to boxes based on model predictions, making it much easier to implement and compatible with a range of detection architectures and backbones. Our results show that Detic yields excellent detectors even for classes without box annotations. It outperforms prior work on both open-vocabulary and long-tail detection benchmarks. Detic provides a gain of 2.4 mAP for all classes and 8.3 mAP for novel classes on the open-vocabulary LVIS benchmark. On the standard LVIS benchmark, Detic obtains 41.7 mAP when evaluated on all classes, or only rare classes, hence closing the gap in performance for object categories with few samples. For the first time, we train a detector with all the twenty-one-thousand classes of the ImageNet dataset and show that it generalizes to new datasets without finetuning. Code is available at \url{this https URL}. ",,,Open weights (unrestricted),"United States of America,United States of America",,,,32.0,,2025-05-28 16:11,,,,,,"Industry,Academia",,,,768.0,Open source,"Apache, models and code: https://github.com/facebookresearch/Detic","Industry,Academia",$191.45,,"FP32,FP16",19331.67955389363,Hardware,,,,
Vespa,Biology,"Proteins,Protein or nucleotide language model (pLM/nLM)",Technical University of Munich,"CÃ©line Marquet, Michael Heinzinger, Tobias Olenyi, Christian Dallago, Kyra Erckert, Michael Bernhofer, Dmitrii Nechaev & Burkhard Rost ",2021-12-30,Embeddings from protein language models predict conservation and variant effects,https://link.springer.com/article/10.1007/s00439-021-02411-y,,,,231000.0,"""(3) standard convolutional neural network (CNN; with two convolutional layers with a window size of 7, connected through ReLU activations; dropout rate of 0.25; 231 k free parameters)""",,,"ConSurf10k,Eff10k,PMD4k,DMS4,DMS39",ConSurf10k data set contained about 2.7 million samples,2800001.0,"ConSurf10k: 2.7 x 10^6 samples
Eff10k: 1.00737 x 10^5 samples
Total = 2.7 x 10^6 + 1.00737 x 10^5 = 2.800737 x 10^6 â‰ˆ 2.8 x 10^6 datapoints",,,,Supervised,Confident,"The emergence of SARS-CoV-2 variants stressed the demand for tools allowing to interpret the effect of single amino acid variants (SAVs) on protein function. While Deep Mutational Scanning (DMS) sets continue to expand our understanding of the mutational landscape of single proteins, the results continue to challenge analyses. Protein Language Models (pLMs) use the latest deep learning (DL) algorithms to leverage growing databases of protein sequences. These methods learn to predict missing or masked amino acids from the context of entire sequence regions. Here, we used pLM representations (embeddings) to predict sequence conservation and SAV effects without multiple sequence alignments (MSAs). Embeddings alone predicted residue conservation almost as accurately from single sequences as ConSeq using MSAs (two-state Mat- thews Correlation Coefficientâ€”MCCâ€”for ProtT5 embeddings of 0.596 Â± 0.006 vs. 0.608 Â± 0.006 for ConSeq). Inputting the conservation prediction along with BLOSUM62 substitution scores and pLM mask reconstruction probabilities into a simplistic logistic regression (LR) ensemble for Variant Effect Score Prediction without Alignments (VESPA) predicted SAV effect magnitude without any optimization on DMS data. Comparing predictions for a standard set of 39 DMS experiments to other methods (incl. ESM-1v, DeepSequence, and GEMME) revealed our approach as competitive with the state-of-the-art (SOTA) methods using MSA input. No method outperformed all others, neither consistently nor statistically significantly, independently of the performance measure applied (Spearman and Pearson correlation). Finally, we investigated binary effect predictions on DMS experiments for four human proteins. Overall, embedding-based methods have become competi- tive with methods relying on MSAs for SAV effect prediction at a fraction of the costs in computing/energy. Our method predicted SAV effects for the entire human proteome (~ 20 k proteins) within 40 min on one Nvidia Quadro RTX 8000. All methods and data sets are freely available for local and online execution through bioembeddings.com, https://github.com/ Rostlab/VESPA, and PredictProtein.",,,,Germany,ProteinBERT,,,,,2025-05-09 11:32,,,,,,Academia,,,,,,"All methods and data sets are freely available for local and online execution through bioembeddings.com, https://github.com/Rostlab/VESPA, and PredictProtein.",Academia,,,,,,,,,
ERNIE 3.0 Titan,Language,"Language modeling,Language modeling/generation","Baidu,Peng Cheng Laboratory","Shuohuan Wang, Yu Sun, Yang Xiang, Zhihua Wu, Siyu Ding, Weibao Gong, Shikun Feng",2021-12-23,ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training for Language Understanding and Generation,https://arxiv.org/abs/2112.12731,70.0,SOTA improvement,"""Empirical results show that the ERNIE 3.0 Titan outperforms the state-of-the-art models on 68 NLP datasets.""",260000000000.0,"""[We] developed... distributed training technology, including fine-grained parallelism, heterogeneous hardware-aware training, and fault tolerance mechanism to train the 260B model on both Nvidia V100 GPU and Ascend 910 NPU clusters.""
See also:
https://twitter.com/BaiduResearch/status/1468633977242243078?t=6q4zuLNdTSc4GUBe9OM5Aw&s=19",1.0421e+24,"The paper suggests that ERNIE 3.0 Titan uses more compute than GPT-3. This is consistent with the 6ND approximation.

C = 6ND = 6 (FLOP/param/token) * (260B params) * (668B tokens) = 1.0421*10^24 FLOP",ERNIE 3.0 Corpus,,668000000000.0,"""To ensure the success of the pre-training of ERNIE 3.0 Titan, we utilize the ERNIE 3.0 Corpus [ 2 ], a large-scale, wide-variety, and high-quality Chinese text corpora amounting to 4TB""

Assuming 167M words/tokens per GB",,,"NVIDIA Tesla V100 DGXS 32 GB,Huawei Ascend 910",,Confident,"Pre-trained language models have achieved state-of-the-art results in various Natural Language Processing (NLP) tasks. GPT-3 has shown that scaling up pre-trained language models can further exploit their enormous potential. A unified framework named ERNIE 3.0 was recently proposed for pre-training large-scale knowledge enhanced models and trained a model with 10 billion parameters. ERNIE 3.0 outperformed the state-of-the-art models on various NLP tasks. In order to explore the performance of scaling up ERNIE 3.0, we train a hundred-billion-parameter model called ERNIE 3.0 Titan with up to 260 billion parameters on the PaddlePaddle platform. Furthermore, we design a self-supervised adversarial loss and a controllable language modeling loss to make ERNIE 3.0 Titan generate credible and controllable texts. To reduce the computation overhead and carbon emission, we propose an online distillation framework for ERNIE 3.0 Titan, where the teacher model will teach students and train itself simultaneously. ERNIE 3.0 Titan is the largest Chinese dense pre-trained model so far. Empirical results show that the ERNIE 3.0 Titan outperforms the state-of-the-art models on 68 NLP datasets.",,,Hosted access (no API),"China,China",,,,1920.0,,2025-05-28 16:11,,"Peng Cheng Cloud Brain II, Paper on Ernie 3.0",,1048576.0,"""The maximum sequence length of context and
the memory length of language generation is 512 and 128, respectively""

In table 1, they use a global batch size of 512 when data parallelism is ""1"" and 2048 when DP is ""4"". Not sure I fully understand this part but I guess they'd use parallelism as much as possible given how they talk about it.

2048 * 512 = 1048576.","Industry,Academia",checked,,,,Unreleased,"The Ernie 3.0 Titan model was used in Ernie bot. Today, ERNIE has been widely deployed across finance, healthcare, insurance, equity, Internet, logistics, and other fields.

http://research.baidu.com/Blog/index-view?id=165","Industry,Academia",,,FP16,,Operation counting,,,,1
GLIDE,Image generation,"Image generation,Text-to-image",OpenAI,"Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, Mark Chen",2021-12-20,GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models,https://arxiv.org/abs/2112.10741,2795.0,Highly cited,,3500000000.0,"""Samples from a 3.5 billion parameter text-conditional diffusion model using classifier-free guidance are favored by human evaluators to those from DALL-E, even when the latter uses expensive CLIP reranking""",4.7e+22,"""Note that GLIDE was
trained with roughly the same training compute as DALL-E
but with a much smaller model (3.5 billion vs. 12 billion
parameters)""",DALL-E,,250000000.0,"Section 4:
""We train our model on the same dataset as DALL-E (Ramesh et al., 2021)""

This paper used 250M image-text pairs
https://arxiv.org/pdf/2102.12092.pdf",,,,,Speculative,"Diffusion models have recently been shown to generate high-quality synthetic images, especially when paired with a guidance technique to trade off diversity for fidelity. We explore diffusion models for the problem of text-conditional image synthesis and compare two different guidance strategies: CLIP guidance and classifier-free guidance. We find that the latter is preferred by human evaluators for both photorealism and caption similarity, and often produces photorealistic samples. Samples from a 3.5 billion parameter text-conditional diffusion model using classifier-free guidance are favored by human evaluators to those from DALL-E, even when the latter uses expensive CLIP reranking. Additionally, we find that our models can be fine-tuned to perform image inpainting, enabling powerful text-driven image editing. We train a smaller model on a filtered dataset and release the code and weights at this https URL.",,,Open weights (unrestricted),United States of America,,,,,,2025-06-02 09:54,,,,,,Industry,,,,,Unreleased,"MIT license
https://github.com/openai/glide-text2im",Industry,,,FP16,,Comparison with other models,,,,
MoE-1.1T,Language,"Language modeling/generation,Question answering",Meta AI,"Mikel Artetxe, Shruti Bhosale, Naman Goyal, Todor Mihaylov, Myle Ott, Sam Shleifer, Xi Victoria Lin, Jingfei Du, Srinivasan Iyer, Ramakanth Pasunuru, Giri Anantharaman, Xian Li, Shuohui Chen, Halil Akin, Mandeep Baines, Louis Martin, Xing Zhou, Punit Singh Koura, Brian O'Horo, Jeff Wang, Luke Zettlemoyer, Mona Diab, Zornitsa Kozareva, Ves Stoyanov",2021-12-20,Efficient Large Scale Language Modeling with Mixtures of Experts,https://arxiv.org/abs/2112.10684,157.0,,,1100000000000.0,,2.227e+22,Reported directly in paper. Authors calculate FLOPs analytically in appendix G,"English Wikipedia,CC-News,OPENWEBTEXT,CC-Stories,CC100","""We pretrain our models on a union of six Englishlanguage datasets, including the five datasets used to pretrain RoBERTa (Liu et al., 2019) and the
English subset of CC100, totalling 112B tokens corresponding to 453GB:
BookCorpus (Zhu et al., 2019) consists of more
than 10K unpublished books (4GB);
â€¢ English Wikipedia, excluding lists, tables and
headers (12GB);
â€¢ CC-News (Nagel, 2016) contains 63 millions English news articles crawled between September
2016 and February 2019 (76GB);
â€¢ OpenWebText (Gokaslan and Cohen, 2019), an
open source recreation of the WebText dataset
used to train GPT-2 (38GB);
â€¢ CC-Stories (Trinh and Le, 2018) contains a subset of CommonCrawl data filtered to match the
story-like style of Winograd schemas (31GB);
â€¢ English CC100 (Wenzek et al., 2020), a dataset
extracted from CommonCrawl snapshots between January 2018 and December 2018, filtered
to match the style of Wikipedia (292GB)""",112000000000.0,"112B tokens, or 84B words at 0.75 English words/token. 
""We pretrain our models on a union of six Englishlanguage datasets, including the five datasets used to pretrain RoBERTa (Liu et al., 2019) and the
English subset of CC100, totalling 112B tokens""",,,NVIDIA A100,,Confident,"Mixture of Experts layers (MoEs) enable efficient scaling of language models through conditional computation. This paper presents a detailed empirical study of how autoregressive MoE language models scale in comparison with dense models in a wide range of settings: in- and out-of-domain language modeling, zero- and few-shot priming, and full-shot fine-tuning. With the exception of fine-tuning, we find MoEs to be substantially more compute efficient. At more modest training budgets, MoEs can match the performance of dense models using âˆ¼4 times less compute. This gap narrows at scale, but our largest MoE model (1.1T parameters) consistently outperforms a compute-equivalent dense model (6.7B parameters). Overall, this performance gap varies greatly across tasks and domains, suggesting that MoE and dense models generalize differently in ways that are worthy of future study. We make our code and models publicly available for research use.",2.68,,,United States of America,,,,,,2024-11-10 18:29,,,,,,Industry,,,,,,,Industry,,,,,Reported,,,,
Fairseq-dense 13B,Language,Language modeling/generation,Meta AI,"Mikel Artetxe, Shruti Bhosale, Naman Goyal, Todor Mihaylov, Myle Ott, Sam Shleifer, Xi Victoria Lin, Jingfei Du, Srinivasan Iyer, Ramakanth Pasunuru, Giri Anantharaman, Xian Li, Shuohui Chen, Halil Akin, Mandeep Baines, Louis Martin, Xing Zhou, Punit Singh Koura, Brian O'Horo, Jeff Wang, Luke Zettlemoyer, Mona Diab, Zornitsa Kozareva, Ves Stoyanov",2021-12-20,Efficient Large Scale Language Modeling with Mixtures of Experts,https://arxiv.org/abs/2112.10684,157.0,,,13000000000.0,,3.267e+22,Table 1,,"""We pretrain our models on a union of six Englishlanguage datasets, including the five datasets used to pretrain RoBERTa (Liu et al., 2019) and the
English subset of CC100, totalling 112B tokens corresponding to 453GB:
BookCorpus (Zhu et al., 2019) consists of more
than 10K unpublished books (4GB);
â€¢ English Wikipedia, excluding lists, tables and
headers (12GB);
â€¢ CC-News (Nagel, 2016) contains 63 millions English news articles crawled between September
2016 and February 2019 (76GB);
â€¢ OpenWebText (Gokaslan and Cohen, 2019), an
open source recreation of the WebText dataset
used to train GPT-2 (38GB);
â€¢ CC-Stories (Trinh and Le, 2018) contains a subset of CommonCrawl data filtered to match the
story-like style of Winograd schemas (31GB);
â€¢ English CC100 (Wenzek et al., 2020), a dataset
extracted from CommonCrawl snapshots between January 2018 and December 2018, filtered
to match the style of Wikipedia (292GB)""",112000000000.0,"112B tokens, or 84B words at 0.75 English words/token. 
""We pretrain our models on a union of six Englishlanguage datasets, including the five datasets used to pretrain RoBERTa (Liu et al., 2019) and the English subset of CC100, totalling 112B tokens""
...
""All models are trained for 300B tokens with a sequence length of 2048 tokens.""",,,NVIDIA A100,,Likely,"Mixture of Experts layers (MoEs) enable efficient scaling of language models through conditional computation. This paper presents a detailed empirical study of how autoregressive MoE language models scale in comparison with dense models in a wide range of settings: in- and out-of-domain language modeling, zero- and few-shot priming, and full-shot fine-tuning. With the exception of fine-tuning, we find MoEs to be substantially more compute efficient. At more modest training budgets, MoEs can match the performance of dense models using âˆ¼4 times less compute. This gap narrows at scale, but our largest MoE model (1.1T parameters) consistently outperforms a compute-equivalent dense model (6.7B parameters). Overall, this performance gap varies greatly across tasks and domains, suggesting that MoE and dense models generalize differently in ways that are worthy of future study. We make our code and models publicly available for research use.",2.68,,Open weights (unrestricted),United States of America,,,,,,2025-02-17 11:11,,,,,,Industry,,,,,Unreleased,"data not licensed/released: https://github.com/facebookresearch/fairseq/blob/main/examples/moe_lm/data_card.md

repo is MIT-licensed
https://github.com/facebookresearch/fairseq/blob/main/examples/moe_lm/README.md",Industry,,,,,Reported,,,,
LDM-1.45B,Image generation,"Image generation,Text-to-image","Heidelberg University,Runway","Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, BjÃ¶rn Ommer",2021-12-20,High-Resolution Image Synthesis with Latent Diffusion Models,https://arxiv.org/abs/2112.10752,10764.0,Highly cited,,1450000000.0,1.45B,,,LAION-400M,,400000000.0,400M image-text pairs,,,NVIDIA A100,,Confident,"By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at this https URL.",0.66,,Open weights (unrestricted),"Germany,United States of America",,,,,,2025-01-17 15:09,,,,,,"Academia,Industry",,,,,Open source,MIT: https://github.com/CompVis/latent-diffusion/blob/main/LICENSE,"Academia,Industry",,,,,,,,,
XGLM-7.5B,Language,"Translation,Question answering,Language modeling/generation","Meta AI,Facebook AI Research","Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O'Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov, Xian Li",2021-12-20,Few-shot Learning with Multilingual Language Models,https://arxiv.org/abs/2112.10668,245.0,SOTA improvement,"""Our largest model (XGLM7.5B) sets a new state of the art performance for few-shot learning in more than 20 representative languages (including medium- and low-resource languages) for the tasks of commonsense reasoning, natural language inference and machine translation.""",7500000000.0,"""Our largest model with 7.5 billion parameters sets new state of the art""",2.25e+22,"""The XGLM 7.5B model was trained on 256 A100 GPUs for about 3 weeks, at a speed of 311.6k words per second""

256 * 312 teraFLOP/s * 21 * 24 * 3600 * 0.3 utilization assumption ~= 4.3e22

also, it was trained for 500B tokens. Using Compute = 6ND, we have
6 * 500B * 7.5B = 2.25e22

311k tokens per second * 7.5B params * 6 is 1.35e16 FLOP/s. divide that by 312 teraFLOP/s, which is A100 peak compute, gets 43, suggesting low utilization (17%) of the 256-GPU cluster, or somewhat higher if there's more than one token per word. So I'll use the 6ND number.","Subset of CC100-XL,CC100-XL,Common Crawl","*they built a closed dataset based on open Common Crawl ""We extend the pipeline used for mining the CC100 corpus (Conneau et al., 2020; Wenzek et al., 2020) to generate CC100-XL, a significantly larger multilingual dataset covering 68 Common Crawl (CC) snapshots (from Summer 2013 to March/April 2020) and 134 languages.",1740000000.0,"Training Data. Our models are trained on a static multilingual corpus extracted from CommonCrawl, with English text comprising 32.6% of the total number of tokens corresponding to 163B tokens.
163B / 0.326 = 500B total

Note that this dataset is sampled from the much larger CC100-XL, outlined in Appendix F and here: https://huggingface.co/facebook/xglm-7.5B#training-data-statistics

The huggingface link sums to 1.64T tokens, while the Data Card in the appendix claims 1.9T tokens.",504.0,"appendix A : ""The XGLM 7.5B model was trained on 256 A100 GPUs for about 3 weeks, at a speed of 311.6k words per second""",NVIDIA A100,Self-supervised learning,Confident,"Large-scale generative language models such as GPT-3 are competitive few-shot learners. While these models are known to be able to jointly represent many different languages, their training data is dominated by English, potentially limiting their cross-lingual generalization. In this work, we train multilingual generative language models on a corpus covering a diverse set of languages, and study their few- and zero-shot learning capabilities in a wide range of tasks. Our largest model with 7.5 billion parameters sets new state of the art in few-shot learning in more than 20 representative languages, outperforming GPT-3 of comparable size in multilingual commonsense reasoning (with +7.4% absolute accuracy improvement in 0-shot settings and +9.4% in 4-shot settings) and natural language inference (+5.4% in each of 0-shot and 4-shot settings). On the FLORES-101 machine translation benchmark, our model outperforms GPT-3 on 171 out of 182 directions with 32 training examples, while surpassing the official supervised baseline in 45 directions. We conduct an in-depth analysis of different multilingual prompting approaches, showing in particular that strong few-shot learning performance across languages can be achieved via cross-lingual transfer through both templates and demonstration examples. Finally, we evaluate our models in social value tasks such as hate speech detection in five languages and find it has limitations similar to comparable sized GPT-3 models. ",1.0,,Open weights (non-commercial),"United States of America,United States of America",,,,256.0,,2025-05-09 11:32,,,,,,"Industry,Industry",,,,129024.0,Unreleased,"MIT license
https://github.com/facebookresearch/fairseq/tree/main/examples/xglm

https://github.com/facebookresearch/fairseq/blob/main/examples/xglm/model_card.md#primary-intended-use","Industry,Industry",$104152.23,,,206287.25531193183,"Operation counting,Hardware",,,,
Contriever,Language,Language modeling,"Meta AI,University College London (UCL),PSL University,UniversitÃ© Grenoble Alpes","Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, Edouard Grave",2021-12-16,Unsupervised Dense Information Retrieval with Contrastive Learning,https://arxiv.org/abs/2112.09118,590.0,SOTA improvement,"""We observe that when
used as pre-training, contrastive learning leads to strong performance: contriever obtains the best results
among dense bi-encoder methods for the nDCG@10, and is state-of-the-art for the recall@100 (improving the
average recall@100 from 65.0 to 67.1). This strong recall@100 performance can be further exploited by using
a cross-encoder2
to re-rank the retrieved documents: this leads to the state-of-the-art on 8 datasets of the
BEIR benchmark for the nDCG@10, as well as on average""",110000000.0,"Based on BERT base, which had 110m params.

""We initialize the network with the publicly available BERT base uncased model.""",1.57e+20,"Pre-training:
""We use the random cropping data augmentation, with documents of 256 tokens... batch size of 2,048 and 500,000 steps""
256 * 2048 * 500k * 100M * 6 = 1.57e20

Fine-tuning looks unlikely to move final sum much beyond this.","Wikipedia,CCNet","""Documents are simply random piece of text sampled from a mix between Wikipedia and CCNet data """,,,,,,,Likely,"Recently, information retrieval has seen the emergence of dense retrievers, using neural networks, as an alternative to classical sparse methods based on term-frequency. These models have obtained state-of-the-art results on datasets and tasks where large training sets are available. However, they do not transfer well to new applications with no training data, and are outperformed by unsupervised term-frequency methods such as BM25. In this work, we explore the limits of contrastive learning as a way to train unsupervised dense retrievers and show that it leads to strong performance in various retrieval settings. On the BEIR benchmark our unsupervised model outperforms BM25 on 11 out of 15 datasets for the Recall@100. When used as pre-training before fine-tuning, either on a few thousands in-domain examples or on the large MS~MARCO dataset, our contrastive model leads to improvements on the BEIR benchmark. Finally, we evaluate our approach for multi-lingual retrieval, where training data is even scarcer than for English, and show that our approach leads to strong unsupervised performance. Our model also exhibits strong cross-lingual transfer when fine-tuned on supervised English data only and evaluated on low resources language such as Swahili. We show that our unsupervised models can perform cross-lingual retrieval between different scripts, such as retrieving English documents from Arabic queries, which would not be possible with term matching methods.",,,Open weights (non-commercial),"United States of America,United Kingdom of Great Britain and Northern Ireland,France,France",BERT-Large,,actually BERT base,,,2025-02-17 11:11,,,,,,"Industry,Academia,Academia,Academia",,,,,Open (non-commercial),non-commercial for weights/code: https://github.com/facebookresearch/contriever/blob/main/LICENSE,"Industry,Academia,Academia,Academia",,,,,Operation counting,,,,
LongT5,Language,Text summarization,Google Research,"Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung, Yinfei Yang",2021-12-15,LongT5: Efficient Text-To-Text Transformer for Long Sequences,https://arxiv.org/abs/2112.07916,257.0,SOTA improvement,"from abstract: ""We are able to achieve state-of-the-art results on several summarization tasks and outperform the original T5 models on question answering tasks.""",3000000000.0,3B from section 4.1,,"architecture is sparse so we cannot use 6ND method,
from 3.1.1 ""we simply replace the encoder
self-attention operation in T5 with a sparse sliding-
window local attention operation following the im-
plementation in ETC ""
at the end of section 3.1.2 there is information about 
complexity O(l(r + l/k)) of local attention
from 4.1.1 ""We pre-train LongT5 models for 1M steps on
4096 input sequence length and 910 output se-
quence length.
batch size is 128 (from 4.1 configurations section)
so with l = 4096, k = 16, r = 127, 
so l(r+l/k) = 1568768, but we are not sure about constant.

if normal attention have complexity O(l^2), and l^2 = 16777216
16777216/1568768 = 10.7
We can try to estimate that LongT5 would have 10 times less compute that normal architecture.",C4,"from 4.1.1 ""The same as T5.1.1, we pre-train LongT5 only on the C4 dataset
(Raffel et al., 2019b), and we do not apply dropout during pre-training.""",200000000000.0,"size of C4, from https://huggingface.co/datasets/c4 , C4 dataset is a collection of about 750GB of English-language text
200M word/GB * 4/3 token/word * 750GB = 200000000000 tokens

Actual tokens seen:
1M steps * (4096 input len + 910 output len) * 128 batch size = 641B tokens, so around 3.2 epochs.",,,Google TPU v3,,Confident,"Recent work has shown that either (1) increasing the input length or (2) increasing model size can improve the performance of Transformer-based neural models. In this paper, we present a new model, called LongT5, with which we explore the effects of scaling both the input length and model size at the same time. Specifically, we integrated attention ideas from long-input transformers (ETC), and adopted pre-training strategies from summarization pre-training (PEGASUS) into the scalable T5 architecture. The result is a new attention mechanism we call {\em Transient Global} (TGlobal), which mimics ETC's local/global attention mechanism, but without requiring additional side-inputs. We are able to achieve state-of-the-art results on several summarization tasks and outperform the original T5 models on question answering tasks.",3.2,,Open weights (unrestricted),"Multinational,United States of America,Canada,Switzerland",,,,128.0,,2024-11-01 10:04,,,,,,Industry,,,,,Open source,"Apache 2.0: https://github.com/google-research/longt5
train code: https://github.com/google-research/longt5/blob/master/longt5/tasks.py ",Industry,,,,56735.31215774296,,,,,
GLaM,Language,"Language modeling/generation,Question answering",Google,"Nan Du, Yanping Huang, Andrew M. Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, Barret Zoph, Liam Fedus, Maarten Bosma, Zongwei Zhou, Tao Wang, Yu Emma Wang, Kellie Webster, Marie Pellat, Kevin Robinson, Kathleen Meier-Hellstern, Toju Duke, Lucas Dixon, Kun Zhang, Quoc V Le, Yonghui Wu, Zhifeng Chen, Claire Cui",2021-12-13,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,https://arxiv.org/abs/2112.06905,597.0,SOTA improvement,"""As shown in Table 5, GLaM (64B/64E) is better than the dense model and outperforms the previous finetuned state-of-the-art (SOTA) on this dataset in the open-domain setting""",1200000000000.0,1.2 trillion parameters,3.6363112434e+23,"The network activates 96.6 billion parameters per token and trained for 600B tokens.

6 * 600B * 96.6B = 3.478e23

Digitizing figure 4 (d) indicates 139.67 TPU-years of training. 
2.75e14 * 139.67 * 365.25 * 24 * 3600 * 0.3 = 3.636e23

Since these are close, we will use the 6NC estimate and derive hardware utilization from the training time information.

Later they say they measured 326W power usage per chip, which could maybe be used to estimate utilization.","Wikipedia,GLaM dataset","""To train our model, we build a high-quality dataset of 1.6 trillion tokens that are representative of a wide range of natural language use cases. Web pages constitute the vast quantity of data in our unlabeled dataset. However, their quality ranges from professional writing to low-quality comment and forum pages.""",600000000000.0,"The dataset is made of 1.6 trillion tokens, but later in the paper they say they only train the largest model for 600b tokens. 600b / 0.75 words/token = 800b words.

""The complete GLaM training using 600B tokens consumes only 456 MWh and emits 40.2 net tCO2e.""",1366.0,"Note that they give several energy estimates. Use the complete training figures for 600B tokens, not the GPT-3 comparison values with 280B tokens.

""326W measured system power per TPU-v4 chip""
""The complete GLaM training using 600B tokens consumes only
456 MWh""
1024 TPU v4 chips
(456 MWh) / (326W/chip * 1024 chips) = 1366 hours",Google TPU v4,,Confident,"Scaling language models with more data, compute and parameters has driven significant progress in natural language processing. For example, thanks to scaling, GPT-3 was able to achieve strong results on in-context learning tasks. However, training these large dense models requires significant amounts of computing resources. In this paper, we propose and develop a family of language models named GLaM (Generalist Language Model), which uses a sparsely activated mixture-of-experts architecture to scale the model capacity while also incurring substantially less training cost compared to dense variants. The largest GLaM has 1.2 trillion parameters, which is approximately 7x larger than GPT-3. It consumes only 1/3 of the energy used to train GPT-3 and requires half of the computation flops for inference, while still achieving better overall zero-shot and one-shot performance across 29 NLP tasks.",,,Unreleased,United States of America,,,,1024.0,0.287,2025-06-10 15:42,,,,1000000.0,"""We use a maximum sequence
length of 1024 tokens, and pack each input example to have
up to 1 million tokens per batch.""",Industry,checked,,,1398784.0,Unreleased,,Industry,$541437.42,"6ND: 6 * 600B * 96.6B = 3.478e23
Hardware: Digitizing figure 4 (d) indicates 139.67 TPU-years of training. 2.75e14 * 139.67 * 365.25 * 24 * 3600 = 1.212e24 at full utilization
Implies utilization of 0.287",BF16,350743.0055524121,"Operation counting,Hardware",,,,4
Gopher (280B),Language,"Language modeling,Question answering",DeepMind,"Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d'Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, Geoffrey Irving",2021-12-08,"""Scaling Language Models: Methods, Analysis & Insights from Training Gopher""",https://arxiv.org/abs/2112.11446,1122.0,SOTA improvement,"""These models are evaluated on 152 diverse tasks, achieving state-of-the-art performance across the majority""",280000000000.0,"Language modelling provides a step towards intelligent communication systems by harnessing large repositories of written human knowledge to better predict and understand the world. In this paper, we present an analysis of Transformer-based language model performance across a wide range of model scales -- from models with tens of millions of parameters up to a 280 billion parameter model called Gopher. These models are evaluated on 152 diverse tasks, achieving state-of-the-art performance across the majority. Gains from scale are largest in areas such as reading comprehension, fact-checking, and the identification of toxic language, but logical and mathematical reasoning see less benefit. We provide a holistic analysis of the training dataset and model's behaviour, covering the intersection of model scale with bias and toxicity. Finally we discuss the application of language models to AI safety and the mitigation of downstream harms.",6.31e+23,"Table A26
6.31E+08 Train PFLOPs",MassiveTex,,300000000000.0,"""We train all models for 300 billion tokens with a 2048 token context window, using the Adam (Kingma and Ba, 2014) optimiser.""

1 token ~ 0.75 words",920.0,"""We trained Gopher for 920 hours in November and December 2020 in Googleâ€™s Georgia datacentre. The PUE of the datacenter at this time was 1.08; the net tCO2e per MWh in October 2020 was 0.33. Using an estimate of 283W drawn per chip, this leads to a total of 380 net tCO2e""",Google TPU v3,Self-supervised learning,Confident,"We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a 2 trillion token database, our Retrieval-Enhanced Transformer (RETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25Ã— fewer parameters. After fine-tuning, RETRO performance translates to downstream knowledge-intensive tasks such as question answering. RETRO combines a frozen Bert retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training. We typically train RETRO from scratch, yet can also rapidly RETROfit pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale.",1.0,Gopher (280B),Unreleased,United Kingdom of Great Britain and Northern Ireland,,,,4096.0,0.378,2025-05-29 11:00,,,,6000000.0,"Table 1. ""Furthermore, we increase Gopherâ€™s batch size from three to six million tokens per batch during training""",Industry,,,,3768320.0,Unreleased,,Industry,$616611.14,"FLOPS used to train model (table A26): 6.31e23 
FLOPS, per GPU-hours at 100% utilization: 920 * 4096 * 3600 * 1.23e14 =  1.669e24
6.31e23 / 1.669e24 = 0.3780",BF16,1815813.0260876147,Reported,,,,2
Student of Games,Games,"Chess,Go,Poker",DeepMind,"Martin Schmid, Matej Moravcik, Neil Burch, Rudolf Kadlec, Josh Davidson, Kevin Waugh, Nolan Bard, Finbarr Timbers, Marc Lanctot, Zach Holland, Elnaz Davoodi, Alden Christianson, Michael Bowling",2021-12-06,Player of Games,https://arxiv.org/abs/2112.03178,13.0,SOTA improvement,"""Player of Games reaches strong performance in chess and Go, beats the strongest openly available agent in heads-up no-limit Texas hold'em poker (Slumbot), and defeats the state-of-the-art agent in Scotland Yard""",,,3.667927300468287e+22,"""We trained a version of AlphaZero using its original settings in chess and Go, e.g. , using 800 MCTS simulations during training, with 3500 concurrent actors each on a single TPUv4, for a total of 800k training steps. SOG was trained using a similar amount of TPU resources.""",,,,,,,,,Speculative,"Games have a long history as benchmarks for progress in artificial intelligence. Approaches using search and learning produced strong performance across many perfect information games, and approaches using game-theoretic reasoning and learning demonstrated strong performance for specific imperfect information poker variants. We introduce Student of Games, a general-purpose algorithm that unifies previous approaches, combining guided search, self-play learning, and game-theoretic reasoning. Student of Games achieves strong empirical performance in large perfect and imperfect information games -- an important step towards truly general algorithms for arbitrary environments. We prove that Student of Games is sound, converging to perfect play as available computation and approximation capacity increases. Student of Games reaches strong performance in chess and Go, beats the strongest openly available agent in heads-up no-limit Texas hold'em poker, and defeats the state-of-the-art agent in Scotland Yard, an imperfect information game that illustrates the value of guided search, learning, and game-theoretic reasoning.",,,Unreleased,United Kingdom of Great Britain and Northern Ireland,,,,,,2024-11-01 10:04,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
CTR-BERT,Recommendation,Click-through rate prediction,Amazon,"Aashiq Muhamed, Iman Keivanloo, Sujan Perera, James Mracek, Yi Xu, Qingjun Cui, Santosh Rajagopalan, Belinda Zeng, Trishul Chilimb
",2021-12-06,CTR-BERT: Cost-effective knowledge distillation for billion-parameter teacher models,https://neurips2021-nlp.github.io/papers/20/CameraReady/camera_ready_final.pdf,37.0,,,70000000.0,"'CTR-BERTuses about 70 million parameters which can be trained on 8 A100 GPUs in less than a day (<1000 USD).
'",6.469632e+19,"flops = (8) * (312 * 10**12) * (24 * 3600) * (0.3)
(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)

'CTR-BERT
uses about 70 million parameters which can be trained on 8 A100 GPUs in less than a day (<1000
USD).
'",,"citations: '
Our CTR dataset is sampled from online traffic and is different from existing public CTR datasets in
that it comprises of text features in addition to numeric/categorical features.

We sample random train-test splits from 2020 online traffic. As

The train-test splits are sampled from 2021 online traffic and balanced the
same way as OOD data. The train set comprises 200 million data points and the test and validation
sets comprise 25 million points each'",,"more than 200M
citations: 'Our CTR dataset is sampled from online traffic and is different from existing public CTR datasets in
that it comprises of text features in addition to numeric/categorical features.

We sample random train-test splits from 2020 online traffic. As

The train-test splits are sampled from 2021 online traffic and balanced the
same way as OOD data. The train set comprises 200 million data points and the test and validation
sets comprise 25 million points each'",,,NVIDIA A100,,Likely,"While pre-trained large language models (LLM) like BERT have achieved state-of-the-art in several NLP tasks, their performance on tasks with additional grounding e.g. with numeric and categorical features is less studied. In this paper, we study the application of pre-trained LLM for Click-through-rate (CTR) prediction for product advertisement in e-commerce. This is challenging because the model needs to a) learn from language as well as tabular data features, b) maintain low-latency (<5 ms) at inference time, and c) adapt to constantly changing advertisement distribution.
We first show that scaling the pre-trained language model to 1.5 billion parameters significantly improves performance over conventional CTR baselines. We then present CTR-BERT, a novel lightweight cache-friendly factorized model for CTR prediction that consists of twin-structured BERT-like encoders for text with a mechanism for late fusion for text and tabular features. We train the CTR-BERT model using cross-architecture knowledge distillation (KD) and empirically study the interaction between KD and distribution shift in this setting, by a) experimenting with pre-training, distillation pre-finetuning and fine-tuning strategies b) factorizing features based on their distribution shift time scales, that allows the model to readily adapt and be re-trained. Finally, we show that CTR-BERT significantly
outperforms a traditional CTR baseline with a 2.3% relative ROC-AUC lift in
offline experiments and a 2% CTR lift in an online experiment",,,Unreleased,United States of America,,,,8.0,,2025-06-11 17:15,,,,,,Industry,,,,,Unreleased,,Industry,,,,6448.486867604916,Hardware,,,,
T-NLRv5 XXL,Language,,Microsoft,,2021-12-03,Microsoft : Turing-NLRv5 achieves new performance milestones,"https://www.marketscreener.com/quote/stock/MICROSOFT-CORPORATION-4835/news/Microsoft-Turing-NLRv5-achieves-new-performance-milestones-37207301/
https://www.microsoft.com/en-us/research/blog/efficiently-and-effectively-scaling-up-language-model-pretraining-for-best-language-representation-model-on-glue-and-superglue/",,SOTA improvement,"Highest score at SuperGLUE leaderboard version 2.0 in terms of CB (CommitmentBank-Av. F1/Accuracy; together with Ernie 3.0) and ReCoRD (Reading Comprehsention with Commonsense Reasoning-F1/Accuracy)
https://super.gluebenchmark.com/leaderboard/ ",5400000000.0,Table 1 of the blogpost,,,,"""We also leverage the training dataset and the data processing pipeline optimized for developing previous T-NLR releases, including DeBERTa and UniLM, as well as the implementation optimizations from other Microsoft pretraining research efforts, such as TUPE.  """,,,,,,,Confident,"As part of Microsoft AI at Scale(opens in new tab), the Turing family of NLP models are being used at scale across Microsoft to enable the next generation of AI experiences. Today, we are happy to announce that the latest Microsoft Turing model (T-NLRv5) is the state of the art at the top of SuperGLUE and GLUE leaderboards, further surpassing human performance and other models. Notably, T-NLRv5 first achieved human parity on MNLI and RTE on the GLUE benchmark, the last two GLUE tasks which human parity had not yet met. In addition, T-NLRv5 is more efficient than recent pretraining models, achieving comparable effectiveness with 50% fewer parameters and pretraining computing costs.",,,Unreleased,"United States of America,Multinational,India,Belgium",,,,,,2025-05-30 14:20,,,,,,Industry,,,,,Unreleased,"I cannot find repo with model weights or training code though they said:
""We will make T-NLRv5 and its capabilities available in the same way as with other Microsoft Turing models.
We will leverage its increased capabilities to further improve the execution of popular language tasks in A(opens in new tab)zure Cognitive Services(opens in new tab). Customers will automatically benefit from these.

Customers interested in using Turing models for their own specific task can submit a request to join the Turing Private Preview. Finally, we will make T-NLRv5 available to researchers for collaborative projects via the Microsoft Turing Academic Program.""",Industry,,,,,,,,,
GPT-2-Medium+Pixelfly,Language,"Language modeling,Text classification","Stanford University,""SambaNova Systems, Inc"",Peking University,Adobe,University at Buffalo","Tri Dao, Beidi Chen, Kaizhao Liang, Jiaming Yang, Zhao Song, Atri Rudra, Christopher RÃ©",2021-11-30,Pixelated Butterfly: Simple and Efficient Sparse training for Neural Network Models,https://arxiv.org/abs/2112.00029,69.0,,,203000000.0,203M (Table 6),1.2545400000000002e+19,6 FLOP / parameter / token * 203000000 parameters * 103000000 tokens * 100 epochs = 1.25454 Ã— 10^19 FLOP,WikiText-103,"""We show training GPT-2-Small, Medium and its Pixelfly model from scratch on a commonly used NLP benchmarking dataset, wikiText-103.""",,,,,NVIDIA V100,,Confident,"Overparameterized neural networks generalize well but are expensive to train. Ideally, one would like to reduce their computational cost while retaining their generalization benefits. Sparse model training is a simple and promising approach to achieve this, but there remain challenges as existing methods struggle with accuracy loss, slow training runtime, or difficulty in sparsifying all model components. The core problem is that searching for a sparsity mask over a discrete set of sparse matrices is difficult and expensive. To address this, our main insight is to optimize over a continuous superset of sparse matrices with a fixed structure known as products of butterfly matrices. As butterfly matrices are not hardware efficient, we propose simple variants of butterfly (block and flat) to take advantage of modern hardware. Our method (Pixelated Butterfly) uses a simple fixed sparsity pattern based on flat block butterfly and low-rank matrices to sparsify most network layers (e.g., attention, MLP). We empirically validate that Pixelated Butterfly is 3x faster than butterfly and speeds up training to achieve favorable accuracy--efficiency tradeoffs. On the ImageNet classification and WikiText-103 language modeling tasks, our sparse models train up to 2.5x faster than the dense MLP-Mixer, Vision Transformer, and GPT-2 medium with no drop in accuracy.",100.0,GPT-2-Medium+Pixelfly,Unreleased,"United States of America,United States of America,China,United States of America,United States of America",,,,,,2025-03-31 15:29,,,,,,"Academia,Industry,Academia,Industry,Academia",,,,,Open source,"apache for code (train and inference): https://github.com/HazyResearch/fly
train code: https://github.com/HazyResearch/fly/blob/master/src/train.py ","Academia,Industry,Academia,Industry,Academia",,,,,Operation counting,,,,
Quantized ADMM,Language,Language modeling,"Chinese University of Hong Kong (CUHK),Microsoft","Junhao Xu, Xie Chen, Shoukang Hu, Jianwei Yu, Xunying Liu, Helen Meng",2021-11-29,Low-bit Quantization of Recurrent Neural Network Language Models Using Alternating Direction Methods of Multipliers,https://arxiv.org/abs/2111.14836,9.0,,,,,,,,,,,,,,,Unknown,"The high memory consumption and computational costs of Recurrent neural network language models (RNNLMs) limit their wider application on resource constrained devices. In recent years, neural network quantization techniques that are capable of producing extremely low-bit compression, for example, binarized RNNLMs, are gaining increasing research interests. Directly training of quantized neural networks is difficult. By formulating quantized RNNLMs training as an optimization problem, this paper presents a novel method to train quantized RNNLMs from scratch using alternating direction methods of multipliers (ADMM). This method can also flexibly adjust the trade-off between the compression rate and model performance using tied low-bit quantization tables. Experiments on two tasks: Penn Treebank (PTB), and Switchboard (SWBD) suggest the proposed ADMM quantization achieved a model size compression factor of up to 31 times over the full precision baseline RNNLMs. Faster convergence of 5 times in model training over the baseline binarized RNNLM quantization was also obtained. Index Terms: Language models, Recurrent neural networks, Quantization, Alternating direction methods of multipliers.",50.0,Quantized ADMM,Unreleased,"Hong Kong,China,United States of America,Multinational,India,Belgium",,,,,,2024-09-08 22:42,,,,,,"Academia,Industry",,,,,Unreleased,,"Academia,Industry",,,,,,,,,
Transformer LM + MinSen,Language,Language modeling,Chinese University of Hong Kong (CUHK),"Junhao Xu, Shoukang Hu, Jianwei Yu, Xunying Liu, Helen Meng",2021-11-29,Mixed Precision of Quantization of Transformer Language Models for Speech Recognition,https://arxiv.org/abs/2112.11540,11.0,,,,,,,,,,,,,,,Unknown,"State-of-the-art neural language models represented by Transformers are becoming increasingly complex and expensive for practical applications. Low-bit deep neural network quantization techniques provides a powerful solution to dramatically reduce their model size. Current low-bit quantization methods are based on uniform precision and fail to account for the varying performance sensitivity at different parts of the system to quantization errors. To this end, novel mixed precision DNN quantization methods are proposed in this paper. The optimal local precision settings are automatically learned using two techniques. The first is based on a quantization sensitivity metric in the form of Hessian trace weighted quantization perturbation. The second is based on mixed precision Transformer architecture search. Alternating direction methods of multipliers (ADMM) are used to efficiently train mixed precision quantized DNN systems. Experiments conducted on Penn Treebank (PTB) and a Switchboard corpus trained LF-MMI TDNN system suggest the proposed mixed precision Transformer quantization techniques achieved model size compression ratios of up to 16 times over the full precision baseline with no recognition performance degradation. When being used to compress a larger full precision Transformer LM with more layers, overall word error rate (WER) reductions up to 1.7% absolute (18% relative) were obtained.",,Transformer LM + MinSen,Unreleased,"Hong Kong,China",,,,,,2024-09-08 22:43,,,,,,Academia,,,,,Unreleased,,Academia,,,,,,,,,
NÃœWA,"Multimodal,Vision,Image generation,Video,Language","Image generation,Video generation,Text-to-image,Text-to-video","Microsoft Research,Peking University","Chenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang, Daxin Jiang, Nan Duan",2021-11-24,NÃœWA: Visual Synthesis Pre-training for Neural visUal World creAtion,https://arxiv.org/abs/2111.12417,258.0,SOTA improvement,"""NÃœWA achieves state-of-the-art results on text-to-image generation, text-to-video generation, video prediction, etc""",870000000.0,Section 4.1,7.24598784e+21,"From AI Tracker:
""Compute cost: End of Sec 4.1: ""We pre-train on 64 A100 GPUs for two weeks"". 

Half precision FLOPs of A100: 312000000000000

64 gpus *312000000000000 FLOPs *0.3 utilization * 14 day* (24*60*60) seconds / day=7.245988e+21

","Conceptual Captions (CC3M),Moments in Time,VATEX",,,"we first pre-train N  ÌˆUWA on three
datasets: Conceptual Captions [22] for text-to-image (T2I)
generation, which includes 2.9M text-image pairs, Mo-
ments in Time [26] for video prediction (V2V), which in-
cludes 727K videos, and VATEX dataset [43] for text-to-
video (T2V) generation, which includes 241K text-video
pairs.",,,NVIDIA A100,Self-supervised learning,Confident,"This paper presents a unified multimodal pre-trained model called NÃœWA that can generate new or manipulate existing visual data (i.e., images and videos) for various visual synthesis tasks. To cover language, image, and video at the same time for different scenarios, a 3D transformer encoder-decoder framework is designed, which can not only deal with videos as 3D data but also adapt to texts and images as 1D and 2D data, respectively. A 3D Nearby Attention (3DNA) mechanism is also proposed to consider the nature of the visual data and reduce the computational complexity. We evaluate NÃœWA on 8 downstream tasks. Compared to several strong baselines, NÃœWA achieves state-of-the-art results on text-to-image generation, text-to-video generation, video prediction, etc. Furthermore, it also shows surprisingly good zero-shot capabilities on text-guided image and video manipulation tasks. Project repo is this https URL.",,,Unreleased,"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,China",,,,,,2025-06-18 12:44,,,,,,"Industry,Academia",,,,,Unreleased,https://github.com/microsoft/NUWA,"Industry,Academia",,,,,Hardware,,,,
Persia,Recommendation,Recommender system,"ETH Zurich,Kuaishou Technology","Xiangru Lian, Binhang Yuan, Xuefeng Zhu, Yulong Wang, Yongjun He, Honghuan Wu, Lei Sun, Haodong Lyu, Chengjun Liu, Xing Dong, Yiqiao Liao, Mingnan Luo, Congfei Zhang, Jingru Xie, Haonan Li, Lei Chen, Renjie Huang, Jianying Lin, Chengchun Shu, Xuezhong Qiu, Zhishan Liu, Dongying Kong, Lei Yuan, Hai Yu, Sen Yang, Ce Zhang, Ji Liu",2021-11-23,"Persia: An Open, Hybrid System Scaling Deep Learning-based Recommenders up to 100 Trillion Parameters",https://arxiv.org/abs/2111.05897,26.0,,,100000000000000.0,100 trillion,,,,,,,,,NVIDIA V100,,Confident,"Deep learning based models have dominated the current landscape of production recommender systems. Furthermore, recent years have witnessed an exponential growth of the model scale--from Google's 2016 model with 1 billion parameters to the latest Facebook's model with 12 trillion parameters. Significant quality boost has come with each jump of the model capacity, which makes us believe the era of 100 trillion parameters is around the corner. However, the training of such models is challenging even within industrial scale data centers. This difficulty is inherited from the staggering heterogeneity of the training computation--the model's embedding layer could include more than 99.99% of the total model size, which is extremely memory-intensive; while the rest neural network is increasingly computation-intensive. To support the training of such huge models, an efficient distributed training system is in urgent need. In this paper, we resolve this challenge by careful co-design of both the optimization algorithm and the distributed system architecture. Specifically, in order to ensure both the training efficiency and the training accuracy, we design a novel hybrid training algorithm, where the embedding layer and the dense neural network are handled by different synchronization mechanisms; then we build a system called Persia (short for parallel recommendation training system with hybrid acceleration) to support this hybrid training algorithm. Both theoretical demonstration and empirical study up to 100 trillion parameters have conducted to justified the system design and implementation of Persia. We make Persia publicly available (at this https URL) so that anyone would be able to easily train a recommender model at the scale of 100 trillion parameters.",,,Unreleased,"Switzerland,China",,,,,,2024-11-01 10:04,,,,,,"Academia,Industry",,,,,Open source,"https://github.com/PersiaML/PERSIA/blob/main/LICENSE

MIT code","Academia,Industry",,,,,,,,,
Florence,Vision,"Image captioning,Visual question answering,Image classification,Object detection",Microsoft,"Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, Ce Liu, Mengchen Liu, Zicheng Liu, Yumao Lu, Yu Shi, Lijuan Wang, JianFeng Wang, Bin Xiao, Zhen Xiao, Jianwei Yang, Michael Zeng, Luowei Zhou, Pengchuan Zhang",2021-11-22,Florence: A New Foundation Model for Computer Vision,https://arxiv.org/abs/2111.11432v1,758.0,"Historical significance,SOTA improvement",,893000000.0,"""Our Florence pretrained model has in total 893M parameters, including the language transformer with 256M parameters and the CoSwin-H transformer with 637M parameters.""",4.831e+22,"""The model takes 10 days to train on 512 NVIDIA A100 GPUs with 40GB memory per GPU.""
512 * 312 teraFLOPS * 10 days * 35% utilization = 4.831e22 FLOP",FLD-900M,"900 million image-text pairs curated from internet images and descriptions

""We leverage large quantities of image-text data available
publicly on the internet. Specifically, we construct a 900
million image-text-pair dataset, called FLD-900M (FLD
stands for FLorenceDataset), using a programmatic data
curation pipeline that processes around 3 billion Internet
images and their raw descriptions in parallel.  <..>The final form of the FLD-900M dataset consists of 900M images with 900M free-form texts (ranging from one word, phase to sentences), 9.7M unique queries, and 7.5B tokens in total.
",900000000.0,,240.0,10 days on 512 A100 40GB,NVIDIA A100 SXM4 40 GB,Supervised,Confident,"Automated visual understanding of our diverse and open world demands computer vision models to generalize well with minimal customization for specific tasks, similar to human vision. Computer vision foundation models, which are trained on diverse, large-scale dataset and can be adapted to a wide range of downstream tasks, are critical for this mission to solve real-world computer vision applications. While existing vision foundation models such as CLIP, ALIGN, and Wu Dao 2.0 focus mainly on mapping images and textual representations to a cross-modal shared representation, we introduce a new computer vision foundation model, Florence, to expand the representations from coarse (scene) to fine (object), from static (images) to dynamic (videos), and from RGB to multiple modalities (caption, depth). By incorporating universal visual-language representations from Web-scale image-text data, our Florence model can be easily adapted for various computer vision tasks, such as classification, retrieval, object detection, VQA, image caption, video retrieval and action recognition. Moreover, Florence demonstrates outstanding performance in many types of transfer learning: fully sampled fine-tuning, linear probing, few-shot transfer and zero-shot transfer for novel images and objects. All of these properties are critical for our vision foundation model to serve general purpose vision tasks. Florence achieves new state-of-the-art results in majority of 44 representative benchmarks, e.g., ImageNet-1K zero-shot classification with top-1 accuracy of 83.74 and the top-5 accuracy of 97.18, 62.4 mAP on COCO fine tuning, 80.36 on VQA, and 87.8 on Kinetics-600.",,,Unreleased,"United States of America,Multinational,India,Belgium",,,,512.0,,2025-05-28 16:11,,,,,,Industry,checked,,,122880.0,Unreleased,,Industry,$106950.62,,FP16,412831.8485448412,Hardware,,,,16
BASIC-L,Vision,Image classification,Google,"Hieu Pham, Zihang Dai, Golnaz Ghiasi, Kenji Kawaguchi, Hanxiao Liu, Adams Wei Yu, Jiahui Yu, Yi-Ting Chen, Minh-Thang Luong, Yonghui Wu, Mingxing Tan, Quoc V. Le",2021-11-19,Combined Scaling for Zero-shot Transfer Learning,https://arxiv.org/abs/2111.10050,166.0,SOTA improvement,"SOTA on ImageNet for a model that was not trained on ImageNet images:
""We present a combined scaling method â€“ named BASIC â€“ that achieves 85.7% top-1 accuracy on the ImageNet ILSVRC-2012 validation set without learning from any labeled ImageNet example. This accuracy
surpasses best-published similar models â€“ CLIP and ALIGN â€“ by 9.3%""",3070000000.0,2.4B image model + 670M text model,4.12e+22,"6.9k + 1k + 0.8k = 8.7k TPUv4 core-days for BASIC-L, per Table 8

Two cores per chip, and 275 teraflop/s per chip 
(https://cloud.google.com/tpu/docs/system-architecture-tpu-vm#tpu_v4)

275 teraflops * 8700/2 * 24 * 3600 * 0.4 (assumed utilization) = 8.3e22","JFT,ALIGN","For pretraining (Section 8), we use the JFT dataset. This dataset has been
used in previous publications (Zhai et al., 2021; Dosovitskiy et al., 2021; Kolesnikov et al., 2020), but it has been constantly expanded. The JFT version used in our experiments has 5B images, each of which can be associated to one or multiple labels out of 29K possible classes.


""Starting from the ALIGN dataset, which contains 1.7B weakly-aligned image-text pairs (Jia et al., 2021), we collect 5B more image-text pairs, hence expanding the dataset size by roughly 4 times. We acquire
these 5B image-text pairs from the JFT dataset""",6700000000.0,6.7B image-text pairs,,,Google TPU v4,,Likely,"We present a combined scaling method - named BASIC - that achieves 85.7% top-1 accuracy on the ImageNet ILSVRC-2012 validation set without learning from any labeled ImageNet example. This accuracy surpasses best published similar models - CLIP and ALIGN - by 9.3%. Our BASIC model also shows significant improvements in robustness benchmarks. For instance, on 5 test sets with natural distribution shifts such as ImageNet-{A,R,V2,Sketch} and ObjectNet, our model achieves 84.3% top-1 average accuracy, only a small drop from its original ImageNet accuracy. To achieve these results, we scale up the contrastive learning framework of CLIP and ALIGN in three dimensions: data size, model size, and batch size. Our dataset has 6.6B noisy image-text pairs, which is 4x larger than ALIGN, and 16x larger than CLIP. Our largest model has 3B weights, which is 3.75x larger in parameters and 8x larger in FLOPs than ALIGN and CLIP. Finally, our batch size is 65536 which is 2x more than CLIP and 4x more than ALIGN. We encountered two main challenges with the scaling rules of BASIC. First, the main challenge with implementing the combined scaling rules of BASIC is the limited memory of accelerators, such as GPUs and TPUs. To overcome the memory limit, we propose two simple methods which make use of gradient checkpointing and model parallelism. Second, while increasing the dataset size and the model size has been the defacto method to improve the performance of deep learning models like BASIC, the effect of a large contrastive batch size on such contrastive-trained image-text models is not well-understood. To shed light on the benefits of large contrastive batch sizes, we develop a theoretical framework which shows that larger contrastive batch sizes lead to smaller generalization gaps for image-text models such as BASIC.",,,Unreleased,United States of America,,,,,,2025-05-28 16:11,,,,,"65536, but these are image-text pairs not tokens
""For the batch size, we use 65536 contrastive
learning examples per minibatch""",Industry,,,,4350.0,Unreleased,,Industry,$1684.77,,BF16,,Hardware,,,,20
Swin Transformer V2 (SwinV2-G),"Vision,Video","Action recognition,Image classification",Microsoft Research Asia,"Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, Furu Wei, Baining Guo",2021-11-18,Swin Transformer V2: Scaling Up Capacity and Resolution,https://arxiv.org/abs/2111.09883v2,1368.0,"SOTA improvement,Highly cited","""It set new performance records on 4 representative vision tasks, including ImageNet-V2
image classification, COCO object detection, ADE20K semantic segmentation, and Kinetics-400 video action classification.""",3000000000.0,,1.1e+21,"trained on ""<0.5k"" TPUv3 core-days per Table 2 (not trained on TPUs, this is a comparison with other papers)

A core is 123/2 teraflops

500 core-days
= 500 * 123/2 trillion * 24 * 3600 * 0.4 utilization
~= 1.1e21",ImageNet21k,"""We conduct experiments on ImageNet-1K image classification (V1 and V2) [18, 55], COCO object detection [44], and ADE20K semantic segmentation [85]. For the 3B model experiments, we also report the accuracy on Kinetics-400 video action recognition [37].""

â€¢ Image classification. ImageNet-1K V1 and V2 val are
used [18,55] for evaluation. ImageNet-22K [18] which
has 14M images and 22K categories is optionally employed for pre-training. For the pre-training our largest
model SwinV2-G, a privately collected ImageNet22K-ext dataset with 70 million images is used.",,,,,NVIDIA A100 SXM4 40 GB,,Confident,"Large-scale NLP models have been shown to significantly improve the performance on language tasks with no signs of saturation. They also demonstrate amazing few-shot capabilities like that of human beings. This paper aims to explore large-scale models in computer vision. We tackle three major issues in training and application of large vision models, including training instability, resolution gaps between pre-training and fine-tuning, and hunger on labelled data. Three main techniques are proposed: 1) a residual-post-norm method combined with cosine attention to improve training stability; 2) A log-spaced continuous position bias method to effectively transfer models pre-trained using low-resolution images to downstream tasks with high-resolution inputs; 3) A self-supervised pre-training method, SimMIM, to reduce the needs of vast labeled images. Through these techniques, this paper successfully trained a 3 billion-parameter Swin Transformer V2 model, which is the largest dense vision model to date, and makes it capable of training with images of up to 1,536Ã—1,536 resolution. It set new performance records on 4 representative vision tasks, including ImageNet-V2 image classification, COCO object detection, ADE20K semantic segmentation, and Kinetics-400 video action classification. Also note our training is much more efficient than that in Google's billion-level visual models, which consumes 40 times less labelled data and 40 times less training time. Code is available at \url{this https URL}.",,,Open weights (unrestricted),China,,,,,,2025-05-01 10:42,,,,,,Industry,,,,,Open source,"MIT license

https://github.com/microsoft/Swin-Transformer

training code: https://github.com/microsoft/Swin-Transformer/blob/main/get_started.md ",Industry,$2326.66,,,,Hardware,,,,
DeBERTaV3large,Language,"Question answering,Language modeling/generation",Microsoft Research,"Pengcheng He, Jianfeng Gao, Weizhu Chen",2021-11-18,DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing,https://arxiv.org/abs/2111.09543,,,,418000000.0,"from HF ""The DeBERTa V3 large model comes with 24 layers and a hidden size of 1024 . Its total parameter number is 418M since we use a vocabulary containing 128K tokens which introduce 131M parameters in the Embedding layer.  This model was trained using the 160GB data as DeBERTa V2.""",1.07008e+20,6ND = 6 FLOP / parameter / token * 42666666667 tokens * 418000000 parameters = 1.07008e+20 FLOP,"Wikipedia,OPENWEBTEXT,CC-Stories,CC-News,""BookCorpus (BooksCorpus, Toronto Book Corpus)""",,42666666667.0,"""We train those models with 160GB data"" 
from Table 8
Wiki+Book 16GB
OpenWebText 38GB
Stories  31GB
CC-News 76GB
""All the models are trained for 500,000 steps with a batch size of 8192 and warming up steps of 10,000.""

160GB* 200M words per GB * 4/3 tokens per English word ~ 42666666667

Batch Size 8k
Max Steps 500k
sequence length unknown",,,,,Likely,"This paper presents a new pre-trained language model, DeBERTaV3, which improves the original DeBERTa model by replacing mask language modeling (MLM) with replaced token detection (RTD), a more sample-efficient pre-training task. Our analysis shows that vanilla embedding sharing in ELECTRA hurts training efficiency and model performance. This is because the training losses of the discriminator and the generator pull token embeddings in different directions, creating the ""tug-of-war"" dynamics. We thus propose a new gradient-disentangled embedding sharing method that avoids the tug-of-war dynamics, improving both training efficiency and the quality of the pre-trained model. We have pre-trained DeBERTaV3 using the same settings as DeBERTa to demonstrate its exceptional performance on a wide range of downstream natural language understanding (NLU) tasks. Taking the GLUE benchmark with eight tasks as an example, the DeBERTaV3 Large model achieves a 91.37% average score, which is 1.37% over DeBERTa and 1.91% over ELECTRA, setting a new state-of-the-art (SOTA) among the models with a similar structure. Furthermore, we have pre-trained a multi-lingual model mDeBERTa and observed a larger improvement over strong baselines compared to English models. For example, the mDeBERTa Base achieves a 79.8% zero-shot cross-lingual accuracy on XNLI and a 3.6% improvement over XLM-R Base, creating a new SOTA on this benchmark. We have made our pre-trained models and inference code publicly available at this https URL.",,,Open weights (unrestricted),"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational",,,,,,2025-05-30 14:10,,,,,,Industry,,,,,Open source,"MIT license

https://github.com/microsoft/DeBERTa

https://huggingface.co/microsoft/deberta-v3-large",Industry,,,,,Operation counting,microsoft,,,
ViT-G/14 (LiT),Vision,Image classification,Google Research,"Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner, Daniel Keysers, Alexander Kolesnikov, Lucas Beyer",2021-11-15,Zero-Shot Transfer with Locked-image Text Tuning,https://arxiv.org/abs/2111.07991v3,461.0,SOTA improvement,"""For example, it achieves 82.5% accuracy on the challenging ObjectNet test set [1], outperforming the previous state-of-the-art
method [46] by 10.2%.""",3005000000.0,Table 7,,"They start with the ViT-G/14 image model and train their own text model. ViT-G/14 is 3.4e21. 

They also say ""We use 128 TPU cores by default for the above experiments, and 256 TPU cores for our best run with 18 billion seen image-text pairs"" which may be relevant.","Conceptual Captions 12M (CC12M),YFCC-100M,Unspecified unreleased","CC12M, YFCC100m, and their novel dataset:
""Our dataset. We collect 4 billion image and alt-text
pairs following the same process as ALIGN [31], with the
same image-based filtering but simpler text-based filtering.
Appendix L shows that reducing text filtering does not harm
performance. To avoid misleading evaluation results, we
remove from our dataset near-duplicate images of all splits
from all datasets we evaluate on. We do not consider the
creation of our dataset a main contribution of this paper; we
just simplify the data collection process in ALIGN [31] to
demonstrate the efficacy of our methods at scale.""",4000000000.0,"Largest dataset is ""4 billion image and alt-text pairs"". This is rounded down slightly; the other datasets are much smaller.",,,Google TPU v3,,Confident,"This paper presents contrastive-tuning, a simple method employing contrastive training to align image and text models while still taking advantage of their pre-training. In our empirical study we find that locked pre-trained image models with unlocked text models work best. We call this instance of contrastive-tuning ""Locked-image Tuning"" (LiT), which just teaches a text model to read out good representations from a pre-trained image model for new tasks. A LiT model gains the capability of zero-shot transfer to new vision tasks, such as image classification or retrieval. The proposed LiT is widely applicable; it works reliably with multiple pre-training methods (supervised and unsupervised) and across diverse architectures (ResNet, Vision Transformers and MLP-Mixer) using three different image-text datasets. With the transformer-based pre-trained ViT-g/14 model, the LiT model achieves 85.2% zero-shot transfer accuracy on the ImageNet test set, and 82.5% on the challenging out-of-distribution ObjectNet test set.",4.5,,Unreleased,"Multinational,United States of America,Canada,Switzerland",ViT-G/14,,,,,2025-05-30 11:19,,,,,,Industry,,,,,Open source,"Apache 2.0 license

https://colab.research.google.com/github/google-research/big_vision/blob/main/big_vision/configs/proj/image_text/lit.ipynb

https://github.com/google-research/big_vision",Industry,,,,,,,,,
EquiDock,Biology,Proteins,"Massachusetts Institute of Technology (MIT),ETH Zurich,Tencent","Octavian-Eugen Ganea, Xinyuan Huang, Charlotte Bunne, Yatao Bian, Regina Barzilay, Tommi Jaakkola, Andreas Krause",2021-11-15,Independent SE(3)-Equivariant Models for End-to-End Rigid Protein Docking,https://arxiv.org/abs/2111.07786,133.0,,"""Empirically, we achieve significant running time improvements and often outperform existing docking software despite not relying on heavy candidate sampling, structure refinement, or templates.""

did not have best results, per Table 1",,,1.08e+19,"Training details here:

https://docs.nvidia.com/bionemo-framework/latest/models/equidock.html

32 A100s can do 30 epochs per hour on the DIPS dataset. Equidock was trained on 30 epochs on DIPS and 150 epochs on DB5.5. DIPS is about 100x bigger, so the large majority of compute was DIPS.

32 A100-hours = 312 teraflops * 32 * 3600 * 0.3 
~= 1.08e19","DIPS,DB5.5","""We leverage the following datasets: Docking Benchmark 5.5 (DB5.5) (Vreven et al.,
2015) and Database of Interacting Protein Structures (DIPS) (Townshend et al., 2019). DB5.5 is
a gold standard dataset in terms of data quality, but contains only 253 structures. DIPS is a larger
protein complex structures dataset mined from the Protein Data Bank (Berman et al., 2000) and
tailored for rigid body docking. Datasets information is given in Appendix D""",39938.0,"DIPS Dataset:
39,937 protein pairs

Total data points = 39,937 = 3.9937e4",,,,,Likely,"Protein complex formation is a central problem in biology, being involved in most of the cell's processes, and essential for applications, e.g. drug design or protein engineering. We tackle rigid body protein-protein docking, i.e., computationally predicting the 3D structure of a protein-protein complex from the individual unbound structures, assuming no conformational change within the proteins happens during binding. We design a novel pairwise-independent SE(3)-equivariant graph matching network to predict the rotation and translation to place one of the proteins at the right docked position relative to the second protein. We mathematically guarantee a basic principle: the predicted complex is always identical regardless of the initial locations and orientations of the two structures. Our model, named EquiDock, approximates the binding pockets and predicts the docking poses using keypoint matching and alignment, achieved through optimal transport and a differentiable Kabsch algorithm. Empirically, we achieve significant running time improvements and often outperform existing docking software despite not relying on heavy candidate sampling, structure refinement, or templates.",30.0,,Open weights (unrestricted),"United States of America,Switzerland,China",,,,,,2025-04-30 10:04,,,,,,"Academia,Academia,Industry",,,,,,,"Academia,Academia,Industry",,,,,Hardware,,,,
KoGPT,Language,"Language modeling/generation,Chat",Kakao,"Ildoo Kim, Gunsoo Han, Jiyeon Ham, Woonhyuk Baek",2021-11-12,KoGPT: KakaoBrain Korean(hangul) Generative Pre-trained Transformer,https://github.com/kakaobrain/kogpt,,,"competitive with HyperCLOVA on benchmarks, per results table",6166502400.0,"from https://huggingface.co/kakaobrain/kogpt

possibly has 30B parameters based on this source: (would be a different version)

https://post.naver.com/viewer/postView.naver?volumeNo=34605062&memberNo=35753905&vType=VERTICAL",,,ryan,"""KakaoBrain KoGPT was trained on ryan dataset, a dataset known to contain profanity, lewd, political changed, and other harsh language.""

Not sure if that's the only dataset. Couldn't find any info on ""ryan"".

https://huggingface.co/kakaobrain/kogpt",,,,,,,Speculative,"""Kakao, the operator of South Koreaâ€™s No. 1 messenger app KakaoTalk, said Friday that it plans to expand its vertical Korean language-based artificial intelligence services based on an AI language model called KoGPT this year.""

https://www.koreaherald.com/view.php?ud=20230210000564",,,Open weights (non-commercial),Korea (Republic of),,,,,,2024-09-08 22:50,,,,,,Industry,,,,,Open source,"Apache 2.0 code, CC-BY-NC-ND 4.0 (non-commercial) weights

https://github.com/kakaobrain/kogpt/blob/main/LICENSE",Industry,,,,,,,,,
Masked Autoencoders ViT-H,Vision,"Semantic segmentation,Image classification,Image generation",Facebook AI Research,"Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr DollÃ¡r, Ross Girshick",2021-11-11,Masked Autoencoders Are Scalable Vision Learners,https://arxiv.org/abs/2111.06377,5980.0,"Highly cited,SOTA improvement","""By fine-tuning with a 448 size, we achieve 87.8% accuracy, using only IN1K data. The previous best accuracy, among all methods using only IN1K data, is 87.1% (512 size)... We improve over the state-of-the-art by a nontrivial margin in the highly competitive benchmark of IN1K (no external data). Our result is based on vanilla ViT, and we expect advanced networks will perform better.""

See Table 3",632000000.0,"Three models:
ViT-B (86M), ViT-L (304M), ViT-H (632M)",4.6000000000000007e+20,"128 TPU-v3 cores trained for 1600 epochs. Times are given for 800 epochs in Table 2; largest model (ViT-H) took 34.5 hrs for 800.
128 TPU-v3 cores * 0.5 chips/core * 34.5 hours * 2 * 1.23E+14 FLOP/sec / chip * 3600 sec/hour  * 40% utilization = 7.84e20 FLOP

Note that the operations counting method disagrees:
2 Ã— 632000000 connections Ã— 3 Ã— 1281167 training examples Ã— 1600 of epochs  = 7.8e18 FLOP

Manual calculation with `calflops` package roughly agrees with hardware-time calculation: 
286.21 GFLOPS/observation * 1281167 observations * 1600 epochs = 5.86e20 FLOP

See reproduction here: https://colab.research.google.com/drive/1KCsmrfPzT9BgGO_YQthnz4oP3QRqbw5o?usp=sharing

Weighting three estimates equally:
(7.84e20 + 7.8e18 + 5.86e20)/3 = 4.6e20",ImageNet-1k,,1281167.0,,69.0,"Table 2 gives wall times for training ViT-L and ViT-H to 800 epochs; later it is stated that the systems are each trained for 1600 epochs.
(34.5 hours / 800 epochs) * 1600 epochs = 69 hours",,Self-supervised learning,Speculative,"This paper shows that masked autoencoders (MAE) are scalable self-supervised learners for computer vision. Our MAE approach is simple: we mask random patches of the input image and reconstruct the missing pixels. It is based on two core designs. First, we develop an asymmetric encoder-decoder architecture, with an encoder that operates only on the visible subset of patches (without mask tokens), along with a lightweight decoder that reconstructs the original image from the latent representation and mask tokens. Second, we find that masking a high proportion of the input image, e.g., 75%, yields a nontrivial and meaningful self-supervisory task. Coupling these two designs enables us to train large models efficiently and effectively: we accelerate training (by 3x or more) and improve accuracy. Our scalable approach allows for learning high-capacity models that generalize well: e.g., a vanilla ViT-Huge model achieves the best accuracy (87.8%) among methods that use only ImageNet-1K data. Transfer performance in downstream tasks outperforms supervised pre-training and shows promising scaling behavior.",1600.0,,Open weights (non-commercial),United States of America,ViT-Huge/14,,"UNCERTAIN
128 TPU-v3 cores trained for 1600 epochs. Times are given for 800 epochs in Table 2; largest model (ViT-H) took 34.5 hrs for 800.
128 TPU-v3 cores * 0.5 chips/core * 34.5 hours * 2 * 1.23E+14 FLOP/sec / chip * 3600 sec/hour  * 40% utilization = 7.84e20 FLOP

Note that the operations counting method disagrees:
2 Ã— 632000000 connections Ã— 3 Ã— 1281167 training examples Ã— 1600 of epochs  = 7.8e18 FLOP
",,,2025-05-09 11:32,,,,,,Industry,,,,,Open (non-commercial),"Code at https://github.com/facebookresearch/mae

This project is under the CC-BY-NC 4.0 license

training code: https://github.com/facebookresearch/mae/blob/main/PRETRAIN.md ",Industry,,,,,"Hardware,Operation counting",,,,
GPT-2 (AMPS),"Mathematics,Language","Language modeling/generation,Quantitative reasoning",University of California (UC) Berkeley,"Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt",2021-11-08,Measuring Mathematical Problem Solving With the MATH Dataset,https://arxiv.org/abs/2103.03874,,,,1500000.0,,,,Auxiliary Mathematics Problems and Solutions (AMPS),,,,24.0,"Models are trained with 8 A100 GPUs, each requiring less than a day. Unless otherwise specified, for GPT-2 we use the default HuggingFace (Wolf et al., 2020) generation parameters, except that we use beam search.",NVIDIA A100,,Speculative,"Many intellectual endeavors require mathematical problem solving, but this skill remains beyond the capabilities of computers. To measure this ability in machine learning models, we introduce MATH, a new dataset of 12,500 challenging competition mathematics problems. Each problem in MATH has a full step-by-step solution which can be used to teach models to generate answer derivations and explanations. To facilitate future research and increase accuracy on MATH, we also contribute a large auxiliary pretraining dataset which helps teach models the fundamentals of mathematics. Even though we are able to increase accuracy on MATH, our results show that accuracy remains relatively low, even with enormous Transformer models. Moreover, we find that simply increasing budgets and model parameter counts will be impractical for achieving strong mathematical reasoning if scaling trends continue. While scaling Transformers is automatically solving most other text-based tasks, scaling is not currently solving MATH. To have more traction on mathematical problem solving we will likely need new algorithmic advancements from the broader research community.",,,,United States of America,GPT-2 (1.5B),64663142400000000000,8 GPUs * 24 hours * 3600 sec / hour * 311.84 * 10^12 FLOP / sec * 0.3 [assumed utilization] = 64663142400000000000 FLOP,8.0,,2025-03-31 11:03,,,,,,Academia,,,,,,,Academia,,,,6452.509026418002,Hardware,,,,
GPT2+CoreLM+Fine-Tuning,Language,"Language modeling/generation,Named entity recognition",Aristotle University of Thessaloniki,"Nikolaos Stylianou, Ioannis Vlahavas",2021-11-04,CoreLM: Coreference-aware Language Model Fine-Tuning,https://arxiv.org/abs/2111.02687,2.0,,,132000000.0,"""In all our experiments we use the GPT2-small configuration with 124M parameters, with 12 layers and 12 attention heads each for our base model.""

""Our model has 132M parameters, a 6% increase, after the addition of the Entity-Gating layer and the entity representations.""",1.1725747e+20,"GPT-2 124M compute ~ GPT-2 117M compute = 1.1699999999999999 Ã— 10^20 FLOP

1.17 * 10^20 FLOP + 2.57472e+17 FLOP = 1.1725747e+20 FLOP",Gumby,,,batch size of 128,8.0,"""In this setup, fine-tuning takes approximately 8 hours""",NVIDIA Titan V,,Likely,"Language Models are the underpin of all modern Natural Language Processing (NLP) tasks. The introduction of the Transformers architecture has contributed significantly into making Language Modeling very effective across many NLP task, leading to significant advancements in the field. However, Transformers come with a big computational cost, which grows quadratically with respect to the input length. This presents a challenge as to understand long texts requires a lot of context. In this paper, we propose a Fine-Tuning framework, named CoreLM, that extends the architecture of current Pretrained Language Models so that they incorporate explicit entity information. By introducing entity representations, we make available information outside the contextual space of the model, which results in a better Language Model for a fraction of the computational cost. We implement our approach using GPT2 and compare the fine-tuned model to the original. Our proposed model achieves a lower Perplexity in GUMBY and LAMBDADA datasets when compared to GPT2 and a fine-tuned version of GPT2 without any changes. We also compare the models' performance in terms of Accuracy in LAMBADA and Children's Book Test, with and without the use of model-created coreference annotations.",10.0,GPT2+CoreLM+Fine-Tuning,Unreleased,Greece,GPT-2 (124M),257472000000000000,"29800000000000 FLOP / s [Titan V, fp16 - reported] * 8 hours * 3600 sec / hour * 1 GPU * 0.3 [assumed utilization] = 2.57472e+17 FLOP

(it is probably the upper bound since the 8 hours time seems to account for all models fine-tuned in the paper)",1.0,,2025-03-31 15:52,,,,,,Academia,,,,,Unreleased,,Academia,,,FP16,277.0039416764727,Hardware,,,,
CodeT5-base,Language,Code generation,"Salesforce,Nanyang Technological University","Yue Wang, Weishi Wang, Shafiq Joty, Steven C.H. Hoi",2021-11-01,CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation,https://aclanthology.org/2021.emnlp-main.685/,1211.0,SOTA improvement,"""Extensive experiments show that CodeT5 yields state-of-the-art results on the fourteen sub-tasks in CodeXGLUE.""",220000000.0,"""We build CodeT5 based on Huggingfaceâ€™s T5 (Raffel et al., 2020) PyTorch implementation and employ two sizes of CodeT5-small (60M) and CodeT5-base (220M)""",1.56e+21,"""We pre-train the model with the denoising objective for 100 epochs and bimodal dual training for further 50 epochs on a cluster of 16 NVIDIA A100 GPUs with 40G memory. The total training time for CodeT5-small and CodeT5- base is 5 and 12 days, respectively""

16 * 312 teraFLOP/s * 12 * 24 * 3600 * 0.3 (utilization assumption) = 1.56e21","CodeSearchNet,BigQuery","""We follow Feng et al. (2020) to employ CodeSearchNet (Husain et al., 2019) to pre-train CodeT5, which consists of six PLs with both unimodal and bimodal data. Apart from that, we additionally collect two datasets of C/CSharp from
BigQuery1 to ensure that all downstream tasks have overlapped PLs with the pre-training data. In total, we employ around 8.35 million instances for pretraining""",,"""In total, we employ around 8.35 million instances for pretraining"" 
Instances meaning code snippets/examples, not tokens.",288.0,"""The total training time for CodeT5-small and CodeT5- base is 5 and 12 days, respectively""",NVIDIA A100,,Likely,"""Pre-trained models for Natural Languages (NL) like BERT and GPT have been recently shown to transfer well to Programming Languages (PL) and largely benefit a broad set of code-related tasks. Despite their success, most current methods either rely on an encoder-only (or decoder-only) pre-training that is suboptimal for generation (resp. understanding) tasks or process the code snippet in the same way as NL, neglecting the special characteristics of PL such as token types. We present CodeT5, a unified pre-trained encoder-decoder Transformer model that better leverages the code semantics conveyed from the developer-assigned identifiers. Our model employs a unified framework to seamlessly support both code understanding and generation tasks and allows for multi-task learning. Besides, we propose a novel identifier-aware pre-training task that enables the model to distinguish which code tokens are identifiers and to recover them when they are masked. Furthermore, we propose to exploit the user-written code comments with a bimodal dual generation task for better NL-PL alignment. Comprehensive experiments show that CodeT5 significantly outperforms prior methods on understanding tasks such as code defect detection and clone detection, and generation tasks across various directions including PL-NL, NL-PL, and PL-PL. Further analysis reveals that our model can better capture semantic information from code. Our code and pre-trained models are released at https://github.com/salesforce/CodeT5.""",150.0,,Open weights (unrestricted),"United States of America,Singapore",,,,,,2025-05-28 16:11,,,,,,"Industry,Academia",,,,,Open source,"BSD-3-Clause license

https://github.com/salesforce/CodeT5","Industry,Academia",$3114.87,,FP16,,Hardware,,,,
Projected GAN,Image generation,Image generation,Heidelberg University,"Axel Sauer, Kashyap Chitta, Jens MÃ¼ller, Andreas Geiger",2021-11-01,Projected GANs Converge Faster,https://proceedings.neurips.cc/paper/2021/hash/9219adc5c42107c4911e249155320648-Abstract.html,207.0,SOTA improvement,"""It is further compatible with resolutions of up to one Megapixel and advances the state-of-the-art FrÃ©chet Inception Distance (FID) on twenty-two benchmark datasets""",,Possibly calculable from Appendix Table 8,1.05e+19,"""With this setting, each experiment takes roughly 100-200 GPU hours on a NVIDIA V100,
for more details we refer to the appendix.""

""We conduct our experiments on an internal cluster with several nodes, each with up to 8 Quadro RTX
6000 or NVIDIA V100 using PyTorch 1.7.1 and CUDA 11.0.""

In appendix table 7, takes 10.1 seconds per 1k images on 8 Quadro RTX 6000s. Longest training run for Projected GAN appears to be in Figure 4 (left), at 14M images, though this is overtrained and the largest checkpoint used for evaluations was 10M.
10M images * 10.1 s/1000 images * 8 * 3.26e13 FLOP/s * 0.4 = 1.05e19",,They experiment with 22 image datasets. Largest appears to be LSUN-Bedroom at 3M images.,3000000.0,They experiment with 22 image datasets. Largest appears to be LSUN-Bedroom at 3M images.,,,"NVIDIA V100,NVIDIA Quadro RTX 6000",,Confident,"Generative Adversarial Networks (GANs) produce high-quality images but are
challenging to train. They need careful regularization, vast amounts of compute,
and expensive hyper-parameter sweeps. We make significant headway on these issues by projecting generated and real samples into a fixed, pretrained feature space.
Motivated by the finding that the discriminator cannot fully exploit features from
deeper layers of the pretrained model, we propose a more effective strategy that
mixes features across channels and resolutions. Our Projected GAN improves image quality, sample efficiency, and convergence speed. It is further compatible with
resolutions of up to one Megapixel and advances the state-of-the-art FrÃ©chet Inception Distance (FID) on twenty-two benchmark datasets. Importantly, Projected
GANs match the previously lowest FIDs up to 40 times faster, cutting the wall-clock
time from 5 days to less than 3 hours given the same computational resources.",,,Open weights (unrestricted),Germany,,,,,,2025-05-30 13:06,,,,,,Academia,,,,,Open source,"MIT license

https://github.com/autonomousvision/projected-gan",Academia,,,,,Hardware,,,,
S4,Language,Language modeling/generation,Stanford University,"Albert Gu, Karan Goel, Christopher RÃ©",2021-10-31,Efficiently Modeling Long Sequences with Structured State Spaces,https://arxiv.org/abs/2111.00396,1058.0,SOTA improvement,"""S4 achieves strong empirical results across a diverse range of established benchmarks, including... SoTA on every task from the Long Range Arena benchmark""",249000000.0,249M (Table 8),7.8328627e+19,"6 FLOP / token / parameter * 249000000 parameters * 8 GPUs * 8192 tokens/step/GPU * 800000 steps = 7.8328627e+19 FLOP
 
""our S4 model was trained with the simpler AdamW optimizer with a single cosine learning rate cycle with a maximum of 800000 steps.
The initial learning rate was set to 0.0005. We used 8 A100 GPUs with a batch size of 1 per gpu and context size 8192.""

",WikiText-103,,103000000.0,,,,NVIDIA A100,,Likely,"A central goal of sequence modeling is designing a single principled model that can address sequence data across a range of modalities and tasks, particularly on long-range dependencies. Although conventional models including RNNs, CNNs, and Transformers have specialized variants for capturing long dependencies, they still struggle to scale to very long sequences of 10000 or more steps. A promising recent approach proposed modeling sequences by simulating the fundamental state space model (SSM) \( x'(t) = Ax(t) + Bu(t), y(t) = Cx(t) + Du(t) \), and showed that for appropriate choices of the state matrix \( A \), this system could handle long-range dependencies mathematically and empirically. However, this method has prohibitive computation and memory requirements, rendering it infeasible as a general sequence modeling solution. We propose the Structured State Space sequence model (S4) based on a new parameterization for the SSM, and show that it can be computed much more efficiently than prior approaches while preserving their theoretical strengths. Our technique involves conditioning \( A \) with a low-rank correction, allowing it to be diagonalized stably and reducing the SSM to the well-studied computation of a Cauchy kernel. S4 achieves strong empirical results across a diverse range of established benchmarks, including (i) 91\% accuracy on sequential CIFAR-10 with no data augmentation or auxiliary losses, on par with a larger 2-D ResNet, (ii) substantially closing the gap to Transformers on image and language modeling tasks, while performing generation 60Ã— faster (iii) SoTA on every task from the Long Range Arena benchmark, including solving the challenging Path-X task of length 16k that all prior work fails on, while being as efficient as all competitors.",509.02,S4,Open weights (unrestricted),United States of America,,,,8.0,,2025-05-28 16:11,,,,,,Academia,,,,,Open source,"Apache 2 0
repo with training, inference, checkpoints:
https://github.com/state-spaces/s4",Academia,,,FP16,6453.658675376135,,,,,
EfficientZero,Games,Atari,"Tsinghua University,University of California (UC) Berkeley,Shanghai Qi Zhi institute","Weirui Ye, Shaohuai Liu, Thanard Kurutach, Pieter Abbeel, Yang Gao",2021-10-30,Mastering Atari Games with Limited Data,https://arxiv.org/abs/2111.00210,184.0,SOTA improvement,"""Our method is 176% and 163% better
than the previous SoTA performance, in mean and median human normalized score respectively""",,,,"""Our implementation is computationally friendly. To train an Atari agent for 100k steps, it only needs 4 GPUs to train 7 hours.""",,,,,,,,,Unknown,"Reinforcement learning has achieved great success in many applications. However, sample efficiency remains a key challenge, with prominent methods requiring millions (or even billions) of environment steps to train. Recently, there has been significant progress in sample efficient image-based RL algorithms; however, consistent human-level performance on the Atari game benchmark remains an elusive goal. We propose a sample efficient model-based visual RL algorithm built on MuZero, which we name EfficientZero. Our method achieves 194.3% mean human performance and 109.0% median performance on the Atari 100k benchmark with only two hours of real-time game experience and outperforms the state SAC in some tasks on the DMControl 100k benchmark. This is the first time an algorithm achieves super-human performance on Atari games with such little data. EfficientZero's performance is also close to DQN's performance at 200 million frames while we consume 500 times less data. EfficientZero's low sample complexity and high performance can bring RL closer to real-world applicability. We implement our algorithm in an easy-to-understand manner and it is available at this https URL. We hope it will accelerate the research of MCTS-based RL algorithms in the wider community.",,,Unreleased,"China,United States of America,China",,,,,,2025-06-11 18:02,,,,,,"Academia,Academia",,,,,Open source,"GPL-3.0 (copyleft + prohibits incorporating the software into proprietary software)
https://github.com/YeWR/EfficientZero?tab=readme-ov-file","Academia,Academia",,,FP16,,,,,,
Scatterbrain,Language,Language modeling,"Stanford University,Adobe,University at Buffalo","Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri Rudra, Christopher RÃ©",2021-10-28,Scatterbrain: Unifying Sparse and Low-rank Attention Approximation,https://arxiv.org/abs/2110.15343,104.0,,,,,,,WikiText-103,,,,,,,,Unknown,"Recent advances in efficient Transformers have exploited either the sparsity or low-rank properties of attention matrices to reduce the computational and memory bottlenecks of modeling long sequences. However, it is still challenging to balance the trade-off between model quality and efficiency to perform a one-size-fits-all approximation for different tasks. To better understand this trade-off, we observe that sparse and low-rank approximations excel in different regimes, determined by the softmax temperature in attention, and sparse + low-rank can outperform each individually. Inspired by the classical robust-PCA algorithm for sparse and low-rank decomposition, we propose Scatterbrain, a novel way to unify sparse (via locality sensitive hashing) and low-rank (via kernel feature map) attention for accurate and efficient approximation. The estimation is unbiased with provably low error. We empirically show that Scatterbrain can achieve 2.1x lower error than baselines when serving as a drop-in replacement in BigGAN image generation and pre-trained T2T-ViT. On a pre-trained T2T Vision transformer, even without fine-tuning, Scatterbrain can reduce 98% of attention memory at the cost of only 1% drop in accuracy. We demonstrate Scatterbrain for end-to-end training with up to 4 points better perplexity and 5 points better average accuracy than sparse or low-rank efficient transformers on language modeling and long-range-arena tasks.",30.0,Scatterbrain,Unreleased,"United States of America,United States of America,United States of America",,,,,,2024-11-01 10:03,,,Scatterbrain,,,"Academia,Industry,Academia",,,,,Open source,code: https://github.com/HazyResearch/fly,"Academia,Industry,Academia",,,,,,,,,
Eve,Biology,"Protein pathogenicity prediction,Proteins","Harvard Medical School,University of Oxford","Jonathan Frazer, Pascal Notin, Mafalda Dias, Aidan Gomez, Joseph K. Min, Kelly Brock, Yarin Gal and Debora S. Marks",2021-10-27,Disease variant prediction with deep generative models of evolutionary data,https://www.nature.com/articles/s41586-021-04043-8#change-history,411.0,SOTA improvement,"""Our model EVE (evolutionary model of variant effect) not only outperforms computational approaches that rely on labelled data but also performs on par with, if not better than, predictions from high-throughput experiments, which are increasingly used as evidence for variant classification"" [Abstract] - SOTA improvement for very specific task",15010300.0,"""The Bayesian VAE architecture in EVE is comprised of a symmetric 3-layer encoder & decoder architecture (with 2,000-1,000-300 and 300-1,000-2,000 units respectively) and a latent space of dimension 50 [...] We use a single set of parameters for the encoder (Ï•p) and learn a fully-factorized gaussian distribution over the weights of the decoder (Î¸p)""
They train a new VAE for each protein, and it doesn't seem like they trim the input sequence length, so the largest model will be the one trained for the largest input protein. Supplementary materials 1 gives statistics for each protein; the longest is 5202, which would indicate a network of size 15,010,300",,,UniRef100,"""a Bayesian VAE was trained on a multiple sequence alignment retrieved by searching approximately 250 million protein sequences in UniRef""",,,,,,,Likely,"Quantifying the pathogenicity of protein variants in human disease-related genes would have a marked effect on clinical decisions, yet the overwhelming majority (over 98%) of these variants still have unknown consequences1â€“3. In principle, computational methods could support the large-scale interpretation of genetic variants. However, state-of-the-art methods4â€“10 have relied on training machine learning models on known disease labels. As these labels are sparse, biased and of variable quality, the resulting models have been considered insufficiently reliable11. Here we propose an approach that leverages deep generative models to predict variant pathogenicity without relying on labels. By modelling the distribution of sequence variation across organisms, we implicitly capture constraints on the protein sequences that maintain fitness. Our model EVE (evolutionary model of variant effect) not only outperforms computational approaches that rely on labelled data but also performs on par with, if not better than, predictions from high-throughput experiments, which are increasingly used as evidence for variant classifcation12â€“16. We predict the pathogenicity of more than 36 million variants across 3,219 disease genes and provide evidence for the classification of more than 256,000 variants of unknown significance. Our work suggests that models of evolutionary information can provide valuable independent evidence for variant interpretation that will be widely useful in research and clinical settings.",,,Unreleased,"United States of America,United Kingdom of Great Britain and Northern Ireland",,,,,,2025-04-30 10:04,,,,,,"Academia,Academia",,,,,Open source,"The model code is available at https://github.com/OATML-Markslab/EVE
MIT license","Academia,Academia",,,,,,,,,
DALL-E mini,Vision,Text-to-image,Craiyon,"Dayma, Boris and Patil, Suraj and Cuenca, Pedro and Saifullah, Khalid and Abraham, Tanishq and LÃª Kháº¯c, PhÃºc and Melas, Luke and Ghosh, Ritobrata",2021-10-26,DALLÂ·E Mini Model Card ,https://huggingface.co/dalle-mini/dalle-mini,,,,,,3.8e+19,"trained on a TPU v3-8 for 72 hours. That's 8 TPUv3 cores, or 4 TPUv3 chips (123 teraflop/s each)

flops = (4) * (123 * 10**12) * (72 * 3600) * (0.3) = 3.8e19
(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)
from https://huggingface.co/dalle-mini/dalle-mini#dall%C2%B7e-mini-estimated-emissions",,"Conceptual Captions Dataset, which contains 3 million image and caption pairs.
ï»¿Conceptual 12M, which contains 12 million image and caption pairs.
The OpenAI subset of YFCC100M, which contains about 15 million images and that we further sub-sampled to 2 million images due to limitations in storage space. 
from https://huggingface.co/dalle-mini/dalle-mini#training-data",30000000.0,3M+12M+15M = 30M from https://huggingface.co/dalle-mini/dalle-mini#training-data,72.0,from https://huggingface.co/dalle-mini/dalle-mini#dall%C2%B7e-mini-estimated-emissions,Google TPU v3,,Likely,,,,Open weights (unrestricted),United States of America,,,,4.0,,2025-02-17 11:35,,,,,,Industry,,,,288.0,Open source,"Apache 2.0
train code here: https://github.com/borisdayma/dalle-mini/blob/main/tools/train/train.py

data is open with various licenses",Industry,,,,1774.9537602472165,Hardware,,,,
PMLM-large,Biology,Protein or nucleotide language model (pLM/nLM),"Microsoft Research Asia,Nanyang Technological University,Xiâ€™an Jiaotong University,Sun Yat-sen University","Liang He, Shizhuo Zhang, Lijun Wu, Huanhuan Xia, Fusong Ju, He Zhang, Siyuan Liu, Yingce Xia, Jianwei Zhu, Pan Deng, Bin Shao, Tao Qin, Tie-Yan Liu",2021-10-21,Pre-Training Co-Evolutionary Protein Representation via a Pairwise Masked Language Model,https://arxiv.org/abs/2110.15527,28.0,,,250000000.0,"""Following the RoBERTa-base setting, the hidden size, feed forward dimension,
number of encoder layers, and attention heads of the base models are set as 768, 3072, 12, 12 re-
spectively (denoted as MLM-base for MLM and PMLM-base for PMLM). A larger model named
PMLM-large is pre-trained with the same setting except using 34 as the number of encoder layers.""",3.81e+21,1176*60*60*125000000000000*24*0.3=3.81024e+21,,,,,1176.0,"""about seven weeks for PMLM-large""",NVIDIA V100,,Confident,"Understanding protein sequences is vital and urgent for biology, healthcare, and medicine. Labeling approaches are expensive yet time-consuming, while the amount of unlabeled data is increasing quite faster than that of the labeled data due to low-cost, high-throughput sequencing methods. In order to extract knowledge from these unlabeled data, representation learning is of significant value for protein-related tasks and has great potential for helping us learn more about protein functions and structures. The key problem in the protein sequence representation learning is to capture the co-evolutionary information reflected by the inter-residue co-variation in the sequences. Instead of leveraging multiple sequence alignment as is usually done, we propose a novel method to capture this information directly by pre-training via a dedicated language model, i.e., Pairwise Masked Language Model (PMLM). In a conventional masked language model, the masked tokens are modeled by conditioning on the unmasked tokens only, but processed independently to each other. However, our proposed PMLM takes the dependency among masked tokens into consideration, i.e., the probability of a token pair is not equal to the product of the probability of the two tokens. By applying this model, the pre-trained encoder is able to generate a better representation for protein sequences. Our result shows that the proposed method can effectively capture the inter-residue correlations and improves the performance of contact prediction by up to 9% compared to the MLM baseline under the same setting. The proposed model also significantly outperforms the MSA baseline by more than 7% on the TAPE contact prediction benchmark when pre-trained on a subset of the sequence database which the MSA is generated from, revealing the potential of the sequence pre-training method to surpass MSA based methods in general.",,,,"China,Singapore,China,China",,,,24.0,,2025-05-01 10:42,,,,,,"Industry,Academia,Academia,Academia",,,,,,,"Industry,Academia,Academia,Academia",,,,14523.966055408006,,,,,
WD+LR+M,Language,Language modeling,"University of Cambridge,Alan Turing Institute","Ross M. Clarke, Elre T. Oldewage, JosÃ© Miguel HernÃ¡ndez-Lobato",2021-10-20,Scalable One-Pass Optimisation of High-Dimensional Weight-Update Hyperparameters by Implicit Differentiation,https://arxiv.org/abs/2110.10461,9.0,,,,,,,Penn TreeBank (PTB),,,,,,,,Unknown,"Machine learning training methods depend plentifully and intricately on hyperparameters, motivating automated strategies for their optimisation. Many existing algorithms restart training for each new hyperparameter choice, at considerable computational cost. Some hypergradient-based one-pass methods exist, but these either cannot be applied to arbitrary optimiser hyperparameters (such as learning rates and momenta) or take several times longer to train than their base models. We extend these existing methods to develop an approximate hypergradient-based hyperparameter optimiser which is applicable to any continuous hyperparameter appearing in a differentiable model weight update, yet requires only one training episode, with no restarts. We also provide a motivating argument for convergence to the true hypergradient, and perform tractable gradient-based optimisation of independent learning rates for each model parameter. Our method performs competitively from varied random hyperparameter initialisations on several UCI datasets and Fashion-MNIST (using a one-layer MLP), Penn Treebank (using an LSTM) and CIFAR-10 (using a ResNet-18), in time only 2-3x greater than vanilla training.",72.0,WD+LR+M,Unreleased,"United Kingdom of Great Britain and Northern Ireland,United Kingdom of Great Britain and Northern Ireland",,,,,,2024-09-08 23:11,,,,,,"Academia,Government",,,,,Open (non-commercial),"code, no license specified: https://github.com/rmclarke/OptimisingWeightUpdateHyperparameters","Academia,Government",,,,,,,,,
base LM+GNN+kNN,Language,Language modeling,"Shannon.AI,Nanjing University,Nanyang Technological University,Zhejiang University (ZJU)","Yuxian Meng, Shi Zong, Xiaoya Li, Xiaofei Sun, Tianwei Zhang, Fei Wu, Jiwei Li",2021-10-17,GNN-LM: Language Modeling based on Global Contexts via GNN,https://arxiv.org/abs/2110.08743,35.0,SOTA improvement,,274000000.0,"274M (table 1)

""We use the base version of deep Transformer language model with adaptive embeddings (Baevski & Auli, 2018) as our base LM. This model has 16 decoder layers. The dimensionality of word representations is 1,024, the number of multi-attention heads is 16, and the inner dimensionality of feedforward layers is 4,096.""",5.2587456e+19,"base model compute: 4.47*10^19 FLOP 
fine-tune lower bound estimation: 1.69332e+17 FLOP -> 4.47*10^19 FLOP + 1.69332e+17 FLOP = 4.4869332e+19 FLOP total 
fine-tune upper bound estimation: 1.69332e+19 FLOP -> 4.47*10^19 FLOP + 1.69332e+19 FLOP = 6.16332e+19 FLOP total

geometric mean: sqrt(4.4869332e+19*6.16332e+19) = 5.2587456e+19 
",WikiText-103,,,"""During training, data is partitioned into blocks of 3,072 contiguous
tokens.""",,,,,Confident,"Inspired by the notion that ``{\it to copy is easier than to memorize}``, in this work, we introduce GNN-LM, which extends the vanilla neural language model (LM) by allowing to reference similar contexts in the entire training corpus. We build a directed heterogeneous graph between an input context and its semantically related neighbors selected from the training corpus, where nodes are tokens in the input context and retrieved neighbor contexts, and edges represent connections between nodes. Graph neural networks (GNNs) are constructed upon the graph to aggregate information from similar contexts to decode the token. This learning paradigm provides direct access to the reference contexts and helps improve a model's generalization ability. We conduct comprehensive experiments to validate the effectiveness of the GNN-LM: GNN-LM achieves a new state-of-the-art perplexity of 14.8 on WikiText-103 (a 3.9 point improvement over its counterpart of the vanilla LM model), and shows substantial improvement on One Billion Word and Enwiki8 datasets against strong baselines. In-depth ablation studies are performed to understand the mechanics of GNN-LM. \footnote{The code can be found at this https URL",,base LM+GNN+kNN,Open weights (unrestricted),"China,China,Singapore,China",Transformer (Adaptive Input Embeddings) WT103,,"6 FLOP / token / parameter * 274000000 parameters * 103000000 tokens * 1 epoch [assumed for lower bound] = 1.69332e+17 FLOP

upper bound [assuming 100 epochs]: 1.69332e+19 FLOP",,,2025-05-28 16:11,,,,,,"Industry,Academia,Academia,Academia",,,,,Open source,"MIT License, training/eval code and weights
https://github.com/ShannonAI/GNN-LM","Industry,Academia,Academia,Academia",,,FP16,,Operation counting,,,,
T0-XXL,Language,Language modeling,"Hugging Face,Brown University","Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao,  Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal V. Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng-Xin Yong, Harshit Pandey, Michael McKenna, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Tali Bers, Stella Biderman, Leo Gao, Thomas Wolf, Alexander M. Rush",2021-10-15,Multitask Prompted Training Enables Zero-Shot Task Generalization,https://arxiv.org/abs/2110.08207,1526.0,Highly cited,"""we compare T0 to the zero-shot performance of the largest language models available as of writing, i.e., various GPT-3 models up to 175B parameters...
We find that T0 matches or exceeds the performance
of all GPT-3 models on 9 out of 11 held-out datasets""",11000000000.0,"""Unless specified otherwise, we use the XXL version which
has 11B parameters.""",9.1819e+20,"From Table 1 and section B.1, a single run uses 27 hours of a 512 core slice of a TPU-v3 pod. 
512 * 0.5 * 1.23e14 * 3600 * 27 * 0.3 = 9.18e20
(cores) * (chip/core) * (FLOP/chip-sec) * (sec/hour) * (hours) * (utilization assumption)",P3 (Public Pool of Prompts),,,"Multitask - 12 tasks, 62 datasets. See fig 2 for details. 

This is going to be a nightmare to figure out! TODO figure out the sizes of each of these 62 datasets!

All datasets from here: https://arxiv.org/pdf/2109.02846.pdf

From B.2: ""across all of our training runs (including preliminary test experiments not described in this paper) we trained for 250 billion tokens""",27.0,"For main model, 27 hours (Table 1)

Total time taken to train for all experiments was 270 hours ""These training runs corresponded to about 270 total hours of training on a v3-512 Cloud TPU device.""",Google TPU v3,,Confident,"Large language models have recently been shown to attain reasonable zero-shot generalization on a diverse set of tasks (Brown et al., 2020). It has been hypothesized that this is a consequence of implicit multitask learning in language models' pretraining (Radford et al., 2019). Can zero-shot generalization instead be directly induced by explicit multitask learning? To test this question at scale, we develop a system for easily mapping any natural language tasks into a human-readable prompted form. We convert a large set of supervised datasets, each with multiple prompts with diverse wording. These prompted datasets allow for benchmarking the ability of a model to perform completely held-out tasks. We fine-tune a pretrained encoder-decoder model (Raffel et al., 2020; Lester et al., 2021) on this multitask mixture covering a wide variety of tasks. The model attains strong zero-shot performance on several standard datasets, often outperforming models up to 16x its size. Further, our approach attains strong performance on a subset of tasks from the BIG-bench benchmark, outperforming models up to 6x its size. All trained models are available at this https URL and all prompts are available at this https URL.",,,Open weights (unrestricted),"Multinational,United States of America,United States of America",T5-11B,,,256.0,,2024-11-01 10:03,,,,,,"Industry,Academia",,,,69120.0,Open source,"Apache-2.0 license
https://github.com/bigscience-workshop/t-zero

training scripts: https://github.com/bigscience-workshop/t-zero/blob/master/training/README.md ","Industry,Academia",$11671.84,,,113624.87114581963,Hardware,,,,
KnGPT2,Language,Language modeling/generation,"Huawei Noah's Ark Lab,McGill University","Ali Edalati, Marzieh Tahaei, Ahmad Rashid, Vahid Partovi Nia, James J. Clark, Mehdi Rezagholizadeh",2021-10-15,Kronecker Decomposition for GPT Compression,https://arxiv.org/abs/2110.08152,28.0,,,83000000.0,"""The KnGPT2 model is compressed from the GPT-2Small Radford et al. (2019) model. GPT-2Small is 124 million parameters. Our baseline is DistilGPT2 which has about 82 million parameters so our KnGPT2 model is compressed to the same size (83 million parameters) for a fair comparison""",7.9402496e+20,"7.936e+20 FLOP [base model, ""Speculative"" confidence""] + 4.2496e+17 FLOP [fine-tune] = 7.9402496e+20 FLOP",OPENWEBTEXT,,853333333.0,"""KnGPT2 is pre-trained on 10% of OpenWebText""
3.2 GB (Table 3)

3.2GB * 200*10^6 words per GB * 4/3 tokens per word = 853333333 tokens",6.5,6.5 hours (Table 3),,,Speculative,"GPT is an auto-regressive Transformer-based pre-trained language model which has attracted a lot of attention in the natural language processing (NLP) domain due to its state-of-the-art performance in several downstream tasks. The success of GPT is mostly attributed to its pre-training on huge amount of data and its large number of parameters (from ~100M to billions of parameters). Despite the superior performance of GPT (especially in few-shot or zero-shot setup), this overparameterized nature of GPT can be very prohibitive for deploying this model on devices with limited computational power or memory. This problem can be mitigated using model compression techniques; however, compressing GPT models has not been investigated much in the literature. In this work, we use Kronecker decomposition to compress the linear mappings of the GPT-22 model. Our Kronecker GPT-2 model (KnGPT2) is initialized based on the Kronecker decomposed version of the GPT-2 model and then is undergone a very light pre-training on only a small portion of the training data with intermediate layer knowledge distillation (ILKD). Finally, our KnGPT2 is fine-tuned on down-stream tasks using ILKD as well. We evaluate our model on both language modeling and General Language Understanding Evaluation benchmark tasks and show that with more efficient pre-training and similar number of parameters, our KnGPT2 outperforms the existing DistilGPT2 model significantly.",1.0,KnGPT2,Unreleased,"China,Canada",GPT-2 (124M),424960000000000000,6 FLOP / parameter / token * 83000000 parameters * 853333333 tokens = 4.2496e+17 FLOP,,,2025-04-04 15:20,,,KnGPT2,,,"Industry,Academia",,,,,Unreleased,,"Industry,Academia",,,,,Operation counting,,,,
Yuan 1.0,Language,Language modeling,Inspur,"Shaohua Wu, Xudong Zhao, Tong Yu, Rongguo Zhang, Chong Shen, Hongli Liu, Feng Li, Hong Zhu, Jiangang Luo, Liang Xu, Xuanwei Zhang, Jun Liu",2021-10-12,Yuan 1.0: Large-Scale Pre-trained Language Model in Zero-Shot and Few-Shot Learning,https://arxiv.org/abs/2110.04725,51.0,SOTA improvement,"""The zero-shot average scores of both LM and PLM are superior to the SOTA one. On Csldcp, Tnews and Iflytek tasks, we surpass the zero-shot SOTA by a large margin""",245730000000.0,"Table 2: Parameters of Yuan models.
""Parameters (billion)""",3.5380000000001e+23,"Table 9: 4095 petaFLOPS-days which equals 3.538*10^23 FLOP

https://www.wolframalpha.com/input?i=4095+petaFLOPS+*+1+day
","Common Crawl,Wikipedia,Sogue News","""A Chinese corpus with 5TB high-quality text is built, which is sufficient to train Yuan 245B model without sampling the dataset twice.""

In order to obtain the high-quality dataset, we develop a Massive Data Filtering System (MDFS) built on Spark to clean and filter the raw data, and train a Bert-based model to select high quality samples. MDFS is consisted of three parts, data collection, coarse filtering and fine filtering (Fig. 5). The raw data is collected from Common Crawl, Sogou News, SogouT, Encyclopedia, and Books (Table 3). To process these raw data, we run MDFS system on a high performance cluster with 36 nodes.",1000000000000.0,"""Yuan 1.0 was trained on a new Chinese dataset of 5TB high-quality text that was built on 850TB raw data from Internet.""

1 GB ~ 167M words in English or 333M words in Chinese. For a mixed dataset of mostly Chinese, 5TB may be equivalent to around 1T words.

Table 2: 180B training tokens",,,,Self-supervised learning,Confident,"Recent work like GPT-3 has demonstrated excellent performance of Zero-Shot and Few-Shot learning on many natural language processing (NLP) tasks by scaling up model size, dataset size and the amount of computation. However, training a model like GPT-3 requires huge amount of computational resources which makes it challengeable to researchers. In this work, we propose a method that incorporates large-scale distributed training performance into model architecture design. With this method, Yuan 1.0, the current largest singleton language model with 245B parameters, achieves excellent performance on thousands GPUs during training, and the state-of-the-art results on NLP tasks. A data processing method is designed to efficiently filter massive amount of raw data. The current largest high-quality Chinese corpus with 5TB high quality texts is built based on this method. In addition, a calibration and label expansion method is proposed to improve the Zero-Shot and Few-Shot performance, and steady improvement is observed on the accuracy of various tasks. Yuan 1.0 presents strong capacity of natural language generation, and the generated articles are difficult to distinguish from the human-written ones.",,,API access,China,,,,2128.0,0.45,2025-05-28 16:11,,,,6881280.0,"Table 2. Batch size 3360, sequence length 2048. 3360*2048 = 6881280",Industry,,,,,Unreleased,https://github.com/Shawn-IEITSystems/Yuan-1.0,Industry,,"""The Yuan models are trained on a cluster with 2128 GPUs. A stable real performance of 45% of the theoretical peak performance is achieved on this cluster.""",FP16,,Reported,,,,3
TOME,Language,Question answering,"University of Southern California,Google","Michiel de Jong, Yury Zemlyanskiy, Nicholas FitzGerald, Fei Sha, William Cohen",2021-10-12,Mention Memory: incorporating textual knowledge into Transformers through entity mention attention,https://arxiv.org/abs/2110.06176v2,42.0,,,220000000.0,220M from Table 1 entry for TOME,1.03809024e+21,"We have information about hardware ""All models are pre-trained on 128 TPUs using AdamW optimizer (Loshchilov& Hutter, 2019) with learning rate 1e-4 and batch size of 4096."", but no information about training time and exact model of TPUs.

There is information about training 1.5M steps with batch size 4096 * 128 citation from appendix A:
""The Mention Encoder and BATCH-TOME are pre-trained
for 1 million steps with 50k warmup steps, and TOME is trained for 500k additional steps with 25k warmup steps after initialization from BATCH-TOME.""

It seems like 6ND is appropriate, since all components are dense transformers. In this case, compute is:
6 * 1.5M (steps) * 4096 (batch size) * 128 (seq len) * 220M (parameters) = 1.038e21",English Wikipedia,"citation from the start of Appendix A ""We train on English Wikipedia,""",5000000000.0,"Per Wikipedia, the site had 3.755B words as of the most recent checkpoint prior to the paper's publication: https://en.wikipedia.org/wiki/Wikipedia:Size_of_Wikipedia#Yearly_statistics
3.755B * 4/3 = 5B tokens",,,,,Likely,"Natural language understanding tasks such as open-domain question answering often require retrieving and assimilating factual information from multiple sources. We propose to address this problem by integrating a semi-parametric representation of a large text corpus into a Transformer model as a source of factual knowledge. Specifically, our method represents knowledge with `mention memory', a table of dense vector representations of every entity mention in a corpus. The proposed model - TOME - is a Transformer that accesses the information through internal memory layers in which each entity mention in the input passage attends to the mention memory. This approach enables synthesis of and reasoning over many disparate sources of information within a single Transformer model. In experiments using a memory of 150 million Wikipedia mentions, TOME achieves strong performance on several open-domain knowledge-intensive tasks, including the claim verification benchmarks HoVer and FEVER and several entity-based QA benchmarks. We also show that the model learns to attend to informative mentions without any direct supervision. Finally we demonstrate that the model can generalize to new unseen entities by updating the memory without retraining. ",157.29,,Unreleased,"United States of America,United States of America",,,,,,2025-02-17 11:36,,,,524288.0,"""All models are pre-trained on [...] batch size of 4096. Each passage in the batch has length T = 128, excluding entity tokens.""","Academia,Industry",,,,,Open source,"Apache 2.0 (repo license)
https://github.com/google-research/language/tree/master/language/mentionmemory","Academia,Industry",,,FP32,,Operation counting,,,,
Megatron-Turing NLG 530B,Language,Language modeling,"Microsoft,NVIDIA","Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, Elton Zhang, Rewon Child, Reza Yazdani Aminabadi, Julie Bernauer, Xia Song, Mohammad Shoeybi, Yuxiong He, Michael Houston, Saurabh Tiwary, Bryan Catanzaro",2021-10-11,"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model",https://arxiv.org/abs/2201.11990,657.0,"SOTA improvement,Training cost","The 105-layer, transformer-based MT-NLG improved upon the prior state-of-the-art models in zero-, one-, and few-shot settings",530000000000.0,,8.586e+23,"https://www.lesswrong.com/posts/bGuMrzhJdENCo8BxX/nvidia-and-microsoft-releases-530b-parameter-transformer?commentId=HSJSNspKp94tFcSCx

source: https://lair.lighton.ai/akronomicon/
9938 PF-days * 3600 * 24 * 10^15  = 8.586432e+23

6ND estimate: 6 * 530B * 270B = 8.586000e+23","Common Crawl,The Pile,CC-Stories,Realnews"," In addition to Common Crawl data, we leveraged a number of other previously generated datasets. From The Pile, we selected Books3, OpenWebText2, Stack Exchange, PubMed Abstracts,
Wikipedia, Gutenberg (PG-19), BookCorpus2, NIH ExPorter, and Pile-CC datasets. We also included the
CC-Stories and RealNews datasets used to train Megatron",270000000000.0,"""Our training dataset consists of 339 billion tokens and we
trained MT-NLG on 270 billions tokens by blending the 15 training datasets as described above. We also set aside 2% of our data for validation.""

1 token ~ 0.75 words",770.0,"Total compute was 1.17*10^24 FLOP.
They don't directly report the utilization and training speed when using the full Selene supercomputer with 560 DGX * 8 A100/DGX = 4480 GPUs. See section 2.3 Hardware Setup.

At 280 DGX, the utilization is 126/312 = 40% and a batch takes 60 seconds; at 350, it is 39% for 50 seconds; at 420, it is 36% for 44 seconds.

The overall utilization was 30.2% and the full cluster has 560 DGX. Dividing the total compute by the total performance of 4480 A100 at 30.2% utilization gives 770 hours.",NVIDIA A100 SXM4 80 GB,Self-supervised learning,Confident,"Pretrained general-purpose language models can achieve state-of-the-art accuracies in various natural language processing domains by adapting to downstream tasks via zero-shot, few-shot and fine-tuning techniques. Because of their success, the size of these models has increased rapidly, requiring high-performance hardware, software, and algorithmic techniques to enable training such large models. As the result of a joint effort between Microsoft and NVIDIA, we present details on the training of the largest monolithic transformer based language model, Megatron-Turing NLG 530B (MT-NLG), with 530 billion parameters. In this paper, we first focus on the infrastructure as well as the 3D parallelism methodology used to train this model using DeepSpeed and Megatron. Next, we detail the training process, the design of our training corpus, and our data curation techniques, which we believe is a key ingredient to the success of the model. Finally, we discuss various evaluation results, as well as other interesting observations and new properties exhibited by MT-NLG. We demonstrate that MT-NLG achieves superior zero-, one-, and few-shot learning accuracies on several NLP benchmarks and establishes new state-of-the-art results. We believe that our contributions will help further the development of large-scale training infrastructures, large-scale language models, and natural language generations.",,,Unreleased,"United States of America,Multinational,India,Belgium,United States of America",,,,4480.0,0.302,2025-06-18 12:38,,,,3932160.0,"""The sequence length is 2048 and the global batch size is 1920. We used 8-way tensor and 35-way pipeline parallelism. The learning rate is 5.0e âˆ’5 . We used one billion tokens for linear learning rate warmup. We used cosine decay for the learning rate targeting to reach 10% of its value over 340 billion tokens. Over the first 12 billion tokens, we started at a batch size of 32 and gradually increased the batch size in increments of 32, until we reach the final batch size of 1920"" 

Final batch size is 1920 * 2048 = 3932160","Industry,Industry",checked,,,3449600.0,Unreleased,,"Industry,Industry",$3704291.31,"They don't directly report the utilization and training speed when using the full Selene supercomputer with 560 DGX * 8 A100/DGX = 4480 GPUs. See section 2.3 Hardware Setup.

At 280 DGX, the utilization is 126/312 = 40% and a batch takes 60 seconds; at 350, it is 39% for 50 seconds; at 420, it is 36% for 44 seconds.

The overall utilization was 30.2% and the full cluster has 560 DGX. ",BF16,3615658.868639838,Third-party estimation,,,,1
AlphaFold-Multimer,Biology,"Protein folding prediction,Proteins","Google DeepMind,DeepMind","Richard Evans, Michael Oâ€™Neill, Alexander Pritzel, Natasha Antropova, Andrew Senior, Tim Green, Augustin Å½Ã­dek, Russ Bates, Sam Blackwell, Jason Yim, Olaf Ronneberger, Sebastian Bodenstein, Michal Zielinski, Alex Bridgland, Anna Potapenko, Andrew Cowie, Kathryn Tunyasuvunakool, Rishub Jain, Ellen Clancy, Pushmeet Kohli, John Jumper and Demis Hassabis",2021-10-04,Protein complex prediction with AlphaFold-Multimer,https://www.biorxiv.org/content/10.1101/2021.10.04.463034v1,1762.0,"Highly cited,SOTA improvement","""On a benchmark dataset of 17 heterodimer proteins without templates (introduced in [2]) we achieve at least medium accuracy (DockQ [3] â‰¥ 0.49) on 14 targets and high accuracy (DockQ â‰¥ 0.8) on 6 targets, compared to 9 targets of at least medium accuracy and 4 of high accuracy for the previous state of the art system (an AlphaFold-based system from [2])""

""For heteromeric interfaces we successfully predict the interface (DockQ â‰¥ 0.23) in 67% of cases, and produce high accuracy predictions (DockQ â‰¥ 0.8) in 23% of cases, an improvement of +25 and +11 percentage points over the flexible linker modification of AlphaFold [4] respectively""

""For homomeric interfaces we successfully predict the interface in 69% of cases, and produce high accuracy predictions in 34% of cases, an improvement of +5 percentage points in both instances""",,"""Multiple changes to the AlphaFold system were made to adapt it to training on protein complexes, which are detailed below. Summarizing briefly, we [...] make various small adjustments to the structure losses and the model architecture."" [2. Methods]

Hence, this will have approximately the same amount of parameters as AlphaFold2",4.35e+21,"Section: 2.5. Training Regimen
""We train the model to convergence (approximately 10M samples, for 2 weeks) across 128 TPUv3 cores [...]. Then we [...] run two separate fine-tuning stages (one further day of training each)""

Assuming: FP16 and utilization 0.4

Calculation: (14+2) days * 24 hours/day * 60 min/hour * 60 sec/min * (128 TPU cores/2 cores per chip) * 1.23e14 FLOP/s per chip * 0.4 utilization = 4.35e21 FLOPs",PDB (Protein Data Bank),"""The training dataset comprised structures from the Protein Data Bank (PDB) [13] with a maximum release date of 2018-04-30"" [2.5. Training Regimen]",147328.0,"See: https://www.rcsb.org/stats/growth/growth-released-structures for 2018

""We train the model to convergence (approximately 10M samples, for 2 weeks) across 128 TPUv3 cores with a batch size of 1 per TPU core. Then we halve the learning rate and double the number of sequences fed into the MSA stack before running two separate fine-tuning stages (one further day of training each)""

10000000/147328 ~ 68 epochs",384.0,"Section: 2.5. Training Regimen
""We train the model to convergence (approximately 10M samples, for 2 weeks) across 128 TPUv3 cores [...]. Then we [...] run two separate fine-tuning stages (one further day of training each)""",Google TPU v3,,Confident,"While the vast majority of well-structured single protein chains can now be predicted to high accuracy due to the recent AlphaFold [1] model, the prediction of multi-chain protein complexes remains a challenge in many cases. In this work, we demonstrate that an AlphaFold model trained specifically for multimeric inputs of known stoichiometry, which we call AlphaFold-Multimer, significantly increases accuracy of predicted multimeric interfaces over input-adapted single-chain AlphaFold while maintaining high intra-chain accuracy. On a benchmark dataset of 17 heterodimer proteins without templates (introduced in [2]) we achieve at least medium accuracy (DockQ [3] â‰¥ 0.49) on 14 targets and high accuracy (DockQ â‰¥ 0.8) on 6 targets, compared to 9 targets of at least medium accuracy and 4 of high accuracy for the previous state of the art system (an AlphaFold-based system from [2]). We also predict structures for a large dataset of 4,433 recent protein complexes, from which we score all non-redundant interfaces with low template identity. For heteromeric interfaces we successfully predict the interface (DockQ â‰¥ 0.23) in 67% of cases, and produce high accuracy predictions (DockQ â‰¥ 0.8) in 23% of cases, an improvement of +25 and +11 percentage points over the flexible linker modification of AlphaFold [4] respectively. For homomeric interfaces we successfully predict the interface in 69% of cases, and produce high accuracy predictions in 34% of cases, an improvement of +5 percentage points in both instances.",68.0,,Open weights (unrestricted),"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France,United Kingdom of Great Britain and Northern Ireland",AlphaFold 2,,,64.0,,2025-04-30 10:04,,,,,,"Industry,Industry",,,,24576.0,Unreleased,"While the AlphaFold code is licensed under the Apache 2.0 License, the AlphaFold parameters and CASP15 prediction data are made available under the terms of the CC BY 4.0 license

https://github.com/google-deepmind/alphafold","Industry,Industry",$7966.23,,,28413.1771135238,Hardware,,,,
TrOCR,Vision,Character recognition,"Beihang University,Microsoft Research Asia","Minghao Li, Tengchao Lv, Jingye Chen, Lei Cui, Yijuan Lu, Dinei Florencio, Cha Zhang, Zhoujun Li, Furu Wei",2021-09-21,TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models,https://arxiv.org/abs/2109.10282,246.0,SOTA improvement,"from conclusion ""Experiment results show that TrOCR achieves state-of-the-art results on printed, handwritten and scene text recognition with just a simple encoder-decoder model, without any post-processing steps""",558000000.0,558M table 5,,May be computed from github and datasets details. Uses pretrained BEiT and DeiT models.,,"""To build a large-scale high-quality dataset, we sample two million document pages from the publicly available PDF files on the Internet.""
From the Experiment section: ""In total, the first-stage pre-training dataset contains 684M textlines."" ""The handwritten dataset for the second-stage pre-training consists of 17.9M textlines"" ""In total, the printed dataset consists of 3.3M textlines.""
and from MJSynth, SynthText datasets there is ""about 16M text images.""",721200000.0,"The input data to the model are images.
684M + 17.9M + 3.3M + 16M",,,NVIDIA V100,,Confident,"Text recognition is a long-standing research problem for document digitalization. Existing approaches are usually built based on CNN for image understanding and RNN for char-level text generation. In addition, another language model is usually needed to improve the overall accuracy as a post-processing step. In this paper, we propose an end-to-end text recognition approach with pre-trained image Transformer and text Transformer models, namely TrOCR, which leverages the Transformer architecture for both image understanding and wordpiece-level text generation. The TrOCR model is simple but effective, and can be pre-trained with large-scale synthetic data and fine-tuned with human-labeled datasets. Experiments show that the TrOCR model outperforms the current state-of-the-art models on the printed, handwritten and scene text recognition tasks. The TrOCR models and code are publicly available at https://aka.ms/trocr",,,Open weights (unrestricted),"China,China",,,,32.0,,2024-11-01 10:03,,,,,,"Academia,Industry",,,,,Open source,MIT: https://github.com/microsoft/unilm/tree/master/trocr,"Academia,Industry",,,,19378.229980216885,,,,,
LM-GVP,Biology,Protein property prediction,"Amazon Machine Learning Solutions Lab,Johnson & Johnson","Zichen Wang, Steven A. Combs, Ryan Brand, Miguel Romero Calvo, Panpan Xu, George Price, Nataliya Golovach, Emmanuel O. Salawu, Colby J. Wise, Sri Priya Ponnapalli, Peter M. Clark",2021-09-21,LM-GVP: A Generalizable Deep Learning Framework for Protein Property Prediction from Sequence and Structure,https://www.biorxiv.org/content/10.1101/2021.09.21.460852v1.abstract,10.0,,,,,,,,,,,,,NVIDIA V100,,Unknown,"Proteins perform many essential functions in biological systems and can be successfully developed as bio-therapeutics. It is invaluable to be able to predict their properties based on a proposed sequence and structure. In this study, we developed a novel generalizable deep learning framework, LM-GVP, composed of a protein Language Model (LM) and Graph Neural Network (GNN) to leverage information from both 1D amino acid sequences and 3D structures of proteins. Our approach outperformed the state-of-the-art protein LMs on a variety of property prediction tasks including fluorescence, protease stability, and protein functions from Gene Ontology (GO). We also illustrated insights into how a GNN prediction head can guide the protein LM to better leverage structural information. We envision that our deep learning framework will be generalizable to many protein property prediction problems to greatly accelerate protein engineering and drug development.",,,Open weights (non-commercial),"United States of America,United States of America",,,,8.0,,2025-06-11 17:16,,,,,,"Industry,Industry",,,,,Open (non-commercial),"""The source code for training end-to-end models, together with the neural network weights are
available for research and non-commercial use at https://github.com/aws-samples/lm-gvp""

""Datasets used in this study are available to download at:
https://github.com/flatironinstitute/DeepFRI/tree/master/preprocessing/data and
https://github.com/songlab-cal/tape""","Industry,Industry",,,,4844.557495054221,Hardware,,,,
PLATO-XL,Language,Language modeling,Baidu,"Siqi Bao, Huang He, Fan Wang, Hua Wu, Haifeng Wang, Wenquan Wu, Zhihua Wu, Zhen Guo, Hua Lu, Xinxian Huang, Xin Tian, Xinchao Xu, Yingzhan Lin, Zheng-Yu Niu",2021-09-20,PLATO-XL: Exploring the Large-scale Pre-training of Dialogue Generation,https://arxiv.org/abs/2109.09519,57.0,SOTA improvement,,11000000000.0,,9.9e+21,"""In PLATO-XL, each model was trained for a total of 150B tokens, with
a batch size of 2M tokens.""

150B * 11B * 6 = 9.9e21",,,150000000000.0,"""In PLATO-XL, each model was trained for a total of 150B tokens, with
a batch size of 2M tokens.""
811M English and 1.2B Chinese (context, response) samples. So if the average response is at least 75 tokens, the 150B tokens seen in training don't include repeat tokens. This seems plausible.",,,NVIDIA V100,,Confident,"To explore the limit of dialogue generation pre-training, we present the models of PLATO-XL with up to 11 billion parameters, trained on both Chinese and English social media conversations. To train such large models, we adopt the architecture of unified transformer with high computation and parameter efficiency. In addition, we carry out multi-party aware pre-training to better distinguish the characteristic information in social media conversations. With such designs, PLATO-XL successfully achieves superior performances as compared to other approaches in both Chinese and English chitchat. We further explore the capacity of PLATO-XL on other conversational tasks, such as knowledge grounded dialogue and task-oriented conversation. The experimental results indicate that PLATO-XL obtains state-of-the-art results across multiple conversational tasks, verifying its potential as a foundation model of conversational AI.",,,Open weights (unrestricted),China,,,,256.0,,2025-06-05 16:59,,,,,,Industry,,,,,Unreleased,"apache 2.0
https://github.com/PaddlePaddle/Knover/tree/develop/projects/PLATO-XL",Industry,,,,155029.29220816068,Operation counting,,,,
MegaMolBART,Biology,Drug discovery,NVIDIA,,2021-09-14,MegaMolBART,"https://docs.nvidia.com/bionemo-framework/0.4.0/models/megamolbart.html, https://github.com/NVIDIA/MegaMolBART",,,,45000000.0,"""MegaMolBART has eight layers, four attention heads, a hidden space dimension of 256, and contains 45M parameters""",7.2e+20,"""MegaMolBART was trained with data parallelism on 64 V100 32 GB GPUs (4 nodes x 16 GPUs) for 8 epochs (approximately 160k iterations or ~80 wall-clock hours) using a batch size of 32 molecules per GPU (micro batch)""

https://docs.nvidia.com/bionemo-framework/0.4.0/models/megamolbart.html

64 * 130 teraflops * 80 * 3600 * 0.3 = 7.2e20",ZINC 15,"""The ZINC-15 database was used for training [Sterling and Irwin, 2015]. Approximately 1.54 Billion molecules (SMILES strings) were selected from tranches meeting the following constraints: molecular weight <= 500 Daltons, LogP <= 5, reactivity level was â€œreactive,â€ and purchasability was â€œannotated.â€ The compounds were filtered to ensure a maximum length of 512 characters.""",,"1.54B molecules, maximum length of 512 characters. Perhaps ~100B tokens",80.0,,NVIDIA V100,,Likely,"MegaMolBART is a model that understands chemistry and can be used for a variety of cheminformatics applications in drug discovery. The embeddings from its encoder can be used as features for predictive models. Alternatively, the encoder and decoder can be used together to generate novel molecules by sampling the modelâ€™s embedding space.",8.0,,Open weights (unrestricted),United States of America,,,,,,2025-04-30 10:04,,,,,,Industry,,,,,Open source,"apache, inference and training code: https://github.com/NVIDIA/MegaMolBART/blob/dev/LICENSE/license.txt",Industry,,,,,Hardware,,,,
HyperCLOVA 204B,Language,Language modeling/generation,NAVER,,2021-09-10,"South Korea's Naver unveils 'hyperscale AI' platform, language model with more parameters than GPT-3",https://aibusiness.com/nlp/south-korea-s-naver-unveils-hyperscale-ai-platform-language-model-with-more-parameters-than-gpt-3,92.0,SOTA improvement,"""HyperCLOVA with our training configuration shows state-of-the-art in-context zero-shot and few-shot learning performances on various downstream tasks in Korean""",204000000000.0,https://www.navercorp.com/navercorp_/ir/announce/2023/NAVER_CEO%20letter%20to%20shareholders_Aug%202023_Eng.pdf,2.0000000001e+23,"Estimations for 82B model (marked as lower bound estimations)

""For experiments in Section 4, the model trained with 150B is used for fair comparison, because not all models are finished training at the same iteration. However, experiments in Section 5.2 use the model trained with 300B tokens, as HyperCLOVA Studio provided the 39B and 82B models trained with 300B tokens.""

82e9 connections * 2 FLOP/connection * 300e9 tokens * 3 backward pass = 1.476e23 FLOP

Calculation using GPU time corroborates this:
- ""Our model is based on megatron-LM (Shoeybi et al., 2019) and trained on the NVIDIA Superpod, which includes 128 strongly clustered DGX servers with 1,024 A100 GPUs.""
- ""It takes 13.4 days to train a model with 82B parameters with 150B tokens."" Assume 300B tokens takes twice as long, 26.8 days.
- Assume the default of 30% utilization rate for large language models.

1024 A100 GPUs * 312e12 FLOP/second * 0.3 utilization * 26.8 days * 24 * 60 * 60 seconds/day = 2.219e+23 FLOP",Unspecified unreleased,,560000000000.0,"https://twitter.com/arankomatsuzaki/status/1397583304610783238
https://venturebeat.com/ai/naver-trained-a-gpt-3-like-korean-language-model/",,,NVIDIA A100,Self-supervised learning,Speculative,,,,Hosted access (no API),Korea (Republic of),,,,,,2025-06-02 15:42,,,,,,Industry,checked,1.476e+23,,,Unreleased,,Industry,,,,,,,,,5
NLM,Language,Language modeling,"Carnegie Mellon University (CMU),University of California San Diego","Junxian He, Graham Neubig, Taylor Berg-Kirkpatrick",2021-09-09,Efficient Nearest Neighbor Language Models,https://arxiv.org/abs/2109.04212,92.0,,,247000512.0,"The core language model is semi-parametric. There is a non-parametric component which involves learning a datastore which maps the training data to context vectors. This paper improves on the kNN-LM model primarily by reducing the size of the datastore to improve throughput. They also add a small 512 parameter neural net to adaptively adjust the interpolation hyperparameter.

They use the pre-trained model from https://arxiv.org/abs/1809.10853, which had 247M trainable parameters, so the model in this paper one has 247M + 512 parameters.",2.7830000000000004e+19,"The base model used was 247M parameters and trained for 286k updates with an effective batch size of 65,536 tokens.

6 * 247M * 286k * 65536 = 2.778e19 FLOPs

Generating the datastore involves a single forward pass over all of the training examples, for another
2 * 247M * 103M = 5.088e16 FLOPs

They also train the small retrieval adapter, but this converges in ""several minutes with a single GPU"" and so is unlikely to contribute significantly.

The only other computationally expensive step is the k-means datastore pruning: ""we perform 5000 separate k-means clustering passes only for the most frequent 5000 words due to high computational cost"". These are likely computed on CPUs; they have 32 CPU cores from 1.5GHz AMD EPYC 7282. Assuming that this means 32 processors, and not the 32 threads in a single processor, and assuming 4 cycles per multiplication, these CPUs could do 32 * 32 * (1.5e9 / 4) = 3.84e11 FLOP/s, so putting a dent in the 2.8e19 FLOPs from the base model would take a prohibitive amount of time.

Total: 2.783e19",WikiText-103,"""WikiText-103 (Merity et al., 2017) is a standard language modeling benchmark from Wikipedia that has 250K word-level vocabulary. It consists of 103M training tokens""",103000000.0,103M training tokens in WikiText-103,,,NVIDIA GeForce RTX 3090,,Confident,"Non-parametric neural language models (NLMs) learn predictive distributions of text utilizing an external datastore, which allows them to learn through explicitly memorizing the training datapoints. While effective, these models often require retrieval from a large datastore at test time, significantly increasing the inference overhead and thus limiting the deployment of non-parametric NLMs in practical applications. In this paper, we take the recently proposed k-nearest neighbors language model (Khandelwal et al., 2020) as an example, exploring methods to improve its efficiency along various dimensions. Experiments on the standard WikiText-103 benchmark and domain-adaptation datasets show that our methods are able to achieve up to a 6x speed-up in inference speed while retaining comparable performance. The empirical analysis we present may provide guidelines for future research seeking to develop or deploy more efficient non-parametric NLMs.",,NLM,Unreleased,"United States of America,United States of America",,,,1.0,,2025-02-17 11:41,,,,,,"Academia,Academia",,,,,Open source,MIT for code: https://github.com/jxhe/efficient-knnlm,"Academia,Academia",,,,388.2894463979599,Operation counting,,,,
PermuteFormer,Language,Language modeling,Peking University,Peng Chen,2021-09-06,PermuteFormer: Efficient Relative Position Encoding for Long Sequences,https://arxiv.org/abs/2109.02377,20.0,SOTA improvement,"""Results show that
PermuteFormer uniformly improves the performance of Performer, accelerates convergence, and
achieves state-of-the-art on some tasks.""",149697024.0,"Parameterization appears to be similar to a vanilla transformer. 6 layers, hidden dimension of 512, feed forward dimension of 1024, 8 attention heads. This would imply 20,447,232 parameters without embedding weights, and 512*vocab_size embedding weights (assuming tied embedding and unembedding projections)

They appear to use word-level tokenization: ""We evaluate unidirectional PermuteFormer on WikiText-103 (Merity et al., 2017). It is a language modeling dataset with about 103 million tokens,"" and I confirmed that word-level tokenization results in about 102M words across train-test-validation.

If this is the case, there are 267,735 unique words, so the embedding layer alone would be 137,080,320 parameters, for a total of 149,697,024.",2.775e+18,"6 * (30 * 103M) * 149,697,024 = 2.775e18

This seems a bit small relative to their statement: ""It takes about 10 days on 8 V100 GPUs to get all the figures in this paper"" which suggests about 2.7e20 FLOPs at 30% MFU.

Table 2 indicates that Performer and PermuteFormer take 0.23x to 0.58x as long to train as a Transformer model. Figure 2 appears to be the most compute intensive figure, and would take about 4 * (2.775e18) + 1 * (2.775e18 / 0.365) = 1.9e19 FLOPs. ",WikiText-103,,103000000.0,WikiText-103 is about 103M tokens,,Running all code needed to produce plots took about 10 days on 8 V100s,NVIDIA V100,,Speculative,"A recent variation of Transformer, Performer, scales Transformer to longer sequences with a linear attention mechanism. However, it is not compatible with relative position encoding, which has advantages over absolute position encoding. In this paper, we discuss possible ways to add relative position encoding to Performer. Based on the analysis, we propose PermuteFormer, a Performer-based model with relative position encoding that scales linearly on long sequences. PermuteFormer applies position-dependent transformation on queries and keys to encode positional information into the attention module. This transformation is carefully crafted so that the final output of self-attention is not affected by absolute positions of tokens. PermuteFormer introduces negligible computational overhead by design that it runs as fast as Performer. We evaluate PermuteFormer on Long-Range Arena, a dataset for long sequences, as well as WikiText-103, a language modeling dataset. The experiments show that PermuteFormer uniformly improves the performance of Performer with almost no computational overhead and outperforms vanilla Transformer on most of the tasks.",30.0,PermuteFormer,Unreleased,China,,,,8.0,,2025-02-17 11:42,,,,,,Academia,,,,,Open source,no specific license. training code: https://github.com/cpcp1998/PermuteFormer/tree/master/language_model ,Academia,,,,4846.17604411253,Operation counting,,,,
RNS-RNN,Language,Language modeling,University of Notre Dame,"Brian DuSell, David Chiang",2021-09-05,Learning Hierarchical Structures with Differentiable Nondeterministic Stacks,https://arxiv.org/abs/2109.01982,12.0,,,5751892.0,Table 1,,"FLOPs per epoch is possibly calculable, but they don't say how many epochs, just that they stopped after 2 without improvement.",Penn TreeBank (PTB),,912344.0,"Penn TreeBank's training split has 912,344 tokens.",,,,,Confident,"Learning hierarchical structures in sequential data -- from simple algorithmic patterns to natural language -- in a reliable, generalizable way remains a challenging problem for neural language models. Past work has shown that recurrent neural networks (RNNs) struggle to generalize on held-out algorithmic or syntactic patterns without supervision or some inductive bias. To remedy this, many papers have explored augmenting RNNs with various differentiable stacks, by analogy with finite automata and pushdown automata (PDAs). In this paper, we improve the performance of our recently proposed Nondeterministic Stack RNN (NS-RNN), which uses a differentiable data structure that simulates a nondeterministic PDA, with two important changes. First, the model now assigns unnormalized positive weights instead of probabilities to stack actions, and we provide an analysis of why this improves training. Second, the model can directly observe the state of the underlying PDA. Our model achieves lower cross-entropy than all previous stack RNNs on five context-free language modeling tasks (within 0.05 nats of the information-theoretic lower bound), including a task on which the NS-RNN previously failed to outperform a deterministic stack RNN baseline. Finally, we propose a restricted version of the NS-RNN that incrementally processes infinitely long sequences, and we present language modeling results on the Penn Treebank.",100.0,RNS-RNN,Unreleased,United States of America,,,,,,2024-09-28 22:18,,,,1120.0,"Sequence length 35, minibatches of 32",Academia,,,,,Open (non-commercial),"code here, unclear license:
https://github.com/bdusell/nondeterministic-stack-rnn",Academia,,,,,,,,,
FLAN 137B,Language,"Language modeling,Question answering,Language modeling/generation",Google Research,"Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le",2021-09-03,Finetuned Language Models Are Zero-Shot Learners,https://arxiv.org/abs/2109.01652,2994.0,"Highly cited,SOTA improvement","Abstract: 
""FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 datasets that we evaluate.""",137000000000.0,"Abstract:
""We take a 137B parameter pretrained language model and instruction tune it on
over 60 NLP datasets verbalized via natural language instruction templates. We
evaluate this instruction-tuned model, which we call FLAN, on unseen task types.""

Many models seem to be using the same 137B base transformer model?",2.047e+24,"From section 2.4: Pretraining was done over 2.49T tokens.
6 * 2.49T * 137B = 2.047e24 
Also, ""instruction tuning takes around 60 hours on a TPUv3 with 128 cores."" 128 TPUv3 cores = 64 TPUv3 chips. Environmental considerations section claims this took less than 2% of total time
1.23e14 * 64 * 60 * 3600 * 0.3 = 5.10e20","Wikipedia,Unspecified unreleased","Abstract: ""We take a 137B parameter pretrained language model and instruction tune it on over 60 NLP datasets""",2490000000000.0,"""Model architecture and pretraining. In our experiments, we use LaMDA-PT, a dense left-to-right, decoder-only transformer language model of 137B parameters (Thoppilan et al., 2022). This model is pretrained on a collection of web documents (including those with computer code), dialog data, and Wikipedia, tokenized into 2.49T BPE tokens with a 32k vocabulary using the  SentencePiece library (Kudo & Richardson, 2018). Around 10% of the pretraining data was non-English. Note that LaMDA-PT only has language model pretraining (c.f. LaMDA, which was finetuned for dialog).""",,,Google TPU v3,Self-supervised learning,Confident,"This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuningâ€”finetuning language models on a collection of datasets described via instructionsâ€”substantially improves zeroshot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction tune it on over 60 NLP datasets verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 datasets that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning.",1.0,,Unreleased,"Multinational,United States of America,Canada,Switzerland",LaMDA,,"""In our experiments, we use LaMDA-PT, a dense left-to-right,
decoder-only transformer language model of 137B parameters (Thoppilan et al., 2022) [...] Note that LaMDA-PT only has language model pretraining (c.f. LaMDA, which was finetuned for dialog)."" In our entry for LaMDA we only measured pre-training compute, so we just specify LaMDA as the base model of FLAN 137B.",,,2025-05-16 10:30,,,,,,Industry,,,,3840.0,Unreleased,,Industry,$230526.76,,,,Operation counting,,,,
$\infty$-former (SM),Language,Language modeling/generation,"Universidade de Lisboa (ULisboa),DeepMind","Pedro Henrique Martins, Zita Marinho, AndrÃ© F. T. Martins",2021-09-01,$\infty$-former: Infinite Memory Transformer,https://arxiv.org/abs/2109.00301,31.0,,,124000000.0,assuming same as the base model GPT-2 small,1.2e+22,"7.936 Ã— 10^20 FLOP [base model compute estimation, ""Speculative"" confidence] + 1.488e+17 FLOP [fine-tune compute] =  7.937488e+20 FLOP","WikiText-103,PG-19 (Project Gutenberg)","""we fine-tune GPT-2 small (Radford et al., 2019) on Wikitext103 (Merity et al., 2017) and a subset of PG-19 (Rae et al., 2019) containing the first 2,000 books (â‰ˆ 200 million tokens) of the training set""",200000000.0,,,,NVIDIA GeForce RTX 2080 Ti 11GB,,Speculative,"Transformers are unable to model long-term memories effectively, since the amount of computation they need to perform grows with the context length. While variations of efficient transformers have been proposed, they all have a finite memory capacity and are forced to drop old information. In this paper, we propose the \infty-former, which extends the vanilla transformer with an unbounded long-term memory. By making use of a continuous-space attention mechanism to attend over the long-term memory, the \infty-former's attention complexity becomes independent of the context length, trading off memory length with precision. In order to control where precision is more important, \infty-former maintains ""sticky memories"" being able to model arbitrarily long contexts while keeping the computation budget fixed. Experiments on a synthetic sorting task, language modeling, and document grounded dialogue generation demonstrate the \infty-former's ability to retain information from long sequences.",1.0,$\infty$-former (SM),Unreleased,"Portugal,United Kingdom of Great Britain and Northern Ireland",GPT-2 (124M),148800000000000000,"""To understand if long-term memories can be used to extend a pre-trained language model, we fine-tune GPT-2 small (Radford et al., 2019) on Wikitext103 (Merity et al., 2017) and a subset of PG-19 (Rae et al., 2019) containing the first 2,000 books (â‰ˆ 200 million tokens) of the training set""

6 FLOP / token / parameter * 124000000 parameters * 200000000 tokens * 1 epoch = 1.488e+17 FLOP 
",1.0,,2025-04-07 10:42,,,,,,"Academia,Industry",,,,,Open (non-commercial),"fine-tune code (this is a GPT-2 finetune), no clear license: https://github.com/deep-spin/infinite-former ","Academia,Industry",,,,277.3990201821971,Operation counting,,,,
HJRSS,Biology,Protein design,"University of Washington,Microsoft","Sanaa Mansoor, Minkyung Baek, Umesh Madan, Eric Horvitz",2021-09-01,Toward More General Embeddings for Protein Design: Harnessing Joint Representations of Sequence and Structure,https://www.biorxiv.org/content/10.1101/2021.09.01.458592v1.abstract,19.0,,,16000000.0,,,,trRosetta,,,,,,NVIDIA V100,,Confident,"Protein embeddings learned from aligned sequences have been leveraged in a wide array of tasks in protein understanding and engineering. The sequence embeddings are generated through semi-supervised training on millions of sequences with deep neural models defined with hundreds of millions of parameters, and they continue to increase in performance on target tasks with increasing complexity. We report a more data-efficient approach to encode protein information through joint training on protein sequence and structure in a semi-supervised manner. We show that the method is able to encode both types of information to form a rich embedding space which can be used for downstream prediction tasks. We show that the incorporation of rich structural information into the context under consideration boosts the performance of the model by predicting the effects of single-mutations. We attribute increases in accuracy to the value of leveraging proximity within the enriched representation to identify sequentially and spatially close residues that would be affected by the mutation, using experimentally validated or predicted structures.",,,,"United States of America,United States of America,Multinational,India,Belgium",ESM1b,,,1.0,,2025-05-01 10:42,,,,,,"Academia,Industry",,,,,,,"Academia,Industry",,,,332.8788242186365,,,,,
"ALiBi (L=3072, Lvalid = 3072)",Language,Language modeling,"University of Washington,Facebook AI Research,Allen Institute for AI","Ofir Press, Noah A. Smith, Mike Lewis",2021-08-27,"Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",https://arxiv.org/abs/2108.12409,557.0,,,1300000000.0,,8.1e+20,"From figure 5, 6000 GPU hours (Nvidia V100) 

6000*  125 teraflop/s * 3600 * 0.3 = 8.1e20",WikiText-103,"""We first test the extrapolation abilities of various position methods on the WikiText-103 corpus... The training set is about 103 million tokens from English Wikipedia""",103000000.0,"""The training set is about 103 million tokens from English Wikipedia""",,,,,Confident,"Since the introduction of the transformer model by Vaswani et al. (2017), a fundamental question has yet to be answered: how does a model achieve extrapolation at inference time for sequences that are longer than it saw during training? We first show that extrapolation can be enabled by simply changing the position representation method, though we find that current methods do not allow for efficient extrapolation. We therefore introduce a simpler and more efficient position method, Attention with Linear Biases (ALiBi). ALiBi does not add positional embeddings to word embeddings; instead, it biases query-key attention scores with a penalty that is proportional to their distance. We show that this method trains a 1.3 billion parameter model on input sequences of length 1024 that extrapolates to input sequences of length 2048, achieving the same perplexity as a sinusoidal position embedding model trained on inputs of length 2048 but training 11% faster and using 11% less memory. ALiBi's inductive bias towards recency also leads it to outperform multiple strong position methods on the WikiText-103 benchmark.",205.0,"""ALiBi (L=3072, Lvalid = 3072)""",Open weights (unrestricted),"United States of America,United States of America,United States of America",,,,,,2025-01-03 15:52,,,,,,"Academia,Industry,Research collective",,,,,Open source,"weights and training/inference code, MIT: https://github.com/ofirpress/attention_with_linear_biases","Academia,Industry,Research collective",,,,,Hardware,,,,
XLMR-XXL,Language,"Translation,Language modeling/generation",Facebook AI Research,"Naman Goyal, Jingfei Du, Myle Ott, Giri Anantharaman, Alexis Conneau",2021-08-17,Larger-Scale Transformers for Multilingual Masked Language Modeling,https://arxiv.org/abs/2105.00572,107.0,SOTA improvement,"Abstract:
""Our model also outperforms
the RoBERTa-Large model on several English tasks of the GLUE benchmark by 0.3% on average while handling 99 more languages.""",10700000000.0,"Section 2.1:
"" ...XLM-RXXL (L= 48, H = 4096, A = 32, 10.7B params)""",3.3660000000000005e+22,"Trained for 500k steps at a batch size of 2048 with sequence length of 512 = 524,288,000,000 tokens seen.
6 * 10700000000 * 524,288,000,000 = 3.366e22",CC100,,167000000000.0,"""We pretrain the models on the CC100 dataset, which corresponds to 167B tokens in 100 languages.""",,,,,Confident,"Recent work has demonstrated the effectiveness of cross-lingual language model pretraining for cross-lingual understanding. In this study, we present the results of two larger multilingual masked language models, with 3.5B and 10.7B parameters. Our two new models dubbed XLM-R XL and XLM-R XXL outperform XLM-R by 1.8% and 2.4% average accuracy on XNLI. Our model also outperforms the RoBERTa-Large model on several English tasks of the GLUE benchmark by 0.3% on average while handling 99 more languages. This suggests pretrained models with larger capacity may obtain both strong performance on high-resource languages while greatly improving low-resource languages. We make our code and models publicly available.",3.14,,Open weights (unrestricted),United States of America,,,,,,2025-02-17 11:43,,,,1048576.0,Batches of 2048 with sequence length of 512,Industry,,,,,Unreleased,https://github.com/facebookresearch/fairseq/tree/main/examples/xlmr,Industry,,,,,Operation counting,,,,20
ProteinLM,Biology,"Proteins,Protein or nucleotide language model (pLM/nLM)","Tsinghua University,Beijing Academy of Artificial Intelligence / BAAI,Tencent","Yijia Xiao, Jiezhong Qiu, Ziang Li, Chang-Yu Hsieh, Jie Tang
",2021-08-17,Modeling Protein Using Large-scale Pretrain Language Model,https://arxiv.org/abs/2108.07435c,24.0,,,3000000000.0,"""We have trained multiple largescale models on the PFAM[7] dataset, the largest with 3 billion parameters""",1.5999999999999998e+22,"""We pretrained two large models on a 480 GPUs (TeslaV100-32GB) cluster for about three weeks""

21 * 24* 3600 * 480 * 125 teraFLOP/s * 0.3 (utilization) * 0.5 (two models) = 1.6e22",Pfam,"""PFAM[7] is a widely-used database consisting of more than 32 million protein sequences""",9000000001.0,"Total sequences: 32,000,000
Testing (1%): 32,000,000 Ã— 0.01 = 320,000
Remaining: 32,000,000 - 320,000 = 31,680,000
Training (95%): 31,680,000 Ã— 0.95 = 30,096,000
Final calculation: 30,096,000 sequences Ã— 300 tokens/sequence = 9,028,800,000 tokens",252.0,,NVIDIA V100,Self-supervised learning,Confident,"Protein is linked to almost every life process. Therefore, analyzing the biological structure and property of protein sequences is critical to the exploration of life, as well as disease detection and drug discovery. Traditional protein analysis methods tend to be labor-intensive and time-consuming. The emergence of deep learning models makes modeling data patterns in large quantities of data possible. Interdisciplinary researchers have begun to leverage deep learning methods to model large biological datasets, e.g. using long short-term memory and convolutional neural network for protein sequence classification. After millions of years of evolution, evolutionary information is encoded in protein sequences. Inspired by the similarity between natural language and protein sequences, we use large-scale language models to model evolutionary-scale protein sequences, encoding protein biology information in representation. Significant improvements are observed in both token- level and sequence-level tasks, demonstrating that our large-scale model can accurately capture evolution information from pretraining on evolutionary-scale individual sequences. Our code and model are available at https://github.com/THUDM/ProteinLM.",,,Open weights (unrestricted),"China,China,China",,,,48.0,,2025-05-09 11:32,,,,,,"Academia,Academia,Industry",,,,12096.0,Open source,"apache 2.0
https://github.com/THUDM/ProteinLM","Academia,Academia,Industry",,,,29090.009704341683,Hardware,,,,
DNABERT,Biology,Protein or nucleotide language model (pLM/nLM),Northeastern University,"Yanrong Ji, Zhihan Zhou, Han Liu, Ramana V Davuluri",2021-08-15,DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome,https://academic.oup.com/bioinformatics/article/37/15/2112/6128680,479.0,SOTA improvement,"""We show that the single pre-trained transformers model can simultaneously achieve state-of-the-art performance on prediction of promoters, splice sites and transcription factor binding sites, after easy fine-tuning using small task-specific labeled data."" [Abstract] - SOTA improvement on very specific task",110000000.0,"""We used the same model architecture as the BERT base, which consists of 12 Transformer layers with 768 hidden units and 12 attention heads in each layer, and the same parameter setting across all the four DNABERT models during pre-training""

Known to have 110 million parameters as reported in: https://arxiv.org/pdf/1810.04805v2.pdf
""We primarily report results on two model sizes: BERTBASE (L=12, H=768, A=12, Total Parameters=110M) [...]""",1.0700000000000002e+20,"""Since the pre-training of DNABERT model is resource-intensive (about 25â€‰days on 8 NVIDIA 2080Ti GPUs)""

Assuming FP16 and 30% utilization

Calculation = (25 * 24 *3600) s * 2.7e13 FLOP/s per GPU * 8 GPUs * 0.3 utilization = 1.4e20 FLOP

Alternatively:
""DNABERT takes a sequence with a max length of 512 as input... We pre-trained DNABERT for 120k steps with a batch size of 2000""
6 * 512 * 2000 * 120k * 110M = 8.11e19

Geometric mean: 1.07e20",Human Reference Genome (GRCh38/hg38),"""We generated training data from human genome [...]"" [2.2.2 Pre-training]. ",3000000000.0,"The human genome is around 3 billion base pairs (https://useast.ensembl.org/Homo_sapiens/Info/Annotation).
The authors use both non-overlapping sampling and random sampling from a human genome, though the source is unspecified.",600.0,"""Since the pre-training of DNABERT model is resource-intensive (about 25â€‰days on 8 NVIDIA 2080Ti GPUs)""",NVIDIA GeForce RTX 2080 Ti 11GB,,Confident,"Motivation
Deciphering the language of non-coding DNA is one of the fundamental problems in genome research. Gene regulatory code is highly complex due to the existence of polysemy and distant semantic relationship, which previous informatics methods often fail to capture especially in data-scarce scenarios.

Results
To address this challenge, we developed a novel pre-trained bidirectional encoder representation, named DNABERT, to capture global and transferrable understanding of genomic DNA sequences based on up and downstream nucleotide contexts. We compared DNABERT to the most widely used programs for genome-wide regulatory elements prediction and demonstrate its ease of use, accuracy and efficiency. We show that the single pre-trained transformers model can simultaneously achieve state-of-the-art performance on prediction of promoters, splice sites and transcription factor binding sites, after easy fine-tuning using small task-specific labeled data. Further, DNABERT enables direct visualization of nucleotide-level importance and semantic relationship within input sequences for better interpretability and accurate identification of conserved sequence motifs and functional genetic variant candidates. Finally, we demonstrate that pre-trained DNABERT with human genome can even be readily applied to other organisms with exceptional performance. We anticipate that the pre-trained DNABERT model can be fined tuned to many other sequence analyses tasks.",4.04,,Open weights (unrestricted),United States of America,,,,,,2025-05-28 16:12,,,,,,Academia,,,,,Open source,"Apache 2.0, code and weights: https://github.com/jerryji1993/DNABERT",Academia,,,FP16,,"Hardware,Operation counting",,,,
"GPT-2 (1.5B, Curriculum Learning 45K)",Language,Language modeling/generation,Microsoft,"Conglong Li, Minjia Zhang, Yuxiong He",2021-08-13,Curriculum Learning: A Regularization Method for Efficient and Stable Billion-Scale GPT Model Pre-Training,https://arxiv.org/abs/2108.06084,29.0,,,1500000000.0,,2.3811e+21,"""Experiments replicating GPT-2 models (117M and 1.5B) show that <..> our method reduces the required number of training tokens and wall clock time by up to 2.2x and 3.7x, respectively.""

GPT-2 estimated training compute: 1.9200000000009998e+21 FLOP (Speculative confidence) -> 
1.9200000000009998e+21 / 3.7 = 5.1891892e+20 FLOP

"" All of the experiments are performed on 128 NVIDIA V100 GPUs (32GB memory). There are 16 nodes and 8 GPUs per node""
155 hours (Table 2) 

125000000000000 FLOP/sec * 128 GPUs * 155 hours * 3600 sec / hour * 0.3 = 2.6784e+21 FLOP

batch size 4000
sequence length 1000
58.8K steps

6 FLOP / parameter / token * 1.5 * 10^9 parameters * 4000 sequences per batch * 1000 token per sequence * 58800 steps = 2.1168e+21 FLOP

geometric mean of hardware and operation counting method (they are more accurate than comparison with GPT-2): 
sqrt(2.6784e+21*2.1168e+21) = 2.3811e+21 FLOP",WikiText-103,,157000000000.0,"Table 2
SLW 45K bsz4K-seqlen1K 58.8K 157B (1x) training time: 155Hr (2.2x) 
batch size 4000
sequence length 1000
58.8K steps
157B tokens

4*10^6*58800/157*10^9 = 1.6 epochs",155.0,,NVIDIA V100,,Likely,"Recent works have demonstrated great success in pre-training large-scale autoregressive language models on massive GPUs. To reduce the wall-clock training time, a common practice is to increase the batch size and learning rate. However, such practice is often brittle and leads to a so-called stability-efficiency dilemma: increasing the batch sizes and learning rates leads to better training efficiency but can also result in training instability, leading to poor generalization accuracy or failed runs. To better understand this phenomenon, we conduct an in-depth analysis on large-scale pre-training experiments replicating the GPT-2 model. We find that there is a strong correlation between training instability and extreme values of gradient variance, and that samples with long sequence lengths contribute to these extreme gradient variance values, especially at the beginning of the training, indicating that long sequence length can be a main source of training instability. Based on the analysis, we present a Sequence Length Warmup method that aims to solve the training stability-efficiency dilemma. Experiments replicating GPT-2 models show that our approach enables stable training with 8x larger batch size and 4x larger learning rate, whereas the baseline approach struggles with training instability. To achieve the same or better zero-shot evaluation results, our method reduces the required number of training tokens and wall clock time by up to 2.2x and 3.7x, respectively. Experiments replicating GPT-3 model (125M) show that our approach enables stable training with 8x larger batch size and 40x larger learning rate, and retains 99% of the zero-shot accuracy on 11 tasks using 10x less data and 17x less time compared to the original GPT-3 training recipe, while the baseline diverges under the same settings and only retain 95% of accuracy under lower learning rate.",1.6,"""GPT-2 (1.5B, Curriculum Learning 45K)""",Unreleased,"United States of America,Multinational,India,Belgium",,,,128.0,,2025-04-07 11:05,,,,4000000.0,,Industry,,,,,Unreleased,there's a repo for the technique but I don't see training code for this model: https://github.com/microsoft/DeepSpeed ,Industry,,,,77580.26955910088,"Comparison with other models,Hardware,Operation counting",,,,
Jurassic-1-Jumbo,Language,"Language modeling/generation,Chat",AI21 Labs,"Opher Lieber, Or Sharir, Barak Lenz, Yoav Shoham",2021-08-11,Jurassic-1: Technical Details and Evaluation,https://uploads-ssl.webflow.com/60fd4503684b466578c0d307/61138924626a6981ee09caf6_jurassic_tech_paper.pdf,55.0,Training cost,"""Training such a large model, on over 800 GPUs over many months""

Lower-bound cost estimate:
800 GPUs * $1/GPU-hour * 4 months = $2.3M
True cost was probably substantially higher. ""many months"" implies more than 4, and the GPUs were probably more expensive than $1/hour.",178000000000.0,"""Jurassic-1 models come in two sizes, where the Jumbo version, at 178B parameters, is the largest and most sophisticated language model ever released for general use by developers.""",3.7e+23,"see here https://docs.google.com/document/d/1B8x6XYcmB1u6Tmq3VcbAtj5bzhDaj2TcIPyK6Wpupx4/edit

6 * 178B * 300B = 3.204000e+23",,,225000000000.0,"""Our model was trained with the conventional self-supervised auto-regressive training objective on 300B tokens drawn from publicly available resources""

1 token ~ 0.75 words",,,NVIDIA A100,Self-supervised learning,Confident,"Jurassic-1 is a pair of auto-regressive language models recently released by AI21 Labs, consisting of J1-Jumbo, a 178B-parameter model, and J1-Large, a 7B-parameter model. We describe their architecture and training, and evaluate their performance relative to GPT-3. The evaluation is in terms of perplexity, as well as zero-shot and few-shot learning. To that end, we developed a zeroshot and few-shot test suite, which we made publicly available (https://github.com/ai21labs/ lm-evaluation) as a shared resource for the evaluation of mega language models.",,,API access,Israel,,,,,,2025-06-18 12:35,,,,3200000.0,"""Namely, we used a base learning rate of 1.2 Ã— 10âˆ’4 and 0.6 Ã— 10âˆ’4 , and a batch size of 2M and 3.2M tokens, for J1-Large and J1-Jumbo, respectively. We also used a linear warm-up over roughly the first 375 million tokens, and gradually increased the batch size from 32K tokens up to its target value for the first few billion tokens.""",Industry,,,,,Unreleased,,Industry,,,,,Third-party estimation,,,,1
Zidong Taichu,"Multimodal,Speech,Vision,Language","Language modeling/generation,Speech recognition,Image captioning","Chinese Academy of Sciences,Wuhan AI Computing Center",,2021-08-11,Zidong Ancestral multi-modal large model,https://gitee.com/zidongtaichu/multi-modal-models,,Historical significance,"The worldâ€™s first image, language, and audio trimodal pre-trained model.",3200000000.0,å…±32äº¿å‚æ•° translated as A total of 3.2 billion parameters ,8.015999999999999e+20,4.175e10 * 3.2e9 * 6 = 8.016e20 FLOP,,,41750000000.0,"ä¸»è¦é‡‡ç”¨CLUEä¸ŽWMTä¸­æ”¶é›†çš„ä¸­æ–‡æ•°æ®ï¼ŒåŒæ—¶æˆ‘ä»¬åŠ å…¥äº†é¢å¤–æ”¶é›†çš„å¯¹è¯æ•°æ®ä»¥åŠç¿»è¯‘å¹³è¡Œè¯­æ–™ä¸­çš„ä¸­æ–‡éƒ¨åˆ†ï¼Œæ€»å…±çº¦250Gçš„ä¸­æ–‡è¯­æ–™ï¼Œé¢†åŸŸè¦†ç›–å¹¿æ³›ã€‚

From context, seems to mean 250GB

250GB * 167M tokens/GB = 4.175e+10 tokens

https://gitee.com/zidongtaichu/multi-modal-models/tree/master/text#%E6%95%B0%E6%8D%AE%E9%9B%86",,,,,Confident,,,,Open weights (unrestricted),"China,China",,,,,,2025-05-30 10:53,,,,,,"Academia,Government",,,,,Unreleased,"Apache 2.0
https://gitee.com/zidongtaichu/multi-modal-models

I don't see training code there","Academia,Government",,,,,Operation counting,,,,
W2v-BERT,Speech,Speech recognition,"Google Brain,Massachusetts Institute of Technology (MIT)","Yu-An Chung, Yu Zhang, Wei Han, Chung-Cheng Chiu, James Qin, Ruoming Pang, Yonghui Wu",2021-08-07,W2v-BERT: Combining Contrastive Learning and Masked Language Modeling for Self-Supervised Speech Pre-Training,https://arxiv.org/abs/2108.06209v2,352.0,SOTA improvement,"""Our experiments show that w2v-BERT achieves competitive results
compared to current state-of-the-art pre-trained models on the LibriSpeech benchmarks when using the Libri-Light 60k corpus as the
unsupervised data. In particular, when compared to published models such as conformer-based wav2vec 2.0 and HuBERT, our model
shows 5% to 10% relative WER reduction on the test-clean and
test-other subsets""",1000000000.0,1B for XXL model,,,LibriLight,"""We use the Libri-Light unlab-60k subset [34], which contains
about 60,000 hours of unannotated speech audio, for pre-training
w2v-BERT models. For our main results, we use the LibriSpeech
960hr subset [35] as the supervised data, and use the 100hr subset
for ablation studies""",,,,,,,Confident,"Motivated by the success of masked language modeling~(MLM) in pre-training natural language processing models, we propose w2v-BERT that explores MLM for self-supervised speech representation learning. w2v-BERT is a framework that combines contrastive learning and MLM, where the former trains the model to discretize input continuous speech signals into a finite set of discriminative speech tokens, and the latter trains the model to learn contextualized speech representations via solving a masked prediction task consuming the discretized tokens. In contrast to existing MLM-based speech pre-training frameworks such as HuBERT, which relies on an iterative re-clustering and re-training process, or vq-wav2vec, which concatenates two separately trained modules, w2v-BERT can be optimized in an end-to-end fashion by solving the two self-supervised tasks~(the contrastive task and MLM) simultaneously. Our experiments show that w2v-BERT achieves competitive results compared to current state-of-the-art pre-trained models on the LibriSpeech benchmarks when using the Libri-Light~60k corpus as the unsupervised data. In particular, when compared to published models such as conformer-based wav2vec~2.0 and HuBERT, our model shows~5\% to~10\% relative WER reduction on the test-clean and test-other subsets. When applied to the Google's Voice Search traffic dataset, w2v-BERT outperforms our internal conformer-based wav2vec~2.0 by more than~30\% relatively.",,,Unreleased,"United States of America,United States of America",,,,,,2025-05-30 12:10,,,,,,"Industry,Academia",,,,,Unreleased,,"Industry,Academia",,,,,,,,,
YOLOX-X,Vision,Object detection,Megvii Inc,"Zheng Ge, Songtao Liu, Feng Wang, Zeming Li, Jian Sun",2021-08-06,YOLOX: Exceeding YOLO Series in 2021,https://arxiv.org/abs/2107.08430,3207.0,"Highly cited,SOTA improvement",Table 6,99100000.0,"99.1M, table 3",6.34275e+20,"""We train the models for a total of 300 epochs with 5 epochs warmup on COCO train2017 [17]. We use stochastic gradient descent (SGD) for training ... The batch size is 128 by default to typical 8-GPU devices ... input size is evenly drawn from 448 to 832 with 32 strides""

Training is done on 300 epochs of the 2.5 million image-label pairs in COCO train2017.

Table 3 indicates 281.9 GFLOP per forward pass on a 640x640 image. The mean image width/height is 640, though using this to estimate training FLOPs is probably a slight underestimate as FLOPs will scale roughly linearly in the number of pixels (which scale at width^2).

Ignoring this slight issue: 281.9e9 * 2.5M * 300 * 3 = 6.34e20",COCO 2017,"""We train the models for a total of 300 epochs with 5 epochs warmup on COCO train2017""",2500000.0,"2.5 million image-label pairs, per Coco paper https://arxiv.org/abs/1405.0312",,,NVIDIA V100,,Likely,"In this report, we present some experienced improvements to YOLO series, forming a new high-performance detector -- YOLOX. We switch the YOLO detector to an anchor-free manner and conduct other advanced detection techniques, i.e., a decoupled head and the leading label assignment strategy SimOTA to achieve state-of-the-art results across a large scale range of models: For YOLO-Nano with only 0.91M parameters and 1.08G FLOPs, we get 25.3% AP on COCO, surpassing NanoDet by 1.8% AP; for YOLOv3, one of the most widely used detectors in industry, we boost it to 47.3% AP on COCO, outperforming the current best practice by 3.0% AP; for YOLOX-L with roughly the same amount of parameters as YOLOv4-CSP, YOLOv5-L, we achieve 50.0% AP on COCO at a speed of 68.9 FPS on Tesla V100, exceeding YOLOv5-L by 1.8% AP. Further, we won the 1st Place on Streaming Perception Challenge (Workshop on Autonomous Driving at CVPR 2021) using a single YOLOX-L model. We hope this report can provide useful experience for developers and researchers in practical scenes, and we also provide deploy versions with ONNX, TensorRT, NCNN, and Openvino supported. Source code is at this https URL.",300.0,,Open weights (unrestricted),China,,,,8.0,,2025-05-28 16:12,,,,128.0,,Industry,,,,,Open source,"Apache 2.0 for code/weights
https://github.com/Megvii-BaseDetection/YOLOX/blob/main/LICENSE",Industry,,,FP16,4849.522759284234,Operation counting,,,,
FMMformer (2-kernel fast weight + Band20),Language,"Language modeling,Text classification","University of California Los Angeles (UCLA),University of Utah","Tan M. Nguyen, Vai Suliafu, Stanley J. Osher, Long Chen, Bao Wang",2021-08-05,FMMformer: Efficient and Flexible Transformer via Decomposed Near-field and Far-field Attention,https://proceedings.neurips.cc/paper/2021/file/f621585df244e9596dc70a39b579efb1-Paper.pdf,31.0,,,40000000.0,40M (caption to Table 2),2.472e+16,"6 FLOP / token / parameter * 40 * 10^6 parameters * 103*10^6 tokens * 1 epoch [assumed, not given] = 2.472e+16 FLOP

________
estimation from the Algorithmic progress paper: 4.3*10^17 FLOP",WikiText-103,,,,,,NVIDIA GeForce RTX 3090 Ti,,Speculative,"We propose FMMformers, a class of efficient and flexible transformers inspired by the celebrated fast multipole method (FMM) for accelerating interacting particle simulation. FMM decomposes particle-particle interaction into near-field and far-field components and then performs direct and coarse-grained computation, respectively. Similarly, FMMformers decompose the attention into near-field and far-field attention, modeling the near-field attention by a banded matrix and the far-field attention by a low-rank matrix. Computing the attention matrix for FMMformers requires linear complexity in computational time and memory footprint with respect to the sequence length. In contrast, standard transformers suffer from quadratic complexity. We analyze and validate the advantage of FMMformers over the standard transformer on the Long Range Arena and language modeling benchmarks. FMMformers can even outperform the standard transformer in terms
of accuracy by a significant margin. For instance, FMMformers achieve an average classification accuracy of 60.74% over the five Long Range Arena tasks, which is significantly better than the standard transformerâ€™s average accuracy of 58.70%.",,FMMformer (2-kernel fast weight + Band20),Unreleased,"United States of America,United States of America",,,,4.0,,2025-04-07 12:33,,,FMMformer (2-kernel fast weight + Band20),,,"Academia,Academia",,,,,Unreleased,,"Academia,Academia",,,,3637.223067232183,Operation counting,,,,
6-Act Tether,Robotics,Object detection,"Facebook AI Research,Georgia Institute of Technology","Joel Ye, Dhruv Batra, Abhishek Das, Erik Wijmans",2021-08-03,Auxiliary Tasks and Exploration Enable ObjectGoal Navigation,https://openaccess.thecvf.com/content/ICCV2021/html/Ye_Auxiliary_Tasks_and_Exploration_Enable_ObjectGoal_Navigation_ICCV_2021_paper.html,81.0,SOTA improvement,"""Our agents achieve 24.5% success and 8.1% SPL, a 37% and 8% relative improvement over prior state-of-the-art, respectively, on the Habitat ObjectNav Challenge""",5000000.0,"""Agent parameter counts were all 5 âˆ’ 6 million parameters, excluding parameters in auxiliary modules""",,"""In our experiments, we train each of our agents for 8 GPU-weeks (192 GPU-hours)"". No GPU specified.",Matterport,"""We experiment on the Matterport dataset (MP3D [4]), which has 90 scenes and 40 labeled semantic object categories.""",,,,,,Reinforcement learning,Confident,"ObjectGoal Navigation (ObjectNav) is an embodied task wherein agents are to navigate to an object instance in an unseen  environment. Prior works have shown that end-to-end ObjectNav agents that use vanilla visual and recurrent modules, e.g. a CNN+RNN, perform poorly due to overfitting and sample inefficiency. This has motivated current state-of-the-art methods to mix analytic and learned components and operate on explicit spatial maps of the environment. We instead re-enable a generic learned agent by adding auxiliary learning tasks and an exploration reward. Our agents achieve 24.5% success and 8.1% SPL, a 37% and 8% relative improvement over prior state-of-the-art, respectively, on the Habitat ObjectNav Challenge. From our analysis, we propose that agents will act to simplify their visual inputs so as to smooth their RNN dynamics, and that auxiliary tasks reduce overfitting by minimizing effective RNN dimensionality; i.e. a performant ObjectNav agent that must maintain coherent plans over long horizons does so by learning smooth, low-dimensional recurrent dynamics.",,,Open weights (unrestricted),"United States of America,United States of America",,,,,,2025-05-30 11:12,,,,,,"Industry,Academia",,,,,Open source,"MIT license

https://github.com/joel99/objectnav","Industry,Academia",,,,,,,,,
SEER,Vision,"Image embedding,Image classification","Facebook AI Research,INRIA","Priya Goyal, Mathilde Caron, Benjamin Lefaudeux, Min Xu, Pengchao Wang, Vivek Pai, Mannat Singh, Vitaliy Liptchinsky, Ishan Misra, Armand Joulin, Piotr Bojanowski",2021-07-29,Self-supervised Pretraining of Visual Features in the Wild,https://arxiv.org/abs/2103.01988,244.0,SOTA improvement,"SOTA for self-supervised models on ImageNet, which seems fair to consider a different benchmark than ImageNet for supervised models.

""Our final SElf-supERvised (SEER) model,
a RegNetY with 1.3B parameters trained on 1B random
images with 512 GPUs achieves 84.2% top-1 accuracy,
surpassing the best self-supervised pretrained model by 1%""",1300000000.0,"From abstract:
"" Our final SElf-supERvised (SEER) model, a RegNetY with 1.3B parameters...""",1.8e+22,"Numbers from section 3.2, they specifically mention using mixed precision training.
6125 ms / batch * 114890 batches = 8.14 days (they round to 8 in the text)

512 GPUs * 8.14 days * 24h/day * 3600s/h * 125 TFLOP/s * 0.4 (assumed utilization) = 1.800e22

""on 512 V100 32GB NVIDIA GPUs. Training this model on 1 billion images requires 114, 890 training iterations for a batch size of 8, 704 images, summing to 8 days of training over 512 GPUs.""",Instagram,"Section 3.3:
""For our billion scale pretraining, we consider a dataloader that directly samples random, public, and non-EU images from Instagram""

Note the dataset is not static - it is refreshed every 90 days",1000000000.0,"""Overall, we train
on 1B images for a total of 122K iterations.""",195.5,6125 ms / batch * 114890 batches = 195.5 hours,NVIDIA V100,Self-supervised learning,Confident,"Recently, self-supervised learning methods like MoCo, SimCLR, BYOL and SwAV have reduced the gap with supervised methods. These results have been achieved in a control environment, that is the highly curated ImageNet dataset. However, the premise of self-supervised learning is that it can learn from any random image and from any unbounded dataset. In this work, we explore if self-supervision lives to its expectation by training large models on random, uncurated images with no supervision. Our final SElf-supERvised (SEER) model, a RegNetY with 1.3B parameters trained on 1B random images with 512 GPUs achieves 84.2% top-1 accuracy, surpassing the best self-supervised pretrained model by 1% and confirming that self-supervised learning works in a real world setting. Interestingly, we also observe that self-supervised models are good few-shot learners achieving 77.9% top-1 with access to only 10% of ImageNet. Code: this https URL",,,Open weights (non-commercial),"United States of America,France",,,,512.0,,2025-06-18 12:33,,,,,,"Industry,Academia",,,,98304.0,Open (non-commercial),"https://github.com/facebookresearch/vissl/tree/main/projects/SEER

We share instructions on how to train SEER model on GPUs using PyTorch. First, Install VISSL and follow the data setup instructions to easily setup your data input with VISSL.

https://github.com/facebookresearch/vissl/blob/main/projects/SEER/MODEL_LICENSE.md","Industry,Academia",$34114.25,,FP16,310424.75538121304,Hardware,,,,
GOAT,Games,Open ended play,DeepMind,"Open-Ended Learning Team*, Adam Stooke, Anuj Mahajan, Catarina Barros, Charlie Deck, Jakob Bauer, Jakub Sygnowski, Maja Trebacz, Max Jaderberg, Michael Mathieu, Nat McAleese, Nathalie Bradley-Schmieg, Nathaniel Wong, Nicolas Porcel, Roberta Raileanu, Steph Hughes-Fitt, Valentin Dalibard and Wojciech Marian Czarnecki",2021-07-27,Open-Ended Learning Leads to Generally Capable Agents,"https://deepmind.com/blog/article/generally-capable-agents-emerge-from-open-ended-play

https://arxiv.org/abs/2107.12808",163.0,SOTA improvement,likely qualitatively SOTA,3472816.0,estimate described here: https://docs.google.com/document/d/1S9xZyCeITDOs-P1W_-liNW0WgVN-OLsSudVrPXMaLqw/edit?usp=sharing,2.412e+22,"[Final calculation]
(8 TPUs) * (1.23e14 FLOP/TPU-s) * (0.1 utilization) / (50k steps/s) = 1.968e9 FLOP/step

(32 agents) * (383B steps/agent) * (1.968e9 FLOP/step) = 2.412e22 FLOPs

==========================
NOTES BELOW

6.1: Each agent is trained using 8 TPUv3s and consumes approximately 50,000 agent steps (observations) per second.
Multiple agents interacting probably mean a fairly low utilization rate, so letâ€™s assume 0.10
8 * 1.23e14 * 0.1 / 50k = 1.968e9 FLOPs per step

The paper doesnâ€™t say exactly how many agents they train in each population. The original PBT paper uses 32 agents for one task (in general it uses between 10 and 80), so as a guesstimate letâ€™s go with that.

Figure 16: They train over 5 generations. Summing the number of steps, it looks like there were roughly 383B steps
32 * 383B * 1.968e9 = 2.412e22

Final estimate:
2.412e22

I do a confidence interval analysis here and find a 90% CI of 6.9e21 to 1.3e23, so we can call this estimate ""likely"" (within 1 OOM): https://colab.research.google.com/drive/1wGSTQxBExY6Fa0-d7msVumf5-KnsWLe6?usp=sharing",XLand,,390000000000.0,Figure 16 shows steps per generation and agent. In total there are 1.5e10 + 4.0e10 + 2.5e10 + 1.1e11 + 2e11 = 3.9e11 steps per agent.,,"see other notes
",Google TPU v3,Self-supervised learning,Speculative,"In this work we create agents that can perform well beyond a single, individual task, that exhibit much wider generalisation of behaviour to a massive, rich space of challenges. We define a universe of tasks within an environment domain and demonstrate the ability to train agents that are generally capable across this vast space and beyond. The environment is natively multi-agent, spanning the continuum of competitive, cooperative, and independent games, which are situated within procedurally generated physical 3D worlds. The resulting space is exceptionally diverse in terms of the challenges posed to agents, and as such, even measuring the learning progress of an agent is an open research problem. We propose an iterative notion of improvement between successive generations of agents, rather than seeking to maximise a singular objective, allowing us to quantify progress despite tasks being incomparable in terms of achievable rewards. We show that through constructing an open-ended learning process, which dynamically changes the training task distributions and training objectives such that the agent never stops learning, we achieve consistent learning of new behaviours. The resulting agent is able to score reward in every one of our humanly solvable evaluation levels, with behaviour generalising to many held-out points in the universe of tasks. Examples of this zero-shot generalisation include good performance on Hide and Seek, Capture the Flag, and Tag. Through analysis and hand-authored probe tasks we characterise the behaviour of our agent, and find interesting emergent heuristic behaviours such as trial-and-error experimentation, simple tool use, option switching, and cooperation. Finally, we demonstrate that the general capabilities of this agent could unlock larger scale transfer of behaviour through cheap finetuning.",,,Unreleased,United Kingdom of Great Britain and Northern Ireland,,,,,,2025-05-09 11:32,,,,64.0,,Industry,checked,,,,Unreleased,,Industry,$84799.79,,,,Hardware,,,,
HuBERT,Speech,Speech recognition,Facebook AI Research,"Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed",2021-07-27,HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units,https://arxiv.org/abs/2106.07447,2235.0,"Highly cited,SOTA improvement","Abstract: 
"" the
HuBERT model either matches or improves upon the state-ofthe-art wav2vec 2.0 performance on the Librispeech (960h) and
Libri-light (60,000h) benchmarks with 10min, 1h, 10h, 100h, and
960h fine-tuning subsets.""",1000000000.0,"From abstract:
""Using a 1B parameter model, HuBERT shows up to 19% and 13% relative WER reduction on the more challenging dev-other and test-other evaluation subsets""",5.54e+21,"GPU NOT SPECIFIED - for the sake of argument I assume something on the order of 1 TFLOP/s

Numbers from Section IV part C
0.1 * (960h * 32GPUs + 60000h * 256 GPUs) * 3600s/h * 1 TFLOP/s/GPU","LibriSpeech,LibriLight",,820800000.0,"""When the HuBERT model is pre-trained on either the standard Librispeech 960h [24] or the Libri-Light 60k hours [25], it either matches or improves upon the state-of-theart wav2vec 2.0 [6] performance on all fine-tuning subsets of 10mins, 1h, 10h, 100h, and 960h.""

1h ~ 13,680 words
13,680 * 60,000 = 820800000",,,,Self-supervised learning,Speculative,"Self-supervised approaches for speech representation learning are challenged by three unique problems: (1) there are multiple sound units in each input utterance, (2) there is no lexicon of input sound units during the pre-training phase, and (3) sound units have variable lengths with no explicit segmentation. To deal with these three problems, we propose the Hidden-Unit BERT (HuBERT) approach for self-supervised speech representation learning, which utilizes an offline clustering step to provide aligned target labels for a BERT-like prediction loss. A key ingredient of our approach is applying the prediction loss over the masked regions only, which forces the model to learn a combined acoustic and language model over the continuous inputs. HuBERT relies primarily on the consistency of the unsupervised clustering step rather than the intrinsic quality of the assigned cluster labels. Starting with a simple k-means teacher of 100 clusters, and using two iterations of clustering, the HuBERT model either matches or improves upon the state-of-the-art wav2vec 2.0 performance on the Librispeech (960h) and Libri-light (60,000h) benchmarks with 10min, 1h, 10h, 100h, and 960h fine-tuning subsets. Using a 1B parameter model, HuBERT shows up to 19% and 13% relative WER reduction on the more challenging dev-other and test-other evaluation subsets.",,,Open weights (unrestricted),United States of America,,,,,,2025-05-28 16:12,,,,,,Industry,,,,,Open source,"https://github.com/facebookresearch/fairseq/tree/main/examples/hubert

fairseq(-py) is MIT-licensed. The license applies to the pre-trained models as well.
includes training code",Industry,,,FP16,,Hardware,,,,
Codex,Language,Code autocompletion,OpenAI,"Mark Chen , Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger,  Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji,  Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, Wojciech Zaremba ",2021-07-07,Evaluating Large Language Models Trained on Code,"https://openai.com/blog/openai-codex/
https://arxiv.org/abs/2107.03374",3818.0,"Significant use,Highly cited",,12000000000.0,"""With just a single sample, a 12B parameter Codex solves 28.8% of these problems, and a 300M parameter Codex solves 13.2% of these problems""",7.344e+22,"""The original training of GPT-3-12B consumed hundreds of petaflop/sdays of compute, while fine-tuning it to create Codex-12B
consumed a similar amount of compute.""
1 PFLOP/s-day = 8.64e19 FLOPs.
""Hundreds"" is likely between 200 and 900, geometric mean = 425.
2 * 425 * 8.64e19 = 7.344e22
",,"Page 4 of https://arxiv.org/pdf/2107.03374 says â€œOur training dataset was collected in May 2020 from 54 million public software repositories hosted on GitHub.â€ Based on this description, I assume a conservative knowledge cutoff date would be May 1, 2020.",31800000000.0,"""Our training dataset was collected in May 2020 from 54 million public software repositories hosted on GitHub, containing 179 GB of unique Python files under 1 MB. We filtered out files which were likely auto-generated, had average line
length greater than 100, had maximum line length greater
than 1000, or contained a small percentage of alphanumeric
characters. After filtering, our final dataset totaled 159 GB.""

1 GB ~ 200M words",,,,Self-supervised learning,Likely,"We introduce Codex, a GPT language model fine-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8% of the problems, while GPT-3 solves 0% and GPT-J solves 11.4%. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve 70.2% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics.",,,API access,United States of America,GPT-3 13B,,,,,2025-05-09 11:32,,,,,,Industry,checked,,,,Unreleased,"Codex was available via the OpenAI API from announcement: https://openai.com/index/openai-codex/

It is still available via the Research Access Program. https://x.com/OfficialLoganK/status/1638336152800206858",Industry,,,,,,,,,
ERNIE 3.0,Language,"Language modeling,Language modeling/generation",Baidu,"Y Sun, S Wang, S Feng, S Ding, C Pang",2021-07-05,ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation,http://research.baidu.com/Blog/index-view?id=160,347.0,SOTA improvement,"""ERNIE 3.0 achieved new state-of-the-art results across 54 Chinese NLP tasks""",10000000000.0,"""We trained the model with 10 billion parameters on a 4TB corpus consisting of plain texts and a large-scale knowledge graph.""",2.25e+22,"Section 3.3.3: 
""""The model is trained for
a total of 375 billion tokens""

Total compute approximated as 6*N*D",,,668000000000.0,"""To ensure the success of the pre-training of ERNIE 3.0, we construct a large-scale, wide-variety and high-quality Chinese text corpora amounting to 4TB storage size in 11 different categories.""

1 GB ~ 167M chinese words",,,NVIDIA V100,Self-supervised learning,Confident,,,,Open weights (unrestricted),China,,,,384.0,,2025-06-18 12:30,,,,,,Industry,,,,,Open source,"apache 2.0 
train code: https://github.com/PaddlePaddle/PaddleNLP/tree/develop/legacy/model_zoo/ernie-3.0#%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83 ",Industry,$39104.27,,FP16,232943.03314881725,Operation counting,,,,
GemNet-T (OC20),Materials science,"Molecular simulation,Molecular property prediction,Molecular representation learning,Atomistic simulations",Technical University of Munich,"Johannes Gasteiger, Florian Becker, Stephan GÃ¼nnemann",2021-07-01,GemNet: Universal Directional Graph Neural Networks for Molecules,https://arxiv.org/abs/2106.08903,,,,1900000.0,1.9M parameters,7.34832e+17,11340000000000 FLOP / GPU / sec [fp32 assumed] * 60 hours * 3600 sec / hour * 1 GPU * 0.3 [assumed utilization] = 7.34832e+17 FLOP,Open Catalyst 2020 (OC20),,,,60.0,,NVIDIA GeForce GTX 1080 Ti,,Likely,"Effectively predicting molecular interactions has the potential to accelerate molecular dynamics by multiple orders of magnitude and thus revolutionize chemical simulations. Graph neural networks (GNNs) have recently shown great successes for this task, overtaking classical methods based on fixed molecular kernels. However, they still appear very limited from a theoretical perspective, since regular GNNs cannot distinguish certain types of graphs. In this work we close this gap between theory and practice. We show that GNNs with directed edge embeddings and two-hop message passing are indeed universal approximators for predictions that are invariant to translation, and equivariant to permutation and rotation. We then leverage these insights and multiple structural improvements to propose the geometric message passing neural network (GemNet). We demonstrate the benefits of the proposed changes in multiple ablation studies. GemNet outperforms previous models on the COLL, MD17, and OC20 datasets by 34%, 41%, and 20%, respectively, and performs especially well on the most challenging molecules. Our implementation is available online.",,,Open weights (restricted use),Germany,,,,1.0,,2025-05-12 11:34,,,,,,Academia,,,,,Open (restricted use),"Hippocratic license
""Licensor may in its discretion and without obligation <...> to cease use of the Software""

https://github.com/TUM-DAML/gemnet_pytorch",Academia,,,,277.7822898215568,Hardware,,,,
DEQ-Transformer (Post-LN) + Jacobian Regularisation,Language,Language modeling,"Carnegie Mellon University (CMU),Intel Labs","Shaojie Bai, Vladlen Koltun, J. Zico Kolter",2021-06-28,Stabilizing Equilibrium Models by Jacobian Regularization,https://arxiv.org/abs/2106.14342,51.0,,,98000000.0,98M (Table 1),2.9e+19,"26900000000000 FLOP / sec [assumed precision: FP16] *4 GPUs * 250 hours * 3600 sec / hour * 0.3 [assumed utilization] = 2.9052e+19 FLOP

6 FLOP / parameter / token * 98 * 10^6 parameters * 103*10^6 tokens * 23 epochs = 1.392972e+18 FLOP

________________
NB reporting hardware estimation as it was used in the Algorithmic progress paper",WikiText-103,"""All Transformer models are trained for 250K steps""",,,250.0,Figure 1c,NVIDIA GeForce RTX 2080 Ti 11GB,,Likely,"Deep equilibrium networks (DEQs) are a new class of models that eschews traditional depth in favor of finding the fixed point of a single nonlinear layer. These models have been shown to achieve performance competitive with the state-of-the-art deep networks while using significantly less memory. Yet they are also slower, brittle to architectural choices, and introduce potential instability to the model. In this paper, we propose a regularization scheme for DEQ models that explicitly regularizes the Jacobian of the fixed-point update equations to stabilize the learning of equilibrium models. We show that this regularization adds only minimal computational cost, significantly stabilizes the fixed-point convergence in both forward and backward passes, and scales well to high-dimensional, realistic domains (e.g., WikiText-103 language modeling and ImageNet classification). Using this method, we demonstrate, for the first time, an implicit-depth model that runs with approximately the same speed and level of performance as popular conventional deep networks such as ResNet-101, while still maintaining the constant memory footprint and architectural simplicity of DEQs. Code is available at this https URL .",23.0,DEQ-Transformer (Post-LN) + Jacobian Regularisation,Open weights (unrestricted),"United States of America,Multinational,United States of America",,,,4.0,,2025-04-07 13:06,,,,,,"Academia,Industry",,,,,Open source,"code and pretrained models, MIT: https://github.com/locuslab/deq/tree/master/DEQ-Sequence
train script: https://github.com/locuslab/deq/blob/master/DEQ-Sequence/train_transformer.py ","Academia,Industry",,,,2022.3901774609187,"Operation counting,Hardware",,,,
Adaptive Input Transformer + RD,Language,Language modeling,"Microsoft Research Asia,Soochow University","Xiaobo Liang, Lijun Wu, Juntao Li, Yue Wang, Qi Meng, Tao Qin, Wei Chen, Min Zhang, Tie-Yan Liu",2021-06-28,R-Drop: Regularized Dropout for Neural Networks,https://arxiv.org/abs/2106.14448,379.0,SOTA improvement,"""In particular, it yields substantial
improvements when applied to fine-tune large-scale pre-trained models, e.g., ViT, RoBERTa-large, and BART, and achieves state-of-the-art (SOTA) performances with the vanilla Transformer model """,247000000.0,,8.58e+19,"""We train Transformer model for 50k steps and Adaptive Input Transformer for 286k steps""

assuming 1 step took 1 second:

125000000000000 FLOP / sec [assumed precision: bf16] * 286000 seconds * 8 GPUs * 0.3 [assumed utilization] = 8.58e+19 FLOP",,,,,79.4,""" The training is on 8 Tesla V100 GPU cards""
286k steps ~ [assumption] 286000 seconds = 79.4 hours",NVIDIA V100,,Likely,"Dropout is a powerful and widely used technique to regularize the training of deep neural networks. In this paper, we introduce a simple regularization strategy upon dropout in model training, namely R-Drop, which forces the output distributions of different sub models generated by dropout to be consistent with each other. Specifically, for each training sample, R-Drop minimizes the bidirectional KL-divergence between the output distributions of two sub models sampled by dropout. Theoretical analysis reveals that R-Drop reduces the freedom of the model parameters and complements dropout. Experiments on 5 widely used deep learning tasks (18 datasets in total), including neural machine translation, abstractive summarization, language understanding, language modeling, and image classification, show that R-Drop is universally effective. In particular, it yields substantial improvements when applied to fine-tune large-scale pre-trained models, e.g., ViT, RoBERTa-large, and BART, and achieves state-of-the-art (SOTA) performances with the vanilla Transformer model on WMT14 Englishâ†’German translation (30.91 BLEU) and WMT14 Englishâ†’French translation (43.95 BLEU), even surpassing models trained with extra large-scale data and expert-designed advanced variants of Transformer models. Our code is available at GitHub{\url{this https URL}}.",,Adaptive Input Transformer + RD,Unreleased,"China,Taiwan",,,,8.0,,2025-05-28 16:12,,,,,,"Industry,Academia",,,,,Open source,"train and inference code for translation models, MIT license. no weights

https://github.com/dropreg/R-Drop/blob/main/fairseq_src/examples/translation_rdrop/README.md ","Industry,Academia",,,FP16,4853.736425906205,Hardware,,,,
CPM-2,Language,Language generation,"Tsinghua University,Beijing Academy of Artificial Intelligence / BAAI","Zhengyan Zhang, Yuxian Gu, Xu Han, Shengqi Chen, Chaojun Xiao, Zhenbo Sun, Yuan Yao, Fanchao Qi, Jian Guan, Pei Ke, Yanzheng Cai, Guoyang Zeng, Zhixing Tan, Zhiyuan Liu, Minlie Huang, Wentao Han, Yang Liu, Xiaoyan Zhu, Maosong Sun",2021-06-24,CPM-2: Large-scale Cost-effective Pre-trained Language Models,https://arxiv.org/abs/2106.10715,81.0,,,11000000000.0,"""we pretrain two models: an encoder-decoder bilingual model with 11 billion parameters (CPM2) and its corresponding MoE version with
198 billion parameters. """,3.600000000001e+22,"epoch count not stated
C = 6 FLOP * 11B * 464B = ",WuDao Corpora,,464000000000.0,"""We pre-train our model on WuDaoCorpus (Yuan et al., 2021), which contains 2.3TB cleaned Chinese data as well as 300GB cleaned English data.""

2300GB * 167M tokens/GB + 300GB * 267M tokens/GB = 464 billion tokens

https://docs.google.com/document/d/1G3vvQkn4x_W71MKg0GmHVtzfd9m0y3_Ofcoew0v902Q/edit#heading=h.ieihc08p8dn0",,,,,Likely,"In recent years, the size of pre-trained language models (PLMs) has grown by leaps and bounds. However, efficiency issues of these large-scale PLMs limit their utilization in real-world scenarios. We present a suite of cost-effective techniques for the use of PLMs to deal with the efficiency issues of pre-training, fine-tuning, and inference. (1) We introduce knowledge inheritance to accelerate the pre-training process by exploiting existing PLMs instead of training models from scratch. (2) We explore the best practice of prompt tuning with large-scale PLMs. Compared with conventional fine-tuning, prompt tuning significantly reduces the number of task-specific parameters. (3) We implement a new inference toolkit, namely InfMoE, for using large-scale PLMs with limited computational resources. Based on our cost-effective pipeline, we pre-train two models: an encoder-decoder bilingual model with 11 billion parameters (CPM-2) and its corresponding MoE version with 198 billion parameters. In our experiments, we compare CPM-2 with mT5 on downstream tasks. Experimental results show that CPM-2 has excellent general language intelligence. Moreover, we validate the efficiency of InfMoE when conducting inference of large-scale models having tens of billions of parameters on a single GPU. All source code and model parameters are available at this https URL.",,,,"China,China",,,,,,2025-02-17 11:46,,,,,,"Academia,Academia",,,,,,,"Academia,Academia",,,,,Operation counting,,,,
EfficientNetV2-XL,Vision,"Image classification,Neural Architecture Search - NAS","Google,Google Brain","Mingxing Tan, Quoc V. Le",2021-06-23,EfficientNetV2: Smaller Models and Faster Training,https://arxiv.org/abs/2104.00298,2048.0,"Highly cited,SOTA improvement","""EfficientNetV2 achieves 87.3% top-1 accuracy on ImageNet ILSVRC2012, outperforming the recent ViT by 2.0% accuracy while 
 training 5x-11x faster using the same computing resources.""",208000000.0,"208M for XL version (Table 7, page 7)",9.56e+19,"Table 7, page 7: 45 hours on 32 TPUv3 cores.

""Each v3 TPU chip contains two TensorCores.""
TPU performance per chip = 123e12 FLOP/s
32 cores = 16 chips

123e12 FLOP/s per chip * (32 cores / 2 cores per chip) * 45 hours * 3600 seconds/hour * 0.30 utilization = 9.56e19 FLOP

https://www.wolframalpha.com/input?i=123+terahertz+*+16+*+45+hours+*+0.3","ImageNet21k,ILSVRC 2012 subset of ImageNet","""Setup: ImageNet21k (Russakovsky et al., 2015) contains
about 13M training images with 21,841 classes. The original
ImageNet21k doesnâ€™t have train/eval split, so we reserve randomly picked 100,000 images as validation set and use the
remaining as training set. We largely reuse the same training
settings as ImageNet ILSVRC2012 with a few changes: (1)
we change the training epochs to 60 or 30 to reduce training
time, and use cosine learning rate decay that can adapt to
different steps without extra tuning; (2) since each image
has multiple labels, we normalize the labels to have sum
of 1 before computing softmax loss. After pretrained on
ImageNet21k, each model is finetuned on ILSVRC2012 for
15 epochs using cosine learning rate decay.""

Based on Table 7, where XL is trained on 5x as many TPU-hours as S, despite having 10x as many parameters, XL was probably trained on 30 epochs and S on 60 epochs.

",14180000.0,"""ImageNet21k (Russakovsky et al., 2015) contains about 13M training images with 21,841 classes. The original ImageNet21k doesnâ€™t have train/eval split, so we reserve randomly picked 100,000 images as validation set and use the remaining as training set...
After pretrained on ImageNet21k, each model is finetuned on ILSVRC2012 for 15 epochs using cosine learning rate decay.""

12.9M + 1.28M ~= 14,180,000",45.0,Table 7,Google TPU v3,Supervised,Confident,"This paper introduces EfficientNetV2, a new family of convolutional networks that have faster training speed and better parameter efficiency than previous models. To develop this family of models, we use a combination of training-aware neural architecture search and scaling, to jointly optimize training speed and parameter efficiency. The models were searched from the search space enriched with new ops such as Fused-MBConv. Our experiments show that EfficientNetV2 models train much faster than state-of-the-art models while being up to 6.8x smaller.
Our training can be further sped up by progressively increasing the image size during training, but it often causes a drop in accuracy. To compensate for this accuracy drop, we propose to adaptively adjust regularization (e.g., dropout and data augmentation) as well, such that we can achieve both fast training and good accuracy.
With progressive learning, our EfficientNetV2 significantly outperforms previous models on ImageNet and CIFAR/Cars/Flowers datasets. By pretraining on the same ImageNet21k, our EfficientNetV2 achieves 87.3% top-1 accuracy on ImageNet ILSVRC2012, outperforming the recent ViT by 2.0% accuracy while training 5x-11x faster using the same computing resources. Code will be available at this https URL.",30.0,,Open weights (unrestricted),"United States of America,United States of America",,,,16.0,,2025-05-28 16:12,,,,4096.0,"""Each model is trained for 350 epochs with total batch size 4096""","Industry,Industry",,,,,Open source,"code and weights: https://github.com/google/automl/tree/master/efficientnetv2

Apache-2.0 license","Industry,Industry",$104.34,,FP16,7119.606126290003,Hardware,,,,
ALIGN,"Multimodal,Vision,Language","Representation learning,Image classification",Google Research,"Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yunhsuan Sung, Zhen Li, Tom Duerig",2021-06-11,Scaling up visual and vision-language representation learning with noisy text supervision,https://arxiv.org/abs/2102.05918,3019.0,"Highly cited,SOTA improvement","""The aligned visual and language representations... set new state-of-the-art results on Flickr30K and MSCOCO image-text retrieval benchmarks""",820000000.0,"From author communication

480M (image tower) + 340 M (text tower)",2.5986700000009994e+22,"From author communication
14.82K TPUv3 core-days
Precision: bfloat16

Estimation
TPUv3 at float16: 123 TFLOPS/chip

123*10^12 TFLOPS/chip * (1 chip / 2 cores) * 14820 TPU core-days * 86400 s/day * 33% utilization = 2.599*10^22 FLOP
https://www.wolframalpha.com/input?i=14820+days+*+123+teraFLOPS+%2F+2+*+0.33","Conceptual Captions (CC3M),FIT400M",,1600000000.0,"Dataset contains 1.8B image-text pairs, then some duplicates are removed.",347.3,14820 TPU core-hours / 1024 TPU cores = 347.3 hours,Google TPU v3,Self-supervised learning,Confident,"Pre-trained representations are becoming crucial for many NLP and perception tasks. While representation learning in NLP has transitioned to training on raw text without human annotations, visual and vision-language representations still rely heavily on curated training datasets that are expensive or require expert knowledge. For vision applications, representations are mostly learned using datasets with explicit class labels such as ImageNet or OpenImages. For vision-language, popular datasets like Conceptual Captions, MSCOCO, or CLIP all involve a non-trivial data collection (and cleaning) process. This costly curation process limits the size of datasets and hence hinders the scaling of trained models. In this paper, we leverage a noisy dataset of over one billion image alt-text pairs, obtained without expensive filtering or post-processing steps in the Conceptual Captions dataset. A simple dual-encoder architecture learns to align visual and language representations of the image and text pairs using a contrastive loss. We show that the scale of our corpus can make up for its noise and leads to state-of-the-art representations even with such a simple learning scheme. Our visual representation achieves strong performance when transferred to classification tasks such as ImageNet and VTAB. The aligned visual and language representations enables zero-shot image classification and also set new state-of-the-art results on Flickr30K and MSCOCO image-text retrieval benchmarks, even when compared with more sophisticated cross-attention models. The representations also enable cross-modality search with complex text and text + image queries.",,,Unreleased,"Multinational,United States of America,Canada,Switzerland",,,,512.0,,2025-05-09 11:32,,,,16384.0,,Industry,,,,177818.0,Unreleased,,Industry,$32852.92,,,227888.2870513745,Hardware,,,,
Denoising Diffusion Probabilistic Models (LSUN Bedroom),Vision,Image generation,University of California (UC) Berkeley,"Jonathan Ho, Ajay Jain, Pieter Abbeel",2021-06-11,Denoising Diffusion Probabilistic Models,https://arxiv.org/abs/2006.11239,11839.0,"Highly cited,SOTA improvement","Novel approach to image synthesis that yields SOTA results on datasets like CIFAR-10

Abstract: 
""On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. """,256000000.0,"Appendix B: 
"" Our CIFAR10 model has 35.7 million parameters, and our LSUN and
CelebA-HQ models have 114 million parameters. We also trained a larger variant of the LSUN Bedroom model with approximately 256 million parameters by increasing filter count.""",7.840125000000001e+19,"Numbers in Appendix B
""Our CelebA-HQ/LSUN (2562) models train at 2.2 steps per second at batch size 64, [...] The larger LSUN Bedroom model was trained for 1.15M steps.""
10.6h for the CIFAR model (batch size 128, 21 step/s)
2.2 step/s for the LSUN model, 1.15M steps so 702.8 hours

1 step takes 1/2.2 =0.4545 seconds
1.15M steps * 0.4545 seconds = 522675 seconds = 145 hours

This is for TPUv3-8's, which seems to mean 8 cores (standard chip is 125 teraflop/s for 2 cores) -> 4 chips
https://cloud.google.com/tpu/docs/regions-zones

1.25E14 FLOP/s * (4 chips) * 522675 seconds * 0.3 = 78401250000000010000",LSUN Bedroom,,3033042.0,"""We trained on CelebA-HQ for 0.5M steps, LSUN Bedroom for 2.4M steps, LSUN Cat for 1.8M steps, and LSUN Church for 1.2M steps.""

""The CelebA-HQ dataset is a high-quality version of CelebA that consists of 30,000 images at 1024Ã—1024 resolution.""
https://paperswithcode.com/dataset/celeba-hq

LSUN bedroom has 3,033,042 examples. LSUN cat has 1,657,266 examples. LSUN church has 126,227 examples.
https://www.tensorflow.org/datasets/catalog/lsun
",,,Google TPU v3,,Confident,"We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at this https URL",,,Open weights (unrestricted),United States of America,,,,,,2025-06-18 12:25,,,,,,Academia,,,,,Open source,"https://github.com/hojonathanho/diffusion

everything is openly avaiable but no license or terms of use information

train code: https://github.com/hojonathanho/diffusion/blob/master/scripts/run_lsun.py ",Academia,$436.31,,,,Hardware,,,,
DeBERTa,Language,"Language modeling/generation,Question answering",Microsoft,"Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen",2021-06-10,DeBERTa: Decoding-enhanced BERT with Disentangled Attention,https://arxiv.org/abs/2006.03654,2229.0,"Highly cited,SOTA improvement","""DeBERTa significantly outperforms all existing PLMs of similar size on MNLI and creates a new state of the art""",1500000000.0,"""...we scale up DeBERTa by training a larger version that consists of 48 Transform layers with 1.5 billion parameters""

Other versions are smaller and use a smaller pre-training dataset. These are distinguished in the paper (e.g. DeBERTa1.5B is the version of DeBERTa with 1.5 billion parameters).",2.588e+22,"Table 8: 16 DGX-2 nodes (x16 V100s each) for 30 days
16 * 16 * 1.3e14 * 30 * 24 * 3600 * 0.3 = 2.588e22","Wikipedia,CC-Stories,OPENWEBTEXT,""BookCorpus (BooksCorpus, Toronto Book Corpus)""","We pre-train our large models following the setting of BERT (Devlin et al., 2019), except that we use the BPE vocabulary of Radford et al. (2019); Liu et al. (2019c). For training data, we use Wikipedia (English Wikipedia dump3; 12GB), BookCorpus (Zhu et al., 2015) (6GB), OPENWEBTEXT (public Reddit content (Gokaslan & Cohen, 2019); 38GB), and STORIES (a subset of CommonCrawl (Trinh & Le, 2018); 31GB). The total data size after data deduplication (Shoeybi et al., 2019) is about 78G",15600000000.0,""" DeBERTa is pretrained on 78G training data""

1GB ~ 200M words",720.0,30 days (Table 8),NVIDIA V100,Self-supervised learning,Confident,"Recent progress in pre-trained neural language models has significantly improved the performance of many natural language processing (NLP) tasks. In this paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT with disentangled attention) that improves the BERT and RoBERTa models using two novel techniques. The first is the disentangled attention mechanism, where each word is represented using two vectors that encode its content and position, respectively, and the attention weights among words are computed using disentangled matrices on their contents and relative positions, respectively. Second, an enhanced mask decoder is used to incorporate absolute positions in the decoding layer to predict the masked tokens in model pre-training. In addition, a new virtual adversarial training method is used for fine-tuning to improve models' generalization. We show that these techniques significantly improve the efficiency of model pre-training and the performance of both natural language understanding (NLU) and natural langauge generation (NLG) downstream tasks. Compared to RoBERTa-Large, a DeBERTa model trained on half of the training data performs consistently better on a wide range of NLP tasks, achieving improvements on MNLI by +0.9% (90.2% vs. 91.1%), on SQuAD v2.0 by +2.3% (88.4% vs. 90.7%) and RACE by +3.6% (83.2% vs. 86.8%). Notably, we scale up DeBERTa by training a larger version that consists of 48 Transform layers with 1.5 billion parameters. The significant performance boost makes the single DeBERTa model surpass the human performance on the SuperGLUE benchmark (Wang et al., 2019a) for the first time in terms of macro-average score (89.9 versus 89.8), and the ensemble DeBERTa model sits atop the SuperGLUE leaderboard as of January 6, 2021, out performing the human baseline by a decent margin (90.3 versus 89.8).",49.2,,Open weights (unrestricted),"United States of America,Multinational,India,Belgium",,,,256.0,,2025-06-18 12:17,,,,,,Industry,,,,,Open source,"MIT

https://github.com/microsoft/DeBERTa

pretrain code: https://github.com/microsoft/DeBERTa/tree/master/experiments/language_model ",Industry,$6682.23,,FP16,155381.83775233384,Hardware,,,,
CoAtNet,Vision,Image classification,"Google,Google Research,Google Brain","Zihang Dai, Hanxiao Liu, Quoc V. Le, Mingxing Tan",2021-06-09,"CoAtNet: Marrying Convolution and Attention
for All Data Sizes",https://arxiv.org/abs/2106.04803v2,997.0,SOTA improvement,"""Notably, when we further scale up CoAtNet with JFT-3B, it achieves 90.88% top-1 accuracy on ImageNet, establishing a new state-of-the-art result.""",2440000000.0,,4.27e+22,"20.1K TPU-v3 core-days

TPUs have two cores per chip, and a chip is 123 teraflop/s 
https://cloud.google.com/tpu/docs/system-architecture-tpu-vm#tpu_v3

123 teraflop/s * 20100/2 * 24 * 3600 * 0.4 (utilization assumption for non-language models) = 4.27e22",JFT-3B,"When only ImageNet-1K is used for training, CoAtNet achieves 86.0% top-1 accuracy, matching the prior art NFNet [20] under similar computation resource and training conditions. Further, when pre-trained on ImageNet-21K with about 10M images, CoAtNet reaches 88.56% top-1 accuracy when finetuned on ImageNet-1K, matching the ViT-Huge pre-trained on JFT-300M, a 23Ã— larger dataset. Finally, when JFT-3B is used for pre-training, CoAtNet exhibits better efficiency compared to ViT, and pushes the ImageNet-1K top-1 accuracy to 90.88% while using 1.5x less computation of the prior art set by ViT-G/14 [26].",3000000000.0,"Used JFT-3B (3 billion images), but not stated for how many epochs. 

Based on GPU time, training took 4.27e+22 FLOPs. Table 5 indicates 2.586e12 FLOPs per image. Since training is roughly 3x the FLOP cost of inference, implies inference on full training set took 1.42e22 FLOP
Then # images trained over is around 1.42e22 / 2.586e12 = 5,491,105,955

So probably ~1.83 epochs on 3B images",,,Google TPU v3,,Confident,"Transformers have attracted increasing interests in computer vision, but they still fall behind state-of-the-art convolutional networks. In this work, we show that while Transformers tend to have larger model capacity, their generalization can be worse than convolutional networks due to the lack of the right inductive bias. To effectively combine the strengths from both architectures, we present CoAtNets (pronounced â€œcoatâ€ nets), a family of hybrid models built from two key insights: (1) depthwise Convolution and self-Attention can be naturally unified via simple relative attention; (2) vertically stacking convolution layers and attention layers in a principled way is surprisingly effective in improving generalization, capacity and efficiency. Experiments show that our CoAtNets achieve state-of-the-art performance under different resource constraints across various datasets: Without extra data, CoAtNet achieves 86.0% ImageNet top-1 accuracy; When pre-trained with 13M images from ImageNet-21K, our CoAtNet achieves 88.56% top-1 accuracy, matching ViT-huge pre-trained with 300M images from JFT-300M while using 23x less data; Notably, when we further scale up CoAtNet with JFT-3B, it achieves 90.88% top-1 accuracy on ImageNet, establishing a new state-of-the-art result.",1.83,,Unreleased,"United States of America,Multinational,United States of America,Canada,Switzerland,United States of America",,,,,,2025-02-17 11:55,,,,,,"Industry,Industry,Industry",,,,10050.0,Unreleased,,"Industry,Industry,Industry",$1887.16,,,,Hardware,,,,15
EMDR,Language,Question answering,"Mila - Quebec AI (originally Montreal Institute for Learning Algorithms),McGill University,DeepMind","Devendra Singh Sachan, Siva Reddy, William Hamilton, Chris Dyer, Dani Yogatama",2021-06-09,End-to-End Training of Multi-Document Reader and Retriever for Open-Domain Question Answering,https://arxiv.org/abs/2106.05346v2,147.0,SOTA improvement,"""Experiments on three benchmark datasets demonstrate that our proposed method outperforms all existing approaches of comparable size by 2-3% absolute exact match points, achieving new state-of-the-art results.""",440000000.0,Table 2,1.91e+21,"""We run all of our experiments on a machine with 96 CPUs, 1.3TB physical memory, and 16 A100 GPUs. We use PyTorch (Paszke et al., 2019) to implement our proposed model. With this hardware setup, our experiments on NQ and TriviaQA took approximately 25 hours to complete, while experiments on WebQ took roughly 8 hours to complete. Before supervised training, we also perform a one-time unsupervised MSS pre-training for 82,000 steps that took roughly 1 week.""

1 week + 25 hours * 16 A100s
= ~193 * 16 A100-hours
= 193 * 16 * 3600 * 312 trillion * 0.3 = 1.04e21

Additionally, the model uses BERT, ICT, and T5 models. These required:
- BERT: 6 * 110M parameters * (1M * 256 * 256) inputs = 4.33e19 FLOP
- ICT: 6 * 220M parameters * (100k * 4096 * 256) inputs = 1.38e20 FLOP
- T5: 6 * 220M parameters * (1M * 2048 * 256) inputs = 6.92e20 FLOP

Total: 1.04e21 + 4.33e19 + 1.38e20 + 6.92e20 = 1.91e21","Wikipedia,NQ (Natural Questions),TriviaQA","Pre-train different components on Wikipedia, BookCorpus, C4, and OpenWebText (Table 6), then training on the QA datasets",171600000000.0,"At the time of publication there were about 4B words (5.3B tokens) on English Wikipedia: https://en.wikipedia.org/wiki/Wikipedia:Size_of_Wikipedia#Yearly_statistics
BookCorpus has about 1B words (1.3B tokens), C4 has about 156B tokens, and OpenWebText has about 9B tokens.

From Table 6, it looks like all datasets were trained on for over one epoch.

BERT: 1M steps, batches of 256, sequence length 256 = 65.5B tokens vs 6.6B in Wikipedia + BookCorpus
ICT: 100k steps, batches of 4096, sequence length 256 = 104.9B tokens vs 5.3B in Wikipedia
T5: 1M steps, batches of 2048, sequence length 256 = 524.3B tokens vs 170.3B tokens in C4 + Wikipedia + OpenWebText

Total tokens: 171.6 billion

Some tokens were probably seen more times than others, but overall this corresponds to 4.05 epochs on the pre-training data. ",355.0,"""We run all of our experiments on a machine with 96 CPUs, 1.3TB physical memory, and 16 A100 GPUs [...] our experiments on NQ and TriviaQA took approximately 25 hours to complete, while experiments on WebQ took roughly 8 hours to complete. Before supervised training, we also perform a one-time unsupervised MSS pre-training for 82,000 steps that took roughly 1 week""

Additionally, they pre-trained BERT, ICT, and T5 models, which took a combined 8.733e20 FLOPs. On 16 A100s at 0.3 utilization, that would have taken approximately 162 hours.

So total time for the largest experiment (NQ or TriviaQA) is around:
25 + 168 + 162 = 355",NVIDIA A100,,Confident,"We present an end-to-end differentiable training method for retrieval-augmented open-domain question answering systems that combine information from multiple retrieved documents when generating answers. We model retrieval decisions as latent variables over sets of relevant documents. Since marginalizing over sets of retrieved documents is computationally hard, we approximate this using an expectation-maximization algorithm. We iteratively estimate the value of our latent variable (the set of relevant documents for a given question) and then use this estimate to update the retriever and reader parameters. We hypothesize that such end-to-end training allows training signals to flow to the reader and then to the retriever better than staged-wise training. This results in a retriever that is able to select more relevant documents for a question and a reader that is trained on more accurate documents to generate an answer. Experiments on three benchmark datasets demonstrate that our proposed method outperforms all existing approaches of comparable size by 2-3% absolute exact match points, achieving new state-of-the-art results. Our results also demonstrate the feasibility of learning to retrieve to improve answer generation without explicit supervision of retrieval decisions.",4.05,,Open weights (unrestricted),"Canada,Canada,United Kingdom of Great Britain and Northern Ireland",,,,,,2025-05-28 16:12,,,,,,"Academia,Academia,Industry",,,,,Open source,"https://github.com/DevSinghSachan/emdr2

training scripts: https://github.com/DevSinghSachan/emdr2/tree/main/examples ","Academia,Academia,Industry",$2773.31,,FP16,,"Hardware,Operation counting",,,,
ViT-G/14,Vision,Image classification,"Google Brain,Google Research","X Zhai, A Kolesnikov, N Houlsby, L Beyer",2021-06-08,Scaling Vision Transformers,https://arxiv.org/abs/2106.04560,903.0,SOTA improvement,"""we successfully train a ViT model with two billion parameters, which attains a new state-of-the-art on ImageNet of 90.45% top-1 accuracy""",1843000000.0,Table 2 of paper,5.85e+22,"Digitizing Figure 9 indicates training used 27,200 TPUv3 core-days. TPUv3 is 123 teraflop/s per chip, 2 cores per chip.

1.23e14 * (1/2) * 27,200 * 24 * 3600 * 0.4 = 5.78e22

Alternatively, Table 2 indicates 965.3e9 FLOPs per forward pass on a 224^2 image. Table 4 indicates 5 million steps at a (normalized) batch size of 4096, and total flops including backward pass would be 3x the FLOPs from forward passes alone, so we get:
4096 * 5e6 * 965.3e9 * 3 = 5.93e22

(Note that actual batch size appears to have been 32,768)

Geometric mean: sqrt(5.78e22*5.93e22) = 5.85e22

However note that this leaderboard claims ViT-G/14 took 34 PF-days, or 2.94e21 FLOPs: https://web.archive.org/web/20211218185755/https://lair.lighton.ai/akronomicon/","JFT-3B,ImageNet","We trained a large Vision Transformer, ViT-G/14, which
contains nearly two billion parameters. Section 3.6 details
the architectureâ€™s shape. We evaluate the ViT-G/14 model on
a range of downstream tasks, and compare it to recent stateof-the-art results. We fine-tune on ImaegNet",3000000000.0,"""For this study, we use the proprietary JFT-3B dataset, a larger version of the JFT-300M dataset used in many previous works on large-scale computer vision models [31, 18, 11]. This dataset consists of nearly 3 billion images, annotated with a class-hierarchy of around 30k labels via a semi-automatic
pipeline""

Epochs: 5M steps (Table 11) * 32768 (batch size) / 3B = 54.6 epochs",,,Google TPU v3,Self-supervised learning,Confident,"Attention-based neural networks such as the Vision Transformer (ViT) have recently attained state-of-the-art results on many computer vision benchmarks. Scale is a primary ingredient in attaining excellent results, therefore, understanding a model's scaling properties is a key to designing future generations effectively. While the laws for scaling Transformer language models have been studied, it is unknown how Vision Transformers scale. To address this, we scale ViT models and data, both up and down, and characterize the relationships between error rate, data, and compute. Along the way, we refine the architecture and training of ViT, reducing memory consumption and increasing accuracy of the resulting models. As a result, we successfully train a ViT model with two billion parameters, which attains a new state-of-the-art on ImageNet of 90.45% top-1 accuracy. The model also performs well for few-shot transfer, for example, reaching 84.86% top-1 accuracy on ImageNet with only 10 examples per class.",54.6,,Unreleased,"United States of America,Multinational,United States of America,Canada,Switzerland",,,,2048.0,,2025-05-28 16:12,,,,32768.0,,"Industry,Industry",,,,,Open source,"About the weights: https://twitter.com/giffmana/status/1402507421029916672

About the code: Apache 2.0
https://github.com/google-research/big_vision/blob/main/big_vision/configs/proj/scaling_laws/train_vit_g.py","Industry,Industry",$3847.86,,BF16,911614.0493863056,"Hardware,Operation counting",,,,11
AFP+FPI (PTB),Language,Language modeling,University of Sheffield,"Zhengxiong Wang, Anton Ragni",2021-06-04,Approximate Fixed-Points in Recurrent Neural Networks,https://arxiv.org/abs/2106.02417,2.0,,,2040000.0,"'- RNN with 1 hidden layer 
- 200 hidden units
- Vocabulary size = 10,000

The RNN layer will have:

- Input weight matrix U: (vocab size x hidden size) = (10,000 x 200) = 2,000,000 parameters

- Recurrent weight matrix W: (hidden size x hidden size) = (200 x 200) = 40,000 parameters

- Bias vector b: (hidden size) = (200) = 200 parameters 

Total parameters:

U = 2,000,000
W = 40,000 
b = 200

Total parameters = U + W + b
            = 2,000,000 + 40,000 + 200
            = 2,040,200

Therefore, the total number of parameters with 200 hidden units and 10,000 vocab size is 2,040,200.

_______
from the Algorithmic progress paper spreadsheet",223363710000000.0,"6 FLOP / parameter / token * 2,040,200 parameters  * 912344 tokens * 20 epochs = 2.2336371e+14 FLOP",Penn TreeBank (PTB),,912344.0,,,,,,Likely,"Recurrent neural networks are widely used in speech and language processing. Due to dependency on the past, standard algorithms for training these models, such as back-propagation through time (BPTT), cannot be efficiently parallelised. Furthermore, applying these models to more complex structures than sequences requires inference time approximations, which introduce inconsistency between inference and training. This paper shows that recurrent neural networks can be reformulated as fixed-points of non-linear equation systems. These fixed-points can be computed using an iterative algorithm exactly and in as many iterations as the length of any given sequence. Each iteration of this algorithm adds one additional Markovian-like order of dependencies such that upon termination all dependencies modelled by the recurrent neural networks have been incorporated. Although exact fixed-points inherit the same parallelization and inconsistency issues, this paper shows that approximate fixed-points can be computed in parallel and used consistently in training and inference including tasks such as lattice rescoring. Experimental validation is performed in two tasks, Penn Tree Bank and WikiText-2, and shows that approximate fixed-points yield competitive prediction performance to recurrent neural networks trained using the BPTT algorithm.",20.0,AFP+FPI (PTB),Unreleased,United Kingdom of Great Britain and Northern Ireland,,,,,,2025-04-07 14:07,,,,,,Academia,,,,,Unreleased,,Academia,,,,,Operation counting,,,,
GPT2-Large+LHOPT,Language,Language modeling/generation,OpenAI,"Diogo Almeida, Clemens Winter, Jie Tang, Wojciech Zaremba",2021-06-02,A Generalizable Approach to Learning Optimizers,https://arxiv.org/abs/2106.00958,27.0,,,760000000.0,,4.9540697e+21,"base model compute (speculative confidence) 4.9536e+21 FLOP + fine-tune compute 4.6968e+17 FLOP = 4.9540697e+21 FLOP

________
estimation from the Arithmetic progress paper: 1.6E+21 FLOP",WikiText-103,,103000000.0,,,,,,Speculative,"A core issue with learning to optimize neural networks has been the lack of generalization to real world problems. To address this, we describe a system designed from a generalization-first perspective, learning to update optimizer hyperparameters instead of model parameters directly using novel features, actions, and a reward function. This system outperforms Adam at all neural network tasks including on modalities not seen during training. We achieve 2x speedups on ImageNet, and a 2.5x speedup on a language modeling task using over 5 orders of magnitude more compute than the training tasks.",1.0,GPT2-Large+LHOPT,Unreleased,United States of America,GPT-2 (774M),469680000000000000,6 FLOP / parameter / token * 760000000 parameters * 103000000 tokens * 1 epoch = 4.6968e+17 FLOP,,,2025-04-07 15:13,,,GPT2-Large+LHOPT,13000.0,Figure 3,Industry,,,,,Unreleased,there's a repo for the optimizer but no training code for this model: https://github.com/openai/LHOPT,Industry,,,,,Operation counting,,,,
Wu Dao 2.0,"Multimodal,Language,Vision,Image generation","Image captioning,Chat,Image generation,Text-to-image,Language modeling/generation,Question answering,Visual question answering",Beijing Academy of Artificial Intelligence / BAAI,"Tang Jie, Zhai Jidong, Yang Hongxia, Chen Wenguang, Zheng Weimin, Ma Zixuan, He Jiaao, Qiu Jiezhong, Cao Huanqi, Wang Yuanwei, Sun Zhenbo, Zheng Liyan, Wang Haojie, Tang Shizhi, Feng Guanyu, Zeng Aohan, Zhong Runxin, Shi Tianhui, Du Zhengxiao,
Ding Ming, Tiago Antunes, Peng Jinjun, Lin Junyang Zhang Jianwei ",2021-05-31,China's gigantic multi-modal AI is no one-trick pony,https://www.engadget.com/chinas-gigantic-multi-modal-ai-is-no-one-trick-pony-211414388.html,,,,1750000000000.0,"""It's been trained on 1.75 trillion parameters""

MoE architecture (https://keg.cs.tsinghua.edu.cn/jietang/publications/wudao-3.0-meta-en.pdf), unknown amount of active parameters",1.029e+25,"It's a mixture-of-experts model, so 1.75 trillion params likely overstates how much compute was required:

""The parameter scale of Enlightenment 2.0 reached a record-breaking 1.75 trillion. According to reports, the new generation FastMoE technology is the key to the realization of the ""Trillion Model"" cornerstone of Enlightenment 2.0.""

Speculatively assuming 20% of parameters are active and the model was trained for one epoch: 
6 FLOP / token / parameter * 1.75 * 10^12 parameters * 0.2 * 4.9 * 10^12 tokens [see dataset size notes] = 1.029e+25 FLOP",WuDao Corpora,"WuDao Corpora, as of version 2.0, was a large dataset constructed for training Wu Dao 2.0. It contains 3 terabytes of text scraped from web data, 90 terabytes of graphical data (incorporating 630 million text/image pairs), and 181 gigabytes of Chinese dialogue (incorporating 1.4 billion dialogue rounds).
https://en.wikipedia.org/wiki/Wu_Dao#WuDao_Corpora",4900000000000.0,"[tokens assumed[
""Bilingual (Cn and En) data: 4.9T text and images"" 

https://keg.cs.tsinghua.edu.cn/jietang/publications/wudao-3.0-meta-en.pdf",,,,,Speculative,"Sporting 1.75 trillion parameters, Wu Dao 2.0 is roughly ten times the size of Open AI's GPT-3.",,,API access,China,,,,,,2025-06-12 11:06,,,,,,Academia,checked,,,,,"based on this presentation weights and codes could be open sourced but links are unreachable from the US:
https://keg.cs.tsinghua.edu.cn/jietang/publications/wudao-3.0-meta-en.pdf",Academia,,,,,Operation counting,,,,
CODA,Language,"Language modeling,Translation","The University of Hong Kong,Sun Yat-sen University,Shanghai AI Lab","Lin Zheng, Zhiyong Wu, Lingpeng Kong",2021-05-31,Cascaded Head-colliding Attention,https://arxiv.org/abs/2105.14850,2.0,,,246930000.0,Table 1,,"Largest model trained is on WikiText-103. We don't know how many epochs the model trained for, but FLOPs per epoch is roughly
6 * 103M * 246.93M = 1.526e17. However, this work replaces multi-head attention with a more complex block. They don't state the time complexity, but I suspect it is higher than for MHA.",WikiText-103,"""First, we conducted experiments for token-level language modeling on a large-scale benchmark dataset Wikitext-103 (Merity et al., 2016), which consists of articles from Wikipedia with the token number around 103M/218K/246K for the training/validation/testing splits respectively. The vocabulary size is 267,744.""",103000000.0,"""Wikitext-103 [...] consists of articles from Wikipedia with the token number around 103M/218K/246K for the training/validation/testing splits respectively.""",,,,,Confident,"Transformers have advanced the field of natural language processing (NLP) on a variety of important tasks. At the cornerstone of the Transformer architecture is the multi-head attention (MHA) mechanism which models pairwise interactions between the elements of the sequence. Despite its massive success, the current framework ignores interactions among different heads, leading to the problem that many of the heads are redundant in practice, which greatly wastes the capacity of the model. To improve parameter efficiency, we re-formulate the MHA as a latent variable model from a probabilistic perspective. We present cascaded head-colliding attention (CODA) which explicitly models the interactions between attention heads through a hierarchical variational distribution. We conduct extensive experiments and demonstrate that CODA outperforms the transformer baseline, by 0.6 perplexity on \texttt{Wikitext-103} in language modeling, and by 0.6 BLEU on \texttt{WMT14 EN-DE} in machine translation, due to its improvements on the parameter efficiency.\footnote{Our implementation is publicly available at \url{this https URL}.}",,CODA,Unreleased,"Hong Kong,China,China,China",,,,,,2024-09-30 17:43,,,,,,"Academia,Academia,Academia",,,,,Open source,MIT for code: https://github.com/LZhengisme/CODA,"Academia,Academia,Academia",,,,,,,,,
ByT5-XXL,Language,Language modeling,"Google,Google Research","Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, Colin Raffel",2021-05-28,ByT5: Towards a token-free future with pre-trained byte-to-byte models,https://arxiv.org/abs/2105.13626,392.0,SOTA improvement,"""On the most realistic in-language setting, where some gold training data is available in all languages, ByT5 surpasses the previous state-of-art mT5 on all tasks and model sizes""",12900000000.0,"12.9B, from Table 1",8.1e+22,"""Like mT5, we set our sequence length to 1024 (bytes rather than tokens), and train for 1 million steps over batches of 2^20 tokens.""

12.9 billion * 1 million * 2^20 * 6 = ~8.1e22",mC4,,,,,"Table 9 indicates pretraining completes 25 sequences per second on a TPUv3-64 device. 

""we set our sequence length to 1024 (bytes rather than tokens), and train for 1 million steps over batches of 2^20 tokens."" 

So 1024 sequences per step * 1M steps = 1.024 billion sequences
1.024 B / 25 = 40.96M seconds = 11378 hours or 474 days. This seems implausible, so probably they just used a bigger TPU slice for the full training, but this is not indicated.",Google TPU v3,,Likely,"Most widely-used pre-trained language models operate on sequences of tokens corresponding to word or subword units. By comparison, token-free models that operate directly on raw text (bytes or characters) have many benefits: they can process text in any language out of the box, they are more robust to noise, and they minimize technical debt by removing complex and error-prone text preprocessing pipelines. Since byte or character sequences are longer than token sequences, past work on token-free models has often introduced new model architectures designed to amortize the cost of operating directly on raw text. In this paper, we show that a standard Transformer architecture can be used with minimal modifications to process byte sequences. We characterize the trade-offs in terms of parameter count, training FLOPs, and inference speed, and show that byte-level models are competitive with their token-level counterparts. We also demonstrate that byte-level models are significantly more robust to noise and perform better on tasks that are sensitive to spelling and pronunciation. As part of our contribution, we release a new set of pre-trained byte-level Transformer models based on the T5 architecture, as well as all code and data used in our experiments.",,,Open weights (unrestricted),"United States of America,Multinational,United States of America,Canada,Switzerland",,,,,,2024-12-22 14:25,,,,1048576.0,"""Like mT5, we set our sequence length to 1024 (bytes rather than tokens), and train for 1 million steps over batches of 2^20 tokens""","Industry,Industry",,,,,Open source,"Apache:
https://github.com/google-research/byt5/blob/master/LICENSE

see mC4 notes for data accessibility

training script: https://github.com/google-research/byt5/blob/master/README.md ","Industry,Industry",$92453.38,,,,Operation counting,,,,7
Transformer local-attention (NesT-B),Vision,"Image classification,Image generation","Google Cloud,Google Research","Zizhao Zhang, Han Zhang, Long Zhao, Ting Chen, Sercan ArÄ±k, Tomas Pfister",2021-05-26,"Nested Hierarchical Transformer: Towards Accurate, Data-Efficient and Interpretable Visual Understanding",https://arxiv.org/abs/2105.12723v4,5734.0,Highly cited,,90100000.0,"Table A2, NesT-B is the largest size.",2.40576e+19,"17.9 GFLOPS per forward pass
300 epochs
1.28M training examples
3.5 f_to_b pass ratio
(From Imagenet paper-data, Besiroglu et al., forthcoming) 

17.9e9 FLOP *300 epoch *1.28M images *3.5 forward-backward-ratio = 24057600000000000000",ImageNet-1k,,1280000.0,,,,,Self-supervised learning,Confident,"Hierarchical structures are popular in recent vision transformers, however, they require sophisticated designs and massive datasets to work well. In this paper, we explore the idea of nesting basic local transformers on non-overlapping image blocks and aggregating them in a hierarchical way. We find that the block aggregation function plays a critical role in enabling cross-block non-local information communication. This observation leads us to design a simplified architecture that requires minor code changes upon the original vision transformer. The benefits of the proposed judiciously-selected design are threefold: (1) NesT converges faster and requires much less training data to achieve good generalization on both ImageNet and small datasets like CIFAR; (2) when extending our key ideas to image generation, NesT leads to a strong decoder that is 8Ã— faster than previous transformer-based generators; and (3) we show that decoupling the feature learning and abstraction processes via this nested hierarchy in our design enables constructing a novel method (named GradCAT) for visually interpreting the learned model. Source code is available this https URL.",,,Open weights (unrestricted),"Multinational,United States of America,Multinational,United States of America,Canada,Switzerland",,,,,,2025-06-18 12:13,,,,,,"Industry,Industry",,,,,Open source,"Apache-2.0 license, includes train code and evaluation
https://github.com/google-research/nested-transformer","Industry,Industry",,,,,Operation counting,,,,
CogView,Image generation,Text-to-image,"Tsinghua University,Alibaba DAMO Academy","M Ding, Z Yang, W Hong, W Zheng, C Zhou",2021-05-26,CogView: Mastering Text-to-Image Generation via Transformers,https://arxiv.org/abs/2105.13290,631.0,SOTA improvement,"""CogView achieves the state-of-the-art FID on the blurred MS COCO dataset, outperforming previous GAN-based models and a recent similar work DALL-E""",4000000000.0,"""We propose CogView, a 4-billion-parameter Transformer with VQ-VAE tokenizer to advance this problem.""",2.68e+22,"source: https://lair.lighton.ai/akronomicon/
archived: https://github.com/lightonai/akronomicon/tree/main/akrodb",WuDao Corpora,"""We collected about 30 million text-image pairs from multiple channels, and built a 2.5TB new dataset (after tokenization, the size becomes about 250GB).""",50000000000.0,"""We collected about 30 million text-image pairs from multiple channels, and built a 2.5TB new dataset (after tokenization, the size becomes about 250GB).""

250GB * (1 word / 5 bytes) = 50 billion words or 67 billion tokens

So 30M text-image pairs and 50 billion words",,,NVIDIA Tesla V100 DGXS 16 GB,Self-supervised learning,Likely,"Text-to-Image generation in the general domain has long been an open problem, which requires both a powerful generative model and cross-modal understanding. We propose CogView, a 4-billion-parameter Transformer with VQ-VAE tokenizer to advance this problem. We also demonstrate the finetuning strategies for various downstream tasks, e.g. style learning, super-resolution, text-image ranking and fashion design, and methods to stabilize pretraining, e.g. eliminating NaN losses. CogView achieves the state-of-the-art FID on the blurred MS COCO dataset, outperforming previous GAN-based models and a recent similar work DALL-E.",,,Open weights (unrestricted),"China,China",,,,512.0,,2025-05-28 16:12,,,,,,"Academia,Industry",checked,,,,Open source,"Apache 2 license

https://github.com/THUDM/CogView

train script: https://github.com/THUDM/CogView/blob/main/scripts/pretrain_single_node.sh ","Academia,Industry",$60071.71,,FP16,259056.25043301377,Third-party estimation,,,,18
ConSERT,Language,Language modeling,"Meituan University,Beijing University of Posts and Telecommunications","Yuanmeng Yan, Rumei Li, Sirui Wang, Fuzheng Zhang, Wei Wu, Weiran Xu",2021-05-25,ConSERT: A contrastive framework for self-supervised sentence representation transfer,https://arxiv.org/abs/2105.11741,490.0,,"Trains an effective BERT model on small sample sizes and claims to achieve an 8% improvement over previous SOTA on STA datasets. However, Papers With Code shows that none of the results were actually SOTA at the time of publication.",340000000.0,,2.8e+20,"Fine-tuning was done using a single Nvidia V100 GPU for a few minutes -> 1.0E+15 to 5.0E+15 (2 to 10 min)

Foundation model is BeRT with 2.8e+20 FLOP.

So total compute is 2.8e+20.",Chinese STS,,,,0.1,,NVIDIA Tesla V100S PCIe 32 GB,,Confident,"Learning high-quality sentence representations benefits a wide range of natural language processing tasks. Though BERT-based pre-trained language models achieve high performance on many downstream tasks, the native derived sentence representations are proved to be collapsed and thus produce a poor performance on the semantic textual similarity (STS) tasks. In this paper, we present ConSERT, a Contrastive Framework for Self-Supervised Sentence Representation Transfer, that adopts contrastive learning to fine-tune BERT in an unsupervised and effective way. By making use of unlabeled texts, ConSERT solves the collapse issue of BERT-derived sentence representations and make them more applicable for downstream tasks. Experiments on STS datasets demonstrate that ConSERT achieves an 8\% relative improvement over the previous state-of-the-art, even comparable to the supervised SBERT-NLI. And when further incorporating NLI supervision, we achieve new state-of-the-art performance on STS tasks. Moreover, ConSERT obtains comparable results with only 1000 samples available, showing its robustness in data scarcity scenarios.",,,Open weights (unrestricted),"China,China",,,,,,2025-02-17 11:57,,,,,,"Academia,Academia",,,,,Open source,https://github.com/yym6472/ConSERT,"Academia,Academia",$957.27,,,,Hardware,,,,
MedBERT,Medicine,"Medical diagnosis,Text classification,Prediction of hospital stay duration,Prediction of diabetic heart failure (DHF),Prediction of onset of pancreatic cancer (PaCa)","Peng Cheng Laboratory,University of Texas at Houston","Laila Rasmy, Yang Xiang, Ziqian Xie, Cui Tao, Degui Zhi",2021-05-20,Med-BERT: pretrained contextualized embeddings on large-scale structured electronic health records for disease prediction,https://www.nature.com/articles/s41746-021-00455-y,517.0,SOTA improvement,"""This work is the first demonstration of significantly boosted
performance over state-of-the-art methods on multiple
clinical tasks with phenotyped cohorts.""",17000000.0,"17M from ""This is possibly due to the fact that the untrained Med-BERT is an over-parameterized model (around 17 million parameters) with a huge
number of configurations, so it might overfit to the training data""",9.47e+18,"flops = (1) * (3.13e13) * (24*7 * 3600) * (0.5) = 9.47e18
(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)
I assume higher utilization rate, because only 1 GPU is used.
Citation from the text:
""We used a single Nvidia Tesla V100GPU of 32 GB graphics memory capacity, and we trained the model for a week for more than 45 million steps, for which each step consists of 32 patients (batch size)."" - page 11

Note that public code appears not to make use of the tensor core speed up, thus I use 3.13e13 FLOP/sec",Cerner Health Facts,"page 3 data source
""We extracted our cohorts from two databases: Cerner Health
FactsÂ® (version 2017) (Cerner) and Truven Health MarketScanÂ®
(Truven)""
""Our pretraining cohort for Med-BERT is consisting of 28 million
patients extracted from Cerner""""",,"data about 28M patients
""Our pretraining cohort for Med-BERT is consisting of 28 million
patients extracted from Cerner""",168.0,"""We used a single Nvidia Tesla V100GPU of 32 GB graphics memory capacity, and we trained the model for a week for more than 45 million steps, for which each step consists of 32 patients (batch size)."" - page 11",NVIDIA Tesla V100 DGXS 32 GB,,Likely,"Deep learning (DL)-based predictive models from electronic health records (EHRs) deliver impressive performance in many clinical tasks. Large training cohorts, however, are often required by these models to achieve high accuracy, hindering the adoption of DL-based models in scenarios with limited training data. Recently, bidirectional encoder representations from transformers (BERT) and related models have achieved tremendous successes in the natural language processing domain. The pretraining of BERT on a very large training corpus generates contextualized embeddings that can boost the performance of models trained on smaller datasets. Inspired by BERT, we propose Med-BERT, which adapts the BERT framework originally developed for the text domain to the structured EHR domain. Med-BERT is a contextualized embedding model pretrained on a structured EHR dataset of 28,490,650 patients. Fine-tuning experiments showed that Med-BERT substantially improves the prediction accuracy, boosting the area under the receiver operating characteristics curve (AUC) by 1.21â€“6.14% in two disease prediction tasks from two clinical databases. In particular, pretrained Med-BERT obtains promising performances on tasks with small fine-tuning training sets and can boost the AUC by more than 20% or obtain an AUC as high as a model trained on a training set ten times larger, compared with deep learning models without Med-BERT. We believe that Med-BERT will benefit disease prediction studies with small local training datasets, reduce data collection expenses, and accelerate the pace of artificial intelligence aided healthcare.",50.5,,Unreleased,"China,United States of America",,,,1.0,,2025-05-28 16:12,,,,,,"Academia,Academia",,,,168.0,Open source,"Apache 2
https://github.com/ZhiGroup/Med-BERT
training guide: https://github.com/ZhiGroup/Med-BERT/tree/master/Pretraining%20Code 

""Initially we really hoped to share our models but unfortunately, the pre-trained models are no longer sharable. According to SBMI Data Service Office: ""Under the terms of our contracts with data vendors, we are not permitted to share any of the data utilized in our publications, as well as large models derived from those data.""","Academia,Academia",$62.49,,FP32,278.0422249147422,Hardware,,,,
Multitask Unified Model (MUM),"Language,Multimodal,Vision","Language modeling/generation,Image captioning,Question answering,Visual question answering,Translation,Character recognition,Search",Google,,2021-05-18,MUM: A new AI milestone for understanding information,https://blog.google/products/search/introducing-mum/,,,May have been deployed in Google search and other products,,,,,,,,,,,,,Unknown,"When I tell people I work on Google Search, Iâ€™m sometimes asked, ""Is there any work left to be done?"" The short answer is an emphatic â€œYes!â€ There are countless challenges we're trying to solve so Google Search works better for you. Today, weâ€™re sharing how we're addressing one many of us can identify with: having to type out many queries and perform many searches to get the answer you need.

Take this scenario: Youâ€™ve hiked Mt. Adams. Now you want to hike Mt. Fuji next fall, and you want to know what to do differently to prepare. Today, Google could help you with this, but it would take many thoughtfully considered searches â€” youâ€™d have to search for the elevation of each mountain, the average temperature in the fall, difficulty of the hiking trails, the right gear to use, and more. After a number of searches, youâ€™d eventually be able to get the answer you need.

But if you were talking to a hiking expert; you could ask one question â€” â€œwhat should I do differently to prepare?â€ Youâ€™d get a thoughtful answer that takes into account the nuances of your task at hand and guides you through the many things to consider.  

This example is not unique â€” many of us tackle all sorts of tasks that require multiple steps with Google every day. In fact, we find that people issue eight queries on average for complex tasks like this one. 

Today's search engines aren't quite sophisticated enough to answer the way an expert would. But with a new technology called Multitask Unified Model, or MUM, we're getting closer to helping you with these types of complex needs. So in the future, youâ€™ll need fewer searches to get things done. ",,,Unreleased,United States of America,T5-11B,,"Based on T5 (specific model not specified). But also multimodal:

""MUM uses the T5 text-to-text framework and is 1,000 times more powerful than BERT.""",,,2025-06-16 14:29,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
Fairseq + UID: variance,Language,Language modeling/generation,"Google AI,ETH Zurich,University of Cambridge","Jason Wei, Clara Meister, Ryan Cotterell",2021-05-15,A Cognitive Regularizer for Language Modeling,https://arxiv.org/abs/2105.07144,20.0,,,,,,,,,,,,,,,Unknown,"The uniform information density (UID) hypothesis, which posits that speakers behaving optimally tend to distribute information uniformly across a linguistic signal, has gained traction in psycholinguistics as an explanation for certain syntactic, morphological, and prosodic choices. In this work, we explore whether the UID hypothesis can be operationalized as an inductive bias for statistical language modeling. Specifically, we augment the canonical MLE objective for training language models with a regularizer that encodes UID. In experiments on ten languages spanning five language families, we find that using UID regularization consistently improves perplexity in language models, having a larger effect when training data is limited. Moreover, via an analysis of generated sequences, we find that UID-regularized language models have other desirable properties, e.g., they generate text that is more lexically diverse. Our results not only suggest that UID is a reasonable inductive bias for language modeling, but also provide an alternative validation of the UID hypothesis using modern-day NLP tools.",,Fairseq + UID: variance,Unreleased,"Multinational,United States of America,Switzerland,United Kingdom of Great Britain and Northern Ireland",,,,,,2024-10-01 10:01,,,Fairseq + UID: variance,,,"Industry,Academia,Academia",,,,,Unreleased,,"Industry,Academia,Academia",,,,,,,,,
ADM,Image generation,"Image generation,Text-to-image",OpenAI,"Prafulla Dhariwal, Alex Nichol",2021-05-11,Diffusion Models Beat GANs on Image Synthesis,https://arxiv.org/abs/2105.05233,5567.0,"Highly cited,SOTA improvement","""We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models""",559000000.0,"Largest model is denoted ImageNet 512, has 559M parameters",6.199999999999999e+21,"Largest run with their architecture improvements is the ImageNet 512 variant. Table 7 suggests utilization is around 30% for largest models (though we only see 256 x 256 and 128 -> 512)

Table 10: ImageNet 512 variant took 1914 V100-days of training
125e12 FLOP/sec * 1914 days * 24 h/day * 3600 sec/h * 0.3 = 6.2e21","LSUN,ILSVRC 2012 subset of ImageNet","""To evaluate our improved model architecture on unconditional image generation, we train separate diffusion models on three LSUN [71] classes: bedroom, horse, cat""",1281167.0,"Biggest models are trained on ImageNet 512x512. ImageNet ILSVRC has 1,281,167 images in the training set, but it is possible some were filtered due to size.

Note that a smaller model was trained on LSUN {bedroom, horse, cat}, which forms a larger dataset:
3,033,042 + 2,000,340 + 1,657,266 = 6,690,648 images

Epochs â‰ˆ (1,940,000 * 256) / 1,300,000 â‰ˆ 381 epochs",,,NVIDIA V100,,Confident,"We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128Ã—128, 4.59 on ImageNet 256Ã—256, and 7.72 on ImageNet 512Ã—512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256Ã—256 and 3.85 on ImageNet 512Ã—512.",381.0,,Open weights (non-commercial),United States of America,,,,,,2025-05-28 16:12,,,,,,Industry,,,,,Open source,"These models are intended to be used for research purposes only. In particular, they can be used as a baseline for generative modeling research, or as a starting point to build off of for such research.
These models are not intended to be commercially deployed. Additionally, they are not intended to be used to create propaganda or offensive imagery.

repo is here with training code, MIT License
https://github.com/openai/guided-diffusion",Industry,$11274.48,,FP16,,Hardware,,,,
ProtT5-XXL,Biology,"Proteins,Protein or nucleotide language model (pLM/nLM)","Technical University of Munich,Med AI Technology,NVIDIA,Oak Ridge National Laboratory,Google,Seoul National University","A Elnaggar, M Heinzinger, C Dallago, G Rihawi",2021-05-04,ProtTrans: Towards Cracking the Language of Lifeâ€™s Code Through Self-Supervised Learning,"https://www.biorxiv.org/content/10.1101/2020.07.12.199554v3 or 
https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9477085",396.0,SOTA improvement,"""For the per-residue predictions the transfer of the most informative embeddings (ProtT5) for the first time outperformed the state-of-the-art
without using evolutionary information""",11000000000.0,"source: https://lair.lighton.ai/akronomicon/

archived: https://github.com/lightonai/akronomicon/tree/main/akrodb",7.370000000000001e+22,"7.37e22 from:
source: https://lair.lighton.ai/akronomicon/
archived: https://github.com/lightonai/akronomicon/blob/main/akrodb/Technical%20University%20of%20Munich/ProtT5-XXL.json

3.7E+22 from Table 9 https://www.biorxiv.org/content/10.1101/2023.07.05.547496v1

Manual calculation: forward passes on 512 * (4096*920k + 2048*343k) = 2.3T tokens
Backward passes on 15% of those, 2.3T * 0.15 = 343B tokens.
Total FLOPs: (2 * 11B * 2.3T) + (4 * 11B * 343B) = 6.57e22","BFD (Big Fantastic Dataset),UniRef50","First, T5-XL and T5-XXL were trained on BFD for 1.2M and 920k steps respectively (ProtT5-XL-BFD, ProtT5-XXL-BFD). In a second step, ProtT5-XL-BFD and ProtT5-XXL-BFD were fine-tuned on
UniRef50 for 991k and 343k steps respectively (ProtT5-XLU50, ProtT5-XXL-U50).
Table 1 and 2 give enough info to calculate epochs:
BFD: 512 (seq len) * 4096 (global batch) * 920k (steps) / 393B = 4.9 epochs
UniRef50: 512 (seq len) * 2048 (global batch) * 343k (steps) / 14B = 25.7 epochs",407000000000.0,"Table 2. ProtT5-XXL uses BFD100 and UniRef50, which sum to 407 billion amino acids.",,,Google TPU v3,Self-supervised learning,Confident,"Computational biology and bioinformatics provide vast data gold-mines from protein sequences, ideal for Language Models taken from NLP. These LMs reach for new prediction frontiers at low inference costs. Here, we trained two auto-regressive models (Transformer-XL, XLNet) and four auto-encoder models (BERT, Albert, Electra, T5) on data from UniRef and BFD containing up to 393 billion amino acids. The LMs were trained on the Summit supercomputer using 5616 GPUs and TPU Pod up-to 1024 cores. Dimensionality reduction revealed that the raw protein LM-embeddings from unlabeled data captured some biophysical features of protein sequences. We validated the advantage of using the embeddings as exclusive input for several subsequent tasks. The first was a per-residue prediction of protein secondary structure (3-state accuracy Q3=81%-87%); the second were per-protein predictions of protein sub-cellular localization (ten-state accuracy: Q10=81%) and membrane vs. water-soluble (2-state accuracy Q2=91%). For the per-residue predictions the transfer of the most informative embeddings (ProtT5) for the first time outperformed the state-of-the-art without using evolutionary information thereby bypassing expensive database searches. Taken together, the results implied that protein LMs learned some of the grammar of the language of life. To facilitate future work, we released our models at https://github.com/agemagician/ProtTrans.",,,Open weights (unrestricted),"Germany,China,United States of America,United States of America,United States of America,Korea (Republic of)",,,,512.0,,2025-05-28 16:12,,,,,,"Academia,Industry,Government,Industry,Academia",,,,,Unreleased,"Licensed under the Academic Free License version 3.0

The ProtTrans project is a open source project supported by various partner companies and research institutions. We are committed to share all our pre-trained models and knowledge. 
https://github.com/agemagician/ProtTrans","Academia,Industry,Government,Industry,Academia",$85701.26,,FP32,228081.21596876832,"Third-party estimation,Operation counting",,,,7
ProtT5-XXL-BFD,Biology,"Proteins,Protein or nucleotide language model (pLM/nLM)","Technical University of Munich,Med AI Technology,NVIDIA,Oak Ridge National Laboratory,Google,Seoul National University","Ahmed Elnaggar, Michael Heinzinger,  Christian Dallago, Ghalia Rehawi, Yu Wang, Llion Jones, Tom Gibbs, Tamas Feher, Christoph Angerer, Martin Steinegger, Debsindhu Bhowmik, Burkhard Rost",2021-05-04,ProtTrans:Towards Cracking the Language of Life's Code Through Self-Supervised Learning,"https://www.biorxiv.org/content/10.1101/2020.07.12.199554v3 or 
https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9477085",,SOTA improvement,"""For the per-residue predictions the transfer of the most informative embeddings (ProtT5) for the first time outperformed the state-of-the-art without using evolutionary information thereby bypassing expensive database searches.""",11000000000.0,Table 2,3.7e+22,"FLOP = 11B*2*(920k*512*4096) +  11B*4*(920k*512*4096), 920k steps using seq length 512 batch size 4096, ",BFD (Big Fantastic Dataset),"First, T5-XL and T5-XXL were trained on BFD for 1.2M and 920k steps respectively (ProtT5-XL-BFD, ProtT5-XXL-BFD). In a second step, ProtT5-XL-BFD and ProtT5-XXL-BFD were fine-tuned on
UniRef50 for 991k and 343k steps respectively (ProtT5-XLU50, ProtT5-XXL-U50).",,"Table 1: 2122M proteins, 393B amino acids, 572 GB",,,Google TPU v3,Self-supervised learning,Confident,"Computational biology and bioinformatics provide vast data gold-mines from protein sequences, ideal for Language Models taken from NLP. These LMs reach for new prediction frontiers at low inference costs. Here, we trained two auto-regressive models (Transformer-XL, XLNet) and four auto-encoder models (BERT, Albert, Electra, T5) on data from UniRef and BFD containing up to 393 billion amino acids. The LMs were trained on the Summit supercomputer using 5616 GPUs and TPU Pod up-to 1024 cores.

Dimensionality reduction revealed that the raw protein LM-embeddings from unlabeled data captured some biophysical features of protein sequences. We validated the advantage of using the embeddings as exclusive input for several subsequent tasks. The first was a per-residue prediction of protein secondary structure (3-state accuracy Q3=81%-87%); the second were per-protein predictions of protein sub-cellular localization (ten-state accuracy: Q10=81%) and membrane vs. water-soluble (2-state accuracy Q2=91%). For the per-residue predictions the transfer of the most informative embeddings (ProtT5) for the first time outperformed the state-of-the-art without using evolutionary information thereby bypassing expensive database searches. Taken together, the results implied that protein LMs learned some of the grammar of the language of life. To facilitate future work, we released our models at https://github.com/agemagician/ProtTrans.",5.0,,Open weights (unrestricted),"Germany,China,United States of America,United States of America,United States of America,Korea (Republic of)",,,,512.0,,2025-05-28 16:12,,,,,,"Academia,Industry,Government,Industry,Academia",,,,,Unreleased,"Licensed under the Academic Free License version 3.0

The ProtTrans project is a open source project supported by various partner companies and research institutions. We are committed to share all our pre-trained models and knowledge. 
https://github.com/agemagician/ProtTrans","Academia,Industry,Government,Industry,Academia",$43025.06,,FP32,228081.21596876832,Operation counting,,,,14
ProtBERT-BFD,Biology,"Proteins,Protein or nucleotide language model (pLM/nLM)","Technical University of Munich,NVIDIA,Seoul National University,Google,Oak Ridge National Laboratory,Med AI Technology","Ahmed Elnaggar, Michael Heinzinger,  Christian Dallago, Ghalia Rehawi, Yu Wang, Llion Jones, Tom Gibbs, Tamas Feher, Christoph Angerer, Martin Steinegger,  Debsindhu Bhowmik, Burkhard Rost",2021-05-04,ProtTrans:Towards Cracking the Language of Life's Code Through Self-Supervised Learning,"https://www.biorxiv.org/content/10.1101/2020.07.12.199554v3 or 
https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9477085",,SOTA improvement,"""For the per-residue predictions the transfer of the most informative embeddings (ProtT5) for the first time outperformed the state-of-the-art without using evolutionary information thereby bypassing expensive database searches.""",420000000.0,Table 2,3.9e+22,"FLOP = 420M * 6 * (800k*512*32k + 200k*2048*6k) 
1M steps total split into two phases, (1) 800k steps, seq length 512 (batch size 32k) and (2) 200k steps, seq length 2048 (batch size 6k)
single TPU Pod V3-1024 (64 nodes and 1024 TPUs) info from paper and https://huggingface.co/Rostlab/prot_bert_bfd",BFD (Big Fantastic Dataset),"ProtBert: BERT2 was trained using both UniRef100
and BFD-100 datasets (referred to as ProtBert and ProtBertBFD, respectively; Table 2)",8900000000000.0,"""ProtBERT-BFD (420M parameters) saw around 27B proteins during pre-training"" 

Table 1: BFD has 2122M proteins, 393B amino acids, 572 GB
Suggests average amino acid length of 185

Implies 27B * 185 = 5T amino acids seen in training

However, Table 2 suggests number of tokens (amino acids) seen in training was:
(512*32768*800k) + (2048*6144*200k) = 15.9T amino acids in training

Geometric mean = 8.9T",,"figure 3 shows 19 hours per epoch, though this was on a different GPU setup than the one used for training.",Google TPU v3,Self-supervised learning,Confident,"Computational biology and bioinformatics provide vast data gold-mines from protein sequences, ideal for Language Models taken from NLP. These LMs reach for new prediction frontiers at low inference costs. Here, we trained two auto-regressive models (Transformer-XL, XLNet) and four auto-encoder models (BERT, Albert, Electra, T5) on data from UniRef and BFD containing up to 393 billion amino acids. The LMs were trained on the Summit supercomputer using 5616 GPUs and TPU Pod up-to 1024 cores.

Dimensionality reduction revealed that the raw protein LM-embeddings from unlabeled data captured some biophysical features of protein sequences. We validated the advantage of using the embeddings as exclusive input for several subsequent tasks. The first was a per-residue prediction of protein secondary structure (3-state accuracy Q3=81%-87%); the second were per-protein predictions of protein sub-cellular localization (ten-state accuracy: Q10=81%) and membrane vs. water-soluble (2-state accuracy Q2=91%). For the per-residue predictions the transfer of the most informative embeddings (ProtT5) for the first time outperformed the state-of-the-art without using evolutionary information thereby bypassing expensive database searches. Taken together, the results implied that protein LMs learned some of the grammar of the language of life. To facilitate future work, we released our models at https://github.com/agemagician/ProtTrans.",,,Open weights (unrestricted),"Germany,United States of America,Korea (Republic of),United States of America,United States of America,China",,,,1024.0,,2025-05-28 16:12,,,,,,"Academia,Industry,Academia,Industry,Government",,,,,Unreleased,"Licensed under the Academic Free License version 3.0

The ProtTrans project is a open source project supported by various partner companies and research institutions. We are committed to share all our pre-trained models and knowledge. 
https://github.com/agemagician/ProtTrans","Academia,Industry,Academia,Industry,Government",$45350.74,,FP32,456162.43193753663,Operation counting,,,,13
ProtBERT-UniRef,Biology,"Proteins,Protein or nucleotide language model (pLM/nLM)","Technical University of Munich,NVIDIA,Seoul National University,Google,Oak Ridge National Laboratory,Med AI Technology","Ahmed Elnaggar, Michael Heinzinger,  Christian Dallago, Ghalia Rehawi, Yu Wang, Llion Jones, Tom Gibbs, Tamas Feher, Christoph Angerer, Martin Steinegger,  Debsindhu Bhowmik, Burkhard Rost",2021-05-04,ProtTrans:Towards Cracking the Language of Life's Code Through Self-Supervised Learning,"https://www.biorxiv.org/content/10.1101/2020.07.12.199554v3 or 
https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9477085",5.0,,,420000000.0,Table 2,7.269999999999999e+21,"Table 2, two stages with different sequence lengths
Stage 1: 300k steps, sequence length 512, batch size 15360
Stage 2: 100k steps, sequence length 2048, batch size 2560

6ND formula
Stage 1: 6*300000*15360*512*420000000=5.9454259e+21
Stage 2: 6*100000*2560*2048*420000000=1.3212058e+21
Total: 7.2666317e+21",,,,,,,Google TPU v3,,Confident,,,,,"Germany,United States of America,Korea (Republic of),United States of America,United States of America,China",,,,512.0,,2025-05-01 10:42,,,,,,"Academia,Industry,Academia,Industry,Government",,,,,,,"Academia,Industry,Academia,Industry,Government",,,,228081.21596876832,,,,,
ProtT5-XL-U50,Biology,Proteins,,"Ahmed Elnaggar, Michael Heinzinger, View ORCID ProfileChristian Dallago, Ghalia Rehawi, Yu Wang, Llion Jones, Tom Gibbs, Tamas Feher, Christoph Angerer, Martin Steinegger, View ORCID ProfileDebsindhu Bhowmik, Burkhard Rost",2021-05-04,,https://www.biorxiv.org/content/10.1101/2020.07.12.199554v3.full.pdf,,Highly cited,,3000000000.0,Table 2,1.8704498688e+22,"991K steps, 2048 batch, 512 sequence length (Table 2)

Total tokens: 991K*2048*512=1039138816000

FLOP: 6*1039138816000*3000000000=18704498688000000000000",UniRef50,,1039138816000.0,,,,,,Confident,,,,Open weights (non-commercial),,,,,256.0,,2025-05-28 16:59,,,,,,,,,,,Unreleased,"Academic Free License v3.0 License
https://huggingface.co/Rostlab/prot_t5_xl_uniref50

It seems that there is no training code here, only inference and fine-tuning
https://github.com/agemagician/ProtTrans",,,,FP32,,,Rostlab,,,
Transformer-XL + SIS,Language,Language modeling,INRIA,"Sagar Verma, Jean-Christophe Pesquet",2021-05-03,Sparsifying Networks via Subdifferential Inclusion,http://proceedings.mlr.press/v139/verma21b/verma21b.pdf,13.0,,,246000000.0,"246M
""Transformer-XL is a language model with 246 million parameters.""",3.7848651e+20,"base model training compute: 3.7832771e+20 FLOP
 ",,,,,,,,,Speculative,"Sparsifying deep neural networks is of paramount interest in many areas, especially when those networks have to be implemented on lowmemory devices. In this article, we propose a new formulation of the problem of generating sparse weights for a pre-trained neural network. By leveraging the properties of standard nonlinear activation functions, we show that the problem is equivalent to an approximate subdifferential inclusion problem. The accuracy of the approximation controls the sparsity. We show that the proposed approach is valid for a broad class of activation functions (ReLU, sigmoid, softmax). We propose an iterative optimization algorithm to induce sparsity whose convergence is guaranteed. Because of the algorithm flexibility, the sparsity can be ensured from partial training data in a minibatch manner. To demonstrate the effectiveness of our method, we perform experiments on various networks in different applicative contexts: image classification, speech recognition, natural language processing, and time-series forecasting. Project page: https://sagarverma.github.io/compression",,Transformer-XL + SIS,Unreleased,France,Transformer-XL (257M),,,,,2025-04-21 20:10,,,Transformer-XL + SIS,,,Academia,,,,,Unreleased,,Academia,,,,,Comparison with other models,,,,
GPT-J-6B,Language,"Language modeling/generation,Question answering,Automated theorem proving,Code generation","EleutherAI,LAION","Ben Wang, Aran Komatsuzaki",2021-05-01,GPT-J-6B: 6B JAX-Based Transformer,https://arankomatsuzaki.wordpress.com/2021/06/04/gpt-j/,,,,6053381344.0,source: model details table in GitHub,1.5e+22,source: zero shot evaluation table in GitHub,The Pile,,400000000000.0,"""The model was trained on 400B tokens from The Pile dataset with 800GB text.""

1 GB ~ 200M words",840.0,"""GPT-J training took roughly five weeks with TPU v3-256.""
5*7*24=",Google TPU v3,Self-supervised learning,Confident,"We have released GPT-J-6B, 6B JAX-based (Mesh) Transformer LM (Github).
GPT-J-6B performs nearly on par with 6.7B GPT-3 (or Curie) on various zero-shot down-streaming tasks.
You can try out this Colab notebook or free web demo.
This library also serves as an example of model parallelism with xmap on JAX.",1.0,GPT-J-6B,Open weights (unrestricted),"Multinational,United States of America,Multinational,Germany",,,,,0.6,2025-05-09 11:32,,,,,,"Research collective,Research collective",,,,,Unreleased,Apache 2.0. finetune but not training code: https://github.com/kingoflolz/mesh-transformer-jax?tab=readme-ov-file ,"Research collective,Research collective",,"""At the 6B config on a TPU V3-256 pod, GPT-J achieves high absolute efficiency. The hardware has a theoretical maximum of 13.4PFLOPs, and GPT-J achieves 5.4 PFLOPs as measured in the GPT3 paper (ignoring attention computation, ignoring compute-memory tradeoffs like gradient checkpointing). When taking these additional factors into account, 8.1 PFLOPs, or approximately 60% of the theoretical maximum is utilized.""",,,Reported,,,,
ViT + DINO,Vision,Image classification,"INRIA,Facebook AI Research","Mathilde Caron, Hugo Touvron, Ishan Misra, HervÃ© JÃ©gou, Julien Mairal, Piotr Bojanowski, Armand Joulin",2021-04-29,Emerging Properties in Self-Supervised Vision Transformers,https://arxiv.org/abs/2104.14294,4593.0,Highly cited,,85000000.0,"85M, table 1",2.1e+20,"""Overall, training DINO with Vision Transformers
achieves 76.1 top-1 accuracy using two 8-GPU servers for 3
days""

GPU is V100

16 * 125 teraflops * 3 days * 0.4 utilization
= 2.1e20

However, this isn't the best result in the paper (which is 80.1% with ViT-B/8). 76.1% is the result from ViT-B/16 per Table 2, which may be 5x cheaper than ViT-B/8 based on Table 1?

upd:
 ""Table 8: Time and memory requirements. We show total running
time and peak memory per GPU (â€œmem.â€) when running ViT-S/16
DINO models on two 8-GPU machines.""

2*8*125 teraflops*72.6h*3600*0.4=2.09088e+20",ImageNet,"""We pretrain the models on the ImageNet dataset [60] without labels""",,,,,NVIDIA V100,Self-supervised learning,Confident,"In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training, and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base.",300.0,,Open weights (unrestricted),"France,United States of America",,,,,,2025-05-09 11:32,,,,,,"Academia,Industry",,,,,Open source,"models and code including training: https://github.com/facebookresearch/dino

Apache-2.0 license
","Academia,Industry",$380.28,,,,Hardware,,,,
SPALM + kNN,Language,Language modeling/generation,DeepMind,"Dani Yogatama, Cyprien de Masson dâ€™Autume, Lingpeng Kong",2021-04-26,Adaptive Semiparametric Language Models,https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00371/100688/Adaptive-Semiparametric-Language-Models,95.0,,"paper says ""Our implementation produces results that are in the same range as state-of-the-art numbers, demonstrating the strength of our baselines"" I think this suggests it's close to, but not reaching SOTA?",,,,,,,,,,,,,Unknown,"We present a language model that combines a large parametric neural network (i.e., a transformer) with a non-parametric episodic memory component in an integrated architecture. Our model uses extended short-term context by caching local hidden statesâ€”similar to transformer-XLâ€”and global long-term memory by retrieving a set of nearest neighbor tokens at each timestep. We design a gating function to adaptively combine multiple information sources to make a prediction. This mechanism allows the model to use either local context, short-term memory, or long-term memory (or any combination of them) on an ad hoc basis depending on the context. Experiments on word-based and character-based language modeling datasets demonstrate the efficacy of our proposed method compared to strong baselines.",,SPALM + kNN,Unreleased,United Kingdom of Great Britain and Northern Ireland,,,,,,2024-09-09 11:28,,,SPALM + kNN,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
PanGu-Î±,Language,"Language modeling/generation,Text summarization,Question answering",Huawei Noah's Ark Lab,"Wei Zeng, Xiaozhe Ren, Teng Su, Hui Wang, Yi LiaoZhiwei WangXin JiangZhenzhang YangKaisheng WangXiaoda ZhangChen LiZiyan GongYifan YaoXinjing HuangJun WangJianfeng YuQi GuoYue YuYan ZhangJin WangHengtao TaoDasen YanZexuan YiFang PengFangqing JiangHan ZhangLingfeng DengYehong ZhangZhe LinChao ZhangShaojie ZhangMingyue GuoShanzhi GuGaojun FanYaowei WangXuefeng JinQun LiuYonghong Tian",2021-04-25,PanGu-Î±: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation,https://arxiv.org/abs/2104.12369,187.0,,,207000000000.0,Table 1,5.112000000000001e+22,"Figure 8 digitized with https://automeris.io/wpd/?v=5_2, training the 200B model runs for 42.6B tokens. 6 * 200B * 42.6B = 5.112e22

Elsewhere, the akronomicon leaderboard indicates 538 PF-days, or 5.037e22 FLOPs.
https://web.archive.org/web/20220211115721/https://lair.lighton.ai/akronomicon/

Note however, that this seems to contradict Table 4, which appears to indicate that PanGu-Î± 200B saw 317.569B tokens. If this were the case, training compute would be ~ 6 * 200B * 317.569B = 3.811e23",,Custom dataset,258500000000.0,"Column 1 in Table 4 sums to 258.5B tokens. Note however that ""Epochs elapsed when training"" from this table appears to conflict with other statements about training, so my confidence in the info here is somewhat low.",,,Huawei Ascend 910,Self-supervised learning,Likely,"Large-scale Pretrained Language Models (PLMs) have become the new paradigm for Natural Language Processing (NLP). PLMs with hundreds of billions parameters such as GPT-3 have demonstrated strong performances on natural language understanding and generation with \textit{few-shot in-context} learning. In this work, we present our practice on training large-scale autoregressive language models named PanGu-Î±, with up to 200 billion parameters. PanGu-Î± is developed under the MindSpore and trained on a cluster of 2048 Ascend 910 AI processors. The training parallelism strategy is implemented based on MindSpore Auto-parallel, which composes five parallelism dimensions to scale the training task to 2048 processors efficiently, including data parallelism, op-level model parallelism, pipeline model parallelism, optimizer model parallelism and rematerialization. To enhance the generalization ability of PanGu-Î±, we collect 1.1TB high-quality Chinese data from a wide range of domains to pretrain the model. We empirically test the generation ability of PanGu-Î± in various scenarios including text summarization, question answering, dialogue generation, etc. Moreover, we investigate the effect of model scales on the few-shot performances across a broad range of Chinese NLP tasks. The experimental results demonstrate the superior capabilities of PanGu-Î± in performing various tasks under few-shot or zero-shot settings.",1.0,,Unreleased,China,,,,2048.0,,2025-05-09 11:32,,,,,"260k steps, # tokens not too clear (Figure 8 suggests ~40B, seems too low, maybe a typo?)",Industry,,,,,,,Industry,,,,1285806.3528918722,"Operation counting,Third-party estimation",,,,
DiffQ Transformer (16L),Language,Language modeling,Meta AI,"Alexandre DÃ©fossez, Yossi Adi, Gabriel Synnaeve",2021-04-20,Differentiable Model Compression via Pseudo Quantization Noise,https://arxiv.org/abs/2104.09987,36.0,,,247000000.0,"They base their architecture off of ""Transformer (Adaptive Input Embeddings) WT103"", a 16 layer transformer with 16 heads per MHA layer, d_model=1024 and d_ff=4096.",3.02e+19,"The authors say they train their base model following Baevski & Auli (2019). That paper trained for 286k steps in batches of 65,536 tokens.
This suggests training the base model took 6 * 247M * 286k * 65536 = 2.778E19 FLOPs. 

However, DiffQ introduces some additional computational overhead during training: ""Using DiffQ usually increase the training time by some amount. On the language modeling task, the time per batch went from 115ms to 125ms.""

This suggests 125/115 - 1 = 8.7% overhead, relative to the direct FLOPs used to do vanilla forward and backward passes on the model weights.

2.778E19 * 125 / 115 = 3.020e19",WikiText-103,"WikiText-103, which has 103M training tokens",103000000.0,"WikiText-103, which has 103M training tokens",,,NVIDIA V100,Self-supervised learning,Confident,"We propose DiffQ a differentiable method for model compression for quantizing model parameters without gradient approximations (e.g., Straight Through Estimator). We suggest adding independent pseudo quantization noise to model parameters during training to approximate the effect of a quantization operator. DiffQ is differentiable both with respect to the unquantized weights and the number of bits used. Given a single hyper-parameter balancing between the quantized model size and accuracy, DiffQ optimizes the number of bits used per individual weight or groups of weights, in end-to-end training. We experimentally verify that our method is competitive with STE based quantization techniques on several benchmarks and architectures for image classification, language modeling, and audio source separation. For instance, on the ImageNet dataset, DiffQ compresses a 12 layers transformer-based model by more than a factor of 8, (lower than 4 bits precision per weight on average), with a loss of 0.3% in model accuracy. Code is available at this http URL.",,DiffQ Transformer (16L),Unreleased,United States of America,,,,,,2025-05-09 11:32,,,,,,Industry,,,,,Open (non-commercial),training code (non-commercial): https://github.com/facebookresearch/diffq/blob/main/examples/FAIRSEQ_README.md,Industry,,,,,Operation counting,,,,
DLRM-12T,Recommendation,Recommender system,"Meta AI,Carnegie Mellon University (CMU)","Dheevatsa Mudigere, Yuchen Hao, Jianyu Huang, Zhihao Jia, Andrew Tulloch, Srinivas Sridharan, Xing Liu, Mustafa Ozdal, Jade Nie, Jongsoo Park, Liang Luo, Jie Amy Yang, Leon Gao, Dmytro Ivchenko, Aarti Basant, Yuxi Hu, Jiyan Yang, Ehsan K. Ardestani, Xiaodong Wang, Rakesh Komuravelli, Ching-Hsiang Chu, Serhat Yilmaz, Huayu Li, Jiyuan Qian, Zhuobo Feng, Yinbin Ma, Junjie Yang, Ellie Wen, Hong Li, Lin Yang, Chonglin Sun, Whitney Zhao, Dimitry Melts, Krishna Dhulipala, KR Kishore, Tyler Graf, Assaf Eisenman, Kiran Kumar Matam, Adi Gangidi, Guoqiang Jerry Chen, Manoj Krishnan, Avinash Nayak, Krishnakumar Nair, Bharath Muthiah, Mahmoud khorashadi, Pallab Bhattacharya, Petr Lapukhov, Maxim Naumov, Ajit Mathews, Lin Qiao, Mikhail Smelyanskiy, Bill Jia, Vijay Rao",2021-04-12,Software-Hardware Co-design for Fast and Scalable Training of Deep Learning Recommendation Models,https://arxiv.org/abs/2104.05158,126.0,,,12000000000000.0,They instantiated a 12T-parameter model to show that their hardware setup can train it despite the huge memory requirements.,,No training details provided.,,No training details provided.,,No training details provided.,,No training details provided.,NVIDIA A100,,Confident,"Deep learning recommendation models (DLRMs) are used across many business-critical services at Facebook and are the single largest AI application in terms of infrastructure demand in its data-centers. In this paper we discuss the SW/HW co-designed solution for high-performance distributed training of large-scale DLRMs. We introduce a high-performance scalable software stack based on PyTorch and pair it with the new evolution of Zion platform, namely ZionEX. We demonstrate the capability to train very large DLRMs with up to 12 Trillion parameters and show that we can attain 40X speedup in terms of time to solution over previous systems. We achieve this by (i) designing the ZionEX platform with dedicated scale-out network, provisioned with high bandwidth, optimal topology and efficient transport (ii) implementing an optimized PyTorch-based training stack supporting both model and data parallelism (iii) developing sharding algorithms capable of hierarchical partitioning of the embedding tables along row, column dimensions and load balancing them across multiple workers; (iv) adding high-performance core operators while retaining flexibility to support optimizers with fully deterministic updates (v) leveraging reduced precision communications, multi-level memory hierarchy (HBM+DDR+SSD) and pipelining. Furthermore, we develop and briefly comment on distributed data ingestion and other supporting services that are required for the robust and efficient end-to-end training in production environments.",,,,"United States of America,United States of America",,,,,,2024-11-01 10:03,,,,,,"Industry,Academia",,,,,,,"Industry,Academia",,,,,,,,,
Megatron-LM (1T),Language,Text autocompletion,"Microsoft Research,NVIDIA,Stanford University","Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Patwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan Catanzaro, Amar Phanishayee, Matei Zaharia",2021-04-09,Efficient Large-Scale Language Model Training on GPU Clusters,https://arxiv.org/abs/2104.04473,469.0,,Improved SOTA efficiency at distributed training.,1000000000000.0,"[NOTE: They didn't train the model fully end-to-end, probably just to obtain enough information to gauge the ability to do model parallelisation]

""Our approach allows us to perform training iterations on a model with 1 trillion parameters at 502 petaFLOP/s on 3072 GPUs with achieved per-GPU throughput of 52% of theoretical peak.""",,"NOTE: They didn't train the model fully end-to-end, probably just to obtain enough information to gauge the ability to do model parallelisation.

We calculate below the FLOP required for a full training, but we do not populate it in the Training Compute column.

â€œFor the 1 trillion parameter model, we assume that 450 billion tokens are needed for end-to-end training. With 3072 A100 GPUs, we can achieve a per-GPU throughput of 163 teraFLOP/s, and end-to-end training time of 84 days. We believe these training times (using a reasonable number of GPUs) are practical.â€

Table 1 gives a utilisation rate of 52%

Plugging this into the calculator: https://epoch.ai/blog/estimating-training-compute
84 days, 3072 GPUs, NVIDIA A100, FP16, 52% utilisation rate --> 3.6e24 FLOP",,Dataset information not provided.,,,,,NVIDIA A100,Self-supervised learning,Confident,"Large language models have led to state-of-the-art accuracies across a range of tasks. However, training these models efficiently is challenging for two reasons: a) GPU memory capacity is limited, making it impossible to fit large models on even a multi-GPU server, and b) the number of compute operations required to train these models can result in unrealistically long training times. Consequently, new methods of model parallelism such as tensor and pipeline parallelism have been proposed. Unfortunately, naive usage of these methods leads to fundamental scaling issues at thousands of GPUs, e.g., due to expensive cross-node communication or devices spending significant time waiting on other devices to make progress.
In this paper, we show how different types of parallelism methods (tensor, pipeline, and data parallelism) can be composed to scale to thousands of GPUs and models with trillions of parameters. We survey techniques for pipeline parallelism and propose a novel interleaved pipeline parallelism schedule that can improve throughput by 10+% with memory footprint comparable to existing approaches. We quantitatively study the trade-offs between tensor, pipeline, and data parallelism, and provide intuition as to how to configure distributed training of a large model. Our approach allows us to perform training iterations on a model with 1 trillion parameters at 502 petaFLOP/s on 3072 GPUs with achieved per-GPU throughput of 52% of theoretical peak.",,,,"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,United States of America,United States of America",,,,,,2025-05-09 11:32,,,,,,"Industry,Industry,Academia",,,,,,,"Industry,Industry,Academia",,,,,,,,,
Transformer-C,Language,Language modeling,University of Massachusetts Amherst,"Simeng Sun, Mohit Iyyer",2021-04-08,Revisiting Simple Neural Probabilistic Language Models,https://arxiv.org/abs/2104.03474,11.0,,,148000000.0,"148M, Table 6",1.8877734000000005e+18,"6 FLOP / token / parameter * 148000000 parameters * 10240 tokens per batch * 200000 steps = 1.818624e+18 FLOP

11340000000000 FLOP / sec [assumed fp32] * 4 GPUs * 40 hours * 3600 sec/ hour * 0.3 [assumed precision] = 1.959552e+18 FLOP

geometric mean: 
sqrt(1.818624e+18*1.959552e+18) = 1.8877734e+18 ",WikiText-103,,103000000.0,"Table 7
steps  200k
batch size  10240

10240 * 200000 / 103000000 = 19.88 epochs",40.0,table 7,NVIDIA GeForce GTX 1080 Ti,,Confident,"Recent progress in language modeling has been driven not only by advances in neural architectures, but also through hardware and optimization improvements. In this paper, we revisit the neural probabilistic language model (NPLM) of~\citet{Bengio2003ANP}, which simply concatenates word embeddings within a fixed window and passes the result through a feed-forward network to predict the next word. When scaled up to modern hardware, this model (despite its many limitations) performs much better than expected on word-level language model benchmarks. Our analysis reveals that the NPLM achieves lower perplexity than a baseline Transformer with short input contexts but struggles to handle long-term dependencies. Inspired by this result, we modify the Transformer by replacing its first self-attention layer with the NPLM's local concatenation layer, which results in small but consistent perplexity decreases across three word-level language modeling datasets.",19.88,Transformer-C,Unreleased,United States of America,,,,4.0,,2025-04-07 19:37,,,,10240.0,table 7,Academia,,,,,Open source,"permissive license, BSD-3: https://github.com/SimengSun/revisit-nplm",Academia,,,,2026.0414956053005,"Operation counting,Hardware",,,,
T2R + Random Init,Language,Language modeling,"University of Washington,Microsoft,DeepMind,Allen Institute for AI","Jungo Kasai, Hao Peng, Yizhe Zhang, Dani Yogatama, Gabriel Ilharco, Nikolaos Pappas, Yi Mao, Weizhu Chen, Noah A. Smith",2021-03-24,Finetuning Pretrained Transformers into RNNs,https://arxiv.org/abs/2103.13076,51.0,,,450000000.0,"450M
source: https://arxiv.org/pdf/2203.12644
table 3",2.749544e+19,"6 FLOP / token / parameter * 450000000 parameters * 74000 tokens per batch * 286000 steps = 5.71428e+19 FLOP

125000000000000 [assumed bf16] * 98 GPU-hours * 3600 sec / hour * 0.3 [assumed utilization] = 1.323e+19 FLOP

geometric mean:
sqrt(5.71428e+19*1.323e+19) = 2.749544e+19",WikiText-103,,103000000.0,"""We train the model with a batch size of about 74K tokens with a total of 286K steps""

74000*286000 / 103000000 = 205.48 epochs",,Table 1,NVIDIA V100,,Likely,"Transformers have outperformed recurrent neural networks (RNNs) in natural language generation. But this comes with a significant computational cost, as the attention mechanism's complexity scales quadratically with sequence length. Efficient transformer variants have received increasing interest in recent works. Among them, a linear-complexity recurrent variant has proven well suited for autoregressive generation. It approximates the softmax attention with randomized or heuristic feature maps, but can be difficult to train and may yield suboptimal accuracy. This work aims to convert a pretrained transformer into its efficient recurrent counterpart, improving efficiency while maintaining accuracy. Specifically, we propose a swap-then-finetune procedure: in an off-the-shelf pretrained transformer, we replace the softmax attention with its linear-complexity recurrent alternative and then finetune. With a learned feature map, our approach provides an improved tradeoff between efficiency and accuracy over the standard transformer and other recurrent variants. We also show that the finetuning process has lower training cost relative to training these recurrent variants from scratch. As many models for natural language tasks are increasingly dependent on large-scale pretrained transformers, this work presents a viable approach to improving inference efficiency without repeating the expensive pretraining process.",205.48,T2R + Random Init,Unreleased,"United States of America,United States of America,Multinational,India,Belgium,United Kingdom of Great Britain and Northern Ireland,United States of America",,,,8.0,,2025-04-07 20:00,,,,74000.0,,"Academia,Industry,Industry,Research collective",,,,98.0,Unreleased,"There's a repo but it's kind of inscrutable with no docs about T2R, not clear if the training code for this paper is in it:

https://github.com/jungokasai/T2R/tree/master ","Academia,Industry,Industry,Research collective",,,,4864.124132899583,"Operation counting,Hardware",,,,
Unicorn,Language,Question answering,Allen Institute for AI,"Nicholas Lourie, Ronan Le Bras, Chandra Bhagavatula, Yejin Choi",2021-03-24,UNICORN on RAINBOW: A Universal Commonsense Reasoning Model on a New Multitask Benchmark,https://arxiv.org/abs/2103.13009,,,,11000000000.0,,,,RAINBOW,"Introduces RAINBOW, a transfer learning benchmark comprised of Î±NLI, COSMOSQA, HELLASWAG, PIQA, SOCIALIQA, and WINOGRANDE",324742.0,"Train set contains 324742 datapoints.
https://colab.research.google.com/drive/1MKmS96_kYWT4JsNW1GmEtgU63RqPCJ3P#scrollTo=vrAW7-DErrkf

Each dataset is a multiple choice QA, so one gradient calculated per question.

They first train on all six datasets simultaneously; then on each dataset separately for leaderboard submissions.

Epochs: it seems like they test both batch size 16 and 32 for pretraining over 25k steps, the larger of which would correspond to 2.46 epochs over the full dataset.
They then do 25k steps for each dataset, with a batch size of either 16 or 32 depending on the dataset. This means the fine-tuning involved the following number of epochs for each dataset:
- Anli: 2.36
- CosmosQA: 31.67
- HellaSWAG: 20.05
- PIQA: 49.65
- SocialIQA: 23.94
- WinoGrande: 9.90",3.0,"""Training times usually took several hours per run""
Guessing ",,,Confident,"Commonsense AI has long been seen as a near impossible goal -- until recently. Now, research interest has sharply increased with an influx of new benchmarks and models.
We propose two new ways to evaluate commonsense models, emphasizing their generality on new tasks and building on diverse, recently introduced benchmarks. First, we propose a new multitask benchmark, RAINBOW, to promote research on commonsense models that generalize well over multiple tasks and datasets. Second, we propose a novel evaluation, the cost equivalent curve, that sheds new insight on how the choice of source datasets, pretrained language models, and transfer learning methods impacts performance and data efficiency.
We perform extensive experiments -- over 200 experiments encompassing 4800 models -- and report multiple valuable and sometimes surprising findings, e.g., that transfer almost always leads to better or equivalent performance if following a particular recipe, that QA-based commonsense datasets transfer well with each other, while commonsense knowledge graphs do not, and that perhaps counter-intuitively, larger models benefit more from transfer than smaller ones.
Last but not least, we introduce a new universal commonsense reasoning model, UNICORN, that establishes new state-of-the-art performance across 8 popular commonsense benchmarks, aNLI (87.3%), CosmosQA (91.8%), HellaSWAG (93.9%), PIQA (90.1%), SocialIQa (83.2%), WinoGrande (86.6%), CycIC (94.0%) and CommonsenseQA (79.3%).",2.46,,Open weights (unrestricted),United States of America,T5-11B,3,"All experiments were run on Google Cloud using two Google Compute Engine virtual machine (VM) instances communicating with various TPUs. [...] Each VM had 20 vCPUs with 75GB of memory [...] For hardware acceleration, we ran all the experiments using v3-8 TPUs when building off of T5-LARGE or smaller. [...] The T5-11B models were trained using TPU v2-256 and v3-256s with a model parallelism of 16. Training times usually took several hours per run, so we ran many experiments in parallel on the VMs using GNU Parallel (Tange 2011).

Speculatively, ""several hours"" ~= 3 hours on v3-256 (128 chips) would suggest 3 * 3600 * 1.23e14 * 128 * 0.3 = 5.1e19 FLOPs

So unlikely to significantly move the 3.3e22 FLOPs from T5-11B pretraining.",,,2024-12-02 10:25,,,,32.0,,Research collective,,,,,Open source,"https://github.com/allenai/rainbow#downloading-the-weights
Apache 2.0
finetune code: https://github.com/allenai/rainbow/blob/master/bin/fine-tune.py ",Research collective,,,,,Hardware,,,,
GLM-10B,Language,Language modeling/generation,"Tsinghua University,Beijing Academy of Artificial Intelligence / BAAI,Massachusetts Institute of Technology (MIT),Shanghai Qi Zhi institute","Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, Jie Tang",2021-03-18,GLM: General Language Model Pretraining with Autoregressive Blank Infilling,https://arxiv.org/abs/2103.10360,1162.0,,smaller version of the model in this paper,10000000000.0,,4.9397553e+21,"6 FLOP / token / parameter * 10*10^9 parameters * 1024 samples per batch * 512 tokens per sample * 200000 steps [reported for smaller models] = 6.291456e+21 FLOP

31330000000000 [assumed fp16 precision] * 64 GPUs * 1791 hours [see training time notes] * 3600 sec / hour * 0.3 [assumed utilization] = 3.8784635e+21 FLOP

sqrt(6.291456e+21*3.8784635e+21) = 4.9397553e+21 
__________ 
estimation from the Algorithmic progress paper (I am not sure where the dataset size estimation comes from):

6 FLOP / token / parameter * (570649396491.227-334000000000+395050630177) tokens * 10*10^9 parameter = 3.7902002e+22 FLOP ",The Pile,see table https://github.com/THUDM/GLM ,,,1791.0,"""The models
are trained on 64 V100 GPUs for 200K steps with
batch size of 1024 and maximum sequence length
of 512, which takes about 2.5 days for GLMLarge.""
GLMLarge - 335M parameters
2.5 days * 24 hours/day * 10000/335 = 1791 hours",NVIDIA V100,,Likely,"There have been various types of pretraining architectures including autoencoding models (e.g., BERT), autoregressive models (e.g., GPT), and encoder-decoder models (e.g., T5). However, none of the pretraining frameworks performs the best for all tasks of three main categories including natural language understanding (NLU), unconditional generation, and conditional generation. We propose a General Language Model (GLM) based on autoregressive blank infilling to address this challenge. GLM improves blank filling pretraining by adding 2D positional encodings and allowing an arbitrary order to predict spans, which results in performance gains over BERT and T5 on NLU tasks. Meanwhile, GLM can be pretrained for different types of tasks by varying the number and lengths of blanks. On a wide range of tasks across NLU, conditional and unconditional generation, GLM outperforms BERT, T5, and GPT given the same model sizes and data, and achieves the best performance from a single pretrained model with 1.25x parameters of BERT Large , demonstrating its generalizability to different downstream tasks.",1.0,GLM-10B,Open weights (unrestricted),"China,China,United States of America,China",,,,64.0,,2025-04-07 20:37,,,,524288.0,"""The models are trained on 64 V100 GPUs for 200K steps with
batch size of 1024 and maximum sequence length
of 512""","Academia,Academia,Academia",,,,,Open source,Apache 2.0 or MIT for code/weights: https://github.com/THUDM/GLM,"Academia,Academia,Academia",,,,38918.19281791449,"Hardware,Operation counting",,,,
ResNet-RS,Vision,Image classification,"Google Brain,University of California (UC) Berkeley","Irwan Bello, William Fedus, Xianzhi Du, Ekin D. Cubuk, Aravind Srinivas, Tsung-Yi Lin, Jonathon Shlens, Barret Zoph",2021-03-13,Revisiting ResNets: Improved Training and Scaling Strategies,https://arxiv.org/abs/2103.07579,274.0,,,192000000.0,Table 7 appendix B,1.763328e+22,"(350) * (128000000000) * (1312 * 10**5) * 3 = 17633280000000000000000
(epochs) * (inference FLOP) * (dataset size) * (constant to account for backpropagation)
from 4.2 ""Our training method closely matches that of EfficientNet, where we train for 350 epochs, but with a few small differences""350 epochs from description of Table 8 in appendix C",,"""In a large-scale semi-supervised learning setup, ResNet-RS obtains a 4.7x training speed-up on TPUs (5.5x on GPUs) over EfficientNet-B5 when co-trained on ImageNet and an additional 130M pseudo-labeled images.""
""We train ResNets-RS on the combination of 1.2M labeled ImageNet images and 130M pseudo-labeled images, in a similar fashion to Noisy Studen""
 ""We use the same dataset of 130M images pseudo-labeled as Noisy Student, """,131200000.0,"1.2M + 130M = 131.2M 
""In a large-scale semi-supervised learning setup, ResNet-RS obtains a 4.7x training speed-up on TPUs (5.5x on GPUs) over EfficientNet-B5 when co-trained on ImageNet and an additional 130M pseudo-labeled images.""""We train ResNets-RS on the combination of 1.2M labeled ImageNet images and 130M pseudo-labeled images, in a similar fashion to Noisy Studen""
 ""We use the same dataset of 130M images pseudo-labeled as Noisy Student""",,,Google TPU v3,,Confident,"Novel computer vision architectures monopolize the spotlight, but the impact of the model architecture is often conflated with simultaneous changes to training methodology and scaling strategies. Our work revisits the canonical ResNet (He et al., 2015) and studies these three aspects in an effort to disentangle them. Perhaps surprisingly, we find that training and scaling strategies may matter more than architectural changes, and further, that the resulting ResNets match recent state-of-the-art models. We show that the best performing scaling strategy depends on the training regime and offer two new scaling strategies: (1) scale model depth in regimes where overfitting can occur (width scaling is preferable otherwise); (2) increase image resolution more slowly than previously recommended (Tan & Le, 2019). Using improved training and scaling strategies, we design a family of ResNet architectures, ResNet-RS, which are 1.7x - 2.7x faster than EfficientNets on TPUs, while achieving similar accuracies on ImageNet. In a large-scale semi-supervised learning setup, ResNet-RS achieves 86.2% top-1 ImageNet accuracy, while being 4.7x faster than EfficientNet NoisyStudent. The training techniques improve transfer performance on a suite of downstream tasks (rivaling state-of-the-art self-supervised algorithms) and extend to video classification on Kinetics-400. We recommend practitioners use these simple revised ResNets as baselines for future research. ",350.0,,Open weights (unrestricted),"United States of America,United States of America",,,,,,2025-02-17 12:01,,,,,,"Industry,Academia",,,,,Unreleased,"apache 2.0: 
https://github.com/tensorflow/tpu/tree/master/models/official/resnet/resnet_rs/configs","Industry,Academia",,,,,Operation counting,,,,
AraELECTRA,Language,"Language modeling/generation,Question answering",American University of Beirut,"Wissam Antoun, Fady Baly, Hazem Hajj",2021-03-07,AraELECTRA: Pre-Training Text Discriminators for Arabic Language Understanding,https://arxiv.org/abs/2012.15516,,,,136000000.0,"12 encoder layers, 12 attention heads, 768 hidden size, and 512 maximum input sequence length for a total of 136M
parameters. ",2.5587079e+20,"6 FLOP / parameter / token * 262144000000 tokens * 136000000 parameters = 2.139095e+20 FLOP

A TPUv3-8 has 8 cores. TPUv3 has 2 cores per chip. So 4 chips.

123000000000000 FLOP / chip / sec * 4 chips * 576 hours [see training time notes] * 3600 sec / hour * 0.3 [assumed utilization] = 3.0606336e+20 FLOP

sqrt(2.139095e+20*3.0606336e+20) = 2.5587079e+20 FLOP","OSCAR,Arabic Wikipedia,OSIAN corpus,Arabic Corpus","â€¢ The unshuffled OSCAR corpus (Ortiz Suarez Â´ et al., 2020).
â€¢ The Arabic Wikipedia dump from September 2020.
â€¢ The 1.5B words Arabic Corpus (El-Khair, 2016).
â€¢ The OSIAN corpus (Zeroual et al., 2019).
â€¢ News articles provided by As-safir newspaper.",262144000000.0,"pre-training: The model was pretrained for 2 million steps with a batch size of 256.

sequence length: 512

2*10^6*512 *256 = 262144000000 total training tokens",576.0,"""Pre-training took 24 days to finish on a TPUv3-8 slice""

24 days = 576 hours",Google TPU v3,,Confident,"Advances in English language representation enabled a more sample-efficient pre-training task by Efficiently Learning an Encoder that Classifies Token Replacements Accurately (ELECTRA). Which, instead of training a model to recover masked tokens, it trains a discriminator model to distinguish true input tokens from corrupted tokens that were replaced by a generator network. On the other hand, current Arabic language representation approaches rely only on pretraining via masked language modeling. In this paper, we develop an Arabic language representation model, which we name AraELECTRA. Our model is pretrained using the replaced token detection objective on large Arabic text corpora. We evaluate our model on multiple Arabic NLP tasks, including reading comprehension, sentiment analysis, and named-entity recognition and we show that AraELECTRA outperforms current state-of-the-art Arabic language representation models, given the same pretraining data and with even a smaller model size.",,,Open weights (unrestricted),Lebanon,,,,8.0,,2025-05-01 13:47,,,,,,Academia,,,,,Open source,https://huggingface.co/aubmindlab/araelectra-base-discriminator,Academia,,,,3568.3750214042025,"Operation counting,Hardware",aubmindlab,,,
M6-T,"Multimodal,Language,Vision","Chat,Image captioning",Alibaba,"An Yang, Junyang Lin, Rui Men, Chang Zhou, Le Jiang, Xianyan Jia, Ang Wang, Jie Zhang, Jiamang Wang, Yong Li, Di Zhang, Wei Lin, Lin Qu, Jingren Zhou, Hongxia Yang",2021-03-05,M6-T: Exploring Sparse Expert Models and Beyond,https://arxiv.org/abs/2105.15082,76.0,SOTA improvement,"Improves on hardware SOTA for similar problems

Abstract: 
""We push the model
scale to over 1 trillion parameters and implement it on solely 480 NVIDIA V100-32GB GPUs, in comparison with the recent SOTAs [11; 6] on 2048 TPU cores.""",1002700000000.0,Table 5. Note model is sparse MoE with 960 experts; not all parameters are activated on the forward pass.,5.5e+21,Estimate taken from https://www.governance.ai/research-paper/recent-trends-chinas-llm-landscape,M6-Corpus,M6-Corpus is a Chinese language multimodal dataset with 60.5B images and 111.8B tokens of text,1900000000000.0,60.5B images and 111.8B tokens of text,,,NVIDIA Tesla V100 DGXS 32 GB,Self-supervised learning,Likely,"Mixture-of-Experts (MoE) models can achieve promising results with outrageous large amount of parameters but constant computation cost, and thus it has become a trend in model scaling. Still it is a mystery how MoE layers bring quality gains by leveraging the parameters with sparse activation. In this work, we investigate several key factors in sparse expert models. We observe that load imbalance may not be a significant problem affecting model quality, contrary to the perspectives of recent studies, while the number of sparsely activated experts k and expert capacity C in top-k routing can significantly make a difference in this context. Furthermore, we take a step forward to propose a simple method called expert prototyping that splits experts into different prototypes and applies k top-1 routing. This strategy improves the model quality but maintains constant computational costs, and our further exploration on extremely large-scale models reflects that it is more effective in training larger models. We push the model scale to over 1 trillion parameters and implement it on solely 480 NVIDIA V100-32GB GPUs, in comparison with the recent SOTAs on 2048 TPU cores. The proposed giant model achieves substantial speedup in convergence over the same-size baseline.",,,Unreleased,China,,,,480.0,,2025-05-28 16:13,,,,,,Industry,,,,,Unreleased,,Industry,$13156.86,,FP16,243309.1333554511,Third-party estimation,,,,
Generative BST,Language,Language modeling/generation,Facebook AI Research,"Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Kurt Shuster, Eric M. Smith, Y-Lan Boureau, Jason Weston",2021-03-05,Recipes for building an open-domain chatbot,https://arxiv.org/abs/2004.13637,932.0,SOTA improvement,"Abstract:
""Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.""",9431810048.0,The largest model is a transformer with 9.4B parameters (Table 2),1.449e+22,"""Both our 2.7B and 9.4B parameter models were trained with batches of approximately 500k label BPE tokens per batch [...] The 9.4B parameter model was trained [...] for a total of 200k SGD steps.""

Also note that the full dataset contains 56.8B label BPE tokens and 88.8B context tokens, so for each batch of 500k label tokens, there are likely 500k * 88.8B / 56.8B = 780k context tokens.

6 * 9.4318B * 200k * (500k + 780k) = 1.449e22",,"Section 6. Pre-training is done on Pushshift.io Reddit: ""Our final dataset contains 1.50B comments totaling 56.8B label BPE tokens and 88.8B context tokens.""

They finetune on a few smaller datasets: ConvAI2 (140k utterances), Empathetic Dialogues (50k utterances), Wizard of Wikipedia (194k utterances), Blended Skill Talk (76k utterances, generated by training language models on the previous three datasets and selecting the best outputs from each)",56800000000.0,"Section 6. Pre-training is done on Pushshift.io Reddit: ""Our final dataset contains 1.50B comments totaling 56.8B label BPE tokens and 88.8B context tokens.""
None of the fine-tuning datasets put a significant dent in the total dataset size.

Epochs: they do 200k steps, where each batch has 500k label tokens = 100B label tokens seen. 56.8B label tokens in pre-training dataset, so 1.76 epochs",,,,,Confident,"Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, and displaying knowledge, empathy and personality appropriately, while maintaining a consistent persona. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.",1.76,,Open weights (unrestricted),United States of America,,,,,,2025-06-06 10:48,,,,,,Industry,,,,,Open source,"MIT license

https://github.com/facebookresearch/ParlAI
https://parl.ai/projects/recipes/",Industry,,,FP16,,Operation counting,,,,19
ProteinGAN,Biology,"Proteins,Protein generation","Vilnius University,Chalmers University of Technology","Donatas Repecka, Vykintas Jauniskis, Laurynas Karpus, Elzbieta Rembeza, Irmantas Rokaitis, Jan Zrimec, Simona Poviloniene, Audrius Laurynenas, Sandra Viknander, Wissam Abuajwa, Otto Savolainen, Rolandas Meskys, Martin K. M. Engqvist & Aleksej Zelezniak",2021-03-04,Expanding functional protein sequence spaces using generative adversarial networks,https://www.nature.com/articles/s42256-021-00310-5,,,,60000000.0,"""The final architecture of the network comprised 45 layers with over 60 million trainable parameters""",4.3e+18,"2.5 million steps, batch size 64, 210 hours on NVIDIA Tesla P100 system",UniProtKB,,5330001.0,"16,706 Ã— 319 = 5,329,214 tokens

Total datapoints = 5.33 million tokens",210.0,,NVIDIA P100,Unsupervised,Confident,"De novo protein design for catalysis of any desired chemical reaction is a long-standing goal in protein engineering because of the broad spectrum of technological, scientific and medical applications. However, mapping protein sequence to protein function is currently neither computationally nor experimentally tangible. Here, we develop ProteinGAN, a self-attention-based variant of the generative adversarial network that is able to â€˜learnâ€™ natural protein sequence diversity and enables the generation of functional protein sequences. ProteinGAN learns the evolutionary relationships of protein sequences directly from the complex multidimensional amino-acid sequence space and creates new, highly diverse sequence variants with natural-like physical properties. Using malate dehydrogenase (MDH) as a template enzyme, we show that 24% (13 out of 55 tested) of the ProteinGAN-generated and experimentally tested sequences are soluble and display MDH catalytic activity in the tested conditions in vitro, including a highly mutated variant of 106 amino-acid substitutions. ProteinGAN therefore demonstrates the potential of artificial intelligence to rapidly generate highly diverse functional proteins within the allowed biological constraints of the sequence space.",,,,"Lithuania,Sweden",,,,1.0,,2025-05-09 11:32,,,,,,"Academia,Academia",,,,210.0,,,"Academia,Academia",,,,278.51940448398403,Hardware,,,,
Wu Dao - Wen Su,Biology,"Proteins,Protein or nucleotide language model (pLM/nLM),Protein folding prediction",Beijing Academy of Artificial Intelligence / BAAI,"Tang Jie, Lu Bai, Qiu Jiezhong, Xie Changyu, Xiao Yijia, Zeng Aohan, Li Ziang ",2021-03-01,China's GPT-3? BAAI Introduces Superscale Intelligence Model 'Wu Dao 1.0',https://medium.com/syncedreview/chinas-gpt-3-baai-introduces-superscale-intelligence-model-wu-dao-1-0-98a573fc4d70,,,,,,,,UniParc,,,,,,,Self-supervised learning,Unknown,"Wu Dao â€“ Wen Su, based on Google's BERT language model and trained on the 100-gigabyte UNIPARC database (as well as thousands of gene sequences), was designed for biomolecular structure prediction and protein folding tasks.

Wu Dao â€“ Wen Suâ€™s open-sourced FastMoE is the first high-performance MoE (Mixed Expert Model) system that supports the PyTorch framework and a variety of hardware. Only one line of code is required to complete the MoE transformation, and model training speed is increased by 47 times compared with the traditional PyTorch implementation.",,,Open weights (non-commercial),China,,,,,,2025-06-12 11:07,,,,,,Academia,,,,,,"articles (https://syncedreview.com/2021/03/23/chinas-gpt-3-baai-introduces-superscale-intelligence-model-wu-dao-1-0/) say it is an open source model

based on this presentation weights and codes could be open sourced but links are unreachable from the US:
https://keg.cs.tsinghua.edu.cn/jietang/publications/wudao-3.0-meta-en.pdf",Academia,,,,,,,,,
Meta Pseudo Labels,Vision,Image classification,"Google Brain,Google AI","Hieu Pham, Zihang Dai, Qizhe Xie, Minh-Thang Luong, and Quoc V. Le",2021-03-01,Meta pseudo labels,https://arxiv.org/abs/2003.10580,597.0,SOTA improvement,,480000000.0,"Table 4
 480M",4.79e+22,"From communication with author:

22671 TPU days on specific hardware.

Which hardware did you use and in which configuration?
2048 cores of TPU v3.

Precision: Mixed. bfloat16 for activations, float32 for weights and optimizer slots.

2048 TPUv3 cores means 1024 TPUv3 chips, and the spec is 123e12 FLOP/second per chip with bfloat16 precision (Source: https://cloud.google.com/tpu/docs/system-architecture-tpu-vm)

So the compute estimate is:
1024 chips * 123e12 FLOP/second * 0.4 utilization * 11 days * 24 * 60 * 60 = 4.788191232e+22 FLOP","ImageNet,JFT-300M",,130000000.0,"Section 4
Datasets. For this experiment, we use the entire ImageNet
training set as labeled data, and use the JFT dataset as unlabeled data. The JFT dataset has 300 million images, and
then is filtered down to 130 million images by Noisy Student
using confidence thresholds and up-sampling [77]. We use
the same 130 million images as Noisy Student",264.0,"11 days from section 4:
""We train the model for 1 million steps in total,
which takes about 11 days for EfficientNet-L2 and 10 days
for EfficientNet-B6-Wide. ""

""Specifically, our training process runs on a cluster of 2,048
TPUv3 cores. ""
",Google TPU v3,Self-supervised learning,Likely,"We present Meta Pseudo Labels, a semi-supervised learning method that achieves a new state-of-the-art top-1 accuracy of 90.2% on ImageNet, which is 1.6% better than the existing state-of-the-art. Like Pseudo Labels, Meta Pseudo Labels has a teacher network to generate pseudo labels on unlabeled data to teach a student network. However, unlike Pseudo Labels where the teacher is fixed, the teacher in Meta Pseudo Labels is constantly adapted by the feedback of the student's performance on the labeled dataset. As a result, the teacher generates better pseudo labels to teach the student. Our code will be available at this https URL.",,,Unreleased,"United States of America,Multinational,United States of America",,,,1024.0,,2025-06-18 12:04,,,,,,"Industry,Industry",,,,270336.0,Open source,"Apache-2.0 license
https://github.com/google-research/google-research/blob/master/meta_pseudo_labels/README.md","Industry,Industry",$53844.28,,,456813.0362967626,Hardware,,,,9
SRU++ Large,Language,Language modeling,ASAPP,Tao Lei,2021-02-24,When Attention Meets Fast Recurrence: Training Language Models with Reduced Compute,https://arxiv.org/abs/2102.12459,43.0,SOTA improvement,"""our model achieves a state-of-the-art result on the ENWIK8 dataset using 1.6 days of training on an 8-GPU machine. """,234000000.0,Table 5,2.1173704e+19,"6 FLOP / token / parameter * 234000000 parameters * 1024 tokens per sample * 8*8 samples per batch * 400000 steps = 3.6805018e+19 FLOP

31330000000000 FLOP / sec * 360 GPU-hours * 3600 sec / hour * 0.3 [assumed utilization] = 1.2181104e+19 FLOP

sqrt(3.6805018e+19*1.2181104e+19) = 2.1173704e+19 

",WikiText-103,,103000000.0,"Table 12:
400K training steps
batch size: 8*8 = 64
sequence length = 1024

1024*8*8*400000/103000000 = 254.5 epochs",, 15â€  GPU-days = 360 GPU-hours,NVIDIA V100,,Confident,"Large language models have become increasingly difficult to train because of the growing computation time and cost. In this work, we present SRU++, a highly-efficient architecture that combines fast recurrence and attention for sequence modeling. SRU++ exhibits strong modeling capacity and training efficiency. On standard language modeling tasks such as Enwik8, Wiki-103 and Billion Word datasets, our model obtains better bits-per-character and perplexity while using 3x-10x less training cost compared to top-performing Transformer models. For instance, our model achieves a state-of-the-art result on the Enwik8 dataset using 1.6 days of training on an 8-GPU machine. We further demonstrate that SRU++ requires minimal attention for near state-of-the-art performance. Our results suggest jointly leveraging fast recurrence with little attention as a promising direction for accelerating model training and inference.",254.5,SRU++ Large,Open weights (unrestricted),United States of America,,,,,,2025-05-28 16:13,,,,65536.0,1024*8*8,Industry,,,,360.0,Open source,"MIT license repo. says models available as package: https://github.com/asappresearch/sru

training: https://github.com/asappresearch/sru/blob/master/language_model/train_lm.py ",Industry,,,FP16,,"Operation counting,Hardware",,,,
Linear Transformer (large),Language,"Language modeling,Translation",IDSIA,"Imanol Schlag, Kazuki Irie, JÃ¼rgen Schmidhuber",2021-02-22,Linear Transformers Are Secretly Fast Weight Programmers,https://arxiv.org/abs/2102.11174,176.0,,,90000000.0,90M (medium),3.89e+18,6 FLOP / token / parameter * 90000000 parameters * 103000000 tokens * 70 epochs = 3.8934e+18 FLOP,WikiText-103,,103000000.0,"batch size: 56 sequences
70 epochs
target segment lengths of 384",,,,,Confident,"We show the formal equivalence of linearised self-attention mechanisms and fast weight controllers from the early '90s, where a ``slow"" neural net learns by gradient descent to program the ``fast weights"" of another net through sequences of elementary programming instructions which are additive outer products of self-invented activation patterns (today called keys and values). Such Fast Weight Programmers (FWPs) learn to manipulate the contents of a finite memory and dynamically interact with it. We infer a memory capacity limitation of recent linearised softmax attention variants, and replace the purely additive outer products by a delta rule-like programming instruction, such that the FWP can more easily learn to correct the current mapping from keys to values. The FWP also learns to compute dynamically changing learning rates. We also propose a new kernel function to linearise attention which balances simplicity and effectiveness. We conduct experiments on synthetic retrieval problems as well as standard machine translation and language modelling tasks which demonstrate the benefits of our methods.",70.0,Linear Transformer (large),Unreleased,Switzerland,,,,,,2025-04-08 09:55,,,,,,Academia,,,,,Open source,"code, Apache license: https://github.com/IDSIA/lmtool-fwp/tree/master/example_scripts/2021_linear_transformers_are_secretly_fwps ",Academia,,,,,Operation counting,,,,
MSA Transformer,Biology,"Proteins,Protein or nucleotide language model (pLM/nLM)","Facebook AI Research,University of California (UC) Berkeley,New York University (NYU)","Roshan Rao, Jason Liu, Robert Verkuil, Joshua Meier, John F. Canny, Pieter Abbeel, Tom Sercu, Alexander Rives",2021-02-13,MSA Transformer,https://proceedings.mlr.press/v139/rao21a/rao21a.pdf,433.0,SOTA improvement,"""The performance of the model surpasses current state-of-the-art unsupervised structure learning methods by a wide margin, with far greater parameter efficiency than prior state-of-the-art protein language models""",100000000.0,"""We train an MSA Transformer model with 100M parameters..."" ",5.49e+21,"Based on: https://docs.google.com/spreadsheets/d/1enan21dFx03TkwufHgOwTVNBtuYlqNY9uurjIK6YS-8/edit#gid=0

Number of steps 4.5e5, batch size (tokens) 6.1e7, parameters 1e8

Calculation = 4e8 FLOP/bp * 4.5e5 bp + 2e8 FLOP/fp * 2.75e13 fp

Batch size: 512
Seq length: 100 * 1192 tokens
All models are trained on 32 V100 GPUs for 100k updates. The four models with best contact precision are then further trained to 150k updates. Finally, the best model at 150k updates is trained to 450k updates.

450k * 512 * 100 * 1192 * 100M * 6 = 1.65e22","UniRef50,UniRef30 (FKA UniClust30)","""Models are trained on a dataset of 26 million MSAs. An MSA is generated for each UniRef50 sequence by searching UniClost30 with HHblits.""",9297600000000.0,"""We train an MSA Transformer model with 100M parameters on a large dataset (4.3 TB) of 26 million MSAs, with an average of 1192 sequences per MSA.""
Average sequence is ~300 amino acids/tokens long.
26 million * 1192 * 300 = 9.3T tokens",,,NVIDIA Tesla V100 DGXS 32 GB,,Likely,"Unsupervised protein language models trained across millions of diverse sequences learn structure and function of proteins. Protein language models studied to date have been trained to perform inference from individual sequences. The longstanding approach in computational biology has been to make inferences from a family of evolutionarily related sequences by fitting a model to each family independently. In this work we combine the two paradigms. We introduce a protein language model which takes as input a set of sequences in the form of a multiple sequence alignment. The model interleaves row and column attention across the input sequences and is trained with a variant of the masked language modeling objective across many protein families. The performance of the model surpasses current state-of-the-art unsupervised structure learning methods by a wide margin, with far greater parameter efficiency than prior state-of-the-art protein language models. ",,,Open weights (unrestricted),"United States of America,United States of America,United States of America",,,,32.0,,2025-04-30 10:04,,,,,,"Industry,Academia,Academia",,,,,Unreleased,"MIT: https://github.com/facebookresearch/esm

looks like no training code","Industry,Academia,Academia",$13256.94,,,16227.834954676886,Operation counting,,,,
top-down frozen classifier,Speech,Speech recognition,"University of Edinburgh,Toshiba Cambridge Research Laboratory","Shucong Zhang, Cong-Thanh Do, Rama Doddipatla, Erfan Loweimi, Peter Bell, Steve Renals",2021-02-09,Train your classifier first: Cascade Neural Networks Training from upper layers to lower layers,https://arxiv.org/abs/2102.04697,2.0,SOTA improvement,"""Table 2 demonstrates that, to the best of our knowledge, top-down training results in state-of-the art character error rates for LSTM-based endto-end models on WSJ""",,,,,WSJ,,,,,,,,Unknown,"Although the lower layers of a deep neural network learn features which are transferable across datasets, these layers are not transferable within the same dataset. That is, in general, freezing the trained feature extractor (the lower layers) and retraining the classifier (the upper layers) on the same dataset leads to worse performance. In this paper, for the first time, we show that the frozen classifier is transferable within the same dataset. We develop a novel top-down training method which can be viewed as an algorithm for searching for high-quality classifiers. We tested this method on automatic speech recognition (ASR) tasks and language modelling tasks. The proposed method consistently improves recurrent neural network ASR models on Wall Street Journal, self-attention ASR models on Switchboard, and AWD-LSTM language models on WikiText-2.",,top-down frozen classifier,Unreleased,"United Kingdom of Great Britain and Northern Ireland,United Kingdom of Great Britain and Northern Ireland",,,,,,2024-10-15 11:02,,,,,,"Academia,Industry",,,,,Unreleased,,"Academia,Industry",,,,,,,,,
Selfish-RNN (SNT-ASGD) Stacked LSTMs,Language,Language modeling,"Eindhoven University of Technology,University of Twente","Shiwei Liu, Decebal Constantin Mocanu, Yulong Pei, Mykola Pechenizkiy",2021-01-22,Selfish Sparse RNN Training,https://arxiv.org/abs/2101.09048,39.0,,"SOTA for sparse networks, but presumably not in general",25200000.0,25.2M (Table 2),1.178e+16,"Table 10:
3.1e16 FLOP * 0.38 = 1.178e+16 FLOP

6 FLOP/token/parameter * 25200000 parameters * 912344 tokens * 100 epochs = 1.3794641e+16 FLOP",Penn TreeBank (PTB),,912344.0,,,,,,Confident,"Sparse neural networks have been widely applied to reduce the computational demands of training and deploying over-parameterized deep neural networks. For inference acceleration, methods that discover a sparse network from a pre-trained dense network (dense-to-sparse training) work effectively. Recently, dynamic sparse training (DST) has been proposed to train sparse neural networks without pre-training a dense model (sparse-to-sparse training), so that the training process can also be accelerated. However, previous sparse-to-sparse methods mainly focus on Multilayer Perceptron Networks (MLPs) and Convolutional Neural Networks (CNNs), failing to match the performance of dense-to-sparse methods in the Recurrent Neural Networks (RNNs) setting. In this paper, we propose an approach to train intrinsically sparse RNNs with a fixed parameter count in one single run, without compromising performance. During training, we allow RNN layers to have a non-uniform redistribution across cell gates for better regularization. Further, we propose SNT-ASGD, a novel variant of the averaged stochastic gradient optimizer, which significantly improves the performance of all sparse training methods for RNNs. Using these strategies, we achieve state-of-the-art sparse training results, better than the dense-to-sparse methods, with various types of RNNs on Penn TreeBank and Wikitext-2 datasets. Our codes are available at this https URL.",100.0,Selfish-RNN (SNT-ASGD) Stacked LSTMs,Open weights (non-commercial),"Netherlands,Netherlands",,,,,,2025-04-08 09:26,,,,,,"Academia,Academia",,,,,Open (non-commercial),code and weights (stacked LSTM). no clear license https://github.com/Shiweiliuiiiiiii/Selfish-RNN ,"Academia,Academia",,,,,"Reported,Operation counting",,,,
DeiT-B,Vision,Image classification,"Meta AI,Sorbonne University","Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, HervÃ© JÃ©gou",2021-01-15,Training data-efficient image transformers & distillation through attention,https://arxiv.org/abs/2012.12877,5607.0,Highly cited,,86000000.0,(DeiT-B),7.884e+19,"2*86000000 parameters*3*1280000 training examples*300 epochs=1.98144e+17 FLOPs

compute [FLOP] = training time [s] Ã— # of GPUs/TPUs Ã— peak FLOP/s Ã— utilization rate

(53h+20h)*3600*8*125000000000000 peak FLOP/s*0.3=7.884e+19

",ImageNet,,1280000.0,,53.0,"A typical training of 300 epochs takes 37 hours with 2 nodes or 53 hours on a single node for the DeiT-B.
In this paper, we train a vision transformer on a single 8-GPU node in two
to three days (53 hours of pre-training, and optionally 20 hours of fine-tuning) that is competitive with convnets having a similar number of parameters and efficiency. It uses Imagenet as the sole training set.",NVIDIA V100,,Confident,"Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. However, these visual transformers are pre-trained with hundreds of millions of images using an expensive infrastructure, thereby limiting their adoption.
In this work, we produce a competitive convolution-free transformer by training on Imagenet only. We train them on a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1% (single-crop evaluation) on ImageNet with no external data.
More importantly, we introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention. We show the interest of this token-based distillation, especially when using a convnet as a teacher. This leads us to report results competitive with convnets for both Imagenet (where we obtain up to 85.2% accuracy) and when transferring to other tasks. We share our code and models.",300.0,,Open weights (unrestricted),"United States of America,France",,,,,,2024-11-26 09:59,,,,,,"Industry,Academia",,,,,Open source,"models, train, inference: https://github.com/facebookresearch/deit/blob/main/README_deit.md 

Apache-2.0 license","Industry,Academia",,,,,Hardware,,,,
Switch,Language,Text autocompletion,Google,"William Fedus, Barret Zoph, Noam Shazeer",2021-01-11,Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity,https://arxiv.org/abs/2101.03961,1628.0,"Highly cited,SOTA improvement",""" On ANLI (Nie et al., 2019), Switch XXL improves over the prior state-of-the-art to get a 65.7 accuracy versus the prior best of 49.4 (Yang et al., 2020)... Finally, we also conduct an early examination of the modelâ€™s knowledge with three closed-book knowledge-based tasks: Natural
Questions, WebQuestions and TriviaQA, without additional pre-training using Salient Span Masking (Guu et al., 2020). In all three cases, we observe improvements over the prior stateof-the-art T5-XXL model (without SSM)""",1571000000000.0,"""Combining expert, model and data parallelism, we design two large Switch Transformer models, one with 395 billion and 1.6 trillion parameters""
Table 9 gives more precise count of 1571B parameters",8.22e+22,"Table 4
https://arxiv.org/ftp/arxiv/papers/2104/2104.10350.pdf",C4,,576000000000.0,"""In our protocol we pre-train with 2^20 (1,048,576) tokens
per batch for 550k steps amounting to 576B total tokens.""

1 token ~ 0.75 words",648.0,"see table 4 in https://arxiv.org/ftp/arxiv/papers/2104/2104.10350.pdf
",Google TPU v3,Self-supervised learning,Confident,"In deep learning, models typically reuse the same parameters for all inputs. Mixture of Experts (MoE) defies this and instead selects different parameters for each incoming example. The result is a sparsely-activated model -- with outrageous numbers of parameters -- but a constant computational cost. However, despite several notable successes of MoE, widespread adoption has been hindered by complexity, communication costs and training instability -- we address these with the Switch Transformer. We simplify the MoE routing algorithm and design intuitive improved models with reduced communication and computational costs. Our proposed training techniques help wrangle the instabilities and we show large sparse models may be trained, for the first time, with lower precision (bfloat16) formats. We design models based off T5-Base and T5-Large to obtain up to 7x increases in pre-training speed with the same computational resources. These improvements extend into multilingual settings where we measure gains over the mT5-Base version across all 101 languages. Finally, we advance the current scale of language models by pre-training up to trillion parameter models on the ""Colossal Clean Crawled Corpus"" and achieve a 4x speedup over the T5-XXL model.
",,,Open weights (unrestricted),United States of America,,,,1024.0,0.2797,2025-06-18 12:02,,,,,,Industry,,,,663552.0,Unreleased,"Apache 2 for weights: https://huggingface.co/google/switch-c-2048 

paper links to this repo but not clear that the training hyperparams for Switch are here:
https://github.com/google-research/t5x

",Industry,$139663.56,"Table 4 in https://arxiv.org/pdf/2104.10350 gives measured performance of 34.4 TFLOP/s, vs. peak achievable FLOP/s of 123 TFLOP/s on the TPUv3 being used.
34.4/123 = 0.27967",BF16,457311.78237926256,Third-party estimation,,,,5
DALL-E,Image generation,"Text-to-image,Image generation",OpenAI,"Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, Ilya Sutskever",2021-01-05,Zero-Shot Text-to-Image Generation,"https://openai.com/blog/dall-e/

https://arxiv.org/abs/2102.12092",4003.0,"Significant use,Highly cited",,12000000000.0,DALLÂ·E is a 12-billion parameter version of GPT-3 trained to generate images from text descriptions,4.7e+22,"source: https://lair.lighton.ai/akronomicon/

archived: https://github.com/lightonai/akronomicon/tree/main/akrodb

VAE training ""on 64 16 GB NVIDIA V100 GPUs, with a per-GPU batch size of 8, resulting in a total batch size of 512. It is trained for a total of 3,000,000 updates.""

Transformer training: ""We trained the model using 1024, 16 GB NVIDIA V100 GPUs and a total batch size of 1024, for a total of 430,000 updates.""; ""We concatenate up to 256 BPE-encoded text tokens with the 32 Ã— 32 = 1024 image tokens""
Total tokens: 430000 steps * 1024 batch size * 1280 sequence length = 563609600000

Transformer FLOP: 6 * 12B parameter * 563609600000 tokens = 4.057989e+22

Estimating the VAE at ~15% seems reasonable",DALL-E,"To scale up to 12-billion parameters, we created a dataset of
a similar scale to JFT-300M (Sun et al., 2017) by collecting
250 million text-images pairs from the internet. This dataset
does not include MS-COCO, but does include Conceptual
Captions and a filtered subset of YFCC100M (Thomee et al.,
2016). As MS-COCO was created from the latter, our training data includes a fraction of the MS-COCO validation images (but none of the captions).",250000000.0,"""To scale up to 12-billion parameters, we created a dataset of a similar scale to JFT-300M (Sun et al., 2017) by collecting
250 million text-images pairs from the internet. ""

number of epochs: 
1024 batch size * 430,000 updates / 250,000,000 = 1.76",,"""We trained the model using 1024, 16 GB NVIDIA V100 GPUs and a total batch size of 1024, for a total of 430,000 updates.
At the start of training, we use a linear schedule to ramp up the step size to 4.5 Â· 10âˆ’4 over 5000 updates, and halved the
step size each time the training loss appeared to plateau. We did this a total of five times, ending training with a final step
size that was 32 times smaller than the initial one. """,NVIDIA Tesla V100 DGXS 16 GB,Self-supervised learning,Likely,"Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.",1.76,,API access,United States of America,,,,1024.0,,2025-06-18 12:00,,,,,,Industry,,,,,Unreleased,,Industry,$118437.36,,FP16,519741.92129196384,Third-party estimation,,,,9
CLIP (ViT L/14@336px),"Multimodal,Vision,Language,Video","Zero-shot image classification,Character recognition,Video description",OpenAI,"Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever",2021-01-05,Learning Transferable Visual Models From Natural Language Supervision,https://arxiv.org/abs/2103.00020,20331.0,"Highly cited,SOTA improvement",,370000000.0,"Image encoder
Vision Transformer
Table 1 in https://arxiv.org/pdf/2010.11929.pdf
Authors fine-tuned ViT L/14 at additional 336px resolution, hence the @336 (See ViT)
307M params

Text encoder
~Transformer (from paper)
63M params",1.05e+22,"https://docs.google.com/document/d/156miAJkFN9DDX06C3s03UDsretCtymCKiGDddLBCgQE/edit?usp=sharing

",Unspecified unreleased,"Custom image-text pairs from the internet

we constructed a new dataset of 400 million (image,
text) pairs collected form a variety of publicly available
sources on the Internet. To attempt to cover as broad a set
of visual concepts as possible, we search for (image, text)
pairs as part of the construction process whose text includes
one of a set of 500,000 queries",400000000.0,,288.0,"â€œThe largest ResNet model, RN50x64, took 18 days to train on 592 V100 GPUs while the largest Vision Transformer took 12 days on 256 V100 GPUsâ€",NVIDIA V100,Self-supervised learning,Confident,"State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at this https URL.",,,Open weights (unrestricted),United States of America,,,,256.0,,2025-06-18 11:48,,,,,,Industry,,,,,Unreleased,"MIT License

https://github.com/OpenAI/CLIP",Industry,$24638.52,,FP16,155922.57638758916,Third-party estimation,,,,18
Subformer (122M),Language,Language modeling,"National Institute of Advanced Industrial Science and Technology (AIST),The University of Tokyo","Machel Reid, Edison Marrese-Taylor, Yutaka Matsuo",2021-01-01,Subformer: Exploring Weight Sharing for Parameter Efficiency in Generative Transformers,https://arxiv.org/abs/2101.00234,45.0,,,122000000.0,"122M (Table 5)
",5.1450348e+18,6 FLOP/token/parameter * 122000000 parameters * 8 GPUs * 3072 tokens per batch per GPU * 286000 steps = 5.1450348e+18 FLOP,WikiText-103,,100000000.0,"""we use 8 GPUs with 3072 tokens per GPU and an update frequency of 3""
""warming up the learning rate to 1.0 for 16K iterations, and then annealing for 270K iterations""

8*3072*286000/100000000 = 70.29 epochs",,,NVIDIA V100,,Confident,"Transformers have shown improved performance when compared to previous architectures for sequence processing such as RNNs. Despite their sizeable performance gains, as recently suggested, the model is computationally expensive to train and with a high parameter budget. In light of this, we explore parameter-sharing methods in Transformers with a specific focus on generative models. We perform an analysis of different parameter sharing/reduction methods and develop the Subformer. Our model combines sandwich-style parameter sharing, which overcomes naive cross-layer parameter sharing in generative models, and self-attentive embedding factorization (SAFE). Experiments on machine translation, abstractive summarization and language modeling show that the Subformer can outperform the Transformer even when using significantly fewer parameters.",70.29,Subformer (122M),Unreleased,"Japan,Japan",,,,8.0,,2025-04-08 11:26,,,,24576.0,8*3072,"Academia,Academia",,,,,Unreleased,"code, but not for language modeling: https://github.com/machelreid/subformer/blob/master/README.md ","Academia,Academia",,,,4873.014568662182,Operation counting,,,,
AraGPT2-Mega,Language,"Language modeling/generation,Question answering",American University of Beirut,"Wissam Antoun, Fady Baly, Hazem Hajj",2020-12-31,AraGPT2: Pre-Trained Transformer for Arabic Language Generation,https://arxiv.org/abs/2012.15520,86.0,,,1500000000.0,"source: https://lair.lighton.ai/akronomicon/

archived: https://github.com/lightonai/akronomicon/tree/main/akrodb",2e+21,"source: https://github.com/lightonai/akronomicon/blob/10adaca9c74afa7d11f196947e410d248f25abe9/akrodb/American%20University%20of%20Beirut/AraGPT2-Mega.json

Akronomicon uses units of petaflop/s-days. 20 petaflop/s-days ~= 2e21 FLOP.

Our own validation of this estimate is below.

For the Mega model: 9 days on a TPUv3-128, bfloat16 precision  (from author communication)

A TPUv3-128 has 128 cores (you can infer this from footnote 9 on p.4 of the paper - 128 * 16GB = 2TB). TPUv3 has 2 cores per chip. So 64 chips.

TPUv3 FLOP/s: 1.23E+14

Utilization: use default value of 30% for Language domain (https://epoch.ai/blog/estimating-training-compute)

64 chips * 30% * 1.23E+14 FLOP/s * 9 days * 24h/day * 3600s/h
~= 2e21 FLOP

num of examples (seq len = 1024): 9.7M
batch size: 256
number of steps: 800k

6 FLOP / token / parameter * 1.46 * 10^9 parameters * 800000 steps * 256 sequences per step * 1024 tokens per step = 1.8371052e+21 FLOP
","OSCAR,Arabic Wikipedia,OSIAN corpus,Arabic Corpus","â€¢ The unshuffled OSCAR corpus (Ortiz Suarez Â´ et al., 2020).
â€¢ The Arabic Wikipedia dump from September 2020.
â€¢ The 1.5B words Arabic Corpus (El-Khair, 2016).
â€¢ The OSIAN corpus (Zeroual et al., 2019).
â€¢ News articles provided by As-safir newspaper.",9932800000.0,"""The total dataset size is 77GB with 8.8B words [word count was done after preprocessing, where a white
space is inserted before and after punctuations, brackets, numbers... which increased the total word count]""

num of examples (seq len = 1024): 9.7M
batch size: 256
number of steps: 800k

9.7*10^6 examples * 1024 tokens per sequence = 9932800000 tokens",216.0,"https://github.com/aub-mind/arabert?tab=readme-ov-file#AraGPT2

9 days = 216 hours",Google TPU v3,Self-supervised learning,Confident,"Recently, pre-trained transformer-based architectures have proven to be very efficient at language modeling and understanding, given that they are trained on a large enough corpus. Applications in language generation for Arabic are still lagging in comparison to other NLP advances primarily due to the lack of advanced Arabic language generation models. In this paper, we develop the first advanced Arabic language generation model, AraGPT2, trained from scratch on a large Arabic corpus of internet text and news articles. Our largest model, AraGPT2-mega, has 1.46 billion parameters, which makes it the largest Arabic language model available. The Mega model was evaluated and showed success on different tasks including synthetic news generation, and zero-shot question answering. For text generation, our best model achieves a perplexity of 29.8 on held-out Wikipedia articles. A study conducted with human evaluators showed the significant success of AraGPT2-mega in generating news articles that are difficult to distinguish from articles written by humans. We thus develop and release an automatic discriminator model with a 98% percent accuracy in detecting model-generated text. The models are also publicly available, hoping to encourage new research directions and applications for Arabic NLP.",20.0,,Open weights (unrestricted),Lebanon,,,,128.0,,2025-05-09 11:32,,,,,,Academia,,,,,Open source,"apache-like license:
https://github.com/aub-mind/arabert/blob/master/aragpt2/LICENSE

https://huggingface.co/aubmindlab/aragpt2-mega",Academia,,,BF16,57177.97757573645,"Third-party estimation,Hardware,Operation counting",aubmindlab,,,
Shortformer,Language,Language modeling,"University of Washington,Facebook AI Research,Allen Institute for AI","Ofir Press, Noah A. Smith, Mike Lewis",2020-12-31,Shortformer: Better Language Modeling using Shorter Inputs,https://arxiv.org/abs/2012.15832,81.0,,,247000000.0,247M (Table 5),3.04e+18,"6 FLOP / parameter / token * 247000000 parameters * 103000000 tokens * 205 epochs = 3.129243e+19 FLOP 

________
earlier here was a mistake (1 OOM) from when the calculation for the Algorithmic progress paper was done
previous estimation: 3.04e+18 FLOP",WikiText-103,,103000000.0,,,,,,Confident,"Increasing the input length has been a driver of progress in language modeling with transformers. We identify conditions where shorter inputs are not harmful, and achieve perplexity and efficiency improvements through two new methods that decrease input length. First, we show that initially training a model on short subsequences before moving on to longer ones both reduces overall training time and, surprisingly, substantially improves perplexity. Second, we show how to improve the efficiency of recurrence methods in transformers, which let models condition on previously processed tokens when generating sequences that exceed the maximal length the transformer can handle at once. Existing methods require computationally expensive relative position embeddings; we introduce a simple alternative of adding absolute position embeddings to queries and keys instead of to word embeddings, which efficiently produces superior results. We show that these recurrent models also benefit from short input lengths. Combining these techniques speeds up training by a factor of 1.65, reduces memory usage, and substantially improves perplexity on WikiText-103, without adding any parameters.",205.0,Shortformer,Unreleased,"United States of America,United States of America,United States of America",,,,,,2025-04-08 12:06,,,,,,"Academia,Industry,Research collective",,,,,Open source,"code, MIT: https://github.com/ofirpress/shortformer 
train: https://github.com/ofirpress/shortformer/blob/master/fairseq_cli/train.py ","Academia,Industry,Research collective",,,,,Operation counting,,,,
ERNIE-Doc (247M),Language,"Language modeling,Language modeling/generation",Baidu,"Siyu Ding, Junyuan Shang, Shuohuan Wang, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang",2020-12-31,ERNIE-Doc: A Retrospective Long-Document Modeling Transformer,https://arxiv.org/abs/2012.15688,47.0,SOTA improvement,"""ERNIE-DOC improved the state-of-the-art language modeling result of perplexity to 16.8 on WikiText103""",247000000.0,,3.0302798e+19,6 FLOP / parameter / token * 247000000 parameters * 416000 steps * 128 sequences per batch * 384 tokens per sequence = 3.0302798e+19 FLOP,WikiText-103,,,"Table 11:
sequence length 384
batch size 128
training steps 16000 + 400000 = 416000 

416000*128*384/103000000 = 198.5  epochs",,,,,Confident,"Transformers are not suited for processing long documents, due to their quadratically increasing memory and time consumption. Simply truncating a long document or applying the sparse attention mechanism will incur the context fragmentation problem or lead to an inferior modeling capability against comparable model sizes. In this paper, we propose ERNIE-Doc, a document-level language pretraining model based on Recurrence Transformers. Two well-designed techniques, namely the retrospective feed mechanism and the enhanced recurrence mechanism, enable ERNIE-Doc, which has a much longer effective context length, to capture the contextual information of a complete document. We pretrain ERNIE-Doc to explicitly learn the relationships among segments with an additional document-aware segment-reordering objective. Various experiments were conducted on both English and Chinese document-level tasks. ERNIE-Doc improved the state-of-the-art language modeling result of perplexity to 16.8 on WikiText-103. Moreover, it outperformed competitive pretraining models by a large margin on most language understanding tasks, such as text classification and question answering.",198.5,ERNIE-Doc (247M),Open weights (unrestricted),China,,,,,,2025-04-08 11:50,,,,,,Industry,,,,,Unreleased,"weights available, not sure there's training code for WT-103: https://github.com/PaddlePaddle/ERNIE/tree/repro/ernie-doc",Industry,,,,,Operation counting,,,,
CT-MoS (WT2),Language,Language modeling,"Google,National Tsing Hua University","Pei-Hsin Wang, Sheng-Iou Hsieh, Shih-Chieh Chang, Yu-Ting Chen, Jia-Yu Pan, Wei Wei, Da-Chang Juan",2020-12-25,Contextual Temperature for Language Modeling,https://arxiv.org/abs/2012.13575,17.0,SOTA improvement,"""Experimental results confirm that the
proposed method significantly improves state-of-the-art language models, achieving a perplexity of 55.31 and 62.89 on
the test set of Penn Treebank and WikiText-2""",45000000.0,"45M
Table 2",5.4e+17,6 FLOP / parameter / token * 45000000 parameters * 2000000 tokens * 1000 epochs = 5.4e+17 FLOP,WikiText-2,,2000000.0,,,,NVIDIA GeForce GTX 1080 Ti,,Confident,"Temperature scaling has been widely used as an effective approach to control the smoothness of a distribution, which helps the model performance in various tasks. Current practices to apply temperature scaling assume either a fixed, or a manually-crafted dynamically changing schedule. However, our studies indicate that the individual optimal trajectory for each class can change with the context. To this end, we propose contextual temperature, a generalized approach that learns an optimal temperature trajectory for each vocabulary over the context. Experimental results confirm that the proposed method significantly improves state-of-the-art language models, achieving a perplexity of 55.31 and 62.89 on the test set of Penn Treebank and WikiText-2, respectively. In-depth analyses show that the behaviour of the learned temperature schedules varies dramatically by vocabulary, and that the optimal schedules help in controlling the uncertainties. These evidences further justify the need for the proposed method and its advantages over fixed temperature schedules.",1000.0,CT-MoS (WT2),Unreleased,"United States of America,Taiwan",,,,4.0,,2025-04-08 12:19,,,,,,"Industry,Academia",,,,,Unreleased,,"Industry,Academia",,,,2030.739275278243,Operation counting,,,,
CT-MoS + DynamicEval (WT2),Language,Language modeling,"National Tsing Hua University,Google","Pei-Hsin Wang, Sheng-Iou Hsieh, Shih-Chieh Chang, Yu-Ting Chen, Jia-Yu Pan, Wei Wei, Da-Chang Juan",2020-12-25,Contextual Temperature for Language Modeling,https://arxiv.org/abs/2012.13575,17.0,,"other model in the paper (without DynamicEval) was better, per Table 2",45000000.0,,5.4e+17,6 FLOP / parameter / token * 45000000 parameters * 2000000 tokens * 1000 epochs = 5.4e+17 FLOP,WikiText-2,,2000000.0,,,,,,Confident,"Temperature scaling has been widely used as an effective approach to control the smoothness of a distribution, which helps the model performance in various tasks. Current practices to apply temperature scaling assume either a fixed, or a manually-crafted dynamically changing schedule. However, our studies indicate that the individual optimal trajectory for each class can change with the context. To this end, we propose contextual temperature, a generalized approach that learns an optimal temperature trajectory for each vocabulary over the context. Experimental results confirm that the proposed method significantly improves state-of-the-art language models, achieving a perplexity of 55.31 and 62.89 on the test set of Penn Treebank and WikiText-2, respectively. In-depth analyses show that the behaviour of the learned temperature schedules varies dramatically by vocabulary, and that the optimal schedules help in controlling the uncertainties. These evidences further justify the need for the proposed method and its advantages over fixed temperature schedules.",1000.0,CT-MoS + DynamicEval (WT2),Unreleased,"Taiwan,United States of America",,,,,,2025-04-08 12:40,,,,,,"Academia,Industry",,,,,Unreleased,,"Academia,Industry",,,,,Operation counting,,,,
DensePhrases,Language,Question answering,"Korea University,Princeton University","Jinhyuk Lee, Mujeen Sung, Jaewoo Kang, Danqi Chen",2020-12-23,Learning Dense Representations of Phrases at Scale,https://arxiv.org/abs/2012.12624v3,108.0,SOTA improvement,"from abstract ""our model DensePhrases improves over previous phrase retrieval models by 15%-25% absolute accuracy and matches the performance of state-of-the-art retriever-reader models. """,,may be possible to estimate from batch size (8) and maximum memory of GPUs (96GB),2.09952e+18," flops = (8) * (1215 * 10**10) * (20 * 3600) * 3 // 10 = 2099520000000000000
(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)

model of GPU from appendix B (Titan Xp)
number of GPUs from table in appendix A
flops from https://www.techpowerup.com/gpu-specs/titan-xp.c2948","SQuAD,NQ (Natural Questions)","from appendix D ""The number of generated questions is 327,302 and 1,126,354 for SQuAD and Natural Questions, respectively.""",58000000.0,"from appendix D ""The number of generated questions is 327,302 and 1,126,354 for SQuAD and Natural Questions, respectively.""
assuming 40 words per question we get around ~ 58M",20.0,appendix A row 3,NVIDIA TITAN Xp,,Speculative,"Open-domain question answering can be reformulated as a phrase retrieval problem, without the need for processing documents on-demand during inference (Seo et al., 2019). However, current phrase retrieval models heavily depend on sparse representations and still underperform retriever-reader approaches. In this work, we show for the first time that we can learn dense representations of phrases alone that achieve much stronger performance in open-domain QA. We present an effective method to learn phrase representations from the supervision of reading comprehension tasks, coupled with novel negative sampling methods. We also propose a query-side fine-tuning strategy, which can support transfer learning and reduce the discrepancy between training and inference. On five popular open-domain QA datasets, our model DensePhrases improves over previous phrase retrieval models by 15%-25% absolute accuracy and matches the performance of state-of-the-art retriever-reader models. Our model is easy to parallelize due to pure dense representations and processes more than 10 questions per second on CPUs. Finally, we directly use our pre-indexed dense phrase representations for two slot filling tasks, showing the promise of utilizing DensePhrases as a dense knowledge base for downstream tasks. ",4.0,,Open weights (unrestricted),"Korea (Republic of),United States of America",,,,8.0,,2025-02-17 14:50,,,,,,"Academia,Academia",,,,160.0,Open source,Apache 2.0: https://github.com/princeton-nlp/DensePhrases,"Academia,Academia",,,,4061.6594477324606,Hardware,,,,
RaSoR,Language,Question answering,"Korea University,Princeton University","Jinhyuk Lee, Mujeen Sung, Jaewoo Kang, Danqi Chen",2020-12-23,Learning Recurrent Span Representations for Extractive Question Answering,https://arxiv.org/abs/1611.01436,149.0,,,,May be estimated from author's repository https://github.com/shimisalant/RaSoR,,"May be estimated from author's repository https://github.com/shimisalant/RaSoR
citation from the paper about training machine ""All models are implemented using TensorFlow3 and trained on the SQUAD training set using the ADAM (Kingma & Ba, 2015) optimizer with a mini-batch size of 4 and trained using 10 asynchronous training threads on a single machine""",SQuAD,"from section 5 ""We train on the 80k (question, passage, answer span) triples in the SQUAD training set and report results on the 10k examples in the SQUAD development and test sets.""",5333333.0,"4000000 words in SQuAD * 4/3 tokens per word = 5,333,333 tokens",,,,,Confident,"Open-domain question answering can be reformulated as a phrase retrieval problem, without the need for processing documents on-demand during inference (Seo et al., 2019). However, current phrase retrieval models heavily depend on sparse representations and still underperform retriever-reader approaches. In this work, we show for the first time that we can learn dense representations of phrases alone that achieve much stronger performance in open-domain QA. We present an effective method to learn phrase representations from the supervision of reading comprehension tasks, coupled with novel negative sampling methods. We also propose a query-side fine-tuning strategy, which can support transfer learning and reduce the discrepancy between training and inference. On five popular open-domain QA datasets, our model DensePhrases improves over previous phrase retrieval models by 15%-25% absolute accuracy and matches the performance of state-of-the-art retriever-reader models. Our model is easy to parallelize due to pure dense representations and processes more than 10 questions per second on CPUs. Finally, we directly use our pre-indexed dense phrase representations for two slot filling tasks, showing the promise of utilizing DensePhrases as a dense knowledge base for downstream tasks.",,,,"Korea (Republic of),United States of America",,,,,,2024-10-01 10:00,,,,,,"Academia,Academia",,,,,,,"Academia,Academia",,,,,,,,,
VQGAN + CLIP,Image generation,Text-to-image,Heidelberg University,"Patrick Esser, Robin Rombach, BjÃ¶rn Ommer",2020-12-17,Taming Transformers for High-Resolution Image Synthesis,https://arxiv.org/abs/2012.09841,2200.0,"Highly cited,SOTA improvement",,,,,,,,,I'm confused - I guess they pretrained on several different datasets? I think the model is also able to do zero-shot learning,,,,,Unknown,"Designed to learn long-range interactions on sequential data, transformers continue to show state-of-the-art results on a wide variety of tasks. In contrast to CNNs, they contain no inductive bias that prioritizes local interactions. This makes them expressive, but also computationally infeasible for long sequences, such as high-resolution images. We demonstrate how combining the effectiveness of the inductive bias of CNNs with the expressivity of transformers enables them to model and thereby synthesize high-resolution images. We show how to (i) use CNNs to learn a context-rich vocabulary of image constituents, and in turn (ii) utilize transformers to efficiently model their composition within high-resolution images. Our approach is readily applied to conditional synthesis tasks, where both non-spatial information, such as object classes, and spatial information, such as segmentations, can control the generated image. In particular, we present the first results on semantically-guided synthesis of megapixel images with transformers and obtain the state of the art among autoregressive models on class-conditional ImageNet. Code and pretrained models can be found at this https URL .",,,Open weights (unrestricted),Germany,,,,,,2025-05-30 13:00,,,,,,Academia,,,,,Open source,"MIT license
https://github.com/CompVis/taming-transformers",Academia,,,,,,,,,
ESM1b,Biology,"Proteins,Protein or nucleotide language model (pLM/nLM)","Facebook AI Research,New York University (NYU)","Alexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, Myle Ott, C. Lawrence Zitnick, Jerry Ma, and Rob Fergus",2020-12-15,Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences,https://www.pnas.org/doi/abs/10.1073/pnas.2016239118,1676.0,"Highly cited,SOTA improvement","""We apply the representations to a range of prediction tasks and find that they improve state-of-art features across the applications.""",652400000.0,See Table 9,5.1e+21,"Information: 
128 NVIDIA V100 GPUs [Pre-training details]
8.5 hours on 64 GPUs per epoch, 56 epochs of UR50/S [Appendix B, ESM-1b Hyperparameter optimization, Experimental set-up]
128 NVIDIA V100 GPU, assuming  V100 PCIe half precision 130 TFLOPS and 0.3 utilization rate

Estimate: (8.5*56*3600) s * 1.3e14 FLOP/s * 0.3 *64 = 4.277e21 FLOP

6NC method:
UR50/S has 27.1M sequences, which are capped at 1024 amino acids. 
27.1M * 1024 * 56 * 652.4M * 6 = 6.08e21 FLOP

Geometric mean: 5.1e21",UniRef50,"In our experiments, we explore datasets withup to 250 million sequences of the UniParc database (33), whichhas 86 billion amino acids.",,,,,NVIDIA V100,,Confident,"In the field of artificial intelligence, a combination of scale in data and model capacity enabled by unsupervised learning has led to major advances in representation learning and statistical generation. In the life sciences, the anticipated growth of sequencing promises unprecedented data on natural sequence diversity. Protein language modeling at the scale of evolution is a logical step toward predictive and generative artificial intelligence for biology. To this end, we use unsupervised learning to train a deep contextual language model on 86 billion amino acids across 250 million protein sequences spanning evolutionary diversity. The resulting model contains information about biological properties in its representations. The representations are learned from sequence data alone. The learned representation space has a multiscale organization
reflecting structure from the level of biochemical properties of amino acids to remote homology of proteins. Information about secondary and tertiary structure is encoded in the representations and can be identified by linear projections. Representation learning
produces features that generalize across a range of applications, enabling state-of-the-art supervised prediction of mutational effect and secondary structure and improving state-of-the-art features for long-range contact prediction.",56.0,,Open weights (unrestricted),"United States of America,United States of America",,,,128.0,,2025-04-30 10:04,,,,,,"Industry,Academia",,,,,Unreleased,MIT: https://github.com/facebookresearch/esm,"Industry,Academia",$924.32,,,77997.75584661258,"Hardware,Operation counting",,,,
RoBERTa (PFAM),Biology,Protein or nucleotide language model (pLM/nLM),"IBM Research,ETH Zurich","Modestas Filipavicius, Matteo Manica, Joris Cadow, Maria Rodriguez Martinez",2020-12-05,Pre-training Protein Language Models with Label-Agnostic Binding Pairs Enhances Performance in Downstream Tasks,https://arxiv.org/abs/2012.03084,19.0,,,,,1.2276e+19,"1. Hardware: 4x NVIDIA Tesla P100 SXM2 GPUs (18.7 TFLOP/s per GPU in FP16)
The architecture is based on Bert-base, with 110M parameters

6*9300000000*2*110000000=1.2276e+19",Pfam,,9300000000.0,31M sequences with an estiamted length of 300: 31M*300=9300000000,,,NVIDIA P100,,Likely,"Less than 1% of protein sequences are structurally and functionally annotated. Natural Language Processing (NLP) community has recently embraced self-supervised learning as a powerful approach to learn representations from unlabeled text, in large part due to the attention-based context-aware Transformer models. In this work we present a modification to the RoBERTa model by inputting during pre-training a mixture of binding and non-binding protein sequences (from STRING database). However, the sequence pairs have no label to indicate their binding status, as the model relies solely on Masked Language Modeling (MLM) objective during pre-training. After fine-tuning, such approach surpasses models trained on single protein sequences for protein-protein binding prediction, TCR-epitope binding prediction, cellular-localization and remote homology classification tasks. We suggest that the Transformer's attention mechanism contributes to protein binding site discovery. Furthermore, we compress protein sequences by 64% with the Byte Pair Encoding (BPE) vocabulary consisting of 10K subwords, each around 3-4 amino acids long. Finally, to expand the model input space to even larger proteins and multi-protein assemblies, we pre-train Longformer models that support 2,048 tokens. Further work in token-level classification for secondary structure prediction is needed. Code available at: this https URL",2.0,,,"United States of America,Multinational,Ireland,United Kingdom of Great Britain and Northern Ireland,Brazil,Switzerland,Switzerland",,,,4.0,,2025-05-01 10:42,,,,,,"Industry,Academia",,,,,,,"Industry,Academia",,,,2031.643942464676,Operation counting,,,,
CPM-Large,Language,Language modeling,"Tsinghua University,Beijing Academy of Artificial Intelligence / BAAI","Z Zhang, X Han, H Zhou, P Ke, Y Gu, D Ye, Y Qin, Y Su",2020-12-01,CPM: A Large-scale Generative Chinese Pre-trained Language Model,https://arxiv.org/abs/2012.00413,105.0,SOTA improvement,"""CPM outperforms CDial-GPT with a large margin in the few-shot experiment, showing the generalization ability of our model.""",2600000000.0,"""To the best of our knowledge, CPM, with 2.6 billion parameters and 100GB Chinese training data, is the largest Chinese pre-trained language mode""",2.6052e+20,"source: https://lair.lighton.ai/akronomicon/

archived: https://github.com/lightonai/akronomicon/tree/main/akrodb

6*2600000000 parameter *16700000000 tokens=2.605200e+20",Unspecified unreleased,"we construct a new sub-word vocabulary, containing both words and characters.",16700000000.0,"""language model, with 2.6 billion parameters and 100GB Chinese training data.""

We use the conversion factor 1GB ~ 167M words

100GB ~ 16700000000 tokens / words

",336.0,"""It takes two weeks to train our largest model using 64 NVIDIA V100.""",NVIDIA V100,Self-supervised learning,Confident,"Pre-trained Language Models (PLMs) have proven to be beneficial for various downstream NLP tasks. Recently, GPT-3, with 175 billion parameters and 570GB training data, drew a lot of attention due to the capacity of few-shot (even zero-shot) learning. However, applying GPT-3 to address Chinese NLP tasks is still challenging, as the training corpus of GPT-3 is primarily English, and the parameters are not publicly available. In this technical report, we release the Chinese Pre-trained Language Model (CPM) with generative pre-training on large-scale Chinese training data. To the best of our knowledge, CPM, with 2.6 billion parameters and 100GB Chinese training data, is the largest Chinese pre-trained language model, which could facilitate several downstream Chinese NLP tasks, such as conversation, essay generation, cloze test, and language understanding. Extensive experiments demonstrate that CPM achieves strong performance on many NLP tasks in the settings of few-shot (even zero-shot) learning. The code and parameters are available at this https URL.",,,Open weights (unrestricted),"China,China",,,,64.0,,2025-06-18 11:40,,,,,,"Academia,Academia",,,,,Unreleased,"MIT license

https://github.com/TsinghuaAI/CPM-1-Generate","Academia,Academia",$7340.10,,,39011.038545758856,Third-party estimation,,,,
Profile Prediction,Biology,"Protein or nucleotide language model (pLM/nLM),Proteins","University of Washington,Salesforce Research","Pascal Sturmfels, Jesse Vig, Ali Madani, Nazneen Fatema Rajani",2020-12-01,Profile Prediction: An Alignment-Based Pre-Training Task for Protein Sequence Models,https://arxiv.org/abs/2012.00195,21.0,,,,,4.9999999999999993e+20,"1. Hardware setup: 8x NVIDIA Tesla V100 GPUs (130 TFLOP/s each)

2. Training duration: 2 weeks (1.2e+6 seconds) - directly provided

3. Utilization rate: 40%

4. Final calculation:
   8 GPUs Ã— 1.30e+14 FLOP/s Ã— 1.2e+6 seconds Ã— 0.4 utilization = 5.0e+20 FLOPs",Pfam,,8000000001.0,"32 million sequences Ã— 250 residues/sequence = 8 billion data points
[32 Ã— 10^6 Ã— 250 = 8 Ã— 10^9]",,,NVIDIA V100,,Confident,"For protein sequence datasets, unlabeled data has greatly outpaced labeled data due to the high cost of wet-lab characterization. Recent deep-learning approaches to protein prediction have shown that pre-training on unlabeled data can yield useful representations for downstream tasks. However, the optimal pre-training strategy remains an open question. Instead of strictly borrowing from natural language processing (NLP) in the form of masked or autoregressive language modeling, we introduce a new pre-training task: directly predicting protein profiles derived from multiple sequence alignments. Using a set of five, standardized downstream tasks for protein models, we demonstrate that our pre-training task along with a multi-task objective outperforms masked language modeling alone on all five tasks. Our results suggest that protein sequence models may benefit from leveraging biologically-inspired inductive biases that go beyond existing language modeling techniques in NLP.",,,,"United States of America,United States of America",,,,8.0,,2025-05-01 10:42,,,,,,"Academia,Industry",,,,,,,"Academia,Industry",,,,4876.379818219857,Hardware,,,,
AlphaFold 2,Biology,"Protein folding prediction,Proteins",DeepMind,"John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Kathryn Tunyasuvunakool, Olaf Ronneberger, Russ Bates, Augustin Å½Ã­dek, Alex Bridgland, Clemens Meyer, Simon A A Kohl, Anna Potapenko, Andrew J Ballard, Andrew Cowie, Bernardino Romera-Paredes, Stanislav Nikolov, Rishub Jain, Jonas Adler, Trevor Back, Stig Petersen, David Reiman, Martin Steinegger, Michalina Pacholska, David Silver, Oriol Vinyals, Andrew W Senior, Koray Kavukcuoglu, Pushmeet Kohli, Demis Hassabis.",2020-11-30,Highly accurate protein structure prediction with AlphaFold,https://www.nature.com/articles/s41586-021-03819-2,21186.0,"Historical significance,Highly cited","""Here we provide the first computational method that can regularly predict protein structures with atomic accuracy even in cases in which no similar structure is known"" [Abstract]

>17790 citations",93000000.0,"https://arxiv.org/abs/2207.05477 reimplements AlphaFold 2 in a more efficient way, and states there are 93M parameters in the original version (Table 1)",2.99e+21,"123 teraFLOPS / TPU v3 chip * 128 cores * (1 chip / 2 cores) * 11 days * 40% utilization = 2.99e21 FLOP
https://www.wolframalpha.com/input?i=123+teraFLOPS+*+128+*+11+days+*+0.4

""Training regimen"" section: 
""We train the model on Tensor Processing Unit (TPU) v3 with a batch size of 1 per TPU core, hence the model uses 128 TPU v3 cores. [...] The initial training stage takes approximately 1 week, and the fine-tuning stage takes approximately 4 additional days.""","PDB (Protein Data Bank),UniRef30 (FKA UniClust30),UniRef90,MGnify,BFD (Big Fantastic Dataset),UniProtKB","""Inputs and data sources"" section:
""The following versions of public datasets were used in this study. Our models were trained on a copy of the PDB downloaded on 28 August 2019. For finding template structures at prediction time, we used a copy of the PDB downloaded on 14 May 2020, and the PDB70 clustering database downloaded on 13 May 2020. For MSA lookup at both training and prediction time, we used Uniref90 v.2020_01, BFD, Uniclust30 v.2018_08 and MGnify v.2018_12. For sequence distillation, we used Uniclust30 v.2018_08 to construct a distillation structure dataset. Full details are provided in Supplementary Methods 1.2.""

AlphaFold needs multiple genetic (sequence) databases to run:

BFD,
MGnify,
PDB70,
PDB (structures in the mmCIF format),
PDB seqres â€“ only for AlphaFold-Multimer,
UniRef30 (FKA UniClust30),
UniProt â€“ only for AlphaFold-Multimer,
UniRef90",530000.0,"3 different types of input data to the network:
(1) Amino acid sequence
(2) Multiple sequence alignments (MSA) to sequences from evolutionarily related proteins
(3) Template structures (3D atom coordinates of homologous structures), where available

Training data is processed into the following two datasets that are sampled with different probabilities. 
Supplementary Material, Section 1.2.4. Training data:
""With 75% probability a training example comes from the self-distillation set (see subsection 1.3) and with 25% probability the training example is a known structure from the Protein Data Bank""

Supplementary Material, Section 1.3 Self-distillation dataset:
""This gives a final dataset of 355,993 sequences"". An initial model was used to predict structures for these sequences.

PDB dataset size in 2020: https://www.rcsb.org/stats/growth/growth-released-structures
172788

Therefore, estimate for number of protein structures available for training (for which amino acid sequence, MSA and homologue template info is also available as input to network): 528781 [~530k]",264.0,7 days pretrain and 4 days finetune,Google TPU v3,,Likely,"Proteins are essential to life, and understanding their structure can facilitate a mechanistic understanding of their function. Through an enormous experimental effort, the structures of around 100,000 unique proteins have been determined, but this represents a small fraction of the billions of known protein sequences. Structural coverage is bottlenecked by the months to years of painstaking effort required to determine a single protein structure. Accurate computational approaches are needed to address this gap and to enable large-scale structural bioinformatics. Predicting the three-dimensional structure that a protein will adopt based solely on its amino acid sequenceâ€”the structure prediction component of the â€˜protein folding problemâ€™â€”has been an important open research problem for more than 50 years. Despite recent progress, existing methods fall far short of atomic accuracy, especially when no homologous structure is available. Here we provide the first computational method that can regularly predict protein structures with atomic accuracy even in cases in which no similar structure is known. We validated an entirely redesigned version of our neural network-based model, AlphaFold, in the challenging 14th Critical Assessment of protein Structure Prediction (CASP14), demonstrating accuracy competitive with experimental structures in a majority of cases and greatly outperforming other methods. Underpinning the latest version of AlphaFold is a novel machine learning approach that incorporates physical and biological knowledge about protein structure, leveraging multi-sequence alignments, into the design of the deep learning algorithm.",,,Open weights (unrestricted),United Kingdom of Great Britain and Northern Ireland,,,,,,2025-05-28 16:13,,,,,,Industry,,,,,Unreleased,"While the AlphaFold code is licensed under the Apache 2.0 License, the AlphaFold parameters and CASP15 prediction data are made available under the terms of the CC BY 4.0 license

code in this repo is inference code:
https://github.com/google-deepmind/alphafold",Industry,$3841.77,,BF16,,Hardware,,,,
KEPLER,Language,Relation extraction,"Tsinghua University,Mila - Quebec AI (originally Montreal Institute for Learning Algorithms),HEC,CIFAR AI Research,Princeton University,University of Montreal / UniversitÃ© de MontrÃ©al","Xiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhiyuan Liu, Juanzi Li, and Jian Tang.",2020-11-23,KEPLER: A Unified Model for Knowledge Embedding and Pre- trained Language Representation.,https://arxiv.org/abs/1911.06136,568.0,SOTA improvement,"""Experimental results show that KEPLER achieves state-of-the-art performances
on various NLP tasks""",125000000.0,,1.66e+21,"From author communication

""About 128 GPU-days using Nvidia V100 (16GB). ""

precision: float16

V100 GPU for float16: 28000000000000 (2.8E+13)

0.4 * 28TFLOP/s * 128 GPU-days * 24h/day * 3600s/h
= 1.24E+20

""and use the released roberta.base parameters for
initialization, which is a common practice to save
pre-training time""

Roberta base FLOP: 1.536e+21
Total:1.660000e+21
","Wikipedia,""BookCorpus (BooksCorpus, Toronto Book Corpus)"",Wikidata5M","From author communication

    For the language modeling objective, we use Wikipedia+BookCorpus datasets (about 13GB).    For the knowledge embedding objective, we use Wikidata5m (about 1GB).",3300000000.0,"For BookCorpus + English Wikipedia: 800M + 2500M

For Wikidata5M: 20614279
See table 1. Contains ""entities"", ""relations"", and ""triplets""",,,,Self-supervised learning,Confident,"Pre-trained language representation models (PLMs) cannot well capture factual knowledge from text. In contrast, knowledge embedding (KE) methods can effectively represent the relational facts in knowledge graphs (KGs) with informative entity embeddings, but conventional KE models cannot take full advantage of the abundant textual information. In this paper, we propose a unified model for Knowledge Embedding and Pre-trained LanguagE Representation (KEPLER), which can not only better integrate factual knowledge into PLMs but also produce effective text-enhanced KE with the strong PLMs. In KEPLER, we encode textual entity descriptions with a PLM as their embeddings, and then jointly optimize the KE and language modeling objectives. Experimental results show that KEPLER achieves state-of-the-art performances on various NLP tasks, and also works remarkably well as an inductive KE model on KG link prediction. Furthermore, for pre-training and evaluating KEPLER, we construct Wikidata5M, a large-scale KG dataset with aligned entity descriptions, and benchmark state-of-the-art KE methods on it. It shall serve as a new KE benchmark and facilitate the research on large KG, inductive KE, and KG with text. The source code can be obtained from this https URL.",,,Unreleased,"China,Canada,France,Canada,United States of America,Canada",RoBERTa Base,,,,,2025-06-18 11:28,,,,,,"Academia,Academia,Academia,Research collective,Academia,Academia",,,,,Open source,"MIT License, includes train code https://github.com/THU-KEG/KEPLER","Academia,Academia,Academia,Research collective,Academia,Academia",,,FP16,,Hardware,,,,
AWD-FWM (WT2),Language,Language modeling,"IDSIA,Microsoft Research","Imanol Schlag, Tsendsuren Munkhdalai, JÃ¼rgen Schmidhuber",2020-11-16,Learning Associative Inference Using Fast Weight Memory,https://arxiv.org/abs/2011.07831,35.0,,,37000000.0,"""all WT2 models have roughly 37M parameters""",7.104e+17,6 FLOP / parameter / token * 37000000 parameters * 2000000 tokens * 1600 epochs = 7.104e+17 FLOP,WikiText-2,,2000000.0,,,,,,Confident,"Humans can quickly associate stimuli to solve problems in novel contexts. Our novel neural network model learns state representations of facts that can be composed to perform such associative inference. To this end, we augment the LSTM model with an associative memory, dubbed Fast Weight Memory (FWM). Through differentiable operations at every step of a given input sequence, the LSTM updates and maintains compositional associations stored in the rapidly changing FWM weights. Our model is trained end-to-end by gradient descent and yields excellent performance on compositional language reasoning problems, meta-reinforcement-learning for POMDPs, and small-scale word-level language modelling.",1600.0,AWD-FWM (WT2),Unreleased,"Switzerland,United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational",,,,,,2025-04-08 12:45,,,,,,"Academia,Industry",,,,,Open source,"code, repo license is MIT: https://github.com/ischlag/Fast-Weight-Memory-public/tree/main/language-modelling/fwm 
train and eval code: https://github.com/ischlag/Fast-Weight-Memory-public/blob/main/language-modelling/fwm/FWM-README.md ","Academia,Industry",,,,,Operation counting,,,,
CPCProt,Biology,Protein or nucleotide language model (pLM/nLM),University of Toronto,"Amy X. Lu, Haoran Zhang, Marzyeh Ghassemi, Alan Moses",2020-11-10,Self-Supervised Contrastive Learning of Protein Representations By Mutual Information Maximization,https://www.biorxiv.org/content/10.1101/2020.09.04.283929v2.abstract,95.0,,,1700000.0,,,,Pfam,,32207059.0,,,,,,Confident,"Pretrained embedding representations of biological sequences which capture meaningful properties can alleviate many problems associated with supervised learning in biology. We apply the principle of mutual information maximization between local and global information as a self-supervised pretraining signal for protein embeddings. To do so, we divide protein sequences into fixed size fragments, and train an autoregressive model to distinguish between subsequent fragments from the same protein and fragments from random proteins. Our model, CPCProt, achieves comparable performance to state-of-the-art self-supervised models for protein sequence embeddings on various downstream tasks, but reduces the number of parameters down to 2% to 10% of benchmarked models. Further, we explore how downstream assessment protocols affect embedding evaluation, and the effect of contrastive learning hyperparameters on empirical performance. We hope that these results will inform the development of contrastive learning methods in protein biology and other modalities.",19.0,,,Canada,,,,,,2025-05-01 10:42,,,,,,Academia,,,,,,,Academia,,,,,,,,,
HiPPO-LegS,Vision,Image classification,"Stanford University,University at Buffalo","Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, Christopher Re",2020-10-23,HiPPO: Recurrent Memory with Optimal Polynomial Projections,https://arxiv.org/abs/2008.07669,,,,,,,,P-MNIST,,,,,,NVIDIA P100,,Unknown,"A central problem in learning from sequential data is representing cumulative history in an incremental fashion as more data is processed. We introduce a general framework (HiPPO) for the online compression of continuous signals and discrete time series by projection onto polynomial bases. Given a measure that specifies the importance of each time step in the past, HiPPO produces an optimal solution to a natural online function approximation problem. As special cases, our framework yields a short derivation of the recent Legendre Memory Unit (LMU) from first principles, and generalizes the ubiquitous gating mechanism of recurrent neural networks such as GRUs. This formal framework yields a new memory update mechanism (HiPPO-LegS) that scales through time to remember all history, avoiding priors on the timescale. HiPPO-LegS enjoys the theoretical benefits of timescale robustness, fast updates, and bounded gradients. By incorporating the memory dynamics into recurrent neural networks, HiPPO RNNs can empirically capture complex temporal dependencies. On the benchmark permuted MNIST dataset, HiPPO-LegS sets a new state-of-the-art accuracy of 98.3%. Finally, on a novel trajectory classification task testing robustness to out-of-distribution timescales and missing data, HiPPO-LegS outperforms RNN and neural ODE baselines by 25-40% accuracy.",,,Open weights (unrestricted),"United States of America,United States of America",,,,,,2025-05-01 10:42,,,,,,"Academia,Academia",,,,,Open source,"apache 2.0
https://github.com/state-spaces/s4","Academia,Academia",,,,,,,,,
ChemBERTa,Biology,Molecular property prediction,"University of Toronto,Reverie Labs,DeepChem","Seyone Chithrananda, Gabriel Grand, Bharath Ramsundar",2020-10-23,ChemBERTa: Large-Scale Self-Supervised Pretraining for Molecular Property Prediction,https://arxiv.org/abs/2010.09885,,,,125000000.0,"""Our implementation of RoBERTa uses 12 attention heads and 6 layers, resulting in 72 distinct attention mechanisms"" -> base model is RoBERTa Base",8.4654366e+18,"6 FLOP / parameter / token * 125 * 10^6 parameters * 64 sequences per batch [assumption] * 512 tokens per sequence [upper bound] * 450000 steps = 1.10592e+19 FLOP

125000000000000 FLOP / GPU / sec [bf16 assumed] * 1 GPU * 48 hours * 3600 sec / hour * 0.3 [assumed utilization] = 6.48e+18 FLOP

sqrt(1.10592e+19* 6.48e+18) = 8.4654366e+18 FLOP",PubChem,,4915200000.0,"10M unique SMILES from PubChem
max. sequence length of 512 tokens
450K steps
""We trained for 10 epochs on all PubChem subsets except for the 10M subset, on which we trained for 3 epochs to avoid observed overfitting. ""

assuming (!) batch size of 64:

64*512*450000 = 14 745 600 000 total tokens -> 4 915 200 000 tokens per epoch -> ~ 500 tokens per SMILE",48.0,"""Pretraining on the largest subset took approx. 48 hours on a single NVIDIA V100 GPU""",NVIDIA V100,,Likely,"GNNs and chemical fingerprints are the predominant approaches to representing molecules for property prediction. However, in NLP, transformers have become the de-facto standard for representation learning thanks to their strong downstream task transfer. In parallel, the software ecosystem around transformers is maturing rapidly, with libraries like HuggingFace and BertViz enabling streamlined training and introspection. In this work, we make one of the first attempts to systematically evaluate transformers on molecular property prediction tasks via our ChemBERTa model. ChemBERTa scales well with pretraining dataset size, offering competitive downstream performance on MoleculeNet and useful attention-based visualization modalities. Our results suggest that transformers offer a promising avenue of future work for molecular representation learning and property prediction. To facilitate these efforts, we release a curated dataset of 77M SMILES from PubChem suitable for large-scale self-supervised pretraining.",3.0,,Open weights (unrestricted),"Canada,United States of America,United States of America",,,,1.0,,2025-05-01 10:42,,,,,,"Academia,Industry,Industry",,,,,,"https://huggingface.co/seyonec/ChemBERTa-zinc-base-v1
https://huggingface.co/seyonec/PubChem10M_SMILES_BPE_450k","Academia,Industry,Industry",,,,335.20719920857016,"Operation counting,Hardware",seyonec,,,
ViT-Huge/14,Vision,Image representation,"Google Brain,Google Research","Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby",2020-10-22,An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale,https://arxiv.org/abs/2010.11929,30079.0,Highly cited,,632000000.0,Table 1 https://arxiv.org/pdf/2010.11929.pdf,4.262e+21,"Table 6: 4.262e21 FLOPs

Agrees with Table 2 (2.5k TPUv3-core days), if MFU is around 0.32. 

2500 * 24 * 3600 * (0.5 * 1.23e14) * 0.32 = 4.25e21","ImageNet-1k,ImageNet21k,JFT-300M","To explore model scalability, we use the ILSVRC-2012 ImageNet dataset with 1k classes and 1.3M images (we refer to it as ImageNet in what follows), its superset ImageNet-21k with 21k classes and 14M images (Deng et al., 2009), and JFT (Sun et al., 2017) with 18k classes and 303M high-resolution images. ",1280000.0,,,,Google TPU v3,,Confident,"While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.",14.0,,Open weights (unrestricted),"United States of America,Multinational,United States of America,Canada,Switzerland",,,,,0.32,2024-11-15 09:29,,,,,,"Industry,Industry",,,,30000.0,Open source,"Apache-2.0 license
https://github.com/google-research/vision_transformer","Industry,Industry",$6724.02,"Stated FLOPs to train is 4.262e21
Training used 2500 TPUv3-core-days (2x chip-days), so at max utilization: 2500 * 24 * 3600 * 2 * 1.23e14 = 5.3136e22
4.261e21 / 5.3136e22
",,,Hardware,,,,
wave2vec 2.0 LARGE,Speech,Speech completion,Facebook,"Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli",2020-10-22,wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations,https://arxiv.org/abs/2006.11477,4623.0,"Highly cited,SOTA improvement","Arguably an ""important"" paper? 

Abstract: 
""We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler.""",317000000.0,"Section 5.1:
""We consider two model sizes: BASE (95m parameters) and LARGE (317m parameters)
",3.87072e+21,"From surveying the authors:

We trained the base model on 64 V100 GPUs for 400k updates. This takes about 3 days to complete. The large model is trained on 128 V100 GPUs for 1 million updates, and this takes about 7 days to complete.

V100 GPU peak: 125TFLOP/s (https://www.nvidia.com/en-gb/data-center/tesla-v100/)
Assume 40% utilization based on default for non-Language domain (https://epoch.ai/blog/estimating-training-compute)

128 GPUs * 40% * 125TFLOP/s * 7 days * 24h/day * 3600s/h
~= 3.870720e+21","LibriSpeech,LibriLight",,727776000.0,"pg 4, section 4.1

""As unlabeled data we consider the Librispeech corpus [40] without transcriptions containing 960 hours of audio (LS-960) or the audio data from LibriVox (LV-60k). For the latter we follow the preprocessing of [27] resulting in 53.2k hours of audio.""

53.2k h * 13,680 words/h = 727776000 words",,,NVIDIA Tesla V100 DGXS 32 GB,Self-supervised learning,Confident,"We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the clean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state of the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and pre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech recognition with limited amounts of labeled data.",,,Open weights (unrestricted),United States of America,,,,,,2025-06-18 11:19,,,,,,Industry,,,,,Open source,"https://github.com/facebookresearch/fairseq/blob/1bba712622b8ae4efb3eb793a8a40da386fe11d0/examples/wav2vec/README.md

fairseq(-py) is MIT-licensed. The license applies to the pre-trained models as well. Repo contains weights and pretrain and finetune code",Industry,$5021.24,,,,Hardware,,,,
CryptoGRU,Language,Language modeling,Indiana University Bloomington,"Bo Feng, Qian Lou, Lei Jiang, Geoffrey C. Fox",2020-10-22,CryptoGRU: Low Latency Privacy-Preserving Text Analysis With GRU,https://arxiv.org/abs/2010.11796,12.0,,improves latency but not accuracy compared to other cryptographic models,,,,,Penn TreeBank (PTB),,,,,,,,Unknown,"Billions of text analysis requests containing private emails, personal text messages, and sensitive online reviews, are processed by recurrent neural networks (RNNs) deployed on public clouds every day. Although prior secure networks combine homomorphic encryption (HE) and garbled circuit (GC) to preserve users' privacy, naively adopting the HE and GC hybrid technique to implement RNNs suffers from long inference latency due to slow activation functions. In this paper, we present a HE and GC hybrid gated recurrent unit (GRU) network, CryptoGRU, for low-latency secure inferences. CryptoGRU replaces computationally expensive GC-based tanh with fast GC-based ReLU, and then quantizes sigmoid and ReLU with a smaller bit length to accelerate activations in a GRU. We evaluate CryptoGRU with multiple GRU models trained on 4 public datasets. Experimental results show CryptoGRU achieves top-notch accuracy and improves the secure inference latency by up to 138Ã— over one of state-of-the-art secure networks on the Penn Treebank dataset.",,CryptoGRU,Unreleased,United States of America,,,,,,2024-09-09 13:28,,,,,,Academia,,,,,Unreleased,"repo here, don't think it has PTB code though https://github.com/bfeng/CryptoGRU ",Academia,,,,,,,,,
GBERT-Large,Language,"Document classification,Named entity recognition","deepset,Bayerische Staatsbibliothek Muenchen","Branden Chan, Stefan Schweter, Timo MÃ¶ller",2020-10-21,German's Next Language Model,https://arxiv.org/abs/2010.10906,244.0,SOTA improvement,'we were able to attain SoTA performance across a set of document classification and named entity recognition (NER) tasks for both models of base and large size.',335000000.0,335M from Table 5,2.2444646e+21,"flops = (64) * (123* 10**12) * (11 * 24 * 3600) * (0.3) = 2.24e21
(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)

'large models were trained on pods of 16 TPUs v3 (128 cores).' - from section 4.1it was trained for 11 days from Table 2","Wikipedia,OPUS,OSCAR,OpenLegalData",Table 1 in the paper,27287800000.0,"163.4GB from Table 1 in the paper
assuming 167M words per GB (German Language) we have 163.4 * 167M = 27287800000.0",264.0,11 days from Table 2,Google TPU v3,,Likely,"In this work we present the experiments which lead to the creation of our BERT and ELECTRA based German language models, GBERT and GELECTRA. By varying the input training data, model size, and the presence of Whole Word Masking (WWM) we were able to attain SoTA performance across a set of document classification and named entity recognition (NER) tasks for both models of base and large size. We adopt an evaluation driven approach in training these models and our results indicate that both adding more data and utilizing WWM improve model performance. By benchmarking against existing German models, we show that these models are the best German models to date. Our trained models will be made publicly available to the research community. ",,,Open weights (unrestricted),"Germany,Germany",,,,64.0,,2025-02-17 12:04,,,,2048.0,,"Industry,Government",,,,16896.0,Unreleased,MIT: https://huggingface.co/deepset/gbert-large,"Industry,Government",$3771.13,,,28634.227317541987,Hardware,,,,
German ELECTRA Large,Language,"Document classification,Named entity recognition","deepset,Bayerische Staatsbibliothek Muenchen","Branden Chan, Stefan Schweter, Timo MÃ¶ller",2020-10-21,German's Next Language Model,https://arxiv.org/abs/2010.10906,244.0,SOTA improvement,'we were able to attain SoTA performance across a set of document classification and named entity recognition (NER) tasks for both models of base and large size.',335000000.0,335M from Table 5,1.42829568e+21,"flops = (64) * (123* 10**12) * (7 * 24 * 3600) * (0.3) = 1.4e21
(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)

'large models were trained on pods of 16 TPUs v3 (128 cores).' - from section 4.1 it was trained for 7 days from Table 2

Agrees with 6CN:
Tokens seen: 512 (seq len) * 1024 (batch size) * 1 million (steps) = 5.24e11
FLOPs: 6 * 335M * 5.24e11 = 1.05e21","Wikipedia,OPUS,OSCAR,OpenLegalData",Table 1 in the paper,36383733333.0,"163.4GB from Table 1 in the paper
assuming 167M words per GB (German Language) we have 163.4 * 167M * 4/3 tokens per word = 36,383,733,333",168.0,7 days from Table 2,Google TPU v3,,Confident,"In this work we present the experiments which lead to the creation of our BERT and ELECTRA based German language models, GBERT and GELECTRA. By varying the input training data, model size, and the presence of Whole Word Masking (WWM) we were able to attain SoTA performance across a set of document classification and named entity recognition (NER) tasks for both models of base and large size. We adopt an evaluation driven approach in training these models and our results indicate that both adding more data and utilizing WWM improve model performance. By benchmarking against existing German models, we show that these models are the best German models to date. Our trained models will be made publicly available to the research community. ",,,Open weights (unrestricted),"Germany,Germany",,,,64.0,,2025-06-05 17:07,,,,,,"Industry,Government",,,,10752.0,Unreleased,MIT: https://huggingface.co/deepset/gelectra-large,"Industry,Government",$2392.29,,,28634.227317541987,"Hardware,Operation counting",deepset,,,
Conformer + Wav2vec 2.0 + Noisy Student,Speech,Speech recognition,"Google,Google Research,Google Brain","Yu Zhang, James Qin, Daniel S. Park, Wei Han, Chung-Cheng Chiu, Ruoming Pang, Quoc V. Le, Yonghui Wu",2020-10-20,Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition,https://arxiv.org/abs/2010.10504v2,294.0,SOTA improvement,"""By doing so, we are able to achieve
word-error-rates (WERs) 1.4%/2.6% on the LibriSpeech test/test-other sets against
the current state-of-the-art WERs 1.7%/3.3%.""",1000000000.0,1B for XXL model,7.6e+21,"""We train with global batch size 2048 on 256/512 Google TPU V3 cores for 3-4 days for the XL/XXL models respectively...
We fine-tune the pre-trained checkpoints (400k steps) with global batch
size 1024/512 on 256/512 Google TPU v3 cores for 1-3 days for the XL/XXL models""

TPU v3 chips are 123 teraflop/s. 2 chips per core

512 cores * 7 days * 24 * 3600 * 123 tflops * (1 chip/2 cores) * 0.4 (assumed utilization) = 7.6e21",LibriLight,"""We pre-train the Conformer encoder akin to wav2vec 2.0 pre-training [6] with 60k hours of unlabeled audio from the ""unlab-60k"" subset of Libri-Light. Unlike in the original work which takes raw waveforms as input, we use log-mel spectrograms... 

The 960h of transcribed audio of the LibriSpeech dataset is used as the supervised data""",,,168.0,7 days,Google TPU v3,,Confident,"We employ a combination of recent developments in semi-supervised learning for automatic speech recognition to obtain state-of-the-art results on LibriSpeech utilizing the unlabeled audio of the Libri-Light dataset. More precisely, we carry out noisy student training with SpecAugment using giant Conformer models pre-trained using wav2vec 2.0 pre-training. By doing so, we are able to achieve word-error-rates (WERs) 1.4%/2.6% on the LibriSpeech test/test-other sets against the current state-of-the-art WERs 1.7%/3.3%.",,,Unreleased,"United States of America,Multinational,United States of America,Canada,Switzerland,United States of America",,,,256.0,,2025-02-17 12:05,,,,,,"Industry,Industry,Industry",,,,,Unreleased,,"Industry,Industry,Industry",$9449.54,,,114539.4599635263,Hardware,,,,
mT5-XXL,Language,"Language modeling,Translation","Google,Google Research","Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, Colin Raffel",2020-10-20,mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer,https://aclanthology.org/2021.naacl-main.41/,2122.0,"Highly cited,SOTA improvement","""Table 2 presents our main results, with perlanguage breakdowns for each task given in Appendix B. Our largest model mT5-XXL exceeds state-of-the-art on all classification and QA tasks and is near SOTA on NER (69.2 vs. 70.1).""",13000000000.0,13 billion,8.2e+22,"""We pre-train our mT5 model variants for 1 million steps on batches of 1024 length-1024 input sequences, corresponding to roughly 1 trillion input tokens total.""

1 million steps * 1024 batchsize * 1024 length * 13 billion params * 6 = 8.2e22

Ignores fine-tuning compute; this is likely a small fraction of pre-training compute.",mC4,"""The C4 dataset was explicitly designed to be English only: any page that was not given a probability of at least 99% of being English by langdetect2 was discarded. In contrast, for mC4 we use cld33 to identify over 100 languages.
Since some of these languages are relatively scarce on the internet, we make use of all of the 71 monthly web scrapes released so far by Common Crawl. This is dramatically more source data than was used for C4, for which the April 2019 web scrape alone was enough to provide plenty of English-language data.""",1000000000000.0,"The model was trained on a subset of 1 trillion tokens.
Full mC4 corpus has data ""totaling 6.6B pages and 6.3T tokens""
Distribution by language is in Appendix A.",,,,,Confident,"The recent â€œText-to-Text Transfer Transformerâ€ (T5) leveraged a unified text-to-text format and scale to attain state-of-the-art results on a wide variety of English-language NLP tasks. In this paper, we introduce mT5, a multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We detail the design and modified training of mT5 and demonstrate its state-of-the-art performance on many multilingual benchmarks. We also describe a simple technique to prevent â€œaccidental translationâ€ in the zero-shot setting, where a generative model chooses to (partially) translate its prediction into the wrong language. All of the code and model checkpoints used in this work are publicly available.",1.0,,Open weights (unrestricted),"United States of America,Multinational,United States of America,Canada,Switzerland",,,,,,2024-12-22 14:26,,,,1048576.0,"""We pre-train our mT5 model variants for 1 million steps on batches of 1024 length-1024 input sequences, corresponding to roughly 1 trillion input tokens total.""","Industry,Industry",,,,,Open source," Apache 2.0 license
training code: https://github.com/google-research/multilingual-t5","Industry,Industry",,,,,Operation counting,,,,5
Memformer (4 encoder + 16 decoder),Language,Language modeling,"UC Davis,Westlake University,Facebook AI","Qingyang Wu, Zhenzhong Lan, Kun Qian, Jing Gu, Alborz Geramifard, Zhou Yu",2020-10-14,Memformer: A Memory-Augmented Transformer for Sequence Modeling,https://arxiv.org/abs/2010.06891,41.0,,,76200000.0,,1.2e+19,"32400000000000 FLOP / second / GPU * 4 GPUs * 96 hours * 3600 sec / hour * 0.3 [assumed utilization] = 1.3436928e+19 FLOP

time could be given for all language modeling not just for this model -> likely to be an upper bound",WikiText-103,,103000000.0,"batch size 128
steps: 150000
sequence length: 128-1024
",96.0,"""We trained our model on NVIDIA V100 16GB and 2080Ti 11GB. <..> The training for language modeling took approximately four days on four GPUs.""

4 days = 96 hours","NVIDIA Tesla V100 DGXS 16 GB,NVIDIA GeForce RTX 2080 Ti 11GB",,Likely,"Transformers have reached remarkable success in sequence modeling. However, these models have efficiency issues as they need to store all the history token-level representations as memory. We present Memformer, an efficient neural network for sequence modeling, that utilizes an external dynamic memory to encode and retrieve past information. Our model achieves linear time complexity and constant memory space complexity when processing long sequences. We also propose a new optimization scheme, memory replay back-propagation (MRBP), which promotes long-range back-propagation through time with a significantly reduced memory requirement. Experimental results show that Memformer has achieved comparable performance compared to the baselines by using 8.1x less memory space and 3.2x faster on inference. Analysis of the attention pattern shows that our external memory slots can encode and retain important information through timesteps.",11.93,Memformer (4 encoder + 16 decoder),Unreleased,"United States of America,China,United States of America",,,,4.0,,2025-04-08 13:02,,,,,,"Academia,Academia,Industry",,,,,Unreleased,,"Academia,Academia,Industry",,,,,Hardware,,,,
LUKE,Language,Question answering,"University of Washington,National Institute of Informatics","Ikuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki Takeda, Yuji Matsumoto",2020-10-02,LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention,https://arxiv.org/abs/2010.01057v1,611.0,SOTA improvement,"from abstract ""In particular, it obtains state-of-the-art results on five well-known datasets: Open Entity (entity typing), TACRED (relation classification), CoNLL-2003 (named entity recognition), ReCoRD (cloze-style question answering), and SQuAD 1.1 (extractive question answering).""",483000000.0,"""The total number of parameters is approximately 483 M, consisting of 355 M in RoBERTa and 128 M in our entity embeddings""",1.8144e+22,"Uses RoBERTa Large as a base model, which used 1.66e22 FLOPs in training.

LUKE's additional training was:
(16) * (1.25e14) * (30 * 24 * 3600) * (0.3) = 1.5552e21
(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)

from appendix A: ""Werun the pretraining on NVIDIAâ€™s PyTorch Docker
container 19.02 hosted on a server with two Intel Xeon Platinum 8168 CPUs and 16 NVIDIA Tesla V100 GPUs. The training takes approximately 30 days.""

Assuming 16 bit tensor core computations, 1.25e14 FLOP/s per V100

Total: 1.65888e22 + 1.5552e21 = 1.8144e22",Wikipedia,"""As input corpus for pretraining, we use the December 2018 version of Wikipedia, comprising approximately 3.5 billion words and 11 million entity annotations. """,3500000000.0,"""As input corpus for pretraining, we use the December 2018 version of Wikipedia, comprising approximately 3.5 billion words and 11 million entity annotations. """,720.0,see compute notes,NVIDIA V100,,Likely,"Entity representations are useful in natural language tasks involving entities. In this paper, we propose new pretrained contextualized representations of words and entities based on the bidirectional transformer. The proposed model treats words and entities in a given text as independent tokens, and outputs contextualized representations of them. Our model is trained using a new pretraining task based on the masked language model of BERT. The task involves predicting randomly masked words and entities in a large entity-annotated corpus retrieved from Wikipedia. We also propose an entity-aware self-attention mechanism that is an extension of the self-attention mechanism of the transformer, and considers the types of tokens (words or entities) when computing attention scores. The proposed model achieves impressive empirical performance on a wide range of entity-related tasks. In particular, it obtains state-of-the-art results on five well-known datasets: Open Entity (entity typing), TACRED (relation classification), CoNLL-2003 (named entity recognition), ReCoRD (cloze-style question answering), and SQuAD 1.1 (extractive question answering). Our source code and pretrained representations are available at this https://github.com/studio-ousia/luke",119.46,,Open weights (unrestricted),"United States of America,Japan",RoBERTa Large,,"LUKE's additional training was:
(16) * (1.25e14) * (30 * 24 * 3600) * (0.3) = 1.5552e21
(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)

from appendix A: ""Werun the pretraining on NVIDIAâ€™s PyTorch Docker
container 19.02 hosted on a server with two Intel Xeon Platinum 8168 CPUs and 16 NVIDIA Tesla V100 GPUs. The training takes approximately 30 days.""

Assuming 16 bit tensor core computations, 1.25e14 FLOP/s per V100LUKE's additional training was:LUKE's additional training was:
(16) * (1.25e14) * (30 * 24 * 3600) * (0.3) = 1.5552e21(16) * (1.25e14) * (30 * 24 * 3600) * (0.3) = 1.5552e21
(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)

from appendix A: ""Werun the pretraining on NVIDIAâ€™s PyTorch Dockerfrom appendix A: ""Werun the pretraining on NVIDIAâ€™s PyTorch Docker
container 19.02 hosted on a server with two Intel Xeon Platinum 8168 CPUs and 16 NVIDIA Tesla V100 GPUs. The training takes approximately 30 days.""container 19.02 hosted on a server with two Intel Xeon Platinum 8168 CPUs and 16 NVIDIA Tesla V100 GPUs. The training takes approximately 30 days.""

Assuming 16 bit tensor core computations, 1.25e14 FLOP/s per V100Assuming 16 bit tensor core computations, 1.25e14 FLOP/s per V100",16.0,,2025-05-28 16:13,,,,2048.0,table in appendix A,Academia,,,,11520.0,Open source,"apache 2.0: https://github.com/studio-ousia/luke?tab=readme-ov-file

data is wikimedia, which has a commercial license: https://dumps.wikimedia.org/legal.html

pretraining: https://github.com/studio-ousia/luke/blob/master/pretraining.md ",Academia,$4186.38,,FP16,9765.799615782104,Hardware,,,,
PAR Transformer Large,Language,Language modeling,NVIDIA,"Swetha Mandava, Szymon Migacz, Alex Fit Florea",2020-09-09,Pay Attention when Required,https://arxiv.org/abs/2009.04534,11.0,,,,,,,WikiText-103,,,,,,,,Unknown,"Transformer-based models consist of interleaved feed-forward blocks - that capture content meaning, and relatively more expensive self-attention blocks - that capture context meaning. In this paper, we explored trade-offs and ordering of the blocks to improve upon the current Transformer architecture and proposed PAR Transformer. It needs 35% lower compute time than Transformer-XL achieved by replacing ~63% of the self-attention blocks with feed-forward blocks, and retains the perplexity on WikiText-103 language modelling benchmark. We further validated our results on text8 and enwiki8 datasets, as well as on the BERT model.",,PAR Transformer Large,Unreleased,United States of America,,,,,,2024-09-09 13:34,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
ProBERTa,Biology,"Proteins,Protein representation learning","University of Illinois Urbana-Champaign (UIUC),Reed College","Ananthan Nambiar, Maeve Heflin, Simon Liu, Sergei Maslov, Mark Hopkins, Anna Ritz",2020-09-01,Transforming the Language of Life: Transformer Neural Networks for Protein Prediction Tasks,https://dl.acm.org/doi/10.1145/3388440.3412467,97.0,SOTA improvement,"""Furthermore, we used embeddings from PRoBERTa for a fundamentally different problem, PPI prediction, using two different
datasets generated from the HIPPIE database and found that with
sufficient data, it substantially outperforms the current state-of-theart method in the conservative scenario.""",44000000.0,"""In total, our model has approximately 44M trainable parameters.""",9.72e+18,"""we pre-train PRoBERTa on 4 NVIDIA V100 GPUs in 18 hours""
4 * 125 tFLOP/s * 18 * 3600 * 0.3 (assumed utilization) = 9.72e18",UniProtKB/Swiss-Prot,"""Pre-training data: We use UniProtKB/Swiss-Prot (450K unique sequences with a mean tokenized length of 129.6 tokens), a collection of experimentally annotated and reviewed amino acid sequences""

Fine tuning uses a subset of 313,214 sequences which have annotated labels.",58320000.0,"450k sequences * 129.6 tokens per sequence = 58,320,000 tokens",18.0,,NVIDIA V100,Self-supervised learning,Confident,"The scientific community is rapidly generating protein sequence information, but only a fraction of these proteins can be experimentally characterized. While promising deep learning approaches for protein prediction tasks have emerged, they have computational limitations or are designed to solve a specific task. We present a Transformer neural network that pre-trains task-agnostic sequence representations. This model is fine-tuned to solve two different protein prediction tasks: protein family classification and protein interaction prediction. Our method is comparable to existing state-of-the-art approaches for protein family classification while being much more general than other architectures. Further, our method outperforms all other approaches for protein interaction prediction. These results offer a promising framework for fine-tuning the pre-trained sequence representations for other protein prediction tasks.",,,Open weights (non-commercial),"United States of America,United States of America",,,,4.0,,2025-06-04 17:12,,,,,,"Academia,Academia",,,,,Open (non-commercial),"no clear license
https://github.com/annambiar/PRoBERTa","Academia,Academia",$26.21,,FP16,2443.1359420423914,Hardware,,,,
Transformer+Recurrent Windows of Context,Language,Language modeling,"Toyota Technological Institute at Chicago,University of Chicago","Davis Yoshida, Allyson Ettinger, Kevin Gimpel",2020-08-16,Adding Recurrence to Pretrained Transformers for Improved Efficiency and Context Size,https://arxiv.org/abs/2008.07027,7.0,,,124000000.0,,7.9375326e+20,"base model compute (speculative confidence): 7.936e+20 FLOP 
fine-tune compute: 1.53264e+17 FLOP 

7.936e+20 FLOP + 1.53264e+17 FLOP  = 7.9375326e+20 FLOP",WikiText-103,,,,,,,,Speculative,"Fine-tuning a pretrained transformer for a downstream task has become a standard method in NLP in the last few years. While the results from these models are impressive, applying them can be extremely computationally expensive, as is pretraining new models with the latest architectures. We present a novel method for applying pretrained transformer language models which lowers their memory requirement both at training and inference time. An additional benefit is that our method removes the fixed context size constraint that most transformer models have, allowing for more flexible use. When applied to the GPT-2 language model, we find that our method attains better perplexity than an unmodified GPT-2 model on the PG-19 and WikiText-103 corpora, for a given amount of computation or memory.",2.0,Transformer+Recurrent Windows of Context,Unreleased,"United States of America,United States of America",GPT-2 (124M),153264000000000000,6 FLOP / token / parameter * 124000000 parameters * 103000000 tokens * 2 epochs = 1.53264e+17 FLOP,,,2025-04-08 13:17,,,,,,"Academia,Academia",,,,,Unreleased,,"Academia,Academia",,,,,Operation counting,,,,
ERNIE-GEN (large),Language,"Language modeling,Language modeling/generation",Baidu,"Dongling Xiao, Han Zhang, Yukun Li, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang",2020-08-06,ERNIE-GEN: An Enhanced Multi-Flow Pre-training and Fine-tuning Framework for Natural Language Generation,https://arxiv.org/abs/2001.11314,118.0,SOTA improvement,"""Empirically, ERNIE-GEN is particularly effective and
achieves state-of-the-art results on a range of NLG tasks
including abstractive summarization (Gigaword and CNN/DailyMail), question generation (SQuAD), dialogue response generation (Persona-Chat) and generative question answering (CoQA)""",340000000.0,"""We train a base model ERNIEGENBASE (L=12, H=768, A=12, Total Parameters=110M)1
and a large model ERNIE-GENLARGE (L=24, H=1024,
A=16, Total Parameters=340M) with parameters initialized
by BERTBASE and BERTLARGE respectively""",2e+20,"430GB text for 1 epoch

approx 430 * 200 million words = 86B words, or 100B tokens per https://docs.google.com/document/d/1G3vvQkn4x_W71MKg0GmHVtzfd9m0y3_Ofcoew0v902Q/edit#heading=h.ieihc08p8dn0

6 * 340 million params * 100 billion tokens ~= 2e20","CC-News,""BookCorpus (BooksCorpus, Toronto Book Corpus)"",WebText2,Wikipedia,C4","""Recent works for pre-training verify that larger scaled pretraining corpora can improve the performances on downstream tasks. We pre-train ERNIE-GENLARGE model on
the 430GB text corpora with 1 epoch and 1M training steps.
Our 430GB text corpora is extracted from the corpus used by
RoBERTa [Liu et al., 2019], T5 [Raffel et al., 2019] and ALBERT [Lan et al., 2020]. We fine-tune ERNIE-GENLARGE
on two abstractive summarization datasets including Gigaword and CNN/Daily Mail, the evaluation results are reported
in Table 9""

RoBERTa and T5 datasets are CC-News, BookCorpus, Wikipedia, WebText2, and C4",86000000000.0,"approx 430 * 200 million words = ~86B words, per https://docs.google.com/document/d/1G3vvQkn4x_W71MKg0GmHVtzfd9m0y3_Ofcoew0v902Q/edit#heading=h.ieihc08p8dn0",,,,,Speculative,,,,Open weights (non-commercial),China,,,,,,2025-05-28 16:13,,,,,,Industry,,,,,Open (non-commercial),"https://github.com/PaddlePaddle/ERNIE/tree/repro/ernie-gen

code/weights with unclear license",Industry,,,FP16,,Operation counting,,,,
DeLighT,Language,Language modeling,"University of Washington,Allen Institute for AI,Facebook AI Research","Sachin Mehta, Marjan Ghazvininejad, Srinivasan Iyer, Luke Zettlemoyer, Hannaneh Hajishirzi",2020-08-03,DeLighT: Deep and Light-weight Transformer,https://arxiv.org/abs/2008.00623,98.0,SOTA improvement,"""Comparison with state-of-the-art methods on machine translation corpora. DeLighT delivers
similar or better performance than state-of-the-art models with fewer parameters.""",99000000.0,99M (Table 4b),3.8016e+18,"6 FLOP / parameter / token * 99 * 10^6 parameters * 100000 steps * 64000 tokens per batch = 3.8016e+18 FLOP

31330000000000 FLOP / second / GPU * 8 GPUs * 30 hours [assumed based on smaller models reported training time] * 3600 sec / hour * 0.3 [assumed utilization] = 8.120736e+18 FLOP

Operation counting method uses less assumptions",WikiText-103,,103000000.0,"""100K iterations with a context length of 512 and an effective
batch size of 64K tokens.""

100000*64000/103000000 = 62.14 epochs",30.0,"Table 5 reports training time for 54M translation model (23h)
it should be more for the 99M language modeling model.",NVIDIA V100,,Likely,"We introduce a deep and light-weight transformer, DeLighT, that delivers similar or better performance than standard transformer-based models with significantly fewer parameters. DeLighT more efficiently allocates parameters both (1) within each Transformer block using the DeLighT transformation, a deep and light-weight transformation, and (2) across blocks using block-wise scaling, which allows for shallower and narrower DeLighT blocks near the input and wider and deeper DeLighT blocks near the output. Overall, DeLighT networks are 2.5 to 4 times deeper than standard transformer models and yet have fewer parameters and operations. Experiments on benchmark machine translation and language modeling tasks show that DeLighT matches or improves the performance of baseline Transformers with 2 to 3 times fewer parameters on average. Our source code is available at: \url{this https URL}",62.14,DeLight,Unreleased,"United States of America,United States of America,United States of America",,,,8.0,,2025-04-08 14:58,,,,64000.0,"""effective batch size of 64K tokens.""","Academia,Research collective,Industry",,,,,Open source,"MIT, training and evaluation for WT103: https://github.com/sacmehta/delight/blob/master/readme_files/lm/wikitext103.md ","Academia,Research collective,Industry",,,,4889.428515149248,"Operation counting,Hardware",,,,
mBART-50,Language,Translation,Facebook AI,"Yuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Naman Goyal, Vishrav Chaudhary, Jiatao Gu, Angela Fan",2020-08-02,"Multilingual Translation with Extensible Multilingual Pretraining and Finetuning
","https://arxiv.org/abs/2008.00401

https://huggingface.co/facebook/mbart-large-50",398.0,,,610000000.0,"610M
from https://github.com/facebookresearch/fairseq/tree/main/examples/mbart",1.4515200000000002e+22,"flops = (256) * (125000000000000) * (2.5 * 7 * 24 * 3600) * (0.3) = 1.45152e+22
(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)


""mBART trained for 2.5 weeks on 256 Nvidia V100 GPUs""
V100 have peak flop 28.26 TFLOPS  from https://www.techpowerup.com/gpu-specs/tesla-v100-pcie-16-gb.c2957",,"multiple sources
Table 6 in the appendix
additionally XLMR from section 5.1
",,"multiple sources
203686055 sentences - summing column 2 in Table 6 in the appendixadditionally XLMR from section 5.1
There are multiple languages so it is hard to estimate number of words.",420.0,"""mBART trained for 2.5 weeks on 256 Nvidia V100 GPUs""",NVIDIA V100,,Confident,"Recent work demonstrates the potential of multilingual pretraining of creating one model that can be used for various tasks in different languages. Previous work in multilingual pretraining has demonstrated that machine translation systems can be created by finetuning on bitext. In this work, we show that multilingual translation models can be created through multilingual finetuning. Instead of finetuning on one direction, a pretrained model is finetuned on many directions at the same time. Compared to multilingual models trained from scratch, starting from pretrained models incorporates the benefits of large quantities of unlabeled monolingual data, which is particularly important for low resource languages where bitext is not available. We demonstrate that pretrained models can be extended to incorporate additional languages without loss of performance. We double the number of languages in mBART to support multilingual machine translation models of 50 languages. Finally, we create the ML50 benchmark, covering low, mid, and high resource languages, to facilitate reproducible research by standardizing training and evaluation data. On ML50, we demonstrate that multilingual finetuning improves on average 1 BLEU over the strongest baselines (being either multilingual from scratch or bilingual finetuning) while improving 9.3 BLEU on average over bilingual baselines from scratch. ",,,Open weights (unrestricted),United States of America,,,,256.0,,2025-02-17 12:14,,,,,,Industry,,,,107520.0,,"Repo has MIT license:

https://github.com/facebookresearch/fairseq/tree/main/examples/mbart",Industry,,,,156465.19682753857,Hardware,,,,
Grown to Prune Two-layer stacked LSTM,Language,Language modeling,"University of Chicago,Toyota Technological Institute at Chicago","Xin Yuan, Pedro Savarese, Michael Maire",2020-07-30,Growing Efficient Deep Networks by Structured Continuous Sparsification,https://arxiv.org/abs/2007.15353,39.0,,,,,,,,,,,,,,,Unknown,"We develop an approach to growing deep network architectures over the course of training, driven by a principled combination of accuracy and sparsity objectives. Unlike existing pruning or architecture search techniques that operate on full-sized models or supernet architectures, our method can start from a small, simple seed architecture and dynamically grow and prune both layers and filters. By combining a continuous relaxation of discrete network structure optimization with a scheme for sampling sparse subnetworks, we produce compact, pruned networks, while also drastically reducing the computational expense of training. For example, we achieve 49.7% inference FLOPs and 47.4% training FLOPs savings compared to a baseline ResNet-50 on ImageNet, while maintaining 75.2% top-1 accuracy -- all without any dedicated fine-tuning stage. Experiments across CIFAR, ImageNet, PASCAL VOC, and Penn Treebank, with convolutional networks for image classification and semantic segmentation, and recurrent networks for language modeling, demonstrate that we both train faster and produce more efficient networks than competing architecture pruning or search methods.",,Grown to Prune Two-layer stacked LSTM,Unreleased,"United States of America,United States of America",,,,,,2024-11-01 10:03,,,,,,"Academia,Academia",,,,,Unreleased,,"Academia,Academia",,,,,,,,,
TransformerXL-LayerFusion-CA,Language,Language modeling,"University of Liverpool,University of Southern California","James O' Neill, Greg Ver Steeg, Aram Galstyan",2020-07-29,Compressing Deep Neural Networks via Layer Fusion,https://arxiv.org/abs/2007.14917,5.0,,,,,,,,,,,,,,,Unknown,"This paper proposes \textit{layer fusion} - a model compression technique that discovers which weights to combine and then fuses weights of similar fully-connected, convolutional and attention layers. Layer fusion can significantly reduce the number of layers of the original network with little additional computation overhead, while maintaining competitive performance. From experiments on CIFAR-10, we find that various deep convolution neural networks can remain within 2\% accuracy points of the original networks up to a compression ratio of 3.33 when iteratively retrained with layer fusion. For experiments on the WikiText-2 language modelling dataset where pretrained transformer models are used, we achieve compression that leads to a network that is 20\% of its original size while being within 5 perplexity points of the original network. We also find that other well-established compression techniques can achieve competitive performance when compared to their original networks given a sufficient number of retraining steps. Generally, we observe a clear inflection point in performance as the amount of compression increases, suggesting a bound on the amount of compression that can be achieved before an exponential degradation in performance.",,TransformerXL-LayerFusion-CA,Unreleased,"United Kingdom of Great Britain and Northern Ireland,United States of America",,,,,,2024-09-09 13:40,,,,,,"Academia,Academia",,,,,Unreleased,,"Academia,Academia",,,,,,,,,
GPT2-LayerFusion-WS,Language,Language modeling,"University of Liverpool,University of Southern California","James O' Neill, Greg Ver Steeg, Aram Galstyan",2020-07-29,Compressing Deep Neural Networks via Layer Fusion,https://arxiv.org/abs/2007.14917,5.0,,,,,,,,,,,,,,,Unknown,"This paper proposes \textit{layer fusion} - a model compression technique that discovers which weights to combine and then fuses weights of similar fully-connected, convolutional and attention layers. Layer fusion can significantly reduce the number of layers of the original network with little additional computation overhead, while maintaining competitive performance. From experiments on CIFAR-10, we find that various deep convolution neural networks can remain within 2\% accuracy points of the original networks up to a compression ratio of 3.33 when iteratively retrained with layer fusion. For experiments on the WikiText-2 language modelling dataset where pretrained transformer models are used, we achieve compression that leads to a network that is 20\% of its original size while being within 5 perplexity points of the original network. We also find that other well-established compression techniques can achieve competitive performance when compared to their original networks given a sufficient number of retraining steps. Generally, we observe a clear inflection point in performance as the amount of compression increases, suggesting a bound on the amount of compression that can be achieved before an exponential degradation in performance.",,GPT2-LayerFusion-WS,Unreleased,"United Kingdom of Great Britain and Northern Ireland,United States of America",,,,,,2024-09-09 13:41,,,,,,"Academia,Academia",,,,,Unreleased,,"Academia,Academia",,,,,,,,,
Hopfield Networks (2020),"Biology,Vision,Language,Medicine","Drug discovery,Language modeling,Object recognition","Johannes Kepler University Linz,Institute of Advanced Research in Artificial Intelligence,University of Oslo","Hubert Ramsauer, Bernhard SchÃ¤fl, Johannes Lehner, Philipp Seidl, Michael Widrich, Thomas Adler, Lukas Gruber, Markus Holzleitner, Milena PavloviÄ‡, Geir Kjetil Sandve, Victor Greiff, David Kreil, Michael Kopp, GÃ¼nter Klambauer, Johannes Brandstetter, Sepp Hochreiter",2020-07-16,Hopfield Networks is All You Need,https://arxiv.org/abs/2008.02217,345.0,SOTA improvement,"""Hopfield layers yielded a new state-ofthe-art when compared to different machine learning methods. Finally, Hopfield
layers achieved state-of-the-art on two drug design datasets""",,,,,"BACE,SIDER","""We test the Hopfield layer HopfieldLayer, on four drug
design datasets. These datasets represent four main areas of modeling tasks in drug design, concretely
to develop accurate models for predicting a) new anti-virals (HIV) by the Drug Therapeutics Program
(DTP) AIDS Antiviral Screen, b) new protein inhibitors, concretely human Î²-secretase (BACE) inhibitors by Subramanian et al. (2016), c) metabolic effects as blood-brain barrier permeability (BBBP)
(Martins et al., 2012) and d) side effects of a chemical compound from the Side Effect Resource
(SIDER) Kuhn et al. (2016). """,,,,,,,Unknown,"We introduce a modern Hopfield network with continuous states and a corresponding update rule. The new Hopfield network can store exponentially (with the dimension of the associative space) many patterns, retrieves the pattern with one update, and has exponentially small retrieval errors. It has three types of energy minima (fixed points of the update): (1) global fixed point averaging over all patterns, (2) metastable states averaging over a subset of patterns, and (3) fixed points which store a single pattern. The new update rule is equivalent to the attention mechanism used in transformers. This equivalence enables a characterization of the heads of transformer models. These heads perform in the first layers preferably global averaging and in higher layers partial averaging via metastable states. The new modern Hopfield network can be integrated into deep learning architectures as layers to allow the storage of and access to raw input data, intermediate results, or learned prototypes. These Hopfield layers enable new ways of deep learning, beyond fully-connected, convolutional, or recurrent networks, and provide pooling, memory, association, and attention mechanisms. We demonstrate the broad applicability of the Hopfield layers across various domains. Hopfield layers improved state-of-the-art on three out of four considered multiple instance learning problems as well as on immune repertoire classification with several hundreds of thousands of instances. On the UCI benchmark collections of small classification tasks, where deep learning methods typically struggle, Hopfield layers yielded a new state-of-the-art when compared to different machine learning methods. Finally, Hopfield layers achieved state-of-the-art on two drug design datasets. The implementation is available at: this https URL",,,Open weights (unrestricted),"Austria,Austria,Norway",,,,,,2025-04-30 10:04,,,,,,"Academia,Academia,Academia",,,,,Unreleased,"copyleft-like license, derivative works must retain this license. code here:
https://github.com/ml-jku/hopfield-layers/blob/master/LICENSE","Academia,Academia,Academia",,,,,,,,,
SemExp,Robotics,Object detection,"Carnegie Mellon University (CMU),Facebook AI Research","Devendra Singh Chaplot, Dhiraj Gandhi, Abhinav Gupta, Ruslan Salakhutdinov",2020-07-02,Object Goal Navigation using Goal-Oriented Semantic Exploration,https://proceedings.neurips.cc/paper/2020/file/2c75cf2681788adaca63aa95ae028b22-Paper.pdf,428.0,SOTA improvement,"""Our method achieves state-of-the-art performance on the object goal navigation task and won the CVPR2020 Habitat ObjectNav challenge""",,,,,"Gibson,Matterport3D (MP3D)","""We use the Gibson [46] and Matterport3D (MP3D) [6] datasets""",,"""Our training and test set consists of a total of 86 scenes (25 Gibson tiny and 61 MP3D) and 16 scenes (5 Gibson tiny and 11 MP3D), respectively""",,,,Reinforcement learning,Unknown,"This work studies the problem of object goal navigation which involves navigating to an instance of the given object category in unseen environments. End-to-end learning-based navigation methods struggle at this task as they are ineffective at exploration and long-term planning. We propose a modular system called, â€˜GoalOriented Semantic Explorationâ€™ which builds an episodic semantic map and uses it to explore the environment efficiently based on the goal object category. Empirical results in visually realistic simulation environments show that the proposed model outperforms a wide range of baselines including end-to-end learning-based methods as well as modular map-based methods and led to the winning entry of the CVPR2020 Habitat ObjectNav Challenge. Ablation analysis indicates that the proposed model learns semantic priors of the relative arrangement of objects in a scene, and uses them to explore efficiently. Domain-agnostic module design allows us to transfer our model to a mobile robot platform and achieve similar performance for object goal navigation in the real-world.",,,Open weights (unrestricted),"United States of America,United States of America",,,,,,2025-05-09 11:32,,,,,,"Academia,Industry",,,,,Open source,MIT code/weights: https://github.com/devendrachaplot/Object-Goal-Navigation,"Academia,Industry",,,,,,,,,
GShard (dense),Language,Translation,Google,"Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, Zhifeng Chen",2020-06-30,GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding,https://arxiv.org/abs/2006.16668,881.0,SOTA improvement,"""such a giant model can efficiently be trained on 2048 TPU v3 accelerators in 4 days to achieve far superior quality for translation from 100 languages to English compared to the prior art""",2300000000.0,"""Our best quality dense single Transformer model (2.3B parameters) achieving âˆ†BLEU of 6.1, was trained with GPipe [15] on 2048 TPU v3 cores for 6 weeks or total of 235.5 TPU v3 core-years.""",4.765e+22,"Trained for a total of 235.5 TPU v3 core-years.
Hardware estimate: 235.5 * 365.25 * 24 * 3600 * (1.23e14 / 2) * 0.3 = 1.371e23

Footnote 10 indicates 300k steps and 4M tokens/step -> 1.2T tokens
Arithmetic estimate: 6 * 2.3B * 1.2T = 1.656e22 FLOPs

Geometric mean: sqrt(1.371e23 * 1.656e22) = 4.765e22",,,346666666667.0,"""We focus on improving the translation quality (measured in terms of BLEU score [48]) from all 100 languages to English. This resulted in approximately 13 billion training examples to be used for model training""

Each example is a sentence pair. Assuming 20 words per sentence and 4/3 tokens per word, that is 13*20*4/3 billion tokens",1008.0,6 weeks = 1008 hours,Google TPU v3,Self-supervised learning,Confident,"Neural network scaling has been critical for improving the model quality in many real-world machine learning applications with vast amounts of training data and compute. Although this trend of scaling is affirmed to be a sure-fire approach for better model quality, there are challenges on the path such as the computation cost, ease of programming, and efficient implementation on parallel devices. GShard is a module composed of a set of lightweight annotation APIs and an extension to the XLA compiler. It provides an elegant way to express a wide range of parallel computation patterns with minimal changes to the existing model code. GShard enabled us to scale up multilingual neural machine translation Transformer model with Sparsely-Gated Mixture-of-Experts beyond 600 billion parameters using automatic sharding. We demonstrate that such a giant model can efficiently be trained on 2048 TPU v3 accelerators in 4 days to achieve far superior quality for translation from 100 languages to English compared to the prior art.",,,Unreleased,United States of America,,,,1024.0,,2025-05-28 16:14,,,,4000000.0,"Table 3, bolded row is best model",Industry,,,,1032192.0,Open source,"training code is open, Apache: https://github.com/tensorflow/lingvo/tree/master/lingvo/tasks/lm",Industry,$256224.77,"Trained for a total of 235.5 TPU v3 core-years.
Hardware: 235.5 * 365.25 * 24 * 3600 * (1.23e14 / 2) = 4.571e23 at full utilization

Footnote 10 indicates 300k steps and 4M tokens/step -> 1.2T tokens
6ND: 6 * 2.3B * 1.2T = 1.656e22 FLOPs

Implied utilization: 1.656e22 / 4.571e23 = 0.036
This seems implausibly low, but it's not clear where a mistake could have entered the calculation.",FP32,459301.9894608488,"Operation counting,Hardware",,,,7
GPT-3 6.7B,Language,"Text autocompletion,Language modeling/generation",OpenAI,"Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei",2020-06-22,Language Models are Few-Shot Learners,https://arxiv.org/abs/2005.14165,,,,6660000000.0,"6.7B
",1.2e+22,"Table D.1
https://arxiv.org/abs/2005.14165","Common Crawl,WebText2,Wikipedia,Books1,Books2","Table 2.2 (other datasets also used)

Page 8 of the linked paper says the model was trained â€œ41 shards of monthly CommonCrawl covering 2016 to 2019â€. The paper was published on May 28, 2020, so I assume based on the paperâ€™s description of the training data that it goes up to December 31, 2019.",300000000000.0,"300b, per table d.1",,,NVIDIA Tesla V100 DGXS 32 GB,,Confident,"Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",,,,United States of America,,,,,,2025-05-07 16:30,Microsoft,,,2000000.0,"2M, per table 2.1",Industry,,,,,,,Industry,,,,,Reported,,,,
GPT-3 XL,Language,"Text autocompletion,Language modeling/generation",OpenAI,"Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei",2020-06-22,Language Models are Few-Shot Learners,https://arxiv.org/abs/2005.14165,,,,1320000000.0, 1.3B,2.38e+21,"Table D.1
https://arxiv.org/abs/2005.14165","Common Crawl,WebText2,Wikipedia,Books1,Books2","Table 2.2 (other datasets also used)

Page 8 of the linked paper says the model was trained â€œ41 shards of monthly CommonCrawl covering 2016 to 2019â€. The paper was published on May 28, 2020, so I assume based on the paperâ€™s description of the training data that it goes up to December 31, 2019.",300000000000.0,"300b, per table d.1",,,NVIDIA Tesla V100 DGXS 32 GB,,Confident,"Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",,,,United States of America,,,,,,2025-05-07 16:31,Microsoft,,,1000000.0,"1M, per table 2.1",Industry,,,,,,,Industry,,,,,Reported,,,,
GPT-3 Small,Language,"Text autocompletion,Language modeling/generation",OpenAI,"Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei",2020-06-22,Language Models are Few-Shot Learners,https://arxiv.org/abs/2005.14165,,,,125000000.0,125M,2.25e+20,"Table D.1
https://arxiv.org/abs/2005.14165","Common Crawl,WebText2,Wikipedia,Books1,Books2","Table 2.2 (other datasets also used)

Page 8 of the linked paper says the model was trained â€œ41 shards of monthly CommonCrawl covering 2016 to 2019â€. The paper was published on May 28, 2020, so I assume based on the paperâ€™s description of the training data that it goes up to December 31, 2019.",300000000000.0,"300b, per table d.1",,,NVIDIA Tesla V100 DGXS 32 GB,,Confident,"Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",,,,United States of America,,,,,,2025-05-07 16:32,Microsoft,,,500000.0," 0.5M, per table 2.1",Industry,,,,,,,Industry,,,,,Reported,,,,
GPT-3 Medium,Language,"Text autocompletion,Language modeling/generation",OpenAI,"Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei",2020-06-22,Language Models are Few-Shot Learners,https://arxiv.org/abs/2005.14165,,,,356000000.0,350M,6.41e+20,"Table D.1
https://arxiv.org/abs/2005.14165","Common Crawl,WebText2,Wikipedia,Books1,Books2","Table 2.2 (other datasets also used)

Page 8 of the linked paper says the model was trained â€œ41 shards of monthly CommonCrawl covering 2016 to 2019â€. The paper was published on May 28, 2020, so I assume based on the paperâ€™s description of the training data that it goes up to December 31, 2019.",300000000000.0,"300b, per table d.1",,,NVIDIA Tesla V100 DGXS 32 GB,,Confident,"Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",,,,United States of America,,,,,,2025-05-07 16:32,Microsoft,,,500000.0," 0.5M, per table 2.1",Industry,,,,,,,Industry,,,,,Reported,,,,
GPT-3 Large,Language,"Text autocompletion,Language modeling/generation",OpenAI,"Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei",2020-06-22,Language Models are Few-Shot Learners,https://arxiv.org/abs/2005.14165,,,,760000000.0,760M,1.37e+21,"Table D.1
https://arxiv.org/abs/2005.14165","Common Crawl,WebText2,Wikipedia,Books1,Books2","Table 2.2 (other datasets also used)

Page 8 of the linked paper says the model was trained â€œ41 shards of monthly CommonCrawl covering 2016 to 2019â€. The paper was published on May 28, 2020, so I assume based on the paperâ€™s description of the training data that it goes up to December 31, 2019.",300000000000.0,"300b, per table d.1",,,NVIDIA Tesla V100 DGXS 32 GB,,Confident,"Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",,,,United States of America,,,,,,2025-05-07 16:32,Microsoft,,,500000.0," 0.5M, per table 2.1",Industry,,,,,,,Industry,,,,,Reported,,,,
GPT-3 2.7B,Language,"Text autocompletion,Language modeling/generation",OpenAI,"Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei",2020-06-22,Language Models are Few-Shot Learners,https://arxiv.org/abs/2005.14165,,,,2650000000.0,"2.7B
",4.77e+21,"Table D.1
https://arxiv.org/abs/2005.14165","Common Crawl,WebText2,Wikipedia,Books1,Books2","Table 2.2 (other datasets also used)

Page 8 of the linked paper says the model was trained â€œ41 shards of monthly CommonCrawl covering 2016 to 2019â€. The paper was published on May 28, 2020, so I assume based on the paperâ€™s description of the training data that it goes up to December 31, 2019.",300000000000.0,"300b, per table d.1",,,NVIDIA Tesla V100 DGXS 32 GB,,Confident,"Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",,,,United States of America,,,,,,2025-05-07 16:32,Microsoft,,,2000000.0,"2M, per table 2.1",Industry,,,,,,,Industry,,,,,Reported,,,,
GPT-3 13B,Language,"Text autocompletion,Language modeling/generation",OpenAI,"Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei",2020-06-22,Language Models are Few-Shot Learners,https://arxiv.org/abs/2005.14165,,,,12850000000.0,13B,2.31e+22,"Table D.1
https://arxiv.org/abs/2005.14165","Common Crawl,WebText2,Wikipedia,Books1,Books2","Table 2.2 (other datasets also used)

Page 8 of the linked paper says the model was trained â€œ41 shards of monthly CommonCrawl covering 2016 to 2019â€. The paper was published on May 28, 2020, so I assume based on the paperâ€™s description of the training data that it goes up to December 31, 2019.",300000000000.0,"300b, per table d.1",,,NVIDIA Tesla V100 DGXS 32 GB,,Confident,"Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",,,,United States of America,,,,,,2025-05-07 16:33,Microsoft,,,1000000.0,"1M , per table 2.1",Industry,,,,,,,Industry,,,,,Reported,,,,
iGPT-XL,"Vision,Image generation",Image completion,OpenAI,"Mark Chen, Alec Radford, Rewon Child, Jeff Wu, Heewoo Jun, Prafulla Dhariwal, David Luan, Ilya Sutskever",2020-06-17,Generative Pretraining from Pixels,https://openai.com/research/image-gpt,1339.0,Highly cited,,6801000000.0,source: https://openai.com/blog/image-gpt/#rfref53,3.3e+22,"Taken from here
https://www.lesswrong.com/posts/wfpdejMWog4vEDLDg/ai-and-compute-trend-isn-t-predictive-of-what-is-happening (""There's no compute data for the largest model, iGPT-XL. But based on the FLOP/s increase from GPT-3 XL (same num of params as iGPT-L) to GPT-3 6.7B (same num of params as iGPT-XL), I think it required 5 times more compute: 3.3 * 10^22 FLOP."")",ILSVRC 2012 subset of ImageNet,,1229920.0,"""We use the ImageNet ILSVRC 2012 training dataset, splitting off 4% as our experimental validation set and report results on the ILSVRC 2012 validation set as our test set.""

https://image-net.org/challenges/LSVRC/2012/

""The goal of this competition is to estimate the content of photographs for the purpose of retrieval and automatic annotation using a subset of the large hand-labeled ImageNet dataset (10,000,000 labeled images depicting 10,000+ object categories) as training.""
",,,NVIDIA Tesla V100 DGXS 32 GB,Self-supervised learning,Likely,"Inspired by progress in unsupervised representation learning for natural language, we examine whether similar models can learn useful representations for images. We train a sequence Transformer to auto-regressively predict pixels, without incorporating knowledge of the 2D input structure. Despite training on low-resolution ImageNet without labels, we find that a GPT-2 scale model learns strong image representations as measured by linear probing, fine-tuning, and low-data classification. On CIFAR-10, we achieve 96.3% accuracy with a linear probe, outperforming a supervised Wide ResNet, and 99.0% accuracy with full finetuning, matching the top supervised pre-trained models. An even larger model trained on a mixture of ImageNet and web images is competitive with self-supervised benchmarks on ImageNet, achieving 72.0% top-1 accuracy on a linear probe of our features.",,,Open weights (unrestricted),United States of America,,,,,,2025-06-18 11:16,,,,,,Industry,,,,,Open source,"Modified MIT, code and weights:

https://github.com/openai/image-gpt?tab=License-1-ov-file#readme

train code: https://github.com/openai/image-gpt/blob/master/src/run.py ",Industry,$98082.34,,,,Third-party estimation,,,,9
iGPT-L,"Image generation,Vision",Image completion,OpenAI,"Mark Chen, Alec Radford, Rewon Child, Jeff Wu, Heewoo Jun, Prafulla Dhariwal, David Luan, Ilya Sutskever",2020-06-17,Generative Pretraining from Pixels,https://openai.com/blog/image-gpt/,1339.0,Highly cited,,1362000000.0,source: https://openai.com/blog/image-gpt/#rfref53,8.91e+21,"We have that ""iGPT-L was trained for roughly 2500 V100-days"" [1]

I assume this is the NVIDIA Tesla V100 GPU. In the specifications, the NVIDIA Tesla V100 has 7 to 8.2 TFLOPS of peak double precision performance and 14 to 16.4 TFLOPS of peak single precision performance and 112 to 130 TFLOPS of peak tensor performance [2].

I suppose the one that makes sense using if peak tensor performance, for ~125 TFLOPS peak tensor performance more or less.
Following OpenAIs AI and compute we apply a 0.33 utitilization factor [3].

In total we get 2500 V100-days * (24*60*60) seconds/day * 125 TFLOPS * 0.33 = 8.91e+21 FLOPS = 89.1 PF-days.

[1] https://openai.com/blog/image-gpt/
[2] https://images.nvidia.com/content/technologies/volta/pdf/volta-v100-datasheet-update-us-1165301-r5.pdf
[3] https://openai.com/blog/ai-and-compute/",ILSVRC 2012 subset of ImageNet,,1229920.0,"""We use the ImageNet ILSVRC 2012 training dataset, splitting off 4% as our experimental validation set and report results on the ILSVRC 2012 validation set as our test set.""

https://image-net.org/challenges/LSVRC/2012/

""The goal of this competition is to estimate the content of photographs for the purpose of retrieval and automatic annotation using a subset of the large hand-labeled ImageNet dataset (10,000,000 labeled images depicting 10,000+ object categories) as training.""
",,,NVIDIA Tesla V100 DGXS 32 GB,Self-supervised learning,Confident,"Inspired by progress in unsupervised representation learning for natural language, we examine whether similar models can learn useful representations for images. We train a sequence Transformer to auto-regressively predict pixels, without incorporating knowledge of the 2D input structure. Despite training on low-resolution ImageNet without labels, we find that a GPT-2 scale model learns strong image representations as measured by linear probing, fine-tuning, and low-data classification. On CIFAR-10, we achieve 96.3% accuracy with a linear probe, outperforming a supervised Wide ResNet, and 99.0% accuracy with full finetuning, matching the top supervised pre-trained models. An even larger model trained on a mixture of ImageNet and web images is competitive with self-supervised benchmarks on ImageNet, achieving 72.0% top-1 accuracy on a linear probe of our features.",,,Open weights (unrestricted),United States of America,,,,,,2025-06-18 11:08,,,,,,Industry,,,,60000.0,Open source,modified MIT: https://github.com/openai/image-gpt?tab=License-1-ov-file#readme,Industry,$30093.44,,,,Hardware,,,,17
6-Layer-Tensor-Transformer+AdaHessian,Language,Language modeling,"""NERSC, Lawrence Berkeley National Laboratory"",University of California (UC) Berkeley","Zhewei Yao, Amir Gholami, Sheng Shen, Mustafa Mustafa, Kurt Keutzer, Michael W. Mahoney",2020-06-01,ADAHESSIAN: An Adaptive Second Order Optimizer for Machine Learning,https://arxiv.org/abs/2006.00719,229.0,,"""We show that AdaHessian achieves new state-of-the-art results by a large margin as compared
to other adaptive optimization methods, including variants of Adam""",85500000.0,""" Following [35], a three-layer tensorized transformer core-1 for PTB and a six-layer tensorized transformer core-1 for Wikitext-103 are used in the experiments""

[35] Ma, X.; Zhang, P.; Zhang, S.; Duan, N.; Hou, Y.; Zhou,
M.; and Song, D. 2019. A tensorized transformer for
language modeling. In Advances in Neural Information
Processing Systems, 2229â€“2239.

https://arxiv.org/pdf/1906.09777

Tensorized transformer has 85.5M parameters",1.58e+18,6 FLOP / parameter / token * 85500000 parameters * 103000000 tokens * 30 epochs = 1.58517e+18 FLOP,WikiText-103,,103000000.0,"batch size 120
10^7 steps (Figure 10)",,,,,Confident,"We introduce ADAHESSIAN, a second order stochastic optimization algorithm which dynamically incorporates the curvature of the loss function via ADAptive estimates of the HESSIAN. Second order algorithms are among the most powerful optimization algorithms with superior convergence properties as compared to first order methods such as SGD and Adam. The main disadvantage of traditional second order methods is their heavier per iteration computation and poor accuracy as compared to first order methods. To address these, we incorporate several novel approaches in ADAHESSIAN, including: (i) a fast Hutchinson based method to approximate the curvature matrix with low computational overhead; (ii) a root-mean-square exponential moving average to smooth out variations of the Hessian diagonal across different iterations; and (iii) a block diagonal averaging to reduce the variance of Hessian diagonal elements. We show that ADAHESSIAN achieves new state-of-the-art results by a large margin as compared to other adaptive optimization methods, including variants of Adam. In particular, we perform extensive tests on CV, NLP, and recommendation system tasks and find that ADAHESSIAN: (i) achieves 1.80%/1.45% higher accuracy on ResNets20/32 on Cifar10, and 5.55% higher accuracy on ImageNet as compared to Adam; (ii) outperforms AdamW for transformers by 0.13/0.33 BLEU score on IWSLT14/WMT14 and 2.7/1.0 PPL on PTB/Wikitext-103; (iii) outperforms AdamW for SqueezeBert by 0.41 points on GLUE; and (iv) achieves 0.032% better score than Adagrad for DLRM on the Criteo Ad Kaggle dataset. Importantly, we show that the cost per iteration of ADAHESSIAN is comparable to first order methods, and that it exhibits robustness towards its hyperparameters.",30.0,6-Layer-Tensor-Transformer+AdaHessian,Unreleased,"United States of America,United States of America",,,,,,2025-04-09 11:59,,,,,,"Government,Academia",,,,,Open source,MIT for code. doesn't have training script for WT103 but looks fairly adaptable: https://github.com/amirgholami/ADAHESSIAN ,"Government,Academia",,,,,Operation counting,,,,
GPT-3 175B (davinci),Language,"Text autocompletion,Language modeling/generation",OpenAI,"Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei",2020-05-28,Language Models are Few-Shot Learners,https://arxiv.org/abs/2005.14165,32643.0,"Highly cited,Training cost",,175000000000.0,"""we train GPT-3, an autoregressive language model with 175 billion parameters""",3.14e+23,"Table D.1
https://arxiv.org/abs/2005.14165","Common Crawl,WebText2,Wikipedia,Books1,Books2","Table 2.2 (other datasets also used)

Page 8 of the paper linked above says the model was trained â€œ41 shards of monthly CommonCrawl covering 2016 to 2019â€. The paper was published on May 28, 2020, so I assume based on the paperâ€™s description of the training data that it goes up to December 31, 2019.",374000000000.0,"From table 2.2, we determine that there are 410 + 19 + 12 + 55 + 3 = 499 billion tokens. 

We multiply this by 0.75 to give 374B words. 

3.74e11

========================
[Anson: I think the calculation below doesn't look at all the data, the CommonCrawl data only constitutes 60% of the data. Multiplying by 5/3 gives 4.75e11]

""The CommonCrawl data was downloaded from 41 shards of monthly CommonCrawl covering 2016 to 2019, constituting 45TB of compressed plaintext before filtering and 570GB after filtering, roughly equivalent to 400 billion byte-pair-encoded tokens. ""

Converted to words using 
http://extraconversion.com/data-storage/gigabits/gigabits-to-words.html

2.85e11",355.2,14.8 days according to https://arxiv.org/ftp/arxiv/papers/2104/2104.10350.pdf,NVIDIA Tesla V100 DGXS 32 GB,Self-supervised learning,Confident,"Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",0.6,GPT-3 175B (davinci),API access,United States of America,,,,10000.0,0.1968,2025-05-16 10:30,Microsoft,,,3200000.0,"3.2M, per table 2.1",Industry,checked,,,3552000.0,Unreleased,"https://openai.com/blog/openai-api
",Industry,$2056969.34,"""The paper states that they had a peak FLOP for a V100 at 125 TFLOP/s, while they achieved 24.6, which results in a utilization of 0.1968"" -Alexander Erben",,5100759.605963441,Reported,,,,2
GPT3-6.7B (rerun of original),Language,Language modeling/generation,"Microsoft,OpenAI","Greg Yang, Edward J. Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ryder, Jakub Pachocki, Weizhu Chen, Jianfeng Gao",2020-05-28,Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer,https://arxiv.org/abs/2203.03466,120.0,,,6700000000.0,6.7B,1.2e+22,6 FLOP / token / parameter * 6.7 * 10^9 parameters * 300 * 10^9 tokens = 1.206e+22 FLOP,,"Page 8 of https://arxiv.org/pdf/2005.14165 says the model was trained â€œ41 shards of monthly CommonCrawl covering 2016 to 2019â€. The paper was published on May 28, 2020, so I assume based on the paperâ€™s description of the training data that it goes up to December 31, 2019.",300000000000.0,"300B tokens (Figure 15) - same as original gpt-3
10K training steps",,,NVIDIA V100,,Confident,"Hyperparameter (HP) tuning in deep learning is an expensive process, prohibitively so for neural networks (NNs) with billions of parameters. We show that, in the recently discovered Maximal Update Parametrization (muP), many optimal HPs remain stable even as model size changes. This leads to a new HP tuning paradigm we call muTransfer: parametrize the target model in muP, tune the HP indirectly on a smaller model, and zero-shot transfer them to the full-sized model, i.e., without directly tuning the latter at all. We verify muTransfer on Transformer and ResNet. For example, 1) by transferring pretraining HPs from a model of 13M parameters, we outperform published numbers of BERT-large (350M parameters), with a total tuning cost equivalent to pretraining BERT-large once; 2) by transferring from 40M parameters, we outperform published numbers of the 6.7B GPT-3 model, with tuning cost only 7% of total pretraining cost. A Pytorch implementation of our technique can be found at this http URL and installable via `pip install mup`.",1.0,GPT3-6.7B (rerun of original),Unreleased,"United States of America,Multinational,India,Belgium,United States of America",,,,,,2025-05-07 16:27,,,GPT3-6.7B (rerun of original),,,"Industry,Industry",,,,,Unreleased,"repo here, don't think there's GPT-3 code though https://github.com/microsoft/mup/blob/main/README.md ","Industry,Industry",,,FP32,,Operation counting,,,,
DETR,Vision,Object detection,Facebook,"Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko",2020-05-26,End-to-End Object Detection with Transformers,https://arxiv.org/abs/2005.12872,10594.0,Highly cited,,60000000.0,60M per Table 1,4e+20,"""Training the baseline model for 300 epochs on 16 V100 GPUs takes 3 days, with 4 images per GPU (hence a total batch size of 64). For the longer schedule used to compare with Faster R-CNN we train for 500 epochs with learning rate drop after 400 epochs. This schedule adds 1.5 AP compared to the shorter schedule.""

48 V100-days for baseline DETR model. Larger model had 1.5x the params and 5/3 as many epochs, so required ~2.5x as much training compute.

125 teraflop/s * 2.5 * 48 * 24 * 3600 * 0.3 (assumed utilization) ~ 4e20",COCO 2017,"""We perform experiments on COCO 2017 detection and panoptic segmentation datasets [24,18], containing 118k training images and 5k validation images""",123000.0,,,,NVIDIA V100,Supervised,Confident,"Abstract. We present a new method that views object detection as a
direct set prediction problem. Our approach streamlines the detection
pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation
that explicitly encode our prior knowledge about the task. The main
ingredients of the new framework, called DEtection TRansformer or
DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given
a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output
the final set of predictions in parallel. The new model is conceptually
simple and does not require a specialized library, unlike many other
modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster RCNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation
in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at
https://github.com/facebookresearch/detr.",500.0,,Open weights (unrestricted),United States of America,,,,,,2025-05-09 11:32,,,,64.0,,Industry,,,,,Open source,Apache 2.0: https://github.com/facebookresearch/detr,Industry,$959.91,,,,Hardware,,,,
Retrieval-Augmented Generator,Language,Question answering,"Facebook,New York University (NYU),University College London (UCL)","Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich KÃ¼ttler, Mike Lewis, Wen-tau Yih, Tim RocktÃ¤schel, Sebastian Riedel, Douwe Kiela",2020-05-22,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,https://arxiv.org/abs/2005.11401v4,3572.0,"Highly cited,SOTA improvement","""Our RAG models achieve state-of-the-art results on open Natural Questions [29], WebQuestions [3] and CuratedTrec [2] """,626000000.0,"""Our RAG models contain the trainable parameters for the BERT-base query and document encoder of DPR, with 110M parameters each (although we do not train the document encoder ourselves) and 406M trainable parameters from BART-large, 406M parameters, making a total of 626M trainable parameters""",,"not enough info, e.g. no training time reported:

""We train with mixed precision floating point arithmetic [40], distributing training across 8, 32GB NVIDIA V100 GPUs, though training and inference can be run on one GPU""","Wikipedia,NQ (Natural Questions)",,,,,,NVIDIA Tesla V100 PCIe 32 GB,,Confident,"Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.",,,Open weights (unrestricted),"United States of America,United States of America,United Kingdom of Great Britain and Northern Ireland",,,,,,2025-05-28 16:14,,,,,,"Industry,Academia,Academia",,,,,Unreleased,"It's in HF transformers library:
https://huggingface.co/docs/transformers/en/model_doc/rag

this library has an apache license: https://github.com/huggingface/transformers/blob/main/LICENSE","Industry,Academia,Academia",,,FP16,,,,,,
rTop-k(distributed setting),Language,Language modeling,Stanford University,"Leighton Pate Barnes, Huseyin A. Inan, Berivan Isik, Ayfer Ozgur",2020-05-21,rTop-k: A Statistical Estimation Approach to Distributed SGD,https://arxiv.org/abs/2005.10761,56.0,,,69000000.0," We adopted a 2-layer LSTM language model architec- ture with 1500 hidden units per layer

Okay, let's break this down:

- You have a 2-layer LSTM model
- Each LSTM layer has 1500 hidden units
- The vocabulary size is 10k

For each LSTM layer:

- There are 4 weight matrices (input, forget, output, cell state) per layer 
- Each weight matrix has dimensions (input size x 1500 hidden units)
- Plus 1500 bias terms per matrix

Layer 1:
- Input size is 10k (vocab size)
- Weight matrices: 4 * (10k * 1500) = 60 million parameters 
- Biases: 4 * 1500 = 6000 parameters

Layer 2: 
- Input size is 1500 (output of layer 1)  
- Weight matrices: 4 * (1500 * 1500) = 9 million parameters
- Biases: 4 * 1500 = 6000 parameters

Total parameters = 60 million + 6000 + 9 million + 6000 
             = 69,012,000

So the total number of parameters in this 2-layer 1500-unit LSTM with 10k vocab size is approximately 69 million.",1.4352996e+16,6 FLOP / token / parameter * 69000000 parameters * 912344 tokens * 38 epochs = 1.4352996e+16 FLOP,Penn TreeBank (PTB),,912344.0,,,,,,Likely,"The large communication cost for exchanging gradients between different nodes significantly limits the scalability of distributed training for large-scale learning models. Motivated by this observation, there has been significant recent interest in techniques that reduce the communication cost of distributed Stochastic Gradient Descent (SGD), with gradient sparsification techniques such as top-k and random-k shown to be particularly effective. The same observation has also motivated a separate line of work in distributed statistical estimation theory focusing on the impact of communication constraints on the estimation efficiency of different statistical models. The primary goal of this paper is to connect these two research lines and demonstrate how statistical estimation models and their analysis can lead to new insights in the design of communication-efficient training techniques. We propose a simple statistical estimation model for the stochastic gradients which captures the sparsity and skewness of their distribution. The statistically optimal communication scheme arising from the analysis of this model leads to a new sparsification technique for SGD, which concatenates random-k and top-k, considered separately in the prior literature. We show through extensive experiments on both image and language domains with CIFAR-10, ImageNet, and Penn Treebank datasets that the concatenated application of these two sparsification methods consistently and significantly outperforms either method applied alone.",38.0,rTop-k(distributed setting),Unreleased,United States of America,,,,,,2025-04-09 12:24,,,,,,Academia,,,,,Unreleased,,Academia,,,,,Operation counting,,,,
Conformer,Speech,Speech recognition,Google,"Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, Ruoming Pang",2020-05-16,Conformer: Convolution-augmented Transformer for Speech Recognition,https://arxiv.org/abs/2005.08100v1,2655.0,"Highly cited,SOTA improvement","""Conformer significantly outperforms the previous Transformer and CNN based models achieving state-of-the-art accuracies. On the widely used LibriSpeech benchmark, our model achieves WER of 2.1%/4.3% without using a language model and 1.9%/3.9% with an external language model on test/testother""",118800000.0,118.8M for Conformer(L),,,LibriSpeech,,,,,,,,Confident,"Recently Transformer and Convolution neural network (CNN) based models have shown promising results in Automatic Speech Recognition (ASR), outperforming Recurrent neural networks (RNNs). Transformer models are good at capturing content-based global interactions, while CNNs exploit local features effectively. In this work, we achieve the best of both worlds by studying how to combine convolution neural networks and transformers to model both local and global dependencies of an audio sequence in a parameter-efficient way. To this regard, we propose the convolution-augmented transformer for speech recognition, named Conformer. Conformer significantly outperforms the previous Transformer and CNN based models achieving state-of-the-art accuracies. On the widely used LibriSpeech benchmark, our model achieves WER of 2.1%/4.3% without using a language model and 1.9%/3.9% with an external language model on test/testother. We also observe competitive performance of 2.7%/6.3% with a small model of only 10M parameters.",,,Unreleased,United States of America,,,,,,2024-11-01 10:03,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
ONLSTM-SYD,Language,Language modeling,"Westlake University,Institute for Advanced Study,McGill University,Mila - Quebec AI (originally Montreal Institute for Learning Algorithms),CIFAR AI Research,University of Montreal / UniversitÃ© de MontrÃ©al","Wenyu Du, Zhouhan Lin, Yikang Shen, Timothy J. O'Donnell, Yoshua Bengio, Yue Zhang",2020-05-12,Exploiting Syntactic Structure for Better Language Modeling: A Syntactic Distance Approach,https://arxiv.org/abs/2005.05864,15.0,,,25000000.0,"25M
Table 1 ",1.368516e+17,6 FLOP / parameter / token * 25 * 10^6 parameters * 912 344 tokens * 1000 epochs = 1.368516e+17 FLOP,Penn TreeBank (PTB),,912344.0,,,,,,Confident,"It is commonly believed that knowledge of syntactic structure should improve language modeling. However, effectively and computationally efficiently incorporating syntactic structure into neural language models has been a challenging topic. In this paper, we make use of a multi-task objective, i.e., the models simultaneously predict words as well as ground truth parse trees in a form called ""syntactic distances"", where information between these two separate objectives shares the same intermediate representation. Experimental results on the Penn Treebank and Chinese Treebank datasets show that when ground truth parse trees are provided as additional training signals, the model is able to achieve lower perplexity and induce trees with better quality.",1000.0,ONLSTM-SYD,Unreleased,"China,United States of America,Canada,Canada,Canada,Canada",,,,,,2025-04-09 12:54,,,,,,"Academia,Academia,Academia,Academia,Research collective,Academia",,,,,Open source,"BSD-3 license: 

https://github.com/wenyudu/SDLM ","Academia,Academia,Academia,Academia,Research collective,Academia",,,,,Operation counting,,,,
ContextNet,Speech,Speech recognition,Google,"Wei Han, Zhengdong Zhang, Yu Zhang, Jiahui Yu, Chung-Cheng Chiu, James Qin, Anmol Gulati, Ruoming Pang, Yonghui Wu",2020-05-07,ContextNet: Improving Convolutional Neural Networks for Automatic Speech Recognition with Global Context,https://arxiv.org/abs/2005.03191v3,237.0,SOTA improvement,"""We demonstrate that on the widely used Librispeech
benchmark, ContextNet achieves a word error rate (WER) of
2.1%/4.6% without external language model (LM), 1.9%/4.1%
with LM and 2.9%/7.0% with only 10M parameters on the
clean/noisy LibriSpeech test sets. This compares to the
best previously published model of 2.0%/4.6% with LM and
3.9%/11.3% with 20M parameters""",112700000.0,Table 5,,"Uses pre-trained joint network from https://arxiv.org/pdf/1811.06621, so total training compute should factor this in.",LibriSpeech,,1710000000.0,"970 hours of speech in the LibreSpeech experiments. There is mention of a ""large scale experiment"" which trained on audio from YouTube videos. Tracing the references, it appears to be similar to the dataset used in https://arxiv.org/abs/1610.09975, which has 125,000 hours of transcribed audio for training. However, they mention in footnote 2 that the training and evaluation set have changed from previous experiments.
125k hours at 13,680 words per hour = 1.71B words",,,,,Likely,"Convolutional neural networks (CNN) have shown promising results for end-to-end speech recognition, albeit still behind other state-of-the-art methods in performance. In this paper, we study how to bridge this gap and go beyond with a novel CNN-RNN-transducer architecture, which we call ContextNet. ContextNet features a fully convolutional encoder that incorporates global context information into convolution layers by adding squeeze-and-excitation modules. In addition, we propose a simple scaling method that scales the widths of ContextNet that achieves good trade-off between computation and accuracy. We demonstrate that on the widely used LibriSpeech benchmark, ContextNet achieves a word error rate (WER) of 2.1%/4.6% without external language model (LM), 1.9%/4.1% with LM and 2.9%/7.0% with only 10M parameters on the clean/noisy LibriSpeech test sets. This compares to the previous best published system of 2.0%/4.6% with LM and 3.9%/11.3% with 20M parameters. The superiority of the proposed ContextNet model is also verified on a much larger internal dataset.",,,Unreleased,United States of America,,,,,,2024-11-01 10:03,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
NAS+ESS (156M),Language,"Neural Architecture Search - NAS,Language modeling","Northeastern University (China),Chinese Academy of Sciences,NiuTrans Research,Kingsoft","Yinqiao Li, Chi Hu, Yuhao Zhang, Nuo Xu, Yufan Jiang, Tong Xiao, Jingbo Zhu, Tongran Liu, Changliang Li",2020-05-06,Learning Architectures from an Extended Search Space for Language Modeling,https://arxiv.org/abs/2005.02593,12.0,SOTA improvement,"""Our ESS method
achieves state-of-the-art result on the PTB task""",156000000.0,156M (Table 2),2.89e+18,6 FLOP / token / parameter * 156 * 10^6 parameters * 103000000 tokens * 30 epochs = 2.89224e+18 FLOP,WikiText-103,,103000000.0,"""The batch size was 128 and the number of hidden units was
300""",,2890000000000000000 FLOP / (11340000000000 FLOP / sec [fp32] * 1 GPU * 3600 sec / hour * 0.3 [assumed utilization]) = 235 hours,NVIDIA GeForce GTX 1080 Ti,,Confident,"Neural architecture search (NAS) has advanced significantly in recent years but most NAS systems restrict search to learning architectures of a recurrent or convolutional cell. In this paper, we extend the search space of NAS. In particular, we present a general approach to learn both intra-cell and inter-cell architectures (call it ESS). For a better search result, we design a joint learning method to perform intra-cell and inter-cell NAS simultaneously. We implement our model in a differentiable architecture search system. For recurrent neural language modeling, it outperforms a strong baseline significantly on the PTB and WikiText data, with a new state-of-the-art on PTB. Moreover, the learned architectures show good transferability to other systems. E.g., they improve state-of-the-art systems on the CoNLL and WNUT named entity recognition (NER) tasks and CoNLL chunking task, indicating a promising line of research on large-scale pre-learned architectures.",30.0,NAS+ESS (156M),Unreleased,"China,China,China,China",,,,1.0,,2025-04-09 13:16,,,,,,"Academia,Academia,Industry",,,,,Unreleased,,"Academia,Academia,Industry",,,,280.3988579029138,Operation counting,,,,
UnifiedQA,Language,Question answering,"Allen Institute for AI,University of Washington","Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, Hannaneh Hajishirzi",2020-05-02,UnifiedQA: Crossing Format Boundaries With a Single QA System,https://arxiv.org/abs/2005.00700v3,674.0,SOTA improvement,"""We then introduce UNIFIEDQA (Â§3.2) that is a QA system
trained on datasets in multiple formats, indicating
new state-of-the-art results on 10 datasets and generalization to unseen datasets.""",11000000000.0,11B (based on T5-11B),1.65e+19,"A.2: ""In the experiments, we use v3-8 TPUs for T5 models... pretraining UNIFIEDQA approximately takes about 36 hours on T5(11B)""

4 * 1.23e14 * 36 * 3600 * 0.3 = 1.91e19

Alternatively, input (ouput) size of 512 (100) tokens, batch size of 8, trained for 100k steps. Input tokens only need the forward pass.
((2 * 11B * 512 * 8) + (6 * 11B * 100 * 8)) * 100k = 1.43e19

Took geometric mean of these estimates:
sqrt(1.91e19*1.43e19) = 1.65e19",,"""We empirically chose the following 8 seed datasets for training UNIFIEDQA, 3 based on their effectiveness in our pilot study (details deferred to Section 5) assessing which datasets are most valuable for out-of-format training:
â€¢ EX: SQuAD 1.1, SQuAD 2.0
â€¢ AB: NarrativeQA
â€¢ MC: RACE, ARC, OBQA, MCTest
â€¢ YN: BoolQ""",97309860.0,"Table 2:
SQuAD 1.1: 87k examples, avg total length of 136.2 + 3.0
SQuAD 2.0: 130k examples, avg total length of 139.9 + 2.6
NarrativeQA: 65k examples, avg total length of 563.6 + 6.2
RACE: 87k examples, avg total length of 317.9 + 6.9
ARC (easy): 2k examples, avg total length of 39.4 + 3.7
ARC (hard): 1k examples, avg total length of  47.4 + 5.0
OBQA: 4k examples, avg total length of 28.7 + 3.6
MCTest: 1.4k examples, avg total length of 245.4 + 4.0
BoolQ: 9k examples, avg total length of 105.1 + 1.0

Total tokens: 97,309,860",36.0,"""pretraining UNIFIEDQA approximately takes about 36 and 55 hours, on T5(11B) and BART models, respectively.""",Google TPU v3,,Confident,"Question answering (QA) tasks have been posed using a variety of formats, such as extractive span selection, multiple choice, etc. This has led to format-specialized models, and even to an implicit division in the QA community. We argue that such boundaries are artificial and perhaps unnecessary, given the reasoning abilities we seek to teach are not governed by the format. As evidence, we use the latest advances in language modeling to build a single pre-trained QA model, UnifiedQA, that performs surprisingly well across 17 QA datasets spanning 4 diverse formats. UnifiedQA performs on par with 9 different models that were trained on individual datasets themselves. Even when faced with 12 unseen datasets of observed formats, UnifiedQA performs surprisingly well, showing strong generalization from its out-of-format training data. Finally, simply fine-tuning this pre-trained QA model into specialized models results in a new state of the art on 6 datasets, establishing UnifiedQA as a strong starting point for building QA systems.",1.88,,Open weights (unrestricted),"United States of America,United States of America",T5-11B,38000000000000000000,"""â€¢ Infrastructure: In the experiments, we use v3-8 TPUs for T5 models, and eight 32GB GPUs for
BART models.
â€¢ Time spent to build UNIFIEDQA: pretraining UNIFIEDQA approximately takes about 36 and 55
hours, on T5(11B) and BART models, respectively.""

8 * 123 TFLOPS * 36 * 3600 * 0.3 (utilization assumption) = 3.8e19",8.0,,2025-06-04 17:11,,,,27.0,,"Research collective,Academia",,,,,Open source,"Apache 2.0 
https://github.com/allenai/unifiedqa

no clear license but it is safe to assume that github repo Apache 2.0 license governs the model weights as well since they are interlinked
https://huggingface.co/allenai/unifiedqa-v2-t5-11b-1363200","Research collective,Academia",,,,3593.01452898213,"Operation counting,Hardware",allenai,,,
ATLAS,Language,Question answering,"Allen Institute for AI,University of Washington","Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, Hannaneh Hajishirzi",2020-05-02,UnifiedQA: Crossing Format Boundaries With a Single QA System,https://arxiv.org/abs/2005.00700,674.0,SOTA improvement,"from abstract: ""Finally, simply fine-tuning this pre-trained QA model into specialized models results in a new state of the art on 6 datasets""",11000000000.0,"11B from appendix A.2 : Model sizes: ""Most of the experiments are done on T5(11B) which has 11 billion parameters. We also report experiments with BART (large) with 440 million parameters.""",3.825792e+19,"flops = (8) * (123 * 10**12) * (36 * 3600) * (0.3)
(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)

from Appendix A.2: ""Time spent to build UNIFIEDQA: pretraining UNIFIEDQA approximately takes about 36 and 55 hours, on T5(11B) and BART models, respectively.""
so 36h for T5

""Infrastructure: In the experiments, we use v3-8 TPUs for T5 models, and eight 32GB GPUs for BART models.""

from https://cloud.google.com/tpu/docs/system-architecture-tpu-vm#tpu_chip
tpu chip have peak flops 123 teraflops
so 8 chips have peak flops 123 * 8`",SQuAD 1.1,"from appendix A.1 - multiple QA datasets, In section 3 there is description how batches are created from multiple datasets.",,"from appendix A.1 - multiple QA datasets - it may be possible to estimate by summing sizes of all datasets
I am not sure if all data is used as system is trained for 100K steps (from appendix A.2)
with batch size 8 (appendix A.1)",36.0,"Appendix A.2: Time spent to build UNIFIEDQA: pretraining UNIFIEDQA approximately takes about 36 and 55 hours, on T5(11B) and BART models, respectively.",Google TPU v3,,Confident,"Question answering (QA) tasks have been posed using a variety of formats, such as extractive span selection, multiple choice, etc. This has led to format-specialized models, and even to an implicit division in the QA community. We argue that such boundaries are artificial and perhaps unnecessary, given the reasoning abilities we seek to teach are not governed by the format. As evidence, we use the latest advances in language modeling to build a single pre-trained QA model, UnifiedQA, that performs surprisingly well across 17 QA datasets spanning 4 diverse formats. UnifiedQA performs on par with 9 different models that were trained on individual datasets themselves. Even when faced with 12 unseen datasets of observed formats, UnifiedQA performs surprisingly well, showing strong generalization from its out-of-format training data. Finally, simply fine-tuning this pre-trained QA model into specialized models results in a new state of the art on 6 datasets, establishing UnifiedQA as a strong starting point for building QA systems.",,,Open weights (unrestricted),"United States of America,United States of America",,,,,,2025-06-14 14:09,,,,,,"Research collective,Academia",,,,,Open source,"Apache 2.0 license, includes models and training code: https://github.com/allenai/unifiedqa","Research collective,Academia",$59.12,,,,Hardware,,,,
"Segatron XL large, M=384",Language,Language modeling,"University of Waterloo,Peking University,RSVP.ai","He Bai, Peng Shi, Jimmy Lin, Yuqing Xie, Luchen Tan, Kun Xiong, Wen Gao, Ming Li",2020-04-30,Segatron: Segment-Aware Transformer for Language Modeling and Understanding,https://arxiv.org/abs/2004.14996,13.0,,,257000000.0,"""Following Transformer-XL, we train a base size model and a large size model. <..> The large model is an 18 layer
Transformer with a hidden size of 1024 and 16 attention
heads""",2.6527334e+19,6 FLOP / parameter / token * 257000000 parameters * 350000 steps * 128 sequences per batch * 384 tokens per sequence = 2.6527334e+19 FLOP,WikiText-103,,103000000.0,"""This model is trained with 350K steps with a batch
size of 128. The sequence length and memory length during training and testing all equal 150 for the base model
and 384 for the large model""

350000*128*384 / 103000000 =167.02 epochs",,,,,Confident,"Transformers are powerful for sequence modeling. Nearly all state-of-the-art language models and pre-trained language models are based on the Transformer architecture. However, it distinguishes sequential tokens only with the token position index. We hypothesize that better contextual representations can be generated from the Transformer with richer positional information. To verify this, we propose a segment-aware Transformer (Segatron), by replacing the original token position encoding with a combined position encoding of paragraph, sentence, and token. We first introduce the segment-aware mechanism to Transformer-XL, which is a popular Transformer-based language model with memory extension and relative position encoding. We find that our method can further improve the Transformer-XL base model and large model, achieving 17.1 perplexity on the WikiText-103 dataset. We further investigate the pre-training masked language modeling task with Segatron. Experimental results show that BERT pre-trained with Segatron (SegaBERT) can outperform BERT with vanilla Transformer on various NLP tasks, and outperforms RoBERTa on zero-shot sentence representation learning.",167.02,"""Segatron XL large, M=384""",Open weights (non-commercial),"Canada,China,China",,,,,,2025-04-09 13:39,,,,49152.0,128*384,"Academia,Academia,Industry",,,,,Open (non-commercial),"training code and weights, no clear license: https://github.com/rsvp-ai/segatron_aaai?tab=readme-ov-file 
train: https://github.com/rsvp-ai/segatron_aaai/blob/main/segabert/README.md ","Academia,Academia,Industry",,,,,Operation counting,,,,
Once for All,Vision,Image classification,"MIT-IBM Watson AI Lab,Massachusetts Institute of Technology (MIT),IBM","Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, and Song Han",2020-04-29,Once for all: Train one network and specialize it for efficient deployment.,https://arxiv.org/abs/1908.09791,1157.0,SOTA improvement,"""In particular, OFA achieves a new SOTA 80.0% ImageNet top-1 accuracy under the mobile setting""",7700000.0,"""Since
all of these sub-networks share the same weights (i.e., Wo) (Cheung et al., 2019), we only require
7.7M parameters to store all of them. Without sharing, the total model size will be prohibitive""",6.237e+20,"4.2k V100-hours (table 1)
0.33 utilization rate
V100 FP16 Tensor FLOPs: 125000000000000

4200 hours*60*60* 125000000000000 FLOP/s *0.33 utilization =623700000000000000000
",ImageNet,,,,,,NVIDIA V100,,Confident,"We address the challenging problem of efficient inference across many devices and resource constraints, especially on edge devices. Conventional approaches either manually design or use neural architecture search (NAS) to find a specialized neural network and train it from scratch for each case, which is computationally prohibitive (causing CO2 emission as much as 5 cars' lifetime) thus unscalable. In this work, we propose to train a once-for-all (OFA) network that supports diverse architectural settings by decoupling training and search, to reduce the cost. We can quickly get a specialized sub-network by selecting from the OFA network without additional training. To efficiently train OFA networks, we also propose a novel progressive shrinking algorithm, a generalized pruning method that reduces the model size across many more dimensions than pruning (depth, width, kernel size, and resolution). It can obtain a surprisingly large number of sub-networks (>1019) that can fit different hardware platforms and latency constraints while maintaining the same level of accuracy as training independently. On diverse edge devices, OFA consistently outperforms state-of-the-art (SOTA) NAS methods (up to 4.0% ImageNet top1 accuracy improvement over MobileNetV3, or same accuracy but 1.5x faster than MobileNetV3, 2.6x faster than EfficientNet w.r.t measured latency) while reducing many orders of magnitude GPU hours and CO2 emission. In particular, OFA achieves a new SOTA 80.0% ImageNet top-1 accuracy under the mobile setting (<600M MACs). OFA is the winning solution for the 3rd Low Power Computer Vision Challenge (LPCVC), DSP classification track and the 4th LPCVC, both classification track and detection track. Code and 50 pre-trained models (for many devices & many latency constraints) are released at this https URL.",180.0,,Open weights (unrestricted),"United States of America,United States of America,United States of America",,,,,,2025-06-18 11:09,Amazon Web Services,,,,,"Academia,Industry,Academia,Industry",,,,4200.0,Open source,"MIT license: https://github.com/mit-han-lab/once-for-all

repo contains inference and training code. models available via library","Academia,Industry,Academia,Industry",$1753.93,,FP32,,Hardware,,,,
Go-explore,Games,Atari,"Uber AI,OpenAI","Adrien Ecoffet, Joost Huizinga, Joel Lehman, Kenneth O. Stanley, Jeff Clune",2020-04-27,"First return, then explore",https://arxiv.org/abs/2004.12919,314.0,SOTA improvement,"""GoExplore solves all heretofore unsolved Atari games (meaning those for which algorithms could not previously
outperform humans when evaluated following current community standards for Atari3) and surpasses the state
of the art on all hard-exploration games""",,,,,,,,,,,,,Unknown,"The promise of reinforcement learning is to solve complex sequential decision problems autonomously by specifying a high-level reward function only. However, reinforcement learning algorithms struggle when, as is often the case, simple and intuitive rewards provide sparse and deceptive feedback. Avoiding these pitfalls requires thoroughly exploring the environment, but creating algorithms that can do so remains one of the central challenges of the field. We hypothesise that the main impediment to effective exploration originates from algorithms forgetting how to reach previously visited states (""detachment"") and from failing to first return to a state before exploring from it (""derailment""). We introduce Go-Explore, a family of algorithms that addresses these two challenges directly through the simple principles of explicitly remembering promising states and first returning to such states before intentionally exploring. Go-Explore solves all heretofore unsolved Atari games and surpasses the state of the art on all hard-exploration games, with orders of magnitude improvements on the grand challenges Montezuma's Revenge and Pitfall. We also demonstrate the practical potential of Go-Explore on a sparse-reward pick-and-place robotics task. Additionally, we show that adding a goal-conditioned policy can further improve Go-Explore's exploration efficiency and enable it to handle stochasticity throughout training. The substantial performance gains from Go-Explore suggest that the simple principles of remembering states, returning to them, and exploring from them are a powerful and general approach to exploration, an insight that may prove critical to the creation of truly intelligent learning agents.",,,Unreleased,"United States of America,United States of America",,,,,,2025-01-06 11:35,,,,,,"Industry,Industry",,,,,Open (non-commercial),non-commercial code: https://github.com/uber-research/go-explore/blob/master/LICENSE,"Industry,Industry",,,,,,,,,
Cube-Space AutoEncoder,"Vision,Search",Visual puzzles,MIT-IBM Watson AI Lab,"Masataro Asai, Christian Muise",2020-04-27,Learning Neural-Symbolic Descriptive Planning Models via Cube-Space Priors: The Voyage Home (to STRIPS),https://arxiv.org/abs/2004.12850,56.0,,,,It may be possible to estimate from section 4.,1.0660896e+17,"(1) * (4113 * 10**9) * (24 * 3600) * (0.3) = 106608960000000000
(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate) = 

""For each domain, we searched for 100 iterations (â‰ˆ15min/iter, 24 hours total) on a Tesla K80""
4.113 TFLOPS from https://www.techpowerup.com/gpu-specs/tesla-k80.c2616


",,"""Finally, we tested Mandrill 15-puzzle, a significantly more
challenging 4x4 variant of the sliding tile puzzle (Figure 1).
We trained the network with more hyperparameter tuning it-
erations (300) and a larger training set (50000). We gener-
ated l = 14, 21 instances (20 each) and ran the system (Ta-
ble 2, bottom right). """,50000.0,"""Finally, we tested Mandrill 15-puzzle, a significantly more
challenging 4x4 variant of the sliding tile puzzle (Figure 1).
We trained the network with more hyperparameter tuning it-
erations (300) and a larger training set (50000). We gener-
ated l = 14, 21 instances (20 each) and ran the system (Ta-
ble 2, bottom right). """,24.0,"""For each domain, we searched for 100 iterations (â‰ˆ15min/iter, 24 hours total) on a Tesla K80""",NVIDIA Tesla K80,,Confident,"We achieved a new milestone in the difficult task of enabling agents to learn about their environment autonomously. Our neuro-symbolic architecture is trained end-to-end to produce a succinct and effective discrete state transition model from images alone. Our target representation (the Planning Domain Definition Language) is already in a form that off-the-shelf solvers can consume, and opens the door to the rich array of modern heuristic search capabilities. We demonstrate how the sophisticated innate prior we place on the learning process significantly reduces the complexity of the learned representation, and reveals a connection to the graph-theoretic notion of ""cube-like graphs"", thus opening the door to a deeper understanding of the ideal properties for learned symbolic representations. We show that the powerful domain-independent heuristics allow our system to solve visual 15-Puzzle instances which are beyond the reach of blind search, without resorting to the Reinforcement Learning approach that requires a huge amount of training on the domain-dependent reward information.",,,,United States of America,,,,1.0,,2025-02-17 12:22,,,,,,"Academia,Industry",,,,24.0,,,"Academia,Industry",,,,336.5460747498431,Hardware,,,,
DiffStk-MRNN,Language,Language modeling,"Pennsylvania State University,Rochester Institute of Technology","Ankur Mali, Alexander Ororbia, Daniel Kifer, Clyde Lee Giles",2020-04-04,Recognizing Long Grammatical Sequences Using Recurrent Networks Augmented With An External Differentiable Stack,https://arxiv.org/abs/2004.07623,13.0,,,1010000.0,"All models trained on this dataset consisted of 100 hidden units

Okay, let's break this down:

- RNN with 1 hidden layer
- 100 hidden units
- Vocabulary size = 10,000

The RNN layer will have:

- Input weight matrix U: dimensions (vocab size x hidden size) = (10,000 x 100) = 1,000,000 parameters

- Recurrent weight matrix W: dimensions (hidden size x hidden size) = (100 x 100) = 10,000 parameters 

- Bias vector b: (hidden size) = (100) = 100 parameters

So the total parameters is:

U = 1,000,000 
W = 10,000
b = 100

Total parameters = U + W + b 
            = 1,000,000 + 10,000 + 100
            = 1,010,100

Therefore, the total number of parameters for this 1-layer RNN with 100 hidden units and 10,000 vocab size is 1,010,100.",276440230000000.0,6 FLOP / token / parameter * 1010000 parameters * 912 344 tokens * 50 epochs = 2.7644023e+14 FLOP,Penn TreeBank (PTB),,912344.0,,,,,,Likely,"Recurrent neural networks (RNNs) are a widely used deep architecture for sequence modeling, generation, and prediction. Despite success in applications such as machine translation and voice recognition, these stateful models have several critical shortcomings. Specifically, RNNs generalize poorly over very long sequences, which limits their applicability to many important temporal processing and time series forecasting problems. For example, RNNs struggle in recognizing complex context free languages (CFLs), never reaching 100% accuracy on training. One way to address these shortcomings is to couple an RNN with an external, differentiable memory structure, such as a stack. However, differentiable memories in prior work have neither been extensively studied on CFLs nor tested on sequences longer than those seen in training. The few efforts that have studied them have shown that continuous differentiable memory structures yield poor generalization for complex CFLs, making the RNN less interpretable. In this paper, we improve the memory-augmented RNN with important architectural and state updating mechanisms that ensure that the model learns to properly balance the use of its latent states with external memory. Our improved RNN models exhibit better generalization performance and are able to classify long strings generated by complex hierarchical context free grammars (CFGs). We evaluate our models on CGGs, including the Dyck languages, as well as on the Penn Treebank language modelling task, and achieve stable, robust performance across these benchmarks. Furthermore, we show that only our memory-augmented networks are capable of retaining memory for a longer duration up to strings of length 160.",50.0,DiffStk-MRNN,Unreleased,"United States of America,United States of America",,,,,,2025-04-09 13:55,,,,,,"Academia,Academia",,,,,Unreleased,,"Academia,Academia",,,,,Operation counting,,,,
Agent57,Games,Atari,DeepMind,"AP Badia, B Piot, S Kapturowski",2020-03-30,Agent57: Outperforming the Atari Human Benchmark,https://arxiv.org/abs/2003.13350,485.0,SOTA improvement,"""We propose Agent57, the first deep RL agent that outperforms the standard human benchmark on all 57 Atari games""",,,,,,,,,,,,,Unknown,,,,Unreleased,United Kingdom of Great Britain and Northern Ireland,,,,,,2024-11-01 10:03,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
AraBERT,Language,"Language modeling/generation,Question answering",American University of Beirut,"Wissam Antoun, Fady Baly, Hazem Hajj",2020-03-30,AraBERT: Transformer-based Model for Arabic Language Understanding,https://arxiv.org/abs/2003.00104v2,,,,110000000.0,"We use the BERTbase configuration that has 12 encoder blocks, 768 hidden dimensions, 12 attention heads, 512 maximum sequence
length, and a total of âˆ¼110M parameters",3.1765134e+19,"6 FLOP / parameter / token * 110000000 parameters * 81920000000 total training tokens [see dataset size notes] = 5.40672e+19 FLOP

45000000000000 FLOP / sec / chip * 4 chips [=8 cores] * 96 hours * 3600 sec / hour * 0.3 [assumed utilization] = 1.86624e+19 FLOP 

sqrt(5.40672e+19*1.86624e+19) = 3.1765134e+19 FLOP","Arabic Corpus,OSIAN corpus","Since the Arabic Wikipedia Dumps are small compared to the English ones, we manually scraped Arabic news websites for articles. In addition, we used two publicly available large Arabic corpora: (1) the 1.5 billion words Arabic Corpus (El-Khair, 2016), which is a contemporary corpus that includes more than 5 million articles extracted from ten major news sources covering 8 countries, and (2) OSIAN: the Open Source International Arabic News Corpus (Zeroual et al., 2019) that consists of 3.5 million articles (âˆ¼1B tokens) from 31 news sources in 24 Arab countries.",3095937662.0,"The final size of the pre-training dataset, after removing duplicate sentences, is 70 million sentences, corresponding to âˆ¼24GB of text

total of ""1,250,000 steps. To speed up the training time, the first 900K steps were trained on sequences of 128 tokens, and the remaining steps were trained on sequences of 512 tokens.""

""batch size of 512 and 128 for sequence length of 128 and 512 respectively. Training took 4 days, for 27 epochs over all the tokens.""

[see AraGPT2-Mega dataset size notes] 77GB of arabic text ~ 9932800000 tokens -> 24GB ~ 3095937662 tokens

900000*128*512 + 350000*512*128 = 81920000000 total training tokens ~ 26-27 epochs as reported 



",96.0,4 days = 96 hours,Google TPU v2,,Confident,"The Arabic language is a morphologically rich language with relatively few resources and a less explored syntax compared to English. Given these limitations, Arabic Natural Language Processing (NLP) tasks like Sentiment Analysis (SA), Named Entity Recognition (NER), and Question Answering (QA), have proven to be very challenging to tackle. Recently, with the surge of transformers based models, language-specific BERT based models have proven to be very efficient at language understanding, provided they are pre-trained on a very large corpus. Such models were able to set new standards and achieve state-of-the-art results for most NLP tasks. In this paper, we pre-trained BERT specifically for the Arabic language in the pursuit of achieving the same success that BERT did for the English language. The performance of AraBERT is compared to multilingual BERT from Google and other state-of-the-art approaches. The results showed that the newly developed AraBERT achieved state-of-the-art performance on most tested Arabic NLP tasks. The pretrained araBERT models are publicly available on this https URL hoping to encourage research and applications for Arabic NLP.",27.0,,Open weights (unrestricted),Lebanon,,,,8.0,,2025-05-01 14:02,,,,,,Academia,,,,,,https://huggingface.co/aubmindlab/bert-base-arabert,Academia,,,,4576.289412806967,"Operation counting,Hardware",aubmindlab,,,
AraBERT LArge v2,Language,"Language modeling/generation,Question answering",American University of Beirut,"Wissam Antoun, Fady Baly, Hazem Hajj",2020-03-30,AraBERT v1 & v2 : Pre-training BERT for Arabic Language Understanding,https://huggingface.co/aubmindlab/bert-large-arabertv2,,,,371000000.0,371M,1.5399499e+21,"6 FLOP / parameter / token * (128*13440*250000 + 512*2056*300000) total training tokens [see dataset size notes] * 371000000 parameters = 1.6603324e+21 FLOP

123000000000000 FLOP / chip / sec * 64 chips [=128 cores] * 168 hours * 3600 sec / hour * 0.3 [assumed utilization] = 1.4282957e+21 FLOP

sqrt(1.6603324e+21*1.4282957e+21) = 1.5399499e+21 FLOP","OSCAR,Arabic Wikipedia,OSIAN corpus,Arabic Corpus","â€¢ The unshuffled OSCAR corpus (Ortiz Suarez Â´ et al., 2020).
â€¢ The Arabic Wikipedia dump from September 2020.
â€¢ The 1.5B words Arabic Corpus (El-Khair, 2016).
â€¢ The OSIAN corpus (Zeroual et al., 2019).
â€¢ News articles provided by As-safir newspaper.",9932800000.0,"num of examples with seq len (128 / 512): 520M / 245M	
128 (Batch Size/ Num of Steps): 13440 / 250K
512 (Batch Size/ Num of Steps): 2056 / 300K

9932800000 tokens - size of the dataset (see AraGPT2-Mega dataset size notes) 

(128*13440*250000 + 512*2056*300000) / 9932800000 = 75 epochs",168.0,7 days = 168 hours,Google TPU v3,,Confident,"AraBERT is an Arabic pretrained language model based on Google's BERT architechture. AraBERT uses the same BERT-Base config. More details are available in the AraBERT Paper and in the AraBERT Meetup

There are two versions of the model, AraBERTv0.1 and AraBERTv1, with the difference being that AraBERTv1 uses pre-segmented text where prefixes and suffixes were split using the Farasa Segmenter.

We evaluate AraBERT models on different downstream tasks and compare them to mBERT, and other state of the art models (To the extent of our knowledge). The Tasks were Sentiment Analysis on 6 different datasets (HARD, ASTD-Balanced, ArsenTD-Lev, LABR), Named Entity Recognition with the ANERcorp, and Arabic Question Answering on Arabic-SQuAD and ARCD",75.0,,Open weights (unrestricted),Lebanon,,,,128.0,,2025-05-01 14:14,,,,,,Academia,,,,,,https://huggingface.co/aubmindlab/bert-large-arabertv2,Academia,,,,57530.49547528759,"Operation counting,Hardware",aubmindlab,,,
MetNet,Earth science,Weather forecasting,Google,"Casper Kaae SÃ¸nderby, Lasse Espeholt, Jonathan Heek, Mostafa Dehghani, Avital Oliver, Tim Salimans, Shreya Agrawal, Jason Hickey, Nal Kalchbrenner",2020-03-24,MetNet: A Neural Weather Model for Precipitation Forecasting,https://arxiv.org/abs/2003.12140,250.0,SOTA improvement,"""MetNet improves upon the current operational NWP system HRRR for up to 8 hours of lead time""
... 
""Numerical Weather Prediction is the most successful framework to perform medium- and longrange (up to 6 days with high confidence) forecast to date (Bauer et al., 2015).""",,,,,,"""Precipitation provides a benchmark for a highly varying and densely measured target (Agrawal
et al.). We cast precipitation forecasting as a structured prediction problem where the output comes
in the form of a three-dimensional tensor. Each value of the tensor corresponds to a time and a
location and indicates the corresponding rate of precipitation measured in mm/h. Target precipitation rates are estimated by the Multi Radar Multi Sensor (MRMS) ground based radars as a
function of the returned radar echoes (Zhang et al., 2016). The spatial size obtained from MRMS
is 7000 Ã— 2500 covering the continental United States. Each pixel covers 0.01â—¦ of longitude and
latitude corresponding to approximately 1 km2
. In addition to MRMS frames, the available input
data include the 16 spectral bands of the optical Geostationary Operational Environmental Satellite
16 (GOES-16). Figure 1 contains examples of MRMS and GOES-16 frames.""",,,,,,,Unknown,,,,Unreleased,United States of America,,,,,,2024-11-01 10:03,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
ELECTRA,Language,Text autocompletion,"Stanford University,Google,Google Brain","Kevin Clark, Minh-Thang Luong, Quoc V. Le, Christopher D. Manning",2020-03-23,ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators,https://arxiv.org/abs/2003.10555v1,2968.0,Highly cited,,335000000.0,https://github.com/google-research/electra,3.0999999999999995e+21,"Table 8: ""ELECTRA-1.75M"" used 3.1e21 train FLOPs. Note that the actual parameter count is 335M. The 1.75M refers to the number of training steps.

This doesn't quite line up with a 6ND estimate, 
6 * 335M * (1.75M * 2048 * 128) = 9.22e20 FLOPs
I'm inferring 128 sequence length, possibly this is 256 or 512?","""BookCorpus (BooksCorpus, Toronto Book Corpus)"",Wikipedia,ClueWeb,Gigaword","""For most experiments we pre-train on the same data as BERT, which consists
of 3.3 Billion tokens from Wikipedia and BooksCorpus (Zhu et al., 2015). However, for our Large
model we pre-trained on the data used for XLNet (Yang et al., 2019), which extends the BERT
dataset to 33B tokens by including data from ClueWeb (Callan et al., 2009), CommonCrawl, and
Gigaword (Parker et al., 2011).""",25000000000.0,"33B tokens or ~25B words

""For most experiments we pre-train on the same data as BERT, which consists
of 3.3 Billion tokens from Wikipedia and BooksCorpus (Zhu et al., 2015). However, for our Large
model we pre-trained on the data used for XLNet (Yang et al., 2019), which extends the BERT
dataset to 33B tokens by including data from ClueWeb (Callan et al., 2009), CommonCrawl, and
Gigaword (Parker et al., 2011).""",,table 1,,Self-supervised learning,Likely,"Masked language modeling (MLM) pre-training methods such as BERT corrupt the input by replacing some tokens with [MASK] and then train a model to reconstruct the original tokens. While they produce good results when transferred to downstream NLP tasks, they generally require large amounts of compute to be effective. As an alternative, we propose a more sample-efficient pre-training task called replaced token detection. Instead of masking the input, our approach corrupts it by replacing some tokens with plausible alternatives sampled from a small generator network. Then, instead of training a model that predicts the original identities of the corrupted tokens, we train a discriminative model that predicts whether each token in the corrupted input was replaced by a generator sample or not. Thorough experiments demonstrate this new pre-training task is more efficient than MLM because the task is defined over all input tokens rather than just the small subset that was masked out. As a result, the contextual representations learned by our approach substantially outperform the ones learned by BERT given the same model size, data, and compute. The gains are particularly strong for small models; for example, we train a model on one GPU for 4 days that outperforms GPT (trained using 30x more compute) on the GLUE natural language understanding benchmark. Our approach also works well at scale, where it performs comparably to RoBERTa and XLNet while using less than 1/4 of their compute and outperforms them when using the same amount of compute.",,,Open weights (unrestricted),"United States of America,United States of America,United States of America",,,,,,2025-05-28 16:14,,,,262144.0,,"Academia,Industry,Industry",,,,,Open source,"models and training code, Apache 2.0: https://github.com/google-research/electra","Academia,Industry,Industry",,,FP32,,Reported,,,,20
Tensor-Transformer(1core)+PN (WT103),Language,Language modeling,University of California (UC) Berkeley,"Sheng Shen, Zhewei Yao, Amir Gholami, Michael W. Mahoney, Kurt Keutzer",2020-03-17,PowerNorm: Rethinking Batch Normalization in Transformers,https://arxiv.org/abs/2003.07845,60.0,SOTA improvement,"""The results are reported in Table 1. In the first section of
rows, we report state-of-the-art results for these two tasks with comparable model sizes""",85300000.0,"six layers tensorized transformer core-1 for Wikitext-103, following (Ma et al., 2019).

https://arxiv.org/abs/1906.09777",1.58e+18,6 FLOP / parameter / token * 85300000 parameters * 103000000 tokens * 30 epochs = 1.581462e+18 FLOP,WikiText-103,,103000000.0,,,,,,Confident,"The standard normalization method for neural network (NN) models used in Natural Language Processing (NLP) is layer normalization (LN). This is different than batch normalization (BN), which is widely-adopted in Computer Vision. The preferred use of LN in NLP is principally due to the empirical observation that a (naive/vanilla) use of BN leads to significant performance degradation for NLP tasks; however, a thorough understanding of the underlying reasons for this is not always evident. In this paper, we perform a systematic study of NLP transformer models to understand why BN has a poor performance, as compared to LN. We find that the statistics of NLP data across the batch dimension exhibit large fluctuations throughout training. This results in instability, if BN is naively implemented. To address this, we propose Power Normalization (PN), a novel normalization scheme that resolves this issue by (i) relaxing zero-mean normalization in BN, (ii) incorporating a running quadratic mean instead of per batch statistics to stabilize fluctuations, and (iii) using an approximate backpropagation for incorporating the running statistics in the forward pass. We show theoretically, under mild assumptions, that PN leads to a smaller Lipschitz constant for the loss, compared with BN. Furthermore, we prove that the approximate backpropagation scheme leads to bounded gradients. We extensively test PN for transformers on a range of NLP tasks, and we show that it significantly outperforms both LN and BN. In particular, PN outperforms LN by 0.4/0.6 BLEU on IWSLT14/WMT14 and 5.6/3.0 PPL on PTB/WikiText-103. We make our code publicly available at \url{this https URL}.",30.0,Tensor-Transformer(1core)+PN (WT103),Open weights (unrestricted),United States of America,,,,,,2025-05-28 16:14,,,,,,Academia,,,,,Open source,copyleft license: https://github.com/sIncerass/powernorm/blob/master/LICENSE,Academia,,,FP32,,Operation counting,,,,
ProGen,Biology,"Protein generation,Proteins,Protein or nucleotide language model (pLM/nLM)","Salesforce Research,Stanford University","Ali Madani, Bryan McCann, Nikhil Naik, Nitish Shirish Keskar, Namrata Anand, Raphael R. Eguchi,  View ORCID ProfilePo-Ssu Huang, Richard Socher",2020-03-13,ProGen: Language Modeling for Protein Generation,https://www.biorxiv.org/content/10.1101/2020.03.07.982272v2,241.0,,,1200000000.0,"""We train a 1.2B-parameter language model, ProGen, on âˆ¼280M protein sequences""",3.7e+20,"Our model was implemented in TensorFlow (Abadi et al.,
2016) and trained with a global batch size of 64 distributed
across 256 cores of a Cloud TPU v3 Pod for 1M iterations. Training took approximately two weeks using Adagrad (Duchi et al., 2011)

4.00E+12*256*60**2*24*14*0.3 = 3.7e20

7.6E+21 FLOPs from Table 9 https://www.biorxiv.org/content/10.1101/2023.07.05.547496v1",,,1049000000000.0," 1,049B from Table 9 https://www.biorxiv.org/content/10.1101/2023.07.05.547496v1",,,,Self-supervised learning,Likely,"Generative modeling for protein engineering is key to solving fundamental problems in synthetic biology, medicine, and material science. We pose protein engineering as an unsupervised sequence generation problem in order to leverage the exponentially growing set of proteins that lack costly, structural annotations. We train a 1.2B-parameter language model, ProGen, on âˆ¼280M protein sequences conditioned on taxonomic and keyword tags such as molecular function and cellular component. This provides ProGen with an unprecedented range of evolutionary sequence diversity and allows it to generate with fine-grained control as demonstrated by metrics based on primary sequence similarity, secondary structure accuracy, and conformational energy.",5.0,,,"United States of America,United States of America",,,,,,2025-05-09 11:32,,,,,,"Industry,Academia",,,,,,,"Industry,Academia",,,,,"Hardware,Third-party estimation",,,,
TransformerXL + spectrum control,Language,Language modeling,"University of California Los Angeles (UCLA),JD.com","Lingxiao Wang, Jing Huang, Kevin Huang, Ziniu Hu, Guangtao Wang, Quanquan Gu",2020-03-11,Improving Neural Language Generation with Spectrum Control,https://openreview.net/forum?id=ByxY8CNtvr,80.0,SOTA improvement,"""We demonstrate that our spectrum control method outperforms the state-of-the-art Transformer-XL modeling for language model""",151000000.0,"151M (Table 2)

"" On the large WikiText-103 dataset, we implement our method based on the state-of-the-art Transformer-XL based models (Dai et al., 2019). We follow the same settings reported in (Dai et al., 2019), and our implementation is based on the
official code for Transformer-XL.""",2.6289761e+19,"6 FLOP / parameter / token *  151000000 parameters * 103000000 tokens * 250 epochs = 2.33295e+19 FLOP

31330000000000 FLOP / sec / GPU [fp16] * 4 GPUs * 3152 sec per epoch * 250 epochs * 0.3 [assumed precision] = 2.9625648e+19 FLOP

sqrt(2.33295e+19*2.9625648e+19) = 2.6289761e+19 FLOP

'speculative' confidence due to the assumption of the amount of epochs",WikiText-103,,103000000.0,"""For WikiText-103 dataset, we use four NVIDIA Tesla V100
GPU and set the batch size to be 40."" 
unknown amount of epochs and training steps but they say they follow original Transformer-XL setup (400K steps + 16K warmup steps, 128 batch size, 384 sequence length) 

400000*4*40*384 / 103000000 = 248 epochs  ",219.0,3152 sec per epoch (Table 7) * 250 epochs / 3600 sec/hour = 219 hours,NVIDIA V100,,Speculative,"Recent Transformer-based models such as Transformer-XL and BERT have achieved huge success on various natural language processing tasks. However, contextualized embeddings at the output layer of these powerful models tend to degenerate and occupy an anisotropic cone in the vector space, which is called the representation degeneration problem. In this paper, we propose a novel spectrum control approach to address this degeneration problem. The core idea of our method is to directly guide the spectra training of the output embedding matrix with a slow-decaying singular value prior distribution through a reparameterization framework. We show that our proposed method encourages isotropy of the learned word representations while maintains the modeling power of these contextual neural models. We further provide a theoretical analysis and insight on the benefit of modeling singular value distribution. We demonstrate that our spectrum control method outperforms the state-of-the-art Transformer-XL modeling for language model, and various Transformer-based models for machine translation, on common benchmark datasets for these tasks.",250.0,TransformerXL + spectrum control,Unreleased,"United States of America,China",,,,4.0,,2025-04-09 14:48,,,,61440.0,4*40*384,"Academia,Industry",,,,,Unreleased,,"Academia,Industry",,,,2452.621142775376,"Operation counting,Hardware",,,,
LSTM-3-layer+Gadam,Language,Language modeling,"University of Oxford,University of Bristol,University of Cambridge","Diego Granziol, Xingchen Wan, Samuel Albanie, Stephen Roberts",2020-03-02,Iterative Averaging in the Quest for Best Test Error,https://arxiv.org/abs/2003.01247,5.0,,,24000000.0,24M imputed from Merity et al. 2017 https://arxiv.org/abs/1708.02182,2.6275507e+16,6 FLOP / parameter / token * 24000000 parameters * 912344 tokens * 200 epochs = 2.6275507e+16 FLOP,Penn TreeBank (PTB),,912344.0,,,,NVIDIA GeForce RTX 2080 Ti 11GB,,Likely,"We analyse and explain the increased generalisation performance of iterate averaging using a Gaussian process perturbation model between the true and batch risk surface on the high dimensional quadratic. We derive three phenomena \latestEdits{from our theoretical results:} (1) The importance of combining iterate averaging (IA) with large learning rates and regularisation for improved regularisation. (2) Justification for less frequent averaging. (3) That we expect adaptive gradient methods to work equally well, or better, with iterate averaging than their non-adaptive counterparts. Inspired by these results\latestEdits{, together with} empirical investigations of the importance of appropriate regularisation for the solution diversity of the iterates, we propose two adaptive algorithms with iterate averaging. These give significantly better results compared to stochastic gradient descent (SGD), require less tuning and do not require early stopping or validation set monitoring. We showcase the efficacy of our approach on the CIFAR-10/100, ImageNet and Penn Treebank datasets on a variety of modern and classical network architectures.",200.0,LSTM-3-layer+Gadam,Unreleased,"United Kingdom of Great Britain and Northern Ireland,United Kingdom of Great Britain and Northern Ireland,United Kingdom of Great Britain and Northern Ireland",,,,1.0,,2025-04-10 10:47,,,,,,"Academia,Academia,Academia",,,,,Unreleased,,"Academia,Academia,Academia",,,,280.8050317098096,Operation counting,,,,
Feedback Transformer,Language,Language modeling,"LORIA,University of Lorraine,Facebook AI Research","Angela Fan, Thibaut Lavril, Edouard Grave, Armand Joulin, Sainbayar Sukhbaatar",2020-02-21,Addressing Some Limitations of Transformers with Feedback Memory,https://arxiv.org/abs/2002.09402,41.0,SOTA improvement,"""As shown in Table 4, the Feedback
Transformer model achieves a new SOTA performance (on Enwiki8) of 0.96 bit-per-byte despite its small size.""",126000000.0,"Table 3 shows 126M.

There is another instance of the Feedback Transformer mentioned in Table 9 with 139M parameters.",7.690547e+18,"6 FLOP / token / parameter * 126*10^6 parameters * 256 tokens per sequences * 512 sequences per batch * 210000 steps = 2.0808991e+19 FLOP

assuming V100 GPU fp16:

31330000000000 FLOP/sec/GPU * 1 GPU * 84 hours * 3600 sec / hour * 0.3 [assumed utilization] = 2.8422576e+18 FLOP

sqrt(2.0808991e+19*2.8422576e+18) = 7.690547e+18 FLOP

___________
in the Algorithmic progress paper they used estimation of 4.41e+19 FLOP also with low confidence",WikiText-103,,103000000.0,"""The models are trained for 200k steps and the finetuned for additional 10k steps.""

from Table 9 (describes 139M model)
batch size 512
sequence length 256

256*512*210000 / 103000000 = 267 epochs

""speculative"" confidence since there is no clear description of the 126M model and numbers are assumed based on 139M model description",84.0,""" Our Feedback architecture takes 3.5 days to train""
3.5*24 = 84 hours",,,Speculative,"Transformers have been successfully applied to sequential, auto-regressive tasks despite being feedforward networks. Unlike recurrent neural networks, Transformers use attention to capture temporal relations while processing input tokens in parallel. While this parallelization makes them computationally efficient, it restricts the model from fully exploiting the sequential nature of the input. The representation at a given layer can only access representations from lower layers, rather than the higher level representations already available. In this work, we propose the Feedback Transformer architecture that exposes all previous representations to all future representations, meaning the lowest representation of the current timestep is formed from the highest-level abstract representation of the past. We demonstrate on a variety of benchmarks in language modeling, machine translation, and reinforcement learning that the increased representation capacity can create small, shallow models with much stronger performance than comparable Transformers.",267.23,Feedback Transformer,Unreleased,"France,France,United States of America",,,,,,2025-04-10 11:49,,,,131072.0,256*512,"Academia,Academia,Industry",,,,,Unreleased,,"Academia,Academia,Industry",,,,,"Operation counting,Hardware",,,,
Turing-NLG,Language,"Text autocompletion,Language generation",Microsoft,Corby Rosset,2020-02-13,Turing-NLG: A 17-billion-parameter language model by Microsoft,https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/,114.0,SOTA improvement,"from paper: ""Turing Natural Language Generation (T-NLG) is a 17 billion parameter language model by Microsoft that outperforms the state of the art on many downstream NLP tasks""",17000000000.0,,1.57e+22,"source: https://lair.lighton.ai/akronomicon/
157 PF-days * 3600 * 24 * 10^15  = 1.35648e+22

archived: https://github.com/lightonai/akronomicon/tree/main/akrodb

6ND=6*17000000000*46400000000=4.7328e+21 (confidence regarding dataset size - likely)

Authors of ""AI and Memory Wall"" (https://github.com/amirgholami/ai_and_memory_wall) estimated model's training compute as 28,000,000 PFLOP = 2.8*10^22 FLOP",,,46400000000.0,"Authors say they pretrain on the same data as for Megatron-LM. 

From the Megatron-LM paper: https://arxiv.org/pdf/1909.08053.pdf

""The resulting aggregate corpus contains 174 GB of deduplicated text.""

174GB * 2e8words/GB = 3.48e10 words
3.48e10 words (if english) *4/3 = 46400000000 tokens

confidence - likely",,,NVIDIA Tesla V100 DGXS 32 GB,Self-supervised learning,Likely,"Turing Natural Language Generation (T-NLG) is a 17 billion parameter language model by Microsoft that outperforms the state of the art on many downstream NLP tasks. We present a demo of the model, including its freeform generation, question answering, and summarization capabilities, to academics for feedback and research purposes. <|endoftext|>",3.39,Turing-NLG,Unreleased,"United States of America,Multinational,India,Belgium",,,,256.0,,2025-05-09 11:32,,,,524288.0,,Industry,,,,,Unreleased,,Industry,$51659.71,,,130885.13499433055,"Third-party estimation,Operation counting",,,,11
ALBERT-xxlarge,Language,"Language modeling/generation,Question answering","Toyota Technological Institute at Chicago,Google","Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut",2020-02-09,ALBERT: A Lite BERT for Self-supervised Learning of Language Representations.,https://arxiv.org/abs/1909.11942,5901.0,Highly cited,,235000000.0,,2.39e+21,"32 hours of training
512 TPU V3s
0.33 utilization rate

123000000000000 FLOP / chip / sec * 512 TPUs * 32 hours * 3600 sec / hour * 0.33 [assumed utilization] = 2.3940956e+21 FLOP

""We train all models for 125,000 steps unless otherwise specified""
""All the model updates use a batch size of 4096 ""
""We always limit the maximum input length to 512, and randomly generate input sequences shorter than 512 with a probability of 10%.""

6 FLOP / parameter / token * 235000000 parameters * 512 tokens per sequence * 4096 sequences per batch * 125000 steps =  3.6962304e+20 FLOP

Authors of ""AI and Memory Wall"" (https://github.com/amirgholami/ai_and_memory_wall) estimated model's training compute as 31,000,000 PFLOP = 3.1*10^22 FLOP","Wikipedia,""BookCorpus (BooksCorpus, Toronto Book Corpus)""","""To keep the comparison as meaningful as possible, we follow the BERT (Devlin et al., 2019) setup in using the BOOKCORPUS (Zhu et al., 2015) and English Wikipedia (Devlin et al., 2019) for pretraining baseline models. These two corpora consist of around 16GB of uncompressed text. W""",3300000000.0,"Pretraining same as for BERT - Wikipedia and BookCorpus

""For the pre-training corpus we use the BooksCorpus (800M words) (Zhu et al., 2015) and English Wikipedia (2,500M words)""",32.0,,Google TPU v3,Self-supervised learning,Speculative,"Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and \squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at this https URL.",79.4,,Open weights (unrestricted),"United States of America,United States of America",,,,512.0,,2025-05-09 11:32,,,,2097152.0,"Sequences are capped at 512 tokens; 10% of the time they'll use an input less than 512 long. Batches are over 4096 sequences. Tokens per batch: 2,097,152","Academia,Industry",,,,,Open source,Apache 2.0 code/weights. repo includes training code: https://github.com/google-research/ALBERT,"Academia,Industry",$4439.92,,,230378.35820081632,"Hardware,Third-party estimation,Operation counting",,,,19
TaLK Convolution,Language,Language modeling,Carleton University,"Vasileios Lioutas, Yuhong Guo",2020-02-08,Time-aware Large Kernel Convolutions,https://arxiv.org/abs/2002.03184,28.0,SOTA improvement,"""[We] set a new state-of-the-art result on the
IWSLT De-En and CNN-DailyMail datasets""",240000000.0,"Table 5

""For the language model, we followed the same configuration
as Baevski & Auli (2019). We used 17 decoding layers, each
layer with a 1024 hidden size, a 4096 feed-forward hidden
size and 8 heads. The adaptive input factor was set to 4.""",2.6990346e+19,6 FLOP / parameter / token * 240000000 parameters * 286000 steps * 65536 tokens per batch = 2.6990346e+19 FLOP,WikiText-103,,103000000.0,""" We replicated their setup and partition the training data into blocks of 512 contiguous tokens""

"" same setup as in Baevski & Auli (2019)""  https://arxiv.org/abs/1809.10853
in that paper they trained for 286k steps in batches of 65,536 tokens.

286000*65536 / 103000000= 182 epochs (same as in as in Baevski & Auli (2019))",,,NVIDIA GeForce RTX 2080 Ti 11GB,,Likely,"To date, most state-of-the-art sequence modeling architectures use attention to build generative models for language based tasks. Some of these models use all the available sequence tokens to generate an attention distribution which results in time complexity of O(n2). Alternatively, they utilize depthwise convolutions with softmax normalized kernels of size k acting as a limited-window self-attention, resulting in time complexity of O(kâ‹…n). In this paper, we introduce Time-aware Large Kernel (TaLK) Convolutions, a novel adaptive convolution operation that learns to predict the size of a summation kernel instead of using a fixed-sized kernel matrix. This method yields a time complexity of O(n), effectively making the sequence encoding process linear to the number of tokens. We evaluate the proposed method on large-scale standard machine translation, abstractive summarization and language modeling datasets and show that TaLK Convolutions constitute an efficient improvement over other attention/convolution based approaches.",182.0,TaLK Convolution,Unreleased,Canada,,,,8.0,,2025-05-28 16:14,,,,65536.0,same setup as in Baevski & Auli (2019),Academia,,,,,Unreleased,"MIT, code and weights (though this repo is for translation not WT-103):

https://github.com/lioutasb/TaLKConvolutions?tab=readme-ov-file",Academia,,,FP16,4090.615920439178,Operation counting,,,,
Meena,Language,Text autocompletion,Google Brain,"Dongling Xiao, Han Zhang, Yukun Li, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang",2020-01-28,Towards a Human-like Open-Domain Chatbot,https://arxiv.org/abs/2001.09977,879.0,SOTA improvement,"""We also propose a human evaluation metric called Sensibleness and
Specificity Average (SSA)... the full version of Meena (with a filtering mechanism and tuned decoding) scores 79% SSA, 23% higher in absolute SSA than the existing chatbots we evaluated""",2600000000.0,"""We present Meena, a multi-turn open-domain chatbot trained end-to-end on data mined and filtered from public domain social media conversations. This 2.6B parameter neural network is simply trained to minimize perplexity of the next token.""",1.12e+23,"https://arxiv.org/ftp/arxiv/papers/2104/2104.10350.pdf
Table 4

In the paper: ""We trained our best model for 30 days on a TPUv3 Pod (2,048 TPU cores) on the Meena dataset containing 40B words (or 61B BPE tokens) [...] by the end of training, the model had traversed the full
training set 164 times (or epochs) and observed a total of about 10T tokens""

Hardware: 30 * 24 * 3600 * (2048/2) * 1.23e14 * 0.3 = 9.794e22
Ops counting: 6 * 10T * 2.6B = 1.56E23
Geometric mean: sqrt(9.79e22*1.56E23) = 1.24e23, very close to the figure in the link above.",,,40000000000.0,"""The final Meena dataset contains 341GB of text
(40B words)""

Converting from GB to words yields 6.8e10, which is in the same OOM",720.0,"We trained our best model for 30 days on a TPUv3 Pod (2,048 TPU cores)",Google TPU v3,Self-supervised learning,Confident,"We present Meena, a multi-turn open-domain chatbot trained end-to-end on data mined and filtered from public domain social media conversations. This 2.6B parameter neural network is simply trained to minimize perplexity of the next token. We also propose a human evaluation metric called Sensibleness and Specificity Average (SSA), which captures key elements of a human-like multi-turn conversation. Our experiments show strong correlation between perplexity and SSA. The fact that the best perplexity end-to-end trained Meena scores high on SSA (72% on multi-turn evaluation) suggests that a human-level SSA of 86% is potentially within reach if we can better optimize perplexity. Additionally, the full version of Meena (with a filtering mechanism and tuned decoding) scores 79% SSA, 23% higher in absolute SSA than the existing chatbots we evaluated.",164.0,,Unreleased,United States of America,,,,1024.0,0.3431,2025-05-16 10:30,,,,82655.0,"61B tokens over 738k training steps, or 82655 tokens per batch on average. Not certain about warmup, etc",Industry,,,,737280.0,Unreleased,,Industry,$206760.38,"Per https://arxiv.org/ftp/arxiv/papers/2104/2104.10350.pdf
1.12e23 FLOPs used
1024 TPUv3s for 30 days: 30 * 24 * 3600 * 1024 * 1.23e14 = 3.2647e23
1.12e23 / 3.2647e23 = 0.3431",,460879.8620037727,"Hardware,Operation counting,Third-party estimation",,,,3
ContextNet + Noisy Student,Speech,Speech recognition,Google,"Daniel S. Park, Yu Zhang, Ye Jia, Wei Han, Chung-Cheng Chiu, Bo Li, Yonghui Wu, Quoc V. Le",2020-01-19,Improved Noisy Student Training for Automatic Speech Recognition,https://arxiv.org/abs/2005.09629v2,225.0,SOTA improvement,"""We are thus able to improve upon the previous state-of-the-art clean/noisy test WERs achieved on LibriSpeech 100h (4.74%/12.20%) and LibriSpeech (1.9%/4.1%)""",,,8.16e+21,"""We train 6 generations of models numbered 0 to 5, where
we count the baseline model trained with the supervised set
as the zeroth generation. Each generation is trained ... on 32 Google
Cloud TPU chips for 10 days.""

The TPU version is likely v3 given this is a 2020 paper.

we get 6 * 10 * 24 * 3600 * 32 * 123 tflops * 0.4  (assumed utilization) = 8.16e21","LibriSpeech,LibriLight","""LibriSpeech 100-860 is a semi-supervised task where the clean 100h subset of LibriSpeech [6] is taken to be the supervised set, while the remaining 860h of audio is taken to be the unlabeled set. The unlabeled audio consists of 360h of clean data and 500h of noisy data. We tokenize the transcripts using a WPM model [37] with vocabulary size 16k constructed from the clean 100h subset transcripts.""

Inputs are mel-spectrograms, but unclear the duration of each.",,,1440.0,roughly 10 days,Google TPU v3,,Confident,"Recently, a semi-supervised learning method known as ""noisy student training"" has been shown to improve image classification performance of deep networks significantly. Noisy student training is an iterative self-training method that leverages augmentation to improve network performance. In this work, we adapt and improve noisy student training for automatic speech recognition, employing (adaptive) SpecAugment as the augmentation method. We find effective methods to filter, balance and augment the data generated in between self-training iterations. By doing so, we are able to obtain word error rates (WERs) 4.2%/8.6% on the clean/noisy LibriSpeech test sets by only using the clean 100h subset of LibriSpeech as the supervised set and the rest (860h) as the unlabeled set. Furthermore, we are able to achieve WERs 1.7%/3.4% on the clean/noisy LibriSpeech test sets by using the unlab-60k subset of LibriLight as the unlabeled set for LibriSpeech 960h. We are thus able to improve upon the previous state-of-the-art clean/noisy test WERs achieved on LibriSpeech 100h (4.74%/12.20%) and LibriSpeech (1.9%/4.1%).",,,Unreleased,United States of America,,,,,,2025-02-17 12:24,,,,,,Industry,,,,,Unreleased,,Industry,$14226.05,,,,Hardware,,,,15
AlphaFold,Biology,"Protein folding prediction,Proteins",DeepMind,"Andrew W. Senior, Richard Evans, John Jumper, James Kirkpatrick, Laurent Sifre, Tim Green, Chongli Qin, Augustin Å½Ã­dek, Alexander W. R. Nelson, Alex Bridgland, Hugo Penedones, Stig Petersen, Karen Simonyan, Steve Crossan, Pushmeet Kohli, David T. Jones, David Silver, Koray Kavukcuoglu, Demis Hassabis",2020-01-15,Improved protein structure prediction using potentials from deep learning,https://www.nature.com/articles/s41586-019-1923-7,2773.0,"SOTA improvement,Highly cited","""AlphaFold represents a considerable advance
in protein-structure prediction."" [Abstract]",16340840.0,"""Neural network hyperparameters"" section of https://www.nature.com/articles/s41586-019-1923-7:
â€œ7 Ã— 4 Blocks with 256 channels, cycling through dilations 1, 2, 4, 8â€
â€œ48 Ã— 4 Blocks with 128 channels, cycling through dilations 1, 2, 4, 8â€

""Distogram prediction"" section:
""For the final layer, a position-specific bias was used""

Extended Data Fig.1 (b): 
Shows that each block consists of 9 layers:
(1) Batch norm
(2) Elu
(3) Project down (halves number of dimensions)
(4) Batch norm
(5) Elu
(6) 3x3 kernel with dilation
(7) Batch norm
(8) Elu
(9) Project up (doubles number of dimensions)

Dilations don't change the number of parameters in each filter
Assuming that projection layers are convolutional layers with 1x1 kernels

Parameter estimate for each layer in a 256 channel block:
(1) 256*2            = 512
(2) 0
(3) 1*1*256*128 = 32768
(4) 128*2            = 256 
(5) 0
(6) 3*3*128*128 = 147456
(7) 128*2            = 256 
(8) 0
(9) 1*1*128*256 + 256 = 33024
Total                             = 214272

Parameter estimate for each layer in a 128 channel block:
(1) 128*2            = 256
(2) 0
(3) 1*1*128*64   = 8192
(4) 64*2              = 128
(5) 0
(6) 3*3*64*64     = 36864
(7) 64*2              = 128
(8) 0
(9) 1*1*64*128 + 128 = 8320
Total                   = 53897

Estimate total network = 7*4*214272 + 48*4*53897 = 5992616 + 10348224
                                     = 16340840
                                     ~ 16e6

Within a factor of 2 of the estimate of 21M parameters stated in: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7305407/

[Previous approximation: 7 * 4 * 256 * 3 * 3 * 256 + 48 * 4 * 128 * 3 * 3 * 128 = 44826624]",1e+20,"Estimated in the blogpost below

https://www.lesswrong.com/posts/wfpdejMWog4vEDLDg/ai-and-compute-trend-isn-t-predictive-of-what-is-happening

""AlphaFold: they say they trained on GPU and not TPU. Assuming V100 GPU, it's 5 days * 24 hours/day * 3600 sec/hour * 8 V100 GPU * 100*10^12 FLOP/s * 33% actual GPU utilization = 10^20 FLOP.""","PDB (Protein Data Bank),UniRef30 (FKA UniClust30)","""Our models are trained on structures extracted from the PDB"" [""Data"" section]

""For each training sequence, we searched for and aligned to the training sequence similar protein sequences in the Uniclust3035 dataset"" [""Data"" section]",300000001.0,"Training Domains: 29,427
Average Residues per Domain: 100
Data Points per Domain: 100 Ã— 100 = 10,000
Total Data Points = 29,427 Ã— 10,000 = 294,270,000 â‰ˆ 3.0 Ã— 10â¸",120.0,"""Training time: about 5 days for 600,000 steps""",,Self-supervised learning,Speculative,"Protein structure prediction can be used to determine the three-dimensional shape of a protein from its amino acid sequence. This problem is of fundamental importance as the structure of a protein largely determines its function; however, protein structures can be difficult to determine experimentally. Considerable progress has recently been made by leveraging genetic information. It is possible to infer which amino acid residues are in contact by analysing covariation in homologous sequences, which aids in the prediction of protein structures. Here we show that we can train a neural network to make accurate predictions of the distances between pairs of residues, which convey more information about the structure than contact predictions. Using this information, we construct a potential of mean force that can accurately describe the shape of a protein. We find that the resulting potential can be optimized by a simple gradient descent algorithm to generate structures without complex sampling procedures. The resulting system, named AlphaFold, achieves high accuracy, even for sequences with fewer homologous sequences. In the recent Critical Assessment of Protein Structure Prediction (CASP13)â€”a blind assessment of the state of the fieldâ€”AlphaFold created high-accuracy structures (with template modelling (TM) scores of 0.7 or higher) for 24 out of 43 free modelling domains, whereas the next best method, which used sampling and contact information, achieved such accuracy for only 14 out of 43 domains. AlphaFold represents a considerable advance in protein-structure prediction. We expect this increased accuracy to enable insights into the function and malfunction of proteins, especially in cases for which no structures for homologous proteins have been experimentally determined.",,,Unreleased,United Kingdom of Great Britain and Northern Ireland,,,,,,2025-05-09 11:32,,,,,,Industry,,,,,Unreleased,,Industry,,,,,"Hardware,Third-party estimation",,,,
DD-PPO,Robotics,Object detection,"Georgia Institute of Technology,Facebook AI Research,Oregon State University,Simon Fraser University","Erik Wijmans, Abhishek Kadian, Ari Morcos, Stefan Lee, Irfan Essa, Devi Parikh, Manolis Savva, Dhruv Batra",2019-12-19,DD-PPO: Learning Near-Perfect PointGoal Navigators from 2.5 Billion Frames,https://openreview.net/forum?id=H1gX8C4YPr,411.0,SOTA improvement,"""This agent achieves state-of-art on the Habitat Challenge 2019 RGB track (rank 2 entry has 0.89 SPL).""",,"no parameter count but some architecture details: ""The policy is parameterized by a 2-layer LSTM with a 512-dimensional hidden state. It takes three inputs: the previous action, the target relative to the current state, and the output of the visual encoder. The LSTMâ€™s output is used to produce a softmax distribution over the action space and an estimate of the value function. See Appendix C for full details.""",7.8e+20,"""Using DD-PPO, we train agents for 2.5 Billion steps of experience with 64 Tesla V100 GPUs in 2.75 days â€“ 180 GPU-days of training""

125 teraFLOP/s (exact V100 model not specified) * 180 * 24 * 3600 * 0.4 (assumed utilization) = 7.8e20",,"""We experiment with several different sources of data. First, we utilize the training data released
as part of the Habitat Challenge 2019, consisting of 72 scenes from the Gibson dataset (Xia et al.,
2018). We then augment this with all 90 scenes in the Matterport3D dataset (Chang et al., 2017) to
create a larger training set (note that Matterport3D meshes tend to be larger and of better quality).2
Furthermore, Savva et al. (2019) curated the Gibson dataset by rating every mesh reconstruction on
a quality scale of 0 to 5 and then filtered all splits such that each only contains scenes with a rating of
4 or above (Gibson-4+), leaving all scenes with a lower rating previously unexplored. We examine
training on the 332 scenes from the original train split with a rating of 2 or above (Gibson-2+).""",,,66.0,2.75 days,NVIDIA V100,Reinforcement learning,Likely,"We present Decentralized Distributed Proximal Policy Optimization (DD-PPO), a method for distributed reinforcement learning in resource-intensive simulated environments. DD-PPO is distributed (uses multiple machines), decentralized (lacks a centralized server), and synchronous (no computation is ever ""stale""), making it conceptually simple and easy to implement. In our experiments on training virtual robots to navigate in Habitat-Sim, DD-PPO exhibits near-linear scaling -- achieving a speedup of 107x on 128 GPUs over a serial implementation. We leverage this scaling to train an agent for 2.5 Billion steps of experience (the equivalent of 80 years of human experience) -- over 6 months of GPU-time training in under 3 days of wall-clock time with 64 GPUs. ",,,Unreleased,"United States of America,United States of America,United States of America,Canada",,,,64.0,,2025-05-28 16:14,,,,,,"Academia,Industry,Academia,Academia",,,,,Unreleased,MIT license for environment used to train. doesn't seem like it has training code for this model. https://github.com/facebookresearch/habitat-lab,"Academia,Industry,Academia,Academia",$1926.90,,FP32,39314.53850266432,Hardware,,,,
SeqVec,Biology,Proteins,Technical University of Munich,"Michael Heinzinger, Ahmed Elnaggar, Yu Wang, Christian Dallago, Dmitrii Nechaev, Florian Matthes & Burkhard Rost ",2019-12-17,Modeling aspects of the language of life through transfer-learning protein sequences,https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-019-3220-8,,,,93000000.0,"""The model had about 93â€‰M (mega/million) free parameters""",4.1e+19,"3 weeks, 5 NVIDIA Titan GPUs (Assuming NVIDIA Titan V and 30% utilization rate for calculation) with 12 GB memory, ",UniRef50,UniRef50,9500000000.0,"""We found UniRef50 to contain almost ten times more tokens (9.5 billion amino acids) than the largest existing NLP corpus (1 billion words)""
Elsewhere notes 9.6B, possibly one figure is rounded.",508.0,,NVIDIA Titan V,,Confident,"Background
Predicting protein function and structure from sequence is one important challenge for computational biology. For 26â€‰years, most state-of-the-art approaches combined machine learning and evolutionary information. However, for some applications retrieving related proteins is becoming too time-consuming. Additionally, evolutionary information is less powerful for small families, e.g. for proteins from the Dark Proteome. Both these problems are addressed by the new methodology introduced here.
Results
We introduced a novel way to represent protein sequences as continuous vectors (embeddings) by using the language model ELMo taken from natural language processing. By modeling protein sequences, ELMo effectively captured the biophysical properties of the language of life from unlabeled big data (UniRef50). We refer to these new embeddings as SeqVec (Sequence-to-Vector) and demonstrate their effectiveness by training simple neural networks for two different tasks. At the per-residue level, secondary structure (Q3â€‰=â€‰79%â€‰Â±â€‰1, Q8â€‰=â€‰68%â€‰Â±â€‰1) and regions with intrinsic disorder (MCCâ€‰=â€‰0.59â€‰Â±â€‰0.03) were predicted significantly better than through one-hot encoding or through Word2vec-like approaches. At the per-protein level, subcellular localization was predicted in ten classes (Q10â€‰=â€‰68%â€‰Â±â€‰1) and membrane-bound were distinguished from water-soluble proteins (Q2â€‰=â€‰87%â€‰Â±â€‰1). Although SeqVec embeddings generated the best predictions from single sequences, no solution improved over the best existing method using evolutionary information. Nevertheless, our approach improved over some popular methods using evolutionary information and for some proteins even did beat the best. Thus, they prove to condense the underlying principles of protein sequences. Overall, the important novelty is speed: where the lightning-fast HHblits needed on average about two minutes to generate the evolutionary information for a target protein, SeqVec created embeddings on average in 0.03â€‰s. As this speed-up is independent of the size of growing sequence databases, SeqVec provides a highly scalable approach for the analysis of big data in proteomics, i.e. microbiome or metaproteome analysis.
Conclusion
Transfer-learning succeeded to extract information from unlabeled sequence databases relevant for various protein prediction tasks. SeqVec modeled the language of life, namely the principles underlying protein sequences better than any features suggested by textbooks and prediction methods. The exception is evolutionary information, however, that information is not available on the level of a single sequence.",,,Open weights (unrestricted),Germany,,,,5.0,,2025-04-30 10:04,,,,,,Academia,,,,2540.0,Unreleased,"MIT license. doesn't look like it has training code
https://github.com/rostlab/SeqVec",Academia,,,,2559.654268344264,Hardware,,,,
OpenAI Five,Games,Dota 2,OpenAI,"Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, PrzemysÅ‚aw DÄ™biak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, Rafal JÃ³zefowicz, Scott Gray, Catherine Olsson, Jakub Pachocki, Michael Petrov, Henrique P. d.O. Pinto, Jonathan Raiman, Tim Salimans, Jeremy Schlatter, Jonas Schneider, Szymon Sidor, Ilya Sutskever, Jie Tang, Filip Wolski, Susan Zhang",2019-12-13,Dota 2 with Large Scale Deep Reinforcement Learning,https://arxiv.org/abs/1912.06680,1629.0,"Highly cited,SOTA improvement","""On April 13th, 2019, OpenAI Five became the first AI system to defeat the world champions at an esports game.""",159000000.0,"""We define a policy (Ï€) as a function from the history of observations to a probability distribution over actions, which we parameterize as a recurrent neural network with approximately 159 million parameters (Î¸)."" pg. 3 of paper

source: https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389",6.7e+22,"""770Â±50 PFlops/sÂ·days of compute"" for the model that played against world champions. They did a single training run that took 10 months.

While the model was playing against world champions, they continued training for a few days, so that the resulting model used even more training compute: 820Â±50 PFlops/sÂ·days.

Finally, they also trained a Rerun model with 150Â±5 PFlops/sÂ·days of compute.

Source: Dota 2 with Large Scale Deep Reinforcement Learning
https://arxiv.org/abs/1912.06680

You cannot multiply the hardware quantity by training time to get the quantity of GPU-hours! Page 5: "" the number of GPUs (up to 1536 at the peak)""

From this NVIDIA blogpost, it appears they were using P100s:
https://developer.nvidia.com/blog/ai-learns-to-play-dota-2-with-human-precision/#:~:text=AI%20Learns%20to%20Play%20Dota,The%20neural",,,454321373184.0,"""Although the Dota 2 engine runs at 30 frames per second, OpenAI Five only acts on every 4th
frame which we call a timestep""
--> 7.5 timesteps/s

""OpenAI Five is a single training run that ran from June 30th, 2018 to April 22nd, 2019. "" --> 296 days

296 * 24*3600 * 7.5 = 1.92e8

This number seems a little low? The DQN paper had 1e7 timesteps. Might be to do with sample efficiency?

EDIT 14/06/2022
Multiple copies of OpenAI Five were trained in parallel, so the total training time is much higher than 296 days.
Table 1 shows 220,000 GPU iterations, each iteration has a batch size of between 1M and 3M timesteps (Table 2), so the total number of episodes is on the order of 2e11",7104.0,"""OpenAI Five is a single training run that ran from June 30th, 2018 to April 22nd, 2019. "" --> 296 days",NVIDIA P100,Self-supervised learning,Confident,"On April 13th, 2019, OpenAI Five became the first AI system to defeat the world champions at an esports game. The game of Dota 2 presents novel challenges for AI systems such as long time horizons, imperfect information, and complex, continuous state-action spaces, all challenges which will become increasingly central to more capable AI systems. OpenAI Five leveraged existing reinforcement learning techniques, scaled to learn from batches of approximately 2 million frames every 2 seconds. We developed a distributed training system and tools for continual training which allowed us to train OpenAI Five for 10 months. By defeating the Dota 2 world champion (Team OG), OpenAI Five demonstrates that self-play reinforcement learning can achieve superhuman performance on a difficult task.
",,,Unreleased,United States of America,,,,1536.0,,2025-05-09 11:32,,,,,,Industry,,,,10911744.0,Unreleased,,Industry,,,,786395.8382790724,,,,,3
OpenAI Five Rerun,Games,Dota 2,OpenAI,"Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung,
PrzemysÅ‚aw â€œPsyho"" DÄ™biak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, Rafal JÃ³zefowicz, Scott Gray, Catherine Olsson, Jakub Pachocki, Michael Petrov, Henrique PondÃ© de Oliveira Pinto, Jonathan Raiman, Tim Salimans, Jeremy Schlatter, Jonas Schneider, Szymon Sidor, Ilya Sutskever, Jie Tang, Filip Wolski, Susan Zhang",2019-12-13,Dota 2 with Large Scale Deep Reinforcement Learning,https://cdn.openai.com/dota-2.pdf,1629.0,"Highly cited,SOTA improvement","""On April 13th, 2019, OpenAI Five became the first AI system to defeat the world champions at an esports game.""",159000000.0,"""We define a policy (Ï€) as a function from the history of observations to a probability distribution
over actions, which we parameterize as a recurrent neural network with approximately 159 million
parameters (Î¸)."" pg. 3 of paper

source: https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389",1.3e+22,"THIS CALCULATION IS FOR RERUN

""Rerun took 2 months and 150 Â± 5 PFlops/sÂ·days of compute (see Figure 4)""



source: https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389",,,53084160000.0,"54k iterations (Fig 7)
with a batch size of 983040 (Table 2)",,,NVIDIA P100,Self-supervised learning,Confident,"On April 13th, 2019, OpenAI Five became the first AI system to defeat the world champions at an esports game. The game of Dota 2 presents novel challenges for AI systems such as long time horizons, imperfect information, and complex, continuous state-action spaces, all challenges which will become increasingly central to more capable AI systems. OpenAI Five leveraged existing reinforcement learning techniques, scaled to learn from batches of approximately 2 million frames every 2 seconds. We developed a distributed training system and tools for continual training which allowed us to train OpenAI Five for 10 months. By defeating the Dota 2 world champion (Team OG), OpenAI Five demonstrates that self-play reinforcement learning can achieve superhuman performance on a difficult task.",,,Unreleased,United States of America,,,,512.0,,2025-06-18 10:53,,,,,,Industry,,,,,Unreleased,,Industry,,,,262131.94609302413,Third-party estimation,,,,10
MMLSTM,Language,Language modeling,"Beijing University of Posts and Telecommunications,University of West London","Kai Shuang, Rui Li, Mengyu Gu, Jonathan Loo, Sen Su",2019-12-05,Majorâ€“Minor Long Short-Term Memory for Word-Level Language Model,http://repository.uwl.ac.uk/id/eprint/6490/1/Loo_etal_IEEE_TNNLS_2019_Major-minor_long_short-term_memory_for_word-level_language_model.pdf,17.0,SOTA improvement,"""In experiments, we demonstrate the language model with MMLSTMs surpasses the existing state-of-the-art model on Penn Treebank (PTB) and WikiText-2 (WT2) datasets""",75000000.0,Table VII,2.3175e+18,6 FLOP / token / parameter * 75000000 parameters * 103000000 tokens * 50 epochs [assumption] = 2.3175e+18 FLOP,WikiText-103,,103000000.0,,,,,,Likely,"Abstractâ€”Language model plays an important role in natural
language processing (NLP) systems like machine translation,
speech recognition, learning token embeddings, natural language generation and text classification. Recently, the multi-layer Long Short-Term Memory (LSTM) models have been demonstrated to achieve promising performance on word-level language modeling.
For each LSTM layer, larger hidden size usually means more
diverse semantic features, which enables the language model to
perform better. However, we have observed that when a certain
LSTM layer reaches a sufficiently large scale, the promotion
of overall effect will slow down as its hidden size increases. In
this paper, we analyze that an important factor leading to this
phenomenon is the high correlation between the newly extended hidden states and original hidden states, which hinders diverse feature expression of the LSTM. As a result, when the scale is large enough, simply lengthening the LSTM hidden states will cost tremendous extra parameters but has little effect. We propose a simple yet effective improvement on each LSTM layer consisting of a large-scale Major LSTM and a smallscale Minor LSTM to break the high correlation between the two parts of hidden states, which we call Major-Minor LSTMs (MMLSTMs). In experiments, we demonstrate the language model with MMLSTMs surpasses the existing state-of-the-art model on Penn Treebank (PTB) and WikiText-2 (WT2) datasets, and outperforms the baseline by 3.3 points in perplexity on WikiText-103 dataset without increasing model parameter counts.",50.0,MMLSTM,Unreleased,"China,United Kingdom of Great Britain and Northern Ireland",,,,,,2025-04-10 12:24,,,,,,"Academia,Academia",,,,,Unreleased,,"Academia,Academia",,,,,Operation counting,,,,
StarGAN v2,"Vision,Image generation",Image generation,"NAVER,Yonsei University,Swiss Federal Institute of Technology","Yunjey Choi, Youngjung Uh, Jaejun Yoo, Jung-Woo Ha",2019-12-04,StarGAN v2: Diverse Image Synthesis for Multiple Domains,https://arxiv.org/abs/1912.01865,1560.0,"Highly cited,SOTA improvement","""Votes from AMT workers for the most preferred method
regarding visual quality and style reflection (%). StarGAN v2 outperforms the baselines with remarkable margins in all aspects.""",,,,,"CelebA,AFHQ","""Datasets. We evaluate StarGAN v2 on CelebA-HQ [21] and our new AFHQ dataset (Appendix A)""",,,,,,,Unknown,"A good image-to-image translation model should learn a mapping between different visual domains while satisfying the following properties: 1) diversity of generated images and 2) scalability over multiple domains. Existing methods address either of the issues, having limited diversity or multiple models for all domains. We propose StarGAN v2, a single framework that tackles both and shows significantly improved results over the baselines. Experiments on CelebA-HQ and a new animal faces dataset (AFHQ) validate our superiority in terms of visual quality, diversity, and scalability. To better assess image-to-image translation models, we release AFHQ, high-quality animal faces with large inter- and intra-domain differences. The code, pretrained models, and dataset can be found at this https URL.",,,Open weights (non-commercial),"Korea (Republic of),Korea (Republic of),Switzerland",,,,,,2025-02-03 15:25,,,,,,"Industry,Academia,Academia",,,,,Open (non-commercial),https://github.com/clovaai/stargan-v2?tab=readme-ov-file non-commercial,"Industry,Academia,Academia",,,,,,,,,
bRSM + cache,Language,Language modeling,"Numenta,Incubator 491","Jeremy Gordon, David Rawlinson, Subutai Ahmad",2019-12-02,Long Distance Relationships without Time Travel: Boosting the Performance of a Sparse Predictive Autoencoder in Sequence Modeling,https://arxiv.org/abs/1912.01116,4.0,,,2550000.0,Table 2,275400000000000.0,"6 FLOP / parameter / token * 2550000 parameters * 300 tokens per batch [speculatively, I am not sure if it is sequences per batch or tokens per batch] * 60000 steps = 2.754e+14 FLOP",Penn TreeBank (PTB),,912344.0,"batch size 300 

""the bRSM model overfits quickly to the PTB training
set, as illustrated by increasing volatility and ultimately a quick rise in test loss after 40-60,000 mini-batches of training.""

300*60000 / 912344 =  19.7 epochs
",,,,,Speculative,"In sequence learning tasks such as language modelling, Recurrent Neural Networks must learn relationships between input features separated by time. State of the art models such as LSTM and Transformer are trained by backpropagation of losses into prior hidden states and inputs held in memory. This allows gradients to flow from present to past and effectively learn with perfect hindsight, but at a significant memory cost. In this paper we show that it is possible to train high performance recurrent networks using information that is local in time, and thereby achieve a significantly reduced memory footprint. We describe a predictive autoencoder called bRSM featuring recurrent connections, sparse activations, and a boosting rule for improved cell utilization. The architecture demonstrates near optimal performance on a non-deterministic (stochastic) partially-observable sequence learning task consisting of high-Markov-order sequences of MNIST digits. We find that this model learns these sequences faster and more completely than an LSTM, and offer several possible explanations why the LSTM architecture might struggle with the partially observable sequence structure in this task. We also apply our model to a next word prediction task on the Penn Treebank (PTB) dataset. We show that a 'flattened' RSM network, when paired with a modern semantic word embedding and the addition of boosting, achieves 103.5 PPL (a 20-point improvement over the best N-gram models), beating ordinary RNNs trained with BPTT and approaching the scores of early LSTM implementations. This work provides encouraging evidence that strong results on challenging tasks such as language modelling may be possible using less memory intensive, biologically-plausible training regimes",19.0,bRSM + cache,Unreleased,"United States of America,Australia",,,,,,2025-04-10 12:43,,,,,,"Industry,Industry",,,,,Open source,GNU (copyleft) license for code: https://github.com/numenta/nupic.research/tree/master/projects/rsm ,"Industry,Industry",,,,,Operation counting,,,,
Transformer-XL DeFINE (141M),Language,Language modeling,"University of Washington,Allen Institute for AI","Sachin Mehta, Rik Koncel-Kedziorski, Mohammad Rastegari, Hannaneh Hajishirzi",2019-11-27,DeFINE: DEep Factorized INput Token Embeddings for Neural Sequence Modeling,https://arxiv.org/abs/1911.12385,22.0,SOTA improvement,"""Compared to state-of-the-art methods including adaptive input representations,
this technique results in a 6% to 20% drop in perplexity""",141000000.0,Table 2b,1.74276e+18,"6 FLOP / token / parameter * 141000000 parameters * 103000000 tokens * 20 epochs [assumption based on number of epochs for LSTMs] =  1.74276e+18 FLOP

_______________
older estimation:  6.2 Ã— 10^18 (no explantion how it was calculated)",WikiText-103,,103000000.0,,,,,,Speculative,"For sequence models with large vocabularies, a majority of network parameters lie in the input and output layers. In this work, we describe a new method, DeFINE, for learning deep token representations efficiently. Our architecture uses a hierarchical structure with novel skip-connections which allows for the use of low dimensional input and output layers, reducing total parameters and training time while delivering similar or better performance versus existing methods. DeFINE can be incorporated easily in new or existing sequence models. Compared to state-of-the-art methods including adaptive input representations, this technique results in a 6% to 20% drop in perplexity. On WikiText-103, DeFINE reduces the total parameters of Transformer-XL by half with minimal impact on performance. On the Penn Treebank, DeFINE improves AWD-LSTM by 4 points with a 17% reduction in parameters, achieving comparable performance to state-of-the-art methods with fewer parameters. For machine translation, DeFINE improves the efficiency of the Transformer model by about 1.4 times while delivering similar performance.",20.0,Transformer-XL DeFINE (141M),Unreleased,"United States of America,United States of America",,,,,,2025-05-28 16:14,,,,,,"Academia,Research collective",,,,,Unreleased,,"Academia,Research collective",,,FP32,,Operation counting,,,,
Photo-Geometric Autoencoder,"3D modeling,Vision",3D reconstruction,University of Oxford,"Shangzhe Wu, Christian Rupprecht, Andrea Vedaldi
",2019-11-25,Unsupervised Learning of Probably Symmetric Deformable 3D Objects From Images in the Wild,https://arxiv.org/abs/1911.11130,292.0,SOTA improvement,"""Our model outperforms a
current state-of-the-art 3D reconstruction method that uses 2D keypoint supervision""",,,,,"CelebA,3DFAW,BFM","""We test our method on three human face datasets: CelebA [35], 3DFAW [21, 27, 73, 69] and BFM [47]""",,,,,,,Unknown,"We propose a method to learn 3D deformable object categories from raw single-view images, without external supervision. The method is based on an autoencoder that factors each input image into depth, albedo, viewpoint and illumination. In order to disentangle these components without supervision, we use the fact that many object categories have, at least in principle, a symmetric structure. We show that reasoning about illumination allows us to exploit the underlying object symmetry even if the appearance is not symmetric due to shading. Furthermore, we model objects that are probably, but not certainly, symmetric by predicting a symmetry probability map, learned end-to-end with the other components of the model. Our experiments show that this method can recover very accurately the 3D shape of human faces, cat faces and cars from single-view images, without any supervision or a prior shape model. On benchmarks, we demonstrate superior accuracy compared to another method that uses supervision at the level of 2D image correspondences.",30.0,,Open weights (unrestricted),United Kingdom of Great Britain and Northern Ireland,,,,,,2025-02-03 18:37,,,,,,Academia,,,,,Open source,MIT license: https://github.com/elliottwu/unsup3d,Academia,,,,,,,,,
FastSpeech,Speech,Text-to-speech,"Zhejiang University (ZJU),Microsoft Research","Yi Ren, Yangjun Ruan, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, Tie-Yan Liu",2019-11-20,"FastSpeech: Fast, Robust and Controllable Text to Speech",https://arxiv.org/abs/1905.09263,,,,30100000.0,30.1M,7.1712e+18,125000000000000*53.12*3600*0.3 = 7.1712e+18,LJSpeech,"We conduct experiments on LJSpeech dataset [10], which contains 13,100 English audio clips and the corresponding text transcripts, with the total audio length of approximate 24 hours. We randomly split the dataset into 3 sets: 12500 samples for training, 300 samples for validation and 300 samples for testing.",,,,53.12 h (Table 2 https://arxiv.org/pdf/2006.04558),NVIDIA V100,,Likely,"Neural network based end-to-end text to speech (TTS) has significantly improved the quality of synthesized speech. Prominent methods (e.g., Tacotron 2) usually first generate mel-spectrogram from text, and then synthesize speech from the mel-spectrogram using vocoder such as WaveNet. Compared with traditional concatenative and statistical parametric approaches, neural network based end-to-end models suffer from slow inference speed, and the synthesized speech is usually not robust (i.e., some words are skipped or repeated) and lack of controllability (voice speed or prosody control). In this work, we propose a novel feed-forward network based on Transformer to generate mel-spectrogram in parallel for TTS. Specifically, we extract attention alignments from an encoder-decoder based teacher model for phoneme duration prediction, which is used by a length regulator to expand the source phoneme sequence to match the length of the target mel-spectrogram sequence for parallel mel-spectrogram generation. Experiments on the LJSpeech dataset show that our parallel model matches autoregressive models in terms of speech quality, nearly eliminates the problem of word skipping and repeating in particularly hard cases, and can adjust voice speed smoothly. Most importantly, compared with autoregressive Transformer TTS, our model speeds up mel-spectrogram generation by 270x and the end-to-end speech synthesis by 38x. Therefore, we call our model FastSpeech.",,,Unreleased,"China,United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational",,,,4.0,,2025-05-01 10:42,,,,,,"Academia,Industry",,,,53.0,Unreleased,,"Academia,Industry",,,,2458.746030907577,Hardware,,,,
MuZero,Games,Atari,DeepMind,"J Schrittwieser, I Antonoglou, T Hubert, K Simonyan",2019-11-19,Mastering Atari Go Chess and Shogi by Planning with a Learned Model,https://arxiv.org/abs/1911.08265v2,1768.0,"Highly cited,SOTA improvement",,36864000.0,"Both the representation and dynamics function use the same architecture asAlphaZero, but with 16 instead of20 residual blocks [15]. We use 3x3 kernels and 256 hidden planes for each convolution.

Previous downsampling:
â€¢  1 convolution with stride 2 and 128 output planes, output resolution 48x48.â€¢  2 residual blocks with 128 planesâ€¢  1 convolution with stride 2 and 256 output planes, output resolution 24x24.â€¢  3 residual blocks with 256 planes.â€¢  Average pooling with stride 2, output resolution 12x12.â€¢  3 residual blocks with 256 planes.â€¢  Average pooling with stride 2, output resolution 6x6.",4.8e+19,"third-generation Google Cloud TPU
(For each board game, we used 16 TPUs for training and 1000 TPUs for self-play)
For each game in Atari, we used 8 TPUs for training and 32 TPUs for self-play
Training for 12 hours (for Atari)
Data from Parameter, Compute and Data Trends in Machine Learning
Google v3 TPU: 1.23E+14 FLOP/s (although with the caveat that it might be not applicable)
Utilization rate 
In LaMDA: Language Models for Dialog Applications, they report for TPU V3: 56.5%
Calculations for Atari:
12 hours â†’ 43200 seconds
(8 TPUs for training) * (1.23*10^14 FLOP/s) * (43.2 *10^3 s) * (0.565 utilization rate) = 2.4017472 * 10^19 FLOP
Training time missing for boardgames
Assumption also 12 hours 
Also: 2.4017472 * 10^19 FLOP
Total cost â‰ˆ 4.8 * 10^19 FLOP",,,20000000000.0,"Table 1
https://arxiv.org/pdf/1911.08265.pdf",,,,Self-supervised learning,Speculative,"Constructing agents with planning capabilities has long been one of the main challenges in the pursuit of artificial intelligence. Tree-based planning methods have enjoyed huge success in challenging domains, such as chess and Go, where a perfect simulator is available. However, in real-world problems the dynamics governing the environment are often complex and unknown. In this work we present the MuZero algorithm which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging and visually complex domains, without any knowledge of their underlying dynamics. MuZero learns a model that, when applied iteratively, predicts the quantities most directly relevant to planning: the reward, the action-selection policy, and the value function. When evaluated on 57 different Atari games - the canonical video game environment for testing AI techniques, in which model-based planning approaches have historically struggled - our new algorithm achieved a new state of the art. When evaluated on Go, chess and shogi, without any knowledge of the game rules, MuZero matched the superhuman performance of the AlphaZero algorithm that was supplied with the game rules.",,,Unreleased,United Kingdom of Great Britain and Northern Ireland,,,,,,2025-06-18 10:49,,,,,,Industry,,,,,Unreleased,,Industry,,,,,Hardware,,,,
Transformer - LibriVox + Decoding/Rescoring,Speech,Speech recognition,Facebook,"Gabriel Synnaeve, Qiantong Xu, Jacob Kahn, Tatiana Likhomanenko, Edouard Grave, Vineel Pratap, Anuroop Sriram, Vitaliy Liptchinsky, Ronan Collobert",2019-11-19,End-to-end ASR: from Supervised to Semi-Supervised Learning with Modern Architectures,https://arxiv.org/abs/1911.08460v3,238.0,SOTA improvement,"""Results with decoding/rescoring are shown in Table 2, where we reach 2.09% and 4.11% on test-clean and test-other , respectively, and are further improvements on the state-of-the-art.""",296000000.0,Table 2,,"""Models are trained on 64 GPUs each with an overall batch size of 256 for ResNet and TDS and 320 for Transformer. With only LIBRISPEECH, all models converged in under a week; with pseudo-labels from LIBRIVOX, training required 2-3 weeks""

GPU not specified","LibriSpeech,LibriVox","""LIBRIVOX2
is a large collection of freely-available audio books. Using tools provided with the LIBRILIGHT dataset [26], we select 72K hours of read speech from English book listings and run several preprocessing
steps. After filtering samples to remove readings of duplicate text and corrupted audio, we remove all audio for which
the speaker has overlap with a sample in LIBRISPEECH... the resulting audio corpus contains 53.8K hours of read speech.""",,"""the resulting audio corpus contains 53.8K hours of read speech""",,,,,Confident,"We study pseudo-labeling for the semi-supervised training of ResNet, Time-Depth Separable ConvNets, and Transformers for speech recognition, with either CTC or Seq2Seq loss functions. We perform experiments on the standard LibriSpeech dataset, and leverage additional unlabeled data from LibriVox through pseudo-labeling. We show that while Transformer-based acoustic models have superior performance with the supervised dataset alone, semi-supervision improves all models across architectures and loss functions and bridges much of the performance gaps between them. In doing so, we reach a new state-of-the-art for end-to-end acoustic models decoded with an external language model in the standard supervised learning setting, and a new absolute state-of-the-art with semi-supervised training. Finally, we study the effect of leveraging different amounts of unlabeled audio, propose several ways of evaluating the characteristics of unlabeled audio which improve acoustic modeling, and show that acoustic models trained with more audio rely less on external language models.",,,Open weights (unrestricted),United States of America,,,,,,2024-11-01 10:03,,,,,,Industry,,,,,,BSD license: https://github.com/jakeju/wav2letter?tab=License-1-ov-file#readme,Industry,,,,,,,,,
Long-range sequence Compressive Transformers,Language,Language modeling,DeepMind,"Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Timothy P. Lillicrap",2019-11-13,Compressive Transformers for Long-Range Sequence Modelling,https://arxiv.org/abs/1911.05507,541.0,,,,eighteen-layered Compressive Transformer,1.0202112e+20,123000000000000 FLOP / second/ GPU * 64 GPUs * 12 hours * 3600 sec/hour * 0.3 [assumed utilization] = 1.0202112e+20 FLOP,WikiText-103,,103000000.0,"""for word-based LM we used 16, 000 warmup steps with 500, 000 decay steps""
""a sequence window size all equal to 512""
"" a total batch size of 128""

516000*512*128/103000000 = 328.32 epochs",12.0,"""The model converged in a little over 12 hours.""",Google TPU v3,,Confident,"We present the Compressive Transformer, an attentive sequence model which compresses past memories for long-range sequence learning. We find the Compressive Transformer obtains state-of-the-art language modelling results in the WikiText-103 and Enwik8 benchmarks, achieving 17.1 ppl and 0.97 bpc respectively. We also find it can model high-frequency speech effectively and can be used as a memory mechanism for RL, demonstrated on an object matching task. To promote the domain of long-range sequence learning, we propose a new open-vocabulary language modelling benchmark derived from books, PG-19.",328.32,Compressive Transformers for Long-Range Sequence Modelling,Unreleased,United Kingdom of Great Britain and Northern Ireland,,,,64.0,,2025-05-01 10:42,,,,65536.0,512*128,Industry,,,,,Unreleased,,Industry,,,,28853.7843015362,Hardware,,,,
Noisy Student (L2),Vision,Image classification,"Carnegie Mellon University (CMU),Google","Q Xie, MT Luong, E Hovy, QV Lee",2019-11-11,Self-training with Noisy Student improves ImageNet classification,https://arxiv.org/abs/1911.04252v4,2217.0,"Highly cited,SOTA improvement","""Noisy Student Training achieves 88.4% top-1 accuracy on ImageNet, which is 2.0% better than the state-of-the-art model""",480000000.0,,2.612e+22,"""Our largest model, EfficientNet-L2, needs to be trained for 6 days on a Cloud TPU v3 Pod, which has 2048 cores, if the unlabeled batch size is 14x the labeled batch size""
TPU v3 gets 1.23e14 FLOP/s per chip, with 2 cores per chip

1024 * 1.23e14 * 6 * 24 * 3600 * 0.4 = 2.612e22","ImageNet,JFT",,81000000.0,"""Due to duplications, there are only 81M unique images among these 130M images.""",144.0,6 days,Google TPU v3,,Confident,"We present Noisy Student Training, a semi-supervised learning approach that works well even when labeled data is abundant. Noisy Student Training achieves 88.4% top-1 accuracy on ImageNet, which is 2.0% better than the state-of-the-art model that requires 3.5B weakly labeled Instagram images. On robustness test sets, it improves ImageNet-A top-1 accuracy from 61.0% to 83.7%, reduces ImageNet-C mean corruption error from 45.7 to 28.3, and reduces ImageNet-P mean flip rate from 27.8 to 12.2.
Noisy Student Training extends the idea of self-training and distillation with the use of equal-or-larger student models and noise added to the student during learning. On ImageNet, we first train an EfficientNet model on labeled images and use it as a teacher to generate pseudo labels for 300M unlabeled images. We then train a larger EfficientNet as a student model on the combination of labeled and pseudo labeled images. We iterate this process by putting back the student as the teacher. During the learning of the student, we inject noise such as dropout, stochastic depth, and data augmentation via RandAugment to the student so that the student generalizes better than the teacher. Models are available at this https URL. Code is available at this https URL.",,,Unreleased,"United States of America,United States of America",,,,1024.0,,2025-06-18 10:14,,,,,,"Academia,Industry",,,,,Open source,apache 2.0 license: https://github.com/google-research/noisystudent train script: https://github.com/google-research/noisystudent/blob/master/local_scripts/imagenet/train.sh ,"Academia,Industry",$43900.61,,FP32,461681.1110628138,Hardware,,,,6
Sandwich Transformer,Language,Language modeling,"Allen Institute for AI,Facebook AI Research","Ofir Press, Noah A. Smith, Omer Levy",2019-11-10,Improving Transformer Models by Reordering their Sublayers,https://arxiv.org/abs/1911.03864,82.0,SOTA improvement,"""Sandwich transformers achieve state-of-the-art results on the enwik8 character-level language modeling dataset and on an additional word-level corpus,
but have no significant effect on machine translation""",209000000.0,"209M
""All of our experiments use the same hyperparameters as Baevski and Auliâ€™s original model.""",2.3504093e+19,"6 FLOP / token / parameter * 209000000 parameters * 286000 steps * 65536 tokens per batch [same as Baevski and Auli (2019) = 2.3504093e+19 FLOP

__________

in the Algorithmic progress paper they assumed 180 epochs (same as Baevski and Auli 2019 Transformer, but that one was trained on WT 103 not the book corpus) -> training compute was estimated to be 1.58E+20 FLOP","""BookCorpus (BooksCorpus, Toronto Book Corpus)""",,700000000.0,"""while retaining the other architectural aspects and hyperparameter settings from Baevski and Auli (2019)""

Baevski and Auli (2019): 286k steps in batches of 65,536 tokens.

286000*65536/700000000 = 27 epochs",,,,,Speculative,"Multilayer transformer networks consist of interleaved self-attention and feedforward sublayers. Could ordering the sublayers in a different pattern lead to better performance? We generate randomly ordered transformers and train them with the language modeling objective. We observe that some of these models are able to achieve better performance than the interleaved baseline, and that those successful variants tend to have more self-attention at the bottom and more feedforward sublayers at the top. We propose a new transformer pattern that adheres to this property, the sandwich transformer, and show that it improves perplexity on multiple word-level and character-level language modeling benchmarks, at no cost in parameters, memory, or training time. However, the sandwich reordering pattern does not guarantee performance gains across every task, as we demonstrate on machine translation models. Instead, we suggest that further exploration of task-specific sublayer reorderings is needed in order to unlock additional gains.",27.0,Sandwich Transformer,Unreleased,"United States of America,United States of America",,,,,,2025-04-10 13:43,,,,65536.0,,"Research collective,Industry",,,,,Open (non-commercial),non-commercial training and inference code: https://github.com/ofirpress/sandwich_transformer,"Research collective,Industry",,,,,Operation counting,,,,
CamemBERT,Language,Language modeling/generation,"Facebook,INRIA,Sorbonne University","Louis Martin, Benjamin Muller, Pedro Javier Ortiz SuÃ¡rez, Yoann Dupont, Laurent Romary, Ã‰ric Villemonte de la Clergerie, DjamÃ© Seddah, BenoÃ®t Sagot",2019-11-10,CamemBERT: a Tasty French Language Model,https://arxiv.org/abs/1911.03894,990.0,SOTA improvement,"""Our best performing model CamemBERT reaches or improves the state of the art in all four downstream tasks."" (part-of-speech tagging, dependency parsing, named entity recognition and natural language inference tasks)",335000000.0,"CamemBERT Large, Table 4",8.3e+20,"""Unless otherwise specified, our models use the BASE architecture, and are pretrained for 100k backpropagation steps on 256 Nvidia V100 GPUs (32GB each) for a day""

256 V100-days

256 * 125 teraflops * 24 * 3600 * 0.3 (assumed utilization)
= 8.3e20


""Following (Liu et al., 2019), we optimize the model using Adam (Kingma and Ba, 2014) (Î²1 = 0.9, Î²2 = 0.98) for 100k steps with large batch sizes of 8192 sequences, each sequence containing at most 512 tokens""

Using compute = 6*N*D, that's 6 * (100k * 8192 * 512) * 335M= 8.43e20",CCNet,"""we train another model with the LARGE architecture, referred to as CamemBERTLARGE, for a fair comparison with XLM-RLARGE. This model is trained with the CCNet corpus, described in Sec. 6, for 100k steps""

Other models in paper are trained with the French portion of OSCAR. See footnote 12.",31900000000.0," 31.9B tokens, Table 6.",24.0,1 day for each model (may not have been a full 24 hours),NVIDIA V100,,Confident,"Pretrained language models are now ubiquitous in Natural Language Processing. Despite their success, most available models have either been trained on English data or on the concatenation of data in multiple languages. This makes practical use of such models --in all languages except English-- very limited. In this paper, we investigate the feasibility of training monolingual Transformer-based language models for other languages, taking French as an example and evaluating our language models on part-of-speech tagging, dependency parsing, named entity recognition and natural language inference tasks. We show that the use of web crawled data is preferable to the use of Wikipedia data. More surprisingly, we show that a relatively small web crawled dataset (4GB) leads to results that are as good as those obtained using larger datasets (130+GB). Our best performing model CamemBERT reaches or improves the state of the art in all four downstream tasks.",13.0,,Open weights (unrestricted),"United States of America,France,France",,,,,,2025-02-17 12:27,,,,,,"Industry,Academia,Academia",,,,,Unreleased,"MIT: 
https://camembert-model.fr/","Industry,Academia,Academia",$2319.54,,,,"Hardware,Operation counting",,,,
Self-Attention and Convolutional Layers,Vision,Image classification,Ecole Polytechnique FÂ´edÂ´erale de Lausanne (EPFL),"Jean-Baptiste Cordonnier, Andreas Loukas & Martin Jaggi",2019-11-08,On the Relationship between Self-Attention and Convolutional Layers,https://arxiv.org/abs/1911.03584,550.0,,,29500000.0,from Table 1,6.75e+17,"(15e9) * (300) * (50000) * 3 = 6.75e+17
(inference compute) * (epochs) * (dataset size) * (constant to account for backpropagation)
 
epochs from appendix B table 2
inference compute from table 1",CIFAR-10,"Figure 2 in the paper
and line 29 from https://github.com/epfml/attention-cnn/blob/master/train.py (access date 30.01.2024)",50000.0,size of CIFAR-10,,,,,Confident,"Recent trends of incorporating attention mechanisms in vision have led researchers to reconsider the supremacy of convolutional layers as a primary building block. Beyond helping CNNs to handle long-range dependencies, Ramachandran et al. (2019) showed that attention can completely replace convolution and achieve state-of-the-art performance on vision tasks. This raises the question: do learned attention layers operate similarly to convolutional layers? This work provides evidence that attention layers can perform convolution and, indeed, they often learn to do so in practice. Specifically, we prove that a multi-head self-attention layer with sufficient number of heads is at least as expressive as any convolutional layer. Our numerical experiments then show that self-attention layers attend to pixel-grid patterns similarly to CNN layers, corroborating our analysis. Our code is publicly available. ",300.0,,Unreleased,Switzerland,,,,,,2025-02-17 12:27,,,,,,Academia,,,,,Open source,"Apache 2.0 for train/eval code: https://github.com/epfml/attention-cnn
data is CIFAR which doesn't have a clear license",Academia,,,,,Operation counting,,,,
XLM-RoBERTa,Language,"Named entity recognition,Question answering,Text classification",Facebook AI,"Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco GuzmÃ¡n, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov
",2019-11-05,Unsupervised Cross-lingual Representation Learning at Scale,https://arxiv.org/abs/1911.02116,5668.0,"Highly cited,SOTA improvement","citation ""which obtains state-of-the-art perfor-
mance on cross-lingual classification, sequence la-
beling and question answering""",550000000.0,"The number of parameters in the model is specified as ""550M params"" for XLM-R.",2.076e+22,"""We use the multilingual MLM loss and train our XLM-R model for
1.5 Million updates on five-hundred 32GB Nvidia
V100 GPUs with a batch size of 8192. ""

6ND:
Sequence length was probably 512, based on follow up paper (XLM-R XXL)
6 * 550e6 * 1.5e6 * 8192 * 512 = 2.076e22
",CC100,"The training dataset and size are mentioned as ""using more than two terabytes of filtered CommonCrawl data"" and the model being trained on ""100 languages"".",125250000000.0,size of CC100 - copied from other rows,,,NVIDIA Tesla V100 DGXS 32 GB,,Confident,"This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7% in XNLI accuracy for Swahili and 11.4% for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code, data and models publicly available.",,,Open weights (non-commercial),United States of America,,,,500.0,,2025-05-02 10:51,,,,,,Industry,,,,,Open (non-commercial),"non-commercial: https://github.com/facebookresearch/XLM?tab=License-1-ov-file#readme

data is wikipedia",Industry,,,,256204.94677329363,Operation counting,,,,7
Base LM + kNN LM + Continuous Cache,Language,Language modeling,"Stanford University,Facebook AI Research","Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, Mike Lewis",2019-11-01,Generalization through Memorization: Nearest Neighbor Language Models,https://arxiv.org/abs/1911.00172,721.0,SOTA improvement,"""GNN-LM achieves a new state-of-the-art perplexity of 14.8 on WikiText-103""",247000000.0,"""we take the exact architecture and optimization described by Baevski & Auli (2019) and use it to create a kNN-LM for inference. This model consists of 16 layers, each with 16 self-attention heads, 1024 dimensional hidden states, and 4096 dimensional feedforward layers, amounting to 247M trainable parameters.""",3.0529200000000004e+19,"6 FLOP / parameter / token * 247*10^6 parameters * 103000000 tokens * 200 epochs = 3.05292e+19 FLOP


__________
for the Algorithmic progress paper 7.3 Ã— 10^18 FLOP was estimated similar to supposedly base model (transformer) ",WikiText-103,,103000000.0,""" During this forward pass, each target token is provided a minimum of 1536 tokens of prior context for WIKITEXT-103""

200 epochs - figure 8",,,,,Likely,"We introduce kNN-LMs, which extend a pre-trained neural language model (LM) by linearly interpolating it with a k-nearest neighbors (kNN) model. The nearest neighbors are computed according to distance in the pre-trained LM embedding space, and can be drawn from any text collection, including the original LM training data. Applying this augmentation to a strong Wikitext-103 LM, with neighbors drawn from the original training set, our kNN-LM achieves a new state-of-the-art perplexity of 15.79 - a 2.9 point improvement with no additional training. We also show that this approach has implications for efficiently scaling up to larger training sets and allows for effective domain adaptation, by simply varying the nearest neighbor datastore, again without further training. Qualitatively, the model is particularly helpful in predicting rare patterns, such as factual knowledge. Together, these results strongly suggest that learning similarity between sequences of text is easier than predicting the next word, and that nearest neighbor search is an effective approach for language modeling in the long tail.",200.0,Base LM + kNN LM + Continuous Cache,Unreleased,"United States of America,United States of America",,,,,,2025-04-10 14:14,,,,,,"Academia,Industry",,,,,Open source,"Training code, MIT: https://github.com/urvashik/knnlm","Academia,Industry",,,,,Operation counting,,,,
AlphaStar,Games,StarCraft,DeepMind,"Oriol Vinyals,Igor Babuschkin,Wojciech M. Czarnecki,MichaÃ«l Mathieu,Andrew Dudzik,Junyoung Chung,David H. Choi,Richard Powell,Timo Ewalds,Petko Georgiev,Junhyuk Oh,Dan Horgan,Manuel Kroiss,Ivo Danihelka,Aja Huang,Laurent Sifre,Trevor Cai,John P. Agapiou,Max Jaderberg,Alexander S. Vezhnevets,RÃ©mi Leblond,Tobias Pohlen,Valentin Dalibard,David Budden,Yury Sulsky,James Molloy,Tom L. Paine,Caglar Gulcehre,Ziyu Wang,Tobias Pfaff,Yuhuai Wu,Roman Ring,Dani Yogatama,Dario WÃ¼nsch,Katrina McKinney,Oliver Smith,Tom Schaul,Timothy Lillicrap,Koray Kavukcuoglu,Demis Hassabis,Chris Apps,David Silver",2019-10-30,Grandmaster level in StarCraft II using multi-agent reinforcement learning,https://www.deepmind.com/blog/alphastar-grandmaster-level-in-starcraft-ii-using-multi-agent-reinforcement-learning,3296.0,Highly cited,,139000000.0,"AlphaStar has 139 million weights, but only 55 million weights are required during inference.",5.9250000000001e+22,"384 TPUv3 chips for 44 days. Assume 33% utilization.
https://www.wolframalpha.com/input?i=123+teraFLOPS+*+384+*+0.33+*+44+days

""Each agent was trained using 32 third-generation tensor processing units (TPUs23) over 44 days.""

12 agents * 34 chips = 384 chicks
",,,,"Multiple data types. First supervised learning, then other stuff",1056.0,"""Each agent was trained using 32 third-generation tensor
processing units (TPUs) over 44 days""",Google TPU v3,,Confident,"Many real-world applications require artificial agents to compete and coordinate with other agents in complex environments. As a stepping stone to this goal, the domain of StarCraft has emerged as an important challenge for artificial intelligence research, owing to its iconic and enduring status among the most difficult professional esports and its relevance to the real world in terms of its raw complexity and multi-agent challenges. Over the course of a decade and numerous competitions1â€“3, the strongest agents have simplified important aspects of the game, utilized superhuman capabilities, or employed hand-crafted sub-systems4. Despite these advantages, no previous agent has come close to matching the overall skill of top StarCraft players. We chose to address the challenge of StarCraft using generalpurpose learning methods that are in principle applicable to other complex domains: a multi-agent reinforcement learning algorithm that uses data from both human and agent games within a diverse league of continually adapting strategies and counter-strategies, each represented by deep neural networks5,6. We evaluated our agent, AlphaStar, in the full game of StarCraft II, through a series of online games against human players. AlphaStar was rated at Grandmaster level for all three StarCraft races and above 99.8% of officially ranked human players.",,,Unreleased,United Kingdom of Great Britain and Northern Ireland,,,,384.0,,2025-06-18 10:12,,,,,,Industry,,,,405504.0,Open source,"Apache 2.0, training tools: https://github.com/google-deepmind/alphastar

training instructions here: https://github.com/google-deepmind/alphastar/blob/main/alphastar/unplugged/README.md ",Industry,$125758.10,,,173176.688897332,Hardware,,,,3
T5-3B,Language,Text autocompletion,Google,"Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu",2019-10-23,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,https://arxiv.org/abs/1910.10683,16685.0,Highly cited,,2800000000.0,"page 37, 3B and 11B. ""To further explore what kind of performance is possible when using larger models, we consider two additional variants. In both cases, we use d_model = 1024, a 24 layer encoder and decoder, and dkv = 128. For the â€œ3Bâ€ variant, we use dff = 16,384 with 32-headed attention, which results in around 2.8 billion parameters; for â€œ11Bâ€ we use dff = 65,536 with 128-headed attention producing a model with about 11 billion parameters""",9.0000000001e+21,"Akronomicon states 1.04e+22 FLOP. Archived source: https://github.com/lightonai/akronomicon/tree/main/akrodb
However, this seems dubiously high.

""We pre-train each model for 2^19 = 524,288 steps on C4 before fine-tuning.""
""In total, this batch size and number of steps corresponds to pre-training on 2^35 â‰ˆ 34B tokens.""
""To compare these mixing strategies on equal footing with our baseline pre-train-then-fine-tune results, we train multi-task models for the same total number of steps: 2^19 + 2^18 = 786,432""
Using the 6DN approximation gives: 6 FLOP/token/param * 2^35 pretrain tokens * (1+1/2 finetune tokens per pretrain token) * 1 iteration of training data* 2.8 billion parameters = 8.659e20 FLOP
https://www.wolframalpha.com/input?i=6+*+2%5E35+*+2.8+billion+*+1.5

update: 9.0E+21 per FLAN paper from Google 
https://arxiv.org/pdf/2210.11416.pdf",C4,,25500000000.0,"""This produces a collection of text that is not only orders of magnitude larger than most data sets used for pre-training (about 750 GB) but also
comprises reasonably clean and natural English text. We dub this data set the â€œColossal Clean Crawled Corpusâ€ (or C4 for short) and release it as part of TensorFlow Datasets""
750GB * 200M word/GB = 1.5e11

""In total, this batch size and number of steps corresponds to pre-training on 2^35 â‰ˆ 34B tokens.""
""Note that 2^35 tokens only covers a fraction of the entire C4 data set, so we never repeat any data during pre-training.""
The fraction is 25.5 billion / 150 billion = 0.17 epochs.",,,Google TPU v3,Self-supervised learning,Confident,"Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.",0.17,,Open weights (unrestricted),United States of America,,,,,,2025-05-09 11:32,,,,,,Industry,,,,,Open source,"Apache for code and weights:
https://github.com/google-research/text-to-text-transfer-transformer

Data is C4 which is open
training script: https://github.com/google-research/text-to-text-transfer-transformer?tab=readme-ov-file#training ",Industry,$1613.19,,,,"Third-party estimation,Reported",,,,7
T5-11B,Language,"Text autocompletion,Language modeling/generation",Google,"Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu",2019-10-23,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,https://arxiv.org/abs/1910.10683,16685.0,Highly cited,,11000000000.0,The full 11-billion parameter model,3.3e+22,"https://arxiv.org/ftp/arxiv/papers/2104/2104.10350.pdf
Table 4, 4.05e22

update: 3.3e22 per FLAN paper from Google 
https://arxiv.org/pdf/2210.11416.pdf

6ND rule suggests somewhat more FLOPs:
6 * 1T * 11B = 6.6e22",C4,,200000000000.0,"""This produces a collection of text that is not only
orders of magnitude larger than most data sets used for pre-training (about 750 GB) but also comprises reasonably clean and natural English text. We dub this data set the â€œColossal Clean Crawled Corpusâ€ (or C4 for short) and release it as part of TensorFlow Datasets""

750GB * 200M word/GB * 4/3 tokens per word = 2e11.

Total tokens seen is about 1T:  ""We therefore pre-train our models for 1 million steps on a batch size of 2^11 sequences of length 512, corresponding to a total of about 1 trillion pre-training tokens""",481.9,"4.05*10^22 FLOP at 37.073% utilization on 512 TPU v3 chips (123 TFLOPS) -> 482 hours
https://www.wolframalpha.com/input?i=4.05*10%5E22+seconds+%2F+%28512*123*10%5E12%29+*%28123%2F45.6%29",Google TPU v3,Self-supervised learning,Confident,"Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.",,,Open weights (unrestricted),United States of America,,,,512.0,0.3707,2025-05-09 11:32,,,,65536.0,"""We use a maximum sequence length of 512 and a batch size of 128 sequences. Whenever possible, we â€œpackâ€ multiple sequences into each entry of the batch10 so that our batches contain roughly 2^16 = 65,536 tokens""",Industry,,,,246733.0,Open source,"Apache for code and weights:
https://github.com/google-research/text-to-text-transfer-transformer

Data is C4 which is open",Industry,$75524.39,Table 4 in https://arxiv.org/pdf/2104.10350 gives measured performance of  45.6 TFLOP/s out of maximum 123 TFLOP/s = 0.3707,,230938.24900457988,"Reported,Operation counting,Third-party estimation",,,,4
LSTM(large)+Sememe+cell,Language,Language modeling,"Tsinghua University,Beijing University of Posts and Telecommunications,Huawei Noah's Ark Lab","Yujia Qin, Fanchao Qi, Sicong Ouyang, Zhiyuan Liu, Cheng Yang, Yasheng Wang, Qun Liu, Maosong Sun",2019-10-20,Improving Sequence Modeling Ability of Recurrent Neural Networks via Sememes,https://arxiv.org/abs/1910.08910,19.0,,,48000000.0,"Based on the details in the paper, here is the calculation for the number of parameters in the LSTM(large)+Sememe+cell model:

The model has 2 LSTM layers, each with 1500 hidden units.

It uses the ""+cell"" method to incorporate sememes, which introduces an additional sememe-LSTM cell in each LSTM unit.

For each original 1500 unit LSTM layer:

Input size is 300 (embedding dimension)
Hidden size is 1500
4 weight matrices of size 300 x 1500 = 450,000 params each
4 recurrent weight matrices of size 1500 x 1500 = 2,250,000 params each
4 bias vectors of size 1500 = 6,000 params each
Total = 450,000 x 4 + 2,250,000 x 4 + 6,000 x 4 = 12,006,000
For each sememe-LSTM cell:

Input size is 2000 (number of sememes)
Hidden size is 1500
Same weight and bias computations as above
Total = 12,006,000
Since there are 2 LSTM layers each with a sememe-LSTM cell:

Total params = 2 * (12,006,000 + 12,006,000)
Total = 48,024,000
Therefore, the total number of parameters for the LSTM(large)+Sememe+cell model is approximately 48 million.",2.304e+16,6 FLOP / token / parameter * 48000000 parameters * 2000000 tokens * 40 epochs = 2.304e+16 FLOP,WikiText-2,,2000000.0,"""The maximum training epoch number is 40 and the gradient norm clip boundary is 0.25.""",,,,,Likely,"Sememes, the minimum semantic units of human languages, have been successfully utilized in various natural language processing applications. However, most existing studies exploit sememes in specific tasks and few efforts are made to utilize sememes more fundamentally. In this paper, we propose to incorporate sememes into recurrent neural networks (RNNs) to improve their sequence modeling ability, which is beneficial to all kinds of downstream tasks. We design three different sememe incorporation methods and employ them in typical RNNs including LSTM, GRU and their bidirectional variants. In evaluation, we use several benchmark datasets involving PTB and WikiText-2 for language modeling, SNLI for natural language inference and another two datasets for sentiment analysis and paraphrase detection. Experimental results show evident and consistent improvement of our sememe-incorporated models compared with vanilla RNNs, which proves the effectiveness of our sememe incorporation methods. Moreover, we find the sememe-incorporated models have higher robustness and outperform adversarial training in defending adversarial attack. All the code and data of this work can be obtained at this https URL.",40.0,LSTM(large)+Sememe+cell,Unreleased,"China,China,China",,,,,,2025-04-10 15:51,,,,,,"Academia,Academia,Industry",,,,,Open (non-commercial),code for WT/PTB. no license. https://github.com/thunlp/SememeRNN/blob/master/LM/main.py ,"Academia,Academia,Industry",,,,,Operation counting,,,,
M4-50B,Language,Translation,Google,"Ankur Bapna, Orhan Firat",2019-10-11,"Exploring Massively Multilingual, Massive Neural Machine Translation",https://blog.research.google/2019/10/exploring-massively-multilingual.html,,SOTA improvement,,50000000000.0,"(sparse architecture)

""By modifying the Transformer architecture through the substitution of the vanilla feed-forward layers with sparsely-gated mixture of experts, we drastically scale up the model capacity, allowing us to successfully train and pass 50 billion parameters, which further improved translation quality across the board.""",,"Sparse architecture, so training compute is uncertain",,"""we push the limits of research on multilingual NMT by training a single NMT model on 25+ billion sentence pairs, from 100+ languages to and from English, with 50+ billion parameters.""",,25+ billion sentence pairs,,,,,Confident,"Over the last few years there has been enormous progress in the quality of machine translation (MT) systems, breaking language barriers around the world thanks to the developments in neural machine translation (NMT). The success of NMT however, owes largely to the great amounts of supervised training data. But what about languages where data is scarce, or even absent? Multilingual NMT, with the inductive bias that â€œthe learning signal from one language should benefit the quality of translation to other languagesâ€, is a potential remedy.",,,Unreleased,United States of America,,,,,,2024-09-05 14:08,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
AlphaX-1,Vision,"Neural architecture search for computer vision,Image classification,Object detection,Image captioning","Facebook AI Research,Brown University","Linnan Wang, Yiyang Zhao, Yuu Jinnai, Yuandong Tian, Rodrigo Fonseca1",2019-10-02,AlphaX: eXploring Neural Architectures with Deep Neural Networks and Monte Carlo Tree Search,https://arxiv.org/abs/1903.11059,92.0,SOTA improvement,"""In 12 GPU days and 1000 samples, AlphaX found an architecture that reaches 97.84\% top-1 accuracy on CIFAR-10, and 75.5\% top-1 accuracy on ImageNet, exceeding SOTA NAS methods in both the accuracy and sampling efficiency""",5400000.0,"Table 3: multiadds for AlphaX-1 579M, parameters 5.4M",8.89344e+17,""" Our models for ImageNet use polynomial learning rate
schedule, starting with 0.05 and decay through 200 epochs.""

1280000 images * 200 epochs *3 forward-backward adjustment * 1158000000 forward FLOP =889344000000000000","ImageNet,COCO","12800000 + 200000=1480000
I assume they used 1,281,167 training images when referred to Imagenet and 200 000 when referred to MS COCO

""We set up the ImageNet training using
the standard mobile configuration with the input image size
of (224 Ã— 224)[45]. More details are available in the appendix. AlphaX sampled 1000 networks, and we selected
the top 20 networks in the pre-training to fine-tune another
530 epochs. 

We use AlphaX-1 model pre-trained on ImageNet
dataset. The training dataset is MSCOCO for object
detection[15] which contains 90 classes of objects. Each
image is scaled to 300 Ã— 300 in RGB channels. We trained
the model with 200k iterations with 0.04 initial learning rate
and the batch size is set to 24. We applied the exponential learning rate decay schedule with the 0.95 decay factor. Our
model uses momentum optimizer with momentum rate set
to 0.9. We also use the L2 weight decay for training. We
process each image with random horizontal flip and random
crop[22]. We set the matched threshold to 0.5, which means
only the probability of an object over 0.5 is effective to appear on the image. We use 8000 subsets of validation images in MSCOCO validation set and report the mean average precision (mAP) as computed with the standard COCO metric library[16].""
",1280000.0,"Standard image net training size, not otherwise specified",,,NVIDIA GeForce GTX 1080 Ti,,Confident,"Neural Architecture Search (NAS) has shown great success in automating the design of neural networks, but the prohibitive amount of computations behind current NAS methods requires further investigations in improving the sample efficiency and the network evaluation cost to get better results in a shorter time. In this paper, we present a novel scalable Monte Carlo Tree Search (MCTS) based NAS agent, named AlphaX, to tackle these two aspects. AlphaX improves the search efficiency by adaptively balancing the exploration and exploitation at the state level, and by a Meta-Deep Neural Network (DNN) to predict network accuracies for biasing the search toward a promising region. To amortize the network evaluation cost, AlphaX accelerates MCTS rollouts with a distributed design and reduces the number of epochs in evaluating a network by transfer learning guided with the tree structure in MCTS. In 12 GPU days and 1000 samples, AlphaX found an architecture that reaches 97.84\% top-1 accuracy on CIFAR-10, and 75.5\% top-1 accuracy on ImageNet, exceeding SOTA NAS methods in both the accuracy and sampling efficiency. Particularly, we also evaluate AlphaX on NASBench-101, a large scale NAS dataset; AlphaX is 3x and 2.8x more sample efficient than Random Search and Regularized Evolution in finding the global optimum. Finally, we show the searched architecture improves a variety of vision applications from Neural Style Transfer, to Image Captioning and Object Detection.",,,Unreleased,"United States of America,United States of America",,,,,,2025-06-18 10:05,,,,,,"Industry,Academia",,,,,Open (non-commercial),"code, no license specified: https://github.com/linnanwang/AlphaX-NASBench101
training: https://github.com/linnanwang/AlphaX-NASBench101/blob/master/net_training.py ","Industry,Academia",,,,,,,,,
DistilBERT,Language,Text autocompletion,Hugging Face,"Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf",2019-10-02,"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",https://arxiv.org/abs/1910.01108,6345.0,Highly cited,,66000000.0,Table 3,1.24416e+19,"Section 3: DistilBERT was trained on 8 16GB V100 GPUs for approximately 90 hours.

1.6e13*8*60**2*90*0.3 = 1.2e19","Wikipedia,""BookCorpus (BooksCorpus, Toronto Book Corpus)""","Section 3: We train DistilBERT on the same corpus as the original BERT model: a concatenation of English Wikipedia and Toronto Book Corpus [Zhu et al., 2015].",,,,,NVIDIA Tesla V100 DGXS 16 GB,,Confident,,,,Open weights (unrestricted),"Multinational,United States of America",,,,,,2025-06-18 09:53,,,,,,Industry,,,,,Open source,"code, including train: https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation

weights: https://huggingface.co/distilbert/distilbert-base-uncased

repo license is apache: https://github.com/huggingface/transformers/blob/main/LICENSE

Wikipedia is open, BookCorpus is not",Industry,,,,,Hardware,,,,
Alleviated TOI 10 (WT103),Language,Language modeling,"Ecole Polytechnique FÂ´edÂ´erale de Lausanne (EPFL),Swisscom,University of Freiburg","NoÃ©mien Kocher, Christian Scuito, Lorenzo Tarantino, Alexandros Lazaridis, Andreas Fischer, Claudiu Musat",2019-09-18,Alleviating Sequence Information Loss with Data Overlapping and Prime Batch Sizes,https://arxiv.org/abs/1909.08700,0.0,,,,,,,WikiText-103,,,,,,,,Unknown,"In sequence modeling tasks the token order matters, but this information can be partially lost due to the discretization of the sequence into data points. In this paper, we study the imbalance between the way certain token pairs are included in data points and others are not. We denote this a token order imbalance (TOI) and we link the partial sequence information loss to a diminished performance of the system as a whole, both in text and speech processing tasks. We then provide a mechanism to leverage the full token order information -Alleviated TOI- by iteratively overlapping the token composition of data points. For recurrent networks, we use prime numbers for the batch size to avoid redundancies when building batches from overlapped data points. The proposed method achieved state of the art performance in both text and speech related tasks.",1000.0,Alleviated TOI 10 (WT103),Unreleased,"Switzerland,Switzerland,Germany",,,,,,2024-09-05 14:08,,,,,,"Academia,Industry,Academia",,,,,Open source,BSD 3-Clause License for code: https://github.com/nkcr/overlap-ml ,"Academia,Industry,Academia",,,,,,,,,
Megatron-BERT,Language,Language modeling/generation,NVIDIA,"Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, Bryan Catanzaro",2019-09-17,Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism,https://arxiv.org/abs/1909.08053,1501.0,"Highly cited,SOTA improvement","""Our BERT model achieves SOTA results on the RACE dataset""",3900000000.0,"2.1Source: https://lair.lighton.ai/akronomicon/

Archive on GitHub: https://github.com/lightonai/akronomicon/tree/main/akrodb",2.2e+22,"A third-party source: https://lair.lighton.ai/akronomicon/ claims 5.7e22

The authors report experimenting on 1 V100 GPU and achieving throughput of 39 TFLOPS which is 30% of the peak throughput. Therefore the GPU has a peak throughput of 130 TFLOPS so it is specifically the NVIDIA V100S PCIe.
https://images.nvidia.com/content/technologies/volta/pdf/volta-v100-datasheet-update-us-1165301-r5.pdf

Param-based calculation:
6ND = 6*3.9e9*(2e6+1e4)*1024*512 = 2.5e22 FLOP

1024 is the batch size, 512 is the sequence length (not explicitly stated but they say non-specified hyperparameters follow cited papers).

Time-based calculation:
The 8.3B GPT-like arch took 2.1 days per epoch on 512 GPUs, batch size 512. An epoch was 68.5k iterations with sequence length 1024.

Halving the model size should ~halve the iteration time.
Doubling the batch size should ~double the iteration time.
Halving the sequence length should ~quarter the iteration time (quadratic scaling).

Hence 3.1e-5 days/iteration * 2 * 1/2 * 1/4 = 7.8e-6 days/iteration.

2e6 iterations => seems like 15.6 days training.

On 512 GPUs they achieve a peak throughput of 15.1 PFLOPS.
C=15.1 PFLOPS * 58 days = 2.0e22 FLOP.

If we disregard the Akronomicon estimate and aggregate our two, geometric mean is 2.2e22 FLOP",,,46400000000.0,"""The resulting aggregate corpus contains 174 GB of deduplicated text.""
174e9 bytes * (1 word / 5 bytes) * (4 tokens / 3 words) = 4.64e10 tokens",374.0,"The 8.3B GPT-like arch took 2.1 days per epoch on 512 GPUs, batch size 512, sequence length 1024. An epoch was 68.5k iterations.

BERT: batch size 1024, sequence length 512, 2e6 iterations total.

Halving the model size should ~halve the iteration time.
Doubling the batch size should ~double the iteration time.
Halving the sequence length should ~quarter the iteration time (quadratic scaling).

Hence 3.1e-5 days/iteration * 2 * 1/2 * 1/4 = 7.8e-6 days/iteration.

2e6 iterations => seems like 15.6 days training.",NVIDIA Tesla V100S PCIe 32 GB,Self-supervised learning,Confident,"Recent work in language modeling demonstrates that training large transformer models advances the state of the art in Natural Language Processing applications. However, very large models can be quite difficult to train due to memory constraints. In this work, we present our techniques for training very large transformer models and implement a simple, efficient intra-layer model parallel approach that enables training transformer models with billions of parameters. Our approach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model parallelism, and can be fully implemented with the insertion of a few communication operations in native PyTorch. We illustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 GPUs. We sustain 15.1 PetaFLOPs across the entire application with 76% scaling efficiency when compared to a strong single GPU baseline that sustains 39 TeraFLOPs, which is 30% of peak FLOPs. To demonstrate that large language models can further advance the state of the art (SOTA), we train an 8.3 billion parameter transformer language model similar to GPT-2 and a 3.9 billion parameter model similar to BERT. We show that careful attention to the placement of layer normalization in BERT-like models is critical to achieving increased performance as the model size grows. Using the GPT-2 model we achieve SOTA results on the WikiText103 (10.8 compared to SOTA perplexity of 15.8) and LAMBADA (66.5% compared to SOTA accuracy of 63.2%) datasets. Our BERT model achieves SOTA results on the RACE dataset (90.9% compared to SOTA accuracy of 89.4%).",,,Unreleased,United States of America,,,,512.0,0.2269,2025-05-28 16:14,,,,524288.0,"""we set the batch size to 1024 and use a learning rate of 1.0e4 warmed up over 10,000 iterations and decayed linearly
over 2 million iterations. Other training parameters are kept
the same as (Devlin et al., 2018).""

in Devlin et al (BERT), sequences are 512 tokens",Industry,,,,191488.0,Open source,"training code 

https://github.com/NVIDIA/Megatron-LM/blob/main/pretrain_bert.py 

MIT-like license:
https://github.com/NVIDIA/Megatron-LM/blob/main/LICENSE ",Industry,,"When training the 512 V100 GPT-2 style models, achieved 15.1 PetaFLOP/s vs theoretical maximum of 512 * 130e12 = 66.56 PetaFLOP/s
BERT model used same number of GPUs, likely had similar utilization.
15.1 / 66.56 = 0.2269",FP16,262640.30207328824,"Operation counting,Third-party estimation",,,,4
Megatron-LM (8.3B),Language,Language modeling/generation,NVIDIA,"Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, Bryan Catanzaro",2019-09-17,Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism,https://arxiv.org/abs/1909.08053,1501.0,"Highly cited,SOTA improvement","""Using the GPT-2 model we achieve SOTA results on the WikiText103 (10.8 compared to SOTA perplexity of 15.8) and LAMBADA"" 

GPT-2 model here meaning model similar to GPT-2",8300000000.0,"Source: https://lair.lighton.ai/akronomicon/

Archived source: https://web.archive.org/web/20211220142906/https://lair.lighton.ai/akronomicon/

Data also available on GitHub: https://github.com/lightonai/akronomicon/blob/main/akrodb/NVIDIA/Megatron-LM.json",9.1e+21,"source: https://lair.lighton.ai/akronomicon/

archived: https://github.com/lightonai/akronomicon/tree/main/akrodb

other estimates:

8.3B is a GPT-2-based model (Table 2). ""For GPT-2 models, all training is performed with sequences of 1024 subword units at a batch size of 512 for 300k iterations"" 

I interpret the above as 1024*512*300k = 157B training tokens 

6 * 157 billion * 8.3 billion  = 7.8e21

Also, their training setup achieved 15.1 petaFLOPS or 1.5e16 FLOPS.
(512 V100s is 512 * 125 teraflops = 64 petaFLOPS so they had ~25% utilization)
2.1 days per epoch, ~4.4 epochs
2.1 * 4.4 * 24 * 3600 * 1.5e16 = 1.197e22

These are both close to the akronomicon estimate

Authors of ""AI and Memory Wall"" (https://github.com/amirgholami/ai_and_memory_wall) estimated model's training compute as 8,100,000 PFLOP = 8.1*10^21 FLOP",,"""we aggregate several of the largest language
modeling datasets. We create an aggregate dataset consisting of Wikipedia (Devlin et al., 2018), CC-Stories (Trinh &
Le, 2018), RealNews (Zellers et al., 2019), and OpenWebtext (Radford et al., 2019). To avoid training set leakage
into our downstream tasks we remove the Wikipedia articles
present in the WikiText103 test set (Merity et al., 2016).""",34800000000.0,"""The resulting aggregate
corpus contains 174 GB of deduplicated text.""",327.0,"Reported throughput is 15.1 teraFLOPS per GPU on 512 GPUs
Assume total compute is 9.1e21 FLOP.
Then training time is 327 hours.
https://www.wolframalpha.com/input?i=9.1*10%5E21+FLOP+%2F+%28512*15.1+teraFLOPS%29",NVIDIA Tesla V100 DGXS 32 GB,Self-supervised learning,Likely,"Recent work in language modeling demonstrates that training large transformer models advances the state of the art in Natural Language Processing applications. However, very large models can be quite difficult to train due to memory constraints. In this work, we present our techniques for training very large transformer models and implement a simple, efficient intra-layer model parallel approach that enables training transformer models with billions of parameters. Our approach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model parallelism, and can be fully implemented with the insertion of a few communication operations in native PyTorch. We illustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 GPUs. We sustain 15.1 PetaFLOPs across the entire application with 76% scaling efficiency when compared to a strong single GPU baseline that sustains 39 TeraFLOPs, which is 30% of peak FLOPs. To demonstrate that large language models can further advance the state of the art (SOTA), we train an 8.3 billion parameter transformer language model similar to GPT-2 and a 3.9 billion parameter model similar to BERT. We show that careful attention to the placement of layer normalization in BERT-like models is critical to achieving increased performance as the model size grows. Using the GPT-2 model we achieve SOTA results on the WikiText103 (10.8 compared to SOTA perplexity of 15.8) and LAMBADA (66.5% compared to SOTA accuracy of 63.2%) datasets. Our BERT model achieves SOTA results on the RACE dataset (90.9% compared to SOTA accuracy of 89.4%).",4.4,Megatron-LM (8.3B),Unreleased,United States of America,,,,512.0,0.2269,2025-05-28 16:14,,,,,,Industry,,,,167424.0,Open source,"code (2.5B model is a GPT model): https://github.com/NVIDIA/Megatron-LM?tab=readme-ov-file#megatron-overview  
open license: https://github.com/NVIDIA/Megatron-LM?tab=License-1-ov-file#readme ",Industry,$106142.29,"Achieved 15.1 PetaFLOP/s on 512 V100s, vs theoretical maximum of 512 * 130e12 = 66.56 PetaFLOP/s
15.1 / 66.56 = 0.2269",FP16,262640.30207328824,"Hardware,Operation counting,Third-party estimation",,,,5
Xiaoice,"Language,Vision,Multimodal","Chat,Image captioning,Visual question answering,Language modeling/generation,Translation",Microsoft Research Asia,"Li Zhou, Jianfeng Gao, Di Li, Heung-Yeung Shum",2019-09-14,"The Design and Implementation of XiaoIce, an Empathetic Social Chatbot",https://arxiv.org/abs/1812.08989,,,,,,,,,,,,,,,,Unknown,"This paper describes the development of Microsoft XiaoIce, the most popular social chatbot in the world. XiaoIce is uniquely designed as an AI companion with an emotional connection to satisfy the human need for communication, affection, and social belonging. We take into account both intelligent quotient (IQ) and emotional quotient (EQ) in system design, cast human-machine social chat as decision-making over Markov Decision Processes (MDPs), and optimize XiaoIce for long-term user engagement, measured in expected Conversation-turns Per Session (CPS). We detail the system architecture and key components including dialogue manager, core chat, skills, and an empathetic computing module. We show how XiaoIce dynamically recognizes human feelings and states, understands user intent, and responds to user needs throughout long conversations. Since her launch in 2014, XiaoIce has communicated with over 660 million active users and succeeded in establishing long-term relationships with many of them. Analysis of large scale online logs shows that XiaoIce has achieved an average CPS of 23, which is significantly higher than that of other chatbots and even human conversations.",,,Hosted access (no API),China,,,,,,2024-09-05 14:08,,,,,,Industry,,,,,,,Industry,,,,,,,,,
UDSMProt,Biology,"Proteins,Protein or nucleotide language model (pLM/nLM)",Fraunhofer Heinrich Hertz Institute,"Nils Strodthoff, Patrick Wagner, Markus Wenzel, and Wojciech Samek",2019-09-04,UDSMProt: Universal Deep Sequence Models for Protein Classification,https://www.biorxiv.org/content/10.1101/704874v2.full.pdf,,SOTA improvement,"""The proposed method performs on par with state-of-the-art algorithms that were tailored to these specific tasks or, for two out of three tasks, even outperforms them.""",28303800.0,"Python code:  
# Given LSTM parameters
emb_sz = 400  # embedding size, typically equal to the input size for the first layer
nh = 1150     # number of hidden units
nl = 3        # number of layers

# The formula for a single LSTM layer parameters is:
# P = 4 * ((input_dim + hidden_dim) * hidden_dim + hidden_dim)

# First layer parameters (input_dim is the embedding size)
first_layer_params = 4 * ((emb_sz + nh) * nh + nh)

# For subsequent layers, input_dim is equal to hidden_dim (nh)
subsequent_layer_params = 4 * ((nh + nh) * nh + nh)

# Total parameters for all layers
total_params = first_layer_params + (nl - 1) * subsequent_layer_params

print(total_params)",6.37e+17,"Pretraining:
Table 7 gives max of 499k sequences each at (seemingly) L=1024:
499k * 1024 * 28.3M * 6 = 8.7e16

Finetuning:
Largest downstream task has 104940 sequences (Table 5), each sequence has L=1024 residues, 28.3M parameters, and 30 epochs.
105k * 1024 * 30 * 28.3 * 6 = 5.5e17.","SwissProt,a subset of UniProtKB",,,560K proteins,,,,Self-supervised learning,Likely,"Motivation: Inferring the properties of a protein from its amino acid sequence is one of the key problems in bioinformatics. Most state-of-the-art approaches for protein classification tasks are tailored to single classi- fication tasks and rely on handcrafted features such as position-specific-scoring matrices from expensive database searches. We argue that this level of performance can be reached or even be surpassed by learning a task-agnostic representation once, using self-supervised language modeling, and transferring it to specific tasks by a simple finetuning step.
Results: We put forward a universal deep sequence model that is pretrained on unlabeled protein se- quences from Swiss-Prot and finetuned on protein classification tasks. We apply it to three prototypical tasks, namely enzyme class prediction, gene ontology prediction and remote homology and fold detection. The proposed method performs on par with state-of-the-art algorithms that were tailored to these specific tasks or, for two out of three tasks, even outperforms them. These results stress the possibility of inferring protein properties from the sequence alone and, on more general grounds, the prospects of modern natural language processing methods in omics.",30.0,,Open weights (unrestricted),Germany,,,,,,2025-05-09 11:32,,,,,,Research collective,,,,,Open source,"BSD license, models and code
https://github.com/nstrodt/UDSMProt ",Research collective,,,,,Operation counting,,,,
"DEQ-Transformer (Medium, Adaptive Embedding)",Language,Language modeling,"Carnegie Mellon University (CMU),Intel Labs","Shaojie Bai, J. Zico Kolter, Vladlen Koltun",2019-09-03,Deep Equilibrium Models,https://arxiv.org/abs/1909.01377,596.0,,,110000000.0,110M Table 3,8.1576e+17,6 FLOP / parameter / token * 110000000 parameters * 103000000 tokens * 12 epochs = 8.1576e+17 FLOP,WikiText-103,,,sequences of length 150 ,,,,,Confident,"We present a new approach to modeling sequential data: the deep equilibrium model (DEQ). Motivated by an observation that the hidden layers of many existing deep sequence models converge towards some fixed point, we propose the DEQ approach that directly finds these equilibrium points via root-finding. Such a method is equivalent to running an infinite depth (weight-tied) feedforward network, but has the notable advantage that we can analytically backpropagate through the equilibrium point using implicit differentiation. Using this approach, training and prediction in these networks require only constant memory, regardless of the effective ""depth"" of the network. We demonstrate how DEQs can be applied to two state-of-the-art deep sequence models: self-attention transformers and trellis networks. On large-scale language modeling tasks, such as the WikiText-103 benchmark, we show that DEQs 1) often improve performance over these state-of-the-art models (for similar parameter counts); 2) have similar computational requirements to existing models; and 3) vastly reduce memory consumption (often the bottleneck for training large sequence models), demonstrating an up-to 88% memory reduction in our experiments. The code is available at this https URL .",12.0,"""DEQ-Transformer (Medium, Adaptive Embedding)""",Open weights (unrestricted),"United States of America,Multinational,United States of America",,,,,,2025-04-14 10:25,,,,,,"Academia,Industry",,,,,Open source,"code and weights, MIT: https://github.com/locuslab/deq/tree/master/DEQ-Sequence ","Academia,Industry",,,,,Operation counting,,,,
TripletRes,Biology,"Proteins,Protein folding prediction",University of Michigan,"Yang Li, Chengxin Zhang, Eric W. Bell, Dong-Jun Yu, Yang Zhang",2019-08-13,Ensembling multiple raw coevolutionary features with deep residual neural networks for contact-map prediction in CASP13,https://onlinelibrary.wiley.com/doi/10.1002/prot.25798,,,,,,,"4 GPUs, 50 epochs, batch size between 1-4 depending on sequence length",SCOPe 2.07,"""7,671 non-redundant domains, sequence length 30-400, with sequences for which PDB structure resolution is good""",,,,,,,Unknown,"We report the results of residue-residue contact prediction of a new pipeline built purely on the learning of coevolutionary features in the CASP13 experiment. For a query sequence, the pipeline starts with the collection of multiple sequence alignments (MSAs) from multiple genome and metagenome sequence databases using two complementary Hidden Markov Model (HMM)-based searching tools. Three profile matrices, built on covariance, precision, and pseudolikelihood maximization respectively, are then created from the MSAs, which are used as the input features of a deep residual convolutional neural network architecture for contact-map training and prediction. Two ensembling strategies have been proposed to integrate the matrix features through end-to-end training and stacking, resulting in two complementary programs called TripletRes and ResTriplet, respectively. For the 31 free-modeling domains that do not have homologous templates in the PDB, TripletRes and ResTriplet generated comparable results with an average accuracy of 0.640 and 0.646, respectively, for the top L/5 long-range predictions, where 71% and 74% of the cases have an accuracy above 0.5. Detailed data analyses showed that the strength of the pipeline is due to the sensitive MSA construction and the advanced strategies for coevolutionary feature ensembling. Domain splitting was also found to help enhance the contact prediction performance. Nevertheless, contact models for tail regions, which often involve a high number of alignment gaps, and for targets with few homologous sequences are still suboptimal. Development of new approaches where the model is specifically trained on these regions and targets might help address these problems.",50.0,,Unreleased,United States of America,,,,,,2025-04-30 10:04,,,,,,Academia,,,,,,,Academia,,,,,,,,,
RNN + char4-MS-vec,Language,Language modeling,"NTT Communication Science Laboratories,Tohoku University","Sho Takase, Jun Suzuki, Masaaki Nagata",2019-07-17,Character n-Gram Embeddings to Improve RNN Language Models,https://arxiv.org/abs/1906.05506,26.0,,,226000000.0,226M,,"unknown amount of epochs, total training time and exact type of hardware",WikiText-103,,103000000.0,size of WT103,,char4-MS-vec 165.46 seconds per epoch,NVIDIA Tesla P100 PCIe 16GB,,Confident,"This paper proposes a novel Recurrent Neural Network (RNN) language model that takes advantage of character information. We focus on character n-grams based on research in the field of word embedding construction (Wieting et al. 2016). Our proposed method constructs word embeddings from character ngram embeddings and combines them with ordinary word embeddings. We demonstrate that the proposed method achieves the best perplexities on the language modeling datasets: Penn Treebank, WikiText-2, and WikiText-103. Moreover, we conduct experiments on application tasks: machine translation and headline generation. The experimental results indicate that our proposed method also positively affects these tasks",,RNN + char4-MS-vec,Unreleased,"Japan,Japan",,,,,,2024-09-24 17:22,,,,,,"Industry,Academia",,,,,Unreleased,,"Industry,Academia",,,,,,,,,
Graph-based Semi-Supervised Learning (GSSL) Model on MNIST,Vision,Image classification,West Virginia University,"Fariborz Taherkhani, Hadi Kazemi, Nasser M. Nasrabadi",2019-07-17,Matrix Completion for Graph-Based Deep Semi-Supervised Learning,https://ojs.aaai.org/index.php/AAAI/article/view/4438,,,,,"""There are 64 number of filters in the first convolutional layer
and 128 number of filters in the second and third convolutional
layers, respectively. The size filters in this architecture
are 3  3 and the convolution stride is set to 1 pixel. To
preserve spatial resolution after convolution, spatial padding
of the convolutional layer is fixed to 1 pixel for all 3  3
convolutional layers. The max-pooling layers are placed after
each convolutional layers, respectively; the max-pooling
is carried out on a 2  2 pixel window with a stride of 2.""

Amount of parameters could be estimated based on model architecture",,,MNIST,"""MNIST is handwritten digits of 10 different classes dataset which contains a training set of 60,000 samples, and a test set of 10,000 samples. The digits have been size normalized and centered in a fixed-size ( 28  28) images.
We select 100 samples in the training set as labeled and remaining of it as unlabeled.""",60000.0,"batch size: 128
training steps and epochs unknown",,,NVIDIA GeForce GTX TITAN X,,Unknown,"Convolutional Neural Networks (CNNs) have provided promising achievements for image classification problems. However, training a CNN model relies on a large number of labeled data. Considering the vast amount of unlabeled data available on the web, it is important to make use of these data in conjunction with a small set of labeled data to train a deep learning model. In this paper, we introduce a new iterative Graph-based Semi-Supervised Learning (GSSL) method to train a CNN-based classifier using a large amount of unlabeled data and a small amount of labeled data. In this method, we first construct a similarity graph in which the nodes represent the CNN features corresponding to data points (labeled and unlabeled) while the edges tend to connect the data points with the same class label. In this graph, the missing label of unsupervised nodes is predicted by using a matrix completion method based on rank minimization criterion. In the next step, we use the constructed graph to calculate triplet regularization loss which is added to the supervised loss obtained by initially labeled data to update the CNN network parameters.",,,Unreleased,United States of America,,,,2.0,,2025-05-01 10:42,,,,,,Academia,,,,,Unreleased,,Academia,,,,1027.3561728965055,,,,,
R-Transformer,Language,Language modeling,"Michigan State University,TAL Education Group (Xueersi)","Zhiwei Wang, Yao Ma, Zitao Liu, Jiliang Tang",2019-07-12,R-Transformer: Recurrent Neural Network Enhanced Transformer,https://arxiv.org/abs/1907.05572,97.0,,,15800000.0,https://colab.research.google.com/drive/1afDpFvO3Dtry2Mdg8C1aUBV_sMF7LGb7?usp=sharing,8649021100000000.0,6 FLOP / token / parameter * 15800000 parameters * 912344 tokens * 100 epochs = 8.6490211e+15 FLOP,Penn TreeBank (PTB),,912344.0,"default number of epochs in the code:

https://github.com/DSE-MSU/R-transformer/tree/master/language_word ",,,,,Likely,"Recurrent Neural Networks have long been the dominating choice for sequence modeling. However, it severely suffers from two issues: impotent in capturing very long-term dependencies and unable to parallelize the sequential computation procedure. Therefore, many non-recurrent sequence models that are built on convolution and attention operations have been proposed recently. Notably, models with multi-head attention such as Transformer have demonstrated extreme effectiveness in capturing long-term dependencies in a variety of sequence modeling tasks. Despite their success, however, these models lack necessary components to model local structures in sequences and heavily rely on position embeddings that have limited effects and require a considerable amount of design efforts. In this paper, we propose the R-Transformer which enjoys the advantages of both RNNs and the multi-head attention mechanism while avoids their respective drawbacks. The proposed model can effectively capture both local structures and global long-term dependencies in sequences without any use of position embeddings. We evaluate R-Transformer through extensive experiments with data from a wide range of domains and the empirical results show that R-Transformer outperforms the state-of-the-art methods by a large margin in most of the tasks. We have made the code publicly available at \url{this https URL}.",100.0,R-Transformer,Unreleased,"United States of America,China",,,,,,2025-04-14 11:46,,,,,,"Academia,Industry",,,,,Open (non-commercial),"code, no clear license: https://github.com/DSE-MSU/R-transformer/tree/master/language_word ","Academia,Industry",,,,,Operation counting,,,,
Pluribus,Games,Poker,Facebook AI Research,"Noam Brown, Tuomas Sandholm",2019-07-11,Superhuman AI for multiplayer poker,https://www.science.org/cms/asset/910714a7-ee2a-486e-9970-42fb893b08d9/pap.pdf,651.0,SOTA improvement,"first to beat humans at multiplayer poker: ""Developing a superhuman AI for multiplayer poker was the widely,recognized main remaining milestone. In this paper we describe Pluribus, an AI capable of defeating elite human professionals in six-player no-limit Texas holdâ€™em poker, the most commonly played poker format in the world.""",,,6.6e+16,"Trained in 8 days on a 64 core CPU
https://ai.facebook.com/blog/pluribus-first-ai-to-beat-pros-in-6-player-poker/

""We trained the blueprint strategy for Pluribus in eight days on a 64-core server and required less than 512 GB of RAM. No GPUs were used. At typical cloud computing instance rates, it would cost less than $150 to train.""

Guess: trained on i7 Intel CPU, approx 5e9 FLOP/s for each core.

 https://epoch.ai/blog/estimating-training-compute
8 days, 64 cores, 5e9 FLOP/s, 30% utilization",,,,,,,,,Likely,,,,Unreleased,United States of America,,,,,,2025-06-18 09:48,,,,,,Industry,,,,,Unreleased,,Industry,,,,,Hardware,,,,
All-attention network + adaptive span,Language,Language modeling/generation,Facebook AI Research,"Sainbayar Sukhbaatar, Edouard Grave, Guillaume Lample, Herve Jegou, Armand Joulin",2019-07-02,Augmenting Self-attention with Persistent Memory,https://arxiv.org/abs/1907.01470,121.0,,,133000000.0,"133M (Table 3)

For word level language modeling, we use a model with d = 512 and 36 layers, each with 8 heads
and 2048 persistent vectors.",1.1657733e+19,"31330000000000 FLOP / sec / GPU [fp16] * 64 GPUs * 24 hours * 3600 sec/hour * 0.3 [assumed utilization] = 5.197271e+19 FLOP

6 FLOP / token / parameter * 200000 steps * 64 sequences per batch * 256 tokens per sequence * 133000000 parameters = 2.6148864e+18 FLOP

sqrt(5.197271e+19*2.6148864e+18) = 1.1657733e+19 FLOP

likely confidence: i am uncertain about number of steps and v100 precision 

_________
The Algorithmic Progress paper estimation was 4.6 Ã— 10^19 FLOP",WikiText-103,,103000000.0,"8k warmup steps
A batch consists of 64 samples, each with 256 tokens

Following Baevski and Auli [2] on WikiText-103,
we use tied adaptive softmax and adaptive input with 3 clusters of size 20k, 40k and 200k

assuming 200k steps:

200000*64*256/103000000 = 31.8 epochs ",24.0,"""Training large models takes about a day on 64 V100 GPUs.""",NVIDIA V100,,Likely,"Transformer networks have lead to important progress in language modeling and machine translation. These models include two consecutive modules, a feed-forward layer and a self-attention layer. The latter allows the network to capture long term dependencies and are often regarded as the key ingredient in the success of Transformers. Building upon this intuition, we propose a new model that solely consists of attention layers. More precisely, we augment the self-attention layers with persistent memory vectors that play a similar role as the feed-forward layer. Thanks to these vectors, we can remove the feed-forward layer without degrading the performance of a transformer. Our evaluation shows the benefits brought by our model on standard character and word level language modeling benchmarks.",31.8,All-attention network + adaptive span,Unreleased,United States of America,,,,64.0,,2025-04-14 13:00,,,,16384.0,64*256,Industry,,,,,Unreleased,,Industry,,,,39463.657300276864,"Hardware,Operation counting",,,,
RoBERTa Large,Language,"Question answering,Language modeling/generation","Facebook,University of Washington","Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov",2019-07-01,RoBERTa: A Robustly Optimized BERT Pretraining Approach,https://arxiv.org/abs/1907.11692,21512.0,Highly cited,,355000000.0,"355M 
https://github.com/facebookresearch/fairseq/blob/main/examples/roberta/README.md",8.5067e+21,"Section 5: We pretrain our model using 1024 V100 GPUs for approximately one day.

Note this is the base pretraining comparable to BERT, 100k steps. Subsequently they do more: ""increasing the number of pretraining steps
from 100K to 300K, and then further to 500K"".

So assume 5x the 1024 V100 GPUs for 1d estimate. Mixed precision tensor cores get 1.25e14 FLOP/s.

1024 * 1.25e14 * 5 * 24 * 3600 * 0.3 = 1.65888e22

6ND estimate: batches are 8k sequences of 512 tokens; 500k updates means the model saw 500k * 8k * 512 = 2.048T tokens
6 * 2.048T * 355M = 4.36224e21

geometric mean: sqrt(1.65888e22 * 4.36224e21) = 8.5067e21

Authors of ""AI and Memory Wall"" estimated model's training compute as 4,300,000 PFLOP = 4.3*10^21 FLOP
(https://github.com/amirgholami/ai_and_memory_wall)","CC-News,""BookCorpus (BooksCorpus, Toronto Book Corpus)"",WebText2,Wikipedia","""We consider five English-language corpora of
varying sizes and domains, totaling over 160GB
of uncompressed text. We use the following text
corpora:
â€¢ BOOKCORPUS (Zhu et al., 2015) plus English
WIKIPEDIA. This is the original data used to
train BERT. (16GB).
â€¢ CC-NEWS, which we collected from the English portion of the CommonCrawl News
dataset (Nagel, 2016). The data contains 63
million English news articles crawled between
September 2016 and February 2019. (76GB after filtering).4
â€¢ OPENWEBTEXT (Gokaslan and Cohen, 2019),
an open-source recreation of the WebText corpus described in Radford et al. (2019). The text
is web content extracted from URLs shared on
Reddit with at least three upvotes. (38GB).5
â€¢ STORIES, a dataset introduced in Trinh and Le
(2018) containing a subset of CommonCrawl
data filtered to match the story-like style of
Winograd schemas. (31GB).""",32000000000.0,"160GB*200M words/GB * (4 tokens / 3 words) = 3.2e10 tokens

max steps 500k
batch size  8k
""We pretrain with sequences of at most T = 512 tokens.""

500000*8000*512 = 2.048e+12 tokens",120.0,"First the model is pretrained for 100k steps on 1024 GPUs for 1 day, then pretraining is increased to 500k steps, so assuming they used the same number of GPUs, this would have taken 5 days.",NVIDIA Tesla V100 DGXS 32 GB,,Confident,"Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.",,,Open weights (unrestricted),"United States of America,United States of America",,,,1024.0,,2025-05-28 16:14,,,,,,"Industry,Academia",,,,122880.0,Open source,"code and weights: https://github.com/facebookresearch/fairseq/blob/main/examples/roberta/README.md
pretrain code: https://github.com/facebookresearch/fairseq/blob/main/examples/roberta/README.pretraining.md 

repo is MIT license

","Industry,Academia",$82771.08,"Hardware estimate: 100k steps took 1 day on 1024 V100; 500k steps would have taken about 5 days.
1024 * 1.25e14 * 5 * 24 * 3600 = 5.5296e22 at full utilization

6ND estimate: batches are 8k sequences of 512 tokens; 500k updates means the model saw 500k * 8k * 512 = 2.048T tokens
6 * 2.048T * 355M = 4.36224e21

Implies 4.36224e21 / 5.5296e22 = 0.0789 utilization
That seems on the edge of plausibility; our lowest confirmed utilization is 0.189.",FP16,526193.8152119832,"Hardware,Operation counting,Third-party estimation",,,,5
RoBERTa Base,Language,"Question answering,Language modeling/generation","Facebook,University of Washington","Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov",2019-07-01,RoBERTa: A Robustly Optimized BERT Pretraining Approach,https://arxiv.org/abs/1907.11692,,,,125000000.0,"""Specifically, we begin by training BERT models with the same configuration as BERTBASE (L = 12, H = 768, A = 12, 110M params""

"" This adds approximately 15M and 20M additional parameters for
BERTBASE and BERTLARGE, respectively""

110M+15M = 125M ",1.536e+21,6 FLOP / parameter / token * 125 * 10^6 parameters * 500000 steps * 8000 sequences per batch * 512 tokens per sequence = 1.536e+21 FLOP,"CC-News,""BookCorpus (BooksCorpus, Toronto Book Corpus)"",WebText2,Wikipedia","""We consider five English-language corpora of
varying sizes and domains, totaling over 160GB
of uncompressed text. We use the following text
corpora:
â€¢ BOOKCORPUS (Zhu et al., 2015) plus English
WIKIPEDIA. This is the original data used to
train BERT. (16GB).
â€¢ CC-NEWS, which we collected from the English portion of the CommonCrawl News
dataset (Nagel, 2016). The data contains 63
million English news articles crawled between
September 2016 and February 2019. (76GB after filtering).4
â€¢ OPENWEBTEXT (Gokaslan and Cohen, 2019),
an open-source recreation of the WebText corpus described in Radford et al. (2019). The text
is web content extracted from URLs shared on
Reddit with at least three upvotes. (38GB).5
â€¢ STORIES, a dataset introduced in Trinh and Le
(2018) containing a subset of CommonCrawl
data filtered to match the story-like style of
Winograd schemas. (31GB).""",43000000000.0,"160GB*200M words/GB * (4 tokens / 3 words) = 4.3e10 tokens

max steps 500k
batch size  8k
""We pretrain with sequences of at most T = 512 tokens.""

500000*8000*512 = 2.048e+12 tokens

2.048e+12 / 4.3e10 = 48 epochs",,,,,Confident,"Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.",48.0,,Open weights (unrestricted),"United States of America,United States of America",,,,,,2025-05-01 10:42,,,,,,"Industry,Academia",,,,,Open source,"code and weights: https://github.com/facebookresearch/fairseq/blob/main/examples/roberta/README.md
pretrain code: https://github.com/facebookresearch/fairseq/blob/main/examples/roberta/README.pretraining.md 

repo is MIT license

","Industry,Academia",,,,,Operation counting,,,,
Tensorized Transformer (257M),Language,Language modeling/generation,"Tianjin University,Microsoft Research Asia,Beijing Institute of Technology","Xindian Ma, Peng Zhang, Shuai Zhang, Nan Duan, Yuexian Hou, Ming Zhou, Dawei Song",2019-06-24,A Tensorized Transformer for Language Modeling,https://arxiv.org/abs/1906.09777,154.0,SOTA improvement,"""Table 2: Results and compression with state-of-the-art results on PTB and WikiText-103""",257000000.0,257M (Table 5) ,4.76e+18,"6 FLOP / parameter / token * 257000000 parameters * 103000000 tokens * 30 epochs = 4.76478e+18 FLOP
",WikiText-103,,103000000.0,,,,NVIDIA P40,,Confident,"Latest development of neural models has connected the encoder and decoder through a self-attention mechanism. In particular, Transformer, which is solely based on self-attention, has led to breakthroughs in Natural Language Processing (NLP) tasks. However, the multi-head attention mechanism, as a key component of Transformer, limits the effective deployment of the model to a resource-limited setting. In this paper, based on the ideas of tensor decomposition and parameters sharing, we propose a novel self-attention model (namely Multi-linear attention) with Block-Term Tensor Decomposition (BTD). We test and verify the proposed attention method on three language modeling tasks (i.e., PTB, WikiText-103 and One-billion) and a neural machine translation task (i.e., WMT-2016 English-German). Multi-linear attention can not only largely compress the model parameters but also obtain performance improvements, compared with a number of language modeling approaches, such as Transformer, Transformer-XL, and Transformer with tensor train decomposition.",30.0,Tensorized Transformer (257M),Unreleased,"China,China,China",,,,2.0,,2025-06-15 18:53,,,,,,"Academia,Industry,Academia",,,,,Open (non-commercial),"code, no license: https://github.com/szhangtju/The-compression-of-Transformer","Academia,Industry,Academia",,,,1027.8825149290835,Operation counting,,,,
Walking Minotaur robot,Robotics,Animal (human/non-human) imitation,"University of California (UC) Berkeley,Google Brain","Tuomas Haarnoja, Sehoon Ha, Aurick Zhou, Jie Tan, George Tucker, Sergey Levine",2019-06-19,Learning to Walk via Deep Reinforcement Learning,https://arxiv.org/abs/1812.11103,404.0,SOTA improvement,,,,,,,,,,,,,Reinforcement learning,Unknown,"Deep reinforcement learning (deep RL) holds the promise of automating the acquisition of complex controllers that can map sensory inputs directly to low-level actions. In the domain of robotic locomotion, deep RL could enable learning locomotion skills with minimal engineering and without an explicit model of the robot dynamics. Unfortunately, applying deep RL to real-world robotic tasks is exceptionally difficult, primarily due to poor sample complexity and sensitivity to hyperparameters. While hyperparameters can be easily tuned in simulated domains, tuning may be prohibitively expensive on physical systems, such as legged robots, that can be damaged through extensive trial-and-error learning. In this paper, we propose a sample-efficient deep RL algorithm based on maximum entropy RL that requires minimal per-task tuning and only a modest number of trials to learn neural network policies. We apply this method to learning walking gaits on a real-world Minitaur robot. Our method can acquire a stable gait from scratch directly in the real world in about two hours, without relying on any model or simulation, and the resulting policy is robust to moderate variations in the environment. We further show that our algorithm achieves state-of-the-art performance on simulated benchmarks with a single set of hyperparameters. Videos of training and the learned policy can be found on the project website.",,,Unreleased,"United States of America,United States of America",,,,,,2025-05-09 11:32,,,,,,"Academia,Industry",,,,,Unreleased,,"Academia,Industry",,,,,,,,,
TAPE Transformer,Biology,"Proteins,Protein or nucleotide language model (pLM/nLM)","University of California (UC) Berkeley,Covariant,Google,Chan Zuckerberg Initiative","Roshan Rao, Nicholas Bhattacharya, Neil Thomas, Yan Duan, Xi Chen, John Canny, Pieter Abbeel, Yun S. Song",2019-06-19,Evaluating Protein Transfer Learning with TAPE,https://arxiv.org/abs/1906.08230,697.0,,,38000000.0,"""We use a 12-layer Transformer with a hidden size of 512 units and 8 attention heads, leading to a 38M-parameter model""",3e+19,"""All self-supervised models are trained on four NVIDIA V100 GPUs for one week""

(7 * 24 * 3600) s * 4 GPUs * 3.1e13 FLOP/s * 0.4 (utilization assumption) = 3e19",Pfam,"""We use Pfam [33], a database of thirty-one million protein domains used extensively in bioinformatics, as the pretraining corpus for TAPE""",9660000001.0,"N = 32.2 million sequences
L = 300 residues per sequence
Total datapoints = N Ã— L = 32,200,000 Ã— 300 = 9.66e9 residues",168.0,,NVIDIA V100,Self-supervised learning,Confident,"Protein modeling is an increasingly popular area of machine learning research. Semi-supervised learning has emerged as an important paradigm in protein modeling due to the high cost of acquiring supervised protein labels, but the current literature is fragmented when it comes to datasets and standardized evaluation techniques. To facilitate progress in this field, we introduce the Tasks Assessing Protein Embeddings (TAPE), a set of five biologically relevant semi-supervised learning tasks spread across different domains of protein biology. We curate tasks into specific training, validation, and test splits to ensure that each task tests biologically relevant generalization that transfers to real-life scenarios. We benchmark a range of approaches to semi-supervised protein representation learning, which span recent work as well as canonical sequence learning techniques. We find that self-supervised pretraining is helpful for almost all models on all tasks, more than doubling performance in some cases. Despite this increase, in several cases features learned by self-supervised pretraining still lag behind features extracted by state-of-the-art non-neural techniques. This gap in performance suggests a huge opportunity for innovative architecture design and improved modeling paradigms that better capture the signal in biological sequences. TAPE will help the machine learning community focus effort on scientifically relevant problems. Toward this end, all data and code used to run these experiments are available at https://github.com/songlab-cal/tape.",,,Open weights (unrestricted),"United States of America,United States of America,United States of America,United States of America",,,,4.0,,2025-05-09 11:32,,,,,,"Academia,Industry,Industry,Research collective",,,,672.0,Open source,"BSD license. code, data, and weights:
https://github.com/songlab-cal/tape?tab=readme-ov-file#data","Academia,Industry,Industry,Research collective",,,,2467.192734691173,Hardware,,,,
LaNet-L (CIFAR-10),Vision,"Image classification,Neural Architecture Search - NAS","Brown University,Facebook","Linnan Wang, Saining Xie, Teng Li, Rodrigo Fonseca, Yuandong Tian",2019-06-17,Sample-Efficient Neural Architecture Search by Learning Action Space,https://arxiv.org/abs/1906.06832,44.0,SOTA improvement,"""In practice, LaNAS finds a network that achieves SOTA 99.0% accuracy on CIFAR-10""",44100000.0,44.1M,,"LaNet-L was trained on 150 GPU-days, however the GPU was not specified",CIFAR-10,"Trained on CIFAR-10, no pretraining",60000.0,,,,,Supervised,Confident,"Neural Architecture Search (NAS) has emerged as a promising technique for automatic neural network design. However, existing MCTS based NAS approaches often utilize manually designed action space, which is not directly related to the performance metric to be optimized (e.g., accuracy), leading to sample-inefficient explorations of architectures. To improve the sample efficiency, this paper proposes Latent Action Neural Architecture Search (LaNAS), which learns actions to recursively partition the search space into good or bad regions that contain networks with similar performance metrics. During the search phase, as different action sequences lead to regions with different performance, the search efficiency can be significantly improved by biasing towards the good regions. On three NAS tasks, empirical results demonstrate that LaNAS is at least an order more sample efficient than baseline methods including evolutionary algorithms, Bayesian optimizations, and random search. When applied in practice, both one-shot and regular LaNAS consistently outperform existing results. Particularly, LaNAS achieves 99.0% accuracy on CIFAR-10 and 80.8% top1 accuracy at 600 MFLOPS on ImageNet in only 800 samples, significantly outperforming AmoebaNet with 33x fewer samples. Our code is publicly available at this https URL.",600.0,,Open weights (non-commercial),"United States of America,United States of America",,,,,,2025-05-28 16:14,,,,,,"Academia,Industry",,,,3600.0,Open (non-commercial),"code and weights here, non-commercial license: https://github.com/facebookresearch/LaMCTS/tree/main/LaNAS/LaNet/CIFAR10","Academia,Industry",,,FP16,,,,,,
PG-SWGAN,Image generation,Image generation,ETH Zurich,"Jiqing Wu, Zhiwu Huang, Dinesh Acharya, Wen Li, Janine Thoma, Danda Pani Paudel, Luc Van Gool",2019-06-15,Sliced Wasserstein Generative Models,https://openaccess.thecvf.com/content_CVPR_2019/html/Wu_Sliced_Wasserstein_Generative_Models_CVPR_2019_paper.html,115.0,SOTA improvement,"""For fair comparison, we equip the same progressive growing architecture with our proposed SWGAN objective and its dual
SWD blocks (PG-SWGAN). As shown in Fig. 3 (Right)
and Fig. 5, our PG-SWGAN can outperform PG-WGAN in
terms of both qualitative and quantitative comparison on the
CelebA-HQ and LSUN datasets""",,,,,"CIFAR-10,LSUN,CelebA",,,,,,,,Unknown,"In generative modeling, the Wasserstein distance (WD) has emerged as a useful metric to measure the discrepancy between generated and real data distributions. Unfortunately, it is challenging to approximate the WD of high-dimensional distributions. In contrast, the sliced Wasserstein distance (SWD) factorizes high-dimensional distributions into their multiple one-dimensional marginal distributions and is thus easier to approximate. In this paper, we introduce novel approximations of the primal and dual SWD. Instead of using a large number of random projections, as it is done by conventional SWD approximation methods, we propose to approximate SWDs with a small number of parameterized orthogonal projections in an end-to-end deep learning fashion. As concrete applications of our SWD approximations, we design two types of differentiable SWD blocks to equip modern generative frameworks---Auto-Encoders (AE) and Generative Adversarial Networks (GAN). In the experiments, we not only show the superiority of the proposed generative models on standard image synthesis benchmarks, but also demonstrate the state-of-the-art performance on challenging high resolution image and video generation in an unsupervised manner.",,,Unreleased,Switzerland,,,,,,2025-05-28 16:14,,,,,,Academia,,,,,Open (non-commercial),"looks like code but no weights, no license specified: https://github.com/musikisomorphie/swd",Academia,,,FP32,,,,,,
Char-CNN-BiLSTM,Language,Language modeling,Capital One,"Chris Larson, Tarek Lahlou, Diana Mingels, Zachary Kulis, Erik Mueller",2019-06-13,Telephonetic: Making Neural Language Models Robust to ASR and Semantic Noise,https://arxiv.org/abs/1906.05678,2.0,SOTA improvement,"""Notably, our language model achieves a test perplexity of 37.49 on PTB, which to our knowledge is state-of-the-art among models trained only on PTB.""",,,,,,,,,,,,,Unknown,"Speech processing systems rely on robust feature extraction to handle phonetic and semantic variations found in natural language. While techniques exist for desensitizing features to common noise patterns produced by Speech-to-Text (STT) and Text-to-Speech (TTS) systems, the question remains how to best leverage state-of-the-art language models (which capture rich semantic features, but are trained on only written text) on inputs with ASR errors. In this paper, we present Telephonetic, a data augmentation framework that helps robustify language model features to ASR corrupted inputs. To capture phonetic alterations, we employ a character-level language model trained using probabilistic masking. Phonetic augmentations are generated in two stages: a TTS encoder (Tacotron 2, WaveGlow) and a STT decoder (DeepSpeech). Similarly, semantic perturbations are produced by sampling from nearby words in an embedding space, which is computed using the BERT language model. Words are selected for augmentation according to a hierarchical grammar sampling strategy. Telephonetic is evaluated on the Penn Treebank (PTB) corpus, and demonstrates its effectiveness as a bootstrapping technique for transferring neural language models to the speech domain. Notably, our language model achieves a test perplexity of 37.49 on PTB, which to our knowledge is state-of-the-art among models trained only on PTB.",,Char-CNN-BiLSTM,Unreleased,United States of America,,,,,,2024-09-05 14:08,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
AWD-LSTM + MoS + Partial Shuffled,Language,Language modeling,University of Texas at Austin,"Dilin Wang, Chengyue Gong, Qiang Liu",2019-06-10,Improving Neural Language Modeling via Adversarial Training,https://arxiv.org/abs/1906.03805,112.0,SOTA improvement,"""our method improves on the single model state-of-the-art results for language modeling on Penn Treebank (PTB) and Wikitext-2, achieving test perplexity scores of 46.01 and 38.07, respectively""",35000000.0,35M (Table 2),3.15e+17,6 FLOP / parameter / token * 35000000 parameters * 2000000 tokens * 750 epochs = 3.15e+17 FLOP,WikiText-2,,2000000.0,750 epochs (figure 1c),,,,,Confident,"Recently, substantial progress has been made in language modeling by using deep neural networks. However, in practice, large scale neural language models have been shown to be prone to overfitting. In this paper, we present a simple yet highly effective adversarial training mechanism for regularizing neural language models. The idea is to introduce adversarial noise to the output embedding layer while training the models. We show that the optimal adversarial noise yields a simple closed-form solution, thus allowing us to develop a simple and time efficient algorithm. Theoretically, we show that our adversarial mechanism effectively encourages the diversity of the embedding vectors, helping to increase the robustness of models. Empirically, we show that our method improves on the single model state-of-the-art results for language modeling on Penn Treebank (PTB) and Wikitext-2, achieving test perplexity scores of 46.01 and 38.07, respectively. When applied to machine translation, our method improves over various transformer-based translation baselines in BLEU scores on the WMT14 English-German and IWSLT14 German-English tasks.",750.0,AWD-LSTM + MoS + Partial Shuffled,Open weights (non-commercial),United States of America,,,,,,2025-04-14 13:57,,,,,,Academia,,,,,Open (non-commercial),"code and weights. no license provided:
https://github.com/ChengyueGongR/advsoft",Academia,,,,,Operation counting,,,,
Transformer-XL Large + Phrase Induction,Language,Language modeling/generation,"Massachusetts Institute of Technology (MIT),University of Illinois Urbana-Champaign (UIUC)","Hongyin Luo, Lan Jiang, Yonatan Belinkov, James Glass",2019-06-04,"Improving Neural Language Models by Segmenting, Attending, and Predicting the Future",https://arxiv.org/abs/1906.01702,12.0,SOTA improvement,"""We achieved a new state-of-the-art performance of 17.4 perplexity on the Wikitext-103 dataset""",257000000.0,,3.7848651e+20,"Fine-tuned from pre-trained Transformer-XL Large (upd 3.7832771e+20 FLOP, old estimation 1.09e19 FLOP). 

Total: 3.7832771e20 + 1.588e17 = 3.7848651e+20 FLOP (Speculative confidence same as Transformer XL)",WikiText-103,,,,,,,,Speculative,"Common language models typically predict the next word given the context. In this work, we propose a method that improves language modeling by learning to align the given context and the following phrase. The model does not require any linguistic annotation of phrase segmentation. Instead, we define syntactic heights and phrase segmentation rules, enabling the model to automatically induce phrases, recognize their task-specific heads, and generate phrase embeddings in an unsupervised learning manner. Our method can easily be applied to language models with different network architectures since an independent module is used for phrase induction and context-phrase alignment, and no change is required in the underlying language modeling network. Experiments have shown that our model outperformed several strong baseline models on different data sets. We achieved a new state-of-the-art performance of 17.4 perplexity on the Wikitext-103 dataset. Additionally, visualizing the outputs of the phrase induction module showed that our model is able to learn approximate phrase-level structural knowledge without any annotation.",1.0,Transformer-XL Large + Phrase Induction,Unreleased,"United States of America,United States of America",Transformer-XL (257M),158800000000000000,"Additional 1.6e17 FLOP of fine-tuning from one epoch on WikiText-103:
6 * 257M * 103M = 1.588e17 FLOP.",,,2025-04-21 19:29,,,,,,"Academia,Academia",,,,,Open source,"code license, BSD-3: https://github.com/luohongyin/PILM?tab=BSD-3-Clause-1-ov-file#readme

training: https://github.com/luohongyin/PILM/blob/master/train_span_wt103.sh ","Academia,Academia",,,,,Operation counting,,,,
XLNet,Language,"Language modeling/generation,Question answering,Sentiment classification","Carnegie Mellon University (CMU),Google Brain","Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le",2019-06-01,XLNet: Generalized Autoregressive Pretraining for Language Understanding,https://arxiv.org/abs/1906.08237,7785.0,Highly cited,,340000000.0,"Same size as BERT-Large, which was 340M",6.19e+21,"""Specifically, we train on 512 TPU v3 chips for 500K steps with an Adam weight decay optimizer, linear learning rate decay, and a batch size of 8192, which takes about 5.5 days.""

123 teraflops * 5.5 days * 24 * 3600 * 512 * 0.3 utilization (assumption) ~= 8977858560*10^12=8.9*10^21

Alternatively, 500k steps * batch size 8192 * sequence length 512 = 2.1T training passes. 340 million * 6 * 2 trillion = 4.3e21 FLOP. 

Geometric mean: sqrt(8.9e21 * 4.3e21) = 6.19e21","Wikipedia,""BookCorpus (BooksCorpus, Toronto Book Corpus)""","""Following BERT [10], we use the BooksCorpus [40] and English Wikipedia as part of our pretraining
data, which have 13GB plain text combined. In addition, we include Giga5 (16GB text) [26],
ClueWeb 2012-B (extended from [5]), and Common Crawl [6] for pretraining. We use heuristics
to aggressively filter out short or low-quality articles for ClueWeb 2012-B and Common Crawl,
which results in 19GB and 110GB text respectively. After tokenization with SentencePiece [17], we
obtain 2.78B, 1.09B, 4.75B, 4.30B, and 19.97B subword pieces for Wikipedia, BooksCorpus, Giga5,
ClueWeb, and Common Crawl respectively, which are 32.89B in total.""",,,,,Google TPU v3,,Confident,"With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.",63.76,,Open weights (unrestricted),"United States of America,United States of America",,,,,,2025-05-28 16:15,,,,8192.0,,"Academia,Industry",,,,,Open source,Apache 2.0 for code and weights: https://github.com/zihangdai/xlnet,"Academia,Industry",,,FP32,,"Hardware,Operation counting",,,,6
DLRM-2020,Recommendation,Recommender system,Facebook AI,"Maxim Naumov, Dheevatsa Mudigere, Hao-Jun Michael Shi, Jianyu Huang, Narayanan Sundaraman, Jongsoo Park, Xiaodong Wang, Udit Gupta, Carole-Jean Wu, Alisson G. Azzolini, Dmytro Dzhulgakov, Andrey Mallevich, Ilia Cherniavskii, Yinghai Lu, Raghuraman Krishnamoorthi, Ansha Yu, Volodymyr Kondratenko, Stephanie Pereira, Xianjie Chen, Wenlin Chen, Vijay Rao, Bill Jia, Liang Xiong, Misha Smelyanskiy",2019-05-31,Deep Learning Recommendation Model for Personalization and Recommendation Systems,https://arxiv.org/abs/1906.00091,644.0,SOTA improvement,"""In this paper, we develop a state-of-the-art deep learning recommendation model
(DLRM)""",100000000000.0,"Figure 1

https://arxiv.org/abs/2104.05158",4e+18,"Figure 1

https://arxiv.org/abs/2104.05158",,,,,,,,,Confident,"With the advent of deep learning, neural network-based recommendation models have emerged as an important tool for tackling personalization and recommendation tasks. These networks differ significantly from other deep learning networks due to their need to handle categorical features and are not well studied or understood. In this paper, we develop a state-of-the-art deep learning recommendation model (DLRM) and provide its implementation in both PyTorch and Caffe2 frameworks. In addition, we design a specialized parallelization scheme utilizing model parallelism on the embedding tables to mitigate memory constraints while exploiting data parallelism to scale-out compute from the fully-connected layers. We compare DLRM against existing recommendation models and characterize its performance on the Big Basin AI platform, demonstrating its usefulness as a benchmark for future algorithmic experimentation and system co-design.",,,Unreleased,United States of America,,,,,,2025-06-18 09:35,,,,,,Industry,,,,,Open source,"MIT, training/inference code: https://github.com/facebookresearch/dlrm",Industry,,,FP32,,Reported,,,,
MnasNet-A3,Vision,"Image classification,Object detection",Google,"Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, Quoc V. Le",2019-05-29,MnasNet: Platform-Aware Neural Architecture Search for Mobile,https://arxiv.org/abs/1807.11626,2788.0,Highly cited,,5200000.0,From https://arxiv.org/pdf/1807.11626.pdf,1.5e+21,"""each architecture search takes 4.5 days on 64 TPUv2 devices""
This seems to be referring to a TPUv2 pod, consisting of 64 four-chip modules. The total performance is 11.5 petaFLOPS.
https://en.wikipedia.org/wiki/Tensor_Processing_Unit#Second_generation_TPU
Assuming a 33% utilization rate:

4.5 days * 64 * 180 teraFLOPS * 0.33 = 1.48*10^21 FLOP

However, it is unclear if ""64 TPUv2 devices"" refers to chips or modules, so the true compute might be 1/4 of this amount.",ImageNet,,1280000.0,"""In this paper, we directly perform our architecture search on the ImageNet training set but with fewer training steps (5 epochs). As a common practice, we reserve randomly selected 50K images from the training set as the fixed validation set. """,108.0,,Google TPU v3,,Speculative,"Designing convolutional neural networks (CNN) for mobile devices is challenging because mobile models need to be small and fast, yet still accurate. Although significant efforts have been dedicated to design and improve mobile CNNs on all dimensions, it is very difficult to manually balance these trade-offs when there are so many architectural possibilities to consider. In this paper, we propose an automated mobile neural architecture search (MNAS) approach, which explicitly incorporate model latency into the main objective so that the search can identify a model that achieves a good trade-off between accuracy and latency. Unlike previous work, where latency is considered via another, often inaccurate proxy (e.g., FLOPS), our approach directly measures real-world inference latency by executing the model on mobile phones. To further strike the right balance between flexibility and search space size, we propose a novel factorized hierarchical search space that encourages layer diversity throughout the network. Experimental results show that our approach consistently outperforms state-of-the-art mobile CNN models across multiple vision tasks. On the ImageNet classification task, our MnasNet achieves 75.2% top-1 accuracy with 78ms latency on a Pixel phone, which is 1.8x faster than MobileNetV2 [29] with 0.5% higher accuracy and 2.3x faster than NASNet [36] with 1.2% higher accuracy. Our MnasNet also achieves better mAP quality than MobileNets for COCO object detection. Code is at this https URL",,,Open weights (unrestricted),United States of America,,,,256.0,,2025-05-28 16:15,,,,,,Industry,,,,,Open source,"Apache license: https://github.com/tensorflow/tpu/blob/master/LICENSE
model repo is here, includes training code: https://github.com/tensorflow/tpu/tree/master/models/official/mnasnet 

",Industry,$9551.59,,FP32,115847.74333325788,Hardware,,,,10
MnasNet-A1 + SSDLite,Vision,"Image classification,Object detection,Neural Architecture Search - NAS",Google,"Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, Quoc V. Le",2019-05-29,MnasNet: Platform-Aware Neural Architecture Search for Mobile,https://arxiv.org/abs/1807.11626,2788.0,Highly cited,,4900000.0,From https://arxiv.org/pdf/1807.11626.pdf,1.5e+21,"""each architecture search takes 4.5 days on 64 TPUv2 devices""
This seems to be referring to a TPUv2 pod, consisting of 64 four-chip modules. The total performance is 11.5 petaFLOPS.
https://en.wikipedia.org/wiki/Tensor_Processing_Unit#Second_generation_TPU
Assuming a 33% utilization rate:

4.5 days * 64 * 180 teraFLOPS * 0.33 = 1.48*10^21 FLOP

However, it is unclear if ""64 TPUv2 devices"" refers to chips or modules, so the true compute might be 1/4 of this amount.",COCO,,118000.0,,108.0,,Google TPU v3,,Speculative,"Designing convolutional neural networks (CNN) for mobile devices is challenging because mobile models need to be small and fast, yet still accurate. Although significant efforts have been dedicated to design and improve mobile CNNs on all dimensions, it is very difficult to manually balance these trade-offs when there are so many architectural possibilities to consider. In this paper, we propose an automated mobile neural architecture search (MNAS) approach, which explicitly incorporate model latency into the main objective so that the search can identify a model that achieves a good trade-off between accuracy and latency. Unlike previous work, where latency is considered via another, often inaccurate proxy (e.g., FLOPS), our approach directly measures real-world inference latency by executing the model on mobile phones. To further strike the right balance between flexibility and search space size, we propose a novel factorized hierarchical search space that encourages layer diversity throughout the network. Experimental results show that our approach consistently outperforms state-of-the-art mobile CNN models across multiple vision tasks. On the ImageNet classification task, our MnasNet achieves 75.2% top-1 accuracy with 78ms latency on a Pixel phone, which is 1.8x faster than MobileNetV2 [29] with 0.5% higher accuracy and 2.3x faster than NASNet [36] with 1.2% higher accuracy. Our MnasNet also achieves better mAP quality than MobileNets for COCO object detection. Code is at this https URL",,,Open weights (unrestricted),United States of America,,,,256.0,,2025-05-28 16:15,,,,,,Industry,,,,,Open source,"Apache license: https://github.com/tensorflow/tpu/blob/master/LICENSE
model repo is here, includes training code: https://github.com/tensorflow/tpu/tree/master/models/official/mnasnet 

",Industry,$9551.59,,FP32,115847.74333325788,Hardware,,,,11
RSM,Language,Language modeling,Cerenaut,"David Rawlinson, Abdelrahman Ahmed, Gideon Kowadlo",2019-05-28,Learning distant cause and effect using only local and immediate credit assignment,https://arxiv.org/abs/1905.11589,3.0,,,,,,,,,,,,,,,Unknown,"We present a recurrent neural network memory that uses sparse coding to create a combinatoric encoding of sequential inputs. Using several examples, we show that the network can associate distant causes and effects in a discrete stochastic process, predict partially-observable higher-order sequences, and enable a DQN agent to navigate a maze by giving it memory. The network uses only biologically-plausible, local and immediate credit assignment. Memory requirements are typically one order of magnitude less than existing LSTM, GRU and autoregressive feed-forward sequence learning models. The most significant limitation of the memory is generalization to unseen input sequences. We explore this limitation by measuring next-word prediction perplexity on the Penn Treebank dataset.",,RSM,Unreleased,Australia,,,,,,2024-09-05 14:08,,,,,,,,,,,Open source,"code for PTB, Apache 2.0 license: https://github.com/Cerenaut/rsm ",,,,,,,,,,
EfficientNet-B1,Vision,Image classification,Google,"Mingxing Tan, Quoc V. Le",2019-05-28,EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks,https://arxiv.org/abs/1905.11946,,,,7800000.0,7.8M (Figure 1),,,ImageNet,,,,,,,,Unknown,"Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet.
To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.3% top-1 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flowers (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at this https URL.",,,Open weights (unrestricted),United States of America,,,,,,2025-05-12 13:56,,,,,,Industry,,,,,Open source,Apache license: https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet,Industry,,,,,,,,,
AWD-LSTM-DRILL + dynamic evaluationâ€  (WT2),Language,Language modeling,IDIAP,"Nikolaos Pappas, James Henderson",2019-05-14,Deep Residual Output Layers for Neural Language Generation,https://arxiv.org/abs/1905.05513,7.0,SOTA improvement,"""our models improve over the state-of-the-art by +1.6 perplexity on PennTreebank and by +3.9 perplexity on
Wikitext-2""",34000000.0,"34M, Table 2",4.08e+17,6 FLOP / parameter / token * 34000000 parameters * 2000000 tokens * 1000 epochs = 4.08e+17 FLOP,WikiText-2,,,max epochs - 1000 (from http://github.com/idiap/drill),29.0,106 sec per epoch (Table 3) -> 106000 seconds = 29 hours,,,Likely,"Many tasks, including language generation, benefit from learning the structure of the output space, particularly when the space of output labels is large and the data is sparse. State-of-the-art neural language models indirectly capture the output space structure in their classifier weights since they lack parameter sharing across output labels. Learning shared output label mappings helps, but existing methods have limited expressivity and are prone to overfitting. In this paper, we investigate the usefulness of more powerful shared mappings for output labels, and propose a deep residual output mapping with dropout between layers to better capture the structure of the output space and avoid overfitting. Evaluations on three language generation tasks show that our output label mapping can match or improve state-of-the-art recurrent and self-attention architectures, and suggest that the classifier does not necessarily need to be high-rank to better model natural language if it is better at capturing the structure of the output space.",1000.0,AWD-LSTM-DRILL + dynamic evaluationâ€  (WT2),Open weights (unrestricted),Switzerland,,,,,,2025-06-11 18:01,,,,,,Academia,,,,,Open source,"copyleft license (restricts derivative works to be open)
https://github.com/idiap/drill?tab=GPL-3.0-1-ov-file#readme

train/eval script: https://github.com/idiap/drill/blob/master/main.py ",Academia,,,FP32,,Operation counting,,,,
RaptorX-Contact,Biology,"Protein folding prediction,Proteins",Toyota Technological Institute at Chicago,"Jinbo Xu, Sheng Wang",2019-05-02,Analysis of distance-based protein structure prediction by deep learning in CASP13,https://www.biorxiv.org/content/biorxiv/early/2019/05/02/624460.full.pdf,,,,,,,,PDB25 and UniProt,,450000001.0,"Calculation steps:
1. Training proteins = 11,410 - 900 = 10,510 proteins
2. Residue pairs per protein = (300 Ã— 299)/2 = 44,850 pairs
3. Total data points = 10,510 Ã— 44,850 = 4.73 Ã— 10â¸
Final estimate â‰ˆ 4.5 Ã— 10â¸ data points",,,,,Unknown,"This paper reports the CASP13 results of distance-based contact prediction, threading and folding methods implemented in three RaptorX servers, which are built upon the powerful deep convolutional residual neural network (ResNet) method initiated by us for contact prediction in CASP12. On the 32 CASP13 FM (free-modeling) targets with a median MSA (multiple sequence alignment) depth of 36, RaptorX yielded the best contact prediction among 46 groups and almost the best 3D structure modeling among all server groups without time-consuming conformation sampling. In particular, RaptorX achieved top L/5, L/2 and L long-range contact precision of 70%, 58% and 45%, respectively, and predicted correct folds (TMscore>0.5) for 18 of 32 targets. Although on average underperforming AlphaFold in 3D modeling, RaptorX predicted correct folds for all FM targets with >300 residues (T0950-D1, T0969-D1 and T1000-D2) and generated the best 3D models for T0950-D1 and T0969-D1 among all groups. This CASP13 test confirms our previous findings: (1) predicted distance is more useful than contacts for both template-based and free modeling; and (2) structure modeling may be improved by integrating alignment and co- evolutionary information via deep learning. This paper will discuss progress we have made since CASP12, the strength and weakness of our methods, and why deep learning performed much better in CASP13.",,,Unreleased,United States of America,,,,,,2025-04-30 10:04,,,,,,Academia,,,,,Open (restricted use),"this license for code:
https://github.com/j3xugit/RaptorX-Contact?tab=GPL-3.0-1-ov-file",Academia,,,,,,,,,
Neuro-Symbolic Concept Learner,"Vision,Language",Visual question answering,"Massachusetts Institute of Technology (MIT),Tsinghua University,MIT-IBM Watson AI Lab,DeepMind","Jiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua B. Tenenbaum, Jiajun Wu",2019-04-26,"The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision",https://arxiv.org/abs/1904.12584,695.0,SOTA improvement,"""NS-CLâ€™s modularized design enables interpretable, robust, and accurate visual reasoning: it achieves state-of-the-art performance on the CLEVR datase""",,,,,"CLEVR,VQS,ImageNet","CLEVR, ImageNet, VQS
5000 in CLEVR
64509 in VQS
and whole ImageNet for pretraining
""We train NS-CL on 5K images (<10% of CLEVRâ€™s 70K training images). We generate 20 questions for each image for the entire curriculum learning process""

section 4.3 ""All models use a pre-trained semantic parser on the full CLEVR dataset""

""The only extra supervision of the visual perception module comes from the pre-training of the perception modules on ImageNet (Deng et al., 2009). To quantify the influence of this pre-training""

In appendix G.2 (VQS Dataset):""All models are trained on the first 63,509 images of the training set, and tested on the test split. For hyper-parameter tuning and model selection, the rest 5,000 images from the training set are used for validation.",,"CLEVR, ImageNet, VQS
5000 in CLEVR
64509 in VQS
and whole ImageNet for pretraining
""We train NS-CL on 5K images (<10% of CLEVRâ€™s 70K training images). We generate 20 questions for each image for the entire curriculum learning process""

section 4.3 ""All models use a pre-trained semantic parser on the full CLEVR dataset""

""The only extra supervision of the visual perception module comes from the pre-training of the perception modules on ImageNet (Deng et al., 2009). To quantify the influence of this pre-training""

In appendix G.2 (VQS Dataset):
""All models are trained on the first 63,509 images of the training set, and tested on the test split. For hyper-parameter tuning and model selection, the rest 5,000 images from the training set are used for validation.",,,,,Unknown,"We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them; instead, our model learns by simply looking at images and reading paired questions and answers. Our model builds an object-based scene representation and translates sentences into executable, symbolic programs. To bridge the learning of two modules, we use a neuro-symbolic reasoning module that executes these programs on the latent scene representation. Analogical to human concept learning, the perception module learns visual concepts based on the language description of the object being referred to. Meanwhile, the learned visual concepts facilitate learning new words and parsing new sentences. We use curriculum learning to guide the searching over the large compositional space of images and language. Extensive experiments demonstrate the accuracy and efficiency of our model on learning visual concepts, word representations, and semantic parsing of sentences. Further, our method allows easy generalization to new object attributes, compositions, language concepts, scenes and questions, and even new program domains. It also empowers applications including visual question answering and bidirectional image-text retrieval.",,,Unreleased,"United States of America,China,United States of America,United Kingdom of Great Britain and Northern Ireland",,,,,,2024-09-11 14:34,,,,,,"Academia,Academia,Academia,Industry,Industry",,,,,Open source,MIT code: https://github.com/vacancy/NSCL-PyTorch-Release,"Academia,Academia,Academia,Industry,Industry",,,,,,,,,
DANet,Vision,Semantic segmentation,Chinese Academy of Sciences,"Jun Fu, Jing Liu, Haijie Tian, Yong Li, Yongjun Bao, Zhiwei Fang, Hanqing Lu",2019-04-21,Dual Attention Network for Scene Segmentation,https://openaccess.thecvf.com/content_CVPR_2019/html/Fu_Dual_Attention_Network_for_Scene_Segmentation_CVPR_2019_paper.html,4620.0,Highly cited,,,,,,"Cityscapes,COCO-Stuff,PASCAL-Context",,,,,,,,Unknown,"In this paper, we address the scene segmentation task by capturing rich contextual dependencies based on the self-attention mechanism. Unlike previous works that capture contexts by multi-scale features fusion, we propose a Dual Attention Networks (DANet) to adaptively integrate local features with their global dependencies. Specifically, we append two types of attention modules on top of traditional dilated FCN, which model the semantic interdependencies in spatial and channel dimensions respectively. The position attention module selectively aggregates the features at each position by a weighted sum of the features at all positions. Similar features would be related to each other regardless of their distances. Meanwhile, the channel attention module selectively emphasizes interdependent channel maps by integrating associated features among all channel maps. We sum the outputs of the two attention modules to further improve feature representation which contributes to more precise segmentation results. We achieve new state-of-the-art segmentation performance on three challenging scene segmentation datasets, i.e., Cityscapes, PASCAL Context and COCO Stuff dataset. In particular, a Mean IoU score of 81.5% on Cityscapes test set is achieved without using coarse data.
",,,Open weights (unrestricted),China,,,,,,2025-02-03 19:22,,,,,,Academia,,,,,Open source,"MIT for code and weights: https://github.com/junfu1115/DANet/
train code: https://github.com/junfu1115/DANet/tree/master/experiments/recognition ",Academia,,,,,,,,,
BERT-Large-CAS (PTB+WT2+WT103),Language,"Neural Architecture Search - NAS,Language modeling/generation",Amazon,"Chenguang Wang, Mu Li, Alexander J. Smola",2019-04-20,Language Models with Transformers,https://arxiv.org/abs/1904.09408,110.0,SOTA improvement,"""CAS achieves perplexities between 20.42 and 34.11 on all problems, i.e. on average an improvement of 12.0 perplexity units compared to state-of-the-art LSTMs""",395000000.0,395M (Table 6),1.5405e+20,"6 FLOP / token / parameter * 395000000 parameters * 1300000000 parameters * 50 epochs = 1.5405e+20 FLOP

________
in the Algorithmic progress paper, the estimation was 5.21E+20 FLOP","Penn TreeBank (PTB),WikiText-2,WikiText-103",,1300000000.0,"Table 7:
0.1B (PTB)+ 0.2B (WT-2) + 1.0B (WT-103) = 1.3B

We pick 128 as sequence length and 16 as minibatch size

""We use NT-ASGD (Merity et al., 2017) to train 50 epochs on training datasets""",,,,,Likely,"The Transformer architecture is superior to RNN-based models in computational efficiency. Recently, GPT and BERT demonstrate the efficacy of Transformer models on various NLP tasks using pre-trained language models on large-scale corpora. Surprisingly, these Transformer architectures are suboptimal for language model itself. Neither self-attention nor the positional encoding in the Transformer is able to efficiently incorporate the word-level sequential context crucial to language modeling.
In this paper, we explore effective Transformer architectures for language model, including adding additional LSTM layers to better capture the sequential context while still keeping the computation efficient. We propose Coordinate Architecture Search (CAS) to find an effective architecture through iterative refinement of the model. Experimental results on the PTB, WikiText-2, and WikiText-103 show that CAS achieves perplexities between 20.42 and 34.11 on all problems, i.e. on average an improvement of 12.0 perplexity units compared to state-of-the-art LSTMs. The source code is publicly available.",50.0,BERT-Large-CAS (PTB+WT2+WT103),Unreleased,United States of America,,,,,,2025-04-14 14:23,,,,,,Industry,,,,,Open source,Apache 2.0 license: https://github.com/cgraywang/gluon-nlp-1/blob/lmtransformer/scripts/language_model/train/transformer_lm.py,Industry,,,FP32,,Operation counting,,,,
SpecAugment,Language,Speech recognition,Google Brain," Daniel S. Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, Ekin D. Cubuk, Quoc V. Le",2019-04-18,SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition,https://arxiv.org/abs/1904.08779,3207.0,Highly cited,,,,,,"LibriSpeech,Switchboard,Fisher",,,,,,,,Unknown,,,,Unreleased,United States of America,,,,,,2024-11-01 10:03,,,,,,Industry,,,,,Unreleased,LibriSpeech is open source,Industry,,,,,,,,,
LTM,Language,Language modeling/generation,Murdoch University,"Anupiya Nugaliyadde, Kok Wai Wong, Ferdous Sohel, Hong Xie",2019-04-18,Language Modeling through Long Term Memory Network,https://arxiv.org/abs/1904.08936,19.0,,,,,,,,,,,,,,,Unknown,"Recurrent Neural Networks (RNN), Long Short-Term Memory Networks (LSTM), and Memory Networks which contain memory are popularly used to learn patterns in sequential data. Sequential data has long sequences that hold relationships. RNN can handle long sequences but suffers from the vanishing and exploding gradient problems. While LSTM and other memory networks address this problem, they are not capable of handling long sequences (50 or more data points long sequence patterns). Language modelling requiring learning from longer sequences are affected by the need for more information in memory. This paper introduces Long Term Memory network (LTM), which can tackle the exploding and vanishing gradient problems and handles long sequences without forgetting. LTM is designed to scale data in the memory and gives a higher weight to the input in the sequence. LTM avoid overfitting by scaling the cell state after achieving the optimal results. The LTM is tested on Penn treebank dataset, and Text8 dataset and LTM achieves test perplexities of 83 and 82 respectively. 650 LTM cells achieved a test perplexity of 67 for Penn treebank, and 600 cells achieved a test perplexity of 77 for Text8. LTM achieves state of the art results by only using ten hidden LTM cells for both datasets.",,LTM,Unreleased,Australia,,,,,,2024-09-05 14:08,,,,,,Academia,,,,,Unreleased,,Academia,,,,,,,,,
MEGNet (molecule model),Materials science,Molecular property prediction,University of California San Diego,"Chi Chen, Weike Ye, Yunxing Zuo, Chen Zheng, Shyue Ping Ong",2019-04-10,Graph Networks as a Universal Machine Learning Framework for Molecules and Crystals,https://arxiv.org/abs/1812.05055,1297.0,Highly cited,"Citation count: 1297 as of April 8, 2025 (source: https://pubs.acs.org/doi/10.1021/acs.chemmater.9b01294)",8720.0,Calculations here: https://docs.google.com/document/d/1BTmyZ9KVTIwkp9z9tRRFsWM5A0QcXiufXrIhujffyso/edit?tab=t.0#heading=h.dcux1bvmijlm,4.536e+17,"Calculations here: 
https://docs.google.com/document/d/1BTmyZ9KVTIwkp9z9tRRFsWM5A0QcXiufXrIhujffyso/edit?tab=t.0#heading=h.c0h2t2icf5xr

Assuming a single GPU (the model is quite small):
1.0e+3*100*1.1e+13*0.4=4.5e+17",,QM9 data set (source: https://arxiv.org/abs/1812.05055),130462.0,,28.0,,NVIDIA GeForce GTX 1080 Ti,,Speculative,"Graph networks are a new machine learning (ML) paradigm that supports both relational reasoning and combinatorial generalization. Here, we develop universal MatErials Graph Network (MEGNet) models for accurate property prediction in both molecules and crystals. We demonstrate that the MEGNet models outperform prior ML models such as the SchNet in 11 out of 13 properties of the QM9 molecule data set. Similarly, we show that MEGNet models trained on âˆ¼60,000 crystals in the Materials Project substantially outperform prior ML models in the prediction of the formation energies, band gaps and elastic moduli of crystals, achieving better than DFT accuracy over a much larger data set. We present two new strategies to address data limitations common in materials science and chemistry. First, we demonstrate a physically-intuitive approach to unify four separate molecular MEGNet models for the internal energy at 0 K and room temperature, enthalpy and Gibbs free energy into a single free energy MEGNet model by incorporating the temperature, pressure and entropy as global state inputs. Second, we show that the learned element embeddings in MEGNet models encode periodic chemical trends and can be transfer-learned from a property model trained on a larger data set (formation energies) to improve property models with smaller amounts of data (band gaps and elastic moduli).",1000.0,,,United States of America,,,,1.0,,2025-05-01 10:42,,,,,,Academia,,,,,,,Academia,,,,282.85734112857654,,,,,
MEGNet (crystal formation energy model),Materials science,Molecular property prediction,University of California San Diego,"Chi Chen, Weike Ye, Yunxing Zuo, Chen Zheng, Shyue Ping Ong",2019-04-10,Graph Networks as a Universal Machine Learning Framework for Molecules and Crystals,https://arxiv.org/abs/1812.05055,1297.0,Highly cited,Source: https://pubs.acs.org/doi/10.1021/acs.chemmater.9b01294,26128.0,Calculations here: https://docs.google.com/document/d/1BTmyZ9KVTIwkp9z9tRRFsWM5A0QcXiufXrIhujffyso/edit?tab=t.0#heading=h.913mrln2g0cv ,4.536e+17,"Calculations here: https://docs.google.com/document/d/1BTmyZ9KVTIwkp9z9tRRFsWM5A0QcXiufXrIhujffyso/edit?tab=t.0#heading=h.obxgi47b0fxe

Updated calculation assuming 100s per epoch and a single GPU.
1000*100*11340000000000*0.4=453600000000000000",Materials Project,,69239.0,,28.0,,NVIDIA GeForce GTX 1080 Ti,,Likely,"Graph networks are a new machine learning (ML) paradigm that supports both relational reasoning and combinatorial generalization. Here, we develop universal MatErials Graph Network (MEGNet) models for accurate property prediction in both molecules and crystals. We demonstrate that the MEGNet models outperform prior ML models such as the SchNet in 11 out of 13 properties of the QM9 molecule data set. Similarly, we show that MEGNet models trained on âˆ¼60,000 crystals in the Materials Project substantially outperform prior ML models in the prediction of the formation energies, band gaps and elastic moduli of crystals, achieving better than DFT accuracy over a much larger data set. We present two new strategies to address data limitations common in materials science and chemistry. First, we demonstrate a physically-intuitive approach to unify four separate molecular MEGNet models for the internal energy at 0 K and room temperature, enthalpy and Gibbs free energy into a single free energy MEGNet model by incorporating the temperature, pressure and entropy as global state inputs. Second, we show that the learned element embeddings in MEGNet models encode periodic chemical trends and can be transfer-learned from a property model trained on a larger data set (formation energies) to improve property models with smaller amounts of data (band gaps and elastic moduli).",1000.0,,,United States of America,,,,,,2025-05-28 16:15,,,,,,Academia,,,,,,,Academia,,,FP32,,,,,,
MEGNet (crystal band gap model),Materials science,Molecular property prediction,University of California San Diego,"Chi Chen, Weike Ye, Yunxing Zuo, Chen Zheng, Shyue Ping Ong",2019-04-10,Graph Networks as a Universal Machine Learning Framework for Molecules and Crystals,https://arxiv.org/abs/1812.05055,1297.0,Highly cited,Source: https://pubs.acs.org/doi/10.1021/acs.chemmater.9b01294,26128.0,"Calculations here:
https://docs.google.com/document/d/1BTmyZ9KVTIwkp9z9tRRFsWM5A0QcXiufXrIhujffyso/edit?tab=t.0#heading=h.913mrln2g0cv",4.536e+17,"Calculations here:
https://docs.google.com/document/d/1BTmyZ9KVTIwkp9z9tRRFsWM5A0QcXiufXrIhujffyso/edit?tab=t.0#heading=h.lk6l1te7vrkv


Updated calculation assuming 100s per epoch and a single GPU.
1000*100*11340000000000*0.4=453600000000000000",Materials Project,,45901.0,,28.0,,NVIDIA GeForce GTX 1080 Ti,,Likely,"Graph networks are a new machine learning (ML) paradigm that supports both relational reasoning and combinatorial generalization. Here, we develop universal MatErials Graph Network (MEGNet) models for accurate property prediction in both molecules and crystals. We demonstrate that the MEGNet models outperform prior ML models such as the SchNet in 11 out of 13 properties of the QM9 molecule data set. Similarly, we show that MEGNet models trained on âˆ¼60,000 crystals in the Materials Project substantially outperform prior ML models in the prediction of the formation energies, band gaps and elastic moduli of crystals, achieving better than DFT accuracy over a much larger data set. We present two new strategies to address data limitations common in materials science and chemistry. First, we demonstrate a physically-intuitive approach to unify four separate molecular MEGNet models for the internal energy at 0 K and room temperature, enthalpy and Gibbs free energy into a single free energy MEGNet model by incorporating the temperature, pressure and entropy as global state inputs. Second, we show that the learned element embeddings in MEGNet models encode periodic chemical trends and can be transfer-learned from a property model trained on a larger data set (formation energies) to improve property models with smaller amounts of data (band gaps and elastic moduli).",1000.0,,,United States of America,,,,1.0,,2025-05-01 10:42,,,,,,Academia,,,,,,,Academia,,,,282.85734112857654,,,,,
MEGNet (crystal elasticity model),Materials science,Molecular property prediction,University of California San Diego,"Chi Chen, Weike Ye, Yunxing Zuo, Chen Zheng, Shyue Ping Ong",2019-04-10,Graph Networks as a Universal Machine Learning Framework for Molecules and Crystals,https://arxiv.org/abs/1812.05055,1297.0,Highly cited,Source: https://pubs.acs.org/doi/10.1021/acs.chemmater.9b01294,26128.0,"Calculations here:
https://docs.google.com/document/d/1BTmyZ9KVTIwkp9z9tRRFsWM5A0QcXiufXrIhujffyso/edit?tab=t.0#heading=h.913mrln2g0cv
",4.536e+17,"Calculations here:
https://docs.google.com/document/d/1BTmyZ9KVTIwkp9z9tRRFsWM5A0QcXiufXrIhujffyso/edit?tab=t.0#heading=h.3ria9ya1ty1o


Updated calculation assuming 100s per epoch and a single GPU.
1000*100*11340000000000*0.4=453600000000000000
",Materials Project,,,,28.0,,NVIDIA GeForce GTX 1080 Ti,,Likely,"Graph networks are a new machine learning (ML) paradigm that supports both relational reasoning and combinatorial generalization. Here, we develop universal MatErials Graph Network (MEGNet) models for accurate property prediction in both molecules and crystals. We demonstrate that the MEGNet models outperform prior ML models such as the SchNet in 11 out of 13 properties of the QM9 molecule data set. Similarly, we show that MEGNet models trained on âˆ¼60,000 crystals in the Materials Project substantially outperform prior ML models in the prediction of the formation energies, band gaps and elastic moduli of crystals, achieving better than DFT accuracy over a much larger data set. We present two new strategies to address data limitations common in materials science and chemistry. First, we demonstrate a physically-intuitive approach to unify four separate molecular MEGNet models for the internal energy at 0 K and room temperature, enthalpy and Gibbs free energy into a single free energy MEGNet model by incorporating the temperature, pressure and entropy as global state inputs. Second, we show that the learned element embeddings in MEGNet models encode periodic chemical trends and can be transfer-learned from a property model trained on a larger data set (formation energies) to improve property models with smaller amounts of data (band gaps and elastic moduli).",1000.0,,,United States of America,,,,,,2025-05-28 16:15,,,,,,Academia,,,,,,,Academia,,,FP32,,,,,,
WeNet (Penn Treebank),Language,"Neural Architecture Search - NAS,Language modeling",Amazon,"Zhiheng Huang, Bing Xiang",2019-04-08,WeNet: Weighted Networks for Recurrent Network Architecture Search,https://arxiv.org/abs/1904.03819,5.0,SOTA improvement,"""We show that an architecture found by WeNets arXiv:1904.03819v1 [cs.NE] 8 Apr 2019 WeNet: Weighted Networks for Recurrent Network Architecture Search achieves state-of-the-art results on the Penn Treebank language dataset""",23000000.0,Table 1,7.30000001e+17,"PTB has 912344 tokens. The model has 23M parameters and was trained for 6k epochs. If the model was dense, 6 FLOP/token/param/epoch * 6k epochs * 23M params * 912k tokens = 1.05e18 FLOP.

Alternatively, the model was trained on 1 V100 GPU and ""In terms of efficiency, the overall cost... is within 1 GPU day"" so the training time was around or below 24 hours. Half precision and 30% utilization would be a pretty good match for the arithmetic estimate: 24 hours * 30% * 28 TFLOPS = 7.3e17 FLOP.",Penn TreeBank (PTB),,,,24.0,,NVIDIA V100,,Likely,,6000.0,WeNet (WT2),Unreleased,United States of America,,,,1.0,,2025-02-17 12:32,,,,70000.0,"They use BPTT with length 35. During architecture search data batch size is 20 and network batch size is 100. While training the architecture they end up finding, batch size is 64. So effective batch size is 35 * 20 * 100 = 70,000 during architecture search, and 35 * 64 = 2,240 during final training. ",Industry,,,,,Unreleased,PTB dataset,Industry,,,FP16,339.4439274233134,"Hardware,Operation counting",,,,
Cross-lingual alignment,Language,Translation,"Tel Aviv University,Massachusetts Institute of Technology (MIT)","Tal Schuster, Ori Ram, Regina Barzilay, and Amir Globerson.",2019-04-04,"Cross-lingual alignment of contextual word embeddings, with applications to zero- shot dependency parsing.",https://arxiv.org/abs/1902.09492,201.0,SOTA improvement,"""our method consistently outperforms the previous state-of-the-art on 6 tested languages""",,,2.56e+18,"From author communication:

Precision: float32

Hardware: 4 GPU  NVIDIA 1080Ti

NVIDIA 1080Ti: 1.06E+13

Compute: 7 GPU-days

0.4 * 1.06E+13 FLOP/s * 7 days * 24h/day * 3600s/h
= 2.56E+18","Wikipedia,CoNLL2017",,,,,,NVIDIA GeForce GTX 1080 Ti,,Speculative,,,,Open weights (unrestricted),"Israel,United States of America",ELMo,,,,,2025-06-18 09:28,,,,,,"Academia,Academia",,,,,Open source,"MIT license
https://github.com/TalSchuster/CrossLingualContextualEmb","Academia,Academia",,,,,Hardware,,,,
FAIRSEQ Adaptive Inputs,Language,Language modeling/generation,"Facebook AI Research,Google Brain","Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, Michael Auli",2019-04-01,"fairseq: A Fast, Extensible Toolkit for Sequence Modeling",https://arxiv.org/abs/1904.01038,2991.0,Highly cited,,247000000.0,"""The first model has 16 blocks, inner dimension 4K and embedding dimension 1K""

247M as in Baevski and Auli (2019) Transformer",3.1804274e+19,"6 FLOP / parameter / token * 247000000 parameters * 103000000 tokens * 180 epochs = 2.747628e+19 FLOP 

for translation model, Table 2:

31330000000000 FLOP / second / GPU * 128 GPUs * 8.5 hours * 3600 seconds / hour * 0.3 [assumed precision] = 3.6814003e+19 FLOP

sqrt(2.747628e+19*3.6814003e+19) = 3.1804274e+19 FLOP

Speculative confidence since amount of parameters and epochs are assumed as well as hardware estimation is given for another model in the paper

__________
In the Algorithmic progress paper the estimation was 7.30E+18 FLOP",WikiText-103,,103000000.0,assuming same number of epochs as in Baevski and Auli (2019) Transformer - 180,,,NVIDIA V100,,Speculative,"fairseq is an open-source sequence modeling toolkit that allows researchers and developers to train custom models for translation, summarization, language modeling, and other text generation tasks. The toolkit is based on PyTorch and supports distributed training across multiple GPUs and machines. We also support fast mixed-precision training and inference on modern GPUs. A demo video can be found at this https URL",180.0,FAIRSEQ Adaptive Inputs,Unreleased,"United States of America,United States of America",,,,,,2025-04-14 14:48,,,,,,"Industry,Industry",,,,,Open source,"weights and training, Repo is MIT-licensed
https://github.com/facebookresearch/fairseq/blob/main/examples/language_model/README.adaptive_inputs.md ","Industry,Industry",,,,,"Operation counting,Hardware",,,,
UniRep,Biology,"Proteins,Protein or nucleotide language model (pLM/nLM)",Harvard University,"Ethan C. Alley, Grigory Khimulya, Surojit Biswas, Mohammed AlQuraishi & George M. Church  ",2019-03-26,Unified rational protein engineering with sequence-based deep representation learning,https://www.nature.com/articles/s41592-019-0598-1,812.0,,,18200000.0,"""1,900-dimensional single-layer multiplicative LSTM (~18.2 million parameters)""",2.2e+19,"""Training was performed using data parallelism on four Nvidia K80 GPUs (mLSTM-1,900) or two Nvidia K-40s (4Ã— mLSTM-256, 4Ã— mLSTM-64). The mLSTM-1,900 model was trained for ~770,000 weight updates, or ~3.5â€‰weeks wall clock time, corresponding to ~1 epoch."" [Methods - Unsupervised training dataset]

Assuming 30% utilization rate and single-precision performance

Estimate: 3.5 weeks * 7 days/week * 24 hours/day * 60 min/hour * 60 sec/min * 4 GPUs *8.73e12 FLOP/sec * 0.3",UniRef50,"""we use a recurrent neural network (RNN) to learn statistical representations of proteins from ~24 million UniRef50 (ref. 22) sequences""",,~24M protein sequences,588.0,,NVIDIA Tesla K80,Self-supervised learning,Likely,"Rational protein engineering requires a holistic understanding of protein function. Here, we apply deep learning to unlabeled amino-acid sequences to distill the fundamental features of a protein into a statistical representation that is semantically rich and structurally, evolutionarily and biophysically grounded. We show that the simplest models built on top of this unified representation (UniRep) are broadly applicable and generalize to unseen regions of sequence space. Our data-driven approach predicts the stability of natural and de novo designed proteins, and the quantitative function of molecularly diverse mutants, competitively with the state-of-the-art methods. UniRep further enables two orders of magnitude efficiency improvement in a protein engineering task. UniRep is a versatile summary of fundamental protein features that can be applied across protein engineering informatics.",1.0,,Open weights (non-commercial),United States of America,,,,4.0,,2025-06-11 18:01,,,,,,Academia,,,,2352.0,Open source,"creative commons non-commercial for weights, GNU General Public License for code. data is UniRef50, which has a commercial license
https://github.com/churchlab/UniRep",Academia,,,,2471.867298164383,Hardware,,,,
SciBERT,Language,"Relation extraction,Sentiment classification,Text classification",Allen Institute for AI,"Iz Beltagy, Kyle Lo, Arman Cohan",2019-03-26,SciBERT: A Pretrained Language Model for Scientific Text,https://arxiv.org/abs/1903.10676,2808.0,"Highly cited,SOTA improvement","""We demon-
strate statistically significant improvements
over BERT and achieve new state-of-the-
art results on several of these tasks""",110000000.0,"110M
size of bert base from https://huggingface.co/google-bert/bert-base-uncased
relevant citation: 
""We use the original BERT code to
train SCIBERT on our corpus with the same con-
figuration and size as BERT-Base. We train 4
different versions of SCIBERT: (i) cased or un-
cased and (ii) BASEVOCAB or SCIVOCAB. The
two models that use BASEVOCAB are finetuned
from the corresponding BERT-Base models. The
other two models that use the new SCIVOCAB are
trained from scratch.""",8.926848e+19,"4*123e12*0.3*(7*24*3600) = 8.926848e+19
(num gpu) * (peak compute) * (assumed utilization rate) * (time in seconds)
We have:
 4 TPUv3 chips.123teraFLOPS per chip.
7 days of training
""We use a single TPU v3 with 8 cores. Training the SCIVOCAB models from scratch on our corpus takes 1 week (5 days with max length 128, then 2 days with max length 512). ""

If this compute estimate is accurate and BERT is approximately dense, then C=6eND -> e=C/6ND ~= 40 epochs.",,"""We train SCIBERT on a random
sample of 1.14M papers from Semantic
Scholar (Ammar et al., 2018). """,3300000000.0,"""The average paper length is 154 sentences (2,769 tokens) resulting in a corpus size of 3.17B tokens, similar to the 3.3B tokens on which BERT was trained.""",168.0,1 week,Google TPU v3,,Confident,"Obtaining large-scale annotated data for NLP tasks in the scientific domain is challenging and expensive. We release SciBERT, a pretrained language model based on BERT (Devlin et al., 2018) to address the lack of high-quality, large-scale labeled scientific data. SciBERT leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve performance on downstream scientific NLP tasks. We evaluate on a suite of tasks including sequence tagging, sentence classification and dependency parsing, with datasets from a variety of scientific domains. We demonstrate statistically significant improvements over BERT and achieve new state-of-the-art results on several of these tasks. The code and pretrained models are available at this https://github.com/allenai/scibert/",,,Open weights (unrestricted),United States of America,,,,4.0,,2025-01-03 15:53,,,,,,Research collective,,,,672.0,Open source,"apache 2.0, code and weights: https://github.com/allenai/scibert/",Research collective,$247.26,,,1812.702685320548,Hardware,,,,
NMT Transformer 437M,Language,Translation,"Google,Bar-Ilan University","Roee Aharoni, Melvin Johnson, Orhan Firat",2019-02-28,Massively Multilingual Neural Machine Translation,https://arxiv.org/abs/1903.00089,520.0,SOTA improvement,"""We report results on the publicly available TED talks multilingual corpus where we show that massively multilingual many-to-many models are effective in low resource settings, outperforming the previous state-of-the-art while supporting up to 59 languages.""",437700000.0,"""Regarding the model, for these experiments we
use a larger Transformer model with 6 layers in
both the encoder and the decoder, model dimension set to 1024, hidden dimension size of 8192,
and 16 attention heads. This results in a model
with approximately 473.7M parameters.""",,,,"""Since we are not aware of a publicly available resource for this purpose, we construct an in-house
dataset. This dataset includes 102 language pairs
which we â€œmirrorâ€ to-and-from English, with up
to one million examples per language pair. This
results in 103 languages in total, and 204 translation directions which we train simultaneously.""

96M total examples, per Table 4",,"96M total examples, per Table 4. One sentence per example?",,,,,Confident,"Multilingual neural machine translation (NMT) enables training a single model that supports translation from multiple source languages into multiple target languages. In this paper, we push the limits of multilingual NMT in terms of number of languages being used. We perform extensive experiments in training massively multilingual NMT models, translating up to 102 languages to and from English within a single model. We explore different setups for training such models and analyze the trade-offs between translation quality and various modeling decisions. We report results on the publicly available TED talks multilingual corpus where we show that massively multilingual many-to-many models are effective in low resource settings, outperforming the previous state-of-the-art while supporting up to 59 languages. Our experiments on a large-scale dataset with 102 languages to and from English and up to one million examples per direction also show promising results, surpassing strong bilingual baselines and encouraging future work on massively multilingual NMT.",,,Unreleased,"United States of America,Israel",,,,,,2024-09-05 14:08,,,,,,"Industry,Academia",,,,,Unreleased,,"Industry,Academia",,,,,,,,,
KataGo,Games,Go,Jane Street,David J. Wu,2019-02-27,Accelerating Self-Play Learning in Go,https://arxiv.org/abs/1902.10565,85.0,SOTA improvement,Better than ELF OpenGo while using 1/50th the compute.,2500000.0,https://arxiv.org/abs/2210.00849 gives parameter count for AlphaZero in Fig 1b.,2.32e+19,"""[KataGo] surpasses the strength of ELF OpenGo after training on about 27 V100 GPUs for 19 days""
14.13 teraFLOP/s * 19 days = 2.32e+19 FLOP",,"Self-play: ""In total, KataGoâ€™s main run lasted for 19 days using a maximum of 28 V100 GPUs at any time (averaging 26-27) and generated about 241 million training samples across 4.2 million games.""",241000000.0,241 million training samples across 4.2 million games,456.0,27 processors for 19 days,NVIDIA Tesla V100 DGXS 16 GB,Self-supervised learning,Speculative,"By introducing several improvements to the AlphaZero process and architecture, we greatly accelerate self-play learning in Go, achieving a 50x reduction in computation over comparable methods. Like AlphaZero and replications such as ELF OpenGo and Leela Zero, our bot KataGo only learns from neural-net-guided Monte Carlo tree search self-play. But whereas AlphaZero required thousands of TPUs over several days and ELF required thousands of GPUs over two weeks, KataGo surpasses ELF's final model after only 19 days on fewer than 30 GPUs. Much of the speedup involves non-domain-specific improvements that might directly transfer to other problems. Further gains from domain-specific techniques reveal the remaining efficiency gap between the best methods and purely general methods such as AlphaZero. Our work is a step towards making learning in state spaces as large as Go possible without large-scale computational resources.",,,Open weights (unrestricted),"Multinational,United States of America",,,,,,2025-05-28 16:15,,,,,,Industry,,,,,Open source,"permissive license https://github.com/lightvector/KataGo/blob/master/LICENSE

training here: https://github.com/lightvector/KataGo/blob/master/SelfplayTraining.md ",Industry,$104.91,,FP16,,Hardware,,,,
ProxylessNAS,Vision,"Image classification,Neural Architecture Search - NAS",Massachusetts Institute of Technology (MIT),"Han Cai, Ligeng Zhu, and Song Han",2019-02-23,ProxylessNAS: Direct neural architecture search on target task and hardware,https://arxiv.org/abs/1812.00332,1806.0,Highly cited,,,,3.723192e+18,"For their searched Imagenet models, they used 200 GPU hours on a V100 GPU.

At FP32, a V100 GPU has a peak performance of 1.56E+13 FLOPS.

Utilization rate of 0.33.

200 h * 3600 second / hour* 1.6e+13 flop /second * 0.33 = 3.7e+18 flop",ImageNet,,1280000.0,,,,NVIDIA V100,,Confident,"Neural architecture search (NAS) has a great impact by automatically designing effective neural network architectures. However, the prohibitive computational demand of conventional NAS algorithms (e.g. 104 GPU hours) makes it difficult to \emph{directly} search the architectures on large-scale tasks (e.g. ImageNet). Differentiable NAS can reduce the cost of GPU hours via a continuous representation of network architecture but suffers from the high GPU memory consumption issue (grow linearly w.r.t. candidate set size). As a result, they need to utilize~\emph{proxy} tasks, such as training on a smaller dataset, or learning with only a few blocks, or training just for a few epochs. These architectures optimized on proxy tasks are not guaranteed to be optimal on the target task. In this paper, we present \emph{ProxylessNAS} that can \emph{directly} learn the architectures for large-scale target tasks and target hardware platforms. We address the high memory consumption issue of differentiable NAS and reduce the computational cost (GPU hours and GPU memory) to the same level of regular training while still allowing a large candidate set. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of directness and specialization. On CIFAR-10, our model achieves 2.08\% test error with only 5.7M parameters, better than the previous state-of-the-art architecture AmoebaNet-B, while using 6Ã— fewer parameters. On ImageNet, our model achieves 3.1\% better top-1 accuracy than MobileNetV2, while being 1.2Ã— faster with measured GPU latency. We also apply ProxylessNAS to specialize neural architectures for hardware with direct hardware metrics (e.g. latency) and provide insights for efficient CNN architecture design.",,,Open weights (unrestricted),United States of America,,,,,,2025-06-18 09:26,,,,,,Academia,,,,200.0,Open source,MIT for code+weights https://github.com/MIT-HAN-LAB/ProxylessNAS,Academia,$122.74,,FP32,,Hardware,,,,
SSA,Biology,Protein embedding,Massachusetts Institute of Technology (MIT),"Tristan Bepler, Bonnie Berger",2019-02-22,Learning protein sequence embeddings using information from structure,https://arxiv.org/abs/1902.08661,338.0,,,,,1.296e+19,"""All models were implemented in PyTorch and trained on a single NVIDIA Tesla V100 GPU. Each model took roughly 3 days to train and required 16 GB of GPU RAM""
1.3e+14*0.4*259200s=1.3e+19",,,,"
",72.0,"""Each model took roughly 3 days to train and required 16 GB of GPU RAM""",NVIDIA V100,,Confident,"Inferring the structural properties of a protein from its amino acid sequence is a challenging yet important problem in biology. Structures are not known for the vast majority of protein sequences, but structure is critical for understanding function. Existing approaches for detecting structural similarity between proteins from sequence are unable to recognize and exploit structural patterns when sequences have diverged too far, limiting our ability to transfer knowledge between structurally related proteins. We newly approach this problem through the lens of representation learning. We introduce a framework that maps any protein sequence to a sequence of vector embeddings --- one per amino acid position --- that encode structural information. We train bidirectional long short-term memory (LSTM) models on protein sequences with a two-part feedback mechanism that incorporates information from (i) global structural similarity between proteins and (ii) pairwise residue contact maps for individual proteins. To enable learning from structural similarity information, we define a novel similarity measure between arbitrary-length sequences of vector embeddings based on a soft symmetric alignment (SSA) between them. Our method is able to learn useful position-specific embeddings despite lacking direct observations of position-level correspondence between sequences. We show empirically that our multi-task framework outperforms other sequence-based methods and even a top-performing structure-based alignment method when predicting structural similarity, our goal. Finally, we demonstrate that our learned embeddings can be transferred to other protein sequence problems, improving the state-of-the-art in transmembrane domain prediction.",,,,United States of America,,,,1.0,,2025-05-01 10:42,,,,,,Academia,,,,,,,Academia,,,,339.7842620513041,Hardware,,,,
code2seq,Language,Language modeling,"Technion - Israel Institute of Technology,Facebook AI Research","Uri Alon, Shaked Brody, Omer Levy, Eran Yahav",2019-02-21,code2seq: Generating Sequences from Structured Representations of Code,https://arxiv.org/abs/1808.01400,,,,37000000.0,while our code2seq model had only 37M,1.1513908e+19,6*37*10^6*997393280*52 = 1.1513908e+19,,,997393280.0,"Java-large â€“ A new dataset of the 9500 top-starred Java projects from GitHub that were created since January 2007. We randomly select 9000 projects for training, 250 for validation and 300 for testing. This dataset contains about 16M examples and we make it publicly available.

Table 5: average code length is 65 tokens
size of Java Large is 15,344,512 examples

15,344,512 * 65 = 997393280 tokens
",,,,,Confident,"The ability to generate natural language sequences from source code snippets has a variety of applications such as code summarization, documentation, and retrieval. Sequence-to-sequence (seq2seq) models, adopted from neural machine translation (NMT), have achieved state-of-the-art performance on these tasks by treating source code as a sequence of tokens. We present CODE2SEQ: an alternative approach that leverages the syntactic structure of programming languages to better encode source code. Our model represents a code snippet as the set of compositional paths in its abstract syntax tree (AST) and uses attention to select the relevant paths while decoding. We demonstrate the effectiveness of our approach for two tasks, two programming languages, and four datasets of up to 16M examples. Our model significantly outperforms previous models that were specifically designed for programming languages, as well as state-of-the-art NMT models. An interactive online demo of our model is available at this http URL. Our code, data and trained models are available at this http URL.",52.0,,Open weights (unrestricted),"Israel,United States of America",,,,,,2025-05-01 10:42,,,,,,"Academia,Industry",,,,,Open source,"Our code, data and trained models are available at
http://github.com/tech-srl/code2seq

MIT License","Academia,Industry",,,,,Operation counting,,,,
GPT-2 (1.5B),Language,Language modeling/generation,OpenAI,"Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever",2019-02-14,Language Models are Unsupervised Multitask Learners,https://openai.com/blog/better-language-models/,19228.0,Highly cited,,1500000000.0,"""GPT-2 is a large transformer-based language model with 1.5 billion parameters""",1.920000000001e+21,"Estimating based on compute = 6 FLOP/token/param * epochs * parameters * tokens.

40GB dataset is approximately 8B words, or 1/0.75 * 8B = 10.66B tokens.

The number of epochs is not reported, but another paper [1] claims in table 1 that it is 20 or 100 epochs, and another paper [2] claims 12 epochs based on communication with the GPT-2 authors (page 4).

12 epochs is the modal, most credible value. Mean of probability mass is probably around 20 epochs, so calculating from that value:

6 * (40 * 200 million * 1/0.75 * 20) * 1.5 billion parameters = 1.92e21
https://www.wolframalpha.com/input?i=6+FLOP+*+20+*+%2840+billion+%2F+5+*+%284%2F3%29%29+*+1.5+billion

[1] https://arxiv.org/abs/1906.06669 One Epoch Is All You Need
[2] https://www.usenix.org/system/files/sec21-carlini-extracting.pdf Extracting Data From Large Language Models

It also appears the model was trained on TPU v3 chips:
https://huggingface.co/openai-community/gpt2",WebText,"â€œwe created a new web scrape which emphasizes document quality. To do this we only scraped web pages which have been curated/filtered by humans. Manually filtering a full web scrape would be exceptionally expensive so as a starting point, we scraped all outbound links from Reddit, a social media platform, which received at least 3 karma. This can be thought of as a heuristic indicator for whether other users found the link interesting, educational, or just funny.
The resulting dataset, WebText, contains the text subset of these 45 million links. To extract the text from HTML responses we use a combination of the Dragnet (Peters &
Lecocq, 2013) and Newspaper1 content extractors. All results presented in this paper use a preliminary version of WebText which does not include links created after Dec 2017â€ (https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf, page 3)",10666666667.0,"â€œAll results presented in this paper use a preliminary version of WebText which does not include links created after Dec 2017 and which after de-duplication and some heuristic based cleaning contains slightly over 8 million documents for a total of 40 GB of text.â€
40GB is approximately 8e9 words.
",,,Google TPU v3,Self-supervised learning,Speculative,"Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.",20.0,GPT-2 (1542M),Open weights (unrestricted),United States of America,,,,,,2025-05-09 11:32,,,,,,Industry,,,,,Unreleased,"modified MIT
https://github.com/openai/gpt-2?tab=License-1-ov-file#readme",Industry,,,,,Operation counting,,,,7
Compress-LSTM (66M),Language,Language modeling,"Samsung R&D Institute Russia,National Research University Higher School of Economics","Artem M. Grachev, Dmitry I. Ignatov, Andrey V. Savchenko",2019-02-06,Compression of Recurrent Neural Networks for Efficient Language Modeling,https://arxiv.org/abs/1902.02380,38.0,,,66000000.0,,3.31e+16,6 FLOP / parameter / token * 66000000 parameters * 929000 tokens * 90 epochs = 3.310956e+16 FLOP,Penn TreeBank (PTB),,929000.0,,,,,,Confident,"Recurrent neural networks have proved to be an effective method for statistical language modeling. However, in practice their memory and run-time complexity are usually too large to be implemented in real-time offline mobile applications. In this paper we consider several compression techniques for recurrent neural networks including Long-Short Term Memory models. We make particular attention to the high-dimensional output problem caused by the very large vocabulary size. We focus on effective compression methods in the context of their exploitation on devices: pruning, quantization, and matrix decomposition approaches (low-rank factorization and tensor train decomposition, in particular). For each model we investigate the trade-off between its size, suitability for fast inference and perplexity. We propose a general pipeline for applying the most suitable methods to compress recurrent neural networks for language modeling. It has been shown in the experimental study with the Penn Treebank (PTB) dataset that the most efficient results in terms of speed and compression-perplexity balance are obtained by matrix decomposition techniques.",90.0,Compress-LSTM (66M),Unreleased,"Russia,Russia",,,,,,2025-04-14 14:58,,,,,,"Industry,Academia",,,,,Unreleased,,"Industry,Academia",,,,,Operation counting,,,,
Hanabi 4 player,Games,Hanabi,"DeepMind,University of Oxford,Carnegie Mellon University (CMU),Google Brain",,2019-02-01,The Hanabi Challenge: A New Frontier for AI Research,https://arxiv.org/abs/1902.00506,229.0,Historical significance,Adapted some SOTA RL algorithms to a new task that posed research challenges,764000.0,source: https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389,4.3e+18,14.13e+12 FLOP/s * 7 days * 86400 s/day * 0.50 utilization = 4.3e+18 FLOP,,,,,,,NVIDIA V100,,Confident,,,,Unreleased,"United Kingdom of Great Britain and Northern Ireland,United Kingdom of Great Britain and Northern Ireland,United States of America,United States of America",,,,,,2025-06-18 06:22,,,,,,"Industry,Academia,Academia,Industry",,,,,Unreleased,,"Industry,Academia,Academia,Industry",,,,,Hardware,,,,
Mono3D++,"3D modeling,Vision","3D segmentation,Object detection","University of California Los Angeles (UCLA),Megvii Inc","Tong He, Stefano Soatto",2019-01-11,Mono3D++: Monocular 3D Vehicle Detection with Two-Scale 3D Hypotheses and Task Priors,https://arxiv.org/abs/1901.03446,124.0,,,,,4.85606016e+18,"(4) * (6.691 * 10**12) * (168* 3600) * (0.3) = 
(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate) = 

training time - about one week = 168 hours from
""It takes about one week to train the 2D bounding box net-work, and two hours for the orientation/3D scale network
on KITTI with 4 TITAN-X GPUs. The landmark detector is
trained on Pascal3D. The training process for the monocu-
lar depth estimation network is unsupervised using KITTI
stereo-pairs, which takes around 5 to 12 hours depending
on the amount of data available. ""

gpu flops - FP32 (float) 6.691 TFLOPS from https://www.techpowerup.com/gpu-specs/geforce-gtx-titan-x.c2632",KITTI,"""t takes about one week to train the 2D bounding box net-
work, and two hours for the orientation/3D scale network
on KITTI with 4 TITAN-X GPUs. The landmark detector is
trained on Pascal3D. The training process for the monocu-
lar depth estimation network is unsupervised using KITTI
stereo-pairs, which takes around 5 to 12 hours depending
on the amount of data available. ""


""We evaluate our method on the KITTI object detection
benchmark. This dataset contains 7, 481 training images """,7481.0,"""We evaluate our method on the KITTI object detection benchmark. This dataset contains 7, 481 training images """,168.0,"""about one week"" from section 3.4 ",NVIDIA GeForce GTX TITAN X,,Confident,"We present a method to infer 3D pose and shape of vehicles from a single image. To tackle this ill-posed problem, we optimize two-scale projection consistency between the generated 3D hypotheses and their 2D pseudo-measurements. Specifically, we use a morphable wireframe model to generate a fine-scaled representation of vehicle shape and pose. To reduce its sensitivity to 2D landmarks, we jointly model the 3D bounding box as a coarse representation which improves robustness. We also integrate three task priors, including unsupervised monocular depth, a ground plane constraint as well as vehicle shape priors, with forward projection errors into an overall energy function. ",,,Unreleased,"United States of America,China",,,,4.0,,2025-01-22 12:17,,,,,,"Academia,Industry",,,,672.0,Unreleased,,"Academia,Industry",,,,2063.2867743998872,Hardware,,,,
Transformer-XL (257M),Language,Language modeling/generation,"Carnegie Mellon University (CMU),Google Brain","Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov",2019-01-09,Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context,https://arxiv.org/abs/1901.02860,3430.0,Highly cited,,257000000.0,"Transformer-XL Large, Table 1",3.7832771e+20,"6 FLOP / token / parameter * 257000000 parameters * 103000000 tokens * 1908 epochs [see dataset size notes] = 3.0304001e+20 FLOP

from training code (https://github.com/kimiyoung/transformer-xl/blob/master/tf/scripts/wt103_large_tpu.sh) they used 64 tpv3 cores 

123000000000000 FLOP/s/chip* (64 cores / 2 cores per chip) * 4000000 steps *  0.1 sec / step [assumption] * 0.3 [assumed utilization] = 4.7232e+20 FLOP

geometric mean
sqrt(3.0304001e+20 * 4.7232e+20) = 3.7832771e+20

speculative confidence given assumptions used
_________
previous estimation in the algorithmic progress paper was 1.09 Ã— 10^19 FLOP without explanation 
",WikiText-103,,103000000.0,"from the training code (https://github.com/kimiyoung/transformer-xl/blob/master/pytorch/run_wt103_large.sh):

--tgt_len 384
--batch_size 128
--max_step 4000000 

384*128*4000000 / 103000000 = 1908 epochs",,,Google TPU v3,,Speculative,"Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.",1908.0,Transformer-XL Large,Open weights (unrestricted),"United States of America,United States of America",,,,32.0,,2025-05-28 16:16,,,,49152.0,"384*128 --tgt_len 384
--batch_size 128","Academia,Industry",,,,,Open source,"Apache 2.0, includes train code
https://github.com/kimiyoung/transformer-xl?tab=Apache-2.0-1-ov-file#readme","Academia,Industry",,,FP32,14526.185855419688,"Operation counting,Hardware",,,,16
Decoupled weight decay regularization,Vision,Image classification,University of Freiburg,Ilya Loshchilov and Frank Hutter,2019-01-04,Decoupled weight decay regularization.,https://arxiv.org/abs/1711.05101,2061.0,Highly cited,,36500000.0,"From author communication

WideResNet 28-10 models with 36.5 million parameters (3.65E+07)",4.716e+17,"From author communication

Per image: 5.24 billion FLOPs (5.24E+09)  Per training run: 50k times 5.24E+09 times 1800 epochs

5240000000*50000*1800=471600000000000000=4.72e17",CIFAR-10,,50000.0,,,,,,Confident,"L2 regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is \emph{not} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L2 regularization (often calling it ""weight decay"" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by \emph{decoupling} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at this https URL",,,Open weights (unrestricted),Germany,,,,,,2025-06-18 06:16,,,,,,Academia,,,,,Open source,"license: https://github.com/loshchil/AdamW-and-SGDW/blob/master/LICENSE

code, including checkpoints: https://github.com/loshchil/AdamW-and-SGDW/blob/master/README.md ",Academia,,,,,Operation counting,,,,
Transformer + Average Attention Network,Language,Language modeling,University of Electronic Science and Technology of China,"Jian Guo Zhang, Jian Ping Li, Huang Li",2019-01-01,Language Modeling with Transformer,https://ieeexplore.ieee.org/abstract/document/9067534,126.0,,,,,,,WikiText-103,,,,,,,,Unknown,"To date, the main method of language modeling is based on recurrent neural networks or convolutional neural networks. We show that two simple models which get inspiration from Transformer [1]. Compare to other attention network, the Transformer which just use self-attention and FFN (feedforward network) are highly efficient in training. We apply the idea which from the mechanism of Transformer to language module. We predict the future elements by the long-term dependence of context words which through the ANN (average attention network) and self-attention mechanism. We test our model on WikiText-103 where our model achieves 22.13 perplexity and on the Google Billion Word benchmark, which we achieve 26.31 perplexity.",,Transformer + Average Attention Network,Unreleased,China,,,,,,2024-09-05 14:08,,,,,,Academia,,,,,Unreleased,,Academia,,,,,,,,,
DMPFold,Biology,"Proteins,Protein folding prediction,Protein contact and distance prediction",University College London (UCL),"Joe G. Greener, Shaun M. Kandathil and David T. Jones",2018-11-29,Deep learning extends de novo protein modelling coverage of genomes using iteratively predicted structural constraints,https://www.nature.com/articles/s41467-019-11994-0,174.0,,,3800000.0,"Based on Fig. 9:

Distance predictor network: [Total = 17684]
(1) Maxout2D: 0 parameters
(2) ResBlock x 18: 2*64*18 = 16384 parameters (a shift and scale parameter for every InstanceNorm2D layer)
(3) Conv2D 1x1: 64*20 + 20 = 1300 parameters
(4) Softmax: 0 parameters

Hydrogen bond predictor network: [Total = 3691073]
(1) Maxout2D: 0 parameters
(2) ResBlock x 18: (2*64*2 + 5*5*64*64*2)*18 = 3 691 008 parameters
(3) Conv2D 1x1: 64*1 + 1 = 65 parameters
(4) Sigmoid: 0 parameters

Torsion angles and errors network [Total = 115459]
(1) Maxout2D: 0 parameters
(2) ResBlock x 18: 2*64*18 = 16384 parameters (a shift and scale parameter for every InstanceNorm2D layer)
(3) BLSTM: 4Ã—2Ã—(N+M)Ã—M where M is 128 (hidden units in BLSTM layer) and N is 64 (input dimensionality) = 98304
(4) Conv1D 1x1: 256*3 + 3 = 771 parameters

Estimate total parameters = 3.8e6 parameters

[See Section 'Additional constraint types and iterative predictions': ""a bidirectional recurrent LSTM layer with 128 hidden units (BLSTM in Fig. 9c), which embeds each row of the final 2-D 64-channel feature map in a single 256-D vector (concatenation of 128-D final timestep output states of the forward and reverse direction LSTM passes)""]",,"""Training of all models was performed using the Adam optimiser for 75
epochs with default parameters""

""The training set here was based on the same 6729 protein chains, â‰¤500 residues in length, with non-redundancy at the 25% sequence identity level and no unresolved main chain atoms""

Estimate = 2 * 3.8e6 * 3 * 6729 * 75 * 486 ~ 5.59e15 FLOP

This seems likely to be too low.",PSICOV150,"First published in Jones et al. 2012: https://academic.oup.com/bioinformatics/article/28/2/184/198108

""All other aspects of training, including data augmentation procedures were out as
previously described for DMP. The training set here was based on the same 6729 protein chains, â‰¤500 residues in length""

In DMP publication: (https://academic.oup.com/bioinformatics/article/34/19/3308/4987145?login=false)
""We assessed the mean precision achieved by DeepCov on the now standard PSICOV150 set of proteins and alignments, described in Jones et al. (2012).""",3270294.0,"""The training set here was based on the same 6729 protein chains""
Each chain is <= 500 residues long, per this paper, average animal protein is 486 amino acids long.
6729 * 486 = 3,270,294 residues total",,,,,Likely,"The inapplicability of amino acid covariation methods to small protein families has limited their use for structural annotation of whole genomes. Recently, deep learning has shown promise in allowing accurate residue-residue contact prediction even for shallow sequence alignments. Here we introduce DMPfold, which uses deep learning to predict inter-atomic distance bounds, the main chain hydrogen bond network, and torsion angles, which it uses to build models in an iterative fashion. DMPfold produces more accurate models than two popular methods for a test set of CASP12 domains, and works just as well for transmembrane proteins. Applied to all Pfam domains without known structures, confident models for 25% of these so-called dark families were produced in under a week on a small 200 core cluster. DMPfold provides models for 16% of human proteome UniProt entries without structures, generates accurate models with fewer than 100 sequences in some cases, and is freely available.",75.0,,Open weights (unrestricted),United Kingdom of Great Britain and Northern Ireland,,,,,,2025-04-30 10:04,,,,,,Academia,,,,,Open source,license: https://github.com/psipred/DMPfold?tab=GPL-3.0-1-ov-file#readme,Academia,,,,,,,,,
Multi-cell LSTM,Language,Language modeling,University of Hyderabad,"Thomas Cherian, Akshay Badola, Vineet Padmanabhan",2018-11-15,Multi-cell LSTM Based Neural Language Model,https://arxiv.org/abs/1811.06477,6.0,SOTA improvement,"""The proposed multi-cell LSTM language models outperform the state-of-the-art results on well-known Penn Treebank (PTB) setup""",7200000.0,"Based on the details in the paper, the number of parameters in the Multi-cell LSTM model can be calculated as follows:

The model has 2 hidden LSTM layers, each with 1500 hidden units

Each LSTM unit is a multi-cell LSTM with 10 memory cells per unit

For a standard LSTM layer with n hidden units:

W matrix: n x input_size
U matrix: n x n
4 bias vectors of size n (for input, forget, cell, output gates)
So for each multi-cell LSTM layer with 1500 units and 10 cells per unit:

W matrix: 1500 x input_size
U matrix: 1500 x 1500
4 bias vectors of size 1500
Number of parameters is same as standard LSTM layer

For the 2 hidden layers:

Input size for Layer 1: embedding dimension (estimated 300 in paper)

Input size for Layer 2: 1500 (output of layer 1)

Total params =
Layer 1: 1500 x (300 + 1500 + 4) = 2,706,000
Layer 2: 1500 x (1500 + 1500 + 4) = 4,506,000

Total Parameters = 2,706,000 + 4,506,000 = 7,212,000

So the total number of parameters for the Multi-cell LSTM model with 2 layers of 1500 units and 10 cells per unit is approximately 7.2 million.",2006640000000000.0,6 FLOP / parameter / token * 7200000 parameters * 929000 tokens * 50 epochs = 2.00664e+15 FLOP,Penn TreeBank (PTB),,929000.0,50 epochs (from Figure 4),,,,,Likely,"Language models, being at the heart of many NLP problems, are always of great interest to researchers. Neural language models come with the advantage of distributed representations and long range contexts. With its particular dynamics that allow the cycling of information within the network, `Recurrent neural network' (RNN) becomes an ideal paradigm for neural language modeling. Long Short-Term Memory (LSTM) architecture solves the inadequacies of the standard RNN in modeling long-range contexts. In spite of a plethora of RNN variants, possibility to add multiple memory cells in LSTM nodes was seldom explored. Here we propose a multi-cell node architecture for LSTMs and study its applicability for neural language modeling. The proposed multi-cell LSTM language models outperform the state-of-the-art results on well-known Penn Treebank (PTB) setup.",50.0,Multi-cell LSTM,Unreleased,India,,,,,,2025-04-14 15:06,,,,,,Academia,,,,,Unreleased,,Academia,,,,,Operation counting,,,,
Fine-tuned-AWD-LSTM-DOC (fin),Language,Language modeling,Samsung R&D Institute Russia,"Vadim Popov, Mikhail Kudinov",2018-11-12,Fine-tuning of Language Models with Discriminator,https://arxiv.org/abs/1811.04623,2.0,SOTA improvement,"""The novel approach that we propose allows us to reach state-of-theart quality on Penn Treebank: perplexity decreases from 52.4 to 52.1.""",46000000.0,"This is the model trained on Penn Treebank, which uses as a base model the 23M model from Table 7 in https://aclanthology.org/D18-1489.pdf

They additionally train a discriminator with the same architecture, so total parameters is 2*23M = 46M",5.188e+16,"Base model uses 4.323e16 FLOPs.
They then train a discriminator using the same architecture for 30 epochs, and then use the discriminator to fine-tune the base model for another 15 epochs. Both of these latter training steps require running forward passes on both the discriminator and the language model, but only doing a backward pass on one of them.

Discriminator training: 
2*23M*30*1044112 + 6*23M*30*1044112 = 5.763e15

LM fine-tuning:
2*23M*15*1044112 + 6*23M*15*1044112 = 2.882e15

Total:
4.323e16 + 5.763e15 + 2.882e15 = 5.188e16",Penn TreeBank (PTB),,1044112.0,"Per https://arxiv.org/pdf/1904.04733:
""The most common split of this corpus, where sections from 0 to 18 are used for training (38 219 sentences, 912 344 tokens), sections from 19 to 21 are used for validation (5 527 sentences, 131 768 tokens), and sections from 22 to 24 are used for testing (5 462 sentences, 129 654 tokens).""

So dev set is 912,344 + 131768 = 1,044,112",,,,,Confident,"Cross-entropy loss is a common choice when it comes to multiclass classification tasks and language modeling in particular. Minimizing this loss results in language models of very good quality. We show that it is possible to fine-tune these models and make them perform even better if they are fine-tuned with sum of cross-entropy loss and reverse Kullback-Leibler divergence. The latter is estimated using discriminator network that we train in advance. During fine-tuning probabilities of rare words that are usually underestimated by language models become bigger. The novel approach that we propose allows us to reach state-of-the-art quality on Penn Treebank: perplexity decreases from 52.4 to 52.1. Our fine-tuning algorithm is rather fast, scales well to different architectures and datasets and requires almost no hyperparameter tuning: the only hyperparameter that needs to be tuned is learning rate.",15.0,Fine-tuned-AWD-LSTM-DOC(fin),Unreleased,Russia,AWD-LSTM-DOC (fin) (23M),,,,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,,Industry,,,,,Operation counting,,,,
Discriminator-tuned LSTM,Language,Language modeling,Samsung R&D Institute Russia,"Vadim Popov, Mikhail Kudinov",2018-11-12,Fine-tuning of Language Models with Discriminator,https://arxiv.org/abs/1811.04623,,,Smaller model trained on Penn Treebank gets SOTA; this is a larger experiment but does not challenge any well-known benchmark.,111920000.0,"The largest experiment is the model trained in section 4.3 (""Large scale experiment""). The dataset here has a vocabulary of 100k words. They use a single-layer LSTM with 500 hidden units, with differentiated softmax for the output layer. d_k for distributed softmax varies from 16-150 depending on the relative frequency of the output word. 

Input layer:
100k * 500 = 50M parameters

LSTM:
4 * (500 + 500) * 500 = 2M parameters

Output layer:
Here we need to assume the structure of differentiated softmax. Let's (somewhat arbitrarily) assume the top 10k words get 150-dim embeddings, the middle 30k get 50-dim embeddings, and the bottom 60k get 16-dim embeddings. Then we have:
(10k * 150) + (30k * 50) + (60k * 16) = 3.96M parameters

So in total, we have
50M + 2M + 3.96M = 55.96M

In other experiments, the authors use the same architecture for the discriminator as for the language model. So total parameter count would appear to be 2 x 55.96M = 111.92M",,"We can estimate the compute used to train the discriminator and to fine-tune the language model. These trained for 320M and 160M tokens, respectively; note that for each of these steps you're doing forward passes on both the discriminator and language model, but only doing backward passes on the component being trained.

However, they don't say how long they pre-trained the language model, which likely constitutes the majority of training.",,"""we trained our model on consisted of 4Gb of English texts gathered from the Internet (blogs, news articles, forums, etc.) Vocabulary consisted of top 100k words from training corpus. Validation and test setsâ€™ sizes were around 100Mb.""",,"4GB of text * 200M words/GB * (0.75 words/token)^-1 = 1,066,666,667 tokens  
  
However, it's unclear whether they train over all of this. They break the data into 20MB chunks and say that each ""epoch"" corresponds to training on one chunk. Training the discriminator and fine-tuning the language model takes 60 and 30 ""epochs"" each, i.e.  
(60 + 30) * 0.02 * 200M * 4/3 = 480M tokens  
  
They appear to have pre-trained their own LSTM language model on the same data, but they don't say how many ""epochs"" or tokens it took to train that model.",,,,Self-supervised learning,Likely,"Cross-entropy loss is a common choice when it comes to multiclass classification tasks and language modeling in particular. Minimizing this loss results in language models of very good quality. We show that it is possible to fine-tune these models and make them perform even better if they are fine-tuned with sum of cross-entropy loss and reverse Kullback-Leibler divergence. The latter is estimated using discriminator network that we train in advance. During fine-tuning probabilities of rare words that are usually underestimated by language models become bigger. The novel approach that we propose allows us to reach state-of-the-art quality on Penn Treebank: perplexity decreases from 52.4 to 52.1. Our fine-tuning algorithm is rather fast, scales well to different architectures and datasets and requires almost no hyperparameter tuning: the only hyperparameter that needs to be tuned is learning rate.",,,,Russia,,,,,,2025-05-09 11:32,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
Mesh-TensorFlow Transformer 4.9B (language),Language,"Language modeling/generation,Translation",Google Brain,"Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koanantakool, Peter Hawkins, Hyoukjoong Mingsheng Lee, Cliff Hong, Ryan Young, Blake Sepassi,  Hechtman",2018-11-05,Mesh-TensorFlow: Deep Learning for Supercomputers,https://arxiv.org/abs/1811.02084,357.0,SOTA improvement,"'Using TPU meshes of up to 512 cores, we train Transformer models with up to 5 billion parameters, surpassing state of the art results on WMT'14 English-to-French translation task and the one-billion-word language modeling benchmark.'",4900000000.0,4.9B from section 9.1 : ''The largest model (4.9B parameters) took 13 hours to train on a 512-core TPUv2 cluster.',1.617408e+20,"flops = (256) * ( 45 * 10**12) * (13 * 3600) * (0.3) = 1.6e20
(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)

from section 9.1 : ''The largest model (4.9B parameters) took 13 hours to train on a 512-core TPUv2 cluster.'
from https://en.wikipedia.org/wiki/Tensor_Processing_Unit 
45TFLOPs per chips","Wikipedia,One Billion Word benchmark",from section 9.1 Wikipedia and one-billion-word language modeling benchmark.,6333333333.0,"from section 9.1. Experiments done on a ""billion word benchmark"" and a 5B token wikipedia dataset. At 4/3 tokens per word, 1.3B tokens in the first.",13.0,"from section 9.1 ""For the billion-word language modeling benchmark, we trained the models for 10 epochs. The largest model (4.9B parameters) took 13 hours to train on a 512-core TPUv2 cluster.""",Google TPU v2,,Confident,"Batch-splitting (data-parallelism) is the dominant distributed Deep Neural Network (DNN) training strategy, due to its universal applicability and its amenability to Single-Program-Multiple-Data (SPMD) programming. However, batch-splitting suffers from problems including the inability to train very large models (due to memory constraints), high latency, and inefficiency at small batch sizes. All of these can be solved by more general distribution strategies (model-parallelism). Unfortunately, efficient model-parallel algorithms tend to be complicated to discover, describe, and to implement, particularly on large clusters. We introduce Mesh-TensorFlow, a language for specifying a general class of distributed tensor computations. Where data-parallelism can be viewed as splitting tensors and operations along the ""batch"" dimension, in Mesh-TensorFlow, the user can specify any tensor-dimensions to be split across any dimensions of a multi-dimensional mesh of processors. A Mesh-TensorFlow graph compiles into a SPMD program consisting of parallel operations coupled with collective communication primitives such as Allreduce. We use Mesh-TensorFlow to implement an efficient data-parallel, model-parallel version of the Transformer sequence-to-sequence model. Using TPU meshes of up to 512 cores, we train Transformer models with up to 5 billion parameters, surpassing state of the art results on WMT'14 English-to-French translation task and the one-billion-word language modeling benchmark. Mesh-Tensorflow is available at this https URL .",10.0,,Unreleased,United States of America,,,,256.0,,2025-05-01 10:42,,,,,,Industry,,,,3328.0,Open source,"code here, apache license: https://github.com/tensorflow/mesh/tree/master/mesh_tensorflow/transformer 

https://github.com/tensorflow/mesh/blob/master/mesh_tensorflow/transformer/transformer.py ",Industry,$935.33,,,148117.2291982981,Hardware,,,,18
Mesh-TensorFlow Transformer 2.9B (translation),Language,"Language modeling/generation,Translation",Google Brain,"Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koanantakool, Peter Hawkins, Hyoukjoong Mingsheng Lee, Cliff Hong, Ryan Young, Blake Sepassi,  Hechtman",2018-11-05,Mesh-TensorFlow: Deep Learning for Supercomputers,https://arxiv.org/abs/1811.02084,357.0,SOTA improvement,"'Using TPU meshes of up to 512 cores, we train Transformer models with up to 5 billion parameters, surpassing state of the art results on WMT'14 English-to-French translation task and the one-billion-word language modeling benchmark.'",2900000000.0,"2.9B from section 9.1 : ""On the WMT14 En-Fr translation tasks (3), we trained the models for 3 epochs. The largest model
(2.9B parameters) was trained for 22 hours on a 128-core TPUv2 cluster.""",6.84288e+19,"flops = (64) * ( 45 * 10**12) * (22 * 3600) * (0.3) = 6.8e19
(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)

from section 9.1 : ""On the WMT14 En-Fr translation tasks (3), we trained the models for 3 epochs. The largest model
(2.9B parameters) was trained for 22 hours on a 128-core TPUv2 cluster.""
from https://en.wikipedia.org/wiki/Tensor_Processing_Unit 
45TFLOPs per chips",WMT14,"from section 9.1 ""On the WMT14 En-Fr translation tasks (3), we trained the models for 3 epochs. The largest model
(2.9B parameters) was trained for 22 hours on a 128-core TPUv2 cluster.""",1800000000.0,"Per Attention is All You Need, WMT 2014 En-Fr is ~36 million sentence pairs. If the average sentence is ~25 tokens (ballpark), dataset size is 
36M * 25 * 2 = 1.8B tokens",22.0,"from section 9.1 ""On the WMT14 En-Fr translation tasks (3), we trained the models for 3 epochs. The largest model
(2.9B parameters) was trained for 22 hours on a 128-core TPUv2 cluster.""",Google TPU v2,,Likely,"Batch-splitting (data-parallelism) is the dominant distributed Deep Neural Network (DNN) training strategy, due to its universal applicability and its amenability to Single-Program-Multiple-Data (SPMD) programming. However, batch-splitting suffers from problems including the inability to train very large models (due to memory constraints), high latency, and inefficiency at small batch sizes. All of these can be solved by more general distribution strategies (model-parallelism). Unfortunately, efficient model-parallel algorithms tend to be complicated to discover, describe, and to implement, particularly on large clusters. We introduce Mesh-TensorFlow, a language for specifying a general class of distributed tensor computations. Where data-parallelism can be viewed as splitting tensors and operations along the ""batch"" dimension, in Mesh-TensorFlow, the user can specify any tensor-dimensions to be split across any dimensions of a multi-dimensional mesh of processors. A Mesh-TensorFlow graph compiles into a SPMD program consisting of parallel operations coupled with collective communication primitives such as Allreduce. We use Mesh-TensorFlow to implement an efficient data-parallel, model-parallel version of the Transformer sequence-to-sequence model. Using TPU meshes of up to 512 cores, we train Transformer models with up to 5 billion parameters, surpassing state of the art results on WMT'14 English-to-French translation task and the one-billion-word language modeling benchmark. Mesh-Tensorflow is available at this https URL .",10.0,,Unreleased,United States of America,,,,64.0,,2025-02-17 12:34,,,,,,Industry,,,,1408.0,Open source,"code here, apache license https://github.com/tensorflow/mesh/tree/master/mesh_tensorflow/transformer 

https://github.com/tensorflow/mesh/blob/master/mesh_tensorflow/transformer/transformer.py  ",Industry,$395.72,,,37029.30729957453,Hardware,,,,
MemoReader,Language,Question answering,"Samsung,Korea University","Seohyun Back, Seunghak Yu, Sathish Indurthi, Jihie Kim, Jaegul Choo",2018-10-31,"MemoReader: Large-Scale Reading Comprehension through Neural Memory Controller
",https://aclanthology.org/D18-1237/,17.0,SOTA improvement,"""TriviaQA. As shown in Table 2, our model,
even without DEBS, outperforms the existing
state-of-the-art method such as â€˜BiDAF + SA +
SNâ€™ by a large margin in all the cases""",,,,"""Our model does require more memory than existing methods, but a single GPU (e.g., M40 with 12GB memory) was enough to train model within a reasonable amount of time""

""Reasonable"" could mean anything, maybe hours to a few days.",TriviaQA,,,,,"""reasonable amount of time"" with a single GPU",NVIDIA M40,,Unknown,"Machine reading comprehension helps machines learn to utilize most of the human
knowledge written in the form of text. Existing approaches made a significant progress comparable to human-level performance, but they
are still limited in understanding, up to a few paragraphs, failing to properly comprehend
lengthy document. In this paper, we propose a novel deep neural network architecture to handle a long-range dependency in RC tasks. In
detail, our method has two novel aspects: (1) an advanced memory-augmented architecture
and (2) an expanded gated recurrent unit with dense connections that mitigate potential information distortion occurring in the memory.
Our proposed architecture is widely applicable
to other models. We have performed extensive experiments with well-known benchmark
datasets such as TriviaQA, QUASAR-T, and
SQuAD. The experimental results demonstrate
that the proposed method outperforms existing
methods, especially for lengthy documents.",,,Unreleased,"Korea (Republic of),Korea (Republic of)",,,,,,2024-10-08 13:36,,,,,,"Industry,Academia",,,,,,,"Industry,Academia",,,,,,,,,
code2vec,Language,Language modeling,"Technion - Israel Institute of Technology,Facebook AI Research","Uri Alon, Meital Zilberstein, Omer Levy, Eran Yahav",2018-10-30,code2vec: Learning Distributed Representations of Code,https://arxiv.org/abs/1803.09473,,,,,,3.1593888e+17,8126000000000.000 * 36 * 3600 * 0.3 = 3.1593888e+17,GitHub,,,"We used a data set of 10, 072 Java GitHub repositories, originally introduced by Alon et al. [2018].
In this dataset, the files from all the projects are shuffled and split to
14,162,842 training (66GB), 415, 046 validation and 413, 915 of test methods

""the average method length is 7 lines"" [of code]",36.0,"When training on a single Tesla K80 GPU, we achieve a training
throughput of more than 1000 methods per second. Therefore, a single training epoch takes about 3 hours, and it takes about 1.5 days to completely train a model.

36/3 = 12 epochs",NVIDIA Tesla K80,,Confident,"We present a neural model for representing snippets of code as continuous distributed vectors (""code embeddings""). The main idea is to represent a code snippet as a single fixed-length code vector, which can be used to predict semantic properties of the snippet. This is performed by decomposing code to a collection of paths in its abstract syntax tree, and learning the atomic representation of each path simultaneously with learning how to aggregate a set of them. We demonstrate the effectiveness of our approach by using it to predict a method's name from the vector representation of its body. We evaluate our approach by training a model on a dataset of 14M methods. We show that code vectors trained on this dataset can predict method names from files that were completely unobserved during training. Furthermore, we show that our model learns useful method name vectors that capture semantic similarities, combinations, and analogies. Comparing previous techniques over the same data set, our approach obtains a relative improvement of over 75%, being the first to successfully predict method names based on a large, cross-project, corpus. Our trained model, visualizations and vector similarities are available as an interactive online demo at this http URL. The code, data, and trained models are available at this https URL.",12.0,,Open weights (unrestricted),"Israel,United States of America",,,,,,2025-05-01 10:42,,,,,,"Academia,Industry",,,,,Open source,"MIT License
https://github.com/tech-srl/code2vec","Academia,Industry",,,,,Hardware,,,,
TrellisNet,Language,Language modeling,"Carnegie Mellon University (CMU),Bosch Center for Artificial Intelligence,Intel Labs","Shaojie Bai, J. Zico Kolter, Vladlen Koltun",2018-10-15,Trellis Networks for Sequence Modeling,https://arxiv.org/abs/1810.06682,138.0,SOTA improvement,"""Experiments demonstrate that trellis networks outperform the current state of the art methods on a variety of challenging benchmarks, including word-level language modeling and character-level language modeling
tasks""",180000000.0,"180M, Table 2",2.78e+18,6 FLOP / parameter / token * 180000000 parameters * 103000000 tokens * 25 epochs = 2.781e+18 FLOP,WikiText-103,,103000000.0,,,,,,Confident,"We present trellis networks, a new architecture for sequence modeling. On the one hand, a trellis network is a temporal convolutional network with special structure, characterized by weight tying across depth and direct injection of the input into deep layers. On the other hand, we show that truncated recurrent networks are equivalent to trellis networks with special sparsity structure in their weight matrices. Thus trellis networks with general weight matrices generalize truncated recurrent networks. We leverage these connections to design high-performing trellis networks that absorb structural and algorithmic elements from both recurrent and convolutional models. Experiments demonstrate that trellis networks outperform the current state of the art methods on a variety of challenging benchmarks, including word-level language modeling and character-level language modeling tasks, and stress tests designed to evaluate long-term memory retention. The code is available at this https URL .",25.0,TrellisNet,Unreleased,"United States of America,Germany,Multinational,United States of America",,,,,,2025-04-14 15:12,,,,,,"Academia,Industry,Industry",,,,,Open source,MIT license for code: https://github.com/locuslab/trellisnet/tree/master/TrellisNet/word_WT103 ,"Academia,Industry,Industry",,,,,Operation counting,,,,
BERT-Large,Language,"Question answering,Text autocompletion",Google,"J Devlin, MW Chang, K Lee, K Toutanova",2018-10-11,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,https://arxiv.org/abs/1810.04805,83567.0,Highly cited,,340000000.0,340M,2.85e+20,"more info here https://docs.google.com/document/d/1B8x6XYcmB1u6Tmq3VcbAtj5bzhDaj2TcIPyK6Wpupx4/edit?usp=sharing
285000000000000000000 = 2.85 Ã— 10^20

""AI and Memory Wall"" paper (https://github.com/amirgholami/ai_and_memory_wall) made an estimation of 250,000 PFLOPS = 2.5*10^20 FLOP","""BookCorpus (BooksCorpus, Toronto Book Corpus)"",English Wikipedia",,3300000000.0,"""For the pre-training corpus we
use the BooksCorpus (800M words) (Zhu et al., 2015) and English Wikipedia (2,500M words)""",96.0,"from appendix A.2: ""Training of BERTLARGE was performed
on 16 Cloud TPUs (64 TPU chips total). Each pre-
training took 4 days to complete.""",Google TPU v2,Self-supervised learning,Confident,"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",40.0,,Open weights (unrestricted),United States of America,,,,64.0,0.2801,2025-05-09 11:32,,,,128000.0,,Industry,,,,6144.0,Open source,"apache 2.0
train+inference code and models here: https://github.com/google-research/bert ",Industry,$1751.48,"Estimated FLOPs used (see training compute notes): 2.85e20
GPU-time: 96 h * 3600 s/h * 64 GPU * 4.6e13 FLOP/GPU-s = 1.017e21 FLOP
2.85e20 / 1.017e21 = 0.2801",,37049.92852412132,"Operation counting,Hardware,Third-party estimation",,,,17
BigGAN-deep 512x512,Image generation,Image generation,"Heriot-Watt University,DeepMind","Andrew Brock, Jeff Donahue, Karen Simonyan",2018-09-28,Large Scale GAN Training for High Fidelity Natural Image Synthesis,https://arxiv.org/abs/1809.11096,4925.0,Highly cited,,112694781.0,"I used the publicly available implementation available at [1]

There I loaded the biggan-deep512/1 model, and ran script [2] to compute the number of parameters

[1] https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/biggan_generation_with_tf_hub.ipynb

[2]
n_params = 0
for var in module.variables:
  n_params += np.prod(var.shape.as_list())
  pass

print(n_params)",1.8e+21,"3e21, estimate taken from:

https://www.lesswrong.com/posts/wfpdejMWog4vEDLDg/ai-and-compute-trend-isn-t-predictive-of-what-is-happening",JFT-300M,,292000000.0,"""To confirm that our design choices are effective for even larger and more complex and diverse datasets, we also present results of our system on a subset of JFT-300M (Sun et al., 2017). The full JFT-300M dataset contains 300M real-world images labeled with 18K categories. Since the category distribution is heavily long-tailed, we subsample the dataset to keep only images with the 8.5K most common labels. The resulting dataset contains 292M images â€“ two orders of magnitude larger than ImageNet. """,48.0,"""We train on a Google TPU v3 Pod, with the number of cores proportional to the resolution: 128 for 128Ã—128, 256 for 256Ã—256, and 512 for 512Ã—512. Training takes between 24 and 48 hours for most models""",Google TPU v3,,Likely,"Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as ImageNet remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities specific to such scale. We find that applying orthogonal regularization to the generator renders it amenable to a simple ""truncation trick,"" allowing fine control over the trade-off between sample fidelity and variety by reducing the variance of the Generator's input. Our modifications lead to models which set the new state of the art in class-conditional image synthesis. When trained on ImageNet at 128x128 resolution, our models (BigGANs) achieve an Inception Score (IS) of 166.5 and Frechet Inception Distance (FID) of 7.4, improving over the previous best IS of 52.52 and FID of 18.6.",,,Open weights (unrestricted),"United Kingdom of Great Britain and Northern Ireland,United Kingdom of Great Britain and Northern Ireland",,,,256.0,,2025-05-28 16:16,,,,,,"Academia,Industry",,,,12288.0,Unreleased,"repo license is Apache:

https://github.com/tensorflow/tfhub.dev/blob/master/assets/docs/deepmind/models/biggan-deep-512/1.md","Academia,Industry",$5170.46,,FP32,116476.34773925862,Third-party estimation,,,,8
Transformer (Adaptive Input Embeddings) WT103,Language,Language modeling,Facebook AI Research,"Alexei Baevski, Michael Auli",2018-09-28,Adaptive Input Representations for Neural Language Modeling,https://arxiv.org/abs/1809.10853,366.0,SOTA improvement,"""On the WikiText-103 benchmark we achieve 18.7 perplexity, an improvement of 10.5 perplexity compared to the previously best published result""",247000000.0,Table 2,4.47e+19,"8 V100s * 67 hours per Table 2.
125e12 FLOP/sec * 8 * 67 * 3600 * 0.3 (utilization assumption) = 7.2e19 FLOP

They also say they trained for 286k steps in batches of 65,536 tokens.
6 * 247M * (286k * 65536) = 2.78e19

geometric mean: sqrt(7.2e19 * 2.78e19) = 4.47e19",WikiText-103,"The training data of WIKITEXT-103 comprises about 100M tokens""",100000000.0,"""The training data of WIKITEXT-103 comprises about 100M tokens""
Datasets are not combined but used to train separate models",67.0,,NVIDIA V100,,Confident,"We introduce adaptive input representations for neural language modeling which extend the adaptive softmax of Grave et al. (2017) to input representations of variable capacity. There are several choices on how to factorize the input and output layers, and whether to model words, characters or sub-word units. We perform a systematic comparison of popular choices for a self-attentional architecture. Our experiments show that models equipped with adaptive embeddings are more than twice as fast to train than the popular character input CNN while having a lower number of parameters. On the WIKITEXT-103 benchmark we achieve 18.7 perplexity, an improvement of 10.5 perplexity compared to the previously best published result and on the BILLION WORD benchmark, we achieve 23.02 perplexity.",180.0,Transformer (Adaptive Input Embeddings),Open weights (unrestricted),United States of America,,,,8.0,,2025-05-28 16:16,,,,,,Industry,,,,4288.0,Open source,"MIT for code and weights: https://github.com/facebookresearch/fairseq/blob/main/examples/language_model/README.adaptive_inputs.md 

inference in other readme: https://github.com/facebookresearch/fairseq/blob/main/examples/language_model/README.md ",Industry,$2880.92,,FP16,4963.480727525225,"Hardware,Operation counting",,,,
LSTM+NeuralCache,Language,Language modeling,"KU Leuven,ESAT - PSI,Apple","Lyan Verwimp, Joris Pelemans, Hugo Van hamme, Patrick Wambacq",2018-09-24,Information-Weighted Neural Cache Language Models for ASR,https://arxiv.org/abs/1809.08826,3.0,SOTA improvement,"""We obtain a 29.9%/32.1% (validation/test set) relative improvement in perplexity with respect to a baseline LSTM LM on the WikiText-2 dataset, outperforming previous work on neural cache LMs"" 
... 

""we observe that neural cache models
consistently outperform regular cache models on this dataset.""",2100000.0,"Given:

Hidden size (H) = 512
Number of hidden layers (L) = 1
Input size (I) is not mentioned, so let's denote it as I
The total number of parameters P in an LSTM can be calculated as follows:

P = 4 * ((I * H) + (H * H) + H)

This is for one layer of LSTM cells. Since the LSTM model described has only one layer, we don't need to multiply by the number of layers.

To calculate the exact number of parameters, we would need to know the input size I. However, if you are looking for the number of parameters just within a single LSTM cell (assuming I is equal to H), then you can substitute I with H in the above formula:

P = 4 * ((H * H) + (H * H) + H)
= 4 * (2 * (H^2) + H)

For H = 512, this becomes:

P = 4 * (2 * (512^2) + 512)
= 4 * (2 * 262144 + 512)
= 4 * (524288 + 512)
= 4 * 524800
â‰ˆ 2,099,200",982800000000000.0,6 FLOP / parameter / token * 2100000 parameters * 2000000 tokens * 39 epochs = 9.828e+14 FLOP,WikiText-2,,2000000.0,""" we stop training anyway after 39 epochs""",,,,,Likely,"Neural cache language models (LMs) extend the idea of regular cache language models by making the cache probability dependent on the similarity between the current context and the context of the words in the cache. We make an extensive comparison of 'regular' cache models with neural cache models, both in terms of perplexity and WER after rescoring first-pass ASR results. Furthermore, we propose two extensions to this neural cache model that make use of the content value/information weight of the word: firstly, combining the cache probability and LM probability with an information-weighted interpolation and secondly, selectively adding only content words to the cache. We obtain a 29.9%/32.1% (validation/test set) relative improvement in perplexity with respect to a baseline LSTM LM on the WikiText-2 dataset, outperforming previous work on neural cache LMs. Additionally, we observe significant WER reductions with respect to the baseline model on the WSJ ASR task.",39.0,LSTM+NeuralCache,Unreleased,"Belgium,Belgium,United States of America",,,,,,2025-05-28 16:16,,,,,,"Academia,Academia,Industry",,,,,Unreleased,,"Academia,Academia,Industry",,,FP32,,Operation counting,,,,
Transformer + Simple Recurrent Unit,Language,Translation,"ASAPP,Cornell University,Google,Princeton University","Tao Lei, Yu Zhang, Sida I. Wang, Hui Dai, Yoav Artzi",2018-09-17,Simple Recurrent Units for Highly Parallelizable Recurrence,https://arxiv.org/abs/1709.02755v5,293.0,SOTA improvement,"""We use the state-of-the-art Transformer
model of Vaswani et al. (2017) as our base architecture... When SRU is incorporated into the architecture,
both the 4-layer and 5-layer model outperform the
Transformer base model""",90000000.0,"5-layer model, Table 3",1.1e+19,"""We use a single NVIDIA Tesla V100 GPU for each model. The published results were obtained
using 8 GPUs in parallel, which provide a large effective batch size during training. To approximate
the setup, we update the model parameters every 5Ã—5120 tokens and use 16,000 warm-up steps
following OpenNMT suggestions. We train each
model for 40 epochs (250,000 steps), and perform
3 independent trials for each model configuration.
A single run takes about 3.5 days with a Tesla V100 GPU.""

125 trillion * 3.5 * 24 * 3600 * 0.3 = 1.1e19",WMT English-German,"""We train translation models on the WMT Englishâ†’German dataset, a standard
benchmark for translation systems (Peitz et al.,
2014; Li et al., 2014; Jean et al., 2015). The
dataset consists of 4.5 million sentence pairs""",,,,,NVIDIA V100,,Confident,"Common recurrent neural architectures scale poorly due to the intrinsic difficulty in parallelizing their state computations. In this work, we propose the Simple Recurrent Unit (SRU), a light recurrent unit that balances model capacity and scalability. SRU is designed to provide expressive recurrence, enable highly parallelized implementation, and comes with careful initialization to facilitate training of deep models. We demonstrate the effectiveness of SRU on multiple NLP tasks. SRU achieves 5--9x speed-up over cuDNN-optimized LSTM on classification and question answering datasets, and delivers stronger results than LSTM and convolutional models. We also obtain an average of 0.7 BLEU improvement over the Transformer model on translation by incorporating SRU into the architecture.",40.0,,Unreleased,"United States of America,United States of America,United States of America,United States of America",,,,8.0,,2025-02-17 14:40,,,,,,"Industry,Academia,Industry,Academia",,,,,Unreleased,"repo, but no training code for translation: https://github.com/taolei87/sru ","Industry,Academia,Industry,Academia",$45.37,,,4964.696746005496,Hardware,,,,
ESRGAN,"Vision,Image generation",Image super-resolution,"Chinese University of Hong Kong (CUHK),Chinese Academy of Sciences,Nanyang Technological University","Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Chen Change Loy, Yu Qiao, Xiaoou Tang",2018-09-01,ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks,https://arxiv.org/abs/1809.00219,3266.0,Highly cited,,,,,,"DIV2K,Flickr2K,OutdoorSceneTraining (OST)",,,,,,,,Unknown,"The Super-Resolution Generative Adversarial Network (SRGAN) is a seminal work that is capable of generating realistic textures during single image super-resolution. However, the hallucinated details are often accompanied with unpleasant artifacts. To further enhance the visual quality, we thoroughly study three key components of SRGAN - network architecture, adversarial loss and perceptual loss, and improve each of them to derive an Enhanced SRGAN (ESRGAN). In particular, we introduce the Residual-in-Residual Dense Block (RRDB) without batch normalization as the basic network building unit. Moreover, we borrow the idea from relativistic GAN to let the discriminator predict relative realness instead of the absolute value. Finally, we improve the perceptual loss by using the features before activation, which could provide stronger supervision for brightness consistency and texture recovery. Benefiting from these improvements, the proposed ESRGAN achieves consistently better visual quality with more realistic and natural textures than SRGAN and won the first place in the PIRM2018-SR Challenge. The code is available at this https URL .",,,,"Hong Kong,China,China,Singapore",,,,,,2025-02-03 19:52,,,,,,"Academia,Academia,Academia",,,,,,,"Academia,Academia,Academia",,,,,,,,,
(ensemble): AWD-LSTM-DOC (fin) Ã— 5 (WT2),Language,Language modeling,"NTT Communication Science Laboratories,Tohoku University","Sho Takase, Jun Suzuki, Masaaki Nagata",2018-08-30,Direct Output Connection for a High-Rank Language Model,https://arxiv.org/abs/1808.10143,36.0,SOTA improvement,"""The proposed method improves the current state-of-the-art language model and achieves the best score on the Penn Treebank and WikiText-2, which are the standard benchmark datasets""",185000000.0,185M (table 8),6.66e+17,6 FLOP / parameter / token * 185000000 parameters * 2000000 tokens * 300 epochs = 6.66e+17 FLOP,WikiText-2,,2000000.0,"batch size 15
300 epochs (figure 2)",,,,,Confident,"This paper proposes a state-of-the-art recurrent neural network (RNN) language model that combines probability distributions computed not only from a final RNN layer but also from middle layers. Our proposed method raises the expressive power of a language model based on the matrix factorization interpretation of language modeling introduced by Yang et al. (2018). The proposed method improves the current state-of-the-art language model and achieves the best score on the Penn Treebank and WikiText-2, which are the standard benchmark datasets. Moreover, we indicate our proposed method contributes to two application tasks: machine translation and headline generation. Our code is publicly available at: this https URL.",300.0,(ensemble): AWD-LSTM-DOC (fin) Ã— 5 (WT2),Open weights (unrestricted),"Japan,Japan",,,,,,2025-04-14 15:29,,,,,,"Industry,Academia",,,,,Open source,"code and weights, MIT: https://github.com/nttcslab-nlp/doc_lm?tab=readme-ov-file ","Industry,Academia",,,,,Operation counting,,,,
Big Transformer for Back-Translation,Language,Translation,"Facebook AI Research,Google Brain","Sergey Edunov, Myle Ott, Michael Auli, David Grangier",2018-08-28,Understanding Back-Translation at Scale,https://arxiv.org/abs/1808.09381,1155.0,"Highly cited,SOTA improvement","""Finally, we scale to hundreds of millions of monolingual sentences and achieve a new state of the art of 35 BLEU on the WMT'14 English-German test set. """,,"""We re-implemented the Transformer model in py-
torch using the fairseq toolkit.1 All experiments
are based on the Big Transformer architecture with
6 blocks in the encoder and decoder. We use the
same hyper-parameters for all experiments, i.e.,
word representations of size 1024, feed-forward
layers with inner dimension 4096. ""

I am not sure what authors mean by 'Big Transformer architecture'",4.7808e+20,"(128) * (1.25e14) * (27*3600 + 40*60) * (0.3)  = 4.7808e20
(number of gpus) * (peak flops) * (seconds) * (assumed utilization rate)  

""We run experiments on DGX-1 machines with 8Nvidia V100 GPUs and machines are interconnected by Infiniband. Experiments are run on 16
machines and we perform 30K synchronous updates.""
""We also use the NCCL2 library [...] with 16-bit floating point
operations""

NCCL2 supported tensor core operations at 1.25e14 FLOP/s on a V100 for FP16 

in section 5.6 we have

""train this system we perform 300K training up-
dates in 27h 40min on 128 GPUs;""",WMT English-German,"""Finally, for WMT English-German we train
on all 226M available monolingual training sen-
tences and perform 250K updates in 22.5 hours on 128 GPUs. """,3390000000.0,"""Finally, for WMT English-German we train on all 226M available monolingual training sentences and perform 250K updates in 22.5 hours on 128 GPUs.""

We assume that 1 sentence have 15 words",27.7,"""training updates in 27h 40min on 128 GPUs""",NVIDIA Tesla V100 DGXS 16 GB,Supervised,Likely,"An effective method to improve neural machine translation with monolingual data is to augment the parallel training corpus with back-translations of target language sentences. This work broadens the understanding of back-translation and investigates a number of methods to generate synthetic source sentences. We find that in all but resource poor settings back-translations obtained via sampling or noised beam outputs are most effective. Our analysis shows that sampling or noisy synthetic data gives a much stronger training signal than data generated by beam or greedy search. We also compare how synthetic data compares to genuine bitext and study various domain effects. Finally, we scale to hundreds of millions of monolingual sentences and achieve a new state of the art of 35 BLEU on the WMT'14 English-German test set. ",,,Open weights (unrestricted),"United States of America,United States of America",,,,128.0,,2025-05-28 16:16,,,,,,"Industry,Industry",,,,3541.0,Open source,"Code and weights, MIT license: https://github.com/facebookresearch/fairseq/blob/main/examples/backtranslation/README.md ","Industry,Industry",$2442.16,,FP16,66225.44602672113,Hardware,,,,11
Big-Little Net,Vision,Image classification,IBM,"Chun-Fu Chen, Quanfu Fan, Neil Mallinar, Tom Sercu, and Rogerio Feris",2018-07-10,Big-Little Net: An Efficient Multi-Scale Feature Representation for Visual and Speech Recognition,https://arxiv.org/abs/1807.03848,92.0,SOTA improvement,"""On object recognition task, we demonstrated that our approach provides approximately 2Ã— speedup over baselines while
improving accuracy, and the result significantly outperforms the state-of-the-art networks by a large margin in terms of accuracy and FLOPs reduction""",77360000.0,Table 2,2.46048e+17,"Using the 6ND formula: 
6Ã—number of tokensÃ—number of parametersÃ—number of epochs
6Ã—1.28Ã—10^6Ã—77360000Ã—110=6.5353728e+16 FLOPs

9.32*10^9 (flops per inference)*1.28Ã—10^6(dataset size)/16 (batch size) * 110 epochs * 3 (to account for backpropagation)= 2.46048e+17 FLOPs",ImageNet,,1280000.0,,,,NVIDIA Tesla K80,,Likely,"In this paper, we propose a novel Convolutional Neural Network (CNN) architecture for learning multi-scale feature representations with good tradeoffs between speed and accuracy. This is achieved by using a multi-branch network, which has different computational complexity at different branches. Through frequent merging of features from branches at distinct scales, our model obtains multi-scale features while using less computation. The proposed approach demonstrates improvement of model efficiency and performance on both object recognition and speech recognition tasks,using popular architectures including ResNet and ResNeXt. For object recognition, our approach reduces computation by 33% on object recognition while improving accuracy with 0.9%. Furthermore, our model surpasses state-of-the-art CNN acceleration approaches by a large margin in accuracy and FLOPs reduction. On the task of speech recognition, our proposed multi-scale CNNs save 30% FLOPs with slightly better word error rates, showing good generalization across domains.",110.0,,Open weights (unrestricted),United States of America,,,,,,2025-02-04 13:04,,,,256.0,"""All the models were trained with 110 epochs, batch size 256""",Industry,,,,,Open source,"Apache 2 license
https://github.com/IBM/BigLittleNet",Industry,,,,,Operation counting,,,,
Big-Little Net (vision),Vision,Object recognition,IBM,"Chun-Fu (Richard) Chen, Quanfu Fan, Neil Mallinar, Tom Sercu, Rogerio Feris",2018-07-10,BIG-LITTLE NET: AN EFFICIENT MULTI-SCALE FEATURE REPRESENTATION FOR VISUAL AND SPEECH RECOGNITION,https://arxiv.org/abs/1807.03848,97.0,,,77360000.0,Table 2 - fifth row,6.2988288e+19,"number of epochs (appendix A1) times flops per inference (from table 2) times dataset size times 3 (to account for backpropagation)
110 * 9.32e9 FLOPs * 256/16 * 1280000 * 3 = 6.3e19",ImageNet,section 4.1,1280000.0,size of ImageNet,,,,,Confident,"In this paper, we propose a novel Convolutional Neural Network (CNN) architecture for learning multi-scale feature representations with good tradeoffs between speed and accuracy. This is achieved by using a multi-branch network, which has different computational complexity at different branches. Through frequent merging of features from branches at distinct scales, our model obtains multi-scale features while using less computation. The proposed approach demonstrates improvement of model efficiency and performance on both object recognition and speech recognition tasks,using popular architectures including ResNet and ResNeXt. For object recognition, our approach reduces computation by 33% on object recognition while improving accuracy with 0.9%. Furthermore, our model surpasses state-of-the-art CNN acceleration approaches by a large margin in accuracy and FLOPs reduction. On the task of speech recognition, our proposed multi-scale CNNs save 30% FLOPs with slightly better word error rates, showing good generalization across domains. The codes are available at https://github.com/IBM/BigLittleNet.",110.0,,Open weights (unrestricted),United States of America,,,,,,2025-03-06 15:00,,,,,,Industry,,,,,Open source,"apache for code/weights: 
https://github.com/IBM/BigLittleNet",Industry,,,,,Operation counting,,,,
Big-Little Net (speech),Speech,Speech recognition,IBM,"Chun-Fu (Richard) Chen, Quanfu Fan, Neil Mallinar, Tom Sercu, Rogerio Feris",2018-07-10,Big-Little Net: An Efficient Multi-Scale Feature Representation for Visual and Speech Recognition,https://arxiv.org/abs/1807.03848,92.0,SOTA improvement,"""Furthermore, our model surpasses state-of-the-art CNN acceleration approaches by a large margin in accuracy and FLOPs reduction. On the task of speech recognition, our proposed multi-scale CNNs save 30% FLOPs with slightly better word error rates, showing good generalization across domains.""",3320000.0,table 3,4.290048e+17,980000000 (number of FLOPs from table 3) * 27360000 (dataset size) * 16 (number of epochs from appendix B.1) = 429004800000000000,"Switchboard,Fisher","""We train ResNet style acoustic models in the hybrid framework on Switchboard+Fisher (2000h) and provide results on Hub5 (Switchboard and Call Home portions). Switchboard is a large dataset with 2000 hours of transcribed speech from 28, 000 speakers""",27360000.0,"""We train ResNet style acoustic models in the hybrid framework on Switchboard+Fisher (2000h) and provide results on Hub5 (Switchboard and Call Home portions). Switchboard is a large dataset with 2000 hours of transcribed speech from 28, 000 speakers""

2000h * 13680 words per hour = 27360000

https://docs.google.com/document/d/1G3vvQkn4x_W71MKg0GmHVtzfd9m0y3_Ofcoew0v902Q/edit#heading=h.3pbt0hfgv7pq",,,,,Speculative,"In this paper, we propose a novel Convolutional Neural Network (CNN) architecture for learning multi-scale feature representations with good tradeoffs between speed and accuracy. This is achieved by using a multi-branch network, which has different computational complexity at different branches. Through frequent merging of features from branches at distinct scales, our model obtains multi-scale features while using less computation. The proposed approach demonstrates improvement of model efficiency and performance on both object recognition and speech recognition tasks,using popular architectures including ResNet and ResNeXt. For object recognition, our approach reduces computation by 33% on object recognition while improving accuracy with 0.9%. Furthermore, our model surpasses state-of-the-art CNN acceleration approaches by a large margin in accuracy and FLOPs reduction. On the task of speech recognition, our proposed multi-scale CNNs save 30% FLOPs with slightly better word error rates, showing good generalization across domains. The codes are available at https://github.com/IBM/BigLittleNet.",16.0,,Open weights (unrestricted),United States of America,,,,,,2025-03-06 15:00,,,,,,Industry,,,,,Open source,"apache for code/weights: 
https://github.com/IBM/BigLittleNet",Industry,,,,,Operation counting,,,,
RCAN,"Image generation,Vision",Image super-resolution,Northeastern University," Yulun Zhang, Kunpeng Li, Kai Li, Lichen Wang, Bineng Zhong, Yun Fu",2018-07-08,Image Super-Resolution Using Very Deep Residual Channel Attention Networks,https://openaccess.thecvf.com/content_ECCV_2018/html/Yulun_Zhang_Image_Super-Resolution_Using_ECCV_2018_paper.html,3889.0,Highly cited,,16000000.0,"""EDSR has much larger number of parameters (43 M) than ours
(16 M), but our RCAN obtains much better performance.""",,,DIV2K,,,,,,,,Unknown,"Convolutional neural network (CNN) depth is of crucial importance for image super-resolution (SR). However, we observe that deeper networks for image SR are more difficult to train. The low-resolution (LR) inputs and features contain abundant low-frequency information, which is treated equally across channels, hence hindering the representational ability of CNNs. To solve these problems, we propose the very deep residual channel attention networks (RCAN). Specifically, we propose residual in residual (RIR) structure to form very deep network, which consists of several residual groups with long skip connections. Each residual group contains some residual blocks with short skip connections. Meanwhile, RIR allows abundant low-frequency information to be bypassed through multiple skip connections, making the main network focus on learning high-frequency information. Furthermore, we propose channel attention mechanism to adaptively rescale channel-wise features by considering interdependencies among channels. Extensive experiments show that our RCAN achieves better accuracy and visual improvements against state-of-the-art methods.",,,,United States of America,,,,,,2025-05-28 16:16,,,,,,Academia,,,,,,,Academia,,,FP32,,,,,,
QT-Opt,"Robotics,Vision",Robotic manipulation,"Google Brain,University of California (UC) Berkeley","Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog, Eric Jang, Deirdre Quillen, Ethan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke, Sergey Levine",2018-06-27,QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation,https://arxiv.org/abs/1806.10293,1442.0,Highly cited,,1200000.0,"""The Q-function QÎ¸(s, a) is represented in our system by a large convolutional neural network with 1.2M parameters""",3.4875e+19,"""We distribute training across 10 GPUs, using asynchronous SGD with momentum... This system allows us to train the Q-function at 40 steps per second with a batch size of 32 across 10 NVIDIA P100 GPUs.""

""We found empirically that a large number of gradient steps (up to 15M) were needed to train an effective Q-function...""

15M steps * 0.025 seconds/step *  9.30E+12 FLOP/sec/GPU * 10 GPU = 3.4875E+19",,"""... we collected over 580k grasps over the course of several weeks across 7 robots""
",5984870.0,"Observations take up 4TB of disk space, and the input space is a 472x472 RGB image.

Assuming 24 bit depth color (8 bits per channel), that suggests 472 * 472 * 3 * 8 bits = 668.352 kB per image (this could be off by a factor of 2 depending on actual bit depth)

4 TB / 668.352 kB = 5,984,870 images; around 10 per grasp attempt.

15M gradient steps with batchsize 32 implies:
15M steps * 32 images/step * 1/5984870 images ~= each image seen 80 times",104.2,"""We distribute training across 10 GPUs, using asynchronous SGD with momentum... This system allows us to train the Q-function at 40 steps per second with a batch size of 32 across 10 NVIDIA P100 GPUs.""

""We found empirically that a large number of gradient steps (up to 15M) were needed to train an effective Q-function...""

15M steps * 0.025 seconds/step *  1/3600 hours/second = 104.2 hours",NVIDIA P100,Reinforcement learning,Likely,"In this paper, we study the problem of learning vision-based dynamic manipulation skills using a scalable reinforcement learning approach. We study this problem in the context of grasping, a longstanding challenge in robotic manipulation. In contrast to static learning behaviors that choose a grasp point and then execute the desired grasp, our method enables closed-loop vision-based control, whereby the robot continuously updates its grasp strategy based on the most recent observations to optimize long-horizon grasp success. To that end, we introduce QT-Opt, a scalable self-supervised vision-based reinforcement learning framework that can leverage over 580k real-world grasp attempts to train a deep neural network Q-function with over 1.2M parameters to perform closed-loop, real-world grasping that generalizes to 96% grasp success on unseen objects. Aside from attaining a very high success rate, our method exhibits behaviors that are quite distinct from more standard grasping systems: using only RGB vision-based perception from an over-the-shoulder camera, our method automatically learns regrasping strategies, probes objects to find the most effective grasps, learns to reposition objects and perform other non-prehensile pre-grasp manipulations, and responds dynamically to disturbances and perturbations.",80.0,,Unreleased,"United States of America,United States of America",,,,,,2025-05-09 11:32,,,,,,"Industry,Academia",,,,,,There is a public implementation of the architecture at https://github.com/quantumiracle/QT_Opt (not from any of the paper's co-authors),"Industry,Academia",$1317.80,,,,Hardware,,,,
S + I-Attention (3),Language,Language modeling,"National Research University Higher School of Economics,Samsung R&D Institute Russia","Artyom Gadetsky, Ilya Yakubovskiy, Dmitry Vetrov",2018-06-26,Conditional Generators of Words Definitions,https://arxiv.org/abs/1806.10090,63.0,,,,,,,Oxford Dictionary,,,,,,,,Unknown,"We explore recently introduced definition modeling technique that provided the tool for evaluation of different distributed vector representations of words through modeling dictionary definitions of words. In this work, we study the problem of word ambiguities in definition modeling and propose a possible solution by employing latent variable modeling and soft attention mechanisms. Our quantitative and qualitative evaluation and analysis of the model shows that taking into account words ambiguity and polysemy leads to performance improvement.",35.0,S + I-Attention (3),Unreleased,"Russia,Russia",,,,,,2024-11-01 10:02,,,,,,"Academia,Industry",,,,,Unreleased,,"Academia,Industry",,,,,,,,,
DARTS,Language,"Language modeling,Neural Architecture Search - NAS","DeepMind,Carnegie Mellon University (CMU)","Hanxiao Liu, Karen Simonyan, Yiming Yang",2018-06-24,DARTS: Differentiable Architecture Search,https://arxiv.org/abs/1806.09055,4045.0,Highly cited,,33000000.0,"33M (Table 4) - parameters reported for PTB, but they say it is the same for WT-2

"" WIKITEXT-2
We use embedding and hidden sizes 700, weight decay 5Ã—10âˆ’7, and hidden-node variational dropout 0.15. Other hyperparameters remain the same as in our PTB experiments.""",3.2366286e+17,"6 FLOP / parameter / token * 33000000 parameters * 2000000 tokens * 300 epochs = 1.188e+17 FLOP

11340000000000 FLOP / second / GPU * 1 GPU * 72 hours * 3600 sec / hour * 0.3 [assumed utilization] = 8.817984e+17 FLOP

sqrt(1.188e+17*8.817984e+17) = 3.2366286e+17 FLOP

'Speculative' confidence since many variables are assumed based on PTB model training",WikiText-2,,2000000.0,300 epochs for PTB (supposedly the same for WT-2,72.0,"for PTB (they say WT-2 training is similar): The training takes 3 days on a single 1080Ti GPU with our PyTorch implementation

3*24 = 72 hours",NVIDIA GeForce GTX 1080 Ti,,Speculative,"This paper addresses the scalability challenge of architecture search by formulating the task in a differentiable manner. Unlike conventional approaches of applying evolution or reinforcement learning over a discrete and non-differentiable search space, our method is based on the continuous relaxation of the architecture representation, allowing efficient search of the architecture using gradient descent. Extensive experiments on CIFAR-10, ImageNet, Penn Treebank and WikiText-2 show that our algorithm excels in discovering high-performance convolutional architectures for image classification and recurrent architectures for language modeling, while being orders of magnitude faster than state-of-the-art non-differentiable techniques. Our implementation has been made publicly available to facilitate further research on efficient architecture search algorithms.",300.0,DARTS,Unreleased,"United Kingdom of Great Britain and Northern Ireland,United States of America",,,,1.0,,2025-04-14 15:52,,,,,,"Industry,Academia",,,,,Open source,"apache 2, training/test for wikitext: https://github.com/quark0/darts/tree/master/rnn ","Industry,Academia",,,,284.6899784455654,"Operation counting,Hardware",,,,
Relational Memory Core,Language,Language modeling,"DeepMind,University College London (UCL)","Adam Santoro, Ryan Faulkner, David Raposo, Jack Rae, Mike Chrzanowski, Theophane Weber, Daan Wierstra, Oriol Vinyals, Razvan Pascanu, Timothy Lillicrap",2018-06-05,Relational recurrent neural networks,https://arxiv.org/abs/1806.01822,235.0,SOTA improvement,"""Finally, we test the RMC on a suite of tasks that may profit from more capable relational reasoning across sequential information, and show large gains in RL domains (e.g. Mini PacMan), program evaluation, and language modeling, achieving state-of-the-art results on the WikiText-103, Project Gutenberg, and GigaWord datasets.""",,,,,WikiText-103,,,,,,,,Unknown,"Memory-based neural networks model temporal data by leveraging an ability to remember information for long periods. It is unclear, however, whether they also have an ability to perform complex relational reasoning with the information they remember. Here, we first confirm our intuitions that standard memory architectures may struggle at tasks that heavily involve an understanding of the ways in which entities are connected -- i.e., tasks involving relational reasoning. We then improve upon these deficits by using a new memory module -- a \textit{Relational Memory Core} (RMC) -- which employs multi-head dot product attention to allow memories to interact. Finally, we test the RMC on a suite of tasks that may profit from more capable relational reasoning across sequential information, and show large gains in RL domains (e.g. Mini PacMan), program evaluation, and language modeling, achieving state-of-the-art results on the WikiText-103, Project Gutenberg, and GigaWord datasets.",,Relational Memory Core,Unreleased,"United Kingdom of Great Britain and Northern Ireland,United Kingdom of Great Britain and Northern Ireland",,,,,,2024-09-05 14:08,,,,,,"Industry,Academia",,,,,Unreleased,"looks like code for the architecture, but not experiment code: https://github.com/google-deepmind/sonnet/blob/v1/sonnet/python/modules/relational_memory.py ","Industry,Academia",,,,,,,,,
GPT-1,Language,"Question answering,Text classification,Language modeling",OpenAI,"A Radford, K Narasimhan, T Salimans, I Sutskever",2018-06-01,Improving Language Understanding by Generative Pre-Training,https://openai.com/blog/language-unsupervised/,10079.0,Highly cited,,117000000.0,"""The model had 117M parameters in total.""

source: https://medium.com/walmartglobaltech/the-journey-of-open-ai-gpt-models-32d95b7b7fb2",1.7578125e+19,"COMPUTE = FORWARD COMPUTE PER TOKEN * 3 BACKWARD FORWARD ADJUSTMENT * EPOCHS * DATASET SIZE

""We train for 100 epochs on minibatches of 64 randomly sampled, contiguous sequences of 512 tokens.""

Authors of ""AI and Memory Wall"" estimated model's training compute as 57,000 PFLOPS = 5.7*10^19 FLOP
(https://github.com/amirgholami/ai_and_memory_wall)","""BookCorpus (BooksCorpus, Toronto Book Corpus)""","""We use the BooksCorpus dataset [71] for training the language model""

Searched for â€œGPT-1 knowledge cutoff date,â€ came to https://github.com/HaoooWang/llm-knowledge-cutoff-dates first, which led to https://computercity.com/artificial-intelligence/knowledge-cutoff-dates-llms.",1000000000.0,"""BookCorpus is a large collection of free novel books written by unpublished authors, which contains 11,038 books (around 74M sentences and 1G words) of 16 different sub-genres (e.g., Romance, Historical, Adventure, etc.).""
https://paperswithcode.com/dataset/bookcorpus

BookCorpus seems to have about 5000MB of content
source: https://huggingface.co/datasets/bookcorpusopen

Assuming a byte-pair encoder similar to GPT-2, there are 8 bytes / token.

So approximately 5000MB / 8 bytes / token = 5e9 / 8 tokens",720.0,"""1 month on 8 GPUs."" from the reference link",NVIDIA Quadro P600,Self-supervised learning,Likely,"Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classification. Although large unlabeled text corpora are abundant, labeled data for learning these specific tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task. In contrast to previous approaches, we make use of task-aware input transformations during fine-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures specifically crafted for each task, significantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9% on commonsense reasoning (Stories Cloze Test), 5.7% on question answering (RACE), and 1.5% on textual entailment (MultiNLI).",,,Open weights (unrestricted),United States of America,,,,8.0,,2025-05-13 13:32,,,,,,Industry,,,,,Open source,"MIT, code and weights
https://github.com/openai/finetune-transformer-lm/blob/master/LICENSE",Industry,,,,663.5535559333689,"Operation counting,Third-party estimation",,,,
RHN+HSG(depth=40),Language,Language modeling,Ben-Gurion University,"Ron Shoham, Haim Permuter",2018-05-23,Highway State Gating for Recurrent Highway Networks: improving information flow through time,https://arxiv.org/abs/1805.09238,0.0,,,,,,,,,,,,,,,Unknown,"Recurrent Neural Networks (RNNs) play a major role in the field of sequential learning, and have outperformed traditional algorithms on many benchmarks. Training deep RNNs still remains a challenge, and most of the state-of-the-art models are structured with a transition depth of 2-4 layers. Recurrent Highway Networks (RHNs) were introduced in order to tackle this issue. These have achieved state-of-the-art performance on a few benchmarks using a depth of 10 layers. However, the performance of this architecture suffers from a bottleneck, and ceases to improve when an attempt is made to add more layers. In this work, we analyze the causes for this, and postulate that the main source is the way that the information flows through time. We introduce a novel and simple variation for the RHN cell, called Highway State Gating (HSG), which allows adding more layers, while continuing to improve performance. By using a gating mechanism for the state, we allow the net to ""choose"" whether to pass information directly through time, or to gate it. This mechanism also allows the gradient to back-propagate directly through time and, therefore, results in a slightly faster convergence. We use the Penn Treebank (PTB) dataset as a platform for empirical proof of concept. Empirical results show that the improvement due to Highway State Gating is for all depths, and as the depth increases, the improvement also increases.",300.0,RHN+HSG(depth=40),Unreleased,Israel,,,,,,2024-09-05 14:08,,,,,,Academia,,,,,Unreleased,,Academia,,,,,,,,,
RHN(depth=40),Language,Language modeling,Ben-Gurion University,"Ron Shoham, Haim Permuter",2018-05-23,Highway State Gating for Recurrent Highway Networks: improving information flow through time,https://arxiv.org/abs/1805.09238,0.0,,,,,,,,,,,,,,,Unknown,"Recurrent Neural Networks (RNNs) play a major role in the field of sequential learning, and have outperformed traditional algorithms on many benchmarks. Training deep RNNs still remains a challenge, and most of the state-of-the-art models are structured with a transition depth of 2-4 layers. Recurrent Highway Networks (RHNs) were introduced in order to tackle this issue. These have achieved state-of-the-art performance on a few benchmarks using a depth of 10 layers. However, the performance of this architecture suffers from a bottleneck, and ceases to improve when an attempt is made to add more layers. In this work, we analyze the causes for this, and postulate that the main source is the way that the information flows through time. We introduce a novel and simple variation for the RHN cell, called Highway State Gating (HSG), which allows adding more layers, while continuing to improve performance. By using a gating mechanism for the state, we allow the net to ""choose"" whether to pass information directly through time, or to gate it. This mechanism also allows the gradient to back-propagate directly through time and, therefore, results in a slightly faster convergence. We use the Penn Treebank (PTB) dataset as a platform for empirical proof of concept. Empirical results show that the improvement due to Highway State Gating is for all depths, and as the depth increases, the improvement also increases.",300.0,RHN(depth=40),Unreleased,Israel,,,,,,2024-09-05 14:08,,,,,,Academia,,,,,Unreleased,,Academia,,,,,,,,,
aLSTM(depth-2)+RecurrentPolicy (WT2),Language,Language modeling,"University of Manchester,Alan Turing Institute","Sebastian Flennerhag, Hujun Yin, John Keane, Mark Elliot",2018-05-22,Breaking the Activation Function Bottleneck through Adaptive Parameterization,https://arxiv.org/abs/1805.08574,12.0,SOTA improvement,"""Without tuning for WT2, both outperform previously published results in 150 epochs (table 3) and converge to new state of the art performance in 190 epochs""",32000000.0,32M (Table 3),7.296e+16,6 FLOP / token / parameter * 32000000 parameters * 2000000 tokens * 190 epochs = 7.296e+16 FLOP,WikiText-2,,2000000.0,"""Both models are trained for 10 000 steps with a batch size of
50 and a learning rate of 0.003.""

""Without tuning for WT2, both outperform previously published results in 150 epochs (table 3) and converge to new state of the art performance in 190 epochs""",,,,,Confident,"Standard neural network architectures are non-linear only by virtue of a simple element-wise activation function, making them both brittle and excessively large. In this paper, we consider methods for making the feed-forward layer more flexible while preserving its basic structure. We develop simple drop-in replacements that learn to adapt their parameterization conditional on the input, thereby increasing statistical efficiency significantly. We present an adaptive LSTM that advances the state of the art for the Penn Treebank and WikiText-2 word-modeling tasks while using fewer parameters and converging in less than half as many iterations.",190.0,aLSTM(depth-2)+RecurrentPolicy (WT2),Unreleased,"United Kingdom of Great Britain and Northern Ireland,United Kingdom of Great Britain and Northern Ireland",,,,,,2025-04-14 15:57,,,,,,"Academia,Government",,,,,Open source,BSD-3 license: https://github.com/flennerhag/alstm/tree/master/examples ,"Academia,Government",,,,,Operation counting,,,,
Dropout-LSTM+Noise(Bernoulli) (WT2),Language,Language modeling,"Columbia University,New York University (NYU),Princeton University","Adji B. Dieng, Rajesh Ranganath, Jaan Altosaar, David M. Blei",2018-05-03,Noisin: Unbiased Regularization for Recurrent Neural Networks,https://arxiv.org/abs/1805.01500,26.0,SOTA improvement,"this is the best model in this paper per Table 4
""On language modeling benchmarks, Noisin improves over dropout by as much as 12.2% on the Penn Treebank and 9.4% on the Wikitext-2 dataset""",51000000.0,"""The large network has 2 layers with 1500 hidden units each. This leads to a model complexity of 51 million parameters.""",1.27e+17,"6 FLOP / parameter / token * 51000000 parameters * 2000000 tokens * 200 epochs = 1.224e+17 FLOP

'Likely' confidence because I am not very sure that 51M paramters and 200 epochs relate to WT-2 model, but it is very likely",WikiText-2,,2000000.0,"""We train the models using truncated backpropagation through time with average stochastic gradient descent (Polyak & Juditsky, 1992) for a maximum of 200 epochs""",,,,,Likely,"Recurrent neural networks (RNNs) are powerful models of sequential data. They have been successfully used in domains such as text and speech. However, RNNs are susceptible to overfitting; regularization is important. In this paper we develop Noisin, a new method for regularizing RNNs. Noisin injects random noise into the hidden states of the RNN and then maximizes the corresponding marginal likelihood of the data. We show how Noisin applies to any RNN and we study many different types of noise. Noisin is unbiased--it preserves the underlying RNN on average. We characterize how Noisin regularizes its RNN both theoretically and empirically. On language modeling benchmarks, Noisin improves over dropout by as much as 12.2% on the Penn Treebank and 9.4% on the Wikitext-2 dataset. We also compared the state-of-the-art language model of Yang et al. 2017, both with and without Noisin. On the Penn Treebank, the method with Noisin more quickly reaches state-of-the-art performance.",200.0,Dropout-LSTM+Noise(Bernoulli) (WT2),Unreleased,"United States of America,United States of America,United States of America",,,,,,2025-05-28 16:16,,,,,,"Academia,Academia,Academia",,,,,Unreleased,"""The source code is available upon request.""","Academia,Academia,Academia",,,FP32,,Operation counting,,,,
LSTM+Noise(Beta),Language,Language modeling,"Columbia University,New York University (NYU),Princeton University","Adji B. Dieng, Rajesh Ranganath, Jaan Altosaar, David M. Blei",2018-05-03,Noisin: Unbiased Regularization for Recurrent Neural Networks,https://arxiv.org/abs/1805.01500,26.0,,not the best model in this paper,51000000.0,,1.27e+17,"""6 FLOP / parameter / token * 51000000 parameters * 2000000 tokens * 200 epochs = 1.224e+17 FLOP

'Likely' confidence because I am not very sure that 51M paramters and 200 epochs relate to WT-2 model, but it is very likely""",WikiText-2,,2000000.0,,,,,,Likely,"Recurrent neural networks (RNNs) are powerful models of sequential data. They have been successfully used in domains such as text and speech. However, RNNs are susceptible to overfitting; regularization is important. In this paper we develop Noisin, a new method for regularizing RNNs. Noisin injects random noise into the hidden states of the RNN and then maximizes the corresponding marginal likelihood of the data. We show how Noisin applies to any RNN and we study many different types of noise. Noisin is unbiased--it preserves the underlying RNN on average. We characterize how Noisin regularizes its RNN both theoretically and empirically. On language modeling benchmarks, Noisin improves over dropout by as much as 12.2% on the Penn Treebank and 9.4% on the Wikitext-2 dataset. We also compared the state-of-the-art language model of Yang et al. 2017, both with and without Noisin. On the Penn Treebank, the method with Noisin more quickly reaches state-of-the-art performance.",200.0,LSTM+Noise(Beta),Unreleased,"United States of America,United States of America,United States of America",,,,,,2025-04-16 13:25,,,,,,"Academia,Academia,Academia",,,,,Unreleased,,"Academia,Academia,Academia",,,,,Operation counting,,,,
ResNeXt-101 32x48d,Vision,Image classification,Facebook,"Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin Bharambe, Laurens van der Maaten",2018-05-02,Exploring the Limits of Weakly Supervised Pretraining,https://arxiv.org/abs/1805.00932,1300.0,"Highly cited,SOTA improvement","""We show improvements on several image classification and object detection tasks, and report the highest ImageNet-1k single-crop, top-1 accuracy to date: 85.4%",829000000.0,"Table 6
",8.74395e+21,"Table 6: 153e9 mult-adds.
Section 2.4: ""minibatches of 8,064 images"".

Compute = 2 * 3 * mult-adds * dataset size = 2 * 3 * 153e9 * 9525e6 = 8.74e21 FLOP

Likely trained on V100s, since Facebook had just upgraded their Big Basin GPU cluster to V100s as of March 2018. The previous iteration of Big Basin had 32 clusters of 8xP100s, while Big Basin v2 had 42 clusters of 8xV100s, which matches the 336 GPUs used in this paper.","ImageNet,Instagram","Instagram images, captioned with hashtags",9525000000.0,Table 3: (300+1925+300+7000) million images,496.0,"""Mahajan et al. (2018) required 19 GPU years to train their ResNeXt101-32x48d"" https://arxiv.org/abs/2103.00020
Models were trained on 336 GPUs, so that suggests 20.65 days or 496 hours",NVIDIA V100,,Confident,,,,Open weights (non-commercial),United States of America,,,,336.0,,2025-02-28 20:38,,,,,,Industry,,,,,Unreleased,"models, non-commercial: https://github.com/facebookresearch/WSL-Images ",Industry,,,,209159.05867416784,Operation counting,,,,4
TF-LM-discourse LSTM (WT2),Language,Language modeling,ESAT - PSI,"Lyan Verwimp, Hugo Van hamme, Patrick Wambacq",2018-05-01,TF-LM: TensorFlow-based Language Modeling Toolkit,https://aclanthology.org/L18-1470.pdf,12.0,,,,,,,,,,,,,,,Unknown,"Recently, an abundance of deep learning toolkits has been made freely available. These toolkits typically offer the building blocks and sometimes simple example scripts, but designing and training a model still takes a considerable amount of time and knowledge. We present language modeling scripts based on TensorFlow that allow one to train and test competitive models directly, by using a pre-defined configuration or changing it to their needs. There are several options for input features (words, characters, words combined with characters, character n-grams) and for batching (sentence- or discourse-level). The models can be used to test the perplexity, predict the next word(s), re-score hypotheses or generate debugging files for interpolation with n-gram models. Additionally, we make available LSTM language models trained on a variety of Dutch texts and English benchmarks, that can be used immediately, thereby avoiding the time and computationally expensive training process. The toolkit is open source and can be found at https://github.com/lverwimp/tf-lm",39.0,TF-LM-discourse LSTM (WT2),Open weights (unrestricted),Belgium,,,,,,2024-09-05 14:08,,,,,,Academia,,,,,Open source,"code and weights, MIT license: https://github.com/lverwimp/tf-lm ",Academia,,,,,,,,,
TF-LM-discourse LSTM (PTB),Language,Language modeling,ESAT - PSI,"Lyan Verwimp, Hugo Van hamme, Patrick Wambacq",2018-05-01,TF-LM: TensorFlow-based Language Modeling Toolkit,https://aclanthology.org/L18-1470.pdf,12.0,,,,,,,,,,,,,,,Unknown,,39.0,TF-LM-discourse LSTM (PTB),Open weights (unrestricted),Belgium,,,,,,2024-09-05 14:08,,,,,,Academia,,,,,Open source,"code and weights, MIT license: https://github.com/lverwimp/tf-lm ",Academia,,,,,,,,,
DNCON2,Biology,"Proteins,Protein folding prediction",University of Missouri,"Badri Adhikari, Jie Hou, Jianlin Cheng",2018-05-01,DNCON2: improved protein contact prediction using two-level deep convolutional neural networks,https://academic.oup.com/bioinformatics/article/34/9/1466/4708303?login=false,173.0,,,,,9.5e+16,"""Our training was conducted on Tesla K20 Nvidia GPUs each having 5 GB of GPU memory, on which, training one model took around 12â€‰h.""

""We train each CNN for a total of 1600 epochs with each epoch of training taking around 2â€‰min.""

Assumptions:
peakFLOP rate 3.52e12FLOP/s (from: https://www.techpowerup.com/gpu-specs/tesla-k20c.c564)
30% utilization rate
1 GPU

Estimate 1: ""training one model took around 12h"" => unclear how many GPUs
(12 *3600) s * 3.52e12 FLOP/s * 0.3 = 4.5e16 FLOP

Estimate 2: ""We train each CNN for a total of 1600 epochs with each epoch of training taking around 2 min.""
(1600 epochs * 2 min/epoch * 60 s/min) * 3.52e12 FLOP/s * 0.3 =  2e17 FLOP

Geometric mean: 9.5e16",PDB (Protein Data Bank),"""We used the original DNCON dataset consisting of 1426 proteins having length between 30 and 300 residues curated before the CASP10 experiment to train and test DNCON2. The protein structures in the dataset were obtained from the Protein Data Bank (PDB)""",1426.0,"""Our raw feature files for all 1426 training proteins""",12.0,"""training one model took around 12â€‰h""",,,Likely,"Motivation
Significant improvements in the prediction of protein residueâ€“residue contacts are observed in the recent years. These contacts, predicted using a variety of coevolution-based and machine learning methods, are the key contributors to the recent progress in ab initio protein structure prediction, as demonstrated in the recent CASP experiments. Continuing the development of new methods to reliably predict contact maps is essential to further improve ab initio structure prediction.

Results
In this paper we discuss DNCON2, an improved protein contact map predictor based on two-level deep convolutional neural networks. It consists of six convolutional neural networksâ€”the first five predict contacts at 6, 7.5, 8, 8.5 and 10â€‰Ã… distance thresholds, and the last one uses these five predictions as additional features to predict final contact maps. On the free-modeling datasets in CASP10, 11 and 12 experiments, DNCON2 achieves mean precisions of 35, 50 and 53.4%, respectively, higher than 30.6% by MetaPSICOV on CASP10 dataset, 34% by MetaPSICOV on CASP11 dataset and 46.3% by Raptor-X on CASP12 dataset, when top L/5 long-range contacts are evaluated. We attribute the improved performance of DNCON2 to the inclusion of short- and medium-range contacts into training, two-level approach to prediction, use of the state-of-the-art optimization and activation functions, and a novel deep learning architecture that allows each filter in a convolutional layer to access all the input features of a protein of arbitrary length.",1600.0,,Open weights (unrestricted),United States of America,,,,,,2025-04-30 10:04,,,,,,Academia,,,,,Open source,license: https://github.com/multicom-toolbox/DNCON2?tab=GPL-3.0-1-ov-file#readme,Academia,,,,,Hardware,,,,
RNNLM + Dynamic KL Regularization (WT2),Language,Language modeling,Northwestern University,"Thanapon Noraset, David Demeter, Doug Downey",2018-04-27,Controlling Global Statistics in Recurrent Neural Network Text Generation,https://ojs.aaai.org/index.php/AAAI/article/view/11993,8.0,,,87600000.0,Calculatable - 2-layer LSTM with 650 hidden units.,2.1024e+16,6 FLOP / parameter / token * 87600000 parameters * 2000000 tokens * 20 epochs = 2.1024e+16 FLOP,WikiText-2,,2000000.0,"""Perplexity validation stops significantly improving after around 20 epochs.""",,,,,Likely,"Recurrent neural network language models (RNNLMs) are an essential component for many language generation tasks such as machine translation, summarization, and automated conversation. Often, we would like to subject the text generated by the RNNLM to constraints, in order to overcome systemic errors (e.g. word repetition) or achieve application-specific goals (e.g. more positive sentiment). In this paper, we present a method for training RNNLMs to simultaneously optimize likelihood and follow a given set of statistical constraints on text generation.  The problem is challenging because the statistical constraints are defined over aggregate model behavior, rather than model parameters, meaning that a straightforward parameter regularization approach is insufficient.  We solve this problem using a dynamic regularizer that updates as training proceeds, based on the generative behavior of the RNNLMs.  Our experiments show that the dynamic regularizer outperforms both generic training and a static regularization baseline.  The approach is successful at improving word-level repetition statistics by a factor of four in RNNLMs on a definition modeling task.  It also improves model perplexity when the statistical constraints are $n$-gram statistics taken from a large corpus.",20.0,RNNLM + Dynamic KL Regularization (WT2),Unreleased,United States of America,,,,,,2025-04-16 13:41,,,,,,Academia,,,,,Unreleased,"code, but not experiment code: https://github.com/northanapon/seqmodel/tree/aaai18 ",Academia,,,,,Operation counting,,,,
RNMT+,Language,Translation,Google AI,"Mia Xu Chen, Orhan Firat, Ankur Bapna, Melvin Johnson, Wolfgang Macherey, George Foster, Llion Jones, Niki Parmar, Mike Schuster, Zhifeng Chen, Yonghui Wu, Macduff Hughes",2018-04-26,The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation,https://arxiv.org/abs/1804.09849,504.0,,,378900000.0,from Table 3 RNMT+ ,1.83e+19,"32 * 9.3 TFLOPS * (120 * 3600) * 0.3 = 3.86e19
(number of gpus) * (peak flops) * (seconds) * (assumed utilization rate)
""All models were trained with synchronous training. RNMT+ and ConvS2S were trained with 32 NVIDIA P100 GPUs"", training time is 120h from Table 1.

Alternatively, typical inference is 2.81e10 FLOP, there are 36.3M examples in the dataset, and they train for 8.5 epochs: 2.81e10 * 36.3M * 8.5 = 8.7e18

Geometric mean: sqrt(3.86e19 * 8.7e18) = 1.83e19",WMT14,"""Table 1 shows our results on the WMTâ€™14 Enâ†’Fr task.""",36300000.0,"""We train our models on the standard WMTâ€™14 Enâ†’Fr and Enâ†’De datasets that comprise 36.3M and 4.5M sentence pairs, respectively""
""For RNMT+, we use sentence-level cross entropy loss.""",120.0,from Table 1,NVIDIA P100,Supervised,Likely,"The past year has witnessed rapid advances in sequence-to-sequence (seq2seq) modeling for Machine Translation (MT). The classic RNN-based approaches to MT were first out-performed by the convolutional seq2seq model, which was then out-performed by the more recent Transformer model. Each of these new approaches consists of a fundamental architecture accompanied by a set of modeling and training techniques that are in principle applicable to other seq2seq architectures. In this paper, we tease apart the new architectures and their accompanying techniques in two ways. First, we identify several key modeling and training techniques, and apply them to the RNN architecture, yielding a new RNMT+ model that outperforms all of the three fundamental architectures on the benchmark WMT'14 English to French and English to German tasks. Second, we analyze the properties of each fundamental seq2seq architecture and devise new hybrid architectures intended to combine their strengths. Our hybrid models obtain further improvements, outperforming the RNMT+ model on both benchmark datasets. ",8.5,,Unreleased,"Multinational,United States of America",,,,32.0,,2025-05-09 11:32,,,,,,Industry,,,,3840.0,Unreleased,,Industry,,,,16602.143459185685,"Hardware,Operation counting",,,,
Diffractive Deep Neural Network,Vision,Digit recognition,University of California Los Angeles (UCLA),"Xing Lin, Yair Rivenson, Nezih T Yardimci, Muhammed Veli, Yi Luo, Mona Jarrahi, and Aydogan Ozcan",2018-04-14,All-Optical Machine Learning Using Diffractive Deep Neural Networks,https://arxiv.org/abs/1804.08711,1464.0,Highly cited,,8000000000.0,"""For example, using five 3D-printed transmission layers, containing a total of 0.2 million neurons and ~8.0 billion connections that are trained using deep learning, we experimentally demonstrated the function of a handwritten digit classifier.""

My understanding is that every connection correspond to the parameter to learn.",,,MNIST,"""For this task, phase-only transmission masks were designed by training a 5-layer D2NN with ~55,000 images from MNIST handwritten digit database (14). """,55000.0,"size of MNIST
""For this task, phase-only transmission masks were designed by training a 5-layer D2NN with ~55,000 images from MNIST handwritten digit database (14). """,,,,Supervised,Likely,"We introduce an all-optical Diffractive Deep Neural Network (D2NN) architecture that can learn to implement various functions after deep learning-based design of passive diffractive layers that work collectively. We experimentally demonstrated the success of this framework by creating 3D-printed D2NNs that learned to implement handwritten digit classification and the function of an imaging lens at terahertz spectrum. With the existing plethora of 3D-printing and other lithographic fabrication methods as well as spatial-light-modulators, this all-optical deep learning framework can perform, at the speed of light, various complex functions that computer-based neural networks can implement, and will find applications in all-optical image analysis, feature detection and object classification, also enabling new camera designs and optical components that can learn to perform unique tasks using D2NNs.",,,,United States of America,,,,,,2025-05-09 11:32,,,,,,Academia,,,,,,,Academia,,,,,,,,,
YOLOv3,Vision,Object detection,University of Washington,"Joseph Redmon, Ali Farhadi",2018-04-08,YOLOv3: An Incremental Improvement,https://arxiv.org/abs/1804.02767,19207.0,Highly cited,,56933216.0,"Feature extractor (ignoring biases)
32*3*3*3 +
64*3*3*32 +
32*1*1*64 +
64*3*3*32 +
128*3*3*64 +
2*(64*1*1*128 +
128*3*3*64) +
256*3*3*128 +
8*(128*1*1*256 +
256*3*3*128) +
512*3*3*256 + 
8*(256*1*1*512 + 
512*3*3*256) + 
1024*3*3*512 + 
4*(512*1*1*1024 +
1024*3*3*512) +
4*4*1024*1000

source: table 1
This is assuming the average pooling step changes the output size from 8x8 to 4x4.

The weights file is 237MB. If the weights are saved as float32, 4 bytes per weight, then there are approximately 237M/4=59M parameters, consistent with the calculation above.",1.3416380823999998e+19,"We use the formula training_compute = ops_per_forward_pass * 3.5 * n_epochs * n_examples

Assuming 160 epochs of training as in https://arxiv.org/pdf/1612.08242.pdf

Table 2: 18700000000 operations 

18700000000 ops * 3.5 *160 epochs * 1281167 images",ImageNet,,1281167.0,Source: https://image-net.org/download.php,,,"NVIDIA M40,NVIDIA GeForce GTX TITAN X",,Likely,"We present some updates to YOLO! We made a bunch of little design changes to make it better. We also trained this new network that's pretty swell. It's a little bigger than last time but more accurate. It's still fast though, don't worry. At 320x320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but three times faster. When we look at the old .5 IOU mAP detection metric YOLOv3 is quite good. It achieves 57.9 mAP@50 in 51 ms on a Titan X, compared to 57.5 mAP@50 in 198 ms by RetinaNet, similar performance but 3.8x faster. As always, all the code is online at this https URL",,,Unreleased,United States of America,,,,,,2025-06-18 05:30,,,,,,Academia,,,,,Unreleased,"code and weights, unclear license: https://pjreddie.com/darknet/yolo/ ",Academia,,,,,Operation counting,,,,
"LSTM (Hebbian, Cache, MbPA)",Language,Language modeling,"DeepMind,University College London (UCL)","Jack W Rae, Chris Dyer, Peter Dayan, Timothy P Lillicrap",2018-03-27,Fast Parametric Learning with Activation Memorization,https://arxiv.org/abs/1803.10049,46.0,SOTA improvement,"""We also show improved performance for word-based language models on news reports (GigaWord), books (Project Gutenberg) and Wikipedia articles (WikiText-103) --- the latter achieving a state-of-the-art perplexity of 29.2.""",530442240.0,"Single layer LSTM with hidden dimension of 2048. Vocabulary for Gutenberg is 242,621; input and output embeddings are tied.

Embedding layer (tied): 242,621 * 2048 = 496,887,808
LSTM layer: 4 * (2048 + 2048) * 2048 = 33,554,432

Total: 496,887,808 + 33,554,432 = 530,442,240",3.33e+19,"They do training runs on a vision task and three language datasets. The largest dataset by size is GigaWord, but the largest training run is on the Gutenberg dataset, at 15B tokens. 

I assume the input embedding is done with an embedding lookup for efficiency rather than a dense matrix multiplication, so we only count FLOPs on the de-embedding.

Ops counting:
6 * 15B * 530,442,240 = 4.774e19

Hardware:
(8 * 1.87e13) * (6 * 24 * 3600) * 0.3 = 2.327e19

Geometric mean: sqrt(4.774e19 * 2.327e19) = 3.33e19

(Note they say 15B steps, but they also say it is 80 epochs on a 175M token dataset, and that it took 6 days on 8 P100s, both of which would agree with 15B tokens, not 15B steps)",Project Gutenberg,"They also do training runs on Omniglot, Wikitext-103, and GigaWord.",175181505.0,"Omniglot: 32k images
Wikitext-103: ""Over 100 million tokens""
Gutenberg: 175,181,505 tokens
GigaWord v5: 4B tokens

Gigaword is the largest dataset, but the largest training run uses Project Gutenberg.",144.0,6 days,NVIDIA P100,,Confident,"Neural networks trained with backpropagation often struggle to identify classes that have been observed a small number of times. In applications where most class labels are rare, such as language modelling, this can become a performance bottleneck. One potential remedy is to augment the network with a fast-learning non-parametric model which stores recent activations and class labels into an external memory. We explore a simplified architecture where we treat a subset of the model parameters as fast memory stores. This can help retain information over longer time intervals than a traditional memory, and does not require additional space or compute. In the case of image classification, we display faster binding of novel classes on an Omniglot image curriculum task. We also show improved performance for word-based language models on news reports (GigaWord), books (Project Gutenberg) and Wikipedia articles (WikiText-103) --- the latter achieving a state-of-the-art perplexity of 29.2.",80.0,"""LSTM (Hebbian, Cache, MbPA)""",Unreleased,"United Kingdom of Great Britain and Northern Ireland,United Kingdom of Great Britain and Northern Ireland",,,,8.0,,2025-02-17 12:41,,,,51200.0,"Sequence length of 100, total of 512 batches. Batches are split between 8 GPUs.","Industry,Academia",,,,,Unreleased,,"Industry,Academia",$590.53,,,4153.309685987008,"Hardware,Operation counting",,,,20
4 layer QRNN (h=2500),Language,Language modeling,Salesforce Research,"Stephen Merity, Nitish Shirish Keskar, Richard Socher",2018-03-22,An Analysis of Neural Language Modeling at Multiple Scales,https://arxiv.org/abs/1803.08240,183.0,SOTA improvement,"""QRNNs achieve stateof-the-art results on character-level (Penn Treebank, enwik8) and word-level (WikiText-103)
datasets, respectively""",151000000.0,Table 6,5.9158815e+17,"20670000000000 FLOP / sec / GPU [fp16 assumed] * 1 GPU * 12 hours * 3600 sec / hour * 0.3 [assumed utilization] = 2.678832e+17FLOP

6 FLOP / token / parameter * 151000000 parameters * 103000000 tokens * 14 epochs = 1.306452e+18 FLOP

sqrt(2.678832e+17*1.306452e+18) = 5.9158815e+17 FLOP
__________________
in the algorithmic progress paper the estimation was 2.4 Ã— 10^17 FLOP under assumption of 26M parameters",WikiText-103,,103000000.0,"""The model was trained for 12 hours (14 epochs)""",12.0,"""Results are obtained in only 12 hours (WikiText-103) to 2 days (enwik8) using a single modern GPU""",NVIDIA Quadro GP100,,Likely,"Many of the leading approaches in language modeling introduce novel, complex and specialized architectures. We take existing state-of-the-art word level language models based on LSTMs and QRNNs and extend them to both larger vocabularies as well as character-level granularity. When properly tuned, LSTMs and QRNNs achieve state-of-the-art results on character-level (Penn Treebank, enwik8) and word-level (WikiText-103) datasets, respectively. Results are obtained in only 12 hours (WikiText-103) to 2 days (enwik8) using a single modern GPU.",14.0,4 layer QRNN (h=2500),Unreleased,United States of America,,,,1.0,,2025-04-16 14:01,,,,,,Industry,,,,12.0,Open source,BSD-3 license: https://github.com/salesforce/awd-lstm-lm ,Industry,,,FP32,268.1693571105067,"Hardware,Operation counting",,,,
Chinese - English translation,Language,Translation,Microsoft,"H Hassan, A Aue, C Chen, V Chowdhary",2018-03-01,Achieving Human Parity on Automatic Chinese to English News Translation,https://www.microsoft.com/en-us/research/publication/achieving-human-parity-on-automatic-chinese-to-english-news-translation/,593.0,SOTA improvement,"""We find that our latest neural machine translation system has reached a new state-of-the-art, and that the translation quality is at human parity when compared to professional human translations""",,,,,,,,,,,,Self-supervised learning,Unknown,,,,,"United States of America,Multinational,India,Belgium",,,,,,2025-05-28 16:16,,,,,,Industry,,,,,,,Industry,,,FP32,,,,,,
Residual Dense Network,"Vision,Image generation",Image super-resolution,"Northeastern University,University of Rochester"," Yulun Zhang, Yapeng Tian, Yu Kong, Bineng Zhong, Yun Fu",2018-02-24,Residual Dense Network for Image Super-Resolution,https://arxiv.org/abs/1802.08797v2,3046.0,Highly cited,,,,,,DIV2K,,,,,,,,Unknown,"A very deep convolutional neural network (CNN) has recently achieved great success for image super-resolution (SR) and offered hierarchical features as well. However, most deep CNN based SR models do not make full use of the hierarchical features from the original low-resolution (LR) images, thereby achieving relatively-low performance. In this paper, we propose a novel residual dense network (RDN) to address this problem in image SR. We fully exploit the hierarchical features from all the convolutional layers. Specifically, we propose residual dense block (RDB) to extract abundant local features via dense connected convolutional layers. RDB further allows direct connections from the state of preceding RDB to all the layers of current RDB, leading to a contiguous memory (CM) mechanism. Local feature fusion in RDB is then used to adaptively learn more effective features from preceding and current local features and stabilizes the training of wider network. After fully obtaining dense local features, we use global feature fusion to jointly and adaptively learn global hierarchical features in a holistic way. Extensive experiments on benchmark datasets with different degradation models show that our RDN achieves favorable performance against state-of-the-art methods.",200.0,,,"United States of America,United States of America",,,,,,2025-05-28 16:16,,,,,,"Academia,Academia",,,,,,,"Academia,Academia",,,FP32,,,,,,
Spectrally Normalized GAN,Image generation,Image generation,"Preferred Networks Inc,Ritsumeikan University,National Institute of Informatics","Takeru Miyato, Toshiki Kataoka, Masanori Koyama, Yuichi Yoshida",2018-02-16,Spectral Normalization for Generative Adversarial Networks,https://arxiv.org/abs/1802.05957,4162.0,Highly cited,,,,,,CIFAR-10,,,,,,,,Unknown,"One of the challenges in the study of generative adversarial networks is the instability of its training. In this paper, we propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator. Our new normalization technique is computationally light and easy to incorporate into existing implementations. We tested the efficacy of spectral normalization on CIFAR10, STL-10, and ILSVRC2012 dataset, and we experimentally confirmed that spectrally normalized GANs (SN-GANs) is capable of generating images of better or equal quality relative to the previous training stabilization techniques.",,,,"Japan,Japan,Japan",,,,,,2025-02-04 12:53,,,,,,"Industry,Academia",,,,,,,"Industry,Academia",,,,,,,,,
TCN (P-MNIST),Language,Image classification,"Carnegie Mellon University (CMU),Intel Labs","Shaojie Bai, J. Zico Kolter, Vladlen Koltun",2018-02-15,Convolutional Sequence Modeling Revisited,https://openreview.net/forum?id=rk8wKk-R-,64.0,SOTA improvement,"""For the permuted sequential MNIST, TCNs outperform state of the art results using recurrent nets (95.9%) with Zoneout+Recurrent BatchNorm (Cooijmans et al., 2016; Krueger et al., 2017), a highly optimized method for regularizing RNNs""",42000.0,,,,P-MNIST,,,,,,,,Confident,,,,,"United States of America,Multinational,United States of America",,,,,,2024-09-05 14:08,,,,,,"Academia,Industry",,,,,,,"Academia,Industry",,,,,,,,,
ENAS,Language,"Language modeling,Neural Architecture Search - NAS","Google Brain,Carnegie Mellon University (CMU),Stanford University","Hieu Pham, Melody Y. Guan, Barret Zoph, Quoc V. Le, Jeff Dean",2018-02-09,Efficient Neural Architecture Search via Parameter Sharing,https://arxiv.org/abs/1802.03268,2760.0,Highly cited,,24000000.0,"24M
Table 1",2.00664e+16,"Training on PTB:
6 FLOP / token / parameter * 24000000 parameters * 929000 tokens * 150 epochs = 2.00664e+16 FLOP",Penn TreeBank (PTB),,929000.0,,,,NVIDIA GeForce GTX 1080 Ti,,Confident,"We propose Efficient Neural Architecture Search (ENAS), a fast and inexpensive approach for automatic model design. In ENAS, a controller learns to discover neural network architectures by searching for an optimal subgraph within a large computational graph. The controller is trained with policy gradient to select a subgraph that maximizes the expected reward on the validation set. Meanwhile the model corresponding to the selected subgraph is trained to minimize a canonical cross entropy loss. Thanks to parameter sharing between child models, ENAS is fast: it delivers strong empirical performances using much fewer GPU-hours than all existing automatic model design approaches, and notably, 1000x less expensive than standard Neural Architecture Search. On the Penn Treebank dataset, ENAS discovers a novel architecture that achieves a test perplexity of 55.8, establishing a new state-of-the-art among all methods without post-training processing. On the CIFAR-10 dataset, ENAS designs novel architectures that achieve a test error of 2.89%, which is on par with NASNet (Zoph et al., 2018), whose test error is 2.65%.",150.0,ENAS,Unreleased,"United States of America,United States of America,United States of America",,,,,,2025-04-16 14:31,,,,,,"Industry,Academia,Academia",,,,,Open source,code for PTB. Apache license: https://github.com/google-research/google-research/tree/master/enas_lm ,"Industry,Academia,Academia",,,,,Operation counting,,,,
DeepLabV3+,Vision,Semantic segmentation,Google,"Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, Hartwig Adam",2018-02-07,Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation,https://arxiv.org/abs/1802.02611v3,11558.0,Highly cited,,,,,,"ImageNet-1k,COCO,JFT-300M",,,,,,,,Unknown,"Spatial pyramid pooling module or encode-decoder structure are used in deep neural networks for semantic segmentation task. The former networks are able to encode multi-scale contextual information by probing the incoming features with filters or pooling operations at multiple rates and multiple effective fields-of-view, while the latter networks can capture sharper object boundaries by gradually recovering the spatial information. In this work, we propose to combine the advantages from both methods. Specifically, our proposed model, DeepLabv3+, extends DeepLabv3 by adding a simple yet effective decoder module to refine the segmentation results especially along object boundaries. We further explore the Xception model and apply the depthwise separable convolution to both Atrous Spatial Pyramid Pooling and decoder modules, resulting in a faster and stronger encoder-decoder network. We demonstrate the effectiveness of the proposed model on PASCAL VOC 2012 and Cityscapes datasets, achieving the test set performance of 89.0\% and 82.1\% without any post-processing. Our paper is accompanied with a publicly available reference implementation of the proposed models in Tensorflow at \url{this https URL}.",,,,United States of America,,,,,,2025-02-04 12:58,,,,,,Industry,,,,,,,Industry,,,,,,,,,
AmoebaNet-A (F=448),Vision,Image classification,Google Brain,"Esteban Real, Alok Aggarwal, Yanping Huang, Quoc V Le",2018-02-05,Regularized Evolution for Image Classifier Architecture Search,https://arxiv.org/abs/1802.01548,2823.0,Highly cited,,469000000.0,Table 2,3.85296912e+20,"450 K40 GPUs for 20k models (approx. 7 days).
(From Imagenet paper-data, Besiroglu et al., forthcoming) ",ImageNet-1k,,1280000.0,,168.0,"""Each experiment ran on 450 K40 GPUs for 20k models (approx. 7 days).""",NVIDIA Tesla K40s,,Confident,"The effort devoted to hand-crafting neural network image classifiers has motivated the use of architecture search to discover them automatically. Although evolutionary algorithms have been repeatedly applied to neural network topologies, the image classifiers thus discovered have remained inferior to human-crafted ones. Here, we evolve an image classifier---AmoebaNet-A---that surpasses hand-designs for the first time. To do this, we modify the tournament selection evolutionary algorithm by introducing an age property to favor the younger genotypes. Matching size, AmoebaNet-A has comparable accuracy to current state-of-the-art ImageNet models discovered with more complex architecture-search methods. Scaled to larger size, AmoebaNet-A sets a new state-of-the-art 83.9% / 96.6% top-5 ImageNet accuracy. In a controlled comparison against a well known reinforcement learning algorithm, we give evidence that evolution can obtain results faster with the same hardware, especially at the earlier stages of the search. This is relevant when fewer compute resources are available. Evolution is, thus, a simple method to effectively discover high-quality architectures.",,,Unreleased,United States of America,,,,450.0,,2025-06-18 05:22,,,,,,Industry,,,,75600.0,Unreleased,"has code, but looks like just a toy model: https://colab.research.google.com/github/google-research/google-research/blob/master/evolution/regularized_evolution_algorithm/regularized_evolution.ipynb ",Industry,$11766.34,,FP32,229206.26838084744,Hardware,,,,11
ELMo,Language,"Question answering,Sentiment classification,Language modeling","University of Washington,Allen Institute for AI","Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer",2018-02-01,Deep contextualized word representations,https://arxiv.org/abs/1802.05365,11108.0,Highly cited,,94000000.0,,3300100000000010.0,3300e12 - https://github.com/amirgholami/ai_and_memory_wall,,,,,,,,Self-supervised learning,Speculative,"We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",,,,"United States of America,United States of America",,,,,,2025-05-09 11:32,,,,,,"Academia,Research collective",,,,,,,"Academia,Research collective",,,,,Third-party estimation,,,,
QRNN,Language,Language modeling,Salesforce Research,"Stephen Merity, Nitish Shirish Keskar, James Bradbury, Richard Socher",2018-02-01,Scalable Language Modeling: WikiText-103 on a Single GPU in 12 hours,https://mlsys.org/Conferences/doc/2018/50.pdf,4.0,SOTA improvement,"""we reduce our per-epoch time substantially and achieve a new state-of-the-art on WikiText-103 despite training for 14 epochs""",135000000.0,"Based on the details provided in the paper, the number of parameters in the QRNN model can be calculated as follows:

Embedding layer size: 400
Number of QRNN layers: 4
Number of nodes per QRNN layer: 2500
Vocabulary size: 267,735 (for WikiText-103 dataset)
Adaptive softmax layer
Parameters:

Embedding layer: 400 x 267,735 = 107,094,000
QRNN layers:
Input to hidden weights per layer: 400 x 2500 = 1,000,000
Hidden to hidden weights per layer: 2500 x 2500 = 6,250,000
Biases per layer: 2500 = 2,500
Total QRNN layers parameters: 4 x (1,000,000 + 6,250,000 + 2,500) = 28,000,000
Adaptive softmax layer: No extra parameters due to weight tying
Total Parameters = Embedding + QRNN layers
= 107,094,000 + 28,000,000
= 135,094,000

So the total number of parameters in the QRNN model is approximately 135 million.

The majority of parameters are in the embedding layer, while the 4 QRNN layers contribute 28 million parameters. The adaptive softmax does not add any extra parameters due to weight tying.",6.8866472e+17,"6 FLOP / parameter / token * 135000000 parameters ['Likely' confidence] * 103000000 tokens * 14 epochs = 1.16802e+18 FLOP

31330000000000 FLOP / sec / GPU [fp16 assumed] * 1 GPU * 12 hours * 3600 sec / hour * 0.3 [assumed utilization] = 4.060368e+17 FLOP 

sqrt(4.060368e+17*1.16802e+18) = 6.8866472e+17 FLOP 
",WikiText-103,,103000000.0,"""We train with a batch size of 60 and a sequence length of 140.""

""""Using this approach we reduce our per-epoch time substantially and achieve a new state-of-the-art on WikiText-103 despite training for 14 epochs, a total time of only 12 hours.""",12.0,"""The model trains at 2980 seconds per epoch on the NVIDIA V100
and 5460 seconds per epoch on the NVIDIA P100.""

""Using this approach we reduce our per-epoch time substantially and achieve a new state-of-the-art on WikiText-103 despite training for 14 epochs, a total time of only 12 hours.""",NVIDIA V100,,Likely,"Word-level language modeling (WLM) is one the foundational tasks of unsupervised natural language processing. Most modern architectures for WLM use several LSTM layers, followed by a softmax layer. Even with larger batch sizes and a multi-GPU setup, training of these networks on large-vocabulary corpora is slow due to increased computation involving the softmax and the high cost of recurrence computation. We propose a model architecture and training strategy that enables us to achieve state-of-the-art performance on the WikiText-103 data set using a single GPU while being substantially faster than an NVIDIA cuDNN LSTM-based model by utilizing the Quasi-Recurrent Neural Network (QRNN), an adaptive softmax with weight tying, and longer sequences within batches.",14.0,QRNN,Unreleased,United States of America,,,,,,2025-05-28 16:17,,,,8400.0,60*140,Industry,,,,,Unreleased,,Industry,,,FP32,,"Operation counting,Hardware",,,,
ULM-FiT,Language,Text classification,"University of San Francisco,Insight Centre NUI Galway,Fast.ai","Jeremy Howard, Sebastian Ruder",2018-01-18,Universal Language Model Fine-tuning for Text Classification,https://arxiv.org/abs/1801.06146,1940.0,Highly cited,,441000000.0,https://files.fast.ai/models/wt103/?C=S;O=D,2.72538e+17,=103000000*441000000*6=2.72538e+17,"IMDb,Yelp,Trec-6,DBpedia,AG news,WikiText-103",,103000000.0,"We pretrain the language model on Wikitext-103
(Merity et al., 2017b) consisting of 28,595 preprocessed Wikipedia articles and 103 million words.

Fine-tuning datasets:
TREC-6 Question 5.5k
IMDb Sentiment 25k
Yelp-bi Sentiment 560k
Yelp-full Sentiment 650k
AG Topic 120k
DBpedia Topic 560k

560+120+650+560+25+5.5=1920.5k = 1920500",,,,,Speculative,"Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100x more data. We open-source our pretrained models and code.",,,Open weights (unrestricted),"United States of America,Ireland",AWD-LSTM,,,,,2025-02-17 12:40,,,,,,"Academia,Academia",,,,,Unreleased,https://nlp.fast.ai/category/classification.html,"Academia,Academia",,,,,Operation counting,,,,
Refined Part Pooling,Vision,Person retrieval,"Tsinghua University,University of Technology Sydney,University of Texas at San Antonio","Yifan Sun, Liang Zheng, Yi Yang, Qi Tian, Shengjin Wang",2018-01-09,Beyond Part Models: Person Retrieval with Refined Part Pooling (and a Strong Convolutional Baseline),https://arxiv.org/abs/1711.09349,2046.0,Highly cited,,,,2.6244e+16,12150000000000*3600*2*0.3=2.6244e+16 FLOP,"ImageNet-1k,Market-1501",,,,1.0,"""With two NVIDIA TITAN XP GPUs and Pytorch as the platform, training an IDE model and a standard
PCB on Market-1501 (12,936 training images) consumes
about 40 and 50 minutes, respectively""",NVIDIA TITAN Xp,,Confident,"Employing part-level features for pedestrian image description offers fine-grained information and has been verified as beneficial for person retrieval in very recent literature. A prerequisite of part discovery is that each part should be well located. Instead of using external cues, e.g., pose estimation, to directly locate parts, this paper lays emphasis on the content consistency within each part.
Specifically, we target at learning discriminative part-informed features for person retrieval and make two contributions. (i) A network named Part-based Convolutional Baseline (PCB). Given an image input, it outputs a convolutional descriptor consisting of several part-level features. With a uniform partition strategy, PCB achieves competitive results with the state-of-the-art methods, proving itself as a strong convolutional baseline for person retrieval.
(ii) A refined part pooling (RPP) method. Uniform partition inevitably incurs outliers in each part, which are in fact more similar to other parts. RPP re-assigns these outliers to the parts they are closest to, resulting in refined parts with enhanced within-part consistency. Experiment confirms that RPP allows PCB to gain another round of performance boost. For instance, on the Market-1501 dataset, we achieve (77.4+4.2)% mAP and (92.3+1.5)% rank-1 accuracy, surpassing the state of the art by a large margin.",,,,"China,Australia,United States of America",,,,2.0,,2025-02-04 13:09,,,,,,"Academia,Academia,Academia",,,,,,,"Academia,Academia,Academia",,,,1040.1094120986245,Hardware,,,,
Tacotron 2,Speech,Speech synthesis,"Google,University of California (UC) Berkeley","Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, Zongheng Yang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, RJ Skerry-Ryan, Rif A. Saurous, Yannis Agiomyrgiannakis, Yonghui Wu",2017-12-19,Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Prediction,https://arxiv.org/abs/1712.05884,2886.0,Highly cited,,,"some architecture details:

""Input characters are represented using a learned 512-dimensional
character embedding, which are passed through a stack of 3 convolutional layers each containing 512 filters with shape 5 Ã— 1, i.e., where
each filter spans 5 characters, followed by batch normalization [18]
and ReLU activations. As in Tacotron, these convolutional layers
model longer-term context (e.g., N-grams) in the input character
sequence. The output of the final convolutional layer is passed into a
single bi-directional [19] LSTM [20] layer containing 512 units (256
in each direction) to generate the encoded features.""",,,,"""We train all models on an internal US English dataset[12], which
contains 24.6 hours of speech from a single professional female
speaker.""",340000.0,"""We train all models on an internal US English dataset[12], which contains 24.6 hours of speech from a single professional female speaker.""

13,680 words/hour * 24.6 = 336528 words",,,,,Confident,"This paper describes Tacotron 2, a neural network architecture for speech synthesis directly from text. The system is composed of a recurrent sequence-to-sequence feature prediction network that maps character embeddings to mel-scale spectrograms, followed by a modified WaveNet model acting as a vocoder to synthesize timedomain waveforms from those spectrograms. Our model achieves a mean opinion score (MOS) of 4.53 comparable to a MOS of 4.58 for professionally recorded speech. To validate our design choices, we present ablation studies of key components of our system and evaluate the impact of using mel spectrograms as the input to WaveNet instead of linguistic, duration, and F0 features. We further demonstrate that using a compact acoustic intermediate representation enables significant simplification of the WaveNet architecture.",,,,"United States of America,United States of America",,,,,,2024-09-05 14:08,,,,,,"Industry,Academia",,,,,,,"Industry,Academia",,,,,,,,,
AlphaZero,Games,"Chess,Shogi,Go",DeepMind,"David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy Lillicrap, Karen Simonyan, Demis Hassabis",2017-12-05,Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm,https://arxiv.org/abs/1712.01815,1594.0,Highly cited,,,,3.667927300468287e+22,Extracted from AI and Compute (https://openai.com/blog/ai-and-compute/) charts by using https://automeris.io/WebPlotDigitizer/.,,,700000.0,"""We trained a separate instance of AlphaZero for each game. Training proceeded for 700,000 steps""",24.0,,"Google TPU v2,Google TPU v1",Self-supervised learning,Likely,"The game of chess is the most widely-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. In contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go, by tabula rasa reinforcement learning from games of self-play. In this paper, we generalise this approach into a single AlphaZero algorithm that can achieve, tabula rasa, superhuman performance in many challenging domains. Starting from random play, and given no domain knowledge except the game rules, AlphaZero achieved within 24 hours a superhuman level of play in the games of chess and shogi (Japanese chess) as well as Go, and convincingly defeated a world-champion program in each case.",,,Unreleased,United Kingdom of Great Britain and Northern Ireland,,,,5064.0,,2025-06-18 05:06,,,,,,Industry,,,,,Unreleased,,Industry,$229918.61,,,,Third-party estimation,,,,3
2-layer-LSTM+Deep-Gradient-Compression,Language,Language modeling,"Tsinghua University,Stanford University,NVIDIA","Yujun Lin, Song Han, Huizi Mao, Yu Wang, William J. Dally",2017-12-05,Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training,https://arxiv.org/abs/1712.01887,1298.0,Highly cited,,6020000.0,"Here is a summary of the calculations to determine the number of parameters in the 2-layer LSTM model with Deep Gradient Compression described in the paper:

Model has 2 LSTM layers with 1500 hidden units each

For an LSTM layer:

W matrix: number of hidden units * input dimension
U matrix: number of hidden units * number of hidden units
4 bias vectors with size = number of hidden units
So params per LSTM layer = hidden_units * (input_dim + hidden_units + 4)

For layer 1:

Input dim is word embedding size (estimated 300)
Params = 1500 * (300 + 1500 + 4) = 1,512,000
For layer 2:

Input is previous layer output (1500)
Params = 1500 * (1500 + 1500 + 4) = 4,506,000
Total Params = Layer 1 + Layer 2
= 1,512,000 + 4,506,000
= 6,018,000

The Deep Gradient Compression technique does not change number of parameters.

So total parameters for the model is approximately 6 million.",1340000000000000.0,6 FLOP / parameter / token * 6020000 parameters ['Likely' confidence] * 929000 tokens * 40 epochs = 1.3422192e+15 FLOP,Penn TreeBank (PTB),,929000.0,""" The warm-up period is 1 epoch out of 40 epochs""",,,,,Likely,"Large-scale distributed training requires significant communication bandwidth for gradient exchange that limits the scalability of multi-node training, and requires expensive high-bandwidth network infrastructure. The situation gets even worse with distributed training on mobile devices (federated learning), which suffers from higher latency, lower throughput, and intermittent poor connections. In this paper, we find 99.9% of the gradient exchange in distributed SGD is redundant, and propose Deep Gradient Compression (DGC) to greatly reduce the communication bandwidth. To preserve accuracy during compression, DGC employs four methods: momentum correction, local gradient clipping, momentum factor masking, and warm-up training. We have applied Deep Gradient Compression to image classification, speech recognition, and language modeling with multiple datasets including Cifar10, ImageNet, Penn Treebank, and Librispeech Corpus. On these scenarios, Deep Gradient Compression achieves a gradient compression ratio from 270x to 600x without losing accuracy, cutting the gradient size of ResNet-50 from 97MB to 0.35MB, and for DeepSpeech from 488MB to 0.74MB. Deep gradient compression enables large-scale distributed training on inexpensive commodity 1Gbps Ethernet and facilitates distributed training on mobile. Code is available at: this https URL.",40.0,2-layer-LSTM+Deep-Gradient-Compression,Unreleased,"China,United States of America,United States of America",,,,,,2025-05-28 16:17,,,,,,"Academia,Academia,Industry",,,,,Unreleased,"repo, but no code for language modeling: https://github.com/synxlin/deep-gradient-compression ","Academia,Academia,Industry",,,FP32,,Operation counting,,,,
PNASNet-5,Vision,Image classification,"Johns Hopkins University,Google AI,Stanford University","C Liu, B Zoph, M Neumann, J Shlens",2017-12-02,Progressive Neural Architecture Search,https://arxiv.org/abs/1712.00559,1911.0,Highly cited,,86100000.0,Table 5,6.629040000000001e+19,"8 times less compute than Zoph (2018), which used 500 p100s for 4 days.
(From Imagenet paper-data, Besiroglu et al., forthcoming) ",ImageNet-1k,,1280000.0,,,,,,Likely,"We propose a new method for learning the structure of convolutional neural networks (CNNs) that is more efficient than recent state-of-the-art methods based on reinforcement learning and evolutionary algorithms. Our approach uses a sequential model-based optimization (SMBO) strategy, in which we search for structures in order of increasing complexity, while simultaneously learning a surrogate model to guide the search through structure space. Direct comparison under the same search space shows that our method is up to 5 times more efficient than the RL method of Zoph et al. (2018) in terms of number of models evaluated, and 8 times faster in terms of total compute. The structures we discover in this way achieve state of the art classification accuracies on CIFAR-10 and ImageNet.",,,,"United States of America,Multinational,United States of America,United States of America",,,,,,2025-06-18 05:12,,,,,,"Academia,Industry,Academia",,,,,,,"Academia,Industry,Academia",,,,,Comparison with other models,,,,14
TriNet,Video,Person re-identification,"Visual Computing Institute,RWTH Aachen University","Alexander Hermans, Lucas Beyer, Bastian Leibe",2017-11-21,In Defense of the Triplet Loss for Person Re-Identification,https://arxiv.org/abs/1703.07737,3038.0,Highly cited,,,,,,,,,,,,,,Unknown,,,,,"Germany,Germany",,,,,,2025-05-28 16:17,,,,,,Academia,,,,,,,Academia,,,FP32,,,,,,
"AWD-LSTM-MoS + dynamic evaluation (WT2, 2017)",Language,Language modeling,Carnegie Mellon University (CMU),"Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, William W. Cohen",2017-11-10,Breaking the Softmax Bottleneck: A High-Rank RNN Language Model,https://arxiv.org/abs/1711.03953,358.0,SOTA improvement,"""Experimental results confirm that the
proposed method significantly improves state-of-the-art language models, achieving a perplexity of 55.31 and 62.89 on
the test set of Penn Treebank and WikiText-2""",35000000.0,35M (Table 2),3.36e+18,"6 FLOP / parameter / token * 35000000 parameters * 2000000 tokens * 8000 epochs = 3.36e+18 FLOP 

_____________
in the algorithmic progress paper the estimation was 4.37 Ã— 10^17 FLOP based on 1000 epochs assumption",WikiText-2,,2000000.0,"max sequence length from https://github.com/zihangdai/mos/blob/master/main.py 
is 110 tokens 
batch size 15 (table 8)
epochs: 8000 (from https://github.com/zihangdai/mos/blob/master/main.py)

",,,,,Likely,"We formulate language modeling as a matrix factorization problem, and show that the expressiveness of Softmax-based models (including the majority of neural language models) is limited by a Softmax bottleneck. Given that natural language is highly context-dependent, this further implies that in practice Softmax with distributed word embeddings does not have enough capacity to model natural language. We propose a simple and effective method to address this issue, and improve the state-of-the-art perplexities on Penn Treebank and WikiText-2 to 47.69 and 40.68 respectively. The proposed method also excels on the large-scale 1B Word dataset, outperforming the baseline by over 5.6 points in perplexity.",8000.0,"""AWD-LSTM-MoS + dynamic evaluation (WT2, 2017)""",Unreleased,United States of America,,,,,,2025-04-16 15:17,,,,,,Academia,,,,,Open source,MIT code: https://github.com/zihangdai/mos ,Academia,,,,,Operation counting,,,,
Fraternal dropout + AWD-LSTM 3-layer (WT2),Language,Language modeling,"Jagiellonian University,Mila - Quebec AI (originally Montreal Institute for Learning Algorithms),University of Montreal / UniversitÃ© de MontrÃ©al","Konrad Zolna, Devansh Arpit, Dendi Suhubdy, Yoshua Bengio",2017-10-31,Fraternal Dropout,https://arxiv.org/abs/1711.00066,55.0,SOTA improvement,"""We evaluate our model and achieve state-of-the-art results in sequence
modeling tasks on two benchmark datasets â€“ Penn Treebank and Wikitext-2""",34000000.0,34M (Table 2),3.06e+17,"6 FLOP / token / parameter * 34000000 parameters * 2000000 tokens * 750 epochs = 3.06e+17 FLOP

_________________
In the Algorithmic Progress paper, the compute was estimated to be 9.85 Ã— 10Â¹â¶ FLOP, assuming 520 epochs reported for the PTB dataset.",WikiText-2,,2000000.0,"code suggests 750 epochs
https://github.com/kondiz/fraternal-dropout/blob/WT2/main.py",,,,,Likely,"Recurrent neural networks (RNNs) are important class of architectures among neural networks useful for language modeling and sequential prediction. However, optimizing RNNs is known to be harder compared to feed-forward neural networks. A number of techniques have been proposed in literature to address this problem. In this paper we propose a simple technique called fraternal dropout that takes advantage of dropout to achieve this goal. Specifically, we propose to train two identical copies of an RNN (that share parameters) with different dropout masks while minimizing the difference between their (pre-softmax) predictions. In this way our regularization encourages the representations of RNNs to be invariant to dropout mask, thus being robust. We show that our regularization term is upper bounded by the expectation-linear dropout objective which has been shown to address the gap due to the difference between the train and inference phases of dropout. We evaluate our model and achieve state-of-the-art results in sequence modeling tasks on two benchmark datasets - Penn Treebank and Wikitext-2. We also show that our approach leads to performance improvement by a significant margin in image captioning (Microsoft COCO) and semi-supervised (CIFAR-10) tasks.",750.0,Fraternal dropout + AWD-LSTM 3-layer (WT2),Unreleased,"Poland,Canada,Canada",,,,,,2025-04-17 15:26,,,,,,"Academia,Academia,Academia",,,,,Open source,BSD-3 license: https://github.com/kondiz/fraternal-dropout ,"Academia,Academia,Academia",,,,,Operation counting,,,,
DCN+,Language,Question answering,Salesforce Research,"Caiming Xiong, Victor Zhong, Richard Socher",2017-10-31,DCN+: Mixed Objective and Deep Residual Coattention for Question Answering,https://arxiv.org/abs/1711.00106v2,121.0,SOTA improvement,"""On the Stanford Question Answering Dataset, our model achieves state-of-the-art results with 75.1% exact match accuracy and 83.1% F1, while the ensemble obtains 78.9% exact match accuracy and 86.0% F1. ""
https://paperswithcode.com/paper/dcn-mixed-objective-and-deep-residual
",,"Not directly repoted - It may be possible to extract number from:
https://github.com/lmn-extracts/dcn_plus/tree/master/question_answering",,"in Figure 4 we see that network was trained on 140k iterations
from https://github.com/lmn-extracts/dcn_plus/tree/master we see that batch size is 64
It should be possible to compute inference FLOPs from repository and estimate training compute",SQuAD,"From start of section 3: ""We train and evaluate our model on the Stanford Question Answering Dataset (SQuAD). ""

",107785.0,"from https://paperswithcode.com/dataset/squad SQuAD have 107,785 question-answer pairs
download-ed dataset from: https://www.kaggle.com/datasets/stanfordu/stanford-question-answering-dataset?resource=download
wc -w on train-v.1.1 returns 4017471 words so around 5.4M tokens

Looks like they probably trained on each token in SQuAD rather than QA pairs, but uncertain.",,,,,Confident,"Traditional models for question answering optimize using cross entropy loss, which encourages exact answers at the cost of penalizing nearby or overlapping answers that are sometimes equally accurate. We propose a mixed objective that combines cross entropy loss with self-critical policy learning. The objective uses rewards derived from word overlap to solve the misalignment between evaluation metric and optimization objective. In addition to the mixed objective, we improve dynamic coattention networks (DCN) with a deep residual coattention encoder that is inspired by recent work in deep self-attention and residual networks. Our proposals improve model performance across question types and input lengths, especially for long questions that requires the ability to capture long-term dependencies. On the Stanford Question Answering Dataset, our model achieves state-of-the-art results with 75.1% exact match accuracy and 83.1% F1, while the ensemble obtains 78.9% exact match accuracy and 86.0% F1. ",,,Unreleased,United States of America,,,,,,2024-09-18 18:09,,,,,,Industry,,,,,,,Industry,,,,,,,,,
S-Norm,Language,Question answering,"University of Washington,Allen Institute for AI","Christopher Clark, Matt Gardner",2017-10-29,Simple and Effective Multi-Paragraph Reading Comprehension,https://arxiv.org/abs/1710.10723v2,453.0,SOTA improvement,"""Overall, we are able to achieve a score of 71.3 F1 on the web portion of TriviaQA, a large improvement from the 56.7 F1 of the previous best system.""",,Not stated. Probably obtainable from github: https://github.com/allenai/document-qa/tree/master,,,TriviaQA,,2000000000.0,"""530k question-document training pairs""

average question length of 14 words and document length of 2895 words, per
 https://www.cs.utexas.edu/~eunsol/files/papers/acl17jcwz.pdf

530,000 * 2895 words on average * 1.33 tokens/word = ~2,000,000,000",,,,,Confident,"We consider the problem of adapting neural paragraph-level question answering models to the case where entire documents are given as input. Our proposed solution trains models to produce well calibrated confidence scores for their results on individual paragraphs. We sample multiple paragraphs from the documents during training, and use a shared-normalization training objective that encourages the model to produce globally correct output. We combine this method with a state-of-the-art pipeline for training models on document QA data. Experiments demonstrate strong performance on several document QA datasets. Overall, we are able to achieve a score of 71.3 F1 on the web portion of TriviaQA, a large improvement from the 56.7 F1 of the previous best system.",1.0,,,"United States of America,United States of America",,,,,,2024-09-05 14:08,,,,,,"Academia,Research collective",,,,,,,"Academia,Research collective",,,,,,,,,
PhraseCond,Language,Question answering,"Carnegie Mellon University (CMU),University of Pittsburgh","Rui Liu, Wei Wei, Weiguang Mao, Maria Chikina",2017-10-28,Phase Conductor on Multi-layered Attentions for Machine Comprehension,https://arxiv.org/abs/1710.10504v2,22.0,SOTA improvement,"""We demonstrate the effectiveness of our proposed model PhaseCond on the SQuAD dataset, showing that our model significantly outperforms both state-of-the-art single-layered and multiple-layered attention models.""",,Unclear how many layers they use for self-attention (N) and fusion (L and K). Could calculate if these were known.,,,SQuAD 1.1,"from start of section 3: ""This paper focuses on the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016) to train and evaluate our model. SQuAD, which has gained a significant attention recently, is a largescale dataset consisting of more than 100,000 questions manually created through crowdsourcing on 536 Wikipedia articles. The dataset is randomly partitioned into a training set (80%), a development set (10%), and a blinded test set (10%).""",90000.0,"10% held out for test, so 100k * 0.9 = 90k",,,,,Confident,"Attention models have been intensively studied to improve NLP tasks such as machine comprehension via both question-aware passage attention model and self-matching attention model. Our research proposes phase conductor (PhaseCond) for attention models in two meaningful ways. First, PhaseCond, an architecture of multi-layered attention models, consists of multiple phases each implementing a stack of attention layers producing passage representations and a stack of inner or outer fusion layers regulating the information flow. Second, we extend and improve the dot-product attention function for PhaseCond by simultaneously encoding multiple question and passage embedding layers from different perspectives. We demonstrate the effectiveness of our proposed model PhaseCond on the SQuAD dataset, showing that our model significantly outperforms both state-of-the-art single-layered and multiple-layered attention models. We deepen our results with new findings via both detailed qualitative analysis and visualized examples showing the dynamic changes through multi-layered attention models.",,,,"United States of America,United States of America",,,,,,2024-09-23 12:45,,,,,,"Academia,Academia",,,,,,,"Academia,Academia",,,,,,,,,
ProgressiveGAN,Vision,Image generation,NVIDIA,"Tero Karras, Timo Aila, Samuli Laine, Jaakko Lehtinen",2017-10-27,"Progressive Growing of GANs for Improved Quality, Stability, and Variation",https://arxiv.org/abs/1710.10196,6778.0,Highly cited,,,,,,,,,,,,,,Unknown,,,,,United States of America,,,,,,2025-05-28 16:17,,,,,,Industry,,,,,,,Industry,,,FP32,,,,,,
LRSO-GAN,Vision,Person re-identification,University of Technology Sydney,"Zhedong Zheng, Liang Zheng, Yi Yang",2017-10-22,Unlabeled Samples Generated by GAN Improve the Person Re-identification Baseline in vitro,https://arxiv.org/abs/1701.07717,1809.0,Highly cited,,,,,,,,,,,,,,Unknown,,,,,Australia,,,,,,2025-05-28 16:17,,,,,,Academia,,,,,,,Academia,,,FP32,,,,,,
AlphaGo Master,Games,Go,DeepMind,"D Silver, J Schrittwieser, K Simonyan, I Antonoglou",2017-10-19,Mastering the game of Go without human knowledge,https://www.nature.com/articles/nature24270,8795.0,Highly cited,,,,2.0001000000000102e+23,"This is a guess. There was no single journal publication that accompanied this model, that gave information about architecture/model training time etc. All I could find was that it has the same architecture as AlphaGo Zero, and that it had roughly the same power consumption as AGZ. See for instance: 
https://deepmind.com/blog/article/alphago-zero-starting-scratch

Since AGZ reaches the ELO of AlphaGo Master in about 25-30 days (60-75% of the total training time), I estimate the compute to be around 60-75% that of AGZ. I round this to 2e23, and I expect this to only be accurate within an OOM.",,,,,72.0,"""Training started from completely random behaviour and continued without human intervention for approximately three days.""",Google TPU v1,,Likely,"A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo's own move selections and also the winner of AlphaGo's games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100-0 against the previously published, champion-defeating AlphaGo. Â© 2017 Macmillan Publishers Limited, part of Springer Nature. All rights reserved.",,,Unreleased,United Kingdom of Great Britain and Northern Ireland,,,,,,2025-06-18 04:00,,,,,,Industry,,,,,Unreleased,,Industry,$471445.32,,,,Benchmarks,,,,2
AlphaGo Zero,Games,Go,DeepMind,"D Silver, J Schrittwieser, K Simonyan, I Antonoglou",2017-10-18,Mastering the game of Go without human knowledge,https://www.nature.com/articles/nature24270,8795.0,Highly cited,,46400244.0,"Input size: 19*19*17=6137, internal dimension 19*19
1 conv block
17*3*3*256=39168
39 residual blocks
2*3*3*256*256=1179648
Policy head
256*2+2*19*19*(19*19+2)=262598
Value head
256*1+1*19*19*256+256*1=92928
Total: 39168+39*1179648+262598+92928=46400966",3.41e+23,"source: https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389


AGZ had two models, one of which was small and another of which was large. The compute for AGZ is for the large model, which has 40 residual blocks instead of 20.

A second way of looking at this... we believe multiple TPUs were used for training. 29 million games * 211 moves per game on average * 0.8 seconds per move = 4.8952E+09 seconds of player-time across all TPUs.

4.8952E+09 seconds of player-time / (40 days * 24 * 60 * 60 seconds of real time) ~= 1,416 players

4 TPUs per player => 4.8952E+09 * 4 = 1.95808E+10 TPU-seconds
Total compute = 1.95808E+10 TPU-seconds * 92E+12 FLOP/(TPU-second) * 0.4 = 7.2e23 FLOP

So similar to the Cotra and Davidson estimate (within a factor of 2).

Alternative operation counting estimate: 
Number of connections (to calculate forward FLOP)
Input size: 19*19*17, Assuming internal dimension stays at 19*19
1 conv block
19*19*17*3*3*256=14139648
40 residual blocks
19*19*2*3*3*256*256=425852928
Policy head
19*19*256*2+2*19*19*(19*19+2)=446918
Value head
19*19*256*1+1*19*19*256+256*1=185088
Total: 14139648+40*425852928+446918+185088=17048888774
Forward FLOP: 2*17048888774=34097777548
Parameter updates â€œParameters were updated from 3.1 million mini-batches of 2,048 positions each.â€
Total updates: 3100000*2048=6348800000
Paramter update FLOP: 3*6348800000*34097777548=649439910290227200000
MCTS move generation FLOP
â€œOver the course of training, 29 million games of selfplay were generated.â€
From the main training details (not 40 block specific): â€œusing 1,600 simulations for each MCTSâ€
Assuming each MCTS simulation requires 1 forward pass
Assuming 200 moves on average per game
MCTS FLOP: 34097777548*29000000*200*1600=3.1642737564544e+23
Total: 3.1642737564544e+23+649439910290227200000=3.170768e+23
",,,5800000000.0,"""Over the course of training, 29 million games of self-play were generated""

Approx 200 moves per Go game on average

https://homepages.cwi.nl/~aeb/go/misc/gostat.html

Thus 200 * 29e6 = 5.8e9",480.0,,Google TPU v1,Self-supervised learning,Confident,"A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo's own move selections and also the winner of AlphaGo's games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100-0 against the previously published, champion-defeating AlphaGo. Â© 2017 Macmillan Publishers Limited, part of Springer Nature. All rights reserved.",,,Unreleased,United Kingdom of Great Britain and Northern Ireland,,,,,,2025-06-18 04:56,,,,,,Industry,,,,,Unreleased,,Industry,$613480.63,,,,"Third-party estimation,Hardware",,,,1
AWD-LSTM+WT+Cache+IOG (WT2),Language,Language modeling,NTT Communication Science Laboratories,"Sho Takase, Jun Suzuki, Masaaki Nagata",2017-09-26,Input-to-Output Gate to Improve RNN Language Models,https://arxiv.org/abs/1709.08907,7.0,SOTA improvement,"""IOG achieves comparable scores to the state-of-the-art on the Penn Treebank
dataset and outperforms the WikiText-2 dataset""",53000000.0,53M (Table 3),3180000000000000.0,6 FLOP / parameter / token * 53000000 parameters * 2000000 tokens * 5 epochs = 3.18e+15 FLOP,WikiText-2,,2000000.0,5 epochs (Table 1),,,,,Confident,"This paper proposes a reinforcing method that refines the output layers of existing Recurrent Neural Network (RNN) language models. We refer to our proposed method as Input-to-Output Gate (IOG). IOG has an extremely simple structure, and thus, can be easily combined with any RNN language models. Our experiments on the Penn Treebank and WikiText-2 datasets demonstrate that IOG consistently boosts the performance of several different types of current topline RNN language models.",5.0,AWD-LSTM+WT+Cache+IOG (WT2),Unreleased,Japan,,,,,,2025-04-17 15:31,,,,,,Industry,,,,,Open (non-commercial),"license, looks non-commercial: https://github.com/nttcslab-nlp/iog?tab=License-1-ov-file#readme 

https://github.com/nttcslab-nlp/iog ",Industry,,,,,Operation counting,,,,
ISS,Language,Language modeling,"Duke University,Microsoft","Wei Wen, Yuxiong He, Samyam Rajbhandari, Minjia Zhang, Wenhan Wang, Fang Liu, Bin Hu, Yiran Chen, Hai Li",2017-09-15,Learning Intrinsic Sparse Structures within Long Short-Term Memory,https://arxiv.org/abs/1709.05027,146.0,SOTA improvement,"""Moreover, ISS learning can find a
smaller RHN model with width 726, meanwhile improve the state-of-the-art perplexity as shown by the second entry in Table 2.""",11100000.0,11.1M (Table 2),3400000000000000.0,6 FLOP / parameter / token * 11100000 parameters * 929000 tokens * 55 epochs = 3.402927e+15 FLOP,Penn TreeBank (PTB),,929000.0,"""All models are trained from scratch for 55 epochs.""",,,,,Confident,"Model compression is significant for the wide adoption of Recurrent Neural Networks (RNNs) in both user devices possessing limited resources and business clusters requiring quick responses to large-scale service requests. This work aims to learn structurally-sparse Long Short-Term Memory (LSTM) by reducing the sizes of basic structures within LSTM units, including input updates, gates, hidden states, cell states and outputs. Independently reducing the sizes of basic structures can result in inconsistent dimensions among them, and consequently, end up with invalid LSTM units. To overcome the problem, we propose Intrinsic Sparse Structures (ISS) in LSTMs. Removing a component of ISS will simultaneously decrease the sizes of all basic structures by one and thereby always maintain the dimension consistency. By learning ISS within LSTM units, the obtained LSTMs remain regular while having much smaller basic structures. Based on group Lasso regularization, our method achieves 10.59x speedup without losing any perplexity of a language modeling of Penn TreeBank dataset. It is also successfully evaluated through a compact model with only 2.69M weights for machine Question Answering of SQuAD dataset. Our approach is successfully extended to non- LSTM RNNs, like Recurrent Highway Networks (RHNs). Our source code is publicly available at this https URL",55.0,ISS,Unreleased,"United States of America,United States of America,Multinational,India,Belgium",,,,,,2025-05-28 16:17,,,,,,"Academia,Industry",,,,,Open source,code (apache): https://github.com/wenwei202/iss-rnns/tree/master/ptb ,"Academia,Industry",,,FP32,,Operation counting,,,,
PyramidNet,Vision,Image classification,Korea Advanced Institute of Science and Technology (KAIST),"Dongyoon Han, Jiwhan Kim, Junmo Kim",2017-09-06,Deep Pyramidal Residual Networks,https://arxiv.org/abs/1610.02915v4,718.0,SOTA improvement,"""In tests using CIFAR-10, CIFAR-100, and ImageNet1k datasets, our PyramidNets outperform all previous state-of-the-art deep network architectures.""",26000000.0,best model had 26M params,2340000000000000.0,6ND=6*26000000*50000*300=2.34e+15,"CIFAR-10,CIFAR-100","""Our PyramidNets are trained using backpropagation [15] by Stochastic Gradient Descent (SGD) with Nesterov momentum for 300 epochs on CIFAR-10 and CIFAR-100 datasets.""
"" CIFAR-10 and CIFAR-100 each contain 32Ã—32-pixel color images, consists of 50,000 training images and 10,000 testing images.""",50000.0,,,,,,Likely,"Deep convolutional neural networks (DCNNs) have shown remarkable performance in image classification tasks in recent years. Generally, deep neural network architectures are stacks consisting of a large number of convolutional layers, and they perform downsampling along the spatial dimension via pooling to reduce memory usage. Concurrently, the feature map dimension (i.e., the number of channels) is sharply increased at downsampling locations, which is essential to ensure effective performance because it increases the diversity of high-level attributes. This also applies to residual networks and is very closely related to their performance. In this research, instead of sharply increasing the feature map dimension at units that perform downsampling, we gradually increase the feature map dimension at all units to involve as many locations as possible. This design, which is discussed in depth together with our new insights, has proven to be an effective means of improving generalization ability. Furthermore, we propose a novel residual unit capable of further improving the classification accuracy with our new network architecture. Experiments on benchmark CIFAR-10, CIFAR-100, and ImageNet datasets have shown that our network architecture has superior generalization ability compared to the original residual networks. Code is available at this https URL}",300.0,,Open weights (unrestricted),Korea (Republic of),,,,,,2025-05-28 16:17,,,,,,Academia,,,,,Open source,https://github.com/jhkim89/PyramidNet,Academia,,,FP32,,Operation counting,,,,
GL-LWGC-AWD-MoS-LSTM + dynamic evaluation (WT2),Language,Language modeling,Ben-Gurion University of the Negev,"Ziv Aharoni, Gal Rattner, Haim Permuter",2017-08-29,Gradual Learning of Recurrent Neural Networks,https://arxiv.org/abs/1708.08863,4.0,SOTA improvement,"""Our GL-LSTM model
overcame the state-of-the-art results with only two layers and 19M parameters, and further improved
the state-of-the-art results with the third layer phase""",38000000.0,38M (Table 2),4.56e+17,6 FLOP / parameter / token * 38000000 parameters * 2000000 tokens * 1000 epochs = 4.56e+17 FLOP,WikiText-2,,2000000.0,""" The model was trained for 1000 epochs until the validation score stopped improving.""",,,,,Confident,"Recurrent Neural Networks (RNNs) achieve state-of-the-art results in many sequence-to-sequence modeling tasks. However, RNNs are difficult to train and tend to suffer from overfitting. Motivated by the Data Processing Inequality (DPI), we formulate the multi-layered network as a Markov chain, introducing a training method that comprises training the network gradually and using layer-wise gradient clipping. We found that applying our methods, combined with previously introduced regularization and optimization methods, resulted in improvements in state-of-the-art architectures operating in language modeling tasks.",1000.0,GL-LWGC-AWD-MoS-LSTM + dynamic evaluation (WT2),Unreleased,Israel,,,,,,2025-04-17 15:38,,,,,,Academia,,,,,Unreleased,,Academia,,,,,Operation counting,,,,
Libratus,Games,Poker,Carnegie Mellon University (CMU),"N Brown, T Sandholm, S Machine",2017-08-19,Libratus: The Superhuman AI for No-Limit Poker,https://www.ijcai.org/proceedings/2017/0772.pdf,97.0,SOTA improvement,Claims to be first ML system to reach superhuman level at No Limit Poker Texas Hold Em,,,5.51e+20,"""In total, Libratus used about 25 million core hours. Of those, about 13 million core hours were used for exploratory experiments and evaluation. About 6 million core hours were spent on the initial abstraction and equilibrium finding component, another 3 million were used for nested subgame solving, and about 3 million were used on the self-improvement algorithm.""

""Like many data-centric supercomputers, Bridges offers a relatively a modest number of FLOPS, but lots of memory: 895 teraflops and 130 TB, respectively.""

I just used the first bullet point (as those are usually independent systems and you only benchmark one of them).
The first system has 752 nodes a 2CPUs a 14cores each.

source: https://www.top500.org/news/bridges-supercomputer-boots-up-at-pittsburgh/



1. 12M core hours for 196 cores
2. We have  895 TFLOPS for 752 nodes a 2 CPUs a 14 cores
2.1 That's 42.5 GFLOPS per core.
3. Running this for 12M h
3.1 12 * 10^6 * 60 * 60 * 42.5 * 10^9 FLOP/S = 1.823e21 FLOPs
4. Assuming 30% utilization
 1.823e21 * 0.3
â†’ 5.51e20 FLOPs",,,,,,"""In total, Libratus used about 25 million core hours. Of those, about 13 million core hours were used for exploratory experiments and evaluation. About 6 million core hours were spent on the initial abstraction and equilibrium finding component, another 3 million were used for nested subgame solving, and about 3 million were used on the self-improvement algorithm.""",,,Likely,"No-limit Texas Holdâ€™em is the most popular variant of poker in the world. Heads-up no-limit Texas Holdâ€™em is the main benchmark challenge for AI in imperfect-information games. We present Libratus, the firstâ€”and so far onlyâ€”AI to defeat top human professionals in that game. Libratusâ€™s architecture features three main modules, each of which has new algorithms: pre-computing a solution to an abstraction of the game which provides a high-level blueprint for the strategy of the AI, a new nested subgame-solving algorithm which repeatedly calculates a more detailed strategy as play progresses, and a self-improving module which augments the pre-computed blueprint over time.",,,Unreleased,United States of America,,,,,,2025-06-18 04:06,,,,,,Academia,,,,857000.0,Unreleased,,Academia,,,,,Hardware,,,,6
Adversarial Joint Adaptation Network (ResNet),Vision,Image classification,"Tsinghua University,University of California (UC) Berkeley","Mingsheng Long, Han Zhu, Jianmin Wang, Michael I. Jordan",2017-08-17,Deep Transfer Learning with Joint Adaptation Networks,https://arxiv.org/abs/1605.06636,2482.0,Highly cited,,60000000.0,"Model is based on ResNet (60m params), might have more parameters though

""We implement all deep methods based on the Caffe framework, and fine-tune from Caffe-provided models of AlexNet (Krizhevsky et al., 2012) and ResNet (He et al., 2016), both are pre-trained on the ImageNet 2012 dataset.""",,,"Office-31,ILSVRC 2012 subset of ImageNet","""Office-31 (Saenko et al., 2010) is a standard benchmark for
domain adaptation in computer vision, comprising 4,652
images and 31 categories collected from three distinct domains: Amazon (A), which contains images downloaded
from amazon.com, Webcam (W) and DSLR (D)... We evaluate all
methods across three transfer tasks A â†’ W, D â†’ W and W
â†’ D, which are widely adopted by previous deep transfer
learning methods""

""We implement all deep methods based on the Caffe framework, and fine-tune from Caffe-provided models of AlexNet
(Krizhevsky et al., 2012) and ResNet (He et al., 2016), both
are pre-trained on the ImageNet 2012 dataset.""",4652.0,,,,,,Speculative,"Deep networks have been successfully applied to learn transferable features for adapting models from a source domain to a different target domain. In this paper, we present joint adaptation networks (JAN), which learn a transfer network by aligning the joint distributions of multiple domain-specific layers across domains based on a joint maximum mean discrepancy (JMMD) criterion. Adversarial training strategy is adopted to maximize JMMD such that the distributions of the source and target domains are made more distinguishable. Learning can be performed by stochastic gradient descent with the gradients computed by back-propagation in linear-time. Experiments testify that our model yields state of the art results on standard datasets.",,,,"China,United States of America",,,,,,2025-05-28 16:17,,,,,,"Academia,Academia",,,,,,,"Academia,Academia",,,FP32,,,,,,
NeuMF (Pinterest),Recommendation,Collaborative filtering,"Shandong University,Texas A&M,National University of Singapore,Columbia University","X He, L Liao, H Zhang, L Nie, X Hu",2017-08-16,Neural Collaborative Filtering,https://arxiv.org/abs/1708.05031,5427.0,Highly cited,,,,,,,,,,,,,,Unknown,,,,,"China,United States of America,Singapore,United States of America",,,,,,2024-11-01 10:02,,,,,,"Academia,Academia,Academia,Academia",,,,,,,"Academia,Academia,Academia,Academia",,,,,,,,,
Cutout-regularized net,Vision,Image classification,"University of Guelph,Vector Institute,CIFAR AI Research"," Terrance DeVries, Graham W. Taylor",2017-08-15,Improved Regularization of Convolutional Neural Networks with Cutout,https://arxiv.org/abs/1708.04552,3446.0,Highly cited,,,,,,,,,,,,,,Unknown,,,,,"Canada,Canada,Canada",,,,,,2024-11-01 10:02,,,,,,"Academia,Academia,Research collective",,,,,,,"Academia,Academia,Research collective",,,,,,,,,
EI-REHN-1000D,Language,Language modeling,Korea Advanced Institute of Science and Technology (KAIST),"Hyunsin Park, Chang D. Yoo",2017-08-14,Early Improving Recurrent Elastic Highway Network,https://arxiv.org/abs/1708.04116,6.0,SOTA improvement,"""The proposed networks showed better performance than other state-of-the-art recurrent networks in all three experiments.""",19000000.0,19M (Table 4),1.06e+16,6 FLOP / parameter / token * 19000000 parameters * 929000 tokens * 100 epochs = 1.05906e+16 FLOP,Penn TreeBank (PTB),,929000.0,"""For all the experiments in this paper, Tensorflow toolkit [15] was used. For training the network, Adam optimizer [16] was adopted with 20 mini-batch size, 100 epochs, and 0.01 learning rate""",,,,,Confident,"To model time-varying nonlinear temporal dynamics in sequential data, a recurrent network capable of varying and adjusting the recurrence depth between input intervals is examined. The recurrence depth is extended by several intermediate hidden state units, and the weight parameters involved in determining these units are dynamically calculated. The motivation behind the paper lies on overcoming a deficiency in Recurrent Highway Networks and improving their performances which are currently at the forefront of RNNs: 1) Determining the appropriate number of recurrent depth in RHN for different tasks is a huge burden and just setting it to a large number is computationally wasteful with possible repercussion in terms of performance degradation and high latency. Expanding on the idea of adaptive computation time (ACT), with the use of an elastic gate in the form of a rectified exponentially decreasing function taking on as arguments as previous hidden state and input, the proposed model is able to evaluate the appropriate recurrent depth for each input. The rectified gating function enables the most significant intermediate hidden state updates to come early such that significant performance gain is achieved early. 2) Updating the weights from that of previous intermediate layer offers a richer representation than the use of shared weights across all intermediate recurrence layers. The weight update procedure is just an expansion of the idea underlying hypernetworks. To substantiate the effectiveness of the proposed network, we conducted three experiments: regression on synthetic data, human activity recognition, and language modeling on the Penn Treebank dataset. The proposed networks showed better performance than other state-of-the-art recurrent networks in all three experiments.",100.0,EI-REHN-1000D,Unreleased,Korea (Republic of),,,,,,2025-05-28 16:17,,,,,,Academia,,,,,Unreleased,,Academia,,,FP32,,Operation counting,,,,
OpenAI TI7 DOTA 1v1,Games,DOTA,OpenAI,,2017-08-11,Dota 2,https://openai.com/research/dota-2,,"Historical significance,SOTA improvement",,,"Section 4 states: ""we used a model with over 150 million parameters"" but this is for the 5v5 agent, not the 1v1.",6.046095222592002e+20,Extracted from AI and Compute (https://openai.com/blog/ai-and-compute/) charts by using https://automeris.io/WebPlotDigitizer/.,,,,,,,,,Confident,"Weâ€™ve created a bot which beats the worldâ€™s top professionals at 1v1 matches of Dota 2 under standard tournament rules. The bot learned the game from scratch by self-play, and does not use imitation learning or tree search. This is a step towards building AI systems which accomplish well-defined goals in messy, complicated situations involving real humans.",,,,United States of America,,,,,,2025-06-18 04:12,,,,,,Industry,,,,,,,Industry,,,,,Third-party estimation,,,,5
RetinaNet-R101,Vision,Object detection,Facebook AI Research,"TY Lin, P Goyal, R Girshick, K He, P Dollar",2017-08-07,Focal loss for dense object detection,https://arxiv.org/abs/1708.02002,16437.0,Highly cited,,53000000.0,source: table 2 in https://arxiv.org/pdf/1911.09070.pdf,2.065392e+18,"""We use synchronized SGD over 8 GPUs with a total of 16 images per minibatch (2 images per GPU). Unless otherwise specified, all models are trained for 90k iterations with an initial learning rate of 0.01, which is then divided by 10 at 60k and again at 80k iterations. We use horizontal image flipping as the only form of data augmentation unless otherwise noted. Weight decay of 0.0001 and momentum of 0.9 are used. The training loss is the sum the focal loss and the standard smooth L1 loss used for box regression [10]. Training time ranges between 10 and 35 hours for the models in Table 1e.""

NVIDIA M40 GPU

35*60**2*0.3*8*6.83E+12 = 2.07e18",COCO,,135000.0,trainval135k split,35.0,"""We use synchronized SGD over 8 GPUs with a total of 16 images per minibatch (2 images per GPU). Unless otherwise specified, all models are trained for 90k iterations with an initial learning rate of 0.01, which is then divided by 10 at 60k and again at 80k iterations. We use horizontal image flipping as the only form of data augmentation unless otherwise noted. Weight decay of 0.0001 and momentum of 0.9 are used. The training loss is the sum the focal loss and the standard smooth L1 loss used for box regression [10]. Training time ranges between 10 and 35 hours for the models in Table 1e.""

NVIDIA M40 GPU

35*60**2*0.3*8*6.83E+12 = 2.07e18",NVIDIA M40,,Confident,,,,,United States of America,,,,8.0,,2025-06-18 04:11,,,,,,Industry,,,,280.0,,,Industry,,,FP32,4174.823263154792,Hardware,,,,
AWD-LSTM - 3-layer LSTM (tied) + continuous cache pointer (WT2),Language,Language modeling,Salesforce Research,"Stephen Merity, Nitish Shirish Keskar, Richard Socher",2017-08-07,Regularizing and Optimizing LSTM Language Models,https://arxiv.org/abs/1708.02182,1176.0,"Highly cited,SOTA improvement","""we achieve an even lower state-of-the-art perplexity of 52.8 on Penn Treebank and 52.0 on WikiText-2.""",33000000.0,33M (Table 2),2.97e+17,6 FLOP / parameter / token * 33000000 parameters * 2000000 tokens * 750 epochs = 2.97e+17 FLOP,WikiText-2,,2000000.0,"""For training the models, we use the NT-ASGD algorithm discussed in the previous section for 750 epochs""",,,,,Confident,"Recurrent neural networks (RNNs), such as long short-term memory networks (LSTMs), serve as a fundamental building block for many sequence learning tasks, including machine translation, language modeling, and question answering. In this paper, we consider the specific problem of word-level language modeling and investigate strategies for regularizing and optimizing LSTM-based models. We propose the weight-dropped LSTM which uses DropConnect on hidden-to-hidden weights as a form of recurrent regularization. Further, we introduce NT-ASGD, a variant of the averaged stochastic gradient method, wherein the averaging trigger is determined using a non-monotonic condition as opposed to being tuned by the user. Using these and other regularization strategies, we achieve state-of-the-art word level perplexities on two data sets: 57.3 on Penn Treebank and 65.8 on WikiText-2. In exploring the effectiveness of a neural cache in conjunction with our proposed model, we achieve an even lower state-of-the-art perplexity of 52.8 on Penn Treebank and 52.0 on WikiText-2.",750.0,AWD-LSTM - 3-layer LSTM (tied) + continuous cache pointer (WT2),Unreleased,United States of America,,,,,,2025-04-17 16:10,,,,,,Industry,,,,,Open source,"bsd-3 license: https://github.com/salesforce/awd-lstm-lm 
train/eval code: https://github.com/salesforce/awd-lstm-lm/blob/master/main.py ",Industry,,,,,Operation counting,,,,
GSM,Language,Question answering,"Peking University,Microsoft Research","Wenhui Wang, Nan Yang, Furu Wei, Baobao Chang, Ming Zhou",2017-07-30,Gated Self-Matching Networks for Reading Comprehension and Question Answering,https://aclanthology.org/P17-1018/,806.0,SOTA improvement,"""At the time of submission of the paper, our model holds the first place on the SQuAD leaderboard for both single and ensemble model.""",,It could be possible to estimate it from section 3.,,,SQuAD,"""We specially focus on the SQuAD dataset to train and evaluate our model, """,4000000.0,"from https://paperswithcode.com/dataset/squad SQuAD have 107,785 question-answer pairs
download-ed dataset from: https://www.kaggle.com/datasets/stanfordu/stanford-question-answering-dataset?resource=download
wc -w on train-v.1.1 returns 4017471 words so around 4M words",,,,,Likely,"In this paper, we present the gated self-matching networks for reading comprehension style question answering, which aims to answer questions from a given passage. We first match the question and passage with gated attention-based recurrent networks to obtain the question-aware passage representation. Then we propose a self-matching attention mechanism to refine the representation by matching the passage against itself, which effectively encodes information from the whole passage. We finally employ the pointer networks to locate the positions of answers from the passages. We conduct extensive experiments on the SQuAD dataset. The single model achieves 71.3% on the evaluation metrics of exact match on the hidden test set, while the ensemble model further boosts the results to 75.9%. At the time of submission of the paper, our model holds the first place on the SQuAD leaderboard for both single and ensemble model.",,,,"China,United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational",,,,,,2024-09-05 14:08,,,,,,"Academia,Industry",,,,,,,"Academia,Industry",,,,,,,,,
ConvS2S (ensemble of 8 models),Language,Translation,Meta AI,"Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, Yann N. Dauphin",2017-07-25,Convolutional Sequence to Sequence Learning,https://arxiv.org/abs/1705.03122,,"Highly cited,SOTA improvement","""We achieve a new state of the art on several public translation benchmark data sets. On the WMTâ€™16 EnglishRomanian task we outperform the previous best result by 1.9 BLEU, on WMTâ€™14 English-French translation we improve over the LSTM model of Wu et al. (2016) by 1.6 BLEU in a comparable setting, and on WMTâ€™14 EnglishGerman translation we ouperform the same model by 0.5
BLEU""",,,5.64e+19,"All models are implemented in Torch (Collobert et al., 2011) and trained on a single Nvidia M40 GPU except for WMTâ€™14 English-French for which we use a multi-GPU setup on a single
machine. We train on up to eight GPUs synchronously by
maintaining copies of the model on each card and split the
batch so that each worker computes 1/8-th of the gradients;
at the end we sum the gradients via Nvidia NCCL.

1. English-Romanian: ""Training took between 6 and 7.5 days on a single GPU.""
7 days * 24 * 3600 * 6.8e12 FLOP/s (Nvidia M40, fp32) * 0.3 = 1.2e18 FLOP

2. English-German: "" We trained this model on a single GPU over a
period of 18.5 days with a batch size of 48"".
18.5 days * 24 * 3600 * 6.8e12 FLOP/s (Nvidia M40, fp32) * 0.3 = 3.3e18 FLOP

3. English-French: ""Our results are based on training
with 8 GPUs for about 37 days and batch size 32 on each
worker.6 ""
37 days * 24 * 3600 * 8 * 6.8e12 FLOP/s (Nvidia M40, fp32) * 0.3 = 5.2e19 FLOP

the minimum compute needed to train ensemble model: 1.2e18 FLOP + 3.3e18 FLOP + 5.2e19 FLOP = 5.65e19 FLOP

I am not sure how much to add more (they say ensemble model consists of 8 models), probably summarization training takes at least 1.2e18 FLOP more. 

","WMT English-German,WMT14,Gigaword",,46600000.0," 2.8M + 4.5M + 35.5M + 3.8M = 46.6M

We consider three major WMT translation tasks as well as
a text summarization task.

WMTâ€™16 English-Romanian. We use the same data and pre-processing as Sennrich et al. (2016b) but remove sentences with more than 175 words. This results in 2.8M sentence pairs for training and we evaluate on newstest2016.2

WMTâ€™14 English-German. We use the same setup as Luong et al. (2015) which comprises 4.5M sentence pairs for training and we test on newstest2014.

WMTâ€™14 English-French. We use the full training set of
36M sentence pairs, and remove sentences longer than 175
words as well as pairs with a source/target length ratio exceeding 1.5. This results in 35.5M sentence-pairs for training.

Abstractive summarization. We train on the Gigaword
corpus (Graff et al., 2003) and pre-process it identically
to Rush et al. (2015) resulting in 3.8M training examples
and 190K for validation.",,,NVIDIA M40,,Likely,"The prevalent approach to sequence to sequence learning maps an input sequence to a variable length output sequence via recurrent neural networks. We introduce an architecture based entirely on convolutional neural networks. Compared to recurrent models, computations over all elements can be fully parallelized during training and optimization is easier since the number of non-linearities is fixed and independent of the input length. Our use of gated linear units eases gradient propagation and we equip each decoder layer with a separate attention module. We outperform the accuracy of the deep LSTM setup of Wu et al. (2016) on both WMT'14 English-German and WMT'14 English-French translation at an order of magnitude faster speed, both on GPU and CPU.",,,,United States of America,,,,,,2025-02-17 12:42,,,,,,Industry,,5.65000000001e+19,,,,,Industry,,,,,Hardware,,,,11
PSPNet,Vision,Image segmentation,Chinese University of Hong Kong (CUHK),"Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, Jiaya Jia",2017-07-21,Pyramid Scene Parsing Network,https://ieeexplore.ieee.org/document/8100143,10876.0,Highly cited,,,,,,,,,,,,,,Unknown,,,,,"Hong Kong,China",,,,,,2025-05-28 16:17,,,,,,Academia,,,,,,,Academia,,,FP32,,,,,,
Densely Connected LSTM + Var. Dropout,Language,Language modeling,Ghent University,"FrÃ©deric Godin, Joni Dambre, Wesley De Neve",2017-07-19,Improving Language Modeling using Densely Connected Recurrent Neural Networks,https://arxiv.org/abs/1707.06130,7.0,,,23000000.0,23M (Table 1),1.28e+16,6 FLOP / parameter / token * 23000000 parameters * 929000 tokens * 100 epochs = 1.28202e+16 FLOP,Penn TreeBank (PTB),,929000.0,"""We trained for 100 epochs and used early stopping.""",,,,,Confident,"In this paper, we introduce the novel concept of densely connected layers into recurrent neural networks. We evaluate our proposed architecture on the Penn Treebank language modeling task. We show that we can obtain similar perplexity scores with six times fewer parameters compared to a standard stacked 2-layer LSTM model trained with dropout (Zaremba et al. 2014). In contrast with the current usage of skip connections, we show that densely connecting only a few stacked layers with skip connections already yields significant perplexity reductions.",100.0,Densely Connected LSTM + Var. Dropout,Unreleased,Belgium,,,,,,2025-04-18 11:06,,,,,,Academia,,,,,Unreleased,,Academia,,,,,Operation counting,,,,
JFT,Vision,"Image classification,Object detection,Semantic segmentation,Pose estimation","Google Research,Carnegie Mellon University (CMU)","Chen Sun, Abhinav Shrivastava, Saurabh Singh, Abhinav Gupta",2017-07-10,Revisiting Unreasonable Effectiveness of Data in Deep Learning Era.,https://arxiv.org/abs/1707.02968,2194.0,Highly cited,,44654504.0,"Uses ResNet-101 architecture, which has 44,654,504 parameters:
https://resources.wolframcloud.com/NeuralNetRepository/resources/ResNet-101-Trained-on-ImageNet-Competition-Data/",8.43e+20,"Tesla K80 performance: 8.13 TFLOP/s

Assume 40% utilization

60 days * 50 GPUs * 40% utilization * 8.13 TFLOP/s/GPU = 8.43*10^20 FLOP",JFT-300M,,300000000.0,,1440.0,,NVIDIA Tesla K80,,Confident,"The success of deep learning in vision can be attributed to: (a) models with high capacity; (b) increased computational power; and (c) availability of large-scale labeled data. Since 2012, there have been significant advances in representation capabilities of the models and computational capabilities of GPUs. But the size of the biggest dataset has surprisingly remained constant. What will happen if we increase the dataset size by 10x or 100x? This paper takes a step towards clearing the clouds of mystery surrounding the relationship between `enormous data' and visual deep learning. By exploiting the JFT-300M dataset which has more than 375M noisy labels for 300M images, we investigate how the performance of current vision tasks would change if this data was used for representation learning. Our paper delivers some surprising (and some expected) findings. First, we find that the performance on vision tasks increases logarithmically based on volume of training data size. Second, we show that representation learning (or pre-training) still holds a lot of promise. One can improve performance on many vision tasks by just training a better base model. Finally, as expected, we present new state-of-the-art results for different vision tasks including image classification, object detection, semantic segmentation and human pose estimation. Our sincere hope is that this inspires vision community to not undervalue the data and develop collective efforts in building larger datasets.",4.0,,,"Multinational,United States of America,Canada,Switzerland,United States of America",,,,50.0,,2025-05-28 16:17,,,,32.0,,"Industry,Academia",,,,72000.0,,,"Industry,Academia",$17239.59,,FP32,31330.704406642573,Hardware,,,,4
NoisyNet-Dueling,Games,Atari,DeepMind,"M Fortunato, MG Azar, B Piot, J Menick",2017-06-30,Noisy Networks for Exploration,https://arxiv.org/abs/1706.10295v3,851.0,SOTA improvement,,,,,,,,,,,,,,Unknown,,,,Unreleased,United Kingdom of Great Britain and Northern Ireland,,,,,,2024-11-01 10:02,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
DeepLabV3,Vision,Semantic segmentation,Google,"Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, Hartwig Adam",2017-06-17,Rethinking Atrous Convolution for Semantic Image Segmentation,https://arxiv.org/abs/1706.05587,7689.0,Highly cited,,,,,,,,,,,,,,Unknown,,,,,United States of America,,,,,,2025-05-28 16:17,,,,,,Industry,,,,,,,Industry,,,FP32,,,,,,
HRA,Games,Ms. Pac-Man,"Maluuba,Microsoft","H Van Seijen, M Fatemi, J Romoff, R Laroche",2017-06-13,Hybrid Reward Architecture for Reinforcement Learning,https://arxiv.org/abs/1706.04208,243.0,SOTA improvement,"""With the best combination, HRA not only outperforms the state-of-the-art on both metrics, it
also significantly outperforms the human score, convincingly demonstrating the strength of HRA.""",,,,,,,,,,,,,Unknown,,,,,"Canada,United States of America,Multinational,India,Belgium",,,,,,2025-05-28 16:17,,,,,,"Industry,Industry",,,,,,,"Industry,Industry",,,FP32,,,,,,
Transformer,Language,Translation,"Google Research,Google Brain","Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin",2017-06-12,Attention Is All You Need,https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf,108604.0,"Highly cited,Historical significance",The original transformer,213000000.0,"This page suggests the transformer has 213M parameters.

""Although there are others architectures that make use of attention layers, none achieves so good results so fast. Not only that, but the only model that can compite against Transformer is the Slicenet22, proposed just fifteen days before. It takes much longer to train, due to the huge amount of parameters it requires (348 million against the 213 millions of Transformer), and the BLEU scores it achieves are slightly worse on average. In short, up to date it offers no profit over Transformer.""

https://ricardokleinklein.github.io/2017/11/16/Attention-is-all-you-need.html",7.4245248e+18,"""The model was trained during 300000 steps, roughly 3.5 days, using 8 NVIDIA P100 GPUs.""

source: https://ricardokleinklein.github.io/2017/11/16/Attention-is-all-you-need.html

NVIDIA Tesla P100 has 9.3 teraFLOPS single-precision performance

source: https://www.nvidia.com/en-gb/data-center/tesla-p100/

We assume 0.33 utilization performance, in line with OpenAI's ""AI and compute"" article

source: https://openai.com/blog/ai-and-compute/

9.3*10^12 FLOP / GPU / sec * 8 GPUs * 3.5 days * 24 hour / day * 3600 sec / hour * 0.33 [assumed utilization] = 7.4245248e+18 FLOP

In the ""AI and Memory Wall"" paper (https://github.com/amirgholami/ai_and_memory_wall) the estimation is 23,000 PFLOPS = 2.3*10^19 FLOPs. They estimate number of parameters as 65M

Note that Table 2 provides a training compute estimate, but appears not to account for utilization.","WMT English-German,WMT14",,1866666667.0,"""We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary ""

In total, this is 40.5 million sentence-pairs. Assuming each sentence pair is 15-20 words in each language, this is 1.2-1.6 billion words.

Convert to tokens: 1.4B * 4/3 = 1.87B tokens",84.0,"We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using
the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the
bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps
(3.5 days).",NVIDIA P100,Self-supervised learning,Confident,"The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature",3.0,,Unreleased,"Multinational,United States of America,Canada,Switzerland,United States of America",,,,8.0,,2025-06-10 13:34,,,,,,"Industry,Industry",,,,672.0,Unreleased,,"Industry,Industry",$438.04,,FP32,4180.032869488383,"Hardware,Third-party estimation",,,,18
EDSR,"Vision,Image generation",Image super-resolution,Seoul National University,"Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, Kyoung Mu Lee",2017-06-10,Enhanced Deep Residual Networks for Single Image Super-Resolution,https://arxiv.org/abs/1707.02921,5348.0,Highly cited,,,,,,,,,,,,,,Unknown,,,,,Korea (Republic of),,,,,,2025-05-28 16:17,,,,,,Academia,,,,,,,Academia,,,FP32,,,,,,
Reading Twice for NLU,Language,Question answering,DeepMind,"Dirk Weissenborn, TomÃ¡Å¡ KoÄiskÃ½, Chris Dyer",2017-06-08,Dynamic Integration of Background Knowledge in Neural NLU Systems,https://arxiv.org/abs/1706.02596v3,62.0,SOTA improvement,"""Our results are competi-
tive with the best systems, achieving a new state
of the art on the recent TriviaQA benchmarks.""",,,,,"TriviaQA,SQuAD","""We use 2 recent DQA
benchmark training and evaluation datasets,
SQuAD (Rajpurkar et al., 2016) and TriviaQA
(Joshi et al., 2017). """,,"both datasets have around 100k training examples.
SQuAD have around 4M words. TriviaQA is larger
""We use 2 recent DQAbenchmark training and evaluation datasets,
SQuAD (Rajpurkar et al., 2016) and TriviaQA
(Joshi et al., 2017). """,,,,,Unknown,"Common-sense and background knowledge is required to understand natural language, but in most neural natural language understanding (NLU) systems, this knowledge must be acquired from training corpora during learning, and then it is static at test time. We introduce a new architecture for the dynamic integration of explicit background knowledge in NLU models. A general-purpose reading module reads background knowledge in the form of free-text statements (together with task-specific text inputs) and yields refined word representations to a task-specific NLU architecture that reprocesses the task inputs with these representations. Experiments on document question answering (DQA) and recognizing textual entailment (RTE) demonstrate the effectiveness and flexibility of the approach. Analysis shows that our model learns to exploit knowledge in a semantically appropriate way. ",,,,United Kingdom of Great Britain and Northern Ireland,,,,,,2024-09-05 14:08,,,,,,Industry,,,,,,,Industry,,,,,,,,,
PointNet++,3D modeling,3D segmentation,Stanford University,"Charles R. Qi, Li Yi, Hao Su, Leonidas J. Guibas",2017-06-07,PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space,https://arxiv.org/abs/1706.02413,9565.0,Highly cited,,,,,,,,,,,,,,Unknown,,,,,United States of America,,,,,,2025-05-28 16:17,,,,,,Academia,,,,,,,Academia,,,FP32,,,,,,
Inflated 3D ConvNet,3D modeling,Action recognition,"DeepMind,University of Oxford","Joao Carreira, Andrew Zisserman",2017-06-01,"Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset",https://arxiv.org/abs/1705.07750,7325.0,Highly cited,,,,,,,,,,,,,,Unknown,,,,,"United Kingdom of Great Britain and Northern Ireland,United Kingdom of Great Britain and Northern Ireland",,,,,,2024-11-01 10:02,,,,,,"Industry,Academia",,,,,,,"Industry,Academia",,,,,,,,,
SRGAN,"Vision,Image generation",Image super-resolution,Twitter,"Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, Wenzhe Shi",2017-05-25,Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network,https://openaccess.thecvf.com/content_cvpr_2017/html/Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper.html,11032.0,Highly cited,,,,,,,,,,,,,,Unknown,,,,Unreleased,United States of America,,,,,,2024-09-05 14:08,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
Low-Cost Collaborative Network,Vision,Image classification,"National University of Singapore,University of Technology Sydney,Qihoo 360 AI Institute","Xuanyi Dong, Junshi Huang, Yi Yang, and Shuicheng Yan",2017-05-15,More is Less: A More Complicated Network with Less Inference Complexity,https://arxiv.org/abs/1703.08651,275.0,,,,,,Seems to suggest 400 epochs were used for CIFAR experiments.,"CIFAR-10,CIFAR-100,ImageNet",,1280000.0,,,,,,Speculative,"In this paper, we present a novel and general network structure towards accelerating the inference process of convolutional neural networks, which is more complicated in
network structure yet with less inference complexity. The
core idea is to equip each original convolutional layer
with another low-cost collaborative layer (LCCL), and the
element-wise multiplication of the ReLU outputs of these
two parallel layers produces the layer-wise output. The
combined layer is potentially more discriminative than the
original convolutional layer, and its inference is faster for
two reasons: 1) the zero cells of the LCCL feature maps will
remain zero after element-wise multiplication, and thus it is
safe to skip the calculation of the corresponding high-cost
convolution in the original convolutional layer; 2) LCCL
is very fast if it is implemented as a 1 Ã— 1 convolution or
only a single filter shared by all channels. Extensive experiments on the CIFAR-10, CIFAR-100 and ILSCRC-2012
benchmarks show that our proposed network structure can
accelerate the inference process by 32% on average with
negligible performance drop.",400.0,,,"Singapore,Australia,China",,,,,,2024-11-01 10:02,,,,,,"Academia,Academia,Industry",,,,,,,"Academia,Academia,Industry",,,,,,,,,
Mnemonic Reader,Language,Question answering,"Fudan University,Microsoft Research","Minghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu, Furu Wei, Ming Zhou",2017-05-08,Reinforced Mnemonic Reader for Machine Reading Comprehension,https://arxiv.org/abs/1705.02798v6,217.0,SOTA improvement,"from the abstract "" Extensive experiments on the Stanford Question Answering Dataset (SQuAD) show that our model achieves state-of-the-art results. Meanwhile, our model outperforms previous systems by over 6% in terms of both Exact Match and F1 metrics on two adversarial SQuAD datasets. """,,may be possible to estimate architecture description,,may be possible to estimate from architecture description,SQuAD,"at the start of section 5.1 Implementation Details (page 5) ""We mainly focus on the SQuAD dataset [Rajpurkar et al.,2016] to train and evaluate our model""",107785.0,size of SQuAD,,,,,Confident,"In this paper, we introduce the Reinforced Mnemonic Reader for machine reading comprehension tasks, which enhances previous attentive readers in two aspects. First, a reattention mechanism is proposed to refine current attentions by directly accessing to past attentions that are temporally memorized in a multi-round alignment architecture, so as to avoid the problems of attention redundancy and attention deficiency. Second, a new optimization approach, called dynamic-critical reinforcement learning, is introduced to extend the standard supervised method. It always encourages to predict a more acceptable answer so as to address the convergence suppression problem occurred in traditional reinforcement learning algorithms. Extensive experiments on the Stanford Question Answering Dataset (SQuAD) show that our model achieves state-of-the-art results. Meanwhile, our model outperforms previous systems by over 6% in terms of both Exact Match and F1 metrics on two adversarial SQuAD datasets. ",,,,"China,United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational",,,,,,2024-09-05 14:08,,,,,,"Academia,Industry",,,,,,,"Academia,Industry",,,,,,,,,
DeepLab (2017),Vision,Image segmentation,"Johns Hopkins University,Google,University College London (UCL)","Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, Alan L. Yuille",2017-04-27,"DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs",https://ieeexplore.ieee.org/abstract/document/7913730,16537.0,Highly cited,,,,,,,,,,,,,,Unknown,,,,,"United States of America,United States of America,United Kingdom of Great Britain and Northern Ireland",,,,,,2024-11-01 10:02,,,,,,"Academia,Industry,Academia",,,,,,,"Academia,Industry,Academia",,,,,,,,,
Tacotron,Speech,"Text-to-speech,Speech synthesis",Google,"Yuxuan Wang, RJ Skerry-Ryan, Daisy Stanton, Yonghui Wu, Ron J. Weiss, Navdeep Jaitly, Zongheng Yang, Ying Xiao, Zhifeng Chen, Samy Bengio, Quoc Le, Yannis Agiomyrgiannakis, Rob Clark, Rif A. Saurous",2017-04-06,Tacotron: Towards End-to-End Speech Synthesis,https://arxiv.org/abs/1703.10135,,,,,I am guessing It can be calculated from the hyperparameters (Table 1),,,,,,"""We train Tacotron on an internal North American English dataset, which contains about 24.6 hours of speech data spoken by a professional female speaker. The phrases are text normalized, e.g. â€œ16â€ is converted to â€œsixteenâ€""

""We train using a batch size of 32, where all sequences are padded to a max length.""

""frame length: 50 ms;
frame shift: 12.5 ms""

""We use 24 kHz sampling rate for all experiments""

2M steps",,,,,Unknown,"A text-to-speech synthesis system typically consists of multiple stages, such as a text analysis frontend, an acoustic model and an audio synthesis module. Building these components often requires extensive domain expertise and may contain brittle design choices. In this paper, we present Tacotron, an end-to-end generative text-to-speech model that synthesizes speech directly from characters. Given <text, audio> pairs, the model can be trained completely from scratch with random initialization. We present several key techniques to make the sequence-to-sequence framework perform well for this challenging task. Tacotron achieves a 3.82 subjective 5-scale mean opinion score on US English, outperforming a production parametric system in terms of naturalness. In addition, since Tacotron generates speech at the frame level, it's substantially faster than sample-level autoregressive methods.",,,Unreleased,United States of America,,,,,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
WGAN-GP,Image generation,Image generation,"Courant Institute of Mathematical Sciences,Mila - Quebec AI (originally Montreal Institute for Learning Algorithms)","Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, Aaron Courville",2017-03-31,Improved Training of Wasserstein GANs,https://arxiv.org/abs/1704.00028,8859.0,Highly cited,,,,,,,,,,,,,,Unknown,,,,,"United States of America,Canada",,,,,,2024-11-01 10:02,,,,,,"Academia,Academia",,,,,,,"Academia,Academia",,,,,,,,,
Mask R-CNN,Vision,Image segmentation,Facebook AI Research,"Kaiming He, Georgia Gkioxari, Piotr DollÃ¡r, Ross Girshick",2017-03-30,Mask R-CNN,https://arxiv.org/abs/1703.06870,24562.0,Highly cited,,,,,,COCO,,,,,"Training with
ResNet-50-FPN on COCO trainval35k takes 32 hours
in our synchronized 8-GPU implementation (0.72s per 16-
image mini-batch), and 44 hours with ResNet-101-FPN",,,Unknown,,,,,United States of America,,,,,,2024-11-01 10:02,,,,,,Industry,,,,,,,Industry,,,,,,,,,
Prototypical networks,Vision,Image classification,"University of Toronto,Twitter"," Jake Snell, Kevin Swersky, Richard S. Zemel",2017-03-15,Prototypical Networks for Few-shot Learning,https://arxiv.org/abs/1703.05175,7282.0,Highly cited,,,,,,,,,,,,,,Unknown,,,,,"Canada,United States of America",,,,,,2024-11-01 10:02,,,,,,"Academia,Industry",,,,,,,"Academia,Industry",,,,,,,,,
SEST,Language,Question answering,Carnegie Mellon University (CMU),"Rui Liu, Junjie Hu, Wei Wei, Zi Yang, Eric Nyberg",2017-03-02,Structural Embedding of Syntactic Trees for Machine Comprehension,https://arxiv.org/abs/1703.00572v3,53.0,,,,,,,SQuAD,"start of section 4 ""We conducted systematic experiments on the SQuAD dataset""",107785.0,"SQuAD 1.1 contains 107,785 question-answer pairs on 536 articles. The paper isn't super clear on how they train the model, but it looks as though there is a backward pass for each answer (not per token).
",,,NVIDIA GeForce GTX 1080,,Confident,"Deep neural networks for machine comprehension typically utilizes only word or character embeddings without explicitly taking advantage of structured linguistic information such as constituency trees and dependency trees. In this paper, we propose structural embedding of syntactic trees (SEST), an algorithm framework to utilize structured information and encode them into vector representations that can boost the performance of algorithms for the machine comprehension. We evaluate our approach using a state-of-the-art neural attention model on the SQuAD dataset. Experimental results demonstrate that our model can accurately identify the syntactic boundaries of the sentences and extract answers that are syntactically coherent over the baseline methods. ",,,,United States of America,,,,1.0,,2024-09-05 14:08,,,,,,Academia,,,,,,,Academia,,,,207.17498102199565,,,,,
DnCNN,"Vision,Image generation",Image super-resolution,"Harbin Institute of Technology,Hong Kong Polytechnic University,ULSee Inc.,Xiâ€™an Jiaotong University","Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, Lei Zhang",2017-02-01,Beyond a Gaussian Denoiser: Residual Learning of Deep CNN for Image Denoising,https://ieeexplore.ieee.org/abstract/document/7839189,6381.0,Highly cited,,,,,,,,,,,,,,Unknown,,,,,"China,Hong Kong,China,China,China",,,,,,2024-11-01 10:02,,,,,,"Academia,Academia,Industry,Academia",,,,,,,"Academia,Academia,Industry,Academia",,,,,,,,,
VDCNN (on Amazon Review Full dataset),Language,Text classification,"Facebook AI Research,University of Le Mans","Alexis Conneau, Holger Schwenk, LoÃ¯c Barrault, Yann Lecun",2017-01-27,Very Deep Convolutional Networks for Text Classification,https://arxiv.org/abs/1606.01781,,,,7800000.0,7.8 M from Table 2.,5.7267e+17,5050000000000 [peak FLOPs] * 15 [epochs] * 7 [h] * 3600 [s] * 0.3 [assumed utilization rate] = 5.7267e+17,Amazon Review Data,,3000000.0,Table 3: Amazon Review Full 3 000k,,"""One epoch took from 24 minutes to 2h45 for depth 9, and from 50 minutes to 7h (on the largest datasets) for depth 29. It took between 10 to 15 epoches to converge.""",NVIDIA Tesla K40s,,Confident,"The dominant approach for many NLP tasks are recurrent neural networks, in particular LSTMs, and convolutional neural networks. However, these architectures are rather shallow in comparison to the deep convolutional networks which have pushed the state-of-the-art in computer vision. We present a new architecture (VDCNN) for text processing which operates directly at the character level and uses only small convolutions and pooling operations. We are able to show that the performance of this model increases with depth: using up to 29 convolutional layers, we report improvements over the state-of-the-art on several public text classification tasks. To the best of our knowledge, this is the first time that very deep convolutional nets have been applied to text processing.",15.0,,,"United States of America,France",,,,1.0,,2025-01-13 12:31,,,,128.0,,"Industry,Academia",,,,,,,"Industry,Academia",,,,282.2017592376111,Hardware,,,,
MoE-Multi,Language,"Language modeling,Translation","Jagiellonian University,Google Brain","N Shazeer, A Mirhoseini, K Maziarz, A Davis",2017-01-23,Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer,https://arxiv.org/abs/1701.06538,2037.0,"Highly cited,SOTA improvement","""On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost""",8700000000.0,"Table 5

https://arxiv.org/abs/1701.06538",9.393905664e+19,"12 days 
64 NVIDIA K40 GPUs (see hardware data sheet for performance)
0.33 util rate
 ",,,133000000000.0,"""We constructed a similar training set consisting of shuffled unique sentences from Googleâ€™s internal news corpus, totalling roughly 100 billion words""
Assuming 100 words = 133 tokens",288.0,12 days,NVIDIA Tesla K40t,,Confident,"The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.",,,Unreleased,"Poland,United States of America",,,,64.0,,2025-06-18 04:09,,,,1365333.0,"""Training was done synchronously on a cluster of up to 64 GPUs as described in section 3. Each training batch consisted of a set of sentence pairs containing roughly 16000 words per GPU."" Although they appear to use word-level tokenization in other experiments, here they use subword tokens: ""Similar to GNMT, to effectively deal with rare words, we used subword units (also known as â€œwordpieces"") (Schuster & Nakajima, 2012) for inputs and outputs in our system."" In total 64 GPUs * 16k words/GPU * 4/3 tokens/word = 1,365,333","Academia,Industry",,,,,,,"Academia,Industry",$3863.74,,FP32,32873.78910003616,Hardware,,,,8
OR-WideResNet,Vision,Image classification,"Duke University,University of Chinese Academy of Sciences","Yanzhao Zhou, Qixiang Ye, Qiang Qiu and Jianbin Jiao",2017-01-07,Oriented Response Networks,https://arxiv.org/abs/1701.01833v2,285.0,SOTA improvement,"""In Sec. 4.3, we upgrade the VGG [38], ResNet [18], and the
WideResNet [45] to ORNs, and train them on CIFAR10 and
CIFAR100 [22], showing the state-of-the-art performance
on the natural image classification task.""",18200000.0,18.2M for largest OR-WideResNet model.,,,CIFAR-10, CIFAR-10 and CIFAR-100,50000.0,,,,NVIDIA Tesla K80,,Confident,"Deep Convolution Neural Networks (DCNNs) are capable of learning unprecedentedly effective image representations. However, their ability in handling significant local and global image rotations remains limited. In this paper, we propose Active Rotating Filters (ARFs) that actively rotate during convolution and produce feature maps with location and orientation explicitly encoded. An ARF acts as a virtual filter bank containing the filter itself and its multiple unmaterialised rotated versions. During back-propagation, an ARF is collectively updated using errors from all its rotated versions. DCNNs using ARFs, referred to as Oriented Response Networks (ORNs), can produce within-class rotation-invariant deep features while maintaining inter-class discrimination for classification tasks. The oriented response produced by ORNs can also be used for image and object orientation estimation tasks. Over multiple state-of-the-art DCNN architectures, such as VGG, ResNet, and STN, we consistently observe that replacing regular filters with the proposed ARFs leads to significant reduction in the number of network parameters and improvement in classification performance. We report the best results on several commonly used benchmarks.",,,,"United States of America,China",,,,,,2024-10-08 13:35,,,,,,"Academia,Academia",,,,,,,"Academia,Academia",,,,,,,,,
DeepStack,Games,Poker,"University of Alberta,Charles University,Czech Technical University","Matej MoravÄÃ­k, Martin Schmid, Neil Burch, Viliam LisÃ½, Dustin Morrill, Nolan Bard, Trevor Davis, Kevin Waugh, Michael Johanson, Michael Bowling",2017-01-06,DeepStack: Expert-Level Artificial Intelligence in No-Limit Poker,https://arxiv.org/abs/1701.01724,855.0,SOTA improvement,"first human-competitive poker AI, confirmed by website: https://www.deepstack.ai/",2500000.0,"Figure 3, p.9

source: https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389",1.446336e+19,"The largest source of compute necessary for training seems to be the data generation job on 20 GPUs. We count this towards the training compute because it requires simulation using the network. This is analogous to the AlphaGo systems simulating Go games.

From p.26: ""For the flop network, one million poker flop situations (from after the flop cards are dealt) were generated and solved. These situations were solved using DeepStackâ€™s depth limited solver with the turn network used for the counterfactual values at public states immediately after the turn card. We used a cluster of 20 GPUS and one-half of a GPU year of computation time.""

Assume they used P100 GPUs because they were common at the time (P100 was released in 2016 and this paper was published in 2017).

But assume low utilization of 10% to hedge on (a) lower-performing GPUs being used, (b) non-FLOP computations taking up a lot of the data generation job.

Calculation:
6 months * 30 days * 24 hours * 3600 seconds * 9.3e12 FLOP/s * 0.1 utilization = 1.446336e+19 FLOP.",,,10000000.0,"""The turn network was trained by solving 10 million randomly generated poker turn
games. These turn games used randomly generated ranges, public cards, and a random pot
size (10).""",218.0,from compute notes - around 9 days  - half a year of GPU compute using 20 GPUs,,,Speculative,,,,,"Canada,Czechia,Czechia",,,,20.0,,2025-05-28 16:18,,,,,,"Academia,Academia,Academia",,,,4368.0,,,"Academia,Academia,Academia",,,FP32,,Hardware,,,,13
GCNN-14,Language,Language modeling,Facebook AI Research,"Yann N. Dauphin, Angela Fan, Michael Auli, David Grangier, Yann N. Dauphin, Angela Fan, Michael Auli, David Grangier",2016-12-23,"Language Modeling with Gated Convolutional Networks, Language Modeling with Gated Convolutional Networks",https://arxiv.org/abs/1612.08083,2179.0,Highly cited,,,,,,WikiText-103,,,,,,,,Unknown,"The pre-dominant approach to language modeling to date is based on recurrent neural networks. Their success on this task is often linked to their ability to capture unbounded context. In this paper we develop a finite context approach through stacked convolutions, which can be more efficient since they allow parallelization over sequential tokens. We propose a novel simplified gating mechanism that outperforms Oord et al (2016) and investigate the impact of key architectural decisions. The proposed approach achieves state-of-the-art on the WikiText-103 benchmark, even though it features long-term dependencies, as well as competitive results on the Google Billion Words benchmark. Our model reduces the latency to score a sentence by an order of magnitude compared to a recurrent baseline. To our knowledge, this is the first time a non-recurrent approach is competitive with strong recurrent models on these large scale language tasks.",35.0,"GCNN-14,GCNN-14",Unreleased,United States of America,,,,,,2024-11-01 10:02,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
"GCRN-M1, dropout",Language,Language modeling,Ecole Polytechnique FÂ´edÂ´erale de Lausanne (EPFL),"Youngjoo Seo, MichaÃ«l Defferrard, Pierre Vandergheynst, Xavier Bresson",2016-12-22,Structured Sequence Modeling with Graph Convolutional Recurrent Networks,https://arxiv.org/abs/1612.07659,684.0,,,42000000.0,42M (Table 2),3040000000000000.0,6 FLOP / parameter / token * 42000000 parameters * 929000 tokens * 13 epochs = 3.043404e+15 FLOP,Penn TreeBank (PTB),,929000.0,"""All experiments have 13 epochs""",,,,,Confident,"This paper introduces Graph Convolutional Recurrent Network (GCRN), a deep learning model able to predict structured sequences of data. Precisely, GCRN is a generalization of classical recurrent neural networks (RNN) to data structured by an arbitrary graph. Such structured sequences can represent series of frames in videos, spatio-temporal measurements on a network of sensors, or random walks on a vocabulary graph for natural language modeling. The proposed model combines convolutional neural networks (CNN) on graphs to identify spatial structures and RNN to find dynamic patterns. We study two possible architectures of GCRN, and apply the models to two practical problems: predicting moving MNIST data, and modeling natural language with the Penn Treebank dataset. Experiments show that exploiting simultaneously graph spatial and dynamic information about data can improve both precision and learning speed.",13.0,"""GCRN-M1, dropout""",Unreleased,Switzerland,,,,,,2025-04-18 11:10,,,,,,Academia,,,,,Unreleased,,Academia,,,,,Operation counting,,,,
Diabetic Retinopathy Detection Net,Vision,Image classification,"UT Austin,University of California (UC) Berkeley,Google","V Gulshan, L Peng, M Coram, MC Stumpe, D Wu",2016-12-13,Development and Validation of a Deep Learning Algorithm for Detection of Diabetic Retinopathy in Retinal Fundus Photographs,https://jamanetwork.com/journals/jama/article-abstract/2588763,3540.0,Highly cited,,,,,,,,,,,,,,Unknown,,,,,"United States of America,United States of America,United States of America",,,,,,2024-09-05 14:08,,,,,,"Academia,Academia,Industry",,,,,,,"Academia,Academia,Industry",,,,,,,,,
GAN-Advancer,Vision,Image classification,OpenAI,"Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi Chen",2016-12-05,Improved Techniques for Training GANs,https://dl.acm.org/doi/10.5555/3157096.3157346,8353.0,Highly cited,,,,,,,,,,,,,,Unknown,"We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3%. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.",,,Unreleased,United States of America,,,,,,2025-05-28 16:18,,,,,,Industry,,,,,Open (non-commercial),"code, no weights, unclear license:
https://github.com/openai/improved-gan 
experiment code: https://github.com/openai/improved-gan/tree/master/mnist_svhn_cifar10 ",Industry,,,FP32,,,,,,
Elastic weight consolidation,"Vision,Games","Image classification,Atari",DeepMind,"James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, Raia Hadsell",2016-12-02,Overcoming catastrophic forgetting in neural networks,https://arxiv.org/abs/1612.00796,6340.0,Highly cited,,,,,,,,,,,,,,Unknown,"The ability to learn tasks in a sequential fashion is crucial to the development of artificial intelligence. Neural networks are not, in general, capable of this and it has been widely thought that catastrophic forgetting is an inevitable feature of connectionist models. We show that it is possible to overcome this limitation and train networks that can maintain expertise on tasks which they have not experienced for a long time. Our approach remembers old tasks by selectively slowing down learning on the weights important for those tasks. We demonstrate our approach is scalable and effective by solving a set of classification tasks based on the MNIST hand written digit dataset and by learning several Atari 2600 games sequentially.",,,,United Kingdom of Great Britain and Northern Ireland,,,,,,2024-11-01 10:02,,,,,,Industry,,,,,,,Industry,,,,,,,,,
PointNet,3D modeling,3D segmentation,Stanford University,"CR Qi, H Su, K Mo, LJ Guibas",2016-12-02,PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation,https://arxiv.org/abs/1612.00593,12651.0,Highly cited,,,,,,,,,,,,,,Unknown,,,,,United States of America,,,,,,2025-05-28 16:18,,,,,,Academia,,,,,,,Academia,,,,,,,,,
Image-to-image cGAN,"Vision,Image generation",Image generation,University of California (UC) Berkeley,"Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, Alexei A. Efros",2016-11-21,Image-to-Image Translation with Conditional Adversarial Networks,https://arxiv.org/abs/1611.07004,18207.0,Highly cited,,,,,,,,,,,,,,Unknown,"We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Indeed, since the release of the pix2pix software associated with this paper, a large number of internet users (many of them artists) have posted their own experiments with our system, further demonstrating its wide applicability and ease of adoption without the need for parameter tweaking. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.",,,,United States of America,,,,,,2024-11-01 10:02,,,,,,Academia,,,,,,,Academia,,,,,,,,,
RefineNet,Vision,Object detection,"University of Adelaide,Australian Centre for Robotic Vision","Guosheng Lin, Anton Milan, Chunhua Shen, Ian Reid",2016-11-20,RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation,https://arxiv.org/abs/1611.06612v3,2736.0,Highly cited,,,,,,,,,,,,,,Unknown,,,,,"Australia,Australia",,,,,,2024-11-01 10:02,,,,,,Academia,,,,,,,Academia,,,,,,,,,
PolyNet,Vision,Image classification,Chinese University of Hong Kong (CUHK),"X Zhang, Z Li, C Change Loy",2016-11-17,PolyNet: A Pursuit of Structural Diversity in Very Deep Networks,https://arxiv.org/abs/1611.05725,282.0,SOTA improvement,"""The Very Deep PolyNet, designed following this direction, demonstrates substantial improvements over the state-of-the-art on the ILSVRC 2012 benchmark. Compared to Inception-ResNet-v2, it reduces the top-5 validation error on single crops from 4.9% to 4.25%, and that on multi-crops from 3.7% to 3.45%.""",92000000.0,,6.4e+19,"Section 5: ""ResNet-500 [has] similar computation
costs to our Very Deep PolyNet"".

ResNet-152 has 11.3e9 FLOP per forward pass (https://arxiv.org/abs/1512.03385, Table 1). Hence ResNet-500 has approx 3.7e10 = 11.3e9*500/152 FLOP per forward pass.

560k iterations, batch size 512:
Train compute = 3.7e10*3*2*560e3 * 512 = 6.4e19",ImageNet,Section 4,1280000.0,,,,NVIDIA GeForce GTX TITAN X,,Likely,,,,,"Hong Kong,China",,,,32.0,,2025-05-28 16:18,,,,,,Academia,,,,,,,Academia,$615.55,,FP32,16797.38519252759,"Comparison with other models,Operation counting",,,,8
Deeply-recursive ConvNet,"Vision,Image generation",Image super-resolution,Seoul National University,"Jiwon Kim, Jung Kwon Lee, Kyoung Mu Lee",2016-11-11,Deeply-Recursive Convolutional Network for Image Super-Resolution,https://arxiv.org/abs/1511.04491,2395.0,Highly cited,,,,,,,,,,,,,,Unknown,,,,,Korea (Republic of),,,,,,2024-11-01 10:02,,,,,,Academia,,,,,,,Academia,,,,,,,,,
NASv3 (CIFAR-10),Vision,"Image classification,Neural Architecture Search - NAS",Google Brain,"Barret Zoph, Quoc V. Le",2016-11-05,Neural Architecture Search with Reinforcement Learning,https://arxiv.org/abs/1611.01578,5098.0,Highly cited,,37400000.0,Table 1,2.2e+21,"50 epochs * 50,000 images * 10.0 GFLOPSs * 12800 networks * 2 add-multiply * 3 backward pass 
= 1.9e6 PF = 22 pfs-days

source: https://openai.com/blog/ai-and-compute/",,,50000.0,CIFAR-10 (does not factor in augmentation procedures),,,,,Likely,"Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines. Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.",,,,United States of America,,,,800.0,,2025-03-06 15:00,,,,,,Industry,,,,,,,Industry,,,,,"Third-party estimation,Operation counting",,,,2
NAS with base 8 and shared embeddings,Language,"Language modeling,Neural Architecture Search - NAS",Google Brain,"Barret Zoph, Quoc V. Le",2016-11-05,Neural Architecture Search with Reinforcement Learning,https://arxiv.org/abs/1611.01578,5098.0,Highly cited,,54000000.0,54M (Table 2),1.05e+16,6 FLOP / parameter / token * 54000000 parameters * 929000 tokens * 35 epochs = 1.053486e+16 FLOP,Penn TreeBank (PTB),,929000.0,"""every child model is constructed and trained for 35 epochs""",,,,,Confident,"Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines. Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.",35.0,Neural Architecture Search with base 8 and shared embeddings,Unreleased,United States of America,,,,,,2025-05-01 10:42,,,,,,Industry,,,,,Unreleased,,Industry,,,,,Operation counting,,,,
BIDAF,Language,Question answering,"University of Washington,Allen Institute for AI","Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, Hannaneh Hajishirzi",2016-11-05,Bidirectional Attention Flow for Machine Comprehension,https://arxiv.org/abs/1611.01603v6,2246.0,"Highly cited,SOTA improvement","""Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test. """,2600000.0,"There are two similar models described in sections ""Models details""
citation from the paper about model for SQuAD ""The model has about 2.6 million parameters""
citation about model for cloze test
""The model architecture used for this task is very similar to that for SQuAD (Section 4) with only a few small changes to adapt it to the cloze test. ""
",3.4686144e+18,"flops = (8) * (6691 * 10**9) * (60 * 3600) * 3 // 10
(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate) =

citation from the section about cloze test experiments ""The entire training process takes roughly 60 hours on eight Titan X GPUs. The other hyper-parameters are identical to the model described in Section 4"" (section 4 is about SQuAD experiments and cloze test experiments require more compute and data).
flops  6.691 TFLOPS from https://www.techpowerup.com/gpu-specs/geforce-gtx-titan-x.c2632","SQuAD,DMQA,GloVe","""In a cloze test, the reader is asked to fill in words that have been removed from a passage, for measuring oneâ€™s ability to comprehend text. Hermann et al. (2015) have recently compiled a massive Cloze-style comprehension dataset, consisting of 300k/4k/3k and 879k/65k/53k (train/dev/test)
examples from CNN and DailyMail news articles, respectively. """,47160000.0,"""In a cloze test, the reader is asked to fill in words that have been removed from a passage, for measuring oneâ€™s ability to comprehend text. Hermann et al. (2015) have recently compiled a massive Cloze-style comprehension dataset, consisting of 300k/4k/3k and 879k/65k/53k (train/dev/test)
examples from CNN and DailyMail news articles, respectively. ""
assuming 40 words per example we get around 47160000 words (SQuAD have around 40 words per example - so I think it should be similar case for this dataset)",60.0,see compute notes,NVIDIA GeForce GTX TITAN X,,Confident,"Machine comprehension (MC), answering a query about a given context paragraph, requires modeling complex interactions between the context and the query. Recently, attention mechanisms have been successfully extended to MC. Typically these methods use attention to focus on a small portion of the context and summarize it with a fixed-size vector, couple attentions temporally, and/or often form a uni-directional attention. In this paper we introduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage hierarchical process that represents the context at different levels of granularity and uses bi-directional attention flow mechanism to obtain a query-aware context representation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test. ",8.0,,Open weights (unrestricted),"United States of America,United States of America",,,,8.0,,2025-01-03 15:53,,,,,,"Academia,Research collective",,,,480.0,Open source,"apache 2.0, code + weights: https://github.com/allenai/bi-att-flow","Academia,Research collective",$41.25,,,4200.468649711526,Hardware,,,,15
VD-LSTM+REAL Large,Language,Language modeling,"Salesforce Research,Stanford University","Hakan Inan, Khashayar Khosravi, Richard Socher",2016-11-04,Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling,https://arxiv.org/abs/1611.01462,397.0,SOTA improvement,"""Our framework leads to state of the art performance on the Penn Treebank""",51000000.0,51M (Table 3),2.13e+16,6 FLOP / parameter / token * 51000000 parameters * 929000 tokens * 75 epochs = 2.132055e+16 FLOP,Penn TreeBank (PTB),,929000.0,75 epochs (Figure 2b),,,,,Confident,"Recurrent neural networks have been very successful at predicting sequences of words in tasks such as language modeling. However, all such models are based on the conventional classification framework, where the model is trained against one-hot targets, and each word is represented both as an input and as an output in isolation. This causes inefficiencies in learning both in terms of utilizing all of the information and in terms of the number of parameters needed to train. We introduce a novel theoretical framework that facilitates better learning in language modeling, and show that our framework leads to tying together the input embedding and the output projection matrices, greatly reducing the number of trainable variables. Our framework leads to state of the art performance on the Penn Treebank with a variety of network models.",75.0,VD-LSTM+REAL Large,Unreleased,"United States of America,United States of America",,,,,,2025-04-18 11:20,,,,,,"Industry,Academia",,,,,Unreleased,,"Industry,Academia",,,,,Operation counting,,,,
SPIDER2,Biology,"Protein folding prediction,Proteins","Griffith University,University of Iowa,Dezhou University","Yuedong Yang, Rhys Heffernan, Kuldip Paliwal, James Lyons, Abdollah Dehzangi, Alok Sharma, Jihua Wang, Abdul Sattar, and Yaoqi Zhou",2016-10-28,"SPIDER2: A Package to Predict Secondary Structure, Accessible Surface Area, and Main-Chain Torsional Angles by Deep Neural Networks",https://link.springer.com/protocol/10.1007/978-1-4939-6406-2_6,,SOTA improvement,"The method provides state-of-the-art, all-in-one accurate prediction of local structure and solvent accessible surface area. ",409536.0,"Three networks, each three layers. First takes in 459 inputs and outputs 12, second and third take in 459 + (12*17) = 663 inputs.

Network 1: (459 * 150 + 150) + (150 * 150 + 150) + (150 * 150 + 150) + (150 * 12 + 12) = 116,112
Networks 2 and 3: (663 * 150 + 150) + (150 * 150 + 150) + (150 * 150 + 150) + (150 * 12 + 12) = 146,712
Total: 116,112 + (2 * 146,712) = 409,536",1.822e+16,"120 epochs, dataset 5789 proteins. There are about 300 residues per protein (115,479 residues / 418 proteins) according to https://www.ncbi.nlm.nih.gov/pmc/articles/PMC22960/. 
First network gets 27 features per residue, second and third get 39.
FLOPs from first: 6 * 116112 * (27 * 300 * 5789 * 120) = 3.92e15
FLOPs from 2nd and 3rd: 2 *6 * 146712 * (39 * 300 * 5789 * 120) = 1.43e16
Total: 1.822E16",Unspecified,,13893600.0,"5,789 nonredundant, high resolution structure.
Assuming ~200 residues per protein, 5,789 * 200 = 1,157,800 residues. Each residue has 12 associated features being predicted on.
1,157,800 * 12 = 13,893,600",,"The authors had a website where sequences could be submitted for processing through the model: ""Each prediction is usually completed within 10 min, but may take up to a few hours depending on how busy the server is and how long the protein chain is [...] Using an external PSSM file can skip the most time consuming step of generating the evolution profile by PSIBLAST, and the executive time reduce to a few seconds"" 

Rough estimate:
It looks like the PSIBLAST step only needs doing once per input, and this takes the majority of the time. If the inference server uses the same hardware that was used for training, 10 mins * 5789 sequences =  965 hours for PSIBLAST calculation. Then assume training on a sequence takes 3x as long as inference (forward + backward pass uses 6 FLOPs per parameter, vs 2 for forward only), so 120 epochs would take:
3 seconds * 3 * 5789 * 120 = 1,737 hours
Total: around 2,702 hours
(This seems on the long side â€“ probably they had better hardware for training, or else there's an incorrect assumption here)",,Supervised,Likely,"Predicting one-dimensional structure properties has played an important role to improve prediction of protein three-dimensional structures and functions. The most commonly predicted properties are secondary structure and accessible surface area (ASA) representing local and nonlocal structural characteristics, respectively. Secondary structure prediction is further complemented by prediction of continuous main-chain torsional angles. Here we describe a newly developed method SPIDER2 that utilizes three iterations of deep learning neural networks to improve the prediction accuracy of several structural properties simultaneously. For an independent test set of 1199 proteins SPIDER2 achieves 82 % accuracy for secondary structure prediction, 0.76 for the correlation coefficient between predicted and actual solvent accessible surface area, 19Â° and 30Â° for mean absolute errors of backbone Ï† and Ïˆ angles, respectively, and 8Â° and 32Â° for mean absolute errors of CÎ±-based Î¸ and Ï„ angles, respectively. The method provides state-of-the-art, all-in-one accurate prediction of local structure and solvent accessible surface area. The method is implemented, as a webserver along with a standalone package that are available in our website: http://sparks-lab.org.",120.0,,Open weights (non-commercial),"Australia,United States of America,China",,,,,,2025-05-09 11:32,,,,,,"Academia,Academia,Academia",,,,,,"some kind of download, unclear license

http://zhouyq-lab.szbl.ac.cn/download/","Academia,Academia,Academia",,,,,Operation counting,,,,
Differentiable neural computer,Language,Question answering,Google DeepMind,"Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka Grabska-BarwiÅ„ska, Sergio GÃ³mez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou, AdriÃ  PuigdomÃ¨nech Badia, Karl Moritz Hermann, Yori Zwols, Georg Ostrovski, Adam Cain, Helen King, Christopher Summerfield, Phil Blunsom, Koray Kavukcuoglu & Demis Hassabis",2016-10-12,Hybrid computing using a neural network with dynamic external memory,https://www.nature.com/articles/nature20101,1521.0,Highly cited,,,,,,,,,,,,,,Unknown,"Artificial neural networks are remarkably adept at sensory processing, sequence learning and reinforcement learning, but are limited in their ability to represent variables and data structures and to store data over long timescales, owing to the lack of an external memory. Here we introduce a machine learning model called a differentiable neural computer (DNC), which consists of a neural network that can read from and write to an external memory matrix, analogous to the random-access memory in a conventional computer. Like a conventional computer, it can use its memory to represent and manipulate complex data structures, but, like a neural network, it can learn to do so from data. When trained with supervised learning, we demonstrate that a DNC can successfully answer synthetic questions designed to emulate reasoning and inference problems in natural language. We show that it can learn tasks such as finding the shortest path between specified points and inferring the missing links in randomly generated graphs, and then generalize these tasks to specific graphs such as transport networks and family trees. When trained with reinforcement learning, a DNC can complete a moving blocks puzzle in which changing goals are specified by sequences of symbols. Taken together, our results demonstrate that DNCs have the capacity to solve complex, structured tasks that are inaccessible to neural networks without external readâ€“write memory.",,,,"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France",,,,,,2024-11-01 10:03,,,,,,Industry,,,,,,,Industry,,,,,,,,,
Xception,Vision,Image classification,Google,FranÃ§ois Chollet,2016-10-07,Xception: Deep Learning with Depthwise Separable Convolutions,https://arxiv.org/abs/1610.02357,13038.0,Highly cited,,22855952.0,Table 3,4.3599999999999993e+20,"60 K80 GPUs * 30 days * 8.5 TFLOPS/GPU * 0.33 utilization  = 4.36e20

Authors of ""AI and Memory Wall"" (https://github.com/amirgholami/ai_and_memory_wall) estimated model's training compute as 450,000 PFLOP = 4.5*10^20 FLOP",JFT,"Also ImageNet, but JFT is significantly larger",350000000.0,"""JFT is an internal Google dataset for large-scale image classification dataset, first introduced by Hinton et al. in [5], which comprises over 350 million high-resolution images annotated with labels from a set of 17,000 classes. To evaluate the performance of a model trained on JFT, we use an auxiliary dataset, FastEval14k""",720.0,"""while the JFT experiments took over one month each.""",NVIDIA Tesla K80,,Confident,"We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters.",,,,United States of America,,,,60.0,,2025-05-08 17:30,,,,,,Industry,,,,43200.0,,,Industry,$12617.63,,,37828.64014138488,"Hardware,Third-party estimation",,,,3
GNMT,Language,Translation,Google,"Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Åukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, Jeffrey Dean",2016-09-26,Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation,https://arxiv.org/abs/1609.08144,6483.0,Highly cited,,278000000.0,"Table 5 in 'Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer'

https://arxiv.org/abs/1701.06538",6.620000000001001e+21,"From AI and Compute:
""sqrt(10 * 100) factor added because production model used 2-3 orders of magnitude more data, but only 1 epoch rather than 10.
96 K80 GPUâ€™s * 9 days * 8.5 TFLOPS * 0.33 utilization * sqrt(10 * 100)  
= 6.9e6 PF = 79 pfs-days""
source: https://openai.com/blog/ai-and-compute/

https://www.wolframalpha.com/input?i=96+*+9+days+*+8.5+TFLOPS+*+0.33+*+sqrt%281000%29
",,,388960152201.0,"[WORDS]
"" On WMT Enâ†’Fr, the training set contains 36M sentence pairs. On WMT Enâ†’De, the training set contains 5M sentence pairs.""
""we also test GNMT on Googleâ€™s translation production corpora, which are two to three decimal orders of magnitudes bigger than the WMT corpora for a given language pair.""

41M sentence pairs * 2 sentences per pair * 15 words/sentence * 10^2.5",,"Test model used 96 K80 for 9 days, then this was scaled up by 31x for the production model, but unclear how many GPUs were used or how long it was trained for. The production run used 96 * 9 days * sqrt(1000) ~= 655730 chip-hours.",NVIDIA Tesla K80,Reinforcement learning,Likely,"Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference. Also, most NMT systems have difficulty with rare words. These issues have hindered NMT's use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google's Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using attention and residual connections. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units (""wordpieces"") for both input and output. This method provides a good balance between the flexibility of ""character""-delimited models and the efficiency of ""word""-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. On the WMT'14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60% compared to Google's phrase-based production system.",1.0,,Hosted access (no API),United States of America,,,,,,2025-06-18 03:55,,,,,,Industry,,,,655730.0,Unreleased,presumably deployed via Google translate,Industry,$193787.11,,FP32,,"Hardware,Third-party estimation",,,,1
Pointer Sentinel-LSTM (medium),Language,Language modeling,"MetaMind Inc,Salesforce","Stephen Merity, Caiming Xiong, James Bradbury, Richard Socher",2016-09-26,Pointer Sentinel Mixture Models,https://arxiv.org/abs/1609.07843,2307.0,"Highly cited,SOTA improvement","""Our pointer sentinel-LSTM model achieves state of the art language modeling performance on the Penn Treebank (70.9 perplexity) while using far fewer parameters than a standard softmax LSTM""",21000000.0,,7490000000000000.0,6 FLOP / parameter / token * 21000000 parameters * 929000 tokens * 64 epochs = 7.491456e+15 FLOP,Penn TreeBank (PTB),,929000.0,"""We also halve the learning rate when validation perplexity is worse than the previous iteration, stopping training when validation perplexity fails to improve for three epochs or when 64 epochs are reached""",,,,,Confident,Recent neural network sequence models with softmax classifiers have achieved their best language modeling performance only with very large hidden states and large vocabularies. Even then they struggle to predict rare or unseen words even if the context makes the prediction unambiguous. We introduce the pointer sentinel mixture architecture for neural sequence models which has the ability to either reproduce a word from the recent context or produce a word from a standard softmax classifier. Our pointer sentinel-LSTM model achieves state of the art language modeling performance on the Penn Treebank (70.9 perplexity) while using far fewer parameters than a standard softmax LSTM. In order to evaluate how well language models can exploit longer contexts and deal with more realistic vocabularies and larger corpora we also introduce the freely available WikiText corpus.,64.0,Pointer Sentinel-LSTM (medium),Unreleased,"United States of America,United States of America",,,,,,2025-04-18 11:26,,,,,,"Industry,Industry",,,,,Unreleased,,"Industry,Industry",,,,,Operation counting,,,,
Zoneout + Variational LSTM (WT2),Language,Language modeling,"MetaMind Inc,Salesforce","Stephen Merity, Caiming Xiong, James Bradbury, Richard Socher",2016-09-26,Pointer Sentinel Mixture Models,https://arxiv.org/abs/1609.07843,2307.0,Highly cited,,21000000.0,21M (Table 3),1.6128e+16,6 FLOP / parameter / token * 21000000 parameters * 2000000 tokens * 64 epochs = 1.6128e+16 FLOP,WikiText-2,,2000000.0,"""We also halve the learning rate when validation perplexity is worse than the previous iteration, stopping training when validation perplexity fails to improve for three epochs or when 64 epochs are reached""",,,NVIDIA Tesla K80,,Confident,Recent neural network sequence models with softmax classifiers have achieved their best language modeling performance only with very large hidden states and large vocabularies. Even then they struggle to predict rare or unseen words even if the context makes the prediction unambiguous. We introduce the pointer sentinel mixture architecture for neural sequence models which has the ability to either reproduce a word from the recent context or produce a word from a standard softmax classifier. Our pointer sentinel-LSTM model achieves state of the art language modeling performance on the Penn Treebank (70.9 perplexity) while using far fewer parameters than a standard softmax LSTM. In order to evaluate how well language models can exploit longer contexts and deal with more realistic vocabularies and larger corpora we also introduce the freely available WikiText corpus.,64.0,Zoneout + Variational LSTM (WT2),Unreleased,"United States of America,United States of America",,,,,,2025-04-18 11:31,,,,,,"Industry,Industry",,,,,Unreleased,,"Industry,Industry",,,,,Operation counting,,,,
Knowledge distillation student model,Language,Translation,Harvard University,"Yoon Kim, Alexander M. Rush",2016-09-22,Sequence-Level Knowledge Distillation,https://arxiv.org/abs/1606.07947,,,,84000000.0,84M from Table 1.,1.008e+17,6ND = 6 FLOP/param/token * 84000000 parameters * 200000000 tokens = 1.008e+17 FLOP (Speculative confidence since the amount of epochs is unknown),WMT English-German,,200000000.0,"""The training set has 4m sentences"". 

If the average sentence is ~25 tokens (ballpark), dataset size is 
4M * 25 * 2 = 200M tokens",,,,,Speculative,"Neural machine translation (NMT) offers a novel alternative formulation of translation that is potentially simpler than statistical approaches. However to reach competitive performance, NMT models need to be exceedingly large. In this paper we consider applying knowledge distillation approaches (Bucila et al., 2006; Hinton et al., 2015) that have proven successful for reducing the size of neural models in other domains to the problem of NMT. We demonstrate that standard knowledge distillation applied to word-level prediction can be effective for NMT, and also introduce two novel sequence-level versions of knowledge distillation that further improve performance, and somewhat surprisingly, seem to eliminate the need for beam search (even when applied on the original teacher model). Our best student model runs 10 times faster than its state-of-the-art teacher with little loss in performance. It is also significantly better than a baseline model trained without knowledge distillation: by 4.2/1.7 BLEU with greedy decoding/beam search. Applying weight pruning on top of knowledge distillation results in a student model that has 13 times fewer parameters than the original teacher model, with a decrease of 0.4 BLEU.",,,,United States of America,,,,,,2025-03-04 10:23,,,,,,Academia,,,,,Open source,"https://github.com/harvardnlp/seq2seq-attn?tab=readme-ov-file
MIT License",Academia,,,,,Operation counting,,,,
Wide Residual Network,Vision,Image classification,UniversitÃ© Paris-Est,"Sergey Zagoruyko, Nikos Komodakis",2016-09-19,Wide Residual Networks,https://arxiv.org/abs/1605.07146,7455.0,Highly cited,,,,,,,,,,,,,,Unknown,,,,,France,,,,,,2025-05-28 16:18,,,,,,Academia,,,,,,,Academia,,,FP32,,,,,,
MS-CNN,Vision,Object detection,"IBM,University of California San Diego","Zhaowei Cai, Quanfu Fan, Rogerio S. Feris, Nuno Vasconcelos",2016-09-17,A Unified Multi-scale Deep Convolutional Neural Network for Fast Object Detection,https://link.springer.com/chapter/10.1007/978-3-319-46493-0_22,1458.0,Highly cited,,,,,,,,,,,,,,Unknown,,,,,"United States of America,United States of America",,,,,,2025-05-28 16:18,,,,,,"Industry,Academia",,,,,,,"Industry,Academia",,,FP32,,,,,,
Stacked hourglass network,Vision,Pose estimation,University of Michigan,"Alejandro Newell, Kaiyu Yang, Jia Deng",2016-09-17,Stacked Hourglass Networks for Human Pose Estimation,https://link.springer.com/chapter/10.1007/978-3-319-46484-8_29,4808.0,Highly cited,,,,,,,,,,,,,,Unknown,,,,,United States of America,,,,,,2025-05-28 16:18,,,,,,Academia,,,,,,,Academia,,,FP32,,,,,,
TSN,Video,Action recognition,"ETH Zurich,Shenzhen Institute of Advanced Technology,Chinese University of Hong Kong (CUHK)","Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoou Tang, Luc Van Gool",2016-09-17,Temporal Segment Networks: Towards Good Practices for Deep Action Recognition,https://link.springer.com/chapter/10.1007/978-3-319-46484-8_2,3621.0,Highly cited,,,,,,,,,,,,,,Unknown,,,,,"Switzerland,China,Hong Kong,China",,,,,,2025-05-28 16:18,,,,,,"Academia,Academia",,,,,,,"Academia,Academia",,,FP32,,,,,,
ResNet-200,Vision,Image classification,Microsoft Research Asia,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun",2016-09-17,Identity Mappings in Deep Residual Networks,https://link.springer.com/chapter/10.1007/978-3-319-46493-0_38,9621.0,Highly cited,,,,2.9741645e+19,"""ResNet-200 takes about 3 weeks to train on 8 GPUs"". didn't specify which GPU
upd: 
common GPU performance for 2016 is 6.83E+12 FLOPs/s (https://epoch.ai/blog/estimating-training-compute#forward-pass-compute-and-parameter-counts-of-common-layers) 
then 6.83E+12*3*7*24*3600*8*0.3=2.9741645e+19 (Speculative)",ImageNet,,1281167.0,,500.0,"""about 3 weeks""",,,Speculative,"Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR10 (4.62 % error) and CIFAR-100, and a 200-layer ResNet on ImageNet.",,,Unreleased,China,,,,,,2025-02-19 12:43,,,,,,Industry,,,,,Open (non-commercial)," https://github.com/KaimingHe/resnet-1k-layers
no definite license",Industry,,,,,Hardware,,,,6
Youtube recommendation model,Recommendation,Recommender system,Google,"Paul Covington, Jay Adams, and Emre Sargin",2016-09-15,Deep Neural Networks for YouTube Recommendations,https://research.google/pubs/pub45530/,2914.0,Highly cited,,,,,,,,,,,,,,Unknown,"YouTube represents one of the largest scale and most sophisticated industrial recommendation systems in existence. In this paper, we describe the system at a high level and focus on the dramatic performance improvements brought by deep learning. The paper is split according to the classic two-stage information retrieval dichotomy: first, we detail a deep candidate generation model and then describe a separate deep ranking model. We also provide practical lessons and insights derived from designing, iterating and maintaining a massive recommendation system with enormous user-facing impact.",,,,United States of America,,,,,,2024-11-01 10:03,,,,,,Industry,,,,,,,Industry,,,,,,,,,
WaveNet,Speech,"Text-to-speech,Speech synthesis,Audio generation",Google DeepMind,"A Oord, S Dieleman, H Zen, K Simonyan",2016-09-12,WaveNet: A Generative Model for Raw Audio,https://arxiv.org/abs/1609.03499,6929.0,Highly cited,,,,,,,,,,,,,,Unknown,"This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.",,,,"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France",,,,,,2024-11-01 10:03,,,,,,Industry,,,,,,,Industry,,,,,,,,,
Multi-task Cascaded CNN,Vision,Face detection,"Chinese Academy of Sciences,Chinese University of Hong Kong (CUHK)","Kaipeng Zhang, Zhanpeng Zhang, Zhifeng Li, Yu Qiao",2016-08-26,Joint Face Detection and Alignment using Multitask cascaded convolutional networks,https://arxiv.org/abs/1604.02878,4605.0,Highly cited,,,,,,,,,,,,,,Unknown,,,,,"China,Hong Kong,China",,,,,,2024-11-01 10:03,,,,,,"Academia,Academia",,,,,,,"Academia,Academia",,,,,,,,,
SimpleNet,Vision,Image classification,"Sensifai,Islamic Azad University,Technicolor R&I,Institute for Research in Fundamental Sciences (IPM)","Seyyed Hossein Hasanpour, Mohammad Rouhani, Mohsen Fayyaz, Mohammad Sabokrou",2016-08-22,"Lets keep it simple, Using simple architectures to outperform deeper and more complex architectures",https://arxiv.org/abs/1608.06037,117.0,SOTA improvement,"""We achieved state-of-theart result on CIFAR10 outperforming several heavier architectures""",5480000.0,SOTA CIFAR-10 model was 5.48m params,,,"CIFAR-10,ImageNet","""We experimented on CIFAR-10/100 Krizhevsky & Hinton (2009), SVHN Netzer et al. (2011), MNIST
Lecun et al. (1998) and ILSVRC 2012 classification task Russakovsky et al. (2015) datasets in order
to evaluate and compare our architecture""",,,,,NVIDIA GeForce GTX 980,,Confident,"Major winning Convolutional Neural Networks (CNNs), such as AlexNet, VGGNet, ResNet, GoogleNet, include tens to hundreds of millions of parameters, which impose considerable computation and memory overhead. This limits their practical use for training, optimization and memory efficiency. On the contrary, light-weight architectures, being proposed to address this issue, mainly suffer from low accuracy. These inefficiencies mostly stem from following an ad hoc procedure. We propose a simple architecture, called SimpleNet, based on a set of designing principles, with which we empirically show, a well-crafted yet simple and reasonably deep architecture can perform on par with deeper and more complex architectures. SimpleNet provides a good tradeoff between the computation/memory efficiency and the accuracy. Our simple 13-layer architecture outperforms most of the deeper and complex architectures to date such as VGGNet, ResNet, and GoogleNet on several well-known benchmarks while having 2 to 25 times fewer number of parameters and operations. This makes it very handy for embedded systems or systems with computational and memory limitations. We achieved state-of-theart result on CIFAR10 outperforming several heavier architectures, near state of the art on MNIST and highly competitive results on CIFAR100 and SVHN. We also outperformed the much larger and deeper architectures such as VGGNet and popular variants of ResNets among others on the ImageNet dataset. Models are made available at: https://github.com/Coderx7/SimpleNet",,,,"Belgium,Iran (Islamic Republic of),France,Iran (Islamic Republic of)",,,,,,2025-05-28 16:18,,,,,,"Industry,Academia",,,,,,,"Industry,Academia",,,FP32,,,,,,
Attend-Infer-Repeat,Vision,Object recognition,Google DeepMind,"SM Ali Eslami, Nicolas Heess, Theophane Weber, Yuval Tassa, David Szepesvari, Geoffrey E Hinton",2016-08-12,"Attend, Infer, Repeat: Fast Scene Understanding with Generative Models",https://arxiv.org/abs/1603.08575,560.0,,,82130304.0,from appendix E: The convolutional neural network uses a 64Ã—(5Ã—5)-64Ã—(5Ã—5)-64Ã—(5Ã—5)-512 architecture.,6.448896e+16,(peak FLOPs for GPU - 1244 GFLOPs) times (training time - 3600 * 48 second) * (0.3 assumed utilization rate) ,MNIST,images of multiple MNIST digits,60000.0,60000 MNIST images,48.0,"48 hours for MNIST model, 72 hours for 3D scenes model",NVIDIA Quadro K4000,,Confident,"We present a framework for efficient inference in structured image models that explicitly reason about objects. We achieve this by performing probabilistic inference using a recurrent neural network that attends to scene elements and processes them one at a time. Crucially, the model itself learns to choose the appropriate number of inference steps. We use this scheme to learn to perform inference in partially specified 2D models (variable-sized variational auto-encoders) and fully specified 3D models (probabilistic renderers). We show that such models learn to identify multiple objects â€“ counting, locating and classifying the elements of a scene â€“ without any supervision, e.g., decomposing 3D images with various numbers of objects in a single forward pass of a neural network at unprecedented speed. We
further show that the networks produce accurate inferences when compared to supervised counterparts, and that their structure leads to improved generalization.",,,,"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France",,,,,,2025-02-17 14:17,,,,,,Industry,,,,,,,Industry,,,,,Hardware,,,,
Order embeddings with layer norm,Vision,Image captioning,University of Toronto,"Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton",2016-07-21,Layer Normalization,https://arxiv.org/abs/1607.06450,,,,,possibly can be calculated based on architecture description,,,COCO,,,,,,,,Confident,"Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.",,,,Canada,Order-Embeddings of Images and Language,,,,,2025-05-01 10:42,,,,,,Academia,,,,,,,Academia,,,,,,,,,
Layer Normalization: The Attentive Reader,Language,Question answering,University of Toronto,"Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton",2016-07-21,Layer Normalization,https://arxiv.org/abs/1607.06450,,,,,possibly can be calculated based on architecture description,,,Question Answering Corpus (CNN + Daily Mail),,,,,,,,Confident,"Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.",,,,Canada,The Attentive Reader,,,,,2025-04-10 04:16,,,,,,Academia,,,,,,,Academia,,,,,,,,,
Layer Normalization: Skip Thoughts,Language,Language modeling,University of Toronto,"Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton",2016-07-21,Layer Normalization,https://arxiv.org/abs/1607.06450,,,,,possibly can be calculated based on architecture description,,,"""BookCorpus (BooksCorpus, Toronto Book Corpus)""",,,,720.0,,,,Confident,"Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.",,,,Canada,Skip-Thoughts,,,,,2025-04-10 04:18,,,,,,Academia,,,,,,,Academia,,,,,,,,,
Layer Normalization: Draw,Image generation,Image generation,University of Toronto,"Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton",2016-07-21,Layer Normalization,https://arxiv.org/abs/1607.06450,,,,,possibly can be calculated based on architecture description,,,MNIST,,50000.0,"The dataset has been split into 50,000
training, 10,000 validation and 10,000 test",,,,,Confident,"Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.",,,,Canada,Draw,,,,,2025-04-10 04:19,,,,,,Academia,,,,,,,Academia,,,,,,,,,
Layer Normalization: Handwriting sequence generation,Image generation,Image generation,University of Toronto,"Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton",2016-07-21,Layer Normalization,https://arxiv.org/abs/1607.06450,,,,3700000.0," The total number of
weights was increased to approximately 3.7M. ",,,IAM Online Handwriting Database (IAM-OnDB),,8525300.0,"12179*700=8525300

There are, in total, 12179 handwriting line sequences. The input
string is typically more than 25 characters and the average handwriting line has a length around 700.",,,,,Speculative,"Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.",,,,Canada,RNN+weight noise+dynamic eval,189261660000000,=8525300*3700000*6=1.8926166e+14,,,2024-12-02 10:24,,,,,,Academia,,,,,,,Academia,,,,,Operation counting,,,,
Character-enriched word2vec,Language,Language modeling,Facebook AI Research,"Piotr Bojanowski, Edouard Grave, Armand Joulin, Tomas Mikolov",2016-07-15,Enriching Word Vectors with Subword Information,https://arxiv.org/abs/1607.04606,9415.0,Highly cited,,,,,,,,,,,,,,Unknown,"Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character n-grams. A vector representation is associated to each character n-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.",,,,United States of America,,,,,,2024-11-01 10:03,,,,,,Industry,,,,,,"repo here, unclear how it corresponds to models in this paper:
https://github.com/facebookresearch/fastText ",Industry,,,,,,,,,
VD-RHN,Language,Language modeling,"ETH Zurich,IDSIA","Julian Georg Zilly, Rupesh Kumar Srivastava, Jan KoutnÃ­k, JÃ¼rgen Schmidhuber",2016-07-12,Recurrent Highway Networks,https://arxiv.org/abs/1607.03474,493.0,SOTA improvement,"""On the larger Wikipedia datasets for character prediction (text8 and enwik8), RHNs outperform all previous results and achieve an entropy of 1.27 bits per character.""",32000000.0,"""To examine the effect of recurrence depth we train RHNs with fixed total parameters (32 M)""",3570000000000000.0,6 FLOP / parameter / token * 32000000 parameters * 929000 tokens * 20 epochs = 3.56736e+15 FLOP,Penn TreeBank (PTB),,929000.0,"""Batch size was fixed to 20, sequence length for truncated backpropagation to 35, learning rate to 0.2, learning rate decay to 1.02 starting at 20 epochs, weight decay to 1e-7 and maximum gradient norm to 10.""",,,,,Confident,"Many sequential processing tasks require complex nonlinear transition functions from one step to the next. However, recurrent neural networks with 'deep' transition functions remain difficult to train, even when using Long Short-Term Memory (LSTM) networks. We introduce a novel theoretical analysis of recurrent networks based on Gersgorin's circle theorem that illuminates several modeling and optimization issues and improves our understanding of the LSTM cell. Based on this analysis we propose Recurrent Highway Networks, which extend the LSTM architecture to allow step-to-step transition depths larger than one. Several language modeling experiments demonstrate that the proposed architecture results in powerful and efficient models. On the Penn Treebank corpus, solely increasing the transition depth from 1 to 10 improves word-level perplexity from 90.6 to 65.4 using the same number of parameters. On the larger Wikipedia datasets for character prediction (text8 and enwik8), RHNs outperform all previous results and achieve an entropy of 1.27 bits per character.",20.0,VD-RHN,Unreleased,"Switzerland,Switzerland",,,,,,2025-04-18 11:37,,,,,,"Academia,Academia",,,,,Open source,MIT for code: https://github.com/jzilly/RecurrentHighwayNetworks ,"Academia,Academia",,,,,Operation counting,,,,
fastText,Language,Text classification,Facebook AI Research,"A Joulin, E Grave, P Bojanowski, T Mikolov",2016-07-06,Bag of Tricks for Efficient Text Classification,https://arxiv.org/abs/1607.01759,4362.0,Highly cited,,,,,,,,,,,,,,Unknown,"This paper explores a simple and efficient baseline for text classification. Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train fastText on more than one billion words in less than ten minutes using a standard multicore~CPU, and classify half a million sentences among~312K classes in less than a minute.",,,,United States of America,,,,,,2024-11-01 10:03,,,,,,Industry,,,,,,,Industry,,,,,,,,,
node2vec,Mathematics,Pattern classification,Stanford University,"Aditya Grover, Jure Leskovec",2016-07-03,node2vec: Scalable Feature Learning for Networks,https://arxiv.org/abs/1607.00653,,,,1280000.0,"If a graph has 10,000 nodes and the embeddings are 128-dimensional, then the total number of parameters would be:
P=10,000Ã—128=1,280,000",,,,,,,,,,,Speculative,"Prediction tasks over nodes and edges in networks require careful effort in engineering features used by learning algorithms. Recent research in the broader field of representation learning has led to significant progress in automating prediction by learning the features themselves. However, present feature learning approaches are not expressive enough to capture the diversity of connectivity patterns observed in networks. Here we propose node2vec, an algorithmic framework for learning continuous feature representations for nodes in networks. In node2vec, we learn a mapping of nodes to a low-dimensional space of features that maximizes the likelihood of preserving network neighborhoods of nodes. We define a flexible notion of a node's network neighborhood and design a biased random walk procedure, which efficiently explores diverse neighborhoods. Our algorithm generalizes prior work which is based on rigid notions of network neighborhoods, and we argue that the added flexibility in exploring neighborhoods is the key to learning richer representations. We demonstrate the efficacy of node2vec over existing state-of-the-art techniques on multi-label classification and link prediction in several real-world networks from diverse domains. Taken together, our work represents a new way for efficiently learning state-of-the-art task-independent representations in complex networks.",,,,United States of America,,,,,,2024-09-05 14:08,,,,,,Academia,,,,,Open source,"https://github.com/aditya-grover/node2vec

MIT license",Academia,,,,,,,,,
Wide & Deep,Recommendation,Recommender system,Google,"HT Cheng, L Koc, J Harmsen, T Shaked",2016-06-24,Wide & Deep Learning for Recommender Systems,https://arxiv.org/abs/1606.07792,3378.0,Highly cited,,,,,,,,,,,,,,Unknown,"Generalized linear models with nonlinear feature transformations are widely used for large-scale regression and classification problems with sparse inputs. Memorization of feature interactions through a wide set of cross-product feature transformations are effective and interpretable, while generalization requires more feature engineering effort. With less feature engineering, deep neural networks can generalize better to unseen feature combinations through low-dimensional dense embeddings learned for the sparse features. However, deep neural networks with embeddings can over-generalize and recommend less relevant items when the user-item interactions are sparse and high-rank. In this paper, we present Wide & Deep learning---jointly trained wide linear models and deep neural networks---to combine the benefits of memorization and generalization for recommender systems. We productionized and evaluated the system on Google Play, a commercial mobile app store with over one billion active users and over one million apps. Online experiment results show that Wide & Deep significantly increased app acquisitions compared with wide-only and deep-only models. We have also open-sourced our implementation in TensorFlow.",,,,United States of America,,,,,,2024-11-01 10:03,,,,,,Industry,,,,,,,Industry,,,,,,,,,
R-FCN,Vision,Object detection,"Tsinghua University,Microsoft Research","Jifeng Dai, Y. Li, Kaiming He, and Jian Sun",2016-06-21,R-fcn: Object detection via region-based fully convolutional networks.,https://arxiv.org/abs/1605.06409,5411.0,Highly cited,,,,7.1935776e+17,"1,464  images in 2012 VOC (https://paperswithcode.com/dataset/pascal-voc)/
9,963 images in 2007 VOC (https://www.tensorflow.org/datasets/catalog/voc)
83K training images in MS COCO  (https://paperswithcode.com/dataset/coco)

They used a Nvidia K40 GPU and report training time/image in seconds (table 3)

Assumed a 0.33 util rate

Section 4.2 (MS COCO):
""Next we evaluate on the MS COCO dataset [ 13 ] that has 80 object categories. Our experiments involve the 80k train set, 40k val set, and 20k test-dev set. We set the learning rate as 0.001 for 90k iterations and 0.0001 for next 30k iterations, with an effective mini-batch size of 8.""
0.45s K40 training time per image (table 3)
K40 has 5046000000000 FLOP/s

Total examples: 120000*8=960000 (12 epochs)
5046000000000 FLOP/s * 960000 images * 0.45 s/image * 0.33 utilization = 719357760000000000 FLOP","PASCAL VOC 2007,PASCAL VOC 2012,COCO",,94427.0,,,,,,Confident,,,,,"China,United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational",,,,,,2025-06-18 03:44,,,,,,"Academia,Industry",,,,,,,"Academia,Industry",,,FP32,,Hardware,,,,18
DMN,Language,"Question answering,Text classification,Language modeling",Salesforce,"Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer, James Bradbury, Ishaan Gulrajani, Victor Zhong, Romain Paulus, Richard Socher",2016-06-20,Ask Me Anything: Dynamic Memory Networks for Natural Language Processing,https://arxiv.org/abs/1506.07285,1187.0,Highly cited,,,,,,,,,,,,,,Unknown,"Most tasks in natural language processing can be cast into question answering (QA) problems over language input. We introduce the dynamic memory network (DMN), a neural network architecture which processes input sequences and questions, forms episodic memories, and generates relevant answers. Questions trigger an iterative attention process which allows the model to condition its attention on the inputs and the result of previous iterations. These results are then reasoned over in a hierarchical recurrent sequence model to generate answers. The DMN can be trained end-to-end and obtains state-of-the-art results on several types of tasks and datasets: question answering (Facebook's bAbI dataset), text classification for sentiment analysis (Stanford Sentiment Treebank) and sequence modeling for part-of-speech tagging (WSJ-PTB). The training for these different tasks relies exclusively on trained word vector representations and input-question-answer triplets.",,,,United States of America,,,,,,2024-09-05 14:08,,,,,,Industry,,,,,,,Industry,,,,,,,,,
PixelCNN,Vision,Image generation,Google DeepMind,"Aaron van den Oord, Nal Kalchbrenner, Oriol Vinyals, Lasse Espeholt, Alex Graves, Koray Kavukcuoglu",2016-06-16,"Conditional Image Generation with PixelCNN Decoders
",https://arxiv.org/abs/1606.05328,3079.0,"Highly cited,SOTA improvement",Best performance on NLL test.,,,,,ImageNet,,,,60.0,We were able to achieve similar performance to the PixelRNN (Row LSTM [30]) in less than half the training time (60 hours using 32 GPUs).,,,Unknown,"This work explores conditional image generation with a new image density model based on the PixelCNN architecture. The model can be conditioned on any vector, including descriptive labels or tags, or latent embeddings created by other networks. When conditioned on class labels from the ImageNet database, the model is able to generate diverse, realistic scenes representing distinct animals, objects, landscapes and structures. When conditioned on an embedding produced by a convolutional network given a single image of an unseen face, it generates a variety of new portraits of the same person with different facial expressions, poses and lighting conditions. We also show that conditional PixelCNN can serve as a powerful decoder in an image autoencoder. Additionally, the gated convolutional layers in the proposed model improve the log-likelihood of PixelCNN to match the state-ofthe-art performance of PixelRNN on ImageNet, with greatly reduced computational cost.",,,,"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France",,,,32.0,,2025-05-01 10:42,,,,,,Industry,,,,,,,Industry,,,,,,,,,
Part-of-sentence tagging model,Language,Part-of-speech tagging,Carnegie Mellon University (CMU),"Xuehe Ma, Eduard Hovy",2016-05-29,End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF,https://arxiv.org/abs/1603.01354,3193.0,Highly cited,,,Architecture described in Table 1,1.454112e+17,"12 hours of training for POS tagging
GeForce GTX TITAN X GPU
0.33 utilization rate
","WSJ,Penn TreeBank (PTB)",WSJ subset of Penn Treebank,912344.0,Table 2,12.0,"""the model training requires about 12 hours for POS tagging and 8
hours for NER""",NVIDIA GeForce GTX TITAN X,,Confident,"State-of-the-art sequence labeling systems traditionally require large amounts of task-specific knowledge in the form of handcrafted features and data pre-processing. In this paper, we introduce a novel neutral network architecture that benefits from both word- and character-level representations automatically, by using combination of bidirectional LSTM, CNN and CRF. Our system is truly end-to-end, requiring no feature engineering or data preprocessing, thus making it applicable to a wide range of sequence labeling tasks. We evaluate our system on two data sets or two sequence labeling tasks â€” Penn Treebank WSJ corpus for part-of-speech (POS) tagging and CoNLL 2003 corpus for named entity recognition (NER). We obtain state-of-the-art performance on both datasets â€” 97.55% accuracy for POS tagging and 91.21% F1 for NER.
",50.0,,,United States of America,,,,1.0,,2025-05-28 16:18,,,,,,Academia,,,,12.0,,,Academia,,,FP32,289.523491253324,Hardware,,,,
Named Entity Recognition model,Language,"Named entity recognition,Language modeling",Carnegie Mellon University (CMU),"Xuezhe Ma, Eduard Hovy",2016-05-29,Layer Normalization,https://arxiv.org/abs/1603.01354,3100.0,Highly cited,,,Architecture in Table 1,9.69408e+16,"8 hours of training for NER
GeForce GTX TITAN X GPU
0.33 utilization rate
",CoNLL2003,Table 2,204567.0,Table 2. 204567 tokens,8.0,"""the model training requires about 12 hours for POS tagging and 8
hours for NER""",NVIDIA GeForce GTX TITAN X,,Confident,"State-of-the-art sequence labeling systems traditionally require large amounts of task-specific knowledge in the form of handcrafted features and data pre-processing. In this paper, we introduce a novel neutral network architecture that benefits from both word- and character-level representations automatically, by using combination of bidirectional LSTM, CNN and CRF. Our system is truly end-to-end, requiring no feature engineering or data preprocessing, thus making it applicable to a wide range of sequence labeling tasks. We evaluate our system on two data sets or two sequence labeling tasks â€” Penn Treebank WSJ corpus for part-of-speech (POS) tagging and CoNLL 2003 corpus for named entity recognition (NER). We obtain state-of-the-art performance on both datasets â€” 97.55% accuracy for POS tagging and 91.21% F1 for NER.
",50.0,,,United States of America,,,,1.0,,2025-05-28 16:18,,,,,,Academia,,,,8.0,,,Academia,,,FP32,289.523491253324,Hardware,,,,
Symmetric Residual Encoder-Decoder Net,"Vision,Image generation",Image super-resolution,"Nanjing University,University of Adelaide","Xiao-Jiao Mao, Chunhua Shen, Yu-Bin Yang",2016-03-30,Image Restoration Using Very Deep Convolutional Encoder-Decoder Networks with Symmetric Skip Connections,https://arxiv.org/abs/1603.09056v2,1184.0,Highly cited,,,,,,,,,,,,,,Unknown,,,,,"China,Australia",,,,,,2025-05-28 16:18,,,,,,"Academia,Academia",,,,,,,"Academia,Academia",,,FP32,,,,,,
Binarized Neural Network (MNIST),Vision,Image classification,"Technion - Israel Institute of Technology,Columbia University,University of Montreal / UniversitÃ© de MontrÃ©al","Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-Yaniv, Yoshua Bengio",2016-03-17,Binarized Neural Networks: Training Neural Networks with Weights and Activations Constrained to +1 or âˆ’1,https://arxiv.org/abs/1602.02830,3299.0,Highly cited,,37000000.0,"Parameter count is not explicitly stated, but they give details:

""The MLP we train on MNIST consists of 3 hidden layers of 4096 binary units (see Section 1) and a L2-SVM output layer""

Approximately 37m, based on 784 pixels * 4096 + 2 * 4096^2",,,MNIST,,60000.0,"60k training images, 10k test in MNIST",,,,,Speculative,"We introduce a method to train Binarized Neural Networks (BNNs) - neural networks with binary weights and activations at run-time. At training-time the binary weights and activations are used for computing the parameters gradients. During the forward pass, BNNs drastically reduce memory size and accesses, and replace most arithmetic operations with bit-wise operations, which is expected to substantially improve power-efficiency. To validate the effectiveness of BNNs we conduct two sets of experiments on the Torch7 and Theano frameworks. On both, BNNs achieved nearly state-of-the-art results over the MNIST, CIFAR-10 and SVHN datasets. Last but not least, we wrote a binary matrix multiplication GPU kernel with which it is possible to run our MNIST BNN 7 times faster than with an unoptimized GPU kernel, without suffering any loss in classification accuracy. The code for training and running our BNNs is available on-line.",1000.0,,,"Israel,United States of America,Canada",,,,,,2024-09-05 14:08,,,,,,"Academia,Academia,Academia",,,,,,,"Academia,Academia,Academia",,,,,,,,,
Order-Embeddings of Images and Language,Vision,Image captioning,University of Toronto,"Ivan Vendrov, Ryan Kiros, Sanja Fidler, Raquel Urtasun",2016-03-01,Order-Embeddings of Images and Language,https://arxiv.org/abs/1511.06361,,,,,possibly can be calculated based on architecture description,,,COCO,,113000.0,"We use the data splits of Karpathy & Li (2015) for training
(113,287 images), validation (5000 images), and test (5000 images).",,,,,Confident,"Hypernymy, textual entailment, and image captioning can be seen as special cases of a single visual-semantic hierarchy over words, sentences, and images. In this paper we advocate for explicitly modeling the partial order structure of this hierarchy. Towards this goal, we introduce a general method for learning ordered representations, and show how it can be applied to a variety of tasks involving images and language. We show that the resulting representations improve performance over current approaches for hypernym prediction and image-caption retrieval.",,,Open weights (unrestricted),Canada,,,,,,2025-04-10 04:33,,,,,,Academia,,,,,Open source,"https://github.com/ivendrov/order-embedding
Apache License 2.0",Academia,,,,,,,,,
BIG LSTM+CNN INPUTS ,Language,Language modeling,Google Brain,"Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, Yonghui Wu",2016-02-11,Exploring the Limits of Language Modeling,https://arxiv.org/abs/1602.02410,,,,1040000000.0,1.04B (Table 1),1.0220883e+20,"assuming (!) 10 days similarly to another model in the paper
240*3600*5046000000000*32*0.3 = 4.1853542e+19

assuming (!) 50 epochs similarly to another model in the paper
6*1.04*10^9*0.8*10^9*50 = 2.496e+20

sqrt(4.1853542e+19*2.496e+20) = 1.0220883e+20",One Billion Word benchmark,,800000000.0,"""The experiments are performed on the 1B Word Benchmark data set introduced by (Chelba et al., 2013), which is
a publicly available benchmark for measuring progress of
statistical language modeling. The data set contains about
0.8B words with a vocabulary of 793471 words, including
sentence boundary markers.""",,"*I assumed Tesla K40s though they stated it was just one of Tesla K40

""The best model needs about 5 days to get to 35 perplexity and 10 days to 32.5.""

10*24 = 240

This model is not the biggest but has the best performance, I assume it is *likely* to have comparativeley similar training time",NVIDIA Tesla K40s,,Likely,"In this work we explore recent advances in Recurrent Neural Networks for large scale Language Modeling, a task central to language understanding. We extend current models to deal with two key challenges present in this task: corpora and vocabulary sizes, and complex, long term structure of language. We perform an exhaustive study on techniques such as character Convolutional Neural Networks or Long-Short Term Memory, on the One Billion Word Benchmark. Our best single model significantly improves state-of-the-art perplexity from 51.3 down to 30.0 (whilst reducing the number of parameters by a factor of 20), while an ensemble of models sets a new record by improving perplexity from 41.0 down to 23.7. We also release these models for the NLP and ML community to study and improve upon.",,,,United States of America,,,,32.0,,2025-05-02 10:15,,,,,,Industry,,,,240.0,,,Industry,,,,16564.40221110092,"Hardware,Operation counting",,,,
10 LSTMS + KN-5 (OPTIMAL WEIGHTS),Language,Language modeling,Google Brain,"Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, Yonghui Wu",2016-02-11,Exploring the Limits of Language Modeling,https://arxiv.org/abs/1602.02410,,,,1040000000.0,1.04B (Table 1),8.7892439e+19,504*3600*5046000000000*32*0.3 =8.7892439e+19 ,One Billion Word benchmark,,800000000.0,"""The experiments are performed on the 1B Word Benchmark data set introduced by (Chelba et al., 2013), which is
a publicly available benchmark for measuring progress of
statistical language modeling. The data set contains about
0.8B words with a vocabulary of 793471 words, including
sentence boundary markers.""",,"*I assumed Tesla K40s though they stated it was just one of Tesla K40

""The best results were achieved after 3 weeks of training.""

21*24 = 504
",NVIDIA Tesla K40s,,Likely,"In this work we explore recent advances in Recurrent Neural Networks for large scale Language Modeling, a task central to language understanding. We extend current models to deal with two key challenges present in this task: corpora and vocabulary sizes, and complex, long term structure of language. We perform an exhaustive study on techniques such as character Convolutional Neural Networks or Long-Short Term Memory, on the One Billion Word Benchmark. Our best single model significantly improves state-of-the-art perplexity from 51.3 down to 30.0 (whilst reducing the number of parameters by a factor of 20), while an ensemble of models sets a new record by improving perplexity from 41.0 down to 23.7. We also release these models for the NLP and ML community to study and improve upon.",,,,United States of America,,,,32.0,,2025-05-02 10:15,,,,,,Industry,,,,504.0,,,Industry,,,,16564.40221110092,Hardware,,,,
A3C FF hs,Games,Atari,"Google,University of Montreal / UniversitÃ© de MontrÃ©al","V Mnih, AP Badia, M Mirza, A Graves",2016-02-04,Asynchronous Methods for Deep Reinforcement Learning,http://arxiv.org/abs/1602.01783v2,8277.0,"Highly cited,SOTA improvement",,,,,,,,,,,,,,Unknown,,,,,"United States of America,Canada",,,,,,2025-05-28 16:18,,,,,,"Industry,Academia",,,,,,,"Industry,Academia",,,FP32,,,,,,
Convolutional Pose Machines,Vision,Pose estimation,Carnegie Mellon University (CMU),"Shih-En Wei, Varun Ramakrishna, Takeo Kanade, Yaser Sheikh",2016-01-30,Convolutional Pose Machines,https://arxiv.org/abs/1602.00134,2658.0,Highly cited,,,,,,,,,,,,,,Unknown,,,,,United States of America,,,,,,2025-05-28 16:18,,,,,,Academia,,,,,,,Academia,,,FP32,,,,,,
AlphaGo Lee,Games,Go,DeepMind,"David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, Demis Hassabis",2016-01-27,Mastering the game of Go with deep neural networks and tree search,https://www.nature.com/articles/nature16961,16057.0,Highly cited,,,,1.9e+21,"This number is pretty uncertain. I expect it to be right to around a factor of 3, at least compared to AlphaGo Fan.

The architecture used was pretty much the same as AlphaGo Fan, but it was ""trained for longer"" and had around 5.33x the number of convolutional layers of AlphaGo Fan (256/48 = 5.33). 

The convolutional layers are the major contributor to the training compute, so I somewhat arbitrarily just multiply the compute for AlphaGo Fan by 5. Thus 3.8e20 * 5 = 1.9e21

Otherwise there has been little said about this model specifically - I've mainly relied on the source for AlphaGo Zero and AlphaGo Fan, linked below

AlphaGo Fan: https://www.nature.com/articles/nature16961

AlphaGo Zero: https://www.nature.com/articles/nature24270.epdf?author_access_token=VJXbVjaSHxFoctQQ4p2k4tRgN0jAjWel9jnR3ZoTv0PVW4gB86EEpGqTRDtpIz-2rmo8-KG06gqVobU5NSCFeHILHcVFUeMsbvwS-lxjqQGg98faovwjxeTUgZAUMnRQ",,,29400000.0,"We trained the policy network pÏƒ to classify positions according to expert moves played in the KGS data set. This data set contains 29.4 million positions from 160,000 games played by KGS 6 to 9 dan human players; 35.4% of the games are handicap games.",696.0,"Training times are given for several components:
- Policy network classifier: 3 weeks
- Policy network RL: 1 day
- Value network regression: 1 week
- Rollout policy: ""Similar to the policy network, the weights Ï€ of the rollout policy are trained from 8 million positions from human games on the Tygem server to maximize log likelihood by stochastic gradient descent. Rollouts execute at approximately 1,000 simulations per second per CPU thread on an empty board."" could suggest (8M sims / 1000 sims/sec) / 3600 sec/hr = 2.2 hours, if each position corresponds to only one simulation (unclear)

Total: 29 days or 696 hours",,,Speculative,"The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses â€˜value networksâ€™ to evaluate board positions and â€˜policy networksâ€™ to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.",,,Unreleased,United Kingdom of Great Britain and Northern Ireland,,,,,,2025-02-19 14:06,,,,,,Industry,,,,,Unreleased,,Industry,,,,,Comparison with other models,,,,1
"Variational (untied weights, MC) LSTM (Large)",Language,Language modeling,University of Cambridge,"Yarin Gal, Zoubin Ghahramani",2015-12-16,A Theoretically Grounded Application of Dropout in Recurrent Neural Networks,https://arxiv.org/abs/1512.05287,1838.0,"Highly cited,SOTA improvement","""The new approach outperforms existing techniques, and to the best of our knowledge improves on the single model state-of-the-art in language modelling with the Penn Treebank (73.4 test perplexity)""",66000000.0,66M according to https://arxiv.org/pdf/1611.01462,5886144000000000.0,6 FLOP / parameter / token * 66000000 parameters * 929000 tokens * 16 epochs = 5.886144e+15 FLOP,Penn TreeBank (PTB),,929000.0,"""We had to use early stopping for the large model with [20]â€™s variant as the model starts overfitting after 16 epochs.""",,,,,Confident,"Recurrent neural networks (RNNs) stand at the forefront of many recent developments in deep learning. Yet a major difficulty with these models is their tendency to overfit, with dropout shown to fail when applied to recurrent layers. Recent results at the intersection of Bayesian modelling and deep learning offer a Bayesian interpretation of common deep learning techniques such as dropout. This grounding of dropout in approximate Bayesian inference suggests an extension of the theoretical results, offering insights into the use of dropout with RNN models. We apply this new variational inference based dropout technique in LSTM and GRU models, assessing it on language modelling and sentiment analysis tasks. The new approach outperforms existing techniques, and to the best of our knowledge improves on the single model state-of-the-art in language modelling with the Penn Treebank (73.4 test perplexity). This extends our arsenal of variational tools in deep learning.",16.0,"""Variational (untied weights, MC) LSTM (Large)""",Unreleased,United Kingdom of Great Britain and Northern Ireland,,,,,,2025-05-28 16:18,,,,,,Academia,,,,,Unreleased,,Academia,,,FP32,,Operation counting,,,,
Advantage Learning,Games,Atari,Google DeepMind,"MG Bellemare, G Ostrovski, A Guez",2015-12-15,Increasing the Action Gap: New Operators for Reinforcement Learning,http://arxiv.org/abs/1512.04860v1,151.0,SOTA improvement,,,,,,,,,,,,,,Unknown,"This paper introduces new optimality-preserving operators on Q-functions. We first describe an operator for tabular representations, the consistent Bellman operator, which incorporates a notion of local policy consistency. We show that this local consistency leads to an increase in the action gap at each state; increasing this gap, we argue, mitigates the undesirable effects of approximation and estimation errors on the induced greedy policies. This operator can also be applied to discretized continuous space and time problems, and we provide empirical results evidencing superior performance in this context. Extending the idea of a locally consistent operator, we then derive sufficient conditions for an operator to preserve optimality, leading to a family of operators which includes our consistent Bellman operator. As corollaries we provide a proof of optimality for Baird's advantage learning algorithm and derive other gap-increasing operators with interesting properties. We conclude with an empirical study on 60 Atari 2600 games illustrating the strong potential of these new operators.",,,,"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France",,,,,,2024-11-01 10:02,,,,,,Industry,,,,,,,Industry,,,,,,,,,
BPL,Image generation,Image generation,"University of Toronto,New York University (NYU),Massachusetts Institute of Technology (MIT)","BM Lake, R Salakhutdinov, JB Tenenbaum",2015-12-11,Human-level concept learning through probabilistic program induction,https://science.sciencemag.org/content/350/6266/1332/,2885.0,Highly cited,,,,,,,,,,,,,,Unknown,,,,,"Canada,United States of America,United States of America",,,,,,2024-11-01 10:02,,,,,,"Academia,Academia,Academia",,,,,,,"Academia,Academia,Academia",,,,,,,,,
ResNet-152 (ImageNet),Vision,Image classification,Microsoft,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun",2015-12-10,Deep Residual Learning for Image Recognition,https://arxiv.org/abs/1512.03385,175697.0,Highly cited,,60200000.0,Taken from https://arxiv.org/abs/1605.07146,1.041408e+19,"11.3 *10^9 mult-adds per forward pass (Table 1)
2 FLOPS/ mult-add
3 for forward & backward pass
1.2 * 10^6 examples in dataset
128 epochs

-> 1.041408 Ã— 10^19 FLOP

Authors of ""AI and Memory Wall"" (https://github.com/amirgholami/ai_and_memory_wall) estimated model's training compute as 11,000 PFLOP = 1.1*10^19 FLOP",ILSVRC 2012 subset of ImageNet,"They won ILSVRC 2015, but actually the classification dataset is the same as 2012",1280000.0,"""We evaluate our method on the ImageNet 2012 classification dataset [36] that consists of 1000 classes. The models are trained on the 1.28 million training images""",,,,,Confident,"Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.",120.0,,,"United States of America,Multinational,India,Belgium",,,,,,2025-05-28 16:19,,,,256.0,,Industry,,,,,,,Industry,,,FP32,,"Operation counting,Third-party estimation",,,,9
DeepSpeech2 (English),Speech,Speech recognition,Baidu Research - Silicon Valley AI Lab,"Dario Amodei, Rishita Anubhai, Eric Battenberg, Carl Case, Jared Casper, Bryan Catanzaro, Jingdong Chen, Mike Chrzanowski, Adam Coates, Greg Diamos, Erich Elsen, Jesse Engel, Linxi Fan, Christopher Fougner, Tony Han, Awni Hannun, Billy Jun, Patrick LeGresley, Libby Lin, Sharan Narang, Andrew Ng, Sherjil Ozair, Ryan Prenger, Jonathan Raiman, Sanjeev Satheesh, David Seetapun, Shubho Sengupta, Yi Wang, Zhiqian Wang, Chong Wang, Bo Xiao, Dani Yogatama, Jun Zhan, Zhenyao Zhu",2015-12-08,Deep Speech 2: End-to-End Speech Recognition in English and Mandarin,https://arxiv.org/abs/1512.02595,2853.0,Highly cited,,38000000.0,All networks have 38 million parameters.,2.6e+19,"1 timestep = (1280 hidden units)^2 * (7 RNN layers * 4 matrices for bidirectional + 2 DNN layers) * (2 for doubling parameters from 36M to 72M) = 98 MFLOP
20 epochs * 12,000 hours * 3600 seconds/hour * 50 samples/sec * 98 MFLOP * 3 add-multiply * 2 backprop 
= 26,000 PF = 0.30 pfs-days

See also AI and Compute by Dario Amodei and OpenAI https://openai.com/research/ai-and-compute",,,163339200.0,"""Our English speech system is trained on 11,940 hours of speech, while the Mandarin system is trained on 9,400 hours.""

11,940 * 13,680 = 163339200",120.0,"""5 days"" from AI and Compute https://openai.com/index/ai-and-compute/",NVIDIA GeForce GTX TITAN X,,Confident,"We show that an end-to-end deep learning approach can be used to recognize either English or Mandarin Chinese speech--two vastly different languages. Because it replaces entire pipelines of hand-engineered components with neural networks, end-to-end learning allows us to handle a diverse variety of speech including noisy environments, accents and different languages. Key to our approach is our application of HPC techniques, resulting in a 7x speedup over our previous system. Because of this efficiency, experiments that previously took weeks now run in days. This enables us to iterate more quickly to identify superior architectures and algorithms. As a result, in several cases, our system is competitive with the transcription of human workers when benchmarked on standard datasets. Finally, using a technique called Batch Dispatch with GPUs in the data center, we show that our system can be inexpensively deployed in an online setting, delivering low latency when serving users at scale.",,,,United States of America,,,,16.0,0.4484,2025-05-28 16:19,,,,,,Industry,,,,1920.0,,,Industry,$206.31,"""Overall the system sustains approximately 50 teraFLOP/second when
training on 16 GPUs. This amounts to 3 teraFLOP/second per GPU which is about 50% of peak theoretical performance""
3 TFLOP / 6.69 TFLOP = 0.4484",FP32,8463.467702301677,"Operation counting,Third-party estimation",,,,5
SSD,Vision,Object detection,,"Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, Alexander C. Berg",2015-12-08,SSD: Single Shot MultiBox Detector,https://arxiv.org/pdf/1512.02325,39468.0,Highly cited,Also listed in Denis Panjuta's List of 100+ AI Algorithms,,Can be calculated from the VGG-16 paper and Figure 2,,,COCO,,115000.0,"Multiple datasets were used (PASCAL VOC, ILSVRC, COCO) but afaict COCO is the largest. Per this paper https://arxiv.org/abs/1703.06870v1, final paragraph before section 4.1, ""As in previous work [3, 21], we train using the union of 80k train images and a 35k subset of val images (trainval35k), and report ablations on the remaining 5k subset of val images (minival)"". So trainval35k is 80k + 35k = 115k",,"Note on training hardware below: unclear if the Titan X was for testing or training. ""We measure the speed with batch size 8 using Titan X and cuDNN v4 with Intel Xeon E5-2667v3@3.20GHz."" I don't know if it's common to use the same hardware for both testing and training.

",NVIDIA GeForce GTX TITAN X,Supervised,Confident,"We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. Our SSD model is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stage and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, MS COCO, and ILSVRC datasets confirm that SSD has comparable accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. Compared to other single stage methods, SSD has much better accuracy, even with a smaller input image size. For 300Ã—300 input, SSD achieves 72.1% mAP on VOC2007 test at 58 FPS on a Nvidia Titan X and for 500Ã—500 input, SSD achieves 75.1% mAP, outperforming a comparable state of the art Faster R-CNN model. Code is available at this https URL",,,Open weights (unrestricted),,VGG16,,,,,2025-05-28 16:19,,,,,,,,,,,,,,,,FP32,,,,,,
Inception v3,Vision,Image classification,"Google,University College London (UCL)","Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, Zbigniew Wojna",2015-12-02,Rethinking the inception architecture for computer vision.,https://arxiv.org/abs/1512.00567,25401.0,Highly cited,,23626728.0,Table 3 from Xception paper,1e+20,"Authors of ""AI and Memory Wall"" (https://github.com/amirgholami/ai_and_memory_wall) estimated model's training compute as 100,000 PFLOP = 1*10^20 FLOP",ILSVRC 2012 subset of ImageNet,,1200000.0,"The full dataset is a lot larger and has far more categories. When people say ""ImageNet"" they're usually referring to the subset of the full dataset with 1000 categories and 1.2million images, found here: https://image-net.org/challenges/LSVRC/2012/",,,,,Likely,"Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2% top-1 and 5.6% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5% top-5 error on the validation set (3.6% error on the test set) and 17.3% top-1 error on the validation set.",,,,"United States of America,United Kingdom of Great Britain and Northern Ireland",,,,,,2025-05-08 17:26,,,,,,"Industry,Academia",,,,,,,"Industry,Academia",,,,,Third-party estimation,,,,3
Netflix Recommender System,Recommendation,Recommender system,Netflix,"CA Gomez-Uribe, N Hunt",2015-12-01,"The Netflix Recommender System: Algorithms, Business Value, and Innovation",https://dl.acm.org/doi/pdf/10.1145/2843948,1092.0,Highly cited,,,,,,,,,,,,,,Unknown,"This article discusses the various algorithms that make up the Netflix recommender system, and describes its business purpose. We also describe the role of search and related algorithms, which for us turns into a
recommendations problem as well. We explain the motivations behind and review the approach that we use to improve the recommendation algorithms, combining A/B testing focused on improving member retention and medium term engagement, as well as offline experimentation using historical member engagement data. We discuss some of the issues in designing and interpreting A/B tests. Finally, we describe some current areas of focused innovation, which include making our recommender system global and language aware.",,,,United States of America,,,,,,2024-09-05 14:08,,,,,,Industry,,,,,,,Industry,,,,,,,,,
Multi-scale Dilated CNN,Vision,Image segmentation,"Princeton University,Intel Labs","Fisher Yu, Vladlen Koltun",2015-11-23,Multi-Scale Context Aggregation by Dilated Convolutions,https://arxiv.org/abs/1511.07122,7976.0,Highly cited,,,,,,,,,,,,,,Unknown,,,,,"United States of America,Multinational,United States of America",,,,,,2025-05-28 16:19,,,,,,"Academia,Industry",,,,,,,"Academia,Industry",,,FP32,,,,,,
Highway Network,Vision,Image classification,"IDSIA,SUPSI","Rupesh Kumar Srivastava, Klaus Greff, JÃ¼rgen Schmidhuber",2015-11-23,Training Very Deep Networks,https://arxiv.org/abs/1507.06228,,,,2300000.0,2.3M from Table 2.,,,"MNIST,CIFAR-100",,,,,,,,Confident,"Theoretical and empirical evidence indicates that the depth of neural networks is crucial for their success. However, training becomes more difficult as depth increases, and training of very deep networks remains an open problem. Here we introduce a new architecture designed to overcome this. Our so-called highway networks allow unimpeded information flow across many layers on information highways. They are inspired by Long Short-Term Memory recurrent networks and use adaptive gating units to regulate the information flow. Even with hundreds of layers, highway networks can be trained directly through simple gradient descent. This enables the study of extremely deep and efficient architectures.",,,,"Switzerland,Switzerland",,,,,,2024-09-05 14:08,,,,,,"Academia,Academia",,,,,,,"Academia,Academia",,,,,,,,,
The Attentive Reader,Language,Question answering,Google DeepMind,"Karl Moritz Hermann, TomÃ¡Å¡ KoÄiskÃ½, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, Phil Blunsom",2015-11-19,Teaching Machines to Read and Comprehend,https://arxiv.org/abs/1506.03340,,,,,possibly can be calculated based on architecture description,,,Question Answering Corpus (CNN + Daily Mail),,1041920000.0,"Table 1 shows statistics of documents and queries used to construct examples. Training examples are document-query-answer pairs
Estimating query-answer pairs at ~30 tokens
Total tokens:
CNN: (762+30)*380000=300960000
Daily Mail: (812+30)*880000=740960000
Total: 1041920000",,,,,Confident,"Teaching machines to read natural language documents remains an elusive challenge. Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation. In this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. This allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure.",,,Open weights (unrestricted),"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France",,,,,,2025-04-10 04:51,,,,,,Industry,,,,,Open source,"https://github.com/cooijmanstim/Attentive_reader/tree/bn

BSD 3-Clause ""New"" or ""Revised"" License",Industry,,,,,,,,,
AlphaGo Fan,Games,Go,DeepMind,"David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, Demis Hassabis",2015-10-01,Mastering the game of Go with deep neural networks and tree search,https://www.nature.com/articles/nature16961,16057.0,"Highly cited,SOTA improvement",,8209984.0,"The input to the policy network is a 19â€‰Ã—â€‰19â€‰Ã—â€‰48 image stack consisting of 48 feature planes. The first hidden layer zero pads the input into a 23â€‰Ã—â€‰23 image, then convolves k filters of kernel size 5â€‰Ã—â€‰5 with stride 1 with the input image and applies a rectifier nonlinearity. Each of the subsequent hidden layers 2 to 12 zero pads the respective previous hidden layer into a 21â€‰Ã—â€‰21 image, then convolves k filters of kernel size 3â€‰Ã—â€‰3 with stride 1, again followed by a rectifier nonlinearity. The final layer convolves 1 filter of kernel size 1â€‰Ã—â€‰1 with stride 1, with a different bias for each position, and applies a softmax function. The match version of AlphaGo used kâ€‰=â€‰192 filters; Fig. 2b and Extended Data Table 3 additionally show the results of training with kâ€‰=â€‰128, 256 and 384 filters.

The input to the value network is also a 19â€‰Ã—â€‰19â€‰Ã—â€‰48 image stack, with an additional binary feature plane describing the current colour to play. Hidden layers 2 to 11 are identical to the policy network, hidden layer 12 is an additional convolution layer, hidden layer 13 convolves 1 filter of kernel size 1â€‰Ã—â€‰1 with stride 1, and hidden layer 14 is a fully connected linear layer with 256 rectifier units. The output layer is a fully connected linear layer with a single tanh unit.",3.8000000000000007e+20,"Assume 0.3 utilisation rate, 1e13 GPU FLOP/s [single precision]. Trained in three stages using 50 GPUs over 3 weeks + 1 day + 1 week

Training compute = (50 GPUs)(29 days)(86400s/day)(0.3 utilisation rate)(1e13 FLOP/s) = 3.8e20 FLOPs
",,,,Supervised learning + self-play,,,,,Likely,"A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGoâ€™s own move selections and also the winner of AlphaGoâ€™s games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100â€“0 against the previously published, champion-defeating AlphaGo.",,,Unreleased,United Kingdom of Great Britain and Northern Ireland,,,,,,2025-06-18 03:28,,,,,,Industry,,,,,Unreleased,,Industry,,,,,Hardware,,,,
Deep Deterministic Policy Gradients,Robotics,"Robotic manipulation,Self-driving car",Google DeepMind,"TP Lillicrap, JJ Hunt, A Pritzel, N Heess, T Erez",2015-09-09,Continuous control with deep reinforcement learning,https://arxiv.org/abs/1509.02971,12124.0,Highly cited,,,,,,,,,,,,,,Unknown,"We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.",,,,"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France",,,,,,2024-11-01 10:02,,,,,,Industry,,,,,,,Industry,,,,,,,,,
LSTM-Char-Large,Language,Language modeling,"Harvard University,New York University (NYU)","Yoon Kim, Yacine Jernite, David Sontag, Alexander M. Rush",2015-08-26,Character-Aware Neural Language Models,https://arxiv.org/abs/1508.06615,2033.0,Highly cited,,19000000.0,19M (Table 3),2650000000000000.0,6 FLOP / parameter / token * 19000000 parameters * 929000 tokens * 25 epochs = 2.64765e+15 FLOP,Penn TreeBank (PTB),,929000.0,"25 epochs based on code (https://github.com/yoonkim/lstm-char-cnn/blob/master/main.lua) and ""We train for 25 epochs on non-Arabic and 30 epochs on Arabic data (which was sufficient for convergence)""",,,,,Confident,"We describe a simple neural language model that relies only on character-level inputs. Predictions are still made at the word-level. Our model employs a convolutional neural network (CNN) and a highway network over characters, whose output is given to a long short-term memory (LSTM) recurrent neural network language model (RNN-LM). On the English Penn Treebank the model is on par with the existing state-of-the-art despite having 60% fewer parameters. On languages with rich morphology (Arabic, Czech, French, German, Spanish, Russian), the model outperforms word-level/morpheme-level LSTM baselines, again with fewer parameters. The results suggest that on many languages, character inputs are sufficient for language modeling. Analysis of word representations obtained from the character composition part of the model reveals that the model is able to encode, from characters only, both semantic and orthographic information.",25.0,LSTM-Char-Large,Unreleased,"United States of America,United States of America",,,,,,2025-04-18 11:59,,,,,,"Academia,Academia",,,,,Open source,"code, MIT license: https://github.com/yoonkim/lstm-char-cnn ","Academia,Academia",,,,,Operation counting,,,,
"Listen, Attend and Spell",Speech,Speech recognition,"Google,Carnegie Mellon University (CMU)","William Chan, Navdeep Jaitly, Quoc Le, Oriol Vinyals",2015-08-20,"Listen, attend and spell: A neural network for large vocabulary conversational speech recognition",https://ieeexplore.ieee.org/document/7472621,2175.0,Highly cited,,,,,,,,,,,,,,Unknown,"We present Listen, Attend and Spell (LAS), a neural speech recognizer that transcribes speech utterances directly to characters without pronunciation models, HMMs or other components of traditional speech recognizers. In LAS, the neural network architecture subsumes the acoustic, pronunciation and language models making it not only an end-to-end trained system but an end-to-end model. In contrast to DNN-HMM, CTC and most other models, LAS makes no independence assumptions about the probability distribution of the output character sequences given the acoustic sequence. Our system has two components: a listener and a speller. The listener is a pyramidal recurrent network encoder that accepts filter bank spectra as inputs. The speller is an attention-based recurrent network decoder that emits each character conditioned on all previous characters, and the entire acoustic sequence. On a Google voice search task, LAS achieves a WER of 14.1% without a dictionary or an external language model and 10.3% with language model rescoring over the top 32 beams. In comparison, the state-of-the-art CLDNN-HMM model achieves a WER of 8.0% on the same set.",,,Unreleased,"United States of America,United States of America",,,,,,2024-11-01 10:02,,,,,,"Industry,Academia",,,,,Unreleased,,"Industry,Academia",,,,,,,,,
Search-Proven Best LSTM,Language,"Language modeling,Neural Architecture Search - NAS",Google,"R. JÃ³zefowicz, Wojciech Zaremba, Ilya Sutskever",2015-07-06,An Empirical Exploration of Recurrent Network Architectures,https://proceedings.mlr.press/v37/jozefowicz15.pdf,2207.0,Highly cited,,20000000.0,20M (Table 3),3340000000000000.0,6 FLOP / parameter / token * 20000000 parameters * 929000 tokens * 30 epochs = 3.3444e+15 FLOP,Penn TreeBank (PTB),,929000.0,""" After training for this number of epochs, we begin to lower the learning rate by a factor of decay each epoch for a total of 30 training epochs in total.""",,,,,Confident,"The Recurrent Neural Network (RNN) is an extremely powerful sequence model that is often difficult to train. The Long Short-Term Memory (LSTM) is a specific RNN architecture whose design makes it much easier to train. While wildly successful in practice, the LSTMâ€™s architecture appears to be ad-hoc so it is not clear if it is optimal, and the significance of its individual components is unclear. In this work, we aim to determine whether the LSTM architecture is optimal or whether much better architectures exist. We conducted a thorough architecture search where we evaluated over ten thousand different RNN architectures, and identified an architecture that outperforms both the LSTM and the recently-introduced Gated Recurrent Unit (GRU) on some but not all tasks. We found that adding a bias of 1 to the LSTMâ€™s forget gate closes the gap between the LSTM and the GRU.",30.0,Search-Proven Best LSTM,Unreleased,United States of America,,,,,,2025-04-18 12:10,,,,,,Industry,,,,,Unreleased,,Industry,,,,,Operation counting,,,,
Skip-Thoughts,Language,Language modeling,"University of Toronto,Massachusetts Institute of Technology (MIT),CIFAR AI Research","Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov, Richard S. Zemel, Antonio Torralba, Raquel Urtasun, Sanja Fidler",2015-06-22,Skip-Thought Vectors,https://arxiv.org/abs/1506.06726,,,,,possibly can be calculated based on architecture description,,,"""BookCorpus (BooksCorpus, Toronto Book Corpus)""",,984846357.0,Table 1,336.0,,,,Confident,"We describe an approach for unsupervised learning of a generic, distributed sentence encoder. Using the continuity of text from books, we train an encoder-decoder model that tries to reconstruct the surrounding sentences of an encoded passage. Sentences that share semantic and syntactic properties are thus mapped to similar vector representations. We next introduce a simple vocabulary expansion method to encode words that were not seen as part of training, allowing us to expand our vocabulary to a million words. After training our model, we extract and evaluate our vectors with linear models on 8 tasks: semantic relatedness, paraphrase detection, image-sentence ranking, question-type classification and 4 benchmark sentiment and subjectivity datasets. The end result is an off-the-shelf encoder that can produce highly generic sentence representations that are robust and perform well in practice. We will make our encoder publicly available.",,,Open weights (unrestricted),"Canada,United States of America,Canada",,,,,,2025-04-10 04:52,,,,,,"Academia,Academia,Research collective",,,,,Open source,"https://github.com/ryankiros/skip-thoughts
Apache 2.0","Academia,Academia,Research collective",,,,,,,,,
BatchNorm,Vision,Image classification,Google,"Sergey Ioffe, Christian Szegedy",2015-06-15,Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,https://arxiv.org/abs/1502.03167,41272.0,Highly cited,,13600000.0,"""The network contains 13.6 Â· 106 parameters""",,,ImageNet,,,,,,,,Confident,"Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9% top-5 validation error (and 4.8% test error), exceeding the accuracy of human raters.",72.0,,,United States of America,GoogLeNet / InceptionV1,,,,,2025-01-21 13:38,,,,,,Industry,,,,,,,Industry,,,,,,,,,
Faster R-CNN,Vision,Object detection,Microsoft Research,"S Ren, K He, R Girshick, J Sun",2015-06-04,Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks,https://arxiv.org/abs/1506.01497,56472.0,Highly cited,,,,,,,,,,,,,,Unknown,,,,Open weights (unrestricted),"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational",,,,,,2025-05-28 16:19,,,,,,Industry,,,,,Open source,"MIT license for repo:
https://github.com/ShaoqingRen/faster_rcnn 

contains weights and training scripts",Industry,,,FP32,,,,,,
Draw,Image generation,Image generation,Google DeepMind,"Karol Gregor, Ivo Danihelka, Alex Graves, Danilo Jimenez Rezende, Daan Wierstra",2015-05-20,DRAW: A Recurrent Neural Network For Image Generation,https://arxiv.org/abs/1502.04623,,,,,possibly can be calculated based on architecture description,,,"SVHN (Street View House Numbers),MNIST,CIFAR-10",,231053.0,,,,,,Confident,"This paper introduces the Deep Recurrent Attentive Writer (DRAW) neural network architecture for image generation. DRAW networks combine a novel spatial attention mechanism that mimics the foveation of the human eye, with a sequential variational auto-encoding framework that allows for the iterative construction of complex images. The system substantially improves on the state of the art for generative models on MNIST, and, when trained on the Street View House Numbers dataset, it generates images that cannot be distinguished from real data with the naked eye.",,,,"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France",,,,,,2025-04-15 08:03,,,,,,Industry,,,,,,,Industry,,,,,,,,,
Deep LSTM video classifier,Video,Video,"University of Texas at Austin,Google","Joe Yue-Hei Ng, Matthew Hausknecht, Sudheendra Vijayanarasimhan, Oriol Vinyals, Rajat Monga, George Toderici",2015-05-01,Beyond Short Snippets: Deep Networks for Video Classification,https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Ng_Beyond_Short_Snippets_2015_CVPR_paper.html,2290.0,Highly cited,,,,,,,,,,,,,,Unknown,,,,,"United States of America,United States of America",,,,,,2025-05-01 10:42,,,,,,"Academia,Industry",,,,,,,"Academia,Industry",,,,,,,,,
Fast R-CNN,Vision,Object detection,Microsoft Research,R Girshick,2015-04-30,Fast R-CNN,https://arxiv.org/abs/1504.08083,23178.0,Highly cited,,,,,,,,,,,,,,Unknown,,,,,"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational",,,,,,2025-05-28 16:19,,,,,,Industry,,,,,,,Industry,,,FP32,,,,,,
genCNN + dyn eval,Language,Language modeling,"Chinese Academy of Sciences,Huawei Noah's Ark Lab,Dublin City University","Mingxuan Wang, Zhengdong Lu, Hang Li, Wenbin Jiang, Qun Liu",2015-03-17,genCNN: A Convolutional Architecture for Word Sequence Prediction,https://aclanthology.org/P15-1151/,33.0,SOTA improvement,"""genCNN outperforms the state-ofthe-arts with big margins.""",8000000.0,8M according to https://arxiv.org/pdf/1508.06615,3.4153451e+16,"5046000000000 FLOP / sec/ GPU * 1 GPU * 48 hours [""Likely"" confidence since 2 days of training refer to another dataset] * 3600 sec / hour * 0.3 [assumed utilization] = 2.6158464e+17 FLOP

Assuming (!) 100 epochs (-> ""Speculative"" confidence""):

6 FLOP / parameter / token * 8000000 parameters * 929000 tokens * 100 epochs = 4.4592e+15 FLOP

sqrt(2.6158464e+17*4.4592e+15 ) = 3.4153451e+16 FLOP

________
in the algorithmic progress report paper the estimation was 7.3 Ã— 10^16 FLOP (hardware-based estimation assuming another Tesla K40 chip)
",Penn TreeBank (PTB),,929000.0,,48.0,""" The optimization is done mainly on a Tesla K40 GPU, which takes about 2 days for the training on a dataset containing 1M sentences.""
this training doesn't refer to the PTB dataset but to wiki dataset so it is likely an upper bound",NVIDIA Tesla K40s,,Speculative,"We propose a convolutional neural network, named genCNN, for word sequence prediction. Different from previous work on neural networkbased language modeling and generation (e.g., RNN or LSTM), we choose
not to greedily summarize the history of words as a fixed length vector. Instead, we use a convolutional neural network to predict the next word with
the history of words of variable length. Also different from the existing feedforward networks for language modeling, our model can effectively fuse
the local correlation and global correlation in the word sequence, with
a convolution-gating strategy specifically designed for the task. We argue
that our model can give adequate representation of the history, and therefore can naturally exploit both the short and long range dependencies. Our
model is fast, easy to train, and readily parallelized. Our extensive experiments on text generation and n-best re-ranking in machine translation show
that genCNN outperforms the state-ofthe-arts with big margins.",,genCNN + dyn eval,Unreleased,"China,China,Ireland",,,,,,2025-04-18 12:32,,,,,,"Academia,Industry,Academia",,,,,Unreleased,,"Academia,Industry,Academia",,,,,"Hardware,Operation counting",,,,
TRPO,Games,Atari,University of California (UC) Berkeley,"John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, Pieter Abbeel",2015-02-19,Trust Region Policy Optimization,https://arxiv.org/pdf/1502.05477,8305.0,Highly cited,Also listed in Denis Panjuta's List of 100+ AI Algorithms,33500.0,,,,,,,,30.0,"""The 500 iterations of our algorithm took about 30 hours (with slight variation between games) on a 16-core computer.""",,,Confident,"We describe an iterative procedure for optimizing policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified procedure, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is similar to natural policy gradient methods and is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.",,,Unreleased,United States of America,,,,,,2025-05-01 10:42,,,,,,Academia,,,,,,,Academia,,,,,,,,,
CRF-RNN,Vision,Image segmentation,"University of Oxford,Stanford University,Baidu","Shuai Zheng, Sadeep Jayasumana, Bernardino Romera-Paredes, Vibhav Vineet, Zhizhong Su, Dalong Du, Chang Huang, Philip H. S. Torr",2015-02-11,Conditional Random Fields as Recurrent Neural Networks,https://arxiv.org/abs/1502.03240,2661.0,Highly cited,,,,,,,,,,,,,,Unknown,,,,,"United Kingdom of Great Britain and Northern Ireland,United States of America,China",,,,,,2024-09-05 14:08,,,,,,"Academia,Academia,Industry",,,,,,,"Academia,Academia,Industry",,,,,,,,,
"MSRA (C, PReLU)",Vision,Image classification,Microsoft Research,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun",2015-02-06,Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification,https://arxiv.org/abs/1502.01852,20078.0,Highly cited,,87048800.0,"I used the architecture in table 3
I ignored biases, and assumed a SPP bin size of 256

3*7*7*96+96*3*3*384+384*3*3*384*5+384*3*3*768+768*3*3*768*5+768*3*3*896+896*3*3*896*5+896*(7*7+3*3+2*2+1)*4096+4096*4096+4096*1000=330581792



",2.397403008e+19,"""training C on eight K40 GPUs, takes about 3-4 weeks""
0.33 util rate
(From Imagenet paper-data, Besiroglu et al., forthcoming) ",,"They won ILSVRC 2015, but actually the classification dataset is the same as 2012",1280000.0,"""We perform the experiments on the 1000-class ImageNet 2012 dataset"", paper; ImageNet 2012 train set size from https://huggingface.co/datasets/imagenet-1k",588.0,,NVIDIA Tesla K40t,,Confident,"Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on our PReLU networks (PReLU-nets), we achieve 4.94% top-5 test error on the ImageNet 2012 classification dataset. This is a 26% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66%). To our knowledge, our result is the first to surpass human-level performance (5.1%, Russakovsky et al.) on this visual recognition challenge.",,,,"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational",,,,,,2025-06-18 02:57,,,,,,Industry,,,,,,,Industry,,,FP32,,Hardware,,,,
DeepLab,Vision,Image segmentation,"Google,University of California Los Angeles (UCLA)","Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, Alan L. Yuille",2014-12-22,Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs,https://arxiv.org/abs/1412.7062,4657.0,Highly cited,,,,,,,,,,,,,,Unknown,,,,,"United States of America,United States of America",,,,,,2025-05-28 16:19,,,,,,"Industry,Academia",,,,,,,"Industry,Academia",,,FP32,,,,,,
Fractional Max-Pooling,Vision,Image classification,University of Warwick,Benjamin Graham,2014-12-18,Fractional Max-Pooling,https://arxiv.org/abs/1412.6071v4,672.0,SOTA improvement,"""for instance, we improve on the state-of-the art for CIFAR-100 without even using dropout.""",27000000.0,27M weights in largest CIFAR-100 model,1e+17,"For the 12M param model, training required ""18 hours on a GeForce GTX 780"". So would be somewhat larger for 27M.

4 TFLOPS * 18 * 3600 * 0.4 = 1e17",CIFAR-100,,,,18.0,,NVIDIA GeForce GTX 780,,Likely,"Convolutional networks almost always incorporate some form of spatial pooling, and very often it is alpha times alpha max-pooling with alpha=2. Max-pooling act on the hidden layers of the network, reducing their size by an integer multiplicative factor alpha. The amazing by-product of discarding 75% of your data is that you build into the network a degree of invariance with respect to translations and elastic distortions. However, if you simply alternate convolutional layers with max-pooling layers, performance is limited due to the rapid reduction in spatial size, and the disjoint nature of the pooling regions. We have formulated a fractional version of max-pooling where alpha is allowed to take non-integer values. Our version of max-pooling is stochastic as there are lots of different ways of constructing suitable pooling regions. We find that our form of fractional max-pooling reduces overfitting on a variety of datasets: for instance, we improve on the state-of-the art for CIFAR-100 without even using dropout.",250.0,,,United Kingdom of Great Britain and Northern Ireland,,,,,,2025-02-17 14:15,,,,,,Academia,,,,,,,Academia,,,,,Hardware,,,,
NTM,"Other,Language",Sequence memorization,Google DeepMind,"Alex Graves, Greg Wayne, Ivo Danihelka",2014-12-10,Neural Turing Machines,https://arxiv.org/abs/1410.5401,2242.0,Highly cited,,,,,,,,,,,,,,Unknown,"We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent. Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.",,,,"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France",,,,,,2024-11-01 10:03,,,,,,Industry,,,,,,,Industry,,,,,,,,,
SNM-skip,Language,Language modeling,Google,"Noam Shazeer, Joris Pelemans, Ciprian Chelba",2014-12-03,Skip-gram Language Modeling Using Sparse Non-negative Matrix Probability Estimation,https://arxiv.org/abs/1412.1454,14.0,SOTA improvement,'When using skip-gram features the models are able to match the state-of-the-art recurrent neural network (RNN) LMs; combining the two modeling techniques yields the best known result on the benchmark. ' - from abstract,62000000000.0,62B from Table 2,2.97600000001e+20,https://www.wolframalpha.com/input?i=0.8+billion+*+62+billion+*+6+FLOP,One Billion Word benchmark,'Our experimental setup used the One Billion Word Benchmark corpus' from section 4.1 - 'Total number of training tokens is about 0.8 billion',1000000000.0,1B from 'Our experimental setup used the One Billion Word Benchmark corpus' from section 4.1 - 'Total number of training tokens is about 0.8 billion',,,,,Speculative,"We present a novel family of language model (LM) estimation techniques named Sparse Non-negative Matrix (SNM) estimation. A first set of experiments empirically evaluating it on the One Billion Word Benchmark shows that SNM n-gram LMs perform almost as well as the well-established Kneser-Ney (KN) models. When using skip-gram features the models are able to match the state-of-the-art recurrent neural network (RNN) LMs; combining the two modeling techniques yields the best known result on the benchmark. The computational advantages of SNM over both maximum entropy and RNN LM estimation are probably its main strength, promising an approach that has the same flexibility in combining arbitrary features effectively and yet should scale to very large amounts of data as gracefully as n-gram LMs do. ",,,,United States of America,,,,,,2025-05-01 10:42,,,,,,Industry,,,,,,,Industry,,,,,Operation counting,,,,
Cascaded LNet-ANet,Vision,Face detection,Chinese University of Hong Kong (CUHK),"Ziwei Liu, Ping Luo, Xiaogang Wang, Xiaoou Tang",2014-11-28,Deep Learning Face Attributes in the Wild,https://arxiv.org/abs/1411.7766,7710.0,Highly cited,,,,,,"ILSVRC 2012 subset of ImageNet,CelebA",,,,,,,,Unknown,"Predicting face attributes in the wild is challenging due to complex face variations. We propose a novel deep learning framework for attribute prediction in the wild. It cascades two CNNs, LNet and ANet, which are fine-tuned jointly with attribute tags, but pre-trained differently. LNet is pre-trained by massive general object categories for face localization, while ANet is pre-trained by massive face identities for attribute prediction. This framework not only outperforms the state-of-the-art with a large margin, but also reveals valuable facts on learning face representation.
(1) It shows how the performances of face localization (LNet) and attribute prediction (ANet) can be improved by different pre-training strategies.
(2) It reveals that although the filters of LNet are fine-tuned only with image-level attribute tags, their response maps over entire images have strong indication of face locations. This fact enables training LNet for face localization with only image-level annotations, but without face bounding boxes or landmarks, which are required by all attribute recognition works.
(3) It also demonstrates that the high-level hidden neurons of ANet automatically discover semantic concepts after pre-training with massive face identities, and such concepts are significantly enriched after fine-tuning with attribute tags. Each attribute can be well explained with a sparse linear combination of these concepts.",,,,"Hong Kong,China",,,,,,2024-11-01 10:03,,,,,,Academia,,,,,,,Academia,,,,,,,,,
Fully Convolutional Networks,Vision,Image segmentation,University of California (UC) Berkeley,"J Long, E Shelhamer, T Darrell",2014-11-14,Fully Convolutional Networks for Semantic Segmentation,https://arxiv.org/abs/1411.4038,35588.0,Highly cited,,,,,,,,,,,,,,Unknown,,,,,United States of America,,,,,,2025-05-28 16:19,,,,,,Academia,,,,,,,Academia,,,FP32,,,,,,
SC-NLM,"Multimodal,Vision,Language",Image captioning,University of Toronto,"Ryan Kiros, R. Salakhutdinov, R. Zemel",2014-11-10,Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models,https://www.semanticscholar.org/paper/Unifying-Visual-Semantic-Embeddings-with-Multimodal-Kiros-Salakhutdinov/2e36ea91a3c8fbff92be2989325531b4002e2afc,,Highly cited,,,,,,"COCO,Flickr30K Entities",,600000.0,"Our LSTM encoder and SC-NLM decoder were trained by concatenating the Flickr30K dataset with the recently released Microsoft COCO dataset [46], which combined give us over 100,000 images and over 500,000 descriptions for training",,,,,Confident,"Inspired by recent advances in multimodal learning and machine translation, we introduce an encoder-decoder pipeline that learns (a): a multimodal joint embedding space with images and text and (b): a novel language model for decoding distributed representations from our space. Our pipeline effectively unifies joint image-text embedding models with multimodal neural language models. We introduce the structure-content neural language model that disentangles the structure of a sentence to its content, conditioned on representations produced by the encoder. The encoder allows one to rank images and sentences while the decoder can generate novel descriptions from scratch. Using LSTM to encode sentences, we match the state-of-the-art performance on Flickr8K and Flickr30K without using object detections. We also set new best results when using the 19-layer Oxford convolutional network. Furthermore we show that with linear encoders, the learned embedding space captures multimodal regularities in terms of vector space arithmetic e.g. *image of a blue car* - ""blue"" + ""red"" is near images of red cars. Sample captions generated for 800 images are made available for comparison.",,,,Canada,,,,,,2024-09-05 14:08,,,,,,Academia,,,,,,,Academia,,,,,,,,,
Spatially-Sparse CNN,Vision,Image classification,University of Warwick,Benjamin Graham,2014-09-23,Spatially-sparse convolutional neural networks,https://arxiv.org/abs/1409.6070v1,260.0,SOTA improvement,SOTA per https://paperswithcode.com/sota/image-classification-on-cifar-10,,Parameter count not stated but is probably derivable from the paper.,,,CIFAR-10,,,,,,,,Unknown,"Convolutional neural networks (CNNs) perform well on problems such as handwriting recognition and image classification. However, the performance of the networks is often limited by budget and time constraints, particularly when trying to train deep networks.
Motivated by the problem of online handwriting recognition, we developed a CNN for processing spatially-sparse inputs; a character drawn with a one-pixel wide pen on a high resolution grid looks like a sparse matrix. Taking advantage of the sparsity allowed us more efficiently to train and test large, deep CNNs. On the CASIA-OLHWDB1.1 dataset containing 3755 character classes we get a test error of 3.82%.
Although pictures are not sparse, they can be thought of as sparse by adding padding. Applying a deep convolutional network using sparsity has resulted in a substantial reduction in test error on the CIFAR small picture datasets: 6.28% on CIFAR-10 and 24.30% for CIFAR-100.",,,,United Kingdom of Great Britain and Northern Ireland,,,,,,2024-09-05 14:08,,,,,,Academia,,,,,,,Academia,,,,,,,,,
GoogLeNet / InceptionV1,Vision,Image classification,"Google,University of Michigan,University of North Carolina","Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich",2014-09-17,Going deeper with convolutions,https://arxiv.org/abs/1409.4842,41425.0,Highly cited,,6797700.0,Computed summing the parameters on table 1 of section 5,1.51e+18,"AI and Compute  (https://openai.com/blog/ai-and-compute/) charts imply a value of 1.51e18 (value extracted using WebPlotDigitizer  https://automeris.io/WebPlotDigitizer/ ).

Based on the paper, there are 1.5B multiply-adds per inference, and 1.2M images in the training set, but an unknown number of epochs. They decrease the learning rate by 4% every 8 epochs, so there are likely many. If the figure from AI and Compute is taken as true, there were likely 140 epochs","ILSVRC 2014 subset of ImageNet,ImageNet",,1200000.0,"""The ILSVRC 2014 classification challenge involves the
task of classifying the image into one of 1000 leaf-node categories in the Imagenet hierarchy. There are about 1.2 million images for training, 50,000 for validation and 100,000 images for testing""
...
""We participated in the challenge with no external data used for training.""",,"""Although we used CPU based implementation only, a rough estimate suggests that the GoogLeNet network could be trained to convergence using few high-end GPUs within a week""",,,Confident,"We propose a deep convolutional neural network architecture codenamed ""Inception"", which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.",827.0,,,"United States of America,United States of America,United States of America",,,,,,2025-02-17 14:14,,,,,,"Industry,Academia,Academia",,,,,,,"Industry,Academia,Academia",,,,,Third-party estimation,,,,
SPN-4+KN5,Language,Language modeling,"Singapore University of Technology & Design,DSO National Laboratories","W. Cheng, Stanley Kok, Hoai Vu Pham, Hai Leong Chieu, K. M. A. Chai",2014-09-14,Language modeling with sum-product networks,https://www.comp.nus.edu.sg/~skok/papers/is14.pdf,102.0,SOTA improvement,"""Our empirical comparisons with
six previous language models indicate that our SPN has superior
performance""",5000000.0,"Estimate from table 2 of https://arxiv.org/abs/1609.07843

The authors of the linked paper draw on estimates from table 3 of https://arxiv.org/pdf/1508.06615.pdf",4.4e+16,"40h, 1 GPU, 1028e9 Peak FLOP/s, 30%

1028000000000 FLOP/s/GPU * 1GPU * 40 hours * 3600 s/hour * 0.3 [assumed utilization] = 4.44096e+16 FLOP",Penn TreeBank (PTB),,,"seems like the authors use a non-standard split for the dataset

""We performed our experiments on the commonly used Penn
Treebank corpus [15], and adhered to the experimental setup
used in previous work [6, 9]. We used sections 0-20, sections
21-22, and sections 23-24 respectively as training, validation
and test sets""

apparently the most common split is ""In the most common split of this corpus, sections from 0 to 18 are used for training (38 219 sentences, 912 344 tokens), sections from 19 to 21 are used for validation (5 527 sentences, 131 768 tokens), and sections from 22 to 24 are used for testing (5 462 sentences, 129 654 tokens)""

unknown amount of epochs
batch size = 100, context length = 5? (because we look at N=4 previous words). training iterations unknown

""We used a learning rate of Î·= 0.1, a mini-batch size of 100,
randomly initialized the weights to a value between 0 and 1, and
imposed an L2 penalty of 10âˆ’5 on all weights. With reference
to Figure 2, We used K = 10000, feature vectors with D = 100
dimensions, and N = 3 and N = 4 previous words.""",40.0,"""""e stopped training our SPN when its performance on the validation set
stops improving at two consecutive evaluation points, or when it has run for 40 hours, whichever occurred first. (It turned out that both SPN-3 and SPN-4 ran for the maximum of 40 hours.)
We parallelized our SPN code2 to run on a GPU, and ran our experiments on a machine with a 2.4 GHz CPU and an NVIDIA Tesla C2075 GPU (448 CUDA cores, 5GB of device memory).""",NVIDIA Tesla C2075,,Likely,"Sum product networks (SPNs) are a new class of deep probabilistic models. They can contain multiple hidden layers while keeping their inference and training times tractable. An SPN consists of interleaving layers of sum nodes and product nodes. A sum node can be interpreted as a hidden variable, and a product node can be viewed as a feature capturing rich interactions among an SPNâ€™s inputs. We show that the ability of SPN to use hidden layers to model complex dependencies among words, and its tractable inference and learning times, make it a suitable framework for a language model. Even though SPNs have been applied to a variety of vision problems [1, 2], we are the first to use it for language modeling. Our empirical comparisons with
six previous language models indicate that our SPN has superior performance.",,SPN-4+KN5,Unreleased,"Singapore,Singapore",,,,1.0,,2025-04-21 11:53,,,,,,"Academia,Government",,,,,Open (non-commercial),"code, no license specified: https://github.com/stakok/lmspn/tree/master/SPNLM 
training code: https://github.com/stakok/lmspn/blob/master/SPNLM/README.doc ","Academia,Government",,,,290.04546165358084,Hardware,,,,
Seq2Seq LSTM,Language,Translation,Google,"Ilya Sutskever, Oriol Vinyals, Quoc V. Le",2014-09-10,Sequence to Sequence Learning with Neural Networks,https://arxiv.org/abs/1409.3215,19578.0,Highly cited,,1920000000.0,"The resulting LSTM has 384M parameters of which 64M are pure recurrent connections (32M for the â€œencoderâ€ LSTM and 32M
for the â€œdecoderâ€ LSTM).
The paper uses an ensemble of 5 LSTMs.",5.6e+19,"384E+6 parameters * 2 FLOP/parameter * (348E+6 + 304E+6 points per epoch) * 7.5 epochs * 3 FLOP/point ~= 1.126656e+19 FLOP
Times 5 independent models in ensemble => 5.6E+19 FLOP

If we assume NVIDIA K40 (in use at the time): 10 days * 24 * 60 * 60 seconds/day * 8 GPUs * 33% * 5e12 FLOP/s * 5 models in ensemble ~= 5.7E+19 FLOP

Authors of ""AI and Memory Wall"" estimated model's training compute as 11,000 PFLOPS = 1.1*10^19 FLOPS
(https://github.com/amirgholami/ai_and_memory_wall)",WMT14,,652000000.0,"[WORDS]
""We used the WMTâ€™14 English to French dataset. We trained our models on a subset of 12M sentences consisting of 348M French words and 304M English words, which is a clean â€œselectedâ€
subset from [29].""",240.0,Training took about 10 days,,,Confident,"Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.",7.5,,,United States of America,,,,,,2025-05-08 16:08,,,,,,Industry,,,,,,,Industry,,,,,"Operation counting,Hardware,Third-party estimation",,,,
Large regularized LSTM,Language,Language modeling,"New York University (NYU),Google Brain","Wojciech Zaremba, Ilya Sutskever, Oriol Vinyals",2014-09-08,Recurrent Neural Network Regularization,https://arxiv.org/abs/1409.2329,3224.0,Highly cited,,66000000.0,"""The large LSTM has 1500 units per layer and its parameters are initialized uniformly in [âˆ’0.04, 0.04]""

66M according to Table 3 of https://arxiv.org/pdf/1611.01462",4.2966069e+16,"3520000000000 FLOP / s/ GPU [tesla k20c assumed, the paper says just ""Nvidia k20""] * 1 GPU * 24 hours [could be less, they report ""the entire day""] * 3600 s / hour * 0.3 [assumed utilization] = 9.12384e+16 FLOP 

6 FLOP / parameter / token * 66000000 parameters * 929000 tokens * 55 epochs = 2.023362e+16 FLOP

sqrt(2.023362e+16*9.12384e+16) = 4.2966069e+16 ",Penn TreeBank (PTB),,929000.0,""" We train the model for 55 epochs with a learning rate of 1""",24.0,"""Training this network takes an entire day on an NVIDIA K20 GPU.""",NVIDIA Tesla K20c,,Confident,"We present a simple regularization technique for Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) units. Dropout, the most successful technique for regularizing neural networks, does not work well with RNNs and LSTMs. In this paper, we show how to correctly apply dropout to LSTMs, and show that it substantially reduces overfitting on a variety of tasks. These tasks include language modeling, speech recognition, image caption generation, and machine translation.",55.0,Large regularized LSTM,Unreleased,"United States of America,United States of America",,,,1.0,,2025-04-21 12:04,,,,,,"Academia,Industry",,,,,Open source,"Apache: https://github.com/wojzaremba/lstm 
train: https://github.com/wojzaremba/lstm/blob/master/main.lua ","Academia,Industry",,,,264.24675821957226,"Hardware,Operation counting",,,,
VGG16,Vision,Image classification,University of Oxford,Karen Simonyan; Andrew Zisserman,2014-09-04,Very Deep Convolutional Networks for Large-Scale Image Recognition,https://arxiv.org/abs/1409.1556,94013.0,Highly cited,,138000000.0,"Source: Table 2
https://arxiv.org/abs/1409.1556",1.2291000000000002e+19,"3 weeks * 4 Titan Black GPUs * 0.30 utilization

Section 3.3: ""On a system equipped with four NVIDIA Titan Black GPUs, training a single net took 2â€“3 weeks depending on the architecture.""

Titan Black performance: 5.645 TFLOPS (assuming FP32)

https://www.wolframalpha.com/input?i=5.645+TFLOPS+*+3+weeks+*+4+*+0.3


",ILSVRC 2012 subset of ImageNet,,1300000.0,"""In this section, we present the image classification results achieved by the described ConvNet architectures on the ILSVRC-2012 dataset (which was used for ILSVRC 2012â€“2014 challenges). The dataset includes images of 1000 classes, and is split into three sets: training (1.3M images), validation (50K images), and testing (100K images with held-out class labels).""

This is confirmed by section 3.1 Training:
""The batch size was set to 256""
""In total, the learning rate was decreased 3 times, and the learning was stopped after 370K iterations (74 epochs)""
256 * 370k/74 = 1.3M",504.0,,NVIDIA GTX Titan Black,,Confident,,74.0,,,United Kingdom of Great Britain and Northern Ireland,,,,4.0,,2025-05-28 16:19,,,,256.0,,Academia,,,,,,,Academia,,,FP32,2137.653074766455,Hardware,,,,
VGG19,Vision,Image classification,University of Oxford,"Karen Simonyan, Andrew Zisserman",2014-09-04,Very Deep Convolutional Networks for Large-Scale Image Recognition,https://arxiv.org/abs/1409.1556,94013.0,Highly cited,,144000000.0,"Source: Table 2
https://arxiv.org/abs/1409.1556",1.1e+19,"Authors of ""AI and Memory Wall"" (https://github.com/amirgholami/ai_and_memory_wall) estimated model's training compute as 11,000 PFLOP = 1.1*10^19 FLOP",ILSVRC 2012 subset of ImageNet,,1300000.0,"""In this section, we present the image classification results achieved by the described
ConvNet architectures on the ILSVRC-2012 dataset (which was used for ILSVRC 2012â€“2014 challenges). The dataset includes images of 1000 classes, and is split into three sets: training (1.3M images), validation (50K images), and testing (100K images with held-out class labels).""",,,,,Likely,"In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.",,,,United Kingdom of Great Britain and Northern Ireland,,,,,,2025-05-28 16:19,,,,,,Academia,,,,,,,Academia,,,FP32,,Third-party estimation,,,,
RNNsearch-50*,Language,Translation,"Jacobs University Bremen,University of Montreal / UniversitÃ© de MontrÃ©al","D Bahdanau, K Cho, Y Bengio",2014-09-01,Neural Machine Translation by Jointly Learning to Align and Translate,https://arxiv.org/abs/1409.0473,26073.0,Highly cited,,,,1.5552e+18,"From https://openai.com/blog/ai-and-compute/ Appendix.

0.018 pfs-days
(86400*10^15*0.018)

252 hours in a Quadro K-6000 GPU (assumed utilization: 0.33)

5196000000000 FLOP/s *252 hours * 3600 second/hour * 0.33 utilization = 1555200000000000000 FLOP",WMT'14 + selection,,348000000.0,"[WORDS]
""WMT â€™14 contains the following English-French parallel corpora: Europarl (61M words), news
commentary (5.5M), UN (421M) and two crawled corpora of 90M and 272.5M words respectively,
totaling 850M words. Following the procedure described in Cho et al. (2014a), we reduce the size of
the combined corpus to have 348M words using the data selection method by Axelrod et al. (2011).""",,,NVIDIA Quadro K6000,,Confident,,,,,"Germany,Canada",,,,,,2025-06-18 02:28,,,,,,"Academia,Academia",,,,,,,"Academia,Academia",,,,,Third-party estimation,,,,
AdClickNet,Recommendation,"Recommender system,Click-through rate prediction",Facebook,"Xinran He, Junfeng Pan, Ou Jin, Tianbing Xu, Bo Liu, Tao Xu, Yanxin Shi, Antoine Atallah, Ralf Herbrich, Stuart Bowers, Joaquin QuiÃ±onero Candela",2014-08-24,Practical Lessons from Predicting Clicks on Ads at Facebook,https://dl.acm.org/doi/10.1145/2648584.2648589,859.0,,,,,,,,,,,,,,,Unknown,"Online advertising allows advertisers to only bid and pay for measurable user responses, such as clicks on ads. As a consequence, click prediction systems are central to most online advertising systems. With over 750 million daily active users and over 1 million active advertisers, predicting clicks on Facebook ads is a challenging machine learning task. In this paper we introduce a model which combines decision trees with logistic regression, outperforming either of these methods on its own by over 3%, an improvement with significant impact to the overall system performance. We then explore how a number of fundamental parameters impact the final prediction performance of our system. Not surprisingly, the most important thing is to have the right features: those capturing historical information about the user or ad dominate other types of features. Once we have the right features and the right model (decisions trees plus logistic regression), other factors play small roles (though even small improvements are important at scale). Picking the optimal handling for data freshness, learning rate schema and data sampling improve the model slightly, though much less than adding a high-value feature, or picking the right model to begin with.",,,,United States of America,,,,,,2024-11-01 10:03,,,,,,Industry,,,,,,,Industry,,,,,,,,,
SmooCT,Games,Poker,University College London (UCL),"Johannes Heinrich, David Silver",2014-07-01,Self-Play Monte-Carlo Tree Search in Computer Poker,https://www.semanticscholar.org/paper/Self-play-Monte-Carlo-tree-search-in-computer-poker-Heinrich-Silver/7b687599b4425aa959036071030e1212a3b359c7,16.0,SOTA improvement,First RL system to achieve superhuman level at Poker Limit Texas Hold Em,,,6.9e+16,"""Each three-player agent was trained for about 12 billion episodes, requiring about 48 hours of training time [...] on a modern computer without using parallelization""

Assume an Intel i7 so 400e9 FLOP/s.
6.9e16 = 400e9*60*60*48",,,12000000000.0,"""Each three-player agentwas trained for about 12 billion episodes""

An episode seems to be a round of betting.",48.0,,,,Likely,"Self-play reinforcement learning has proved to be successful in many perfect information two-player games.
However, research carrying over its theoretical guarantees and practical success to games of imperfect information has been lacking. In this paper, we evaluate selfplay Monte-Carlo Tree Search (MCTS) in limit Texas Holdâ€™em and Kuhn poker. We introduce a variant of the established UCB algorithm and provide first empirical results demonstrating its ability to find approximate Nash equilibria.",,,,United Kingdom of Great Britain and Northern Ireland,,,,,,2025-06-17 11:11,,,,,,Academia,,,,,,,Academia,,,,,Hardware,,,,
DeepFace,Vision,Face verification,"Tel Aviv University,Facebook","Y Taigman, M Yang, MA Ranzato",2014-06-23,DeepFace: Closing the Gap to Human-Level Performance in Face Verification,https://ieeexplore.ieee.org/document/6909616,5998.0,Highly cited,,,,,,,,,,,,,,Unknown,,,,,"Israel,United States of America",,,,,,2024-11-01 10:03,,,,,,"Academia,Industry",,,,,,,"Academia,Industry",,,,,,,,,
RNN-WER,Speech,Speech recognition,"DeepMind,University of Toronto","Alex Graves, Navdeep Jaitly",2014-06-22,Towards End-To-End Speech Recognition with Recurrent Neural Networks,https://proceedings.mlr.press/v32/graves14.html,2805.0,"Highly cited,SOTA improvement","""Finally, by combining the new model with a baseline, we
have achieved state-of-the-art accuracy on the Wall Street
Journal corpus for speaker independent recognition.""",26500000.0,"""The network had five levels of bidirectional LSTM hidden layers, with 500 cells in each layer, giving a total of âˆ¼ 26.5M weights.""",,,WSJ,"""The experiments were carried out on the Wall Street Journal (WSJ) corpus (available as LDC corpus LDC93S6B
and LDC94S13B). The RNN was trained on both the 14
hour subset â€˜train-si84â€™ and the full 81 hour set""

",1100000.0,"dataset is 81 hours

At 228 wpm (https://docs.google.com/document/d/1G3vvQkn4x_W71MKg0GmHVtzfd9m0y3_Ofcoew0v902Q/edit)
that's 81*228*60 = 1,108,080

another source says WSJ contains 37k sentences, so this would be ~30 words per sentence which seems high but roughly right: https://www.arxiv-vanity.com/papers/1903.00216/",,,,Supervised,Likely,"This paper presents a speech recognition system that directly transcribes audio data with text, without requiring an intermediate phonetic representation. The system is based on a combination of the deep bidirectional LSTM recurrent neural network architecture and the Connectionist Temporal Classification objective function. A modification to the objective function is introduced that trains the network to minimise the expectation of an arbitrary transcription loss function. This allows a direct optimisation of the word error rate, even in the absence of a lexicon or language model. The system achieves a word error rate of 27.3% on the Wall Street Journal corpus with no prior linguistic information, 21.9% with only a lexicon of allowed words, and 8.2% with a trigram language model. Combining the network with a baseline system further reduces the error rate to 6.7%.",,,,"United Kingdom of Great Britain and Northern Ireland,Canada",,,,,,2025-05-09 11:32,,,,,,"Industry,Academia",,,,,,,"Industry,Academia",,,,,,,,,
Fragment embedding,Multimodal,Entity embedding,Stanford University,"A. Karpathy, Armand Joulin, Li Fei-Fei",2014-06-21,Deep Fragment Embeddings for Bidirectional Image Sentence Mapping,https://www.semanticscholar.org/paper/Deep-Fragment-Embeddings-for-Bidirectional-Image-Karpathy-Joulin/7f1b111f0bb703b0bd97aba505728a9b0d9b2a54,,SOTA improvement,"""Extensive experimental evaluation shows that reasoning on both the global level of images and sentences and the finer level of their respective fragments significantly improves performance on image-sentence retrieval tasks.""",144496000.0,"Model contains a word embedding. a matrix combining two word embeddings, and image embedding (built upon a pretrained RCNN image model.
Word embedding: 400000 * 200 =80000000 (""Here, We is a d Ã— 400, 000 matrix that encodes a 1-of-k vector into a d-dimensional word vector representation (we use d = 200).""
Embedding dimension: 1000 (""The size of the embedded space is cross-validated, and we found that values of approximately 1000 generally work well.""
Word combination matrix: 400* 1000=400000
Image embedding: 4096*1000=4096000 (""We use the Caffe [41] implementation of the ImageNet Detection RCNN model [27] to detect objects in all images. On our machine with a Tesla K40 GPU, the RCNN processes one image in approximately 25 seconds. We discard the predictions for 200 ImageNet detection classes and only keep the 4096-D activations"")
CNN: 60,000,000 ""The CNN architecture is identical to the one described in Girhsick et al. [26]. It contains approximately 60 million parameters""
Total parameters: 4096000+80000000+400000+60000000=144,496,000",,,Flickr30K Entities,"The datasets contain 1,000, 8,000 and 30,000 images respectively and each image is annotated using Amazon Mechanical Turk with 5 independent sentences.",150000.0,"Largest experiment uses 30000 training images, 30000 * 5 = 150,000 sentences",,,,,Likely,"We introduce a model for bidirectional retrieval of images and sentences through a deep, multi-modal embedding of visual and natural language data. Unlike previous models that directly map images or sentences into a common embedding space, our model works on a finer level and embeds fragments of images (objects) and fragments of sentences (typed dependency tree relations) into a common space. We then introduce a structured max-margin objective that allows our model to explicitly associate these fragments across modalities. Extensive experimental evaluation shows that reasoning on both the global level of images and sentences and the finer level of their respective fragments improves performance on image-sentence retrieval tasks. Additionally, our model provides interpretable predictions for the image-sentence retrieval task since the inferred inter-modal alignment of fragments is explicit.",20.0,,,United States of America,,,,,,2025-03-10 13:45,,,,,,Academia,,,,,,,Academia,,,,,,,,,
SPPNet,Vision,Image classification,"Microsoft,Xiâ€™an Jiaotong University,University of Science and Technology of China (USTC)","Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun",2014-06-18,Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition,https://arxiv.org/abs/1406.4729,10365.0,Highly cited,,,,3.411072e+18,"""All networks in this paper can be
trained on a single GeForce GTX Titan GPU (6 GB memory) within two to four weeks.""
4.7e12 FLOP/s * 4* 7*24*60*60 seconds * 0.3 utilisation",ImageNet-1k,,1280000.0,"Section 3.1: ""We train the networks on the 1000-category training
set of ImageNet 2012.""",672.0,"""All networks in this paper can be trained on a single GeForce GTX Titan GPU (6 GB memory) within two to four weeks.""",NVIDIA GeForce GTX TITAN,,Confident,,,,,"United States of America,Multinational,India,Belgium,China,China",,,,,,2025-06-17 11:09,,,,,,"Industry,Academia,Academia",,,,,,,"Industry,Academia,Academia",,,FP32,,Hardware,,,,
GANs,Image generation,Image generation,University of Montreal / UniversitÃ© de MontrÃ©al,"Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio",2014-06-10,Generative Adversarial Networks,https://arxiv.org/abs/1406.2661,36870.0,Highly cited,,,The paper outlines the G-D framework but doesn't provide information about the structures of their generator and discriminator.,5.184e+17,"From https://openai.com/blog/ai-and-compute/ Appendix

""Less than 0.006 pfs-days""
(86400*10^15*0.006)

Seems extremely speculative, unless someone at OpenAI privately corresponded with the authors. There is no information about compute or training in the GANs paper.",CIFAR-10,,60000.0,"""We trained adversarial nets an a range of datasets including MNIST[23], the Toronto Face Database (TFD) [28], and CIFAR-10 [21].""

MNIST has 60k images 
https://en.wikipedia.org/wiki/MNIST_database

TFD seems to have 2925 examples (?)
https://www.cs.toronto.edu/~urtasun/courses/CSC411/hw3-411.pdf

CIFAR-10 has 60k images
https://www.cs.toronto.edu/~kriz/cifar.html

",,,,,Speculative,"We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.",,,,Canada,,,,,,2025-02-17 14:10,,,,,,Academia,,,,,,,Academia,,,,,Third-party estimation,,,,
Two-stream ConvNets for action recognition,Video,Video classification,University of Oxford,"Karen Simonyan, Andrew Zisserman",2014-06-09,Two-Stream Convolutional Networks for Action Recognition in Videos,https://arxiv.org/abs/1406.2199,7231.0,Highly cited,,,,,,,,,,,,,,Unknown,,,,,United Kingdom of Great Britain and Northern Ireland,,,,,,2025-05-28 16:19,,,,,,Academia,,,,,,,Academia,,,FP32,,,,,,
GRUs,Language,"Language modeling,Translation","University of Montreal / UniversitÃ© de MontrÃ©al,Jacobs University,University of Maine","Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, Yoshua Bengio",2014-06-03,Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation,https://arxiv.org/abs/1406.1078,21840.0,Highly cited,,,,,,,,,,,,,,Unknown,"In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.",,,,"Canada,Germany,United States of America",,,,,,2024-11-01 10:03,,,,,,"Academia,Academia,Academia",,,,,,,"Academia,Academia,Academia",,,,,,,,,
AdaRNN,Language,Sentiment classification,Beihang University,"Li Dong, Furu Wei, Chuanqi Tan, Duyu Tang, M. Zhou, Ke Xu",2014-06-01,Adaptive Recursive Neural Network for Target-dependent Twitter Sentiment Classification,https://www.semanticscholar.org/paper/Adaptive-Recursive-Neural-Network-for-Twitter-Dong-Wei/06e122f475a21d92dba137609c40f35690217475,,Highly cited,,13040.0,"D=25 ""For recursive neural models, the dimension of word vector is set to 25, and f = tanh is used as the nonlinearity function. We employ 10 composition matrices in AdaRNN.""
Composition matrices: ""W âˆˆ R DÃ—2D is the composition matrix, and b is the bias vector.""
C=10 ""We employ 10 composition matrices in AdaRNN.""
Combination matrix: ""S âˆˆ R CÃ—(2D+|e|) is the matrix used to determine which composition function we use, vl , vr are the left and right child vectors, and e are external feature vector. In this work, e is a one-hot binary feature vector which indicates what the dependency type is.""
|e| > 4: (see Figure 2)
Weights: 10 * 25 * 50 + 10 * (50+4) =13040 (ignoring embedding to the 25 dimension embedding space)",,,,,6248.0,"""Training data consists of 6,248 tweets,""",,,,,Confident,"We propose Adaptive Recursive Neural Network (AdaRNN) for target-dependent Twitter sentiment classification. AdaRNN adaptively propagates the sentiments of words to target depending on the context and syntactic relationships between them. It consists of more than one composition functions, and we model the adaptive sentiment propagations as distributions over these composition functions. The experimental studies illustrate that AdaRNN improves the baseline methods. Furthermore, we introduce a manually annotated dataset for target-dependent Twitter sentiment analysis.",,,,China,,,,,,2024-09-05 14:08,,,,,,Academia,,,,,,,Academia,,,,,,,,,
Paragraph Vector,Language,Language modeling,Google,"Quoc V. Le, Tomas Mikolov",2014-05-14,Distributed Representations of Sentences and Documents,https://www.semanticscholar.org/paper/Distributed-Representations-of-Sentences-and-Le-Mikolov/f3de86aeb442216a8391befcacb49e58b478f512,,Highly cited,,32000000.0," 75000*400+5000*400=32000000
""We learn the word vectors and paragraph vectors using 75,000 training documents""
""In PV-DM, the learned vector representations have 400 dimensions for both words and documents""
Paragraph embedding of dimension number of paragraphs * embedding size
Word embedding of dimension |V|*embedding size
Assuming vocabulary of 5000 since results are compared directly to Maas et. al., 2011",,,IMDb,,75000.0,"""25,000 labeled training instances, 25,000 labeled test in-
stances and 50,000 unlabeled training instances.""",,,,,Confident,"Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words. For example, ""powerful,"" ""strong"" and ""Paris"" are equally distant. In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that Paragraph Vectors outperforms bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks.",,,,United States of America,,,,,,2024-09-05 14:08,,,,,,Industry,,,,,,,Industry,,,,,,,,,
OverFeat,Vision,Image classification,New York University (NYU),"Pierre Sermanet, David Eigen, Xiang Zhang, Michael Mathieu, Rob Fergus, Yann LeCun",2013-12-21,"OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks",https://arxiv.org/abs/1312.6229,5148.0,Highly cited,,144000000.0,144M (Table 4),,,ImageNet-1k,,,"""We then extract 5 random crops (and their horizontal flips) of size 221x221 pixels and present these to the network in mini-batches of
size 128.""",,,NVIDIA Tesla K20X,,Confident,"We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat.",80.0,,,United States of America,,,,,,2025-05-01 14:31,,,,,,Academia,,,,,,,Academia,,,,,,,,,
Image generation,Vision,Image clustering,University of Amsterdam,"DP Kingma, M Welling",2013-12-20,Auto-Encoding Variational Bayes,https://arxiv.org/abs/1312.6114,21760.0,Highly cited,,784000.0,"""We trained generative models (decoders) and corresponding encoders
(a.k.a. recognition models) having 500 hidden units in case of MNIST""

784*500*2=784000 (Ignoring latent dimension)",475200000000000.0,"From https://openai.com/blog/ai-and-compute/ Appendix

""less than 0.0000055 pfs-days""
(86400*10^15*0.0000055)

Figure 2 shows evaluations with 10^8 training samples

6*784000*100000000=470400000000000",MNIST,,60000.0,"""We trained generative models of images from the MNIST and Frey Face datasets""

MNIST has 60k images
https://en.wikipedia.org/wiki/MNIST_database

Frey Face has 2k images
https://cs.nyu.edu/~roweis/data.html

",,,,,Confident,,,,,Netherlands,,,,,,2025-06-17 11:08,,,,,,Academia,,,,,,,Academia,,,,,Third-party estimation,,,,
DQN,Games,Atari,DeepMind,"V Mnih, K Kavukcuoglu, D Silver, A Graves",2013-12-19,Playing Atari with Deep Reinforcement Learning,https://arxiv.org/abs/1312.5602,11277.0,Highly cited,,836096.0,"""The input to the neural network consists is an 84 Ã— 84 Ã— 4 image produced by Ï†. The first hidden layer convolves 16 8 Ã— 8 filters with stride 4 with the input image and applies a rectifier nonlinearity [10, 18]. The second hidden layer convolves 32 4 Ã— 4 filters with stride 2, again followed by a rectifier nonlinearity. The final hidden layer is fully-connected and consists of 256 rectifier units. The output layer is a fully connected linear layer with a single output for each valid action. The number of valid actions varied between 4 and 18 on the games we considered.""

Parameter: 4*16*8*8+16*32*4*4+10*10*32*256+18*256=836096",2846883840000000.0,"Network is 84x84x4 input, 16, 8x8, stride 4, 32 4x4 stride 2, 256 fully connected
First layer: 20*20*4*16*8*8 = 1638400
Second layer: 9*9*16*32*4*4 = 663552
Third layer: 9*9*32*256 = 663552
Total ~ 2965504
2965504 * 5M updates * 32 batch size * 2 multiply-add * 3 backward pass
= 2965504*50000*100*32*6 = 2846883840000000

",,,,,,,,,Confident,,,,,United Kingdom of Great Britain and Northern Ireland,,,,,,2025-06-17 10:55,,,,,,Industry,,,,,,,Industry,,,,,Operation counting,,,,
Network in Network,Vision,Image classification,National University of Singapore,"M Lin, Q Chen, S Yan",2013-12-16,Network In Network,https://arxiv.org/abs/1312.4400,6052.0,Highly cited,,,,,,,,,,,,,,Unknown,,,,,Singapore,,,,,,2025-05-28 16:19,,,,,,Academia,,,,,,,Academia,,,FP32,,,,,,
RNN for 1B words,Language,Language modeling,Google,"Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, Tony Robinson",2013-12-11,One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling,https://arxiv.org/abs/1312.3005,1205.0,"Highly cited,SOTA improvement","from abstract: 'We show performance of several well-known types of language models, with the best results achieved with a recurrent neural network based language model. The baseline unpruned Kneser-Ney 5-gram model achieves perplexity 67.6; a combination of techniques leads to 35% reduction in perplexity, or 10% reduction in cross-entropy (bits), over that baseline. '",20000000000.0,20B from Table 1,,"240 hours on 24 CPUs from Table 1. CPU model is not given, but there is mention of using SIMD instructions. 1 SIMD operation is around 4 FLOP. CPU can have around 3e9 operations per second. so around 12e9*24 * 240*3600 = 2.4e17 operations. This estimation doesn't include use of multiple threads. Including use of threads we would probably have around 10 times more operations  so around 2.4e18 FLOPs. This estimation is speculative.",One Billion Word benchmark,"from abstract: ""With almost one billion words of training data, """,1000000000.0,"from abstract: 'With almost one billion words of training data, '",240.0,"from Table 1,240 hours on 24 CPUs",,,Speculative,"We propose a new benchmark corpus to be used for measuring progress in statistical language modeling. With almost one billion words of training data, we hope this benchmark will be useful to quickly evaluate novel language modeling techniques, and to compare their contribution when combined with other advanced techniques. We show performance of several well-known types of language models, with the best results achieved with a recurrent neural network based language model. The baseline unpruned Kneser-Ney 5-gram model achieves perplexity 67.6; a combination of techniques leads to 35% reduction in perplexity, or 10% reduction in cross-entropy (bits), over that baseline.
The benchmark is available as a this http URL project; besides the scripts needed to rebuild the training/held-out data, it also makes available log-probability values for each word in each of ten held-out data sets, for each of the baseline n-gram models. ",,,,United States of America,,,,24.0,,2025-03-04 10:19,,,,,,Industry,,,,5760.0,,,Industry,,,,,,,,,
TransE,Language,Entity embedding,"Universite de Technologie de CompiÃ¨gne â€“ CNRS,Google","Antoine Bordes, Nicolas Usunier, Alberto Garcia- Duran, Jason Weston, and Oksana Yakhnenko",2013-12-05,Translating Embeddings for Modeling Multi- relational Data,https://papers.nips.cc/paper/2013/hash/1cecc7a77928ca8133fa24680a88d2f9-Abstract.html,7039.0,Highly cited,,942000000.0,"Based on the TransE architecture, the authors give a formula for how the model size scales with the dimensionality of the dataset. The model scale is proportional to: k*(n_e+n_r) where k is the embeddings dimension, n_e is the number of entities, and n_r is the number of relationships.

They studied using the TransE model for two datasets: FB15k and FB1M. The FB15k model has 810000 parameters.

FB15k has 14951 entities and 1345 relationships. FB1M has 1000000 entities and 23382 relationships. Therefore, the FB1M model will be bigger than the FB15k model by a factor of (23382e6)/(14951*1345) => N = 8.1e5 * (23382e6)/(14951*1345) = 942e6.",1.340928e+18,"8 GPUs (they don't specify which, so I used the average for FP32 for 2017 from the write-up table)
8 hours 
0.33 util rate",,,17000000.0,"""it can be successfully trained on a large scale data set with 1M
entities, 25k relationships and more than 17M training samples""",,,,,Speculative,,,,,"France,United States of America",,,,,,2025-02-17 14:09,,,,,,"Academia,Industry",,,,,,,"Academia,Industry",,,,,Hardware,,,,
DeViSE,Vision,Semantic embedding,Google,"Andrea Frome, G. Corrado, Jonathon Shlens, Samy Bengio, J. Dean, Marc'Aurelio Ranzato, Tomas Mikolov",2013-12-05,DeViSE: A Deep Visual-Semantic Embedding Model,https://www.semanticscholar.org/paper/DeViSE%3A-A-Deep-Visual-Semantic-Embedding-Model-Frome-Corrado/4aa4069693bee00d1b0759ca3df35e59284e9845,,"Highly cited,SOTA improvement",,,,,,,,5400000000.0,"""We trained a skip-gram text model on a corpus of 5.7 million documents (5.4 billion words) ""
Additionally, an image model component is trained on ImageNet (1.2M images)",,,,,Confident,"Modern visual recognition systems are often limited in their ability to scale to large numbers of object categories. This limitation is in part due to the increasing difficulty of acquiring sufficient training data in the form of labeled images as the number of object categories grows. One remedy is to leverage data from other sources - such as text data - both to train visual models and to constrain their predictions. In this paper we present a new deep visual-semantic embedding model trained to identify visual objects using both labeled image data as well as semantic information gleaned from unannotated text. We demonstrate that this model matches state-of-the-art performance on the 1000-class ImageNet object recognition challenge while making more semantically reasonable errors, and also show that the semantic information can be exploited to make predictions about tens of thousands of image labels not observed during training. Semantic knowledge improves such zero-shot predictions achieving hit rates of up to 18% across thousands of novel labels never seen by the visual model.",,,,United States of America,,,,,,2025-05-21 09:46,,,,,,Industry,,,,,,,Industry,,,,,,,,,
TensorReasoner,Language,Language modeling,Stanford University,"R Socher, D Chen, CD Manning, A Ng",2013-12-01,Reasoning With Neural Tensor Networks for Knowledge Base Completion,https://papers.nips.cc/paper/2013/hash/b337e84de8752b27eda3a12363109e80-Abstract.html,1923.0,Highly cited,,,,,,,,,,,,,,Unknown,,,,,United States of America,,,,,,2024-11-01 10:03,,,,,,Academia,,,,,,,Academia,,,,,,,,,
Visualizing CNNs,Vision,Image classification,New York University (NYU),"MD Zeiler, R Fergus",2013-11-12,Visualizing and Understanding Convolutional Networks,https://arxiv.org/abs/1311.2901,15232.0,Highly cited,,,,5.32e+17,"1 GPU * 12 days * 1.54 TFLOPS/GTX 580 * 0.33 utilization 
= 532 PF = 0.0062 pfs-days

Source: https://openai.com/blog/ai-and-compute

""We stopped training after 70 epochs,
which took around 12 days on a single GTX580 GPU""",,,,,,,NVIDIA GeForce GTX 580,,Confident,,,,,United States of America,,,,,,2025-06-17 10:33,,,,,,Academia,,,,,,,Academia,,,FP32,,"Hardware,Third-party estimation",,,,
RNTN,Language,Sentiment classification,Stanford University,"R. Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, A. Ng, Christopher Potts",2013-10-01,Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank,https://www.semanticscholar.org/paper/Recursive-Deep-Models-for-Semantic-Compositionality-Socher-Perelygin/687bac2d3320083eb4530bf18bb8f8f721477600,,Highly cited,,,,1.422e+16,"""The RNTN would usually achieve its best performance on the dev set after training for 3 - 5 hours.""
",,"""we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences""
Average FP32 FLOPS for 2013: 1.58E+12 (https://epoch.ai/blog/estimating-training-compute)
Assuming utilization of 0.5
Compute estimate: 0.5 * 5*60*60 * 1.58e12=1.422e16=14220000000000000
",155063.0,"""The sentences in the treebank were split into a train (8544), dev (1101) and test splits (2210)""
Training data: 215154*(8544/11855)=155063
",5.0,The RNTN would usually achieve its best performance on the dev set after training for 3 - 5 hours.,,,Likely,"Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classification from 80% up to 85.4%. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7%, an improvement of 9.7% over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases.",,,Unreleased,United States of America,,,,,,2024-12-10 10:52,,,,,,Academia,,,,,Unreleased,,Academia,,,,,,,,,
RCTM,Language,Translation,University of Oxford,"Nal Kalchbrenner, Phil Blunsom",2013-10-01,Recurrent Continuous Translation Models,https://www.semanticscholar.org/paper/Recurrent-Continuous-Translation-Models-Kalchbrenner-Blunsom/944a1cfd79dbfb6fef460360a0765ba790f4027a,,Highly cited,,,,9331200000000000.0,"""The training of an RCTM takes about 15 hours on 3 multicore CPUs""
Given the publication year, a rough estimate for the CPU performance is 16 FP32 per cycle, 4 cores, clock speed 4GHz, utilization of 0.3.
15*60*60*3*4*12*4000000000*0.3=9331200000000000=9.33e15",,"""The training set used for all the experiments comprises a bilingual corpus of 144953 pairs of sentences less than 80 words in length from the news
commentary section of the Eighth Workshop on Machine Translation (WMT) 2013 training data.""",4100000.0,"""The English sentences contain about 4.1M words""",15.0,The training of an RCTM takes about 15 hours on 3 multicore CPUs.,,,Likely,"We introduce a class of probabilistic continuous translation models called Recurrent Continuous Translation Models that are purely based on continuous representations for words, phrases and sentences and do not rely on alignments or phrasal translation units. The models have a generation and a conditioning aspect. The generation of the translation is modelled with a target Recurrent Language Model, whereas the conditioning on the source sentence is modelled with a Convolutional Sentence Model. Through various experiments, we show first that our models obtain a perplexity with respect to gold translations that is > 43% lower than that of stateof-the-art alignment-based translation models. Secondly, we show that they are remarkably sensitive to the word order, syntax, and meaning of the source sentence despite lacking alignments. Finally we show that they match a state-of-the-art system when rescoring n-best lists of translations.",,,,United Kingdom of Great Britain and Northern Ireland,,,,3.0,,2025-02-17 14:08,,,,,,Academia,,,,,,,Academia,,,,,Hardware,,,,
Mitosis,"Vision,Medicine",Object detection,IDSIA,"DanÂ C.Â CireÅŸan, AlessandroÂ Giusti, LucaÂ M.Â Gambardella, JÃ¼rgenÂ Schmidhuber",2013-09-22,Mitosis Detection in Breast Cancer Histology Images with Deep Neural Networks,https://link.springer.com/chapter/10.1007/978-3-642-40763-5_51,1478.0,Highly cited,ICPR 2012 mitosis detection competition winner,37230.0,Sum numbers of weights in Table 1.b,1.37e+17,"""Training each network requires one day of computation with an optimized GPU implementation""

Assuming 1.58E+12 FLOP/second on FP32 (from the table in the Estimating compute post), we get

3600*24*1.58E+12 = 1.37E+17 FLOP",,,1000000.0,"The dataset is built in two stages. First a classifier is trained on small sample, and used to curate a more representative larger dataset.

The final dataset has 1M instances

""We build the actual training set, composed by 1 million instances, which includes
all mitosis pixels (6.6% of the training instances). The remaining 95.4% is sampled
from non-mitosis pixels by assigning to each pixel p a weight D(p).""",24.0,"""Training each network requires one day of computation with an optimized GPU implementation""",,,Likely,,,,,Switzerland,,,,,,2025-06-17 10:12,,,,,,Academia,,,,,,,Academia,,,,,Hardware,,,,
RNN+weight noise+dynamic eval,Language,Language modeling,University of Toronto,Alex Graves,2013-08-04,Generating Sequences With Recurrent Neural Networks,https://arxiv.org/abs/1308.0850,4734.0,Highly cited,,54000000.0,"""the word-level network had 10,000 inputs and outputs and around
54M weights""",4210000000000000.0,6 FLOP / parameter / token * 54000000 parameters * 929000 tokens * 14 epochs = 4.213944e+15 FLOP,Penn TreeBank (PTB),,929000.0,14 epochs (Table 1),,,,,Confident,"This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.",14.0,RNN+weight noise+dynamic eval,Unreleased,Canada,,,,,,2025-04-21 12:10,,,,,,Academia,,,,,Unreleased,,Academia,,,,,Operation counting,,,,
Fisher Vector image classifier,Vision,Image classification,"Universidad Nacional de Cordoba,Inteligent Systems Lab Amsterdam,University of Amsterdam,LEAR Team,INRIA,Xerox Research Centre Europe (XRCE)","orge Sanchez, Florent Perronnin, Thomas Mensink, Jakob Verbeek",2013-06-12,Image Classification with the Fisher Vector: Theory and Practice,https://hal.inria.fr/hal-00830491v2/document,1707.0,Highly cited,,,,90842400000000.0,"They use a Intel Xeon E5-2470 Processor for 2 hours. This can do 12,617 MOps/Sec (average test results, assuming they achieved a similar utilization)
https://www.cpubenchmark.net/cpu.php?cpu=Intel+Xeon+E5-2470+%40+2.30GHz&id=2003

12617000000*2*60*60=90842400000000",ImageNet,,,,2.0,,,,Confident,,,,,"Argentina,Netherlands,Netherlands,France,France,France",,,,,,2025-06-17 10:02,,,,,,"Academia,Academia,Academia,Academia,Industry",,,,,,,"Academia,Academia,Academia,Academia,Industry",,,FP32,,Hardware,,,,
SemVec,Language,Language Structure Modeling,Microsoft Research,"T Mikolov, W Yih, G Zweig",2013-06-09,Linguistic Regularities in Continuous Space Word Representations,https://www.aclweb.org/anthology/N13-1090/,3625.0,Highly cited,,,,,,,,,,,,,,Unknown,,,,,"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational",,,,,,2024-09-15 16:33,,,,,,Industry,,,,,,,Industry,,,,,,,,,
Multilingual DNN,Speech,Speech recognition,Google,"G. Heigold, Vincent Vanhoucke, A. Senior, Patrick Nguyen, Marc'Aurelio Ranzato, M. Devin, J. Dean",2013-05-26,Multilingual acoustic models using distributed deep neural networks,https://www.semanticscholar.org/paper/Multilingual-acoustic-models-using-distributed-deep-Heigold-Vanhoucke/a41b826d23957d6ad4e9e794d20a583a9b567c5d,,"SOTA improvement,Training cost",,206899200.0,"""The input for the DNN is eleven contiguous frames of 40-dimensional log-filterbank features. The DNN consists of four hidden layers each with 2560 nodes""
Network structure: 3 multilingual shared layers, 1 language specific hidden layer + output layer (Figure 2)
Language specific layer output sizes: 1600, 3300, 2900, 5700, 3500, 5500, 6200, 4700, 5100, 4900, 3700 (Table 1)
Shared: 11*40*2560+2560*2560+2560*2560=14233600
Language heads: 11*2560*2560+2560*1600+2560*3300+2560*2900+2560*5700+2560*3500+2560*5500+2560*6200+2560*4700+2560*5100+2560*4900+2560*3700=192665600
Total: 14233600+192665600=206899200=2e8",,Could be estimated if we knew framerate of input filterbanks.,,,77580000.0,"Trained on 80+100+220+270+920+1140+1450+1460+1490+1490=8620h of speech data (Table 1)
Conversion to words using an estimate of 150 wpm: 8620*60*150=77580000 words
",672.0,"""increased training time of roughly four weeks""
4*7*24=672 hours of training",,Supervised,Confident,"Today's speech recognition technology is mature enough to be useful for many practical applications. In this context, it is of paramount importance to train accurate acoustic models for many languages within given resource constraints such as data, processing power, and time. Multilingual training has the potential to solve the data issue and close the performance gap between resource-rich and resource-scarce languages. Neural networks lend themselves naturally to parameter sharing across languages, and distributed implementations have made it feasible to train large networks. In this paper, we present experimental results for cross- and multi-lingual network training of eleven Romance languages on 10k hours of data in total. The average relative gains over the monolingual baselines are 4%/2% (data-scarce/data-rich languages) for cross- and 7%/2% for multi-lingual training. However, the additional gain from jointly training the languages on all data comes at an increased training time of roughly four weeks, compared to two weeks (monolingual) and one week (crosslingual).",,,,United States of America,,,,,,2025-05-28 16:20,,,,,,Industry,,,,,,,Industry,,,FP32,,,,,,
ReLU-Speech,Speech,Speech recognition,"Google,University of Toronto,New York University (NYU)","Matthew D. Zeiler, Marc'Aurelio Ranzato, R. Monga, Mark Z. Mao, K. Yang, Quoc V. Le, Patrick Nguyen, A. Senior, Vincent Vanhoucke, J. Dean, Geoffrey E. Hinton",2013-05-26,On rectified linear units for speech processing,https://www.semanticscholar.org/paper/On-rectified-linear-units-for-speech-processing-Zeiler-Ranzato/64da1980714cfc130632c5b92b9d98c2f6763de6,,"Training cost,SOTA improvement",,101706240.0,"""The overall input dimensionality is 1040,""
""All layers of our networks have 2560 hidden units ""
""used to generate 7969 context-dependent tied acoustic states""
Largest model: 12 hidden layers (Fig 4)
Parameters: 1040*2560+12*2560*2560+2560*7969=101706240
",1.2773376e+17,"""across 4 machines using up to 4 CPUs each""
CPU model not specified, I assumed a Sandy Bridge with 16 FLOP/cycle and 3.3GhZ based on the publication year (4*16*3300000000=211200000000 FLOP/s per machine)
Compute: 4*211200000000*168*60*60*0.3 = 1.53e17

Alternatively, the training set is ""several hundred hours of speech"", with inputs consisting of 26 frames, each frame is 10ms apart.
If we assume 400h of training data, 400h/10ms/26= 5,538,461 inputs
6 * 101706240 * 5,538,461 = 3.38e15 FLOPs per epoch. Number of epochs unstated.",,Several hundred hours of speech,,,168.0,"""The results we report are obtained by training for one week.""",,,Likely,"Deep neural networks have recently become the gold standard for acoustic modeling in speech recognition systems. The key computational unit of a deep network is a linear projection followed by a point-wise non-linearity, which is typically a logistic function. In this work, we show that we can improve generalization and make training of deep networks faster and simpler by substituting the logistic units with rectified linear units. These units are linear when their input is positive and zero otherwise. In a supervised setting, we can successfully train very deep nets from random initialization on a large vocabulary speech recognition task achieving lower word error rates than using a logistic network with the same topology. Similarly in an unsupervised setting, we show how we can learn sparse features that can be useful for discriminative tasks. All our experiments are executed in a distributed environment using several hundred machines and several hundred hours of speech data.",,,,"United States of America,Canada,United States of America",,,,,,2025-02-17 14:07,,,,,,"Industry,Academia,Academia",,,,,,,"Industry,Academia,Academia",,,,,Hardware,,,,
Selective Search,Vision,"Object detection,Image segmentation","University of Trento,University of Amsterdam","JRR Uijlings, KEA Van De Sande, T Gevers",2013-04-02,Selective search for object recognition,https://link.springer.com/article/10.1007/s11263-013-0620-5,5642.0,Highly cited,,,,,,,,,,,,,,Unknown,,,,,"Italy,Netherlands",,,,,,2025-05-01 10:42,,,,,,"Academia,Academia",,,,,,,"Academia,Academia",,,,,,,,,
Maxout Networks,Vision,Image classification,University of Montreal / UniversitÃ© de MontrÃ©al," Ian J. Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, Yoshua Bengio",2013-02-18,Maxout Networks ,https://arxiv.org/abs/1302.4389,2576.0,Highly cited,,,,,,,,,,,,,,Unknown,,,,,Canada,,,,,,2024-09-05 14:08,,,,,,Academia,,,,,,,Academia,,,,,,,,,
Textual Imager,Vision,Object recognition,Stanford University,"R Socher, M Ganjoo, H Sridhar, O Bastani",2013-01-16,Zero-Shot Learning Through Cross-Modal Transfer,https://arxiv.org/abs/1301.3666,1422.0,Highly cited,,,,,,,,,,,,,,Unknown,,,,,United States of America,,,,,,2024-11-01 10:03,,,,,,Academia,,,,,,,Academia,,,,,,,,,
DistBelief NNLM,Language,Semantic embedding,Google,"Tomas Mikolov, Kai Chen, G. Corrado, J. Dean",2013-01-16,Efficient Estimation of Word Representations in Vector Space,https://arxiv.org/abs/1301.3781,41000.0,"Highly cited,SOTA improvement",,,,2.612736e+18,"Trained for 14 days on 180 CPU cores (Table 6)
Roughly estimating the performance of CPUs in a HPC around 2013: 16 FP32 operations per cycle, 2.5GHz, 0.3 utilization
Time: 14*24*60*60=1209600s
FLOPs: 0.3*180*16*2500000000=2160000000000
Training compute: 1209600s * 2160000000000 = 2612736000000000000 = 2.61e18
https://www.wolframalpha.com/input?i=16+FLOP+*+2.5+GHz+*+180+*+14+days+*+0.3",,,6000000000.0,Largest system is trained on 6B words (Table 6),336.0,Trained for 14 days (Table 6),,,Likely,"We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.",,,,United States of America,,,,180.0,,2025-02-17 14:07,,,,,,Industry,,,,,,,Industry,,,,,Hardware,,,,
RNN (SGD+CLR),Audio,"Language modeling,Audio generation",University of Montreal / UniversitÃ© de MontrÃ©al,"Yoshua Bengio, Nicolas Boulanger-Lewandowski and Razvan Pascanu",2012-12-14,Advances in Optimizing Recurrent Networks,https://arxiv.org/abs/1212.0901,647.0,,,195600.0,"It uses 400 hidden units (selected via hyperparameter tuning)
The input size is 88 (corresponding to the 88 piano pitches)
It uses rectified linear units, so no activation function parameters
So the number of parameters would be:
Input to hidden weights: 88 * 400 = 35,200
Hidden to hidden weights: 400 * 400 = 160,000
Biases: 400
Total: ~195,600 parameters
Above estimate is by Claude 2. Should be checked manually.",,,,"""We evaluate our models on the four polyphonic music datasets of varying complexity used in [25]: classical piano music (Pianomidi.de), folk tunes with chords instantiated from ABC notation (Nottingham), orchestral music (MuseData) and the four-part chorales by J.S. Bach (JSB chorales)""",,,,,,,Speculative,"After a more than decade-long period of relatively little research activity in the area of recurrent neural networks, several new developments will be reviewed here that have allowed substantial progress both in understanding and in technical solutions towards more efficient training of recurrent networks. These advances have been motivated by and related to the optimization issues surrounding deep learning. Although recurrent networks are extremely powerful in what they can in principle represent in terms of modeling sequences, their training is plagued by two aspects of the same issue regarding the learning of long-term dependencies. Experiments reported here evaluate the use of clipping gradients, spanning longer time ranges
with leaky integration, advanced momentum techniques, using more powerful output probability models, and encouraging sparser gradients to help symmetry breaking and credit assignment. The experiments are performed on text and music data and show off the combined effects of these techniques in generally improving both training and test error",,,,Canada,,,,,,2024-09-15 16:44,,,,,,Academia,,,,,,,Academia,,,,,,,,,
DistBelief Speech,Speech,Speech recognition,Google,"J. Dean, G. Corrado, R. Monga, Kai Chen, M. Devin, Quoc V. Le, Mark Z. Mao, Marc'Aurelio Ranzato, A. Senior, P. Tucker, Ke Yang, A. Ng",2012-12-03,Large Scale Distributed Deep Networks,https://www.semanticscholar.org/paper/Large-Scale-Distributed-Deep-Networks-Dean-Corrado/3127190433230b3dc1abd0680bb58dced4bcd90e,,"Highly cited,SOTA improvement",,47185920.0,"""We used a deep network with five layers: four hidden layer with sigmoidal activations and 2560 nodes each, and a softmax output layer with 8192 nodes.""
""The network was fully-connected layer-to-layer, for a total of approximately 42 million model parameters.""
2560*2560*4+2560*8192=47185920",3.114e+17,"https://www.wolframalpha.com/input?i=6+FLOP+*+47185920+*+1.1+billion
Number of epochs unknown but most likely 1 and probably under 30.
We could narrow down the uncertainty further if we knew something about the hardware.",,,1100000000.0,"""We trained on a data set of 1.1 billion weakly labeled examples""",120.0,Figure 4,,,Speculative,"Recent work in unsupervised feature learning and deep learning has shown that being able to train large models can dramatically improve performance. In this paper, we consider the problem of training a deep network with billions of parameters using tens of thousands of CPU cores. We have developed a software framework called DistBelief that can utilize computing clusters with thousands of machines to train large models. Within this framework, we have developed two algorithms for large-scale distributed training: (i) Downpour SGD, an asynchronous stochastic gradient descent procedure supporting a large number of model replicas, and (ii) Sandblaster, a framework that supports a variety of distributed batch optimization procedures, including a distributed implementation of L-BFGS. Downpour SGD and Sandblaster L-BFGS both increase the scale and speed of deep network training. We have successfully used our system to train a deep network 30x larger than previously reported in the literature, and achieves state-of-the-art performance on ImageNet, a visual object recognition task with 16 million images and 21k categories. We show that these same techniques dramatically accelerate the training of a more modestly- sized deep network for a commercial speech recognition service. Although we focus on and report performance of these methods as applied to training large neural networks, the underlying algorithms are applicable to any gradient-based machine learning algorithm.",,,,United States of America,,,,,,2025-02-17 14:07,,,,,,Industry,,,,,,,Industry,,,,,Operation counting,,,,
DistBelief Vision,Vision,Image classification,Google,"J. Dean, G. Corrado, R. Monga, Kai Chen, M. Devin, Quoc V. Le, Mark Z. Mao, Marc'Aurelio Ranzato, A. Senior, P. Tucker, Ke Yang, A. Ng",2012-12-03,Large Scale Distributed Deep Networks,https://www.semanticscholar.org/paper/Large-Scale-Distributed-Deep-Networks-Dean-Corrado/3127190433230b3dc1abd0680bb58dced4bcd90e,,"SOTA improvement,Highly cited",,1700000000.0,"""we used Downpour SGD to train the 1.7 billion parameter image model""",,,ImageNet,,16000000.0,For visual object recognition we trained a larger neural network with locally-connected receptive fields on the ImageNet data set of 16 million images,,,,,Likely,"Recent work in unsupervised feature learning and deep learning has shown that being able to train large models can dramatically improve performance. In this paper, we consider the problem of training a deep network with billions of parameters using tens of thousands of CPU cores. We have developed a software framework called DistBelief that can utilize computing clusters with thousands of machines to train large models. Within this framework, we have developed two algorithms for large-scale distributed training: (i) Downpour SGD, an asynchronous stochastic gradient descent procedure supporting a large number of model replicas, and (ii) Sandblaster, a framework that supports a variety of distributed batch optimization procedures, including a distributed implementation of L-BFGS. Downpour SGD and Sandblaster L-BFGS both increase the scale and speed of deep network training. We have successfully used our system to train a deep network 30x larger than previously reported in the literature, and achieves state-of-the-art performance on ImageNet, a visual object recognition task with 16 million images and 21k categories. We show that these same techniques dramatically accelerate the training of a more modestly- sized deep network for a commercial speech recognition service. Although we focus on and report performance of these methods as applied to training large neural networks, the underlying algorithms are applicable to any gradient-based machine learning algorithm.",,,,United States of America,,,,,,2024-09-05 14:08,,,,,,Industry,,,,,,,Industry,,,,,,,,,
Bayesian automated hyperparameter tuning,Other,Mathematical simulation,"University of Toronto,University of Sherbrooke,Harvard University","J Snoek, H Larochelle, RP Adams",2012-12-02,Practical Bayesian optimization of machine learning algorithms,https://arxiv.org/abs/1206.2944,7308.0,Highly cited,,,,,,,,,,,,,,Unknown,,,,,"Canada,Canada,United States of America",,,,,,2024-11-01 10:03,,,,,,"Academia,Academia,Academia",,,,,,,"Academia,Academia,Academia",,,,,,,,,
AlexNet,Vision,Image classification,University of Toronto,"Alex Krizhevsky, Ilya Sutskever, Geoffrey Hinton",2012-09-30,ImageNet Classification with Deep Convolutional Neural Networks,https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html,113243.0,"Highly cited,Historical significance",,60000000.0,"""Our neural network architecture has 60 million parameters.""",4.7e+17,"1.2M images * 90 epochs * 0.75 GFLOP * (2 add-multiply) * (3 backward pass) 
= 470 PF = 0.0054 pfs-days

Source: https://openai.com/blog/ai-and-compute/

Hardware method:
2 GTX 580 3GB GPUs for ""between five and six days"". Assuming 5.5 days and 32-bit training:
1.581 TFLOPS * 5.5 days * 2 = 1.5e18 FLOP
Comparing to the operation counting method, this implies around 31% MFU.

Authors of ""AI and Memory Wall"" (https://github.com/amirgholami/ai_and_memory_wall) estimated model's training compute as 460 PFLOP = 4.6*10^17 FLOP",ImageNet,,1200000.0,"""ImageNet is a dataset of over 15 million labeled high-resolution images belonging to roughly 22,000 categories. The images were collected from the web and labeled by human labelers using Amazonâ€™s Mechanical Turk crowd-sourcing tool. Starting in 2010, as part of the Pascal Visual Object Challenge, an annual competition called the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) has been held. ILSVRC uses a subset of ImageNet with roughly 1000 images in each of 1000 categories. In all, there are roughly 1.2 million training images, 50,000 validation images, and 150,000 testing images.""",132.0,"""Our network takes between five and six days to train on two GTX 580 3GB GPUs.""",NVIDIA GeForce GTX 580,,Confident,"We trained a large, deep convolutional neural network to classify the 1.3 million high-resolution images in the LSVRC-2010 ImageNet training set into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 39.7\% and 18.9\% which is considerably better than the previous state-of-the-art results. The neural network, which has 60 million parameters and 500,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and two globally connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of convolutional nets. To reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective.",,,,Canada,,,,,0.313,2025-05-28 16:20,,,,,,Academia,,,,,,,Academia,,"Operation counting approach for compute estimate implies 470 PFLOP.
Their hardware, 2 GTX 580 GPUs, would perform 1500 PFLOPs if operating in 32-bit for 5.5 days.
This implies a model FLOP utilization rate of 31.3%.",FP32,,"Operation counting,Hardware,Third-party estimation",,,,
LSTM LM,Language,Language modeling,RWTH Aachen University,"M. Sundermeyer, R. SchlÃ¼ter, H. Ney",2012-09-09,LSTM Neural Networks for Language Modeling,https://www.semanticscholar.org/paper/LSTM-Neural-Networks-for-Language-Modeling-Sundermeyer-Schl%C3%BCter/f9a1b3850dfd837793743565a8af95973d395a4e,,Highly cited,,102720000.0,"Multiple models were trained, the largest on transcribed French podcast data.
""We trained an LSTM LM using 300 hidden nodes and 27 M running words of indomain training data""
""Corpus sizes in number of running words; the vocabulary size of the Treebank corpus is 10 K, for Quaero French it is 170 K""
Embedding and unembedding: 2*170000*300=102000000
LSTM: 4*600*300=720000
Total: 102000000+720000=102720000=1.03e8
(Assuming the embedding dimension is the same as the LSTM layer)",1.66e+16,"FLOP per input for LSTM layer is 4*2*(M+N)*M, for N inputs and M outputs.

Embedding FLOPs: 2 * 170000 * 300 = 102,000,000
LSTM FLOPs: 4 * 2 * (300 + 300) * 300 = 1,440,000
Unembedding FLOPs: 2 * 170000 * 300 = 102,000,000
Total: 205,440,000 FLOPs per word per forward pass
For 27M training input words and including backward passes: 27M * 3 * 205,440,000 = 1.66e16

However, it sounds like they're doing something with a secondary acoustic model, so this may be an underestimate.",,,27000000.0,"""We trained an LSTM LM using 300 hidden nodes and 27 M running words of indomain training data.""",,,,,Speculative,"Neural networks have become increasingly popular for the task of language modeling. Whereas feed-forward networks only exploit a fixed context length to predict the next word of a sequence, conceptually, standard recurrent neural networks can take into account all of the predecessor words. On the other hand, it is well known that recurrent networks are difficult to train and therefore are unlikely to show the full potential of recurrent models. These problems are addressed by a the Long Short-Term Memory neural network architecture. In this work, we analyze this type of network on an English and a large French language modeling task. Experiments show improvements of about 8 % relative in perplexity over standard recurrent neural network LMs. In addition, we gain considerable improvements in WER on top of a state-of-the-art speech recognition system.",,,,Germany,,,,,,2025-05-02 10:12,,,,,,Academia,,,,,,,Academia,,,,,Operation counting,,,,
Context-dependent RNN,Language,Language modeling,"Microsoft Research,Brno University of Technology","Tomas Mikolov, Geoffrey Zweig",2012-07-27,Context Dependent Recurrent Neural Network Language Model,https://www.microsoft.com/en-us/research/wp-content/uploads/2012/07/rnn_ctxt_TR.sav_.pdf,707.0,SOTA improvement,New SOTA perplexity on PTB,,,,,,,,,,,,,Unknown,"Recurrent neural network language models (RNNLMs) have recently demonstrated state-of-the-art performance across a variety of tasks. In this paper, we improve their performance by providing a contextual real-valued input vector in association with each word. This vector is used to convey contextual information about the sentence being modeled. By performing Latent Dirichlet Allocation using a block of preceding text, we achieve a topic-conditioned RNNLM. This approach has the key advantage of avoiding the data fragmentation associated with building multiple topic models on different data subsets. We report perplexity results on the Penn Treebank data, where we achieve a new state-of-the-art. We further apply the model to the Wall Street Journal speech recognition task, where we observe 
improvements in word-error-rate.",,,,"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Czechia",,,,,,2024-09-05 14:08,,,,,,"Industry,Academia",,,,,,,"Industry,Academia",,,,,,,,,
Unsupervised High-level Feature Learner,Vision,Image classification,Google,"Quoc V. Le, Marc'Aurelio Ranzato, Rajat Monga, Matthieu Devin, Kai Chen, Greg S. Corrado, Jeff Dean, Andrew Y. Ng",2012-07-12,Building High-level Features Using Large Scale Unsupervised Learning,https://arxiv.org/abs/1112.6209,2909.0,"Highly cited,SOTA improvement","""we trained our network to obtain 15.8% accuracy in recognizing 20,000 object categories from ImageNet, a leap of 70% relative improvement over the previous state-of-the-art""",1000000000.0,"""To answer this, we train a 9-layered locally connected sparse autoencoder with pooling and local contrast normalization on a large dataset of images (the model has 1 billion connections, the dataset has 10 million 200x200 pixel images downloaded from the Internet)""",6e+17,"Assuming 1 epoch, 10 million images and 1 billion parameters, 6*N*D = 6*10^17 FLOP",,10 million 200x200 images extracted from Youtube videos,10000000.0,10 million 200x200 images extracted from Youtube videos,72.0,"""We train this network using model parallelism and asynchronous SGD on a cluster with 1,000 machines (16,000 cores) for three days. """,,Unsupervised,Likely,"We consider the problem of building high-level, class-specific feature detectors from only unlabeled data. For example, is it possible to learn a face detector using only unlabeled images using unlabeled images? To answer this, we train a 9-layered locally connected sparse autoencoder with pooling and local contrast normalization on a large dataset of images (the model has 1 billion connections, the dataset has 10 million 200Ã—200 pixel images downloaded from the Internet). We train this network using model parallelism and asynchronous SGD on a cluster with 1,000 machines (16,000 cores) for three days. Contrary to what appears to be a widely-held intuition, our experimental results reveal that it is possible to train a face detector without having to label images as containing a face or not. Control experiments show that this feature detector is robust not only to translation but also to scaling and out-of-plane rotation. We also find that the same network is sensitive to other high-level concepts such as cat faces and human bodies. Starting with these learned features, we trained our network to obtain 15.8% accuracy in recognizing 20,000 object categories from ImageNet, a leap of 70% relative improvement over the previous state-of-the-art.",,,,United States of America,,,,,,2025-05-09 11:32,,,,,,Industry,,,,,,,Industry,,,,,Operation counting,,,,
Ngram corpus,Language,Language Structure Modeling,Google,"Yuri Lin, Jean-Baptiste Michel, Erez Lieberman Aiden, Jon Orwant, Will Brockman and Slav Petrov",2012-07-08,Syntactic Annotations for the Google Books NGram Corpus,https://aclanthology.org/P12-3029/,489.0,,,,,,,,,,,,,,,Unknown,,,,,United States of America,,,,,,2024-09-15 17:03,,,,,,Industry,,,,,,,Industry,,,,,,,,,
LBL,Language,Language modeling,University College London (UCL),"Andriy Mnih, Yee Whye Teh",2012-06-27,A Fast and Simple Algorithm for Training Neural Probabilistic Language Models,https://arxiv.org/abs/1206.6426,835.0,,,2000000.0,"Vocabulary size (V) = 10,000 words
Context size (c) = 2 words
Feature dimension (d) = 100
Plugging this into the parameter count:

Context word features: 10,000 x 100 = 1,000,000
Target word features: 10,000 x 100 = 1,000,000
Context weights: 2 x 100 x 100 = 20,000
Total parameters: 1,000,000 + 1,000,000 + 20,000 = 2,020,000
So for the LBL model configuration used in the experiments in this paper, with a 10K word vocabulary, 2-word context, and 100D feature vectors, the total number of parameters is approximately 2 million.",502000000000000.0,"6 FLOP / parameter / token * 2000000 parameters ['Likely' confidence] * 929000 tokens * 45 epochs [""Likely"" confidence] = 5.0166e+14 FLOP",Penn TreeBank (PTB),,929000.0,"The paper does not explicitly state the number of epochs used for training the LBL models. However, it provides some relevant information:

In Section 4, it mentions that the learning rates were adapted at the end of each epoch based on the validation set perplexity.

In Section 4.1, it states: ""NCE took about twice as many epochs as ML to converge.""

For maximum likelihood (ML) training, the training time is reported as 21 hours.

So we can deduce:

ML training took 21 hours and NCE took twice as many epochs as ML to converge.

The training was done using mini-batches of 1000 context/word pairs.

Each epoch must have involved passing through the entire 930K word training set.

So as a rough estimate, if we assume epoch time to be proportional to training time, and we know NCE took twice as many epochs:

ML took 21 hours, so approximately 20-25 epochs.
NCE took twice as many, so approximately 40-50 epochs.
Therefore, I would estimate the LBL models were trained for approximately 40-50 epochs. The paper does not provide the exact number.",,,,,Likely,"In spite of their superior performance, neural probabilistic language models (NPLMs) remain far less widely used than n-gram models due to their notoriously long training times, which are measured in weeks even for moderately-sized datasets. Training NPLMs is computationally expensive because they are explicitly normalized, which leads to having to consider all words in the vocabulary when computing the log-likelihood gradients.
We propose a fast and simple algorithm for training NPLMs based on noise-contrastive estimation, a newly introduced procedure for estimating unnormalized continuous distributions. We investigate the behaviour of the algorithm on the Penn Treebank corpus and show that it reduces the training times by more than an order of magnitude without affecting the quality of the resulting models. The algorithm is also more efficient and much more stable than importance sampling because it requires far fewer noise samples to perform well.
We demonstrate the scalability of the proposed approach by training several neural language models on a 47M-word corpus with a 80K-word vocabulary, obtaining state-of-the-art results on the Microsoft Research Sentence Completion Challenge dataset.",45.0,LBL,Unreleased,United Kingdom of Great Britain and Northern Ireland,,,,,,2025-04-21 12:16,,,,,,Academia,,,,,Unreleased,,Academia,,,,,Operation counting,,,,
Dropout (ImageNet),Vision,Image classification,University of Toronto,"GE Hinton, N Srivastava, A Krizhevsky",2012-06-03,Improving neural networks by preventing co-adaptation of feature detectors,https://arxiv.org/abs/1207.0580,7445.0,Highly cited,,,"""We achieved comparable performance of 48.6% error using a single neural network with five convolutional hidden layers interleaved with â€œmax-poolingâ€ layer followed by two globally
connected layers and a final 1000-way softmax layer""",2.731968e+17,"""a single NVIDIA GTX 580 GPU... Training on ImageNet takes
roughly four days with dropout and two days without.""
1.581 TFLOP/s * 4 day * 86400 s/day * 0.5 utilization",ImageNet,,1000000.0,"In 2010, a subset of 1000 classes
with roughly 1000 examples per class was the basis of an object recognition competition...",96.0,4 days with dropout; 2 days without dropout,NVIDIA GeForce GTX 580,,Confident,"Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different thinned networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.",,,Unreleased,Canada,,,,,,2025-06-17 09:55,,,,,,Academia,,,,,Unreleased,,Academia,,,FP32,,Hardware,,,,
Dropout (CIFAR),Vision,Character recognition,University of Toronto,"GE Hinton, N Srivastava, A Krizhevsky",2012-06-03,Improving neural networks by preventing co-adaptation of feature detectors,https://arxiv.org/abs/1207.0580,7445.0,Highly cited,,,,4268700000000000.0,"""a single NVIDIA GTX 580 GPU. Training on CIFAR-10 takes roughly 90 minutes"" p17
1.581 TFLOP/s * 90 min * 60 s/min * 0.5 utilization",CIFAR-10,,60000.0,,1.5,90 minutes,NVIDIA GeForce GTX 580,,Confident,"Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different thinned networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.",,,Unreleased,Canada,,,,,,2025-06-17 09:54,,,,,,Academia,,,,,Open (non-commercial),http://www.cs.toronto.edu/~nitish/dropout see model files,Academia,,,FP32,,Hardware,,,,
Dropout (MNIST),Vision,Character recognition,University of Toronto,"GE Hinton, N Srivastava, A Krizhevsky",2012-06-03,Improving neural networks by preventing co-adaptation of feature detectors,https://arxiv.org/abs/1207.0580,7445.0,Highly cited,,5594010.0,"We show results for 4 nets (784-800-800-10, 784-1200-1200-10, 784-2000-2000-10, 784-1200-1200-1200-10)

784*2000+2000*2000+10*2000+6010=5594010",6039370800000000.0,"Num mul-add / forward pass
2 FLOPs / mult-add
3 total mult-add / fp mult-add
3000 epochs
60000 training samples",MNIST,,60000.0,"The MNIST database contains 60,000 training images and 10,000 testing images (Wikipedia)",,,NVIDIA GeForce GTX 580,,Confident,"Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different thinned networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.",,,Unreleased,Canada,,,,,,2025-06-17 09:53,,,,,,Academia,,,,,Open (non-commercial),http://www.cs.toronto.edu/~nitish/dropout see model files,Academia,,,,,Operation counting,,,,
HOGWILD!,Other,Text classification,University of Wisconsin Madison,"F Niu, B Recht, C RÃ©, SJ Wright",2011-11-11,HOGWILD!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent,https://arxiv.org/abs/1106.5730,2222.0,Highly cited,,,,,,,,,,,,,,Unknown,,,,,United States of America,,,,,,2024-11-01 10:03,,,,,,Academia,,,,,,,Academia,,,,,,,,,
Adaptive Subgrad,Language,Text classification,"Technion - Israel Institute of Technology,Google,University of California (UC) Berkeley","J Duchi, E Hazan, Y Singer",2011-10-03,Adaptive Subgradient Methods for Online Learning and Stochastic Optimization,https://dl.acm.org/doi/10.5555/1953048.2021068,10055.0,Highly cited,,,,,,Reuters RCV1,"They experiement with their method on different models with different modalities, I entry only information about text classification experiment 

Text Classification (Reuters RCV1): They trained binary classifiers for each of the 4 major categories using ADAGRAD and compared them to other methods like RDA, FOBOS, Passive-Aggressive, and AROW.",,,,,,,Unknown,"We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms.",,,,"Israel,United States of America,United States of America",,,,,,2024-11-01 10:03,,,,,,"Academia,Industry,Academia",,,,,,,"Academia,Industry,Academia",,,,,,,,,
Recursive sentiment autoencoder,Language,Text classification,Stanford University,"R. Socher, J. Pennington, E. H. Huang, A. Y. Ng, and C. D. Manning",2011-07-01,Semi-supervised recursive autoencoders for predicting sentiment distributions,https://aclanthology.org/D11-1014/,1477.0,Highly cited,,,,,,,,,"They use several datasets for self-supervised and supervised learning
",,,,,Unknown,,,,,United States of America,,,,,,2024-09-15 17:11,,,,,,Academia,,,,,,,Academia,,,,,,,,,
Recursive Neural Network,"Vision,Language",Representation learning,Stanford University,"R. Socher, Cliff Chiung-Yu Lin, A. Ng, Christopher D. Manning",2011-06-28,Parsing natural scenes and natural language with recursive neural networks,https://www.semanticscholar.org/paper/Parsing-Natural-Scenes-and-Natural-Language-with-Socher-Lin/9c0ddf74f87d154db88d79c640578c1610451eec,,Highly cited,,,,,,WSJ,"We train all models on the Wall Street Journal section of the Penn Treebank using the standard training (2â€“21), development (22) and test (23) splits.",833333.0,"Full WSJ dataset: 1000000 words (https://catalog.ldc.upenn.edu/LDC99T42) 
Using 20 out of 24 splits for training: 1000000*(20/24)=833333.333",,,,,Confident,"Recursive structure is commonly found in the inputs of different modalities such as natural scene images or natural language sentences. Discovering this recursive structure helps us to not only identify the units that an image or sentence contains but also how they interact to form a whole. We introduce a max-margin structure prediction architecture based on recursive neural networks that can successfully recover such structure both in complex scene images as well as sentences. The same algorithm can be used both to provide a competitive syntactic parser for natural language sentences from the Penn Treebank and to outperform alternative approaches for semantic scene segmentation, annotation and classification. For segmentation and annotation our algorithm obtains a new level of state-of-the-art performance on the Stanford background dataset (78.1%). The features from the image parse tree outperform Gist descriptors for scene classification by 4%.",,,,United States of America,,,,,,2024-09-05 14:08,,,,,,Academia,,,,,,,Academia,,,,,,,,,
Cross-Lingual POS Tagger,Language,Part-of-speech tagging,"Carnegie Mellon University (CMU),Google Research","Dipanjan Das, Slav Petrov",2011-06-19,Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections,https://aclanthology.org/P11-1061/,316.0,SOTA improvement,"""Across eight European languages, our approach results in an average absolute improvement of 10.4% over a state-of-the-art baseline, and 16.7% over vanilla hidden Markov models induced with the Expectation Maximization algorithm.""",,,,,,,,,,,,,Unknown,,,,,"United States of America,Multinational,United States of America,Canada,Switzerland",,,,,,2024-09-05 14:08,,,,,,"Academia,Industry",,,,,,,"Academia,Industry",,,,,,,,,
Vector Space Model,Language,Semantic embedding,Stanford University,"Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, A. Ng, Christopher Potts",2011-06-19,Learning Word Vectors for Sentiment Analysis,https://www.semanticscholar.org/paper/Learning-Word-Vectors-for-Sentiment-Analysis-Maas-Daly/1c61f9ef06fe74505775a833ff849185757199e7,,"Highly cited,SOTA improvement","""We evaluate the model using small,
widely used sentiment and subjectivity corpora and find it out-performs several previously introduced methods for sentiment classification""",255000.0,"""We build a fixed dictionary of the 5,000 most frequent tokens""
""For all word vector models, we use 50-dimensional vectors""
Parameters: 5000*50 + 5000=255000
",,,IMDb,"""We induce word representations with our model using 25,000 movie reviews from IMDB.""",75000.0,"""We train a variant of our model which uses 50,000 unlabeled reviews in addition to the labeled set of 25,000 reviews""",,,,,Confident,"Unsupervised vector-based approaches to semantics can model rich lexical meanings, but they largely fail to capture sentiment information that is central to many word meanings and important for a wide range of NLP tasks. We present a model that uses a mix of unsupervised and supervised techniques to learn word vectors capturing semantic term--document information as well as rich sentiment content. The proposed model can leverage both continuous and multi-dimensional sentiment information as well as non-sentiment annotations. We instantiate the model to utilize the document-level sentiment polarity annotations present in many online documents (e.g. star ratings). We evaluate the model using small, widely used sentiment and subjectivity corpora and find it out-performs several previously introduced methods for sentiment classification. We also introduce a large dataset of movie reviews to serve as a more robust benchmark for work in this area.",,,,United States of America,,,,,,2024-09-05 14:08,,,,,,Academia,,,,,,,Academia,,,,,,,,,
Deep Autoencoders,Vision,Image representation,University of Toronto,"A. Krizhevsky, Geoffrey E. Hinton",2011-04-29,Using very deep autoencoders for content-based image retrieval,https://www.semanticscholar.org/paper/Using-very-deep-autoencoders-for-content-based-Krizhevsky-Hinton/88080d28536f36588740737f3b7a1f6c1a409654,,Historical significance,,139808256.0,"2*(3072*8192+8192*4096+4096*2048+2048*1024+1024*512+512*256+256*128+128*64+64*28)=139808256
""n each autoencoder, the hidden layers halve in size until they reach the desired size, except that we use 28 instead of 32""",3.672864e+16,"48*60*60*708500000000*0.3=36728640000000000=3.7e16
GTX 285 with 708.5 GFLOP/s",,"""We tested our models on two subsets of the 80 million tiny images datase""",1600000.0,"""We train on 1.6 million 32 Ã— 32 color images""",48.0,"""The entire training procedure for each autoencoder took about 2 days on an Nvidia GTX 285 GPU.""",NVIDIA GeForce GTX 285,,Confident,"We show how to learn many layers of features on color images and we use these features to initialize deep autoencoders. We then use the autoencoders to map images to short binary codes. Using semantic hashing [6], 28-bit codes can be used to retrieve images that are similar to a query image in a time that is independent of the size of the database. This extremely fast retrieval makes it possible to search using multiple di erent transformations of the query image. 256-bit binary codes allow much more accurate matching and can be used to prune the set of images found using the 28-bit codes.",85.0,,,Canada,,,,1.0,,2025-05-01 10:42,,,,,,Academia,,,,,,,Academia,,,,246.22598104391827,Hardware,,,,
Deep rectifier networks,Vision,Image classification,University of Montreal / UniversitÃ© de MontrÃ©al,"Xavier Glorot, Antoine Bordes, Yoshua Bengio",2011-04-13,Deep sparse rectifier neural networks,https://proceedings.mlr.press/v15/glorot11a.html,7887.0,Highly cited,,,,,,"CIFAR-10,MNIST,NISTP,NORB",,,,,,,,Unknown,"While logistic sigmoid neurons are more biologically plausible than hyperbolic tangent neurons, the latter work better for training multi-layer neural networks. This paper shows that rectifying neurons are an even better model of biological neurons and yield equal or better performance than hyperbolic tangent networks in spite of the hard non-linearity and non-differentiability at zero, creating sparse representations with true zeros which seem remarkably suitable for naturally sparse data. Even though they can take advantage of semi-supervised setups with extra-unlabeled data, deep rectifier networks can reach their best performance without requiring any unsupervised pre-training on purely supervised tasks with large labeled datasets. Hence, these results can be seen as a new milestone in the attempts at understanding the difficulty in training deep but purely supervised neural networks, and closing the performance gap between neural networks learnt with and without unsupervised pre-training.",,,,Canada,,,,,,2024-11-01 10:03,,,,,,Academia,,,,,,,Academia,,,,,,,,,
Optimized Single-layer Net,Vision,Image classification,"University of Michigan,Stanford University","A Coates, A Ng, H Lee",2011-04-11,An analysis of single-layer networks in unsupervised feature learning,http://proceedings.mlr.press/v15/coates11a.html,3716.0,Highly cited,,,,,,,,,,,,,,Unknown,,,,,"United States of America,United States of America",,,,,,2024-11-01 10:03,,,,,,"Academia,Academia",,,,,,,"Academia,Academia",,,,,,,,,
RNN LM,Language,Language modeling,Johns Hopkins University,"Tomas Mikolov, M. KarafiÃ¡t, L. Burget, J. ÄŒernockÃ½, S. Khudanpur",2010-09-26,Recurrent neural network based language model,https://www.semanticscholar.org/paper/Recurrent-neural-network-based-language-model-Mikolov-Karafi%C3%A1t/9819b600a828a57e1cde047bbe710d3446b30da5,5780.0,Highly cited,,70265000.0,"This database entry refers to the 3xRNN rows in Table 2 (static and dynamic likely use the same model ensemble, but allow the model weights to update once when testing the dynamic version).

I assume the 3xRNN represents interpolation between the three largest models shown explicitly (RNN 250/5, RNN 250/2, and RNN 400/10). This seems likely, since smaller models do considerably worse on their own.

In the following colab notebook, I estimate vocabulary sizes for the NYT Gigaword data at around 54.4k, 41.4k, and 27.6k for merge thresholds of 2, 5, and 10, respectively: https://colab.research.google.com/drive/1K5qH0EqXtFwTLESNtp4oelCM28GpGXt6#scrollTo=tedUkbgklNJ3

So the total number of parameters in each constituent model is:
- RNN 250/2: (250 + 54.4k) * 250 + (250 * 54.4k) = 27,262,500
- RNN 250/5: (250 + 41.4k) * 250 + (250 * 41.4k) = 20,762,500
- RNN 400/10: (400 + 27.6k) * 400 + (400 * 27.6k) = 22,240,000

In total: 70,265,000 parameters",5.396e+16,"""Convergence is usually achieved after 10-20 epochs.""
Training was done over a 6.4M subset of the NYT section of English Gigaword.

6 * 70,265,000 * 20 * 6.4M = 5.396e16",WSJ,The training corpus consists of 37M words from NYT section of English Gigaword,6400000.0,"As it is very time consuming to train RNN LM on large data, we have used only up to 6.4M words for training RNN models.",504.0,"""it takes several weeks to train the most complex models.""
Rough guess, 3 weeks = 504 hours

Assuming these models trained at the same time on different machines.",,,Speculative,"A new recurrent neural network based language model (RNN LM) with applications to speech recognition is presented. Results indicate that it is possible to obtain around 50% reduction of perplexity by using mixture of several RNN LMs, compared to a state of the art backoff language model. Speech recognition experiments show around 18% reduction of word error rate on the Wall Street Journal task when comparing models trained on the same amount of data, and around 5% on the much harder NIST RT05 task, even when the backoff model is trained on much more data than the RNN LM. We provide ample empirical evidence to suggest that connectionist language models are superior to standard n-gram techniques, except their high computational (training) complexity. Index Terms: language modeling, recurrent neural networks, speech recognition",1.0,,,United States of America,,,,,,2025-02-18 11:01,,,,,,Academia,,,,,,,Academia,,,,,Operation counting,,,,
Fisher-Boost,Vision,Image classification,Xerox Research Centre Europe (XRCE),Florent PerronninJorge SÃ¡nchezThomas Mensink,2010-09-05,Improving the Fisher Kernel for Large-Scale Image Classification,https://link.springer.com/chapter/10.1007/978-3-642-15561-1_11,3062.0,Highly cited,,,,,,,,,,21.5,"""Extracting and projecting the SIFT features for the 350K training images takes approx. 15h (150ms / image), learning the GMM on a random subset of 1M descriptors approx. 30 min, computing the Fisher vectors. Improving the Fisher Kernel for Large-Scale Image Classification 155
approx. 4h (40ms / image) and learning the 18 classifiers approx. 2h (7 min / class). ""
15 + 0.5 + 4 + 2 = 21.5 hours",,,Unknown,,,,,France,,,,,,2024-09-05 14:08,,,,,,Industry,,,,,,,Industry,,,,,,,,,
ReLU (LFW),Vision,Face recognition,University of Toronto,"Nair, V., Hinton, G. E.",2010-06-15,Rectified linear units improve restricted boltzmann machines,https://dl.acm.org/doi/10.5555/3104322.3104425,16786.0,Highly cited,,,,,,,,,,,,,,Unknown,,,,,Canada,,,,,,2024-11-01 10:03,,,,,,Academia,,,,,,,Academia,,,,,,,,,
Mid-level Features,Vision,Object recognition,"INRIA,Ecole Normale SupÃ¨rieure,New York University (NYU)","YL Boureau, F Bach, Y LeCun, J Ponce",2010-06-13,Learning mid-level features for recognition,https://ieeexplore.ieee.org/document/5539963,1314.0,Highly cited,,,This is extracting low-level SIFT features then max-pooling them and using in a linear SVM. The training compute could be estimated loosely for the SVM part.,,,,,,,,,,,Unknown,,,,,"France,France,United States of America",,,,,,2024-09-05 14:08,,,,,,"Academia,Academia,Academia",,,,,,,"Academia,Academia,Academia",,,,,,,,,
Deconvolutional Network,Vision,Image classification,New York University (NYU),"Matthew D. Zeiler, Dilip Krishnan, Graham W. Taylor and Rob Fergus",2010-06-13,Deconvolutional Networks,https://ieeexplore.ieee.org/document/5539957,1542.0,Highly cited,,,,,,,,,,,,,,Unknown,,,,,United States of America,,,,,,2024-11-01 10:03,,,,,,Academia,,,,,,,Academia,,,,,,,,,
Feedforward NN,Vision,Digit recognition,University of Montreal / UniversitÃ© de MontrÃ©al,"X Glorot, Y Bengio",2010-05-13,Understanding the difficulty of training deep feedforward neural networks,https://proceedings.mlr.press/v9/glorot10a.html,16706.0,Highly cited,,7082000.0,"pg250 of the paper, section 2.3: 
""We optimized feedforward neural networks with one to
five hidden layers, with one thousand hidden units per
layer""

Input is a flattened 32x32 image, which corresponds to an input vector of length 3072

Output is a number from 0-9, so 10 neurons

No. of params: 3072*1000 + 4*1000*1000 + 1000*10 = 7,082,000
",350000000000000.0,"Roughly two times the number of parameters for ops per forward pass. 

So 2*7082000 params*3.5*140 epochs * 50k training images = 3.5e14",MNIST,,,,,,,,Confident,,,,,Canada,,,,,,2025-06-17 09:02,,,,,,Academia,,,,,,,Academia,,,,,Operation counting,,,,
6-layer MLP (MNIST),Vision,Character recognition,"IDSIA,University of Lugano,SUPSI","Dan Claudiu Ciresan, Ueli Meier, Luca Maria Gambardella, Juergen Schmidhuber",2010-03-01,Deep Big Simple Neural Nets Excel on Handwritten Digit Recognition,https://arxiv.org/abs/1003.0358,1264.0,Highly cited,,12110000.0,Table 1,130788000000000.0,"""Networks with up to 12 million weights can successfully be trained by plain gradient descent to achieve test errors below 1% after 20-30 epochs in less than 2 hours of training.""

60k images in each MNIST epoch.

Architecture-based estimate: 6 * 12.11M * 60k * 30 = 1.31e14

We can also get a rough hardware estimate. The authors use single precision, GTX280 gets 6.221e11 FLOPs in single precision. Training 30 epochs takes less than 2 hours, but on each epoch the training set is augmented in online fashion, which takes 93 seconds.
(2*3600 - 93*30) * 6.221e11 * 0.3 = 8.23e14

The architecture approach seems less uncertain.",MNIST,,60000.0,"""MNIST consists of two datasets, one for training (60,000 images) and one for testing (10,000 images). Many studies divide the training set into two sets consisting of 50,000 images for training and 10,000 for validation. Our network is trained on slightly deformed images, continually generated in on-line fashion; hence we may use the whole un-deformed training set for validation, without wasting training images""",2.0,"""less than 2 hours of
training""","NVIDIA GeForce GTX 280,Intel Core 2 Quad Q9450",,Likely,,,,,"Switzerland,Switzerland,Switzerland",,,,,,2025-05-28 16:20,,,,,,"Academia,Academia,Academia",,,,,,,"Academia,Academia,Academia",,,FP32,,Operation counting,,,,
Stacked Denoising Autoencoders,Other,Image classification,"University of Montreal / UniversitÃ© de MontrÃ©al,University of Toronto","P Vincent, H Larochelle, I Lajoie, Y Bengio",2010-01-03,Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion,https://www.jmlr.org/papers/v11/vincent10a.html,6908.0,Highly cited,,,,,,,,,,,,,,Unknown,,,,,"Canada,Canada",,,,,,2024-11-01 10:03,,,,,,"Academia,Academia",,,,,,,"Academia,Academia",,,,,,,,,
Super-vector coding,Vision,Image classification,"University of Illinois Urbana-Champaign (UIUC),NEC Laboratories,Rutgers University","Xi Zhou, Kai Yu, Tong Zhang, and Thomas S. Huang",2010-01-01,Image Classification using Super-Vector Coding of Local Image Descriptors,http://tongzhang-ml.org/papers/eccv10_supervect.pdf,696.0,SOTA improvement,"""Our experiments demonstrate that the proposed classification method achieves state-of-the-art accuracy on the well-known PASCAL benchmarks.""",1025.0,"Somewhat low confidence, but it seems like the number of learnable parameters is the size of the codebook, plus parameters in the SVM used for classification. Since it is a linear SVM, there will be one parameter per input feature, plus a single bias term.

So in total, 512 learnable codebook values, plus 513 SVM parameters = 1025 parameters",,,"PASCAL VOC 2007,PASCAL VOC 2009","""PASCAL VOC 2007 consists of 9,963 images which are divided into
three subsets: training data (2501 images), validation data (2510 images), and test data (4952 images). PASCAL VOC 2009 consists of 14,743 images and correspondingly are divided into three subsets: training data(3473 images), validation data(3581 images), and testing data (7689 images).""",1000000.0,"""PASCAL VOC 2007 consists of 9,963 images which are divided intothree subsets: training data (2501 images), validation data (2510 images), and
test data (4952 images). PASCAL VOC 2009 consists of 14,743 images and correspondingly are divided into three subsets: training data(3473 images), validation data(3581 images), and testing data (7689 images).""

PASCAL VOC 2009 is the larger experiment; images used in training is 3473 + 3581 = 7,054

For each image, the inputs for the codebook training are generated as follows: ""128-dimensional SIFT vectors are extracted over a grid with spacing of 4 pixels on three patch scales (16x16,25x25 and 31x31). The dimension of descriptors is reduced to 80 by applying principal component analysis (PCA). The codebooks C are trained on one million randomly sampled descriptors""

Loss function for learning the SV coding seems to look at the L2 loss on SIFT vectors, so there should be one gradient per descriptor, i.e. 1M inputs.",,,,,Speculative,,,,,"United States of America,United States of America,United States of America",,,,,,2024-10-08 13:37,,,,,,"Academia,Industry,Academia",,,,,,,"Academia,Industry,Academia",,,,,,,,,
3D city reconstruction,3D modeling,3D reconstruction,"University of Washington,Microsoft Research,Cornell University","Sameer Agarwal, Noah Snavely, Ian Simon, Steven M. Seitz and Richard Szeliski",2009-09-29,Building Rome in a Day,https://grail.cs.washington.edu/rome/,2234.0,Highly cited,,,,,,,,,,,,,,Unknown,,,,,"United States of America,United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,United States of America",,,,,,2024-11-01 10:03,,,,,,"Academia,Industry,Academia",,,,,,,"Academia,Industry,Academia",,,,,,,,,
ConvNet Processor,Vision,Face detection,Courant Institute of Mathematical Sciences,"ClÃ©ment Farabet, Cyril Poulet, Jefferson Y Han, Yann LeCun",2009-08-31,CNP: An FPGA-based processor for convolutional networks,https://ieeexplore.ieee.org/abstract/document/5272559/,,,,14423.0,"C1: 6x7x7
C3: 61x7x7
C5: 305x6x6
F6: 80x2
Total = 14423",306000000000000.0,"(2*340 million) * 3 * 30000 * 5
""340 million connection network""
operations per fwd pass ~= 2*340 million
""With a fixed detection threshold, the system reaches a roughly 3% equal error rate on this dataset after only 5 training epochs through the training set.""
",,"""The network was trained on a dataset of faces and non-faces according to the method described in [2]. The dataset contained 45,000 images from various sources, of which 30,000 were used for training, and 15,000 for testing. Each set contains 50% faces, and 50% random images (non faces). The face examples include a wide variety of angles, and slight variations of size and position within the window to improve the robustness of the detector.""",30000.0,"""The dataset contained 45,000 images from various sources, of which 30,000
were used for training, and 15,000 for testing""",,,,,Likely,"Convolutional networks (ConvNets) are biologically inspired hierarchical architectures that can be trained to perform a variety of detection, recognition and segmentation tasks. ConvNets have a feed-forward architecture consisting of multiple linear convolution filters interspersed with pointwise non-linear squashing functions. This paper presents an efficient implementation of ConvNets on a low-end DSP-oriented field programmable gate array (FPGA). The implementation exploits the inherent parallelism of ConvNets and takes full advantage of multiple hardware multiply accumulate units on the FPGA. The entire system uses a single FPGA with an external memory module, and no extra parts. A network compiler software was implemented, which takes a description of a trained ConvNet and compiles it into a sequence of instructions for the ConvNet Processor (CNP). A ConvNet face detection system was implemented and tested. Face detection on a 512 times 384 frame takes 100 ms (10 frames per second), which corresponds to an average performance of 3.4 times 10 9 connections per second for this 340 million connection network. The design can be used for low-power, lightweight embedded vision systems for micro-UAVs and other small robots.",5.0,,Unreleased,United States of America,,,,,,2025-05-01 10:42,,,,,,Academia,,,,,,,Academia,,,,,Operation counting,,,,
Pragmatic Theory solution (Netflix 2009),Recommendation,Movie ratings,Pragmatic Theory Inc.,"M Piotte, M Chabbert",2009-08-01,The Pragmatic Theory solution to the Netflix Grand Prize,https://www.asc.ohio-state.edu/statistics/statgen/joul_aut2009/PragmaticTheory.pdf,111.0,SOTA improvement,Netflix grand prize winner (along with two other teams),,"This is an ensemble of many smaller models. Ideally, the number of parameters of all the sub-models should be added up and recorded here.",,"This is an ensemble of many smaller models. Ideally, the training compute of all the sub-models should be added up and recorded here.",Netflix Prize,,100480507.0,"""Netflix provided a training data set of 100,480,507 ratings that 480,189 users gave to 17,770 movies.""",,,,,Confident,,,,,Canada,,,,,,2024-09-05 14:08,,,,,,Industry,,,,,,,Industry,,,,,,,,,
GPU DBNs,Other,Miscellaneous image analysis,Stanford University,"R Raina, A Madhavan, AY Ng",2009-06-15,Large-scale Deep Unsupervised Learning using Graphics Processors,https://dl.acm.org/doi/abs/10.1145/1553374.1553486?utm_campaign=The+Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-95Z7-X4Dl-RJK6gYKvjyDIrYaGhBeqWoc0ldqyPEKni0Ip6UE7452hr-ygY52z00LBpYgM,1032.0,Highly cited,,100000000.0,"""For example, we are able to reduce the time required to learn a four-layer DBN with 100 million free parameters from several weeks to around a single day.""",1000000000000000.0,"https://www.getguesstimate.com/models/19602

6435 GPU seconds for 1M examples
Single GTX 280 with 622.1 GFLOPS
All results are reported for 1M examples, unclear if they ran larger training experiments.",,,1000000.0,"Table 2 shows the running time for processing 1 million
examples for RBMs of varying size",,,,,Confident,,,,,United States of America,,,,,,2025-06-16 12:22,,,GPU DBNs,,,Academia,,,,,,,Academia,,,FP32,,Hardware,,,,
Conv-DBN,Vision,Image classification,Stanford University,"H Lee, R Grosse, R Ranganath, AY Ng",2009-06-14,Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations,https://dl.acm.org/doi/10.1145/1553374.1553453,2964.0,Highly cited,,,,,,,,,,,,,,Unknown,,,,,United States of America,,,,,,2024-09-15 22:50,,,,,,Academia,,,,,,,Academia,,,,,,,,,
Deep Boltzmann Machines,Other,Image classification,University of Toronto,"Ruslan Salakhutdinov, Geoffrey Hinton",2009-04-16,Deep Boltzmann Machines,https://www.sciencedirect.com/topics/computer-science/deep-boltzmann-machine,2666.0,Highly cited,,,,,,,,,,,,,,Unknown,,,,,Canada,,,,,,2024-09-16 00:34,,,,,,Academia,,,,,,,Academia,,,,,,,,,
RBM Image Classifier,Vision,Image classification,University of Toronto,Alex Krizhevsky,2009-04-08,Learning Multiple Layers of Features from Tiny Images,https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf,32057.0,Highly cited,,80000000.0,"Best performing model (see Figure 3.1) had 10,000 hidden units in one hidden layer and 8000 visible units",,,CIFAR-10,"This paper is the origin of CIFAR-10. However, CIFAR-10 is a labeled subset of all the training data used

""The tiny images dataset on which we based all of our experiments was collected by colleagues at MIT and NYU over the span of six months; it is described in detail in [14]. They assembled it by searching the web for images of every non-abstract English noun in the lexical database WordNet[15, 8]. They used several search engines, including Google, Flickr, and Altavista and kept roughly the rst 3000 results for each search term. After collecting all the images for a particular search term, they removed perfect duplicates and images in which an excessively large portion of the pixels were white, as they tended to be synthetic gures rather than natural images. The search term used to nd an image provides it with a rough label, although it is extremely unreliable due to the nature of online image search technology.
In total, the dataset contains 80 million colour images downscaled to 32 Ã— 32 and spread out across 79000 search terms. Most of our experiments with unsupervised learning were performed on a subset of about 2 million images.""

""We paid students to label a subset of the tiny images dataset... We call this the CIFAR-10 dataset, after the Canadian Institute for Advanced Research, which funded the project""",2000000.0,,,,,,Likely,"Groups at MIT and NYU have collected a dataset of millions of tiny colour images from the web. It
is, in principle, an excellent dataset for unsupervised training of deep generative models, but previous
researchers who have tried this have found it dicult to learn a good set of lters from the images.
We show how to train a multi-layer generative model that learns to extract meaningful features which
resemble those found in the human visual cortex. Using a novel parallelization algorithm to distribute
the work among multiple machines connected on a network, we show how training such a model can be
done in reasonable time.
A second problematic aspect of the tiny images dataset is that there are no reliable class labels
which makes it hard to use for object recognition experiments. We created two sets of reliable labels.
The CIFAR-10 set has 6000 examples of each of 10 classes and the CIFAR-100 set has 600 examples of
each of 100 non-overlapping classes. Using these labels, we show that object recognition is signicantly
improved by pre-training a layer of features on a large set of unlabeled tiny images.",,,,Canada,,,,,,2025-05-28 16:20,,,,,,Academia,,,,,,,Academia,,,FP32,,,,,,
HLBL,Language,Language modeling,University of Toronto,"A. Mnih, Geoffrey E. Hinton",2008-12-08,A Scalable Hierarchical Distributed Language Model,https://www.semanticscholar.org/paper/A-Scalable-Hierarchical-Distributed-Language-Model-Mnih-Hinton/a9fc84f8abe740cdc7ee82e69444d1d00dbe0ceb,,Highly cited,,1846400.0,"""Except for where stated otherwise, the models used for the experiments used 100 dimensional feature vectors and a context size of 5.""
""The vocabulary size for this dataset is 17964.""
Embedding: 17964 * 100 = 1796400
Context matrices: 5 * 100 * 100 = 50000
Unembedding: 0 (tied embedding â€œwhile the matrix of weights from the hidden layer to the output layer is simply the feature vector matrix Râ€)
Total: 1796400 + 50000 = 1846400
",,"6ND: 6 * 1846400 * 14000000 = 155,097,600,000,000 FLOP per epoch.
Not stated how many epochs for training.",,"""APNews dataset containing the Associated Press news stories from 1995 and 1996.""",14000000.0,"""The dataset consists of a 14 million word training set""",,HLBL with largest tree (T7) takes 32 minutes per epoch. Unstated how many epochs they trained for.,,,Confident,"Neural probabilistic language models (NPLMs) have been shown to be competitive with and occasionally superior to the widely-used n-gram language models. The main drawback of NPLMs is their extremely long training and testing times. Morin and Bengio have proposed a hierarchical language model built around a binary tree of words, which was two orders of magnitude faster than the non-hierarchical model it was based on. However, it performed considerably worse than its non-hierarchical counterpart in spite of using a word tree created using expert knowledge. We introduce a fast hierarchical language model along with a simple feature-based algorithm for automatic construction of word trees from the data. We then show that the resulting models can outperform non-hierarchical neural models as well as the best n-gram models.",,,,Canada,,,,,,2024-09-23 18:56,,,,,,Academia,,,,,,,Academia,,,,,,,,,
ADAPTIVE NLPM,Language,Language modeling,University of Toronto,"Andriy Mnih, Geoffrey Hinton",2008-12-08,A Scalable Hierarchical Distributed Language Model,https://papers.nips.cc/paper_files/paper/2008/file/1e056d2b0ebd5c878c550da6ac5d3724-Paper.pdf,1297.0,Highly cited,,12198000.0,"None given but it does say "" Since each non-leaf node in a tree has its own feature vector, the number of free parameters associated with the tree is linear in this quantity"", and the largest model (T7: ADAPATIVE(0.4) x 4) has 121980 of them. The feature vectors are 100-dimensional. I've done the dubious thing of multiplying the two to give an estimate.",,,,,14000000.0,"""The dataset consists of a 14 million word training set""",,"32 minutes per epoch for the largest model, but unfortunately no epoch count given.

""Models were trained using the learning rate of 10^âˆ’3 until the perplexity on the validation set started to increase. Then the learning rate was reduced to 3 Ã— 10^âˆ’5 and training was resumed until the validation perplexity started increasing again.""",,Unsupervised,Confident,"Neural probabilistic language models (NPLMs) have been shown to be competitive with and occasionally superior to the widely-used n-gram language models. The main drawback of NPLMs is their extremely long training and testing times. Morin and Bengio have proposed a hierarchical language model built around a binary tree of words, which was two orders of magnitude faster than the nonhierarchical model it was based on. However, it performed considerably worse than its non-hierarchical counterpart in spite of using a word tree created using expert knowledge. We introduce a fast hierarchical language model along with a simple feature-based algorithm for automatic construction of word trees from the data. We then show that the resulting models can outperform non-hierarchical neural models as well as the best n-gram models.",,,Unreleased,Canada,,,,,,2025-05-09 11:32,,,,,,Academia,,,,,,,Academia,,,,,,,,,
Sparse digit recognition SVM,Vision,Image classification,University of Lubeck,"Kai Labusch, Erhadt Barth, Thomas Martinetz",2008-11-19,Simple method for high-performance digit recognition based on sparse coding,https://pubmed.ncbi.nlm.nih.gov/19000969/,124.0,SOTA improvement,"""Finally, we train a support vector machine (SVM) on the resulting feature vectors and obtain state-of-the-art classification performance in the digit recognition task defined by the MNIST benchmark""",,,,,,,,,,,,,Unknown,"In this brief paper, we propose a method of feature extraction for digit recognition that is inspired by vision research: a sparse-coding strategy and a local maximum operation. We show that our method, despite its simplicity, yields state-of-the-art classification results on a highly competitive digit-recognition benchmark. We first employ the unsupervised Sparsenet algorithm to learn a basis for representing patches of handwritten digit images. We then use this basis to extract local coefficients. In a second step, we apply a local maximum operation to implement local shift invariance. Finally, we train a support vector machine (SVM) on the resulting feature vectors and obtain state-of-the-art classification performance in the digit recognition task defined by the MNIST benchmark. We compare the different classification performances obtained with sparse coding, Gabor wavelets, and principal component analysis (PCA). We conclude that the learning of a sparse representation of local image patches combined with a local maximum operation for feature extraction can significantly improve recognition performance.",,,,Germany,,,,,,2024-11-10 20:09,,,,,,Academia,,,,,,,Academia,,,,,,,,,
Boss (DARPA Urban Challenge),Driving,Self-driving car,Carnegie Mellon University (CMU),"Chris Urmson, Joshua Anhalt, Drew Bagnell,Christopher Baker, Robert Bittner,M. N. Clark, John Dolan, Dave Duggins,Tugrul Galatali, Chris Geyer,Michele Gittleman, Sam Harbaugh,Martial Hebert, Thomas M. Howard,Sascha Kolski, Alonzo Kelly,Maxim Likhachev, Matt McNaughton,Nick Miller, Kevin Peterson, Brian Pilnick,Raj Rajkumar, Paul Rybski, Bryan Salesky,Young-Woo Seo, Sanjiv Singh, Jarrod Snider,Anthony Stentz, William â€œRedâ€ Whittaker,Ziv Wolkowicki, and Jason Ziglar",2008-07-23,Autonomous driving in urban environments: Boss and the Urban Challenge,https://onlinelibrary.wiley.com/doi/abs/10.1002/rob.20255,1840.0,Highly cited,,,,,,,,,,,,,,Unknown,,,,,United States of America,,,,,,2024-09-05 14:08,,,,,,Academia,,,,,,,Academia,,,,,,,,,
Denoising Autoencoders,Other,Image classification,University of Montreal / UniversitÃ© de MontrÃ©al,"Pascal Vincent, Hugo Larechelle, Yoshua Bengio, Pierre- Antoine Manzagol",2008-07-05,Extracting and Composing Robust Features with Denoising Autoencoders,https://dl.acm.org/doi/10.1145/1390156.1390294,7192.0,Highly cited,,,,,,,,,,,,,,Unknown,,,,,Canada,,,,,,2024-11-01 10:04,,,,,,Academia,,,,,,,Academia,,,,,,,,,
Semi-Supervised Embedding for DL,Other,"Image classification,Language Structure Modeling,Text classification","Google,NUANCE Communications,IDIAP,University of Illinois Urbana-Champaign (UIUC)","Jason Weston, Frederick, Ratle, Hossein Mobahi, Ronan Collobert",2008-07-05,Deep Learning via Semi-Supervised Embedding,https://dl.acm.org/doi/10.1145/1390156.1390303,1087.0,Highly cited,,,,,,,,,,,,,,Unknown,,,,,"United States of America,United States of America,Switzerland,United States of America",,,,,,2024-09-16 09:33,,,,,,"Industry,Industry,Academia,Academia",,,,,,,"Industry,Industry,Academia,Academia",,,,,,,,,
Deep Multitask NLP Network,Language,Language modeling,NEC Laboratories,"Ronan Collobert, Jason Weston",2008-07-05,"A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning
",http://icml2008.cs.helsinki.fi/papers/391.pdf,7095.0,"Highly cited,SOTA improvement",,1500000.0,"With a word vector size of 50 and a vocabulary size of 30,000, the embedding matrix has 1,500,000 parameters. There are also some small convolutional and dense layers with far fewer parameters.",,,"PropBank,Penn TreeBank (PTB),Wikipedia","PropBank (1M words) for semantic role labeling task
Penn Treebank (1M words) for part-of-speech tagging and chunking tasks
Stanford NER dataset for named entity recognition task
Wikipedia text (631M words) for unsupervised pretraining",633000000.0,"Section 7: ""631 million words
from Wikipedia""",168.0,1 week on 1 computer,,Unsupervised,Speculative,"We describe a single convolutional neural network architecture that, given a sentence, outputs a host of language processing predictions: part-of-speech tags, chunks, named entity tags, semantic roles, semantically similar words and the likelihood that the sentence makes sense (grammatically and semantically) using a language model. The entire network is trained jointly on all these tasks using weight-sharing, an instance of multitask learning. All the tasks use labeled data except the language model which is learnt from unlabeled text and represents a novel form of semi-supervised learning for the shared tasks. We show how both multitask learning and semi-supervised learning improve the generalization of the shared tasks, resulting in stateof-the-art performance.",,,,United States of America,,,,,,2025-05-09 11:32,,,,,,Industry,,,,,,,Industry,,,,,,,,,
Multiscale deformable part model,Vision,Object detection,"UC Irvine,University of Chicago,Toyota Technological Institute at Chicago","Pedro Felzenszwalb, David McAllester, Deva Ramanan",2008-06-23,"A discriminatively trained, multiscale, deformable part model",https://ieeexplore.ieee.org/abstract/document/4587597,3093.0,Highly cited,,,,,,,,,,,,,,Unknown,,,,,"United States of America,United States of America,United States of America",,,,,,2024-09-05 14:08,,,,,,"Academia,Academia,Academia",,,,,,,"Academia,Academia,Academia",,,,,,,,,
Enhanced Neighborhood-Based Filtering,Recommendation,Recommender system,AT&T,"RM Bell, Y Koren",2007-10-28,Scalable Collaborative Filtering with Jointly Derived Neighborhood Interpolation Weights,https://ieeexplore.ieee.org/abstract/document/4470228,687.0,SOTA improvement,"""We evaluate these methods on the Netflix dataset, where they deliver significantly better results than the commercial Netflix Cinematch recommender system.""",,,,,,,,,,,,,Unknown,,,,,United States of America,,,,,,2024-09-15 17:45,,,,,,Industry,,,,,,,Industry,,,,,,,,,
BLSTM for handwriting (1),Vision,Image classification,"University of Bern,IDSIA,Technical University of Munich","M Liwicki, A Graves, S FernÃ ndez",2007-09-23,A Novel Approach to On-Line Handwriting Recognition Based on Bidirectional Long Short-Term Memory Networks,https://people.idsia.ch//~juergen/icdar_2007.pdf,287.0,SOTA improvement,,,,,,,,,,,,,,Unknown,,,,,"Switzerland,Switzerland,Germany",,,,,,2024-09-15 17:56,,,,,,"Academia,Academia,Academia",,,,,,,"Academia,Academia,Academia",,,,,,,,,
Fisher Kernel GMM,Vision,Image classification,Xerox,"Florent Perronnin, Christopher Dance",2007-07-16,Fisher kernels on visual vocabularies for image categorization,https://ieeexplore.ieee.org/document/4270291,1915.0,Highly cited,,,,,2.5 hours on an AMD Opteron 2.4GHz with 4GB RAM,,"in-house image dataset of 19 object/scene categories

",30000.0,"""Approximately 30K images were available for training and 5K for testing. Both sets were manually multi-labeled""",2.5,"""With Fisher kernels, the training
cost is reduced down to approximately 2h30.""",,Supervised,Likely,"Within the field of pattern classification, the Fisher kernel is a powerful framework which combines the strengths of generative and discriminative approaches. The idea is to
characterize a signal with a gradient vector derived from a
generative probability model and to subsequently feed this
representation to a discriminative classifier. We propose to
apply this framework to image categorization where the input signals are images and where the underlying generative
model is a visual vocabulary: a Gaussian mixture model
which approximates the distribution of low-level features in
images. We show that Fisher kernels can actually be understood as an extension of the popular bag-of-visterms. Our approach demonstrates excellent performance on two challenging databases: an in-house database of 19 object/scene categories and the recently released VOC 2006 database. It is also very practical: it has low computational needs both at training and test time and vocabularies trained on one set of categories can be applied to another set without any significant loss in performance.",,,,United States of America,,,,,,2025-05-09 11:32,,,,,,Industry,,,,,,,Industry,,,,,,,,,
SB-LM,Language,Language modeling,Google,"T. Brants, Ashok Popat, P. Xu, F. Och, J. Dean",2007-06-22,Large Language Models in Machine Translation,https://www.semanticscholar.org/paper/Large-Language-Models-in-Machine-Translation-Brants-Popat/ba786c46373892554b98df42df7af6f5da343c9d,,"Training cost,Highly cited",,300000000000.0,Table 2,1.4494464e+18,"Assuming a Nehalem based processor with 8 FLOP/cycle (https://www.agner.org/optimize/microarchitecture.pdf#page=105.06) , 2 cores and 2.33 GHz clock speed: 8*2*2330000000=37280000000 FLOP/s
Trained for 1 day on 1500 machines (Table 2)
Compute: 1500*37280000000*1*24*60*60*0.3=1449446400000000000=1.4e18
",,"Largest training set is dubbed ""web"", and is described as ""general web data, which was collected in January 2006 (2 trillion tokens)""
Table 2 indicates 1.8T tokens",1800000000000.0,Table 2,24.0,Table 2,,,Likely,"Systems, methods, and computer program products for machine translation are provided. In some implementations a system is provided. The system includes a language model including a collection of n-grams from a corpus, each n-gram having a corresponding relative frequency in the corpus and an order n corresponding to a number of tokens in the n-gram, each n-gram corresponding to a backoff n-gram having an order of n-1 and a collection of backoff scores, each backoff score associated with an n-gram, the backoff score determined as a function of a backoff factor and a relative frequency of a corresponding backoff n-gram in the corpus.",,,,United States of America,,,,1500.0,,2025-05-01 10:42,,,,,,Industry,,,,,,,Industry,,,,,Hardware,,,,
KN-LM,Language,Language modeling,Google,"T. Brants, Ashok Popat, P. Xu, F. Och, J. Dean",2007-06-22,Large Language Models in Machine Translation,https://www.semanticscholar.org/paper/Large-Language-Models-in-Machine-Translation-Brants-Popat/ba786c46373892554b98df42df7af6f5da343c9d,,"Training cost,Highly cited",,21000000000.0,Table 2,7.7303808e+17,"Trained for 2 days on 400 machines (Table 2)
Assuming a Nehalem based processor with 8 FLOP/cycle (https://www.agner.org/optimize/microarchitecture.pdf#page=105.06) , 2 cores and 2.33 GHz clock speed: 8*2*2330000000=37280000000 FLOP/s
Compute: 400*37280000000*2*24*60*60*0.3=773038080000000000=7.7e17",,"Largest dataset (""web"") was deemed to expensive to train with the KN methodology. Largest actually used was ""webnews"", described as ""data collected over several years, up to December 2005, from web pages containing predominantly English news articles (31 billion tokens).""",31000000000.0,Table 2,48.0,Table 2,,,Likely,"Systems, methods, and computer program products for machine translation are provided. In some implementations a system is provided. The system includes a language model including a collection of n-grams from a corpus, each n-gram having a corresponding relative frequency in the corpus and an order n corresponding to a number of tokens in the n-gram, each n-gram corresponding to a backoff n-gram having an order of n-1 and a collection of backoff scores, each backoff score associated with an n-gram, the backoff score determined as a function of a backoff factor and a relative frequency of a corresponding backoff n-gram in the corpus.",,,,United States of America,,,,400.0,,2025-05-01 10:42,,,,,,Industry,,,,,,,Industry,,,,,Hardware,,,,
Empirical evaluation of deep architectures,Other,Image classification,University of Montreal / UniversitÃ© de MontrÃ©al,"Hugo Larechelle, Dumithru Erhan, Aaron C Courville, James Bergsta, Yoshua Bengio",2007-06-01,An empirical evaluation of deep architectures on problems with many factors of variation,https://dl.acm.org/doi/10.1145/1273496.1273556,1149.0,Highly cited,,,,,,,,,,,,,,Unknown,,,,,Canada,,,,,,2024-11-01 10:04,,,,,,Academia,,,,,,,Academia,,,,,,,,,
Greedy layer-wise DNN training,Other,"Image classification,Regression",University of Montreal / UniversitÃ© de MontrÃ©al,"Y Bengio, P Lamblin, D Popovici",2006-12-04,Greedy layer-wise training of deep networks,https://dl.acm.org/doi/10.5555/2976456.2976476,5605.0,Highly cited,,,,,,,,,,,,,,Unknown,,,,,Canada,,,,,,2024-09-16 10:11,,,,,,Academia,,,,,,,Academia,,,,,,,,,
Local Binary Patterns for facial recognition,Vision,Image classification,"University of Oulu,IEEE","Timo Ahonen, Abdenour Hadid, and Matti Pietikainen",2006-12-01,Face Description with Local Binary Patterns: Application to Face Recognition,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.456.1094&rep=rep1&type=pdf,5816.0,Highly cited,,,"Shallowly investigated, couldn't find much.
",,,,,,,,,,,Unknown,,,,,"Finland,Multinational",,,,,,2024-09-15 23:16,,,,,,"Academia,Industry",,,,,,,"Academia,Industry",,,,,,,,,
Sparse Vision Encoding,Vision,Image classification,Stanford University,"Honglak Lee, Alexis Battle, Rajat Raina, Andrew Y. Ng",2006-11-01,Efficient sparse coding algorithms,https://proceedings.neurips.cc/paper_files/paper/2006/file/2d71b2ae158c7c5912cc0bbde2bb9d95-Paper.pdf,3512.0,Highly cited,"Major caveat: the task in question is ""encoding"", which I'm not sure is a valid task. Unusually low confidence across all answers in this form due to the task and training data being atypical.",,,9604800000000.0,"(Just for natural images)

""All experiments were conducted on a Linux machine with AMD Opteron 2GHz CPU and 2GB RAM. ... For example, we were able to learn a set of 1,024 bases (each 14Ã—14 pixels)in about 2 hours and a set of 2,000 bases (each 20Ã—20 pixels) in about 10 hours.""

I filtered for 2GHz Opteron models that came out in 2005, of which there are five: https://www.techpowerup.com/cpu-specs/?mfgr=AMD&released=2005&generation=AMD%20Opteron&sort=name

Found a source which indicates 3 cycles per 32-bit multiply, and 5 per 64 bit: https://www.cse.wustl.edu/~roger/569M/m2066.pdf 
Assuming 32-bit precision, 2e9 cycles/s / 3 cycle/FLOP = 6.67e8 FLOP/s

10 * 3600 * 6.67e8 * 0.4 = 9.6048e12 FLOPs",,,2000.0,"However, these are 1000 20x20 pixel ""bases"", not images",10.0,"""...in about 2 hours and a set of 2,000 bases (each 20Ã—20 pixels) in about 10 hours.""",,Unsupervised,Likely,"Sparse coding provides a class of algorithms for finding succinct representations of stimuli; given only unlabeled input data, it discovers basis functions that capture higher-level features in the data. However, finding sparse codes remains a very difficult computational problem. In this paper, we present efficient sparse coding algorithms that are based on iteratively solving two convex optimization problems: an L1-regularized least squares problem and an L2-constrained least squares problem. We propose novel algorithms to solve both of these optimization problems. Our algorithms result in a significant speedup for sparse coding, allowing us to learn larger sparse codes than possible with previously described algorithms. We apply these algorithms to natural images and demonstrate that the inferred sparse codes exhibit end-stopping and non-classical receptive field surround suppression and, therefore, may provide a partial explanation for these two phenomena in V1 neurons.",,,Unreleased,United States of America,,,,,,2025-05-09 11:32,,,,,,Academia,,,,,,,Academia,,,,,Hardware,,,,
Spatial Pyramid Matching,Vision,Image classification,"INRIA,University of Illinois Urbana-Champaign (UIUC),Ecole Normale SupÃ¨rieure","S Lazebnik, C Schmid, J Ponce",2006-06-17,Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories,https://inc.ucsd.edu/mplab/users/marni/Igert/Lazebnik_06.pdf,9807.0,Highly cited,,,,,,,,,,,,,,Unknown,,,,,"France,United States of America,France",,,,,,2024-09-05 14:08,,,,,,"Academia,Academia,Academia",,,,,,,"Academia,Academia,Academia",,,,,,,,,
FAST,Video,Corner detection,University of Cambridge,Edward Rosten and Tom Drummond,2006-05-07,Machine Learning for High-Speed Corner Detection,https://link.springer.com/chapter/10.1007/11744023_34,5419.0,Highly cited,,,,,,,,,,,,,,Unknown,,,,,United Kingdom of Great Britain and Northern Ireland,,,,,,2024-09-05 14:08,,,,,,Academia,,,,,,,Academia,,,,,,,,,
RL for helicopter flight,Driving,Helicopter driving,"University of California (UC) Berkeley,Stanford University","H. Kim, Michael Jordan, Shankar Sastry, Andrew Ng",2006-03-09,Autonomous helicopter flight via reinforcement learning,https://papers.nips.cc/paper/2003/hash/b427426b8acd2c2e53827970f2c2f526-Abstract.html,362.0,,,,,,,,,,,,,,,Unknown,,,,,"United States of America,United States of America",,,,,,2024-10-01 10:02,,,,,,"Academia,Academia",,,,,,,"Academia,Academia",,,,,,,,,
TFE SVM,Vision,Character recognition,"Centre de Recherche en Automatique de Nancy (CRAN),CENPARMI","Fabian Lauer, Ching Y Suen, Gerard Bloch",2006-02-02,A trainable feature extractor for handwritten digit recognition,https://hal.archives-ouvertes.fr/hal-00018426/en,365.0,SOTA improvement,best at affine-transformed digits in table 4,,,,,,,,,,,,,Unknown,,,,,"France,Canada",,,,,,2024-09-05 14:08,,,TFE SVM,,,"Academia,Academia",,,,,,,"Academia,Academia",,,,,,,,,
Stanley (DARPA Grand Challenge 2),Driving,Self-driving car,Stanford University,"S Thrun, M Montemerlo, H Dahlkamp",2006-01-01,Stanley: The Robot that Won the DARPA Grand Challenge,http://robots.stanford.edu/papers/thrun.stanley05.pdf,2561.0,Highly cited,,,"""Our  approach  and  the underlying  probabilistic  Markov  model  possess  anumber  of  unknown  parameters.  These  parameters include the height threshold, the statistical acceptance  probability  threshold,  and  various  Markov chain error parameters the noise covariances of theprocess noise and the measurement noise. Stanley uses a discriminative learning algorithm for  locally  optimizing  these  parameters.""",,,,,,,,,,,Unknown,,,,,United States of America,,,,,,2024-09-05 14:08,,,,,,Academia,,,,,,,Academia,,,,,,,,,
Monocular Depth Prediction,Vision,Miscellaneous image analysis,Stanford University,"Ashutosh Saxena, Sung H. Chung, and Andrew Y. Ng",2005-12-05,Learning Depth from Single Monocular Images,https://papers.nips.cc/paper_files/paper/2005/file/17d8da815fa21c57af9829fb0a869602-Paper.pdf,1417.0,Highly cited,1000+ citations,1472256.0,"""In detail, we use different parameters (Î¸r, Ïƒ1r, Ïƒ2r) for each row in the image, because the images we consider are taken from a horizontally mounted camera, and thus different rows of the image have different statistical properties.""
""We collected a total of 425 image+depthmap pairs, with an image resolution of 1704x2272 and a depthmap resolution of 86x107""

The dimensionality of each parameter set isn't totally clear, but from equation (1) it seems like Î¸ is the same length as the absolute depth input features (646) and Ïƒ1 and Ïƒ2 are scalar. If so, then we should have:
2272 * (646 + 1 + 1) = 1,472,256 parameters",,,,"""We used a 3-D laser scanner to collect images and their corresponding depthmaps. The scanner uses a SICK 1-D laser range finder mounted on a motor to get 2D scans. We collected a total of 425 image+depthmap pairs, with an image resolution of 1704x2272 and a depthmap resolution of 86x107""",2933137.0,"""We collected a total of 425 image+depthmap pairs, with an image resolution of 1704x2272 and a depthmap resolution of 86x107. In the experimental results reported here, 75% of the images/depthmaps were used for training, and the remaining 25% for hold-out testing""
It seems like they do predictions over the dense values in the depthmap, so
86 * 107 * 425 * 0.75 = 2,933,137",,,,Supervised,Speculative,"We consider the task of depth estimation from a single monocular image. We take a supervised learning approach to this problem, in which we begin by collecting a training set of monocular images (of unstructured outdoor environments which include forests, trees, buildings, etc.) and their corresponding ground-truth depthmaps. Then, we apply supervised learning to predict the depthmap as a function of the image. Depth estimation is a challenging problem, since local features alone are insufficient to estimate depth at a point, and one needs to consider the global context of the image. Our model uses a discriminatively-trained Markov Random Field (MRF) that incorporates multiscale local- and global-image features, and models both depths at individual points as well as the relation between depths at different points. We show that, even on unstructured scenes, our algorithm is frequently able to recover fairly accurate depthmaps.",,,Unreleased,United States of America,,,,,,2025-05-09 11:32,,,,,,Academia,,,,,,,Academia,,,,,,,,,
RankNet,Search,Search,"Microsoft Research,Microsoft","Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier, Matt Deeds, Nicole Hamilton, Greg HullenderAuthors Info & Claims",2005-08-07,Learning to rank using gradient descent,https://dl.acm.org/doi/abs/10.1145/1102351.1102363,,Highly cited,,5711.0,"Model is ""a two layer net with 10 hidden units""
Input is of size 569 ""In all, we use 569 features""
Parameters:
569*10 + 10 for hidden layer
10*1 + 1 for output layer",3482081588304.0,"FLOPs per forward pass: 2*parameters = 11422
FLOPs per pair (data point): two forward passes and one backward pass (""A forward prop is performed for the first sample; each nodeâ€™s activation and gradient value are stored; a forward prop is then performed for the second sample, and the activations and gradients are again stored. The gradient of the cost is then *formula*"") = 2*11422 + 2*11422 = 45688
Total FLOPs = (FLOPs per pair = 45688)*(pairs = 3,464,289)*(epochs = 22) = 3.48E12",,"""The data comprises 17,004 queries for the
English / US market, each with up to 1000 returned
documents.""
...
""This resulted in
our training on 384,314 query/document feature vectors, and on 3,464,289 pairs.""",3464289.0,"""This resulted in our training on 384,314 query/document feature vectors, and on 3,464,289 pairs.""",5.9,Table 6,,,Confident,"We investigate using gradient descent methods for learning ranking functions; we propose a simple probabilistic cost function, and we introduce RankNet, an implementation of these ideas using a neural network to model the underlying ranking function. We present test results on toy data and on data from a commercial internet search engine.",22.0,,Unreleased,"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,United States of America,Multinational,India,Belgium",,,,,,2025-05-01 10:42,,,,,,"Industry,Industry",,,,,,,"Industry,Industry",,,,,Operation counting,,,,
BiLSTM for Speech,Speech,Speech recognition,"IDSIA,Technical University of Munich","A Graves, J Schmidhuber",2005-08-01,Framewise phoneme classification with bidirectional LSTM and other neural network architectures,https://www.sciencedirect.com/science/article/abs/pii/S0893608005001206,4281.0,Highly cited,,152061.0,"""The hidden layer sizes were chosen to ensure that all networks had roughly the same number of weights W (â‰ˆ100,000). However, for the MLPs the network grew with the time-window size, and W varied between 22,061 and 152,061.""",24124575958774.88,"Extracted from AI and Compute (https://openai.com/blog/ai-and-compute/) charts by using https://automeris.io/WebPlotDigitizer/.

Forward FLOP: 2*200000=400000
Trained on 4158 utterances (TIMIT)
""We found that large networks, of around 200,000 weights,
gave good performance""
TIMIT has around 5 hours total. Estimated utterance length: 5*60*60/(4620+1680)=2.86s
The frame size was 5 ms
Frames per utterance: 2.86/0.005=572
Training FLOP for one epoch: 3*400000*572*4158=2854051200000

Epochs unclear, based on OpenAIs estimate it would be 8. ",TIMIT,,36960.0,"https://catalog.ldc.upenn.edu/LDC93s1
One sample utterance has around 10 words

3696 utterances * 10 words = around 37k words",,,,,Likely,,,,,"Switzerland,Germany",,,,,,2025-06-16 12:24,,,,,,"Academia,Academia",,,,,,,"Academia,Academia",,,,,Third-party estimation,,,,
Hierarchical LM,Language,Language modeling,,"Frederic Morin, Yoshua Bengio",2005-01-06,Hierarchical Probabilistic Neural Network Language Model,https://www.semanticscholar.org/paper/Hierarchical-Probabilistic-Neural-Network-Language-Morin-Bengio/c19fbefdeead6a4154a22a9c8551a18b1530033a,,Highly cited,,,,115848000000000.0,"""The computations were performed on Athlon processors with a 1.2 GHz clock""
FP32 per cycle: 4 (""The bottom line is that the Athlon is capable of delivering as many as four 32-bit, single-precision floating-point results per clock cycle"", https://www.pctechguide.com/amd-technology/amd-athlon) 
Training time per epoch: 1609s (table 1)
Epochs: 30 ""Training is performed over about 20 to 30 epochs according to validation set perplexity (early stopping).""
Assumed utilization: 0.5 
Compute estimate: 0.5*1200000000*4*30*1609=115848000000000=1.16e14",Brown corpus,"""The experiments were performed on the Brown corpus, with a reduced vocabulary size of 10,000 words""",1000000.0,"""The corpus has 1,105,515 occurrences of words, split into 3 sets: 900,000 for training, 100,000 for validation (model selection), and 105,515 for testing""",13.4,"Training time per epoch: 1609s (table 1)
Total training time 30*1609/60/60=13.408h
",,,Confident,"In recent years, variants of a neural network architecture for statistical language modeling have been proposed and successfully applied, e.g. in the language modeling component of speech recognizers. The main advantage of these architectures is that they learn an embedding for words (or other symbols) in a continuous space that helps to smooth the language model and provide good generalization even when the number of training examples is insufficient. However, these models are extremely slow in comparison to the more commonly used n-gram models, both for training and recognition. As an alternative to an importance sampling method proposed to speed-up training, we introduce a hierarchical decomposition of the conditional probabilities that yields a speed-up of about 200 both during training and recognition. The hierarchical decomposition is a binary hierarchical clustering constrained by the prior knowledge extracted from the WordNet semantic hierarchy.",30.0,,,,,,,1.0,,2025-02-19 12:35,,,,,,,,,,,,,,,,,,Hardware,,,,
LMICA,Vision,Object detection,,"Yoshitatsu Matsuda, K. Yamaguchi",2004-12-01,"Linear Multilayer Independent Component Analysis for Large Natural Scenes
",https://www.semanticscholar.org/paper/Linear-Multilayer-Independent-Component-Analysis-Matsuda-Yamaguchi/7061b01572fbff2e223ce3abb59f397895b1ebf1,,"Training cost,Historical significance",,4096000.0,"64*64*1000=4096000
""100000 samples of natural scenes of 64 Ã— 64 pixels were given as X""
""LMICA was carried out in 1000 layers""
",2782080000000000.0,"69*60*60*8*2800000000*0.5=2782080000000000=2.78e15
""it consumed about 69 hours with Intel 2.8GHz CPU""
- Assuming they used an Intel Pentium 4 processor with 8 FLOP/cycle (https://en.wikipedia.org/wiki/FLOPS)",,,100000.0,"""100000 samples of natural scenes of 64 Ã— 64 pixels were given as X""",,,,,Confident,"In this paper, linear multilayer ICA (LMICA) is proposed for extracting independent components from quite high-dimensional observed signals such as large-size natural scenes. There are two phases in each layer of LMICA. One is the mapping phase, where a one-dimensional mapping is formed by a stochastic gradient algorithm which makes more highly-correlated (non-independent) signals be nearer incrementally. Another is the local-ICA phase, where each neighbor (namely, highly-correlated) pair of signals in the mapping is separated by the MaxKurt algorithm. Because LMICA separates only the highly-correlated pairs instead of all ones, it can extract independent components quite efficiently from appropriate observed signals. In addition, it is proved that LMICA always converges. Some numerical experiments verify that LMICA is quite efficient and effective in large-size natural image processing.",,,,,,,,,,2025-05-01 10:42,,,,,,,,,,,,,,,,,,Hardware,,,,
Invariant CNN,Vision,Object recognition,New York University (NYU),"Yann LeCun, Fu Jie Huang, L. Bottou",2004-06-27,Learning methods for generic object recognition with invariance to pose and lighting,https://www.semanticscholar.org/paper/Learning-methods-for-generic-object-recognition-to-LeCun-Huang/f354310098e09c1e1dc88758fca36767fd9d084d,,"Highly cited,Historical significance",,90575.0,"""The network has a total of 90,575 trainable parameters.""",974230000000.0,"""A full propagation through the network requires 3,896,920 multiply-adds."" - it's not entirely clear whether this refers to a forward pass or forward + backward pass (I assumed the latter)
""We used a stochastic version of the Levenberg-Marquardt algorithm with diagonal approximation of the Hessian [7], for approximately 250,000 online updates.""
3896920*250000=974230000000=9.7e11",,,24300.0,"""normalized-uniform set: 5 classes, centered, unperturbed objects on uniform backgrounds. 24,300 training samples, 24,300 testing samples.""",,,,,Confident,"We assess the applicability of several popular learning methods for the problem of recognizing generic visual categories with invariance to pose, lighting, and surrounding clutter. A large dataset comprising stereo image pairs of 50 uniform-colored toys under 36 azimuths, 9 elevations, and 6 lighting conditions was collected (for a total of 194,400 individual images). The objects were 10 instances of 5 generic categories: four-legged animals, human figures, airplanes, trucks, and cars. Five instances of each category were used for training, and the other five for testing. Low-resolution grayscale images of the objects with various amounts of variability and surrounding clutter were used for training and testing. Nearest neighbor methods, support vector machines, and convolutional networks, operating on raw pixels or on PCA-derived features were tested. Test error rates for unseen object instances placed on uniform backgrounds were around 13% for SVM and 7% for convolutional nets. On a segmentation/recognition task with highly cluttered images, SVM proved impractical, while convolutional nets yielded 16/7% error. A real-time version of the system was implemented that can detect and classify objects in natural scenes at around 10 frames per second.",10.0,,,United States of America,,,,,,2025-05-01 10:42,,,,,,Academia,,,,,,,Academia,,,,,Operation counting,,,,
Sandstorm (DARPA Grand Challenge I),Driving,Self-driving car,Carnegie Mellon University (CMU),William Red L. Whittaker,2004-06-14,DARPA Grand Challenge Technical Paper,https://ieeexplore.ieee.org/document/1336386,66.0,,,,,,,,,,,,,,,Unknown,,,,,United States of America,,,,,,2024-09-05 14:08,,,,,,Academia,,,,,,,Academia,,,,,,,,,
GPU implementation of neural networks,Vision,Object detection,Soongsil University,"KS Oh, K Jung",2004-06-01,GPU implementation of neural networks,https://www.sciencedirect.com/science/article/pii/S0031320304000524,471.0,,,,,,,,,,,,,,,Unknown,,,,,Korea (Republic of),,,,,,2024-09-15 23:28,,,,,,Academia,,,,,,,Academia,,,,,,,,,
NPLM (Brown),Language,Text autocompletion,University of Montreal / UniversitÃ© de MontrÃ©al,"Yoshua Bengio, RÃ©jean Ducharme, Pascal Vincent, Christian Jauvin",2003-03-15,A Neural Probabilistic Language Model,https://dl.acm.org/doi/10.5555/944919.944966,7627.0,Highly cited,,4124233.0,"""The number of free parameters is |V|(1 + nm + h) + h(1 + (n âˆ’ 1)m) [...] For example, consider the following architecture used in the experiments on the AP (Associated Press) news data: the vocabulary size is |V| = 17,964, the number of hidden units is h = 60, the order of the model is n = 6, the number of word features is m = 100""

Brown corpus: n=5, h=100, m=30, |V|=16383
16383*(1+5*30+100)+100*(1+(5-1)*30)=4124233",132076260000000.0,"""For example, consider the following architecture used in the experiments on the AP (Associated Press) news data: the vocabulary size is |V| = 17,964, the number of hidden units is h = 60, the order of the model is n = 6, the number of word features is m = 100. The total number of numerical operations to process a single training example is approximately |V|(1+nm+h)+h(1+nm)+nm""

Brown corpus: n=5, h=100, m=30, |V|=16383, dataset size = 800000, epochs=20
Forward FLOP: 16383*(1+5*30+100)+100*(1+5*30)+5*30=4127383
Adjusting for backward pass with 1:1 ratio, as the by far largest layer (embedding) doesn't require gradients w.r.t. inputs.
Total FLOP: 2*4127383*800000*20=1.3207626e+14",Brown corpus,,800000.0,"""Comparative experiments were performed on the Brown corpus which is a stream of 1,181,041 words, from a large variety of English texts and books. The first 800,000 words were used for training, the following 200,000 for validation (model selection, weight decay, early stopping) and the remaining 181,041 for testing. The number of different words is 47,578 (including punctuation, distinguishing between upper and lower case, and including the syntactical marks used to separate texts and paragraphs). Rare words with frequency â‰¤ 3 were merged into a single symbol, reducing the vocabulary size to |V| = 16,383.""",,,,,Confident,,,,,Canada,,,,,,2025-06-16 11:37,,,,,,Academia,,,,,,,Academia,,,,,Operation counting,,,,
NPLM (AP News),Language,Text autocompletion,University of Montreal / UniversitÃ© de MontrÃ©al,"Yoshua Bengio, RÃ©jean Ducharme, Pascal Vincent, Christian Jauvin",2003-03-15,A Neural Probabilistic Language Model,https://dl.acm.org/doi/10.5555/944919.944966,7627.0,Highly cited,,11904264.0,"""The number of free parameters is |V|(1 + nm + h) + h(1 + (n âˆ’ 1)m) [...] For example, consider the following architecture used in the experiments on the AP (Associated Press) news data: the vocabulary size is |V| = 17,964, the number of hidden units is h = 60, the order of the model is n = 6, the number of word features is m = 100""

AP News: n=6, h=60, m=100, |V|=17964
17964*(1+6*100+60)+60*(1+(6-1)*100)=11904264",1666869200000000.0,"""For example, consider the following architecture used in the experiments on the AP (Associated Press) news data: the vocabulary size is |V| = 17,964, the number of hidden units is h = 60, the order of the model is n = 6, the number of word features is m = 100. The total number of numerical operations to process a single training example is approximately |V|(1+nm+h)+h(1+nm)+nm""

AP News: n=6, h=60, m=100, |V|=17964, dataset=13994528, epochs=5
Forward FLOP: 17964*(1+6*100+60)+60*(1+6*100)+6*100=11910864
Adjusting for backward pass with 1:1 ratio, as the by far largest layer (embedding) doesn't require gradients w.r.t. inputs.
Total FLOP: 2*11910864*13994528*5=1.6668692e+15",Brown corpus,,13994528.0,"""Comparative experiments were performed on the Brown corpus which is a stream of 1,181,041 words, from a large variety of English texts and books. The first 800,000 words were used for training, the following 200,000 for validation (model selection, weight decay, early stopping) and the remaining 181,041 for testing. The number of different words is 47,578 (including punctuation, distinguishing between upper and lower case, and including the syntactical marks used to separate texts and paragraphs). Rare words with frequency â‰¤ 3 were merged into a single symbol, reducing the vocabulary size to |V| = 16,383.""",,,,,Confident,,,,,Canada,,,,,,2025-06-16 11:37,,,,,,Academia,,,,,,,Academia,,,,,Operation counting,,,,
LDA,Language,Document classification,Stanford University,"David M. Blei, Andrew Y. Ng, Michael I. Jordan",2003-02-02,Latent Dirichlet Allocation,https://jmlr.org/papers/volume3/blei03a/blei03a.pdf,38724.0,Highly cited,,,,,,,,,Multiple experiments with different tasks and datasets,,,,,Unknown,,,,,United States of America,,,,,,2024-09-05 14:08,,,,,,Academia,,,,,,,Academia,,,,,,,,,
Statistical Shape Constellations,Vision,Image classification,California Institute of Technology,"M. Weber, M. Welling, and P. Perona",2003-01-01,Unsupervised Learning of Models for Recognition,https://link.springer.com/content/pdf/10.1007/3-540-45054-8_2.pdf,949.0,Historical significance,,,,,,,,,,,,,,Unknown,,,,,United States of America,,,,,,2024-09-05 14:08,,,,,,Academia,,,,,,,Academia,,,,,,,,,
Web mining + Decision tree recommender,Recommendation,Recommender system,Korea Advanced Institute of Science and Technology (KAIST),"YH Cho, JK Kim, SH Kim",2002-10-01,A personalized recommender system based on web usage mining and decision tree induction,https://reader.elsevier.com/reader/sd/pii/S0957417402000520?token=155B6D1937982D7D0271AFD1CFB034DFD7F3D1DE816B66C025EBC9D0A305BA6DA685DD62989DC05246C794CAC74CDAEF&originRegion=us-east-1&originCreation=20220325235441,656.0,,,,,,,,,,,,,,,Unknown,,,,,Korea (Republic of),,,,,,2024-09-05 14:08,,,,,,Academia,,,,,,,Academia,,,,,,,,,
Tagging via Viterbi Decoding,Language,"Binary classification,Part-of-speech tagging",AT&T,Michael Collins,2002-06-01,Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms,https://dl.acm.org/doi/10.3115/1118693.1118694,2582.0,Highly cited,,,,,,,,,,,,,,Unknown,,,,,United States of America,,,,,,2024-09-05 14:08,,,,,,Industry,,,,,,,Industry,,,,,,,,,
NEAT,Other,Pole balancing,IDSIA,"Justin Bayer, Daan Wierstra, Julian Togelius, JÃ¼rgen Schmidhuber",2002-06-01,Evolving Neural Networks through Augmenting Topologies ,https://direct.mit.edu/evco/article/10/2/99/1123/Evolving-Neural-Networks-through-Augmenting,3402.0,Highly cited,,,,,,,,,,,,,,Unknown,"An important question in neuroevolution is how to gain an advantage from evolving neural network topologies along with weights. We present a method, NeuroEvolution of Augmenting Topologies (NEAT), which outperforms the best fixed-topology method on a challenging benchmark reinforcement learning task. We claim that the increased efficiency is due to (1) employing a principled method of crossover of different topologies, (2) protecting structural innovation using speciation, and (3) incrementally growing from minimal structure. We test this claim through a series of ablation studies that demonstrate that each component is necessary to the system as a whole and to each other. What results is significantly faster learning. NEAT is also an important contribution to GAs because it shows how it is possible for evolution to both optimize and complexify solutions simultaneously, offering the possibility of evolving increasingly complex solutions over generations, and strengthening the analogy with biological evolution.",,,,Switzerland,,,,,,2025-05-01 10:42,,,,,,Academia,,,,,,,Academia,,,,,,,,,
Decision tree (classification),Vision,Face recognition,"Mitsubishi Electric Research Labs,Compaq CRL","P. Viola, M. Jones",2001-12-08,Rapid object detection using a boosted cascade of simple features,https://www.cs.cmu.edu/~efros/courses/LBMV07/Papers/viola-cvpr-01.pdf,23449.0,Highly cited,,12000.0,"Iteratively learns decision tree features, where each feature has 2 parameters (threshold and parity). They learn 6000 features total: ""The complete face detection cascade has 38 stages with over
6000 features""

Additional weights (see Table 1) are a temporary variable of the optimization and not part of the final model. ",63000000000000.0,"
The training compute can be tediously worked out from the pseudocode. I think for dataset size D, number of filters T, the training compute is roughly 180k * D * 3 * T = 6.3e13 FLOPs

The training evaluates each of 180k candidate features at each step, repeated for 6000 steps (as 1 feature is selected per round). 
The operations for one feature evaluation are unclear, but should be low (they only compare specific integer positions in the image). Estimated at 10op. 
180000*6000*10*14460=1.56168e+14",,They scraped the dataset personally for training,14460.0,Section 5: 4916 hand labeled faces  + 9544 non-face images = 14460,,,,,Likely,,,,,"United States of America,United States of America",,,,,,2025-06-16 10:57,,,,,,"Industry,Industry",,,,,,,"Industry,Industry",,,,,Operation counting,,,,
Gradient Boosting Machine,Mathematics,"Pattern classification,Binary classification,Regression",Stanford University,Jerome H. Friedman,2001-10-01,Greedy function approximation: A gradient boosting machine,https://projecteuclid.org/journals/annals-of-statistics/volume-29/issue-5/Greedy-function-approximation-A-gradient-boostingmachine/10.1214/aos/1013203451.full,17891.0,Highly cited,,,,,,,,,,,,,,Unknown,,,,,United States of America,,,,,,2024-09-05 14:08,,,,,,Academia,,,,,,,Academia,,,,,,,,,
Immediate trihead,Language,Language modeling,Brown University,Eugene Charniak,2001-07-06,Immediate-Head Parsing for Language Models,https://dl.acm.org/doi/10.3115/1073012.1073029,422.0,SOTA improvement,"""The perplexity for both of these models significantly improve
upon the trigram model base-line as
well as the best previous grammar based language model""",,,,,,,,,,,,,Unknown,,,,,United States of America,,,,,,2024-09-05 14:08,,,,,,Academia,,,,,,,Academia,,,,,,,,,
PoE MNIST,Vision,Digit recognition,University College London (UCL),"Guy Mayraz, Geoffrey E. Hinton",2000-11-28,Recognizing Hand-written Digits Using Hierarchical Products of Experts,https://proceedings.neurips.cc/paper_files/paper/2000/file/1f1baa5b8edac74eb4eaa329f14a0361-Paper.pdf,,Historical significance,,3925310.0,"10 models, one for each digit. Largest models: 500 epochs, 500 hidden units (Table 2)
""The largest network was the best, even though each digit model contains 392,500 parameters trained on only 4,400 images""
""the classification network had 30 inputs and therefore 300 weights and 10 output biases.""

Total: 392500*10 + 310 = 3,925,310",51810000000000.0,"Each model was trained on 4400 examples: ""The largest network was the best, even though each digit model contains 392,500 parameters trained on only 4,400 images.""

Table 2, largest network trained 500 epochs.
10 * 6 * 392500 * 4400 * 500 = 51,810,000,000,000",MNIST,,60000.0,Total training data size is 60000 but the subnetworks were trained on smaller subsets.,,,,,Confident,"The product of experts learning procedure [1] can discover a set of stochastic binary features that constitute a non-linear generative model of handwritten images of digits. The quality of generative models learned in this way can be assessed by learning a separate model for each class of digit and then comparing the unnormalized probabilities of test images under the 10 different class-specific models. To improve discriminative performance, it is helpful to learn a hierarchy of separate models for each digit class. Each model in the hierarchy has one layer of hidden units and the nth level model is trained on data that consists of the activities of the hidden units in the already trained (n - 1)th level model. After training, each level produces a separate, unnormalized log probabilty score. With a three-level hierarchy for each of the 10 digit classes, a test image produces 30 scores which can be used as inputs to a supervised, logistic classification network that is trained on separate data. On the MNIST database, our system is comparable with current state-of-the-art discriminative methods, demonstrating that the product of experts learning procedure can produce effective generative models of high-dimensional data.",500.0,,,United Kingdom of Great Britain and Northern Ireland,,,,,,2025-05-01 10:42,,,,,,Academia,,,,,,,Academia,,,,,Operation counting,,,,
Neural LM,Language,Language modeling,University of Montreal / UniversitÃ© de MontrÃ©al,"Yoshua Bengio, RÃ©jean Ducharme, Pascal Vincent, Christian Janvin",2000-11-28,A Neural Probabilistic Language Model,https://papers.nips.cc/paper_files/paper/2000/file/728f206c2a01bf572b5940d7d9a8fa4c-Paper.pdf,,"Training cost,Historical significance,Highly cited",,6906980.0,"(30959*100) + (8*100*120) + (120*30959) = 6,906,980
""This is obtained with a network with the direct architecture, 100 randomly initialized words features, 120 hidden units, and n = 8 words of context.""
""The Hansard corpus (Canadian parliament proceedings, French version) is a stream of about 34 million words, of which 32 millions (set A) was used for training, 1.1 million (set B) was used for validation, and 1.2 million (set C) was used for out-of-sample tests. The original data has 106, 936 different words, and those with frequency <= 10 were merged into a single token, yielding IVI = 30,959 different words.""
",6339000000000000.0,"The authors use a trick to avoid having to calculate the final layer for all possible words in the vocabulary. They precompute a ""short list"" of the most common word following any 2 precursor words with a smoothed trigram model, and then only calculate the softmax over words on the short list. This means only a negligible fraction of the unembedding parameters get used, so the effective number of parameters appears to be (30959*100) + (8*100*120) = 3,191,900

""Apparent convergence of the stochastic gradient descent procedure was obtained after around 10 epochs for Hansard""

6ND:
6*3191900*33100000*10=6.339e15
",Hansard Corpus,,33100000.0,"The Hansard corpus (Canadian parliament proceedings, French version) is a stream of about 34 million words, of which 32 millions (set A) was used for training, 1.1 million (set B) was used for validation, and 1.2 million (set C) was used for out-of-sample tests.",,,,,Confident,"A goal of statistical language modeling is to learn the joint probability function of sequences of words. This is intrinsically difficult because of the curse of dimensionality: we propose to fight it with its own weapons. In the proposed approach one learns simultaneously (1) a distributed representation for each word (i.e. a similarity between words) along with (2) the probability function for word sequences, expressed with these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar to words forming an already seen sentence. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach very significantly improves on a state-of-the-art trigram model.",10.0,,,Canada,,,,,,2025-05-01 10:42,,,,,,Academia,,,,,,,Academia,,,,,Operation counting,,,,
SVD in recommender systems,Recommendation,Recommender system,University of Minnesota,"B Sarwar, G Karypis, J Konstan, J Riedl",2000-07-14,Application of Dimensionality Reduction in Recommender System -- A Case Study,http://robotics.stanford.edu/~ronnyk/WEBKDD2000/papers/sarwar.pdf,2126.0,Highly cited,,,,,,,,,,,,,,Unknown,,,,,United States of America,,,,,,2024-09-15 17:47,,,,,,Academia,,,,,,,Academia,,,,,,,,,
Credibilty Network,Vision,"Character recognition,Image classification","University College London (UCL),University of Toronto","Geoffrey E. Hinton, Zoubin Ghahramani, Vee Whye Tah",1999-07-01,Learning to Parse Images,https://proceedings.neurips.cc/paper_files/paper/1999/file/5a142a55461d5fef016acfb927fee0bd-Paper.pdf,,,,324.0,"The size of the credibility network is 256-64-4. The 64 middle layer units are meant to encode low level features, while each of the 4 top level units are meant to encode a digit class",5443200.0,=6*324 parameters *2800 datapoints,CEDAR CDROM-1,"700*4 classes = 2800 
The data used is a set of 4400 images of single digits from the classes 2, 3, 4 and 5 derived from the CEDAR CDROM 1 database [17]. Each image has size 16x16. The size of the credibility network is 256-64-4. The 64 middle layer units are meant to encode low level features, while each of the 4 top level units are meant to encode a digit class. We used 700 images of single digits from each classÂ· to train the network. ",2800.0,,,,,,Speculative,"We describe a class of probabilistic models that we call credibility networks. Using parse trees as internal representations of images, credibility networks are able to perform segmentation and recognition simultaneously, removing the need for ad hoc segmentation heuristics. Promising results in the problem of segmenting handwritten digits were obtained. ",,,Unreleased,"United Kingdom of Great Britain and Northern Ireland,Canada",,,,,,2025-02-17 13:39,,,,,,"Academia,Academia",,,,,Unreleased,,"Academia,Academia",,,,,Operation counting,,,,
LeNet-5,Vision,Character recognition,AT&T,"Yann LeCun, LÃ©on Bottou, Yoshua Bengio, Patrick Haffner",1998-11-01,Gradient-based Learning Applied to Document Recognition,http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf,50261.0,"Historical significance,Highly cited",,60000.0,"""[LeNet5] contains 390408 connections, but only 60000 trainable free parameters because of the weight sharing""",2810937600000.0,"""[LeNet5] contains 390408 connections"" = multiply-adds
MNIST - 60,000 data points
20 epochs
390408*60000*6*20=2.810938e+12",MNIST,,60000.0,"The MNIST database contains 60,000 training images and 10,000 testing images (Wikipedia)",,,,,Confident,,,,,United States of America,,,,,,2025-06-16 10:23,,,,,,Industry,,,,,,,Industry,,,,,Operation counting,,,,
LSTM,Language,Language modeling,Technical University of Munich,Sepp Hochreiter ; Jurgen Schmidhuber,1997-11-15,Long short-term memory,https://direct.mit.edu/neco/article-abstract/9/8/1735/6109/Long-Short-Term-Memory?redirectedFrom=fulltext,82231.0,Highly cited,,10504.0,"Table 2

http://www.bioinf.jku.at/publications/older/2604.pdf",31512000000000.0,"""Due to limited computation time, training is stopped after 5 million sequence presentations""

Each sequence has p=100 elements in the long-delay setting.

COMPUTE = PRESENTATIONS * PRESENTATION LENGTH * UPDATE COMPUTE PER TOKEN

5000000*100*6*10,504.0=",,,1273000.0,"Table 8. The rightmost column lists numbers of training sequences required to achieve the stopping
criterion.

This applies to experiment 5 (multiplication)

Sequences have random lengths, on the order of 100-1000 (table 7 )",,,,,Confident,,,,,Germany,,,,,,2025-06-16 10:18,,,,,,Academia,,,,,,,Academia,,,,,Operation counting,,,,
n-gram LM,Language,Language modeling,"University of Cambridge,Carnegie Mellon University (CMU)","P Clarkson, R Rosenfeld",1997-07-01,Statistical language modeling using the CMU-Cambridge toolkit,https://www.semanticscholar.org/paper/Statistical-language-modeling-using-the-toolkit-Clarkson-Rosenfeld/fdf4aa623e4d5b5edaeb873ed8e8b1cef0b59c87,954.0,,,,,,,,,,,,,,Supervised,Unknown,,,,,"United Kingdom of Great Britain and Northern Ireland,United States of America",,,,,,2025-05-09 11:32,,,,,,"Academia,Academia",,,,,,,"Academia,Academia",,,,,,,,,
Deep Blue,Games,Chess,IBM,"Murray Campbell, A. Joseph Hoane Jr., Feng-hsiung Hsu",1997-05-01,Deep Blue,https://www.sciencedirect.com/science/article/pii/S0004370201001291,1992.0,"Historical significance,Highly cited","Defeated Kasparov in 1997, which was a famous AI milestone.",8000.0,"""The new chess chip had a completely redesigned evaluation function, going from around 6400 features to over 8000""",,"The 8000 features were tuned using a mix of human judgment and automated tools using data on chess matches. Unclear how much total ""compute"" went into this.",,,,,,,,,Speculative,"Deep Blue is the chess machine that defeated then-reigning World Chess Champion Garry Kasparov in a six-game match in 1997. There were a number of factors that contributed to this success, including:
â€¢a single-chip chess search engine,

â€¢a massively parallel system with multiple levels of parallelism,

â€¢a strong emphasis on search extensions,

â€¢a complex evaluation function, and

â€¢effective use of a Grandmaster game database.


This paper describes the Deep Blue system, and gives some of the rationale that went into the design decisions behind Deep Blue.",,,Unreleased,United States of America,,,,,,2024-09-05 14:09,,,,,,Industry,,,,,Unreleased,,Industry,,,,,,,,,
AdaBoost.M2 Digit Recognition,Vision,Digit recognition,AT&T,"Yoav Freund, Robert E. Schapire",1996-07-03,Experiments with a New Boosting Algorithm,https://cseweb.ucsd.edu/~yfreund/papers/boostingexperiments.pdf,12977.0,Highly cited,Also listed in Denis Panjuta's List of 100+ AI Algorithms,,,,,,"""The dataset comes from the US Postal Service (USPS)
and consists of 9709 training examples""",9709.0,,,,,,Confident,"In an earlier paper, we introduced a new â€œboostingâ€ algorithm called AdaBoost which, theoretically, can be used to significantly reduce the error of any learning algorithm that con- sistently generates classifiers whose performance is a little better than random guessing. We also introduced the related notion of a â€œpseudo-lossâ€ which is a method for forcing a learning algorithm of multi-label concepts to concentrate on the labels that are hardest to discriminate. In this paper, we describe experiments we carried out to assess how well AdaBoost with and without pseudo-loss, performs on real learning problems.
We performed two sets of experiments. The first set compared boosting to Breimanâ€™s â€œbaggingâ€ method when used to aggregate various classifiers (including decision trees and single attribute- value tests). We compared the performance of the two methods on a collection of machine-learning benchmarks. In the second set of experiments, we studied in more detail the performance of boosting using a nearest-neighbor classifier on an OCR problem.",,,,United States of America,,,,,,2025-05-01 10:42,,,,,,Industry,,,,,,,Industry,,,,,,,,,
System 11,Vision,Face detection,Carnegie Mellon University (CMU),"HA Rowley, S Baluja, T Kanade",1996-06-18,Neural Network-Based Face Detection,https://ieeexplore.ieee.org/document/655647,6011.0,Highly cited,,6452.0,"System 11 is a combination of Network 1 and Network 2

Network 1 has 2095 connections and network 2 has 4357 connections (see table 1)",25859616000.0,"Since there is no parameter sharing, the forward compute is roughly twice that of the number of parameters. We use a 2:1 forward-backward ratio as this is a shallow network, with most connections in the first layer.

Number of passes (Section 2.1):
* ""Nearly 1,050 face examples were gathered from face databases [...]""
* ""Fifteen face examples are generated for the training set from each original image""

Training loop:
1. ""initial set of nonface images by generating 1,000 random images""
2. Train (presumably on whole set)
3. Run + collect false positives
4. ""Select up to 250 of these subimages [...] and add them into the training set [...] Go to step 2""

""A typical training run selects approximately 8,000 nonface images ""

Selecting 8,000 nonface images implies 8000/250 = 32 loops.

Assuming compute is 3 * N * D, we have
* Loop 1: D = 15*1050 + 1000
* Loop 2: D = 15*1050 + 1000 + 250
* So on.

Hence D overall is 32*(15*1050 + 1000) + 250*32/2*(32+1) = 668,000.

Hence compute = 3 * 12904 * 668e3 = 25859616000",,,9050.0,"""A typical training
run selects approximately 8000 non-face images from the
146,212,178 subimages that are available at all locations
and scales in the training scenery images.""

""Nearly 1050 face examples were gathered from face databases at CMU and Harvard [...] In the training set,15 face examples are generated from each
original image [...]""

""Create an initial set of non-face images by generating
1000 images with random pixel intensities""",,,,,Confident,,,,,United States of America,,,,,,2025-06-16 09:46,,,,,,Academia,,,,,,,Academia,,,,,Operation counting,,,,
MUSIC perceptron,Vision,Image completion,,M. Kocheisen; U.A. Muller; G. Troster,1996-06-03,A neural network for grey level and color correction used in photofinishing,https://ieeexplore.ieee.org/document/549237,,Historical significance,,13607.0,230*55+56*15+16*6+7*3=13607 (Figure 2),881733600000.0,"2*13607*3*10800000=881733600000=8.8e11
Training steps: 400*27000=10800000
""After 400 epochs the error of the network""",,,27000.0,"â€œThe training experiments were carried out on a database of 30,000 photos. Therefor the database was split into ten sets. Nine of them were used for the training and one for the testing.â€",,,,,Confident,"The application of a multilayer perceptron for color and gray level correction in the field of photofinishing is presented. It is shown, that a neural network can improve the overall performance of a state of the art photo printer. The improved correction ability will reduce the number of unsalable pictures and thus lowers the production costs for the photo laboratory. The training experiments were carried out on a database of 30,000 photos using the MUSIC parallel supercomputer. The MUSIC system made it possible, for the first time, to process this large database in a reasonable time.",400.0,,,,,,,,,2025-02-17 13:34,,,,,,,,,,,,,,,,,,Operation counting,,,,
LISSOM,Vision,Digit recognition,University of Texas at Austin,"Yoonsuck Choe, Joseph Sirosh, R. Miikkulainen",1995-11-27,Laterally Interconnected Self-Organizing Maps in Hand-Written Digit Recognition,https://www.semanticscholar.org/paper/Laterally-Interconnected-Self-Organizing-Maps-in-Choe-Sirosh/785f5facc76538ceba6f6b9e2d7b641d322e9854,,"SOTA improvement,Historical significance",,432800.0,"Total connections 32*32*20*20+20*20*48+20*20*10=432800
Input: 32*32, Lissom: 20*20, Output: 10 (Figure 1a), up to 48 lateral connections per Lissom neuron (Figure 1b)
",195544800000.0,"Lissom connections: 32*32*20*20+20*20*48=428800
Lissom compute: 2*428800*3*38*2000=195532800000=1.96e11
Perceptron connections: 20*20*10=4000
Perceptron compute: 2*4000*3*500*1700=20400000000=2e10
Total compute: 195532800000+12000000=195544800000=1.96e11
""LISSOM was trained with 2000 patterns""
""The initial self-organizing map was formed in 8 epochs over the training set, gradually reducing the neighborhood radius from 20 to 8. The lateral connections were then added to the system, and over another 30 epochs,""
""Of these, 1700 were used to train the perceptron layer, ""
""After the SOM and LISSOM maps were organized, a complete set of activation patterns on the two maps were collected. These patterns then formed the training input for the perceptron layer. Two separate versions were each trained for 500 epochs,""

",,,2000.0,"""LISSOM was trained with 2000 patterns""",,,,,Likely,"An application of laterally interconnected self-organizing maps (LISSOM) to handwritten digit recognition is presented. The lateral connections learn the correlations of activity between units on the map. The resulting excitatory connections focus the activity into local patches and the inhibitory connections decorrelate redundant activity on the map. The map thus forms internal representations that are easy to recognize with e.g. a perceptron network. The recognition rate on a subset of NIST database 3 is 4.0% higher with LISSOM than with a regular Self-Organizing Map (SOM) as the front end, and 15.8% higher than recognition of raw input bitmaps directly. These results form a promising starting point for building pattern recognition systems with a LISSOM map as a front end.",38.0,,,United States of America,,,,,,2025-02-19 12:33,,,,,,Academia,,,,,,,Academia,,,,,Operation counting,,,,
Multi-cause Binary Clustering,Other,Miscellaneous image analysis,Xerox,Eric Saund,1995-01-01,A Multiple Cause Mixture Model for Unsupervised Learning,https://ieeexplore.ieee.org/document/6795568,176.0,,,,,,,,,,,,,,,Unknown,,,,,United States of America,,,,,,2024-09-16 00:40,,,,,,Industry,,,,,,,Industry,,,,,,,,,
JPMAX,Vision,Image representation,,Suzanna Becker,1994-12-02,JPMAX: Learning to Recognize Moving Objects as a Model-fitting Problem,https://proceedings.neurips.cc/paper_files/paper/1994/hash/4b0250793549726d5c1ea3906726ebfe-Abstract.html,,Historical significance,,4446.0,"Inputs are 12x12 pixels (Figure 3). They first train the architecture in Figure 2 a), then freeze it and train the additional layer in Figure 3 b).

Figure 2 a): 2*(12*12*15) + 2*15 = 4,350
Figure 2 b): 2*(12*12*15) + 2*15 + 2*(15*3) + 2*3 = 4,446",80828280.0,"Training 2a):  ""The learning took about 3000 iterations of steepest descent"" Assuming each iteration refers to a single image.
6 * 4446 * 3000 = 80,028,000

Training 2b): ""While keeping the first layer of weights frozen, this network was trained using exactly the same cost function as the first layer for about 30 iterations using a gradient-based learning method.""

6 * 4446 * 30 = 800,280

Total: 80,828,280",,,1500.0,â€œFigure 3: 10 of the 1500 training patternsâ€,,,,,Speculative,"Unsupervised learning procedures have been successful at low-level feature extraction and preprocessing of raw sensor data. So far, however, they have had limited success in learning higher-order representations, e.g., of objects in visual images. A promising ap(cid:173) proach is to maximize some measure of agreement between the outputs of two groups of units which receive inputs physically sep(cid:173) arated in space, time or modality, as in (Becker and Hinton, 1992; Becker, 1993; de Sa, 1993). Using the same approach, a much sim(cid:173) pler learning procedure is proposed here which discovers features in a single-layer network consisting of several populations of units, and can be applied to multi-layer networks trained one layer at a time. When trained with this algorithm on image sequences of moving geometric objects a two-layer network can learn to perform accurate position-invariant object classification.",,,,,,,,,,2025-02-18 11:12,,,,,,,,,,,,,,,,,,Operation counting,,,,
Predictive Coding NN,Language,Language modeling,Technical University of Munich,"J. Schmidhuber, Stefan Heil",1994-12-02,Predictive Coding with Neural Nets: Application to Text Compression,https://proceedings.neurips.cc/paper/1994/file/5705e1164a8394aace6018e27d20d237-Paper.pdf,,Historical significance,,206910.0,"5*80*430+430+430*80+80=206910
""P has nk input units and k output units. n is called the ""time-window size""
""Note that the time-window was quite small (n = 5).""
""alphabet consisted of k = 80 possible characters""
""P had 430 hidden units""",18621900000000.0,"2*206910*3*15000000=18621900000000=1.86e13
""The training phase consisted of 25 sweeps through the training set""",,,600000.0,"Training dataset: 15000*40=600000
""The training set for the predictor was given by a set of 40 articles from the newspaper Miinchner M erkur, each containing between 10000 and 20000 characters.""",,,,,Confident,"To compress text files, a neural predictor network P is used to approximate the conditional probability distribution of possible ""next characters"", given n previous characters. P's outputs are fed into standard coding algorithms that generate short codes for characters with high predicted probability and long codes for highly unpredictable characters. Tested on short German newspaper articles, our method outperforms widely used Lempel-Ziv algorithms (used in UNIX functions such as ""compress"" and ""gzip"").",25.0,,,Germany,,,,,,2025-02-18 11:13,,,,,,Academia,,,,,,,Academia,,,,,Operation counting,,,,
Mixture of linear models,Vision,Image classification,,"Geoffrey E. Hinton, M. Revow, P. Dayan",1994-12-02,Recognizing Handwritten Digits Using Mixtures of Linear Models,https://www.semanticscholar.org/paper/Recognizing-Handwritten-Digits-Using-Mixtures-of-Hinton-Revow/9dea20c1e5bbb1f543ff08113ffde5380c679f1f,,Historical significance,,384000.0,"â€œIn the example we describe, 7000 training images are sufficient to fit 384,000 parametersâ€œ",453600000000.0,"0.3*12*60*60*35000000=453600000000=4.54e11
Assuming a utilization of 0.3 and interpreting ""overnight"" as 12 hours.
â€œthe training procedure is fast enough to do the fitting overnight on an R4400-based machine. â€œ
R4400 has 35MFLOPS (â€œCompare this to the 200MHz R4400 which is rated at about 35MFLOPSâ€, http://www.sgidepot.co.uk/perf.html)",,,7000.0,"""7000 training images are sufficient""",12.0,"""the training procedure is fast enough to do the fitting overnight on an R4400-based machine.""",,,Likely,"We construct a mixture of locally linear generative models of a collection of pixel-based images of digits, and use them for recognition. Different models of a given digit are used to capture different styles of writing, and new images are classified by evaluating their log-likelihoods under each model. We use an EM-based algorithm in which the M-step is computationally straightforward principal components analysis (PCA). Incorporating tangent-plane information [12] about expected local deformations only requires adding tangent vectors into the sample covariance matrices for the PCA, and it demonstrably improves performance.",,,,,,,,,,2025-02-18 11:13,,,,,,,,,,,,,,,,,,Hardware,,,,
NeuroChess,Games,Chess,,S. Thrun,1994-12-02,Learning to Play the Game of Chess,https://www.semanticscholar.org/paper/Learning-to-Play-the-Game-of-Chess-Thrun/4bc7a6dcb9e0e6c7a26800532e2a00f5572eea47,,"Historical significance,Highly cited",,72251.0,"""Prior to learning an evaluation function, the model M (175 input, 165 hidden, and 175 output units)"" = 58,090 parameters
""NeuroChess then learns an evaluation network V (175 input units, 0 to 80 hidden units, and one output units)."" = 14,161 parameters
Total: 58,090 + 14,161 = 72,251",858730812676.0,"Lower bound: 0.3*2*24*60*60*1400000=72576000000=7.26e10
Upper bound: 0.3*14*24*60*60*1400000*20=10160640000000=1.02e13
Geometric mean: 858730812676=8.59e11 (speculative)
""Thus far, experiments lasted for 2 days to 2 weeks on I to 20 SUN Sparc Stations. ""
SparcStation has 1.4 MFLOPS (https://ieeexplore.ieee.org/document/63671)
",,,120000.0,"""is trained using a database of 120,000 expert games.""",,,,,Speculative,"This paper presents NeuroChess, a program which learns to play chess from the final outcome of games. NeuroChess learns chess board evaluation functions, represented by artificial neural networks. It integrates inductive neural network learning, temporal differencing, and a variant of explanation-based learning. Performance results illustrate some of the strengths and weaknesses of this approach.",,,,,,,,,,2025-02-19 12:29,,,,,,,,,,,,,,,,,,Hardware,,,,
Ceramic-MLP,Materials science,Pattern classification,Sapienza UniversitÃ  di Roma,"G. Bonifazi, P. Burrascano",1994-01-07,Ceramic powder characterization by multilayer perceptron (MLP) data compression and classification,https://www.sciencedirect.com/science/article/abs/pii/S0921883108605506,,Historical significance,,1888.0,"Parameters: 100*16 + 16*16 + 16*2 = 1888
Architecture: ""The topology of the classifier was X-Y-Y-2, where X is the number of input components, Y is the number of neurons in each hidden layer and the number of neurons in the output layer is two, which is the number of classes. The two hidden layers were considered to have the same number of nodes for simplification purposes. ""
Input size: ""Each pattern consists of a 10 x 10 pixel sub-image.""
Hidden size: ""Experiments have been made on networks with 6, 9, 12 and 16 hidden nodes. """,4531200000.0,"Compute estimate: 2*1888*3*400000=4531200000=4.53e9
Training steps: ""In Fig. 6 we report the classification results obtained on the testing set in the 12 and 16 component compressed data after 400000 training iterations""",,,80.0,"After the pre-processing phase, a training set of 80 patterns and a testing set of 64 patterns were available.",,,,,Likely,"A neural network approach for pattern classification has been explored in the present paper as part of the recent resurgence of interest in this area. Our research has focused on how a multilayer feedforward structure performs in the particular problem of particle characterization. The proposed procedure, after suitable data preprocessing, consists of two distinct phases: in the former, a feedforward neural network is used to obtain an image data compression. In the latter, a neural classifier is trained on the compressed data. All the tests have been conducted on a sample constituted by two different typologies of ceramic particles, each characterized by a different microstructure. The sample image of different particles acquired and directly digitalized by scanning electron microscopy has been processed in order to achieve the best conditions to obtain the boundary profile of each particle. The boundary is thus assumed to be representative of the morphological characteristics of the ceramic products. Using the neural approach, a classification accuracy as high as 100% on a training set of 80 sub-images was achieved. These networks correctly classified up to 96.9% of 64 testing patterns not contained in the training set.",5000.0,,,Italy,,,,,,2025-02-18 11:10,,,,,,Academia,,,,,,,Academia,,,,,Operation counting,,,,
ANN Eye Tracker,Vision,Miscellaneous image analysis,,"S. Baluja, D. Pomerleau",1993-11-29,Non-Intrusive Gaze Tracking Using Artificial Neural Networks,https://www.semanticscholar.org/paper/Non-Intrusive-Gaze-Tracking-Using-Artificial-Neural-Baluja-Pomerleau/574c0cf98825bf09b0ab7bbfe9ba89cd6090745e,,Historical significance,,5620.0,"15*15*20+20+50*10*2+100=5620
Hidden layer is split, 15*15 image input, 2*50 output neurons (see Figure 2)
Hidden size up to 20 neurons (""This architecture was used with varying numbers of hidden units in the single, divided, hidden layer; experiments with 10, 16 and 20 hidden units were performed. "")",17534400000.0,"2*5620*3*520000=17534400000
Training examples: 2000*260=520000
""As mentioned before, 2000 image/position pairs were gathered for training""
""All of the networks described in this paper are trained with the same parameters for 260 epochs""",,"""2000 image/position pairs were gathered for training""",2000.0,,0.6,"""Training the 8x2 hidden layer network using the 15x40 input retina, with 2000 images, takes approximately 30-40 minutes on a Sun SPARC 10 machine. """,,Supervised,Confident,"We have developed an artificial neural network based gaze tracking system which can be customized to individual users. A three layer feed forward network, trained with standard error back propagation, is used to determine the position of a user''s gaze from the appearance of the user''s eye. Unlike other gaze trackers, which normally require the user to wear cumbersome headgear, or to use a chin rest to ensure head immobility, our system is entirely non-intrusive. Currently, the best intrusive gaze tracking systems are accurate to approximately 0.75 degrees. In our experiments, we have been able to achieve an accuracy of 1.5 degrees, while allowing head mobility. In its current implementation, our system works at 15 hz. In this paper we present an empirical analysis of the performance of a large number of artificial neural network architectures for this task. Suggestions for further explorations for neurally based gaze trackers are presented, and are related to other similar artificial neural network applications such as autonomous road following.",260.0,,,,,,,,,2025-05-09 11:32,,,,,,,,,,,,,,,,,,Operation counting,,,,
Siamese-TDNN,Vision,Image classification,Bell Laboratories,"J. Bromley, James W. Bentz, L. Bottou, Isabelle M Guyon, Yann LeCun, C. Moore, Eduard SÃ¤ckinger, Roopak Shah",1993-08-01,"Signature Verification using a ""Siamese"" Time Delay Neural Network",https://www.semanticscholar.org/paper/Signature-Verification-Using-A-%22Siamese%22-Time-Delay-Bromley-Bentz/997dc5d9a058753f034422afe7bd0cc0b8ad808b,,"Historical significance,Highly cited",,744.0,"""The input is 8 by 200 units, the first convolutional layer is 6 by 192 units with each unit's receptive field covering 8 by 9 units of the input. The first averaging layer is 6 by 64 units, the second convolution layer is 4 by 57 with 6 by 8 receptive fields and the second averaging layer is 4 by 19""
""Two separate sub-networks based on Time Delay Neural Networks (Lang and Hinton, 1988, Guyon et al. 1990) act on each input pattern to extract features,""
""All weights could be learnt, but the two sub-networks were constrained to have identical weights.""
L1: H=1, W=200, C=8, K=9, D=6
L2: H=1, W=64, C=6, K=8, D=4
Parameters:  7*9*8+5*8*6=744",12869570138112.0,"8073216*3*7701*69=12869570138112=1.29e13
Forward pass flop: 2*(2*200*200*8*6+2*64*64*6*4)=8073216
""We used up to 7,701 signature pairs""
Epochs: 69 (Table 1)
",,"""In total, 219 people signed between 10 and 20 signatures each, 145 signed genuines, 74 signed forgeries."" ""We used up to 7,701 signature pairs""",7701.0,"""We used up to 7,701 signature pairs""",,,,,Likely,"This paper describes an algorithm for verification of signatures written on a pen-input tablet. The algorithm is based on a novel, artificial neural network, called a ""Siamese"" neural network. This network consists of two identical sub-networks joined at their outputs. During training the two sub-networks extract features from two signatures, while the joining neuron measures the distance between the two feature vectors. Verification consists of comparing an extracted feature vector with a stored feature vector for the signer. Signatures closer to this stored representation than a chosen threshold are accepted, all other signatures are rejected as forgeries.",69.0,,,United States of America,,,,,,2025-02-19 12:29,,,,,,Industry,,,,,,,Industry,,,,,Operation counting,,,,
Futures trading net,Other,Regression,,"Lonnie Hamm, B. Wade Brorsen, and Ramesh Sharda",1993-01-01,Futures trading with a neural network,https://legacy.farmdoc.illinois.edu/nccc134/conf_1993/pdf/confp25-93.pdf,,,,,,5292000000.0,"0.3*35*60*60*140000=5292000000=5.29e9
â€œTraining on the data from 1979-1985 took approximately 35 hoursâ€
â€œThe neural networks were trained on a 386Dx-25mhz processorâ€
0.14 MFLOPS taken from this benchmark: https://www.vogons.org/viewtopic.php?t=46350",,,,,35.0,,,,Speculative,,,,,,,,,,,2025-05-01 10:42,,,,,,,,,,,,,,,,,,Hardware,,,,
Boosting,Vision,Digit recognition,Bell Laboratories,"H. Drucker, R. Schapire, Patrice Y. Simard",1992-11-30,Improving Performance in Neural Networks Using a Boosting Algorithm,https://www.semanticscholar.org/paper/Improving-Performance-in-Neural-Networks-Using-a-Drucker-Schapire/77b5185dafb9e5b884a677a32713e54c253a4e0b,,Historical significance,,2578.0,"â€œThe network has 4645 neurons, 2578 different weights, and 98442 connections.â€œ",,,,,9709.0,â€œdivided into 9709 training examples and 2007 validation samples.â€,,,,,Likely,"A boosting algorithm converts a learning machine with error rate less than 50% to one with an arbitrarily low error rate. However, the algorithm discussed here depends on having a large supply of independent training samples. We show how to circumvent this problem and generate an ensemble of learning machines whose performance in optical character recognition problems is dramatically improved over that of a single network. We report the effect of boosting on four databases (all handwritten) consisting of 12,000 digits from segmented ZIP codes from the United State Postal Service (USPS) and the following from the National Institute of Standards and Testing (NIST): 220,000 digits, 45,000 upper case alphas, and 45,000 lower case alphas. We use two performance measures: the raw error rate (no rejects) and the reject rate required to achieve a 1% error rate on the patterns not rejected. Boosting improved performance in some cases by a factor of three.",,,,United States of America,,,,,,2024-09-05 14:08,,,,,,Industry,,,,,,,Industry,,,,,,,,,
Cancer drug mechanism prediction,Medicine,Drug discovery,National Cancer Institute,"John N. Weinstein, Kurt W. Kohn, Michael R. Grever, Vellarkad N.
Viswanadhan, Lawrence V. Rubinstein, Anne P. Monks, Dominic A. Scudiero, Lester Welch, Antonis D. Koutsoukos, August J. Chiausa, Kenneth D. Paull",1992-10-16,Neural computing in cancer drug development: predicting mechanism of action,https://pubmed.ncbi.nlm.nih.gov/1411538/,,Historical significance,,594.0,"â€œThe network shown has 60 input PEs, one for each cell line, and 6 output PEsâ€œ
â€œNeural networks with three to nine hidden layer PEs usedâ€
9*60 + 6*9 = 594",53460000.0,"2*594*3*15000=53460000=5.35e7
â€œThe extent of training was 15,000 presentationsâ€œ",,,,,,,,,Likely,"Described here are neural networks capable of predicting a drug's mechanism of action from its pattern of activity against a panel of 60 malignant cell lines in the National Cancer Institute's drug screening program. Given six possible classes of mechanism, the network misses the correct category for only 12 out of 141 agents (8.5 percent), whereas linear discriminant analysis, a standard statistical technique, misses 20 out of 141 (14.2 percent). The success of the neural net indicates several things. (i) The cell line response patterns are rich in information about mechanism. (ii) Appropriately designed neural networks can make effective use of that information. (iii) Trained networks can be used to classify prospectively the more than 10,000 agents per year tested by the screening program. Related networks, in combination with classical statistical tools, will help in a variety of ways to move new anticancer agents through the pipeline from in vitro studies to clinical application.",,,,United States of America,,,,,,2025-02-17 15:58,,,,,,Government,,,,,,,Government,,,,,Operation counting,,,,
Golem,Biology,Protein folding prediction,Alan Turing Institute,"S. Muggleton, Ross D. King, M. J. Sternberg",1992-10-01,Protein secondary structure prediction using logic-based machine learning.,https://www.semanticscholar.org/paper/Protein-secondary-structure-prediction-using-Muggleton-King/9f744e48091a24b569435d070920e60db45f4fdc,,"Historical significance,SOTA improvement",,,,,,,"""The input to the program consisted of 12 non-homologous proteins (1612 residues)""",1612.0,Table 1,,,,,Confident,"Many attempts have been made to solve the problem of predicting protein secondary structure from the primary sequence but the best performance results are still disappointing. In this paper, the use of a machine learning algorithm which allows relational descriptions is shown to lead to improved performance. The Inductive Logic Programming computer program, Golem, was applied to learning secondary structure prediction rules for alpha/alpha domain type proteins. The input to the program consisted of 12 non-homologous proteins (1612 residues) of known structure, together with a background knowledge describing the chemical and physical properties of the residues. Golem learned a small set of rules that predict which residues are part of the alpha-helices--based on their positional relationships and chemical and physical properties. The rules were tested on four independent non-homologous proteins (416 residues) giving an accuracy of 81% (+/- 2%). This is an improvement, on identical data, over the previously reported result of 73% by King and Sternberg (1990, J. Mol. Biol., 216, 441-457) using the machine learning program PROMIS, and of 72% using the standard Garnier-Osguthorpe-Robson method. The best previously reported result in the literature for the alpha/alpha domain type is 76%, achieved using a neural net approach. Machine learning also has the advantage over neural network and statistical methods in producing more understandable results.",,,,United Kingdom of Great Britain and Northern Ireland,,,,,,2025-04-30 10:04,,,,,,Government,,,,,,,Government,,,,,,,,,
Fuzzy NN,Speech,Speech recognition,Indian Statistical Institute,"SK Pal, S Mitra",1992-09-01,"Multilayer perceptron, fuzzy sets, and classification",https://ieeexplore.ieee.org/document/159058,1223.0,Highly cited,,1166.0,"Table II: ""he neural network has three hidden layers, with m hidden nodes in each layer"", m = 20, input dim. = 9, output dim. = 6

9*20+20*20+20*20+6*20+66=1166",1403117760.0,1166 params * 2 FLOP/param * (3 for forward + backward pass) * 460 epochs * 436 examples,,,436.0,"""The above-mentioned algorithm was tested on a set of 871 Indian Telugu vowel sounds"" and 50% of the dataset was used. 871*0.5 ~= 436",,,,,Confident,,,,,India,,,,,,2025-06-16 06:47,,,,,,Academia,,,,,,,Academia,,,,,Operation counting,,,,
TD-Gammon,Games,Backgammon,IBM,G Tesauro,1992-05-01,Practical Issues in Temporal Difference Learning,https://papers.nips.cc/paper/1991/file/68ce199ec2c5517597ce0a4d89620f55-Paper.pdf,1344.0,Highly cited,,25000.0,"""The best performance was obtained with a network containing 80 hidden units and over 25,000 weights.""",18232157622832.7,"Extracted from AI and Compute (https://openai.com/blog/ai-and-compute/) charts by using https://automeris.io/WebPlotDigitizer/.

OpenAI estimate: 1.8e13

Hardware estimate (likely overestimates due to simulation effort)
""on an IBM RS/6000 workstation, the smallest network was trained in several hours, while the largest net required two weeks of simulation time.""
IBM RS/6000 achieves 1.5 GFLOPS on Linpack (https://link.springer.com/rwe/10.1007/978-0-387-09766-4_232) 
14*24*60*60*0.5*1500000000=9.072e+14

Operation counting estimate: 
Forward FLOP: 50000
Each legal move had to be evaluated separately, assuming an average of 10 move options (+2 for backward passes of the chosen move):
50000*12*300000*21=3.78e+12

Keeping the OpenAI estimate as the median estimate. ",,,6300000.0,"""This network was trained
for over 300,000 training games""

Each backgammon game has an avg of around 21 movements
https://www.bkgm.com/rgb/rgb.cgi?view+712",,,,,Speculative,,,,,United States of America,,,,,,2025-06-16 09:36,,,,,,Industry,,,,,,,Industry,,,,,"Third-party estimation,Operation counting,Hardware",,,,
Weight Decay,Speech,Speech synthesis,,"A. Krogh, J. Hertz",1991-12-02,A Simple Weight Decay Can Improve Generalization,https://www.semanticscholar.org/paper/A-Simple-Weight-Decay-Can-Improve-Generalization-Krogh-Hertz/48e1de7d085808004d5f0493d486669a3d2930b5,,"Highly cited,Historical significance",,8386.0," 7*26*40+40+40*26+26=8386
""The network had 7 x 26 input units, 40 hidden units and 26 output units""",75474000000.0,"2*8386*3*1500000=75474000000=7.55e10
""It was trained on 400 to 5000 random words from the data base of around 20.000 words,""
""The top full line corresponds to the generalization error after 300 epochs""
",,NetTalk dataset of 20000 words,5000.0,"""It was trained on 400 to 5000 random words from the data base of around 20.000 words,""",,,,,Confident,"It has been observed in numerical simulations that a weight decay can improve generalization in a feed-forward neural network. This paper explains why. It is proven that a weight decay has two effects in a linear network. First, it suppresses any irrelevant components of the weight vector by choosing the smallest vector that solves the learning problem. Second, if the size is chosen right, a weight decay can suppress some of the effects of static noise on the targets, which improves generalization quite a lot. It is then shown how to extend these results to networks with hidden layers and non-linear units. Finally the theory is confirmed by some numerical simulations using the data from NetTalk.",300.0,,,,,,,,,2025-02-17 15:58,,,,,,,,,,,,,,,,,,Operation counting,,,,
RAAM,Other,Representation learning,,Jordan B. Pollack,1990-11-01,Recursive Distributed Representations,https://www.sciencedirect.com/science/article/abs/pii/000437029090005K?via%3Dihub,,Highly cited,,1536.0,"Largest model:
""A 48-16-48 RAAM learned to construct representations ""
Parameters 48*16*2 = 1536",,,,"Trained on sentence fragments, see Figure 10",29.0,29 sentence fragments (Figure 10),,,,,Confident,"A longstanding difficulty for connectionist modeling has been how to represent variable-sized recursive data structures, such as trees and lists, in fixed-width patterns. This paper presents a connectionist architecture which automatically develops compact distributed representations for such compositional structures, as well as efficient accessing mechanisms for them. Patterns which stand for the internal nodes of fixed-valence trees are devised through the recursive use of backpropagation on three-layer auto-associative encoder networks. The resulting representations are novel, in that they combine apparently immiscible aspects of features, pointers, and symbol structures. They form a bridge between the data structures necessary for high-level cognitive tasks and the associative, pattern recognition machinery provided by neural networks.",,,,,,,,,,2024-09-05 14:08,,,,,,,,,,,,,,,,,,,,,,
SexNet compression,Vision,Image representation,,"B. Golomb, D. T. Lawrence, T. Sejnowski",1990-10-01,SEXNET: A Neural Network Identifies Sex From Human Faces,https://www.semanticscholar.org/paper/SEXNET%3A-A-Neural-Network-Identifies-Sex-From-Human-Golomb-Lawrence/cbf90aa78fea0c8a1028705d92bc4bc7808ddeeb,,Historical significance,,72940.0,"900*40*2+40+900=72940
â€œImages sampled at 30x30 were compressed using a 900x40x900 fully connected back-propagation networkâ€",78775200000.0,"2*72940*3*90*2000=78775200000
â€œThe compression network trained for 2000 runs on each of 90 facesâ€",,,90.0,â€œThe compression network trained for 2000 runs on each of 90 facesâ€,,,,Supervised,Confident,"Sex identification in animals has biological importance. Humans are good at making this determination visually, but machines have not matched this ability. A neural network was trained to discriminate sex in human faces, and performed as well as humans on a set of 90 exemplars. Images sampled at 30Ã—30 were compressed using a 900Ã—40Ã—900 fully-connected back-propagation network; activities of hidden units served as input to a back-propagation ""SexNet"" trained to produce values of 1 for male and 0 for female faces. The network's average error rate of 8.1% compared favorably to humans, who averaged 11.6%. Some SexNet errors mimicked those of humans.",2000.0,,,,,,,,,2025-05-09 11:32,,,,,,,,,,,,,,,,,,Operation counting,,,,
SexNet classification,Vision,Image classification,,"B. Golomb, D. T. Lawrence, T. Sejnowski",1990-10-01,SEXNET: A Neural Network Identifies Sex From Human Faces,https://www.semanticscholar.org/paper/SEXNET%3A-A-Neural-Network-Identifies-Sex-From-Human-Golomb-Lawrence/cbf90aa78fea0c8a1028705d92bc4bc7808ddeeb,,"Historical significance,Highly cited",,1640.0,Largest classification model: 40*40 + 40=1640 (Figure 2),,,,,80.0,"â€œEach training on a different 80 faces, leaving a distinct set of 10 untrained faces for testingâ€",,,,,Likely,"Sex identification in animals has biological importance. Humans are good at making this determination visually, but machines have not matched this ability. A neural network was trained to discriminate sex in human faces, and performed as well as humans on a set of 90 exemplars. Images sampled at 30Ã—30 were compressed using a 900Ã—40Ã—900 fully-connected back-propagation network; activities of hidden units served as input to a back-propagation ""SexNet"" trained to produce values of 1 for male and 0 for female faces. The network's average error rate of 8.1% compared favorably to humans, who averaged 11.6%. Some SexNet errors mimicked those of humans.",,,,,,,,,,2024-09-05 14:08,,,,,,,,,,,,,,,,,,,,,,
ISR network,Vision,Character recognition,Stanford University,"James Keeler, David Rumelhart, Wee Leow",1990-10-01,Integrated Segmentation and Recognition of Hand-Printed Numerals,https://papers.nips.cc/paper_files/paper/1990/hash/e46de7e1bcaaced9a54f1e9d0d2f800d-Abstract.html,,Historical significance,,,,,,,,9000.0,"â€œWe used a training and test set of about 9,000 and 1,800 characters respectively. â€œ",,,,,Confident,"Neural network algorithms have proven useful for recognition of individ(cid:173) ual, segmented characters. However, their recognition accuracy has been limited by the accuracy of the underlying segmentation algorithm. Con(cid:173) ventional, rule-based segmentation algorithms encounter difficulty if the characters are touching, broken, or noisy. The problem in these situations is that often one cannot properly segment a character until it is recog(cid:173) nized yet one cannot properly recognize a character until it is segmented. We present here a neural network algorithm that simultaneously segments and recognizes in an integrated system. This algorithm has several novel features: it uses a supervised learning algorithm (backpropagation), but is able to take position-independent information as targets and self-organize the activities of the units in a competitive fashion to infer the positional information. We demonstrate this ability with overlapping hand-printed numerals.",,,,United States of America,,,,,,2024-09-05 14:08,,,,,,Academia,,,,,,,Academia,,,,,,,,,
Bankruptcy-NN,Other,Binary classification,,"M. Odom, R. Sharda",1990-06-17,A neural network model for bankruptcy prediction,https://www.semanticscholar.org/paper/A-neural-network-model-for-bankruptcy-prediction-Odom-Sharda/ead9fa02902850a7418fb5ba720f3d9d8ab2f88b,,"Historical significance,Highly cited",,36.0,"""The input layer consisted of the five nodes, one for each of the ratios. The hidden layer consisted of 5 node.;. The output layer consisted of only one neuron""
30 weights + 6 biases = 36",3059337600.0," 2*36*3*74*191400=3,059,337,600=3.06e9
""Convergence was reached after 191,400 iterations""",,,370.0,"""The first (training) subsample of 74 firms data""
5 inputs per firm
74*5 = 370",24.0,,,,Confident,"A neural network model is developed for prediction of bankruptcy, and it is tested using financial data from various companies. The same set of data is analyzed using a more traditional method of bankruptcy prediction, multivariate discriminant analysis. A comparison of the predictive abilities of both the neural network and the discriminant analysis method is presented. The results show that neural networks might be applicable to this problem",191400.0,,,,,,,,,2025-05-02 10:08,,,,,,,,,,,,,,,,,,Operation counting,,,,
Zip CNN,Vision,Character recognition,"AT&T,Bell Laboratories",Y. LeCun B. Boser J. S. Denker D. Henderson R. E. Howard W. Hubbard L. D. Jackel,1989-12-01,Backpropagation applied to handwritten zip code recognition,https://ieeexplore.ieee.org/document/6795724,10587.0,Highly cited,,9760.0,"""In summary, the network has 1256 units, 64,660 connections, and 9760 independent parameters""",1496338054440.0,"Its a deep CNN so we assume a backward-forward ratio of 2:1
 2*64660*3*23*167693=1496338054440
""The network was trained for 23
passes through the training set (167,693 pattern presentations).""",Buffalo zips,"""The data base used to train and test the network consists of 9298 segmented numerals digitized from handwritten zip codes
that appeared on U.S. mail passing through the Buffalo, NY post office.
Examples of such images are shown in Figure 1. The digits were written
by many different people, using a great variety of sizes, writing styles,
and instruments, with widely varying amounts of care; 7291 examples
are used for training the network and 2007 are used for testing the generalization performance. One important feature of this data base is that
both the training set and the testing set contain numerous examples that
are ambiguous, unclassifiable, or even misclassified. """,7291.0,"The digits were written
by many different people, using a great variety of sizes, writing styles,
and instruments, with widely varying amounts of care; 7291 examples
are used for training the network and 2007 are used for testing the generalization performance",,,,,Confident,,,,,"United States of America,United States of America",,,,,,2025-06-17 08:55,,,,,,"Industry,Industry",,,,,,,"Industry,Industry",,,,,Operation counting,,,,
Innervator,Mathematics,Pattern classification,"Stanford University,California Institute of Technology","Geoffrey Miller, Peter Todd, and Shailesh Hegde",1989-12-01,Designing neural networks using genetic algorithms,https://www.researchgate.net/publication/220885651_Designing_Neural_Networks_using_Genetic_Algorithms,1132.0,Highly cited,,10.0,Each net has 5 units,120000000.0,10 params * 6 FLOP/param/pass * 4 datapoints * 1000 epochs * 50 individuals * 10 generations,,,4.0,,,,,,Confident,,,,,"United States of America,United States of America",,,,,,2025-06-16 06:35,,,,,,"Academia,Academia",,,,,,,"Academia,Academia",,,,,Operation counting,,,,
ALVINN,Driving,Self-driving car,Carnegie Mellon University (CMU),DA Pomerleau,1989-12-01,ALVINN: an autonomous land vehicle in a neural network,https://proceedings.neurips.cc/paper/1988/hash/812b4ba287f5ee0bc9d43bbf5bbe87fb-Abstract.html,1962.0,Highly cited,,36627.0,"1217*29 + 29*46 =36627
â€œEach of these 1217 input units is fully connected to the hidden layer of 29 units, which is in turn fully connected to the output layer. The output layer consists of 46 units, divided into two groups.â€",10548576000.0,"2 * 36627 * 3 * 40 * 1200 = 10548576000 = 1.05e10
36627 parameters
""After 40 epochs of training on the 1200 simulated road snapshots""",Road snapshots,,1200.0,"""Training involves first creating a set of 1200 road snapshots depicting roads with a wide variety of retinal orientations and positions, under a variety of lighting conditions and with realistic noise levels""",,,,,Confident,,40.0,,,United States of America,,,,,,2025-06-16 06:38,,,,,,Academia,,,,,,,Academia,,,,,Operation counting,,,,
Speaker-independent vowel classification,Speech,Speech recognition,University of Washington,"L. Atlas, R. Cole, J. Connor, M. El-Sharkawi, R. Marks, Y. Muthusamy, E. Barnard",1989-11-27,Performance Comparisons Between Backpropagation Networks and Classification Trees on Three Real-World Applications,https://www.semanticscholar.org/paper/Performance-Comparisons-Between-Backpropagation-and-Atlas-Cole/e42d2b89fcb4a1a3dfa63408f424f76975ed1e1b,,Historical significance,,3040.0,"â€œThe MLP consisted of 64 inputs (the DFf coefficients. each nonnalized between zero and one), a single hidden layer of 40 units, and 12 output units;â€",7485696000.0,"2*3040*3*410400=7485696000=7.49e9
â€œThe network was trained on 100 iterations through the 4104 training vectors.â€",,,4104.0,,,,,,Confident,"Multi-layer perceptrons and trained classification trees are two very different techniques which have recently become popular. Given enough data and time, both methods are capable of performing arbitrary non-linear classification. We first consider the important differences between multi-layer perceptrons and classification trees and conclude that there is not enough theoretical basis for the clear-cut superiority of one technique over the other. For this reason, we performed a number of empirical tests on three real-world problems in power system load forecasting, power system security prediction, and speaker-independent vowel identification. In all cases, even for piecewise-linear trees, the multi-layer perceptron performed as well as or better than the trained classification trees.",100.0,,,United States of America,,,,,,2025-02-18 11:25,,,,,,Academia,,,,,,,Academia,,,,,Operation counting,,,,
Handwritten Digit Recognition System,Vision,Digit recognition,AT&T,"Yann LeCun, B. Boser, J. Denker, D. Henderson, R. Howard, W. Hubbard, L. Jackel",1989-11-27,Handwritten Digit Recognition with a Back-Propagation Network,https://www.semanticscholar.org/paper/Handwritten-Digit-Recognition-with-a-Network-LeCun-Boser/86ab4cae682fbd49c5a5bedb630e5a40fa7529f6,,Historical significance,,2578.0,"""In summary, the network has 4635 units, 98442 connections, and 2578 independent parameters.â€œ",181440000000.0,"1.4e6 * 3 * 24 * 60* 60 * 0.5 = 181440000000 = 1.81e11
""A complete training session (30 passes through the training set plus test) takes about 3 days on a SUN SPARCstation 1""
Sparcstation 1 has an estimated compute of 1.4 MFLOPS (source: https://ieeexplore.ieee.org/document/63671 )",,,9840.0,"""After 30 training passes the error rate on training set (7291 handwritten plus 2549 printed digits)""",72.0,"""A complete training session (30 passes through the training set plus test) takes about 3 days on a SUN SPARCstation 1""",,,Confident,"We present an application of back-propagation networks to handwritten digit recognition. Minimal preprocessing of the data was required, but architecture of the network was highly constrained and specifically designed for the task. The input of the network consists of normalized images of isolated digits. The method has 1% error rate and about a 9% reject rate on zipcode digits provided by the U.S. Postal Service.",30.0,,,United States of America,,,,,,2025-02-19 12:27,,,,,,Industry,,,,,,,Industry,,,,,Hardware,,,,
Invariant image recognition,Vision,Representation learning,Complutense University of Madrid,"V. Cruz, G. CristÃ³bal, T. Michaux, S. Barquin",1989-06-18,Invariant image recognition using a multi-network neural model,https://ieeexplore.ieee.org/document/118669,,Historical significance,,,,27000000000.0,"0.5*6*60*60*2.5e6 = 27000000000 = 2.7e10
Trained for 6h on a SUN-4 (section 4)
Assumed utilization of 0.5
SUN-4 is estimated at 2.5e6 FLOP/s (Nordhaus, 2007)",,,,,6.0,Section 4,,,Confident,"A new model which permits visual patterns to be invariant to affine transforms (translations, rotations, and dimensions) is presented. A training multilayer fully connected network of ADALINE neurons is proposed as a preprocessing step for invariant image extraction. A second neural network has been trained by the popular backpropagation algorithm for recovering the real image without distortions. First, the sample invariants are obtained by the preprocessing network. In the second step, the general invariant that includes all the sample invariants is computed. Afterward, the reordered sample invariants are input to a multilayer neural network trained by the backpropagation algorithm. The original image, without distortions, is obtained in the output of this system. Several test images have been computed, and evaluation of the results shows that in the case of images with intrinsic perceptual similarity, the learning procedure leads to a global invariant extraction that requires less computational effort in comparison with an arbitrary training selection. After the training process, this system is able to extract the generalized invariant image from an arbitrary picture recovering the input image without distortions.<<ETX>>",,,,Spain,,,,,,2025-02-17 13:32,,,,,,Academia,,,,,,,Academia,,,,,Hardware,,,,
Truck backer-upper,Driving,Self-driving car,Stanford University,"Derrick Nguyen, Bernard Widrow",1989-06-18,"The truck backer-upper: an example of self-learning in neural networks
",https://ieeexplore.ieee.org/document/118723,,Historical significance,,805.0,6*25+25+8*45+45*6=805 (see Figure 6),,,,,,,,,,,Confident,"Neural networks can be used to solve highly nonlinear control problems. A two-layer neural network containing 26 adaptive neural elements has learned to back up a computer-simulated trailer truck to a loading dock, even when initially jackknifed. It is not yet known how to design a controller to perform this steering task. Nevertheless, the neural net was able to learn of its own accord to do this, regardless of initial conditions. Experience gained with the truck backer-upper should be applicable to a wide variety of nonlinear control problems.",,,,United States of America,,,,,,2024-09-05 14:08,,,,,,Academia,,,,,,,Academia,,,,,,,,,
Time-delay neural networks,Speech,Speech recognition,"Advanced Telecommunications Research Institute,Carnegie Mellon University (CMU)","A. Waibel, T. Hanazawa, G. Hinton, K. Shikano, and K. J. Lang",1989-03-03,Phoneme recognition using time-delay neural networks,https://ieeexplore.ieee.org/abstract/document/21701,3445.0,Highly cited,,,,,,,,,,,,,,Unknown,,,,,"Japan,United States of America",,,,,,2024-09-16 00:04,,,,,,"Industry,Academia",,,,,,,"Industry,Academia",,,,,,,,,
Q-learning,"Robotics,Games","Route finding,System control",University of London,Christopher Watkins,1989-01-01,Learning from delayed rewards,http://www.cs.rhul.ac.uk/~chrisw/thesis.html,8025.0,Highly cited,,,,,,,,,,,,,,Unknown,,,,,United Kingdom of Great Britain and Northern Ireland,,,,,,2024-09-23 00:11,,,,,,Academia,,,,,,,Academia,,,,,,,,,
MLP baggage detector,Vision,Image classification,Science Applications International Corporation / SAIC,"Patrick M Shea, Vincent Lin",1989-01-01,Detection of explosives in checked airline baggage using an artificial neural system,https://www.semanticscholar.org/paper/Detection-of-explosives-in-checked-airline-baggage-Shea-Lin/71da4057401f459bc079696a029aee45a0a89728,,Historical significance,One of the first real-world use cases of neural networks,,"3 layer network, input layer (<20), hidden layer, output layer (3)",,,,,40000.0,"""The database contains about 20,000 bags without simulants and a like number with; although, because of changes made in the systems as they were developed, not all measurements are on the same basis.""",,,,,Confident,"An artificial neural system (ANS) has been applied to the problem of discriminating between suitcases with and without explosives. The input to the ANS was data gathered during the field tests of a prototype explosive detection system. The performance of the ANS is contrasted with the standard statistical technique (discriminant analysis) used, and is shown to exceed the performance of the standard technique over a substantial range. The system that generated the data, the nature of the data, the basics of discriminant analysis, and the technique used in developing the network are described.",2000.0,,,United States of America,,,,,,2024-09-05 14:08,,,,,,Industry,,,,,,,Industry,,,,,,,,,
MLN-ASR,Speech,Speech recognition,McGill University,"Renato De Mori, Yoshua Bengio, RÃ©gis Cardin",1988-08-01,Data-Driven Execution of Multi-Layered Networks for Automatic Speech Recognition,https://aaai.org/papers/00734-aaai88-130-data-driven-execution-of-multi-layered-networks-for-automatic-speech-recognition/,,Historical significance,,10000.0,"â€œFor an MLN of about 10,000 links, the time was 115 CPU msecs for the recognition of a spoken letter and 317 msecs for the learning of a spoken letter on the SUN 4/280. A 20% reduction was obtained on the VAX 8650â€",296425000.0,"â€œFor an MLN of about 10,000 links, the time was 115 CPU msecs for the recognition of a spoken letter and 317 msecs for the learning of a spoken letter on the SUN 4/280. A 20% reduction was obtained on the VAX 8650â€, â€œLearning and recognition were performed on a VAX 8650.â€
Dataset: 70*10*2=1400 (Train) 10*10*2=200 (Test)
â€œThe ten words of the El set were pronounced twice by 80 speakers (40 males and 40 females)â€
â€œThe data from 70 speakers were used as a training set while the data from the remaining 10 speakers (6 males and 4 females) were used for the testâ€
VAX 8650 FLOPS  = 1.67E+06 (Nordhaus)
Training time: 317ms * 1400  * 0.8 = 355040ms = 355s
Estimate: 0.5 * 1.67e6 * 355 = 296425000 = 2.96e8",,,1400.0,,0.1,,,,Confident,"A set of Multi-Layered Networks (MLN) for Automatic Speech Recognition (ASR) is proposed. Such a set allows the integration of information extracted with variable resolution in the time and frequency domains and to keep the number of links between nodes of the networks small in order to allow significant generalization during learning with a reasonable training set size. Subsets of networks can be executed depending on preconditions based on descriptions of the time evolution of signal energies allowing spectral properties that are significant in different acoustic situations to be learned.
Preliminary experiments on speaker-independent recognition of the letters of the E-set are reported. Voices from 70 speakers were used for learning. Voices of 10 new speakers were used for test. An overall error rate of 9.5% was obtained in the test showing that results better than those previously reported can be achieved.",,,,Canada,,,,,,2025-02-17 15:57,,,,,,Academia,,,,,,,Academia,,,,,Hardware,,,,
MADALINE II,Mathematics,Pattern classification,Stanford University,"Rodney Winter, Bernard Widrow",1988-07-24,MADALINE RULE II: A Training Algorithm for Neural Networks,https://ieeexplore.ieee.org/document/23872,81.0,,,,,,,,,,,,,,,Unknown,,,,,United States of America,,,,,,2024-09-05 14:09,,,,,,Academia,,,,,,,Academia,,,,,,,,,
NetTalk (transcription),Speech,Speech synthesis,Princeton University,"TJ Sejnowski, CR Rosenberg",1987-06-06,Parallel Networks that Learn to Pronounce English Text,http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=03A3D3EDF0BAF35405ABCF083411B55E?doi=10.1.1.154.7012&rep=rep1&type=pdf,2558.0,Highly cited,,18629.0,"""The connections in the network are specified by a total of 18629
weight parameters (including a variable threshold for each unit)""",28328002560.0,18629 params * 2 FLOP/param * (3 for forward + backward pass) * 55 epochs * 1024 words/epoch * 4.5 letters/word,,,1024.0,"We used the first two pages of transcriptions, which contained 1024 words from a child in firstgrade",,,,,Confident,,55.0,,,United States of America,,,,,,2025-06-16 06:29,,,,,,Academia,,,,,,,Academia,,,,,Operation counting,,,,
NetTalk (dictionary),Speech,Speech synthesis,Princeton University,"TJ Sejnowski, CR Rosenberg",1987-06-06,Parallel Networks that Learn to Pronounce English Text,http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=03A3D3EDF0BAF35405ABCF083411B55E?doi=10.1.1.154.7012&rep=rep1&type=pdf,2558.0,Highly cited,,18629.0,"""The connections in the network are specified by a total of 18629 weight parameters (including a variable threshold for each unit)""",27664065000.0,18629 params * 2 FLOP/param * (3 for forward + backward pass) * 55 epochs * 1000 words/epoch * 4.5 letters/word (estimated number of letters),,,1000.0,"""A subset of the 1000 most commonly occurring words was selected from this dictionary based on frequency counts in the Brown corpus""",,,,,Confident,,55.0,,,United States of America,,,,,,2025-06-16 06:27,,,,,,Academia,,,,,,,Academia,,,,,Operation counting,,,,
Optimized Multi-Scale Edge Detection,Vision,Object detection,Massachusetts Institute of Technology (MIT),John Canny,1986-11-01,A Computational Approach To Edge Detection,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4767851,37931.0,Highly cited,,,,,,,,,,,,,,Unknown,,,,,United States of America,,,,,,2024-09-16 00:06,,,,,,Academia,,,,,,,Academia,,,,,,,,,
Back-propagation,Mathematics,Triplet completion,"University of California San Diego,Carnegie Mellon University (CMU)","Rumelhart, David E.; Hinton, Geoffrey E.; Williams, Ronald J.",1986-10-01,Learning representations by back-propagating errors,https://www.semanticscholar.org/paper/Learning-representations-by-back-propagating-errors-Rumelhart-Hinton/052b1d8ce63b07fec3de9dbb583772d860b7c769,25930.0,Highly cited,,720.0,"Architecture in Figure 3: 

24+12 input -> 6 + 6 hidden -> 12 hidden -> 6 hidden -> 24 output

Parameters: 6*24+6*12+12*12+6*12+24*12=720",673920000.0,"We assume that the number of mult-adds per pass is equal to the number of parameters -> 2*720=1440 FLOP per forward pass.

""We trained the network for 1500 sweeps""
There are 104 relationship triplets (""[...] of the 104 possible triplets"")

FLOP: 1500*104*3*1440=673920000

",,,104.0,"There are 104 relationship triplets (""[...] of the 104 possible triplets"")",,,,Unsupervised,Confident,,,,,"United States of America,United States of America",,,,,,2025-06-16 06:18,,,,,,"Academia,Academia",,,,,,,"Academia,Academia",,,,,Operation counting,,,,
Distributed representation NN,Other,Representation learning,Carnegie Mellon University (CMU),Geoffrey E. Hinton,1986-08-15,Learning distributed representations of concepts.,https://www.cs.toronto.edu/~hinton/absps/families.pdf,,"Historical significance,Highly cited",,432.0,"Parameters: 24*6 + 12*6 + 12*12 + 12*6 =432
""Figure 5: The activity levels in a five-layer network after it has learned. The bottom layer has 24 input units on the left for representing person 1 and 12 units on the right for representing the relationship. The white squares inside these two groups show the activity levels of the units. There is one active unit in the first group (representing Colin) and one in the second group (representing has-aunt). Each of the two groups of input units is totally connected to its own group of 6 units in the second layer. These two groups of 6 must learn to encode the input terms as distributed patterns of activity. The second layer is totally connected to the central layer of 12 units, and this layer is connected to the penultimate layer of 6 units.""
",388800000.0," 2*432*3*1500*100=388800000=3.9e8
""After 1500 sweeps through all 100 training examples the weights were very stable """,,,100.0,"""After 1500 sweeps through all 100 training examples the weights were very stable """,,,,,Confident,,1500.0,,,United States of America,,,,,,2025-02-18 11:31,,,,,,Academia,,,,,,,Academia,,,,,Operation counting,,,,
PDP model for serial order,Speech,Speech synthesis,University of California San Diego,"Jordan, M.I.",1986-01-05,Serial order: A parallel distributed processing approach,https://www.osti.gov/biblio/6910294,1502.0,Highly cited,,,,,,,,,,,,,,Unknown,,,,,United States of America,,,,,,2024-09-05 14:08,,,,,,Academia,,,,,,,Academia,,,,,,,,,
Error Propagation,Other,"Text classification,Image classification","University of California San Diego,Carnegie Mellon University (CMU)","D. E. Rumelhart, G. E. Hinton, and R. J. Williams",1986-01-03,Learning internal representations by error propagation,https://stanford.edu/~jlmcc/papers/PDP/Volume%201/Chap8_PDP86.pdf,27322.0,Highly cited,,,,,,,,,,,,,,Unknown,,,,,"United States of America,United States of America",,,,,,2024-09-16 09:53,,,,,,"Academia,Academia",,,,,,,"Academia,Academia",,,,,,,,,
Learnability theory of language development,Language,Language Structure Modeling,Massachusetts Institute of Technology (MIT),Steven Pinker,1984-07-01,Language learnability and language development,https://psycnet.apa.org/record/1985-97439-000,4730.0,Highly cited,,,,,,,,,,,,,,Unknown,,,,,United States of America,,,,,,2024-09-23 00:21,,,,,,Academia,,,,,,,Academia,,,,,,,,,
Hierarchical Cognitron,Other,Pattern recognition,NHK Broadcasting Science Research Laboratories,K. Fukushima,1984-04-01,A hierarchical neural network model for associative memory,https://link.springer.com/article/10.1007/BF00337157,,Historical significance,,9315.0,"Parameters 5*5*9*3*3 + 3*3*9*3*3*9 + 9*3*3*9 = 9315
""The numbers of excitatory cells in these four layers were: 7x7 in U0, 5x5 in  U1, 3x3 in U2, and 9 in U3""
""Each feature-extracting cell in layer U1 receives excitatory modifiable afferent connections from 3x3 cells in layer U0""
""On the other hand, each feature, each extracting cell in layers U2 and U3 receives excitatory modifiable connections from all 9 cells in each of the 3 x 3 hypercolumns in the layer preceding it. Therefore, it receives 3 x 3 x 9 afferent excitatory modifiable connections altogether""
",,,,,5.0,"""Five training patterns used for the self-organization are shown in Fig. 4""",,,,,Speculative,"A hierarchical neural network model with feedback interconnections, which has the function of associative memory and the ability to recognize patterns, is proposed. The model consists of a hierarchical multi-layered network to which efferent connections are added, so as to make positive feedback loops in pairs with afferent connections. The cell-layer at the initial stage of the network is the input layer which receives the stimulus input and at the same time works as an output layer for associative recall. The deepest layer is the output layer for pattern-recognition. Pattern-recognition is performed hierarchically by integrating information by converging afferent paths in the network. For the purpose of associative recall, the integrated information is again distributed to lower-order cells by diverging efferent paths. These two operations progress simultaneously in the network. If a fragment of a training pattern is presented to the network which has completed its self-organization, the entire pattern will gradually be recalled in the initial layer. If a stimulus consisting of a number of training patterns superposed is presented, one pattern gradually becomes predominant in the recalled output after competition between the patterns, and the others disappear. At about the same time when the recalled pattern reaches a steady state in he initial layer, in the deepest layer of the network, a response is elicited from the cell corresponding to the category of the finally-recalled pattern. Once a steady state has been reached, the response of the network is automatically extinguished by inhibitory signals from a steadiness-detecting cell. If the same stimulus is still presented after inhibition, a response for another pattern, formerly suppressed, will now appear, because the cells of the network have adaptation characteristics which makes the same response unlikely to recur. Since inhibition occurs repeatedly, the superposed input patterns are recalled one by one in turn.",,,,Japan,,,,,,2024-09-05 14:08,,,,,,Industry,,,,,,,Industry,,,,,,,,,
ASE+ACE,Robotics,Pole balancing,University of Massachusetts Amherst,"Andrew G. Barto, Richard S. Sutton, and Charles W. Anderson",1983-09-01,Neuronlike adaptive elements that can solve difficult learning control problems,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6313077,4296.0,"Highly cited,Historical significance",,324.0,"The system consists of two parts: ACE and ASE, each with 162 weights (=324 parameters). Found in Figures 2 and 3.",324000000.0,"324 * 2 * 500000 = 324000000 = 3.24e8. The calculation assumes ""compute per forward pass"" = ""number of parameters"" = ""compute per backward pass"". Their model only has a single layer and is trained with simple update rules instead of gradient descent. Training details are described in Section IX.

Note that this is the compute for a single run; they appear to have repeated training 10 times for the ASE+ACE system.",,,500000.0,"""Runs consisted of 100 trials unless the run's duration exceeded 500 000 time steps (approximately 2.8 h of simulated real time)"" 
""Almost all runs of the ASE/ACE system [...], were terminated after 500 000"" (Section IX)",2.8,"""Runs consisted of 100 trials unless the run's duration exceeded 500 000 time steps (approximately 2.8 h of simulated real time)"" 
""Almost all runs of the ASE/ACE system [...], were terminated after 500 000"" (Section IX)",,,Likely,"It is shown how a system consisting of two neuronlike adaptive elements can solve a difficult learning control problem. The task is to balance a pole that is hinged to a movable cart by applying forces to the cart's base. It is argued that the learning problems faced by adaptive elements that are components of adaptive networks are at least as difficult as this version of the pole-balancing problem. The learning system consists of a single associative search element (ASE) and a single adaptive critic element (ACE). In the course of learning to balance the pole, the ASE constructs associations between input and output by searching under the influence of reinforcement feedback, and the ACE constructs a more informative evaluation function than reinforcement feedback alone can provide. The differences between this approach and other attempts to solve problems using neurolike elements are discussed, as is the relation of this work to classical and instrumental conditioning in animal learning studies and its possible implications for research in the neurosciences.",,,,United States of America,,,,,,2025-02-17 13:31,,,,,,Academia,,,,,,,Academia,,,,,Operation counting,,,,
Neocognitron,Vision,Character recognition,NHK Broadcasting Science Research Laboratories,"K Fukushima, S Miyake",1980-04-01,Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position,https://link.springer.com/article/10.1007/BF00344251,5782.0,Highly cited,,1140576.0,"""The synaptic connections from S-layers to C-layers
are fixed and unmodifiable. [...]
The numbers of excitatory cells in these seven layers are: 16x16 in U0, 16x16x24 in Us1, 10x10x 24 in Uc1, 8x8x24 in Us2, 6x 6x 24 in Uc2, 2x2x24 in Us3, and 24 in Uc3 
[...]
 the number of input synapses to each S-cell is 5 x 5 in layer Us1 and 5x5x24 in layers Us2 and Us3
[...]
The number of excitatory input synapses to each C-cell is 5x5 in layers Uc1 and Uc2, and is 2x2 in
layer Uc3
""

The number of synapses into each S-layer is:

S1: (16*16*24)*(5*5) 
S2: (8*8*24)*(5*5*24)
S3: (2*2*24)*(5*5*24)

We assume one parameter a per synapse into each cell in a S-layer, and one parameter b per each cell in a S-layer.",273738240.0,"""It does not necessarily mean that all of these input synapses are
always fully reinforced. In usual situations, only some of these input synapses are reinforced, and the rest of them remains in small values [...] Each of the five stimulus patterns has been presented 20 times to the network. By that time, self organization of the network has almost been completed.""

We multiply by 2 to account for multadds in the forward pass. 
There is no real backward pass, weights are only updated sparsely. Estimating 20% additional weight update compute compared to the forward pass:
2*1,140,576.0*1.2*100=273738240
",,,5.0,"""In order to self-organize the network, we have presented five stimulus patterns ""0"", ""1"", ""2"", ""3"", and ""4"", which are shown in Fig. 6""",,,,,Confident,,,,,Japan,,,,,,2025-06-16 05:50,,,,,,Industry,,,,,,,Industry,,,,,Operation counting,,,,
Internal functionality of visual invariants,Vision,Miscellaneous image analysis,Utrecht University,Koenderink & van Doom,1979-05-02,The internal representation of solid shape with respect to vision,https://link.springer.com/article/10.1007/BF00337644,981.0,Historical significance,,,,,,,,,??? Seemingly no info,,,,,Unknown,,,,,Netherlands,,,,,,2024-09-16 09:51,,,,,,Academia,,,,,,,Academia,,,,,,,,,
TD(0),Other,System control,University of Essex,Ian Witten,1977-08-01,An adaptive optimal controller for discrete-time Markov environments,https://www.sciencedirect.com/science/article/pii/S0019995877903540,269.0,Historical significance,,,,,,,,,??? Seemingly no info,,,,,Unknown,,,,,United Kingdom of Great Britain and Northern Ireland,,,,,,2024-09-16 09:48,,,,,,Academia,,,,,,,Academia,,,,,,,,,
Statistical continuous speech recognizer,Speech,Speech recognition,Massachusetts Institute of Technology (MIT),Frederick Jelenick,1976-04-30,Continuous speech recognition by statistical methods,https://ieeexplore.ieee.org/document/1454428,1591.0,Highly cited,,,,,,,"800 sentences, counting 15 words per sentence gives 12000
""All the results given are for a training set of 800 sentences and a test set of 100 sentences""",12000.0,"800 sentences, counting 15 words per sentence gives 12000 words
""All the results given are for a training set of 800 sentences and a test set of 100 sentences""",,,,,Confident,"Statistical methods useful in automatic recognition of continuous speech are described. They concern modeling of a speaker and of an acoustic processor, extraction of the models' statistical parameters, and hypothesis search procedures and likelihood computations of linguistic decoding. Experimental results are presented that indicate the power of the methods.",,,,United States of America,,,,,,2025-05-01 10:42,,,,,,Academia,,,,,,,Academia,,,,,,,,,
Cognitron,Other,"Miscellaneous image analysis,Image classification",Biological Cybernetics,Kunihiko Fukushima,1975-09-01,Cognitron: a self-organizing multilayered neural network,https://link.springer.com/article/10.1007%2FBF00342633,791.0,Historical significance,Precursor of the Neocognitron,21600.0,"4 layers, 288 neurons per layer, weights connect each neuron to only 25 neurons in the previous layer.
Only 3 layers with learnable weights, the first layer is just an input representation (see Fig 5)
3*288*25 parameters",5184000.0,"No real backward pass as weights are sparsely updated. Assuming 20% additional compute for the weight update.
Total compute estimate: 100*2*3*288*25*1.2 = 5184000
",,,5.0,5 examples presented for (at least) 20 cycles = 100 training steps,,,,,Confident,,20.0,,,Japan,,,,,,2025-06-16 05:53,,,,,,Industry,,,,,,,Industry,,,,,Operation counting,,,,
Piecewise linear model,Vision,Image classification,University of Kansas,"R. Haralick, K. Shanmugam, I. Dinstein",1973-11-01,Textural Features for Image Classification,https://ieeexplore.ieee.org/document/4309314,,"Historical significance,Highly cited",,357.0,"16 input features + bias = 17 input features
7*6/2 = 21 Hyperplanes
17*21 = 357 parameters
""For the multicategory problem involving NR categories, a total of NR(NR - 1)/2 hyperplanes are used to partition the pattern space.""
""The input variables to the classifier consisted of the mean variance of the four textural features (f1,f2,f3, andfg obtained from the distance 1 gray-tone spatial-dependence matrices) and eight spectral features (comprised of the mean variance of the image gray-tone values) in each of the four spectral bands""
",,,,,314.0,"""Number of training samples = 314;""",,,,,Confident,"Texture is one of the important characteristics used in identifying objects or regions of interest in an image, whether the image be a photomicrograph, an aerial photograph, or a satellite image. This paper describes some easily computable textural features based on gray-tone spatial dependancies, and illustrates their application in category-identification tasks of three different kinds of image data: photomicrographs of five kinds of sandstones, 1:20 000 panchromatic aerial photographs of eight land-use categories, and Earth Resources Technology Satellite (ERTS) multispecial imagery containing seven land-use categories. We use two kinds of decision rules: one for which the decision regions are convex polyhedra (a piecewise linear decision rule), and one for which the decision regions are rectangular parallelpipeds (a min-max decision rule). In each experiment the data set was divided into two parts, a training set and a test set. Test set identification accuracy is 89 percent for the photomicrographs, 82 percent for the aerial photographic imagery, and 83 percent for the satellite imagery. These results indicate that the easily computable textural features probably have a general applicability for a wide variety of image-classification applications.",,,,United States of America,,,,,,2024-09-05 14:08,,,,,,Academia,,,,,,,Academia,,,,,,,,,
Graph-based structural reasoning,Vision,Image classification,Massachusetts Institute of Technology (MIT),Patrick Winston,1970-09-01,Learning Structural Definitions from Examples,https://dspace.mit.edu/handle/1721.1/6884,1805.0,Highly cited,,,,,,,,,,,,,,Unknown,,,,,United States of America,,,,,,2024-09-23 09:23,,,,,,Academia,,,,,,,Academia,,,,,,,,,
Decision tree adaline,Medicine,Medical diagnosis,Tokyo Medical and Dental University,"T Sano, S Tsuchiya, F Suzuki",1969-05-01,A use of Adaline as an automatic method for interpretation of the electrocardiogram and the vectorcardiogram,https://pubmed.ncbi.nlm.nih.gov/5820353/,,Historical significance,,2450.0,"5 adaline were trained on binary decisions (p. 1)
Each adaline had up to 490 input weights (â€œmeshesâ€)
Total parameters = 5*490=2450
",,,,,80.0,40 positive and negative training examples (p. 2),,,,,Confident,"A learning machine"" adaline neuron"" was employed for automatic diagnosis of the vectorcardiogram and the electrocardiogram. The frontal circle and the horizontal circle were divided into 480 meshes. The features were expressed by a binary digit, whether the vector loops passed through each mesh or not. In a part of the trials, 5 sets of binary digits were applied in addition to QRS duration and direction of inscription of QRS loops and T loops. In this trial a total of 490 meshes were used. Vectorcardiograms were taken by FRANK'S method in 235 cases. Several methods of adaline usage were tried. The best result was obtained so far by successive dichotomies based on the principle of the logical decision tree. First the normal patterns and the abnormal patterns were divided. The correct ratio was 96.5% when the 490 meshes were employed, cases of an output value withinÂ±10 units being regarded as undecided. Next the abnormal cases were divided into two groups depending on whether the QRS duration was more than 0.12 seconds or less. The group of cases with a QRS duration of less than 0.12 seconds was divided into right ventricular hypertrophy and others. The correct ratio was 98.6%. The remaining cases were divided into left ventricular hypertrophy and myocardial infarction, the correct ratio being 88.8%. The group of cases with a QRS duration of more than 0.12 seconds was easily divided into complete left and right bundle branch block in all cases. Here the number of meshes could be decreased to 59 meshes without changing the accuracy appreciably. This attempt showed that the application of the adaline for automatic diagnosis of the vectorcardiogram and the electrocardiogram is promising.",,,,Japan,,,,,,2024-09-05 14:08,,,,,,Academia,,,,,,,Academia,,,,,,,,,
GLEE,Games,Tic Tac Toe,University of Edinburgh,Michie and Chambers,1968-07-01,Boxes: An Experiment in Adaptive Control,https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.474.2430,590.0,Historical significance,,,,,,,,,,,,,,Unknown,,,,,United Kingdom of Great Britain and Northern Ireland,,,,,,2024-09-05 14:08,,,,,,Academia,,,,,,,Academia,,,,,,,,,
BOXES,Games,Pole balancing,University of Edinburgh,Michie and Chambers,1968-07-01,Boxes: An Experiment in Adaptive Control,https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.474.2430,590.0,Historical significance,,,,,,,,,,,,,,Unknown,,,,,United Kingdom of Great Britain and Northern Ireland,,,,,,2024-09-05 14:08,,,,,,Academia,,,,,,,Academia,,,,,,,,,
LTE speaker verification system,Speech,Speech recognition,IBM,K. P. Li; J. E. Dammann; W. D. Chapman,1966-11-01,"Experimental Studies in Speaker Verification, Using an Adaptive System",https://pubs.aip.org/asa/jasa/article-abstract/40/5/966/754180/Experimental-Studies-in-Speaker-Verification-Using?redirectedFrom=fulltext,,Historical significance,,2061.0,"2 connected systems, 1st level LTE and 2nd level LTE.
1st Level: 1810 parameters (""Thus, every 20 msec after the beginning of the utterance, the 15 filter amplitudes were each represented by a 12-bit code, resulting in a 180-bit time sample of the spectrum for that interval. Each time sample was fed to the first-level LTE's, which reduced it to a 10-bit code"")
2nd Level: 251 parameters (""This resulted in a 250-bit input pattern to the second level for the first half-second of each utterance. Each 250-bit pattern was then classified by the LTE into one of two classes"")",105917060.0,"1st and 2nd level system are trained separately, multiple versions of both are trained, I chose the largest clearly described training runs.

1st level LTE compute: 2*1810*28700=103894000=1.04e8
1st level steps: 28700 (""Only 287 samples were selected to train the 10 LTE's. The same algorithm was used as that used with the 100-class gain. Two LTE's
converged before 100 training passes."")

2nd level LTE compute: 2*251*4030=2023060=2e6
2nd level steps: 4030 (31 epochs, 130 training examples, see Table 3)

Total compute: 103894000+2023060=105917060=1.06e8 (assuming no backward pass since they didn't use backpropagation)
",,,417.0,"Split between both systems, 287 for 1st level, 130 for 2nd level.",,,,,Likely,"This paper describes an investigation of the capability of a twoâ€level adaptive linear threshold element (LTE) system to perform speaker discriminations. The study also includes an investigation of discriminating a speaker from an unknown population. The problem has been confined to the verification of an utterance as that of an expected informant. The environment of the experiments is discussed, and the experimental system is described. At the first level LTE, four different kinds of training have been developed for effective transformation and data reduction. At the secondâ€level LTE, different training conditions and different decision processes are investigated and evaluated. Over 90% accuracy is obtained in separating a known speaker from impostors.",131.0,,,United States of America,,,,,,2025-02-17 15:56,,,,,,Industry,,,,,,,Industry,,,,,Operation counting,,,,
Heuristic Reinforcement Learning,Robotics,System control,Purdue University,"M. Waltz, K. Fu",1965-10-01,A heuristic approach to reinforcement learning control systems,https://ieeexplore.ieee.org/document/1098193,,"Historical significance,Highly cited",,,,1080000.0,"Figure 10 shows their largest system is trained for 3h and was trained on an analog IBM 1620 that was simulated on a digital IBM 1710.
Nordhaus, 2007 lists the IBM 1620 at 200 multiplications per second and doesnâ€™t contain the 1710
Flops estimate: 0.5 * 3 * 60 * 60 * 200 = 1080000 = 1.08e6
Assumed utilization of 0.5
",,,,,3.0,Figure 10,,,Speculative,This paper describes a learning control system using a reinforcement technique. The controller is capable of controlling a plant that may be nonlinear and nonstationary. The only a priori information required by the controller is the order of the plant. The approach is to design a controller which partitions the control measurement space into sets called control situations and then learns the best control choice for each control situation. The control measurements are those indicating the state of the plant and environment. The learning is accomplished by reinforcement of the probability of choosing a particular control choice for a given control situation. The system was stimulated on an IBM 1710-GEDA hybrid computer facility. Experimental results obtained from the simulation are presented.,,,,United States of America,,,,,,2025-02-17 15:56,,,,,,Academia,,,,,,,Academia,,,,,Hardware,,,,
MENACE,Games,Tic Tac Toe,University of Edinburgh,Donald Michie,1963-11-01,Experiments on the Mechanization of Game-Learning Part I. Characterization of the Model and its parameters,https://academic.oup.com/comjnl/article/6/3/232/360077,46.0,,,,,,,,,,,,,,,Unknown,,,,,United Kingdom of Great Britain and Northern Ireland,,,,,,2024-10-21 14:34,,,,,,Academia,,,,,,,Academia,,,,,,,,,
STeLLA,Robotics,Robotic manipulation,University of Canterbury,J.H. Andreae and Peter L. Joyce,1963-06-01,STeLLA: A Scheme for a Learning Machine,https://www.sciencedirect.com/science/article/pii/S1474667017696824?via%3Dihub,34.0,,,,,,,,,,,,,,,Unknown,,,,,New Zealand,,,,,,2024-10-21 14:34,,,,,,Academia,,,,,,,Academia,,,,,,,,,
Print Recognition Logic,Vision,Character recognition,IBM,"L. Kamentsky, Chao-Ning Liu",1963-01-01,Computer-Automated Design of Multifont Print Recognition Logic,https://ieeexplore.ieee.org/document/5392331,,Historical significance,,,,22500000.0,"0.5*2.5*60*60*5000 = 22500000 = 2.25e7
Assumed utilization of 0.5
Trained for 2-3h on an IBM 7090 (from Introduction)
Estimated IBM 7090 at 5000 FLOP/s based on multiplications per second (Nordhaus, 2007)
Note: the Nordhaus estimate is very different from Wikipedia's estimate of 100000 FLOP/s, which cites a PowerPoint as source.",,,,,2.5,2-3h (from Introduction),,,Speculative,"A computer program has been written to design character recognition logic based on the processing of data samples. This program consists of two subroutines: (1) to search for logic circuits having certain constraints on hardware design, and (2) to evaluate these logics in terms of their discriminating ability over samples of the character set they are expected to recognize. An executive routine is used to apply these subroutines to select a complete logic with a given performance and complexity. This logic consists of 39 to 96 and gates connected to a shift register and a table look-up or resistance network comparison system.",,,,United States of America,,,,,,2025-02-17 13:28,,,,,,Industry,,,,,,,Industry,,,,,Hardware,,,,
MADALINE I,Other,"Text classification,Image classification",Stanford University,William Combs Ridgway,1962-07-01,An adaptive logic system with generalizing properties,https://www.proquest.com/openview/7898314db50a218b58052ac91e3bde1e/1?,75.0,Historical significance,,,,,,,,,,,,,,Unknown,,,,,United States of America,,,,,,2024-10-21 14:34,,,,,,Academia,,,,,,,Academia,,,,,,,,,
Linear Decision Functions,Mathematics,Binary classification,Bell Laboratories,W. Highleyman,1962-06-01,"Linear Decision Functions, with Application to Pattern Recognition",https://ieeexplore.ieee.org/document/4066882?denied=,,"Historical significance,Highly cited",,,,1559250.0,"0.5*45*35*1980 = 1559250 = 1.56e6
Trained using IBM punched cards, computation took 45 * 35s for all 10 digits (Section Estimating the Linear Decision Function).
Multiplications per second estimate based on publication year: 1.98e3 (regression on Nordhaus data).
Assumed utilization of 0.5",,,500.0,"""Fifty different people were asked, resulting in a sample size of 50 for each of the ten pattern classes. """,0.4,"""Forty-five hyperplanes are required in the complete linear decision function""
""About 35 seconds, on the average, was required to determine a hyperplane, given an initial position.""",,,Speculative,"Many pattern recognition machines may be considered to consist of two principal parts, a receptor and a categorizer. The receptor makes certain measurements on the unknown pattern to be recognized; the categorizer determines from these measurements the particular allowable pattern class to which the unknown pattern belongs. This paper is concerned with the study of a particular class of categorizers, the linear decision function. The optimum linear decision function is the best linear approximation to the optimum decision function in the following sense: 1) ""Optimum"" is taken to mean minimum loss (which includes minimum error systems). 2) ""Linear"" is taken to mean that each pair of pattern classes is separated by one and only one hyperplane in the measurement space. This class of categorizers is of practical interest for two reasons: 1) It can be empirically designed without making any assumptions whatsoever about either the distribution of the receptor measurements or the a priori probabilities of occurrence of the pattern classes, providing an appropriate pattern source is available. 2) Its implementation is quite simple and inexpensive. Various properties of linear decision functions are discussed. One such property is that a linear decision function is guaranteed to perform at least as well as a minimum distance categorizer. Procedures are then developed for the estimation (or design) of the optimum linear decision function based upon an appropriate sampling from the pattern classes to be categorized.",,,,United States of America,,,,,,2025-02-17 13:28,,,,,,Industry,,,,,,,Industry,,,,,Hardware,,,,
PAPA,Other,Binary classification,University of Genoa,"A Gamba, L Gamberini, G Palmieri, R Sanna",1961-09-01,Further experiments with PAPA,https://www.semanticscholar.org/paper/Further-experiments-with-PAPA-Gamba-Gamberini/c3a20b9aa86033cec29f08e69f4bc81e8b329ae2,24.0,,,,,,,,,,,,,,,Unknown,,,,,Italy,,,,,,2024-10-21 14:34,,,,,,Academia,,,,,,,Academia,,,,,,,,,
ADALINE,Vision,Pattern recognition,Stanford University,Widrow and Hoff,1960-06-30,Adaptive switching circuits,https://isl.stanford.edu/~widrow/papers/c1960adaptiveswitching.pdf,6329.0,Highly cited,,17.0,"""The machine's total experience is stored in the values of the weights a0,...,a16""",6600.0,"""The method of searching that has proven most useful is the method of steepest descent""

Apparently each pattern was only shown once to the system.

So the training compute is (forward pass compute) * (3 for backprop) * dataset size

This is a single layer (and single neuron) which does not require gradients w.r.t. inputs - 1:1 forward-backward ratio

",,,100.0,"""The best system, arrived at by slow precise adaptation on the full body of 100 noisy patterns, was able to classify these patterns as desired except for twelve errors.""

https://isl.stanford.edu/~widrow/papers/c1960adaptiveswitching.pdf",,,,,Confident,,,,,United States of America,,,,,,2025-06-16 05:23,,,,,,Academia,,,,,,,Academia,,,,,Operation counting,,,,
Perceptron (1960),Vision,Image classification,Cornell Aeronautical Laboratory,Frank Rosenblatt,1960-03-30,Perceptron Simulation Experiments,https://www.semanticscholar.org/paper/Perceptron-Simulation-Experiments-Rosenblatt/ae76ce1ba27ac29addce4aab93b927e9bc7f7c67,394.0,Historical significance,,1000.0,""" The first program was designed to handle
up to 1000 A units, and a 72 by 72 sensory mosaic. It
was found that this large sensory system presented
stimuli with a fineness of grain considerably better than
the limits of discrimination of a thousand-unit percep-
tron, and at the same time, required an excessive
amount of time for stimulus transformations, since each
illuminated point in the stimulus must be transformed
individually into its image point.""",720000000.0,"4000 * 12000 * 15
from the text ""This program uses the IBM 704 computer to simulate per-ceptual learning, recognition, and spontaneous classification of visual stimuli in the perceptron,""
from https://en.wikipedia.org/wiki/IBM_704 The 704 can execute up to 12,000 floating-point additions per second.
"" For the first system, the computing time averaged about 15 seconds per stimulus cycle, ""
In Fig 10 we see up to 4000 stimuli",,,5000.0,"from the text ""The two main simulation programs total about 5000 words each.""",,,,,Speculative,"An experimental simulation program, which has been in progress at the Cornell Aeronautical Laboratory since 1957, is described. This program uses the IBM 704 computer to simulate perceptual learning, recognition, and spontaneous classification of visual stimuli in the perceptron, a theoretical brain model which has been described elsewhere. The paper includes a brief review of the organization of simple perceptrons, and theoretically predicted performance curves are compared with those obtained from the simulation programs, in several types of experiments, designed to study ""forced"" and ""spontaneous"" learning of pattern discriminations.",,,,United States of America,,,,,,2025-02-18 13:31,,,,,,Academia,,,,,,,Academia,,,,,Hardware,,,,
Samuel Neural Checkers,Games,Checkers,IBM,Arthur L. Samuel,1959-07-01,Some studies in machine learning using the game of checkers,https://ieeexplore.ieee.org/abstract/document/5392560,4671.0,Highly cited,,16.0,"""with 16 terms for generalization learning""

""Mention has been made several times of the procedure
for replacing terms in the scoring polynomial. The program, as it is currently running, contains 38 different
terms (in addition to the piece-advantage term), 16 of
these being included in the scoring polynomial at anyone
time and the remaining 22 being kept in reserve.""",428400000.0,"""it can learn to do this in a remarkably short period of time 8 or 10 hours of machine-playing time)""

""The availability of a larger and faster machine (the IBM 704), coupled with many detailed changes in the programming procedure, leads to a fairly interesting game being played, even without any learning.""

""The Type 704 is the first large-scale, commercially available computer to employ fully automatic floating point arithmetic commands. [...]. Floating point addition or subtraction operations require 84 microseconds.""

source: https://www.ibm.com/ibm/history/exhibits/mainframe/mainframe_PP704.html

""An idea of the learning ability of this procedure can be gained by analyzing an initial test series of 28 games""

""Each game averaged 68 moves (34 to a side), of which approximately 20 caused changes to be made in the scoring polynomial.""

10*3600*1000000/84=428571428",,,53000.0,"Based on number of board positions

At the present time the memory tape contains something over 53,000 board positions (averaging 3.8 word search) which have been selected from a much larger
number of positions by means of the culling techniques
described. While this is still far from the number which
would tax the listing and searching procedures used in
the program, rough estimates, based on the frequency
with which the saved boards are utilized during normal
play (these figures being tabulated automatically), indicate that a library tape containing at least 20 times the
present number of board positions would be needed to
improve the midgame play significantly. At the present
rate of acquisition of new positions this would require
an inordinate amount of play and, consequently, of
machine time.",,,,,Likely,,,,,United States of America,,,,,,2025-06-16 05:05,,,,,,Industry,,,,,,,Industry,,,,,,,,,
Pandemonium (morse),Language,Morse translation,Massachusetts Institute of Technology (MIT),OG Selfridge,1959-02-01,Pandemonium: A Paradigm for Learning,https://aitopics.org/doc/classics:504E1BAC/,1453.0,Highly cited,,,"The paper mentions 11 function types. Unclear how many times they are called (number of ""demons"" in their Pandemonium implementation).",600000000.0,"The paper mentions using an IBM 704, which can execute up to 12,000 floating-point additions per second (https://wikiless.org/wiki/IBM_704). My best guess as to how long it ran for ranges between 1h to 2 days, which when plugged into guesstimate (https://www.getguesstimate.com/models/19625), i.e., taking the log mean, gives a mean estimate of 600M",,,,??? Might need to make a guesstimate here.,,,,,Speculative,,,,,United States of America,,,,,,2025-02-17 13:26,,,,,,Academia,,,,,,,Academia,,,,,Hardware,,,,
Perceptron Mark I,Other,Binary classification,"Cornell Aeronautical Laboratory,Cornell University",F Rosenblatt,1957-01-01,The Perceptronâ€”a perceiving and recognizing automaton,https://blogs.umass.edu/brain-wars/files/2016/03/rosenblatt-1957.pdf,1610.0,"Historical significance,Highly cited",First modern neural network ,1000.0,"""Figure 4.8 Illustration of the Mark 1 perceptron hardware. The photograph on the left shows how the inputs were obtained using a simple camera system in which an input scene, in this case a printed character, was illuminated by powerful lights, and an image focussed onto a 20 Ã— 20 array of cadmium sulphide photocells, giving a primitive 400 pixel image. The perceptron also had a patch board, shown in the middle photograph, which allowed different configurations of input features to be tried. Often these were wired up at random to demonstrate the ability of the perceptron to learn without the need for precise wiring, in contrast to a modern digital computer. The photograph on the right shows one of the racks of adaptive weights. Each weight was implemented using a rotary variable resistor, also called a potentiometer, driven by an electric motor thereby allowing the value of the weight to be adjusted automatically by the learning algorithm.""

source: Bishop, Christopher M. (2006). Pattern Recognition and Machine Learning

The Perceptron had a 400-pixel visual input and 1000 neurons in the hidden layer. https://twitter.com/DiegoKuonen/status/1130352233223262208",694894.94,"Extracted from AI and Compute (https://openai.com/blog/ai-and-compute/) charts by using https://automeris.io/WebPlotDigitizer/.

Additional experiment described in https://babel.hathitrust.org/cgi/pt?id=coo.31924004657973&seq=70 
- 400 input (20x20) - 512 hidden with 40 fixed connections each (not learned) - 1 output (learned)
Parameters: 512*1=512
Forward flop: 41*512=20992
Forward + â€œbackward flopâ€: 43*512=22016 (only last layer was adjusted)
100*22016=2201600
",,,100.0,"Appendix II describes an experiment with 6 stimulus patterns

https://babel.hathitrust.org/cgi/pt?id=coo.31924004657973&seq=47 describes simulation experiments with ""X"" and ""E"" patterns using 100 total training stimuli",,,,,Likely,,,,,"United States of America,United States of America",,,,,,2025-06-19 03:12,,,,,,"Academia,Academia",,,,,,,"Academia,Academia",,,,,Third-party estimation,,,,
Sequence-based pattern recognition,Vision,Character recognition,Massachusetts Institute of Technology (MIT),O. G. Selfridge,1955-03-01,Pattern recognition and modern computers,https://dl.acm.org/doi/10.1145/1455292.1455310,290.0,Historical significance,,,,,,,,,,,,,,Unknown,,,,,United States of America,,,,,,2024-10-21 14:34,,,,,,Academia,,,,,,,Academia,,,,,,,,,
Genetic algorithm,"Mathematics,Biology",Numerical simulation,Institute for Advanced Study,NA Barricelli,1954-07-02,Numerical testing of evolution theories,https://link.springer.com/article/10.1007/BF01556771,266.0,Historical significance,Possibly first computer simulation of a genetic evolution algorithm,,,,,,,,,,,,,Unknown,,,,,United States of America,,,,,,2025-03-21 13:07,,,,,,Academia,,,,,,,Academia,,,,,,,,,
Theseus,Robotics,Maze solving,Bell Laboratories,Claude Shannon,1950-07-02,Mighty Mouse,https://www.technologyreview.com/2018/12/19/138508/mighty-mouse/,,Historical significance,,40.0,"The learned part is the maze configuration. There are 25 squares of the maze. The 16 squares to the left top corner have each one adjacent square down and one adjacent square up, for a total of 16*2 walls. We only need to count the 8 spare walls connecting the squares in the right side and the bottom side. In total there are 16*2+8 walls.",40.0,"The ""training"" consists on the mouse running around and checking each wall (assuming each relay switch is one operation).",,,40.0,Each wall Theseus bumps into is a datapoint,,,,,Confident,,,,,United States of America,,,,,,2025-06-16 04:59,,,,,,Industry,,,,,,,Industry,,,,,Other,,,,
